{"id": "u4PmZOmtko", "number": 22059, "cdate": 1758325494835, "mdate": 1763651847488, "content": {"title": "How Well Can General Vision-Language Models Learn Medicine By Watching Public Educational Videos?", "abstract": "Publicly available biomedical videos, such as those on YouTube, serve as valuable educational resources for medical students. Unlike standard machine learning datasets, these videos are designed for human learners, often mixing medical imagery with narration, explanatory diagrams, and contextual framing. In this work, we investigate whether such pedagogically rich, yet non-standardized and heterogeneous videos can effectively teach general-domain vision-language models biomedical knowledge. To this end, we introduce OpenBiomedVid, a biomedical video instruction tuning dataset comprising 1031 hours of video-caption and Q/A pairs, curated through a multi-step human-in-the-loop pipeline. Diverse biomedical video datasets are rare, and OpenBiomedVid fills an important gap by providing instruction-style supervision grounded in real-world educational content. Surprisingly, despite the informal and heterogeneous nature of these videos, the fine-tuned Qwen-2-VL models exhibit substantial performance improvements across most benchmarks. The 2B model achieves gains of 98.7% on video tasks, 71.2% on image tasks, and 0.2% on text tasks. The 7B model shows improvements of 37.09% on video and 11.2% on image tasks, with a slight degradation of 2.7% on text tasks compared to their respective base models. To address the lack of standardized biomedical video evaluation datasets, we also introduce two new expert curated benchmarks, MIMICEchoQA and SurgeryVideoQA. On these benchmarks, the 2B model achieves gains of 99.1% and 98.1%, while the 7B model shows gains of 22.5% and 52.1%, respectively, demonstrating the models' ability to generalize and perform biomedical video understanding on cleaner and more standardized datasets than those seen during training. These results suggest that educational videos created for human learning offer a surprisingly effective training signal for biomedical VLMs. We release OpenBiomedVid, MIMICEchoQA, SurgeryVideoQA, the fine-tuned models, and the complete codebase to support future research.", "tldr": "Instruction-tuning Qwen-2-VL on 1,031 hours of pedagogical biomedical videos dramatically boosts video and image understanding and includes new expert-curated benchmarks, with all data and code released.", "keywords": ["vision-language models", "biomedicine", "datasets", "evaluations"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fcf643dd9df9746824583d9bb992dfdc21d7b0d9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces OpenBiomedVid, a biomedical video-text dataset (1,031 hours) curated from YouTube educational videos, along with two expert-curated benchmarks—SurgeryVideoQA and MIMICEchoQA—for evaluating biomedical video understanding. The authors further fine-tune Qwen2-VL and InternVL3 models on this dataset, demonstrating improvements on both video and image benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Clear presentation: The paper clearly describes the construction of instruction data, evaluation datasets, and model training procedures, making the methodology easy to follow.\n\n2. Thoughtful data curation: The dataset creation process includes several reasonable and interesting design choices, such as leveraging SigLIP-Medical and Whisper models, to ensure the reliability and quality of the collected data.\n\n3. Practical and novel contribution: While prior works have explored YouTube data for research, the focus on video instruction tuning and the introduction of corresponding evaluation benchmarks fills an important gap in the current biomedical multimodal landscape. The dataset and benchmarks are likely to be of practical use to the community."}, "weaknesses": {"value": "1. Although the paper targets video-level instruction tuning, prior works (e.g., Quilt-1M) have already shown success with image-level instruction data in the medical domain. It remains unclear whether video-level supervision provides a substantial advantage over image-text data for medical VQA tasks. A valuable follow-up experiment could involve creating a subset of the dataset where temporal information is minimized (e.g., few-frame clips or frame-level QA) to empirically assess whether videos or images contribute to the performance gains.\n2. In lines 343–346, the authors state that “performance on video benchmarks remains significantly lower than on text and image benchmarks.” However, the reported results show only modest improvements on text benchmarks (with MedQA even decreasing) and limited gains for PathVQA with the 7B model. The discussion should be more nuanced.\n3. The dataset mainly focuses on videos, but I noticed certain performance improvements on the image-text datasets VQA-RAD and SLAKE. If possible, I encourage the authors to derive and release a medical image instruction tuning dataset from the existing collection. For example, by selecting clips with fewer frames or converting segments that do not require strict temporal encoding into multi-image samples, since many QA pairs may not rely on temporal information. This would further advance the development of medical vision-language models."}, "questions": {"value": "See weaknesses. I may consider raising my score depending on the authors’ response, as I am genuinely interested in this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Jc4vZR6JW6", "forum": "u4PmZOmtko", "replyto": "u4PmZOmtko", "signatures": ["ICLR.cc/2026/Conference/Submission22059/Reviewer_QWpu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22059/Reviewer_QWpu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654023104, "cdate": 1761654023104, "tmdate": 1762942037386, "mdate": 1762942037386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Summary"}, "comment": {"value": "We thank all reviewers for their thoughtful and constructive feedback, and we appreciate the AC for coordinating the review process. Below, we synthesize and address the major concerns raised across multiple reviewers. We have also updated the manuscript with additional sections and result tables.\n\n> 1. Evaluation Bias and Reliability of GPT-4o as a Judge\n\nWe appreciate the concern regarding potential stylistic or evaluator bias introduced by GPT-4o. To assess this, we conducted a comprehensive multi-judge analysis using both GPT-4o and Gemini-2.0-Flash across all 14 evaluated models. As shown in Supplementary Table S1, the two judges produce highly consistent accuracy estimates (overlapping 95% CIs), with 93.0% raw agreement and Cohen’s kappa = 0.748. Their model rankings are nearly identical (Spearman rho = 0.972, Pearson r = 0.993).\n\nTo validate judge reliability further, we ran a human evaluation on 400 items (Supplementary Table S4). Both GPT-4o and Gemini show strong alignment with human annotators (GPT-4o: 90–95%, kappa = 0.72–0.84; Gemini: 89–95%, kappa = 0.66–0.83), and agree with each other at 88–95% (kappa = 0.65–0.85).\n\nImportantly, GPT-4o does not generate new content—the video captions originate from YouTube transcripts; GPT-4o only cleans and formats them into Q/A pairs. Every stage includes human verification, and the two benchmarks (SurgeryVideoQA and MIMICEchoQA) were entirely reviewed by clinical experts. Using LLMs for caption/Q&A generation is standard in prior work (e.g., LLaVA-Med, NeurIPS 2023; Video-ChatGPT, ACL 2024).\n\n> 2. Lack of Deep Failure-Mode Analysis\n\nIn response to reviewer feedback, we performed quantitative and qualitative error analysis for both benchmarks. For MIMICEchoQA, errors concentrate in specific anatomical regions and ultrasound views (e.g., left atrium: 68.4%, pulmonary artery: 66.7%, PSAX/PLAX great-vessels: 70–71%). From 100 manually reviewed errors, ~60% involve severity miscalibration, 25–30% involve threshold ambiguities, and ~10% reflect systematic EF overestimation—patterns consistent with noisy clinical labels and borderline diagnostic categories.\n\nFor SurgeryVideoQA, manual analysis of 100 incorrect predictions reveals interpretable categories: anatomy/localization errors (22%), instrument/technique confusion (25–33%), procedural-step/temporal errors (16%), overly generic answers (10%), and quantitative misestimation (5–10%). These are mostly near-miss errors, reflecting the fine-grained surgical reasoning demanded by the benchmark.\n\n> 3. Data-Quality Measures and Training-Set Noise Characterization\n\nWe provide multiple quantitative quality assessments. For frame filtering, two annotators independently labeled 1,000 SigLIP-selected frames, yielding 95% human–model agreement and 93% human–human agreement. Benchmark construction involved clinical experts: a board-certified cardiologist curated MIMICEchoQA and a medical doctor curated SurgeryVideoQA.\n\nTo further characterize training-set heterogeneity, we reviewed 100 randomly sampled caption–QA pairs. Approximately 15% contained low-information captions, 10% showed mild grounding issues, and 5% had LLM-style artifacts, while 70% were mostly clean and clinically grounded. This distribution reflects the intended heterogeneity of YouTube educational content that our paper investigates. Importantly, both evaluation benchmarks are fully expert-cleaned and do not contain these artifacts.\n\n> 4. Comparison With Biomedical VLMs and Additional Video Baselines\n\nWe appreciate the suggestion to evaluate against medical VLMs. However, cited models such as LLaVA-Med, Med-Flamingo, and MedCLIP are single-image models and cannot process long video sequences (hundreds–thousands of frames) without major architectural modification.\n\nWe expanded Table 1 to include Video-ChatGPT and Video-LLaVA, and our original results already included competitive models capable of video understanding (e.g., InternVideo2.5-Chat-8B, Phi-3.5-vision-instruct, Phi-4-multimodal-instruct). These off-the-shelf video models achieve modest performance (e.g., 7.9–16.4% on SurgeryVideoQA), confirming that our benchmarks are genuinely challenging rather than tailored to Qwen architectures.\n\n> 5. Training under constrained setting (e.g., fixed frame sampling, short 8–16 frame clips)\n\nAs shown in Table S3, we created a “biomed-frame” variant of our dataset by selecting the highest-confidence biomedical frames (via our SigLIP classifier) and pairing them with the same instruction-tuning Q/A. This provides a direct comparison between video-level and frame-level supervision on the same backbones. We find that frame-only tuning captures a substantial portion of the signal, however, full-video supervision remains superior. For instance, on Qwen2-VL-7B, MIMICEchoQA achieves 49.0% with video-level tuning vs. 44.0% with frame-level, and SurgeryVideoQA achieves 25.1% vs. 23.0%."}}, "id": "nRoYMlV7uC", "forum": "u4PmZOmtko", "replyto": "u4PmZOmtko", "signatures": ["ICLR.cc/2026/Conference/Submission22059/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22059/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22059/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763648218387, "cdate": 1763648218387, "tmdate": 1763648218387, "mdate": 1763648218387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the potential of general-purpose vision-language models to be directed towards effective biomedical video understanding by training them on publicly available educational content, primarily sourced from YouTube. The authors have constructed a substantial instruction-tuning corpus, named OpenBiomedVid, which comprises approximately 1,031 hours of clinician-guided biomedical videos, meticulously cleaned captions, and GPT-assisted yet human-verified question-answer pairs. Additionally, they introduce two more challenging expert-curated benchmarks—MIMICEchoQA (echo) and SurgeryVideoQA (surgery)—to ensure that evaluations are not conducted on the same noisy distribution as the training data. Fine-tuning Qwen2-VL (2B/7B) using this dataset results in significant relative improvements in biomedical video question answering (QA) and notable advancements in image visual question answering (VQA). In some instances, performance approaches or even surpasses that of stronger general models when applied to echo-style videos. This finding suggests that \"videos made for humans\" can still serve as an effective supervisory signal for medical vision-language models. However, it is important to note that results on the surgical benchmark remain distinctly lower. This indicates that long and heterogeneous procedural videos continue to pose challenges and highlights concerns regarding stylistic inconsistencies and potential hallucination risks introduced by LLM-in-the-loop curation—a concern the authors attempt to address through human verification. In summary, this work presents a well-motivated dataset and benchmark package along with compelling empirical evidence supporting the notion that public educational videos represent a viable resource for domain adaptation of open vision-language models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear Motivation & Well-Defined Problem: The authors identify an important gap: most biomedical video datasets are either small, narrowly focused, or unsuitable for large-scale multimodal learning. They correctly observe that public educational videos are a massive, untapped source for such efforts.\n- Novel Dataset and Benchmark Creation: The curation of a 1031-hour biomedical video-text dataset from public sources is commendable. The pipeline is systematic, leveraging both expert clinical input and multi-stage LLM filtering. The inclusion of 79,367 Q/A pairs and structured metadata adds substantial utility and diversity.\n- Expert-Curated Evaluation: The introduction of the MIMICEchoQA and SurgeryVideoQA benchmarks is a valuable contribution, offering much-needed standardized, clinically relevant tests for biomedical VLMs. The expert review and curation of questions are a particular highlight."}, "weaknesses": {"value": "The paper creates a same-model, same-style loop: GPT-4o is used to clean captions and generate the training/eval Q&A and metadata, and then the very same GPT-4o is used as the automatic judge to score open-ended answers (binary 0/1). The authors even note the risk of “stylistic bias” and only add a one-off check with Gemini-2.0-Flash, but GPT-4o remains the primary scorer. This tightly couples data generation and evaluation to one model’s style, making gains plausibly evaluation-protocol-driven rather than true capability. \n\n- Lack of evaluation against domain-specific medical VLMs. The paper compares mainly with general-purpose multimodal backbones (Qwen2-VL at different scales, InternVL3-8B, GPT-4o, Gemini-2.0-Flash, etc.) on the proposed biomedical video QA benchmarks, but does not report results for the most directly relevant medical instruction-tuned VLMs such as LLaVA-Med, Med-Flamingo (medical adaptation of Flamingo), or widely used medical multi-image models (e.g., MedCLIP), even though many of them are cited in Related Work. Because these models target clinical/biomedical vision-language understanding on VQA-RAD, PathVQA, SLAKE and related tasks, including at least a frame-based or image-only adaptation of them on the authors’ benchmarks would make the claimed gains over “prior medical VLMs” easier to judge. The current tables therefore make it hard to tell whether the improvement comes from the proposed video-centric data and pipeline or simply from using stronger general backbones. (We acknowledge that some models, e.g. MedCLIP or MedGemini, are image-centric or not fully open, but even a best-effort comparison or a discussion of feasibility would strengthen the evaluation.)\n- Reliance on LLM-as-a-judge without human grounding. For the open-ended SurgeryVideoQA benchmark, the paper evaluates all models with GPT-4o as the automatic grader, assigning binary correctness against the reference answer. Since GPT-4o was also involved in earlier stages of caption refinement and QA generation, this creates a potential style / phrasing bias toward GPT-4o-like answers. The authors do run a second pass with Gemini-2.0-Flash and report that the ranking is broadly consistent, which is helpful, but both judges are frontier LLMs with unknown overlap in training data and alignment objectives, and no human adjudication, inter-rater agreement, or adversarial stress tests are provided. As a result, part of the reported gains on SurgeryVideoQA could still be influenced by judge-specific preferences rather than purely by better video understanding.\n- Potential residual data-quality and provenance issues. Although the paper presents a multi-stage, human-in-the-loop curation pipeline (YouTube retrieval → GPT-labeled frame filtering with a fine-tuned SigLIP → GPT-4o caption refinement → GPT-4o Q/A generation → human verification) and even reports ~95% agreement on the frame-filtering stage, many of the quality controls are described only at a high level. In particular, the paper does not spell out how deep the human verification went (per-clip vs per-QA, random spot checks vs full passes), how many annotators were involved, or what the inter-annotator agreement was beyond the small sample quoted. Moreover, because GPT-4o is used both to clean captions and to synthesize Q/A pairs, typical LLM failure modes—overly generic answers, temporal misalignment with the actual video segment, or medical hallucination—are plausible but not systematically audited or reported. This matters especially because the paper itself shows that models trained on the noisy YouTube-derived corpus still perform noticeably worse on the cleaner, expert-curated benchmarks (MIMICEchoQA, SurgeryVideoQA), suggesting that remaining noise in the large-scale training split may be a limiting factor. A more explicit error analysis (e.g., failed frame segmentation, ambiguous captions, low-quality or hallucinated Q/A) would make the dataset contribution stronger.\n- Underdeveloped video baselines on the proposed benchmarks. Because the core claim of the paper is about video-centric biomedical understanding, the experimental setup on MIMICEchoQA and SurgeryVideoQA would be more convincing if it included stronger, publicly available video–language models beyond the authors’own Qwen2-VL fine-tunes. At minimum, prior general-purpose video chat models (e.g., Video-ChatGPT, Video-LLaVA, PALIGemma-style video variants) could be run in a frame-sampling or short-clip regime to provide external points of reference, even if they are not domain-tuned. Their absence makes the reported gains look partly relative to the authors’ chosen baselines rather than to the broader video–language landscape, and it also hides whether the new benchmarks are genuinely “hard” for off-the-shelf video models or only for image-first VLMs."}, "questions": {"value": "1. You cite several domain-specific multimodal/medical VLMs in Related Work (e.g., LLaVA-Med, Med-Flamingo, MedCLIP), but they do not appear in the main comparison tables. Can you clarify which of these models you actually attempted to run, and what prevented you from including their results?\n\n2. Right now the improvements are mostly against Qwen2-VL/InternVL-style backbones. How can we be sure the gains come from your video-centric data/pipeline rather than simply from using a stronger base model?\n\n3. The evaluation of SurgeryVideoQA was conducted using GPT-4o as the automatic grading system. Given that GPT-4o was also employed for the refinement of captions and question-and-answer pairs, what measures were taken to mitigate potential style bias towards outputs resembling those generated by GPT-4o?\n\n4. You mention using Gemini-2.0-Flash as a second judge and finding similar rankings. Could you provide the agreement numbers between GPT-4o and Gemini on this benchmark?\n\n5. You report ~95% agreement on frame filtering, but on how many samples, with how many annotators, and at which stage of the pipeline was this measured (video-level vs clip-level vs QA-level)?\n\n6. Is human verification applied to every GPT-generated Q/A pair, or only to a sampled subset? If sampled, what was the sampling strategy and coverage?\n\n7. Would it be feasible to run them in a constrained setting (e.g., fixed frame sampling, short 8–16 frame clips) just to position your benchmarks relative to the broader video-VLM ecosystem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QuPCWhrInP", "forum": "u4PmZOmtko", "replyto": "u4PmZOmtko", "signatures": ["ICLR.cc/2026/Conference/Submission22059/Reviewer_fKBP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22059/Reviewer_fKBP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844922411, "cdate": 1761844922411, "tmdate": 1762942037191, "mdate": 1762942037191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether general-purpose vision-language models can effectively learn biomedical knowledge from publicly available educational videos on platforms like YouTube. It introduces OpenBiomedVid, a large-scale, diverse instruction-tuning dataset comprising 1,031 hours of biomedical video content, curated through a multi-step human-in-the-loop pipeline that filters frames, refines captions, and generates question-answer pairs. For evaluation, two new benchmarks are released: Surgery VideoQA and MIMICEchoQA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The effort to create these resources is commendable. The field lacks large-scale biomedical video datasets, and OpenBiomedVid, along with the two new benchmarks, represents a significant contribution."}, "weaknesses": {"value": "The paper is undermined by several major flaws in its methodology and results, which make the central claim (that the models are \"learning medicine\") unconvincing.\n\n1.\tPotential Data Contamination and LLM Bias: A major concern is the pervasive use of GPT-4o throughout the pipeline (frame annotation, caption refinement, Q/A generation, and evaluation). Although the authors implement safeguards, there is a non-trivial risk of the model learning a \"stylistic bias\" or patterns specific to GPT-4o's output, rather than genuine biomedical reasoning. The evaluation with Gemini mitigates this but does not fully eliminate the concern, as the fine-tuned models' training data was itself shaped by GPT-4o. \n\n2.\tAbsolute Performance is Still Low: Despite impressive relative gains, the absolute performance on the video benchmarks remains low (e.g., 25.1% for the 7B model on SurgeryVideoQA). The paper correctly notes that these tasks are challenging, but the low scores highlight that the problem is far from solved and that the models are not yet reliable. This should be more prominently discussed in the context of the claim that models can \"learn medicine.\" \n\n3.\tThe paper claims the models are \"learning medicine,\" but their performance on the standard MedQA text benchmark significantly decreased after fine-tuning. For example, the Qwen-2-VL-7B-Biomed model’s accuracy dropped from a baseline of 52.6% to 47.4%. This suggests the informal, \"noisy\" knowledge from the videos may be conflicting with, or causing the model to \"forget,\" formal textbook knowledge. This directly contradicts the paper's primary narrative.\n\n4.\tLimited Analysis of \"Why\" and Failure Modes: The paper excellently demonstrates that the method works but provides less insight into why and how it fails. A deeper analysis of the types of questions or video segments where the model struggles (e.g., temporal reasoning in long surgical videos vs. static frame understanding in echocardiograms) would be highly valuable. The qualitative examples are good but are primarily success cases. \n\n5.\tClarity on Training-Test Split and Overlap: While the authors state there is no video ID overlap between OpenBiomedVid and SurgeryVideoQA, both are sourced from YouTube. There is a potential for concept or stylistic overlap. A more detailed discussion on how the \"cleanliness\" and focus of the evaluation set differ from the training data would clarify the generalization claim."}, "questions": {"value": "please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lts6YS5utc", "forum": "u4PmZOmtko", "replyto": "u4PmZOmtko", "signatures": ["ICLR.cc/2026/Conference/Submission22059/Reviewer_bp18"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22059/Reviewer_bp18"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933019834, "cdate": 1761933019834, "tmdate": 1762942036875, "mdate": 1762942036875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **OpenBiomedVid**, a large-scale dataset of **1,031 hours of biomedical educational videos** from YouTube, containing **22K clips** and **79K Q/A pairs**. The dataset is created through a **human-in-the-loop pipeline** that combines Whisper transcription, GPT-4o caption refinement, and expert verification. The authors also introduce two benchmarks, **SurgeryVideoQA** and **MIMICEchoQA**, for biomedical video-language evaluation. Fine-tuning **Qwen2-VL (2B/7B)** models on OpenBiomedVid yields strong performance gains across biomedical video and image benchmarks, showing that open educational videos can be an effective training signal."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses an underexplored area: **biomedical video-language modeling**.  \n2. The dataset is **large, diverse, and well-organized**, potentially a valuable community contribution.  \n3. The **data curation pipeline** is systematic, combining LLM-based processing with human verification.  \n4.  Empirical results are consistent across multiple benchmarks and architectures."}, "weaknesses": {"value": "1.  **Evaluation bias** — heavy dependence on GPT-4o for caption refinement, Q/A generation, and evaluation creates potential bias and reproducibility concerns.  \n2. **Lack of deep analysis** — minimal discussion of failure cases, temporal reasoning, or cross-domain generalization.  \n3. **Ethical and legal clarity** — the discussion of data licensing, PHI risk, and content ownership is insufficient.  \n4. **Incremental insight** — while scale is impressive, the core finding (“educational videos help”) feels somewhat intuitive and under-analyzed."}, "questions": {"value": "1. **Will the dataset and benchmarks be fully released?** If so, under what license and with what access restrictions?  \n2. How is **personal or sensitive information** (e.g., PHI, identifiable subjects) detected and filtered in the YouTube videos?  \n3. Does the fine-tuned model show **transferability** to unseen clinical or institutional datasets?  \n4. What is the ratio of educational diagrams/narration-only videos to true clinical imaging?  \n5. Are there **quantitative measures of data quality**, such as annotation agreement or verification accuracy?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QafLh2cICm", "forum": "u4PmZOmtko", "replyto": "u4PmZOmtko", "signatures": ["ICLR.cc/2026/Conference/Submission22059/Reviewer_QzV3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22059/Reviewer_QzV3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942500332, "cdate": 1761942500332, "tmdate": 1762942036656, "mdate": 1762942036656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}