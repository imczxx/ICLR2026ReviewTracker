{"id": "awXCOmF6ia", "number": 14698, "cdate": 1758241964389, "mdate": 1759897354343, "content": {"title": "Zeroth-Order Methods for Stochastic Nonconvex Nonsmooth Composite Optimization", "abstract": "This work aims to solve a stochastic nonconvex nonsmooth composite optimization problem. Previous works on composite optimization problem requires the differentiable part to satisfy Lipschitz smoothness or some relaxed smoothness conditions, which excludes some machine learning examples such as regularized ReLU network and sparse support matrix machine. In this work, we focus on stochastic nonconvex composite optimization problem without any smoothness assumptions. In particular, we propose two new notions of approximate stationary points for such optimization problem (one stronger than the other) and obtain finite-time convergence results of two zeroth-order algorithms to these two approximate stationary points respectively. Finally, we demonstrate that these algorithms are effective using numerical experiments.", "tldr": "We propose two new stationary notions for stochastic nonconvex nonsmooth composite optimization, and obtain convergence rates of two zeroth-order algortihms to the two new stationary points respectively.", "keywords": ["Zeroth-order", "stochastic optimization", "nonconvex optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d9c0d78f872d8ce6b52329ba48e8bc025b3b68fe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes new approximate stationary points and zeroth-order stochastic algorithms for solving the stochastic nonconvex nonsmooth composite optimization problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces novel approximate stationary points by utilizing the Goldstein $\\delta$-subdifferential for the nonsmooth stochastic composite optimization problem.\n\n2. This paper proposes novel zeroth-order stochastic methods with an improved convergence rate than existing work [1].\n\n**References**\n\n[1] Liu, Z., Chen, C., Luo, L., & Low, B. K. H. (2024, July). Zeroth-order methods for constrained nonconvex nonsmooth stochastic optimization. In Forty-first International Conference on Machine Learning."}, "weaknesses": {"value": "1. My primary concern regarding this work is that it appears to be a straightforward extension of the previous study [1], encompassing approximate stationary points and stochastic algorithms. As a result, the contribution of this work seems incremental, and its novelty is limited.\n\n2. Since the objective function $\\phi(x)$ is the sum of two nonsmooth functions, can I just apply the zeroth-order unconstrained stochastic method introduced in [2] on $\\phi(x)$ to achieve a convergence rate of $\\mathcal{O}(d \\delta^{-1} \\epsilon^{-3})$.  \n\n**References**\n\n[1] Liu, Z., Chen, C., Luo, L., & Low, B. K. H. (2024, July). Zeroth-order methods for constrained nonconvex nonsmooth stochastic optimization. In Forty-first International Conference on Machine Learning.\n\n[2] Kornowski, G., & Shamir, O. (2024). An algorithm with optimal dimension-dependence for zero-order nonsmooth nonconvex stochastic optimization. Journal of Machine Learning Research, 25(122), 1-14."}, "questions": {"value": "See weakness 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q2oUW8xo8L", "forum": "awXCOmF6ia", "replyto": "awXCOmF6ia", "signatures": ["ICLR.cc/2026/Conference/Submission14698/Reviewer_Aq7C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14698/Reviewer_Aq7C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760869418723, "cdate": 1760869418723, "tmdate": 1762925063640, "mdate": 1762925063640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies stochastic nonconvex nonsmooth composite optimization problems. The key contributions are: (i) Two new notions of approximate stationarity: ($\\gamma$, $\\delta$, $\\epsilon$)-proximal Goldstein stationary points (PGSP) and ($\\delta$, $\\epsilon$)-conditional gradient Goldstein stationary points (CGGSP) which generalize Goldstein stationary points to composite objectives. (ii) Two zeroth-order algorithms: zeroth-order proximal gradient descent and zeroth-order generalized conditional gradient methods. Both methods achieve finite-time convergence guarantees to the above approximate stationary points, with and without variance reduction. (iii) Improved complexity bounds upon prior work such as Liu et al. (2024). (iv) Empirical validation on a regularized ReLU network illustrating practical convergence of both methods."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The definitions of PGSP and CGGSP extend Goldstein-type stationarity to nonsmooth composite settings, which had no tractable finite-time criteria before.\n\n2. The convergence and complexity proofs are rigorous, connecting zeroth-order smoothing and nonsmooth analysis.\n\n3. The paper clearly relates its framework to proximal methods, conditional gradient methods, and previous Goldstein-stationary notions.\n\n4. Although small-scale, the experiments demonstrate that the methods work as claimed and variance reduction indeed accelerates convergence.\n\n5. The exposition is organized and self-contained, with detailed assumptions, propositions, and proofs."}, "weaknesses": {"value": "1. Only a toy ReLU network example ($d=34$) is shown. There is no comparison with baselines (e.g., stochastic subgradient, first-order PGD, or other zeroth-order methods).\n\n2. While the theory is clean, it is not obvious how these algorithms perform in high-dimensional machine-learning applications.\n\n3. The paper could benefit from clearer motivation and intuition before diving into technicalities.\n\n4. The comparison to contemporary zeroth-order nonconvex optimization papers (e.g., Cutkosky 2023) could be deepened."}, "questions": {"value": "1. Could the proposed stationarity notions be extended to settings where $h$ is nonconvex but prox-friendly?\n2. How sensitive are the algorithms to the smoothing radius $\\delta$ in practice?\n3. Is there any connection between PGSP and the weak subgradient mappings used in Clarke’s generalized gradients?\n4. Have the authors tried larger-scale tasks (e.g., CIFAR or low-rank matrix problems) to test scalability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gDfQij3ujR", "forum": "awXCOmF6ia", "replyto": "awXCOmF6ia", "signatures": ["ICLR.cc/2026/Conference/Submission14698/Reviewer_d6dV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14698/Reviewer_d6dV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854239840, "cdate": 1761854239840, "tmdate": 1762925063258, "mdate": 1762925063258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies zeroth-order methods for stochastic nonconvex nonsmooth composite optimization, proposes PGSP and CGGSP as approximate stationary notions, and analyzes 0-PGD and 0-GCG algorithms with minibatch and variance-reduction gradient estimators, giving finite-time complexity bounds. Experiments on synthetic two-layer ReLU and ResNet-20 validate algorithmic behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors extend the notion of constrained stationarity to general nonsmooth composite settings by replacing the projection operator with a proximal mapping. This yields a unified definition applicable to many practical problems. Two zeroth-order algorithms are proposed to handle different oracle settings (proximal vs LMO). The convergence analyses and intractability results are presented clearly.\n- The paper is generally well-written, logically organized, and easy to follow. Definitions and assumptions are stated clearly, and the appendix provides detailed proofs."}, "weaknesses": {"value": "-  While the definition of PGSP extends previous constrained stationary notions to the nonsmooth setting, this extension is **rather straightforward** — essentially replacing the projection operator in the constrained case by a proximal operator. Similarly, the algorithmic framework closely follows that of **Liu et al. (2024)** for the constrained Lipschitz case, with minor modifications. \n- Although the paper claims an improvement in complexity bounds, the improvement is only in the **parameter dependence** , which is mainly due to the tight bound for $F_\\delta(x_0) - F_\\delta(x_T)$ (as mentioned in **Comparison with Constrained Optimization**), while the overall order of complexity remains identical.  For the tight bound, I think the new term $\\psi - \\psi_*$ introduced in the analysis may weaken the claimed improvement, and it is unclear whether this scaling is indeed tight or essential. The resulting theory, though consistent, does not introduce fundamentally new mathematical tools or algorithmic ideas. A detailed side-by-side comparison of assumptions, complexities, and definitions would strengthen the contribution.\n-  The complexity bounds in Table 1 do not explicitly include $\\gamma$. If the complexity is indeed independent of $\\gamma$, does that imply the same rate holds for any $\\gamma$? This point needs further explanation, as $\\gamma$ appears both in the proximal mapping and smoothing radius, and typically affects the variance–bias trade-off."}, "questions": {"value": "Please clarify whether the derived iteration and query complexities depend on $\\gamma$.\n If not, why does $\\gamma$ appear in the proximal update?\n Intuitively, the step size affects both convergence and stationarity precision — this should be explicitly reflected in the bounds.\n\n$\\delta$ appears both in the smoothing process and the stationarity definition. How should δ be chosen in practice? What happens when δ is too small — does the variance term blow up?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oDtTz1gd1j", "forum": "awXCOmF6ia", "replyto": "awXCOmF6ia", "signatures": ["ICLR.cc/2026/Conference/Submission14698/Reviewer_1QjJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14698/Reviewer_1QjJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893993482, "cdate": 1761893993482, "tmdate": 1762925062586, "mdate": 1762925062586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}