{"id": "NcFtbEJ4EH", "number": 9591, "cdate": 1758129007332, "mdate": 1759897710545, "content": {"title": "Beyond Open-World: COSRA, a Training-Free Self-Refining Approach to Open-Ended Object Detection", "abstract": "Traditional object detection models rely on predefined categories, which limit their ability to recognize unseen objects in open-world settings. Recent efforts in open-world and open-ended detection have begun to address this challenge by enabling models to go beyond closed-set assumptions. However, these approaches often remain limited in terms of scalability, adaptability, or generalization to diverse environments. To overcome these restrictions, we introduce a Context-oriented Open-ended Self-Refining Annotation model (COSRA), a training-free framework that combines context-aware reasoning with self-learning for open-ended object labeling. COSRA leverages Large Language Models (LLMs) to generate candidate labels for unknown objects based on contextual cues from known entities within a scene. COSRA then pairs these labels with visual embeddings to construct an Embedding-Label Repository (ELR), enabling inference without category supervision. To further enhance consistency, we introduce a self-refinement loop that re-evaluates repository labels using visual cohesion analysis and KNN-based majority relabeling. For a newly encountered unknown object, COSRA retrieves visually similar instances from the ELR and applies frequency-based voting and cross-modal re-ranking to assign a robust label. Our experimental results on COCO and LVIS datasets demonstrate that COSRA outperforms state-of-the-art methods and effectively annotates novel categories using only visual and contextual signals without requiring any fine-tuning or retraining.", "tldr": "COSRA is a training-free, context-aware, self-refining method for open-ended object labeling. LLM-proposed labels + visual embeddings (ELR) with KNN/frequency voting and cross-modal re-ranking label novel objects; SOTA on COCO/LVIS.", "keywords": ["Open-Ended Object Detection", "Open-WorldOpen-World Object Detection", "Large Language Models", "Context-Aware Annotation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3b23aba78fb405ec5a8cd755e17e1660a68e2ffa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes COSRA, a training-free framework for open-ended object detection that aims to automatically assign semantic labels (e.g., “faucet” instead of “unknown”) to detected unknown objects. The method combines SAM and Faster R-CNN to propose unknown regions, uses a large language model (LLM) to generate candidate labels based on scene context and visual attributes, and refines these labels through CLIP-based re-ranking and an iterative self-correction process over a dynamic knowledge base called the Embedding-Label Repository (ELR). This enables open-ended naming without fine-tuning or reliance on a predefined vocabulary."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "First, COSRA requires no training on novel categories and relies only on off-the-shelf foundation models (e.g., SAM, CLIP, and LLMs), offering practical convenience for deployment.\nSecond, the use of structured context-aware prompts to guide LLM label generation, combined with the ELR and iterative refinement, yields numerical results on COCO and LVIS that surpass current methods.\nThird, the paper emphasizes open-ended naming, the ability to generate semantic labels without a fixed vocabulary, which conceptually advances beyond traditional open-world object detection."}, "weaknesses": {"value": "The paper addresses an interesting direction: moving beyond open-world detection by assigning meaningful names to unknown objects. However, the experimental design and practical utility suffer from several critical issues:\n\n- Redundancy with LLM capabilities: The core labeling mechanism relies entirely on an LLM to generate semantic names for unknown regions, using visual and contextual cues. Yet foundation models (e.g., GPT-4V, LLaVA 4) already demonstrate strong zero-shot object detection and naming abilities directly from images. It is unclear why a complex pipeline (SAM + detector + ELR + KNN voting) is needed when an LLM could, in principle, process the full scene and output both bounding boxes and labels end-to-end.\n\n- Poor practicality for real-world use: The pipeline depends on multiple large models (SAM, CLIP, VLM, LLM) and expensive operations such as KNN search over a large repository. This makes real-time inference infeasible and limits applicability in resource-constrained settings.\n\n- Risk of evaluation bias and knowledge leakage: The evaluation uses LLM-generated labels for unknown categories on standard benchmarks like COCO and LVIS. However, LLMs may exploit prior knowledge of dataset structure—for example, inferring likely “unknown” classes from the set of “known” classes. As shown in [1], providing GPT-3.5 with known COCO class names enables it to predict plausible unknown categories with high recall (~79%), nearly matching performance when the true unknown labels are given. This suggests that reported gains may stem from dataset-specific priors rather than genuine generalization.\n\n- Insufficient reproducibility: Although pseudocode is provided, the overall pipeline is highly complex, involving prompt engineering, attribute extraction, ELR construction, and multi-stage refinement. Without full implementation details or code, it is difficult for readers to reproduce or build upon the method.\n\n[1] Open World Object Detection in the Era of Foundation Models"}, "questions": {"value": "In light of the above concerns, the authors should address the following:\n\n- Given that modern LLMs can perform open-ended detection directly, why not use an LLM as the primary detector? Please compare COSRA against end-to-end LLM-based detectors (e.g., on RefCOCO or similar grounding benchmarks) to justify the added complexity.\n- Please report the computational cost (e.g., inference time, memory usage) of each component, especially the KNN search and LLM calls, to assess real-world feasibility.\n- Please clarify whether the ELR construction or LLM prompting introduces dataset-specific bias or knowledge leakage, particularly given the strong prior knowledge of COCO/LVIS category structures in current LLMs.\n- Please provide more core code, detailed descriptions of the prompt templates, attribute extraction, and ELR update logic to ensure reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dPETLLp214", "forum": "NcFtbEJ4EH", "replyto": "NcFtbEJ4EH", "signatures": ["ICLR.cc/2026/Conference/Submission9591/Reviewer_XkZN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9591/Reviewer_XkZN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567997668, "cdate": 1761567997668, "tmdate": 1762921138845, "mdate": 1762921138845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes COSRA, a training-free framework for open-ended object detection that combines context-aware reasoning from an LLM with CLIP-based visual alignment. The method iteratively refines object hypotheses and labels through self-learning without requiring fine-tuning or category supervision. The problem is highly relevant, and the approach is conceptually novel, aiming to move beyond conventional open-vocabulary detection."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel problem setting: The paper addresses open-ended object detection, an underexplored but crucial challenge in open-world vision.\n2. Innovative idea: The “training-free + self-refining” approach that integrates LLM reasoning and CLIP similarity is original and well-motivated.\n3. Thorough experimentation: Includes comparisons with diverse baselines (closed-set, open-vocabulary, and open-ended models) and well-structured ablation studies.\n4. Clarity and presentation: The paper is clearly written, and the design of the “Embedding–Label Repository” and refinement loop is easy to follow."}, "weaknesses": {"value": "1. Overstated “training-free” claim: The method heavily depends on pretrained foundation models , making performance and reproducibility strongly tied to these models.\n2. Limited robustness and validation: The system assumes semantic similarity corresponds to visual similarity, which may not hold across object categories. There is no objective metric for evaluating the correctness of generated labels.\n3. Potential circularity: CLIP similarity is used both for candidate selection and evaluation, introducing possible bias or circular validation.\nMissing analysis: The impact of different LLM/VLM choices and LLM hallucinations is not explored.\n4. Scalability concerns: The approach may face efficiency and cost issues in large-scale or real-time detection due to reliance on LLM inference."}, "questions": {"value": "1. On the “training-free” claim:\nCould the authors clarify the boundary of “training-free”? Since COSRA relies on pretrained SAM, CLIP, and LLaMA, to what extent can the system’s performance be considered independent of prior training? It would help to explicitly discuss whether COSRA’s novelty lies in architecture design or knowledge transfer from these pretrained models.\n\n2. LLM dependence and hallucination:\nHow does the system handle cases where the LLM generates incorrect, irrelevant, or hallucinated object labels? Are there mechanisms (e.g., confidence filtering, cross-checking) to ensure semantic validity of generated labels? Quantitative or qualitative examples would strengthen the pape"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jo88nF15Bc", "forum": "NcFtbEJ4EH", "replyto": "NcFtbEJ4EH", "signatures": ["ICLR.cc/2026/Conference/Submission9591/Reviewer_cEFM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9591/Reviewer_cEFM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576785117, "cdate": 1761576785117, "tmdate": 1762921138481, "mdate": 1762921138481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free framework to address open-ended object detection through context-aware reasoning and self-refining mechanisms. The pipeline contains three steps: In the first step, it uses SAM as a class-agnostic region proposal generator. And it classifies proposals into known and unknown subsets. In the second step, it first uses CLIP to assign some predefined attributes to unknown objects and then uses an LLM to generate free-form classnames for them based on the context. Finally, the method refines the predictions via grouping. The model is tested on COCO and LVIS and gets relatively high performance."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-written. Although the proposed method is a little bit complex, this article presents the entire method in a highly organized manner, enabling readers to clearly understand the whole pipeline. Further, the red boxes in equations make them easy to understand.\n2. The paper provides an in-depth analysis in Section 4.3, showing that the method is scalable with the repository size and scene context. These analyses provide a deeper understanding of the method."}, "weaknesses": {"value": "1. The claim in Line 057-059 ''no existing system fully solves open-ended naming cleanly—i.e., given a truly unseen object, produce a correct semantic label (e.g., “giraffe”) without that name ever appearing in its vocabulary'' is overclaimed.\n- The proposed method uses CLIP for similarity comparison, which is pretrained on massive image-text pairs. And novel object classnames exist in CLIP's vocabulary.\n- The proposed method also uses an LLM to generate open-ended classnames. The LLM is pretrained on massive object description texts so that it can predict the classnames based on the context.\n2. As the paper uses a strong LLM to predict classnames, why not directly use a strong MLLM (like Qwen3-VL[1], DAM[2]) to predict the classnames directly from pixels? These models already have strong region perception ability.\n3. The comparisons with other methods in Table1 and Table2 are not fair, as the proposed method uses other strong models like SAM and LLaMA-4.\n4. The comparisons in Table1 and Table2 lack many strong baselines, including but not limited to [3,4,5,6].\n5. As shown in Table 9, the inference speed is significantly slower than the others.\n\nI am quite sorry. But currently, the paradigm does not make sense to me.\n\n\n\n\n[1] Qwen3-VL. https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct\n\n[2] Describe Anything: Detailed Localized Image and Video Captioning. In ICCV 2025.\n\n[3] DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection. In CVPR 2024.\n\n[4] T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy. In ECCV 2024.\n\n[5] LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models. In CVPR 2025.\n\n[6] A Hierarchical Semantic Distillation Framework for Open-Vocabulary Object Detection. In TMM 2025."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BFDqKdpqVa", "forum": "NcFtbEJ4EH", "replyto": "NcFtbEJ4EH", "signatures": ["ICLR.cc/2026/Conference/Submission9591/Reviewer_b61E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9591/Reviewer_b61E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708202844, "cdate": 1761708202844, "tmdate": 1762921138173, "mdate": 1762921138173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenging task of open-ended object detection, where the system must assign correct semantic labels to truly unseen objects (e.g., identifying a “giraffe” without ever having that label in its vocabulary). The proposed approach, COSRA (Context-oriented Open-ended Self-Refining Annotation), is a training-free framework that leverages context-based reasoning and self-refinement. Experiments on COCO and LVIS demonstrate strong performance, suggesting the effectiveness of the method in open-world recognition settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The core idea is clear and well-motivated, and the example provided in the Introduction effectively illustrates the problem setting.\n\nThe paper is well-written and the exposition makes it easy to follow the main contributions.\n\nThe proposed self-refinement strategy by combining iterative visual-cohesion reasoning with K-NN-based relabeling, whihc offers a principled approach to mitigating label noise."}, "weaknesses": {"value": "Although presented as a training-free approach, the method relies heavily on multiple computationally expensive pre-trained models (e.g., SAM, Faster R-CNN, CLIP, LLaMA), which raises questions about practicality and fairness in comparison to other methods.\n\nThe selection of system components is not sufficiently justified. For example, the use of Faster R-CNN rather than DETR for object detection is not explained, and the claim that “any detector can be employed” is not empirically supported.\n\nThe write-up claims SOTA on COCO/LVIS but lacks detailed comparisons to strong baselines."}, "questions": {"value": "What are the memory and resource figures?\n\nProvide failure cases illustrating scenarios where COSRA struggles."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oFLIId7lvM", "forum": "NcFtbEJ4EH", "replyto": "NcFtbEJ4EH", "signatures": ["ICLR.cc/2026/Conference/Submission9591/Reviewer_QQMA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9591/Reviewer_QQMA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987684638, "cdate": 1761987684638, "tmdate": 1762921137783, "mdate": 1762921137783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}