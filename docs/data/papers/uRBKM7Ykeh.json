{"id": "uRBKM7Ykeh", "number": 8497, "cdate": 1758087363814, "mdate": 1759897780499, "content": {"title": "VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs", "abstract": "Multimodal Large Language Models (MLLMs) encounter significant computational and memory bottlenecks from the massive number of visual tokens generated by high-resolution images or multi-image inputs. Previous token compression techniques are often constrained by heuristic rules that risk discarding critical information. They may suffer from biases, such as attention sinks, that lead to sharp performance drops under aggressive compression ratios.To address these limitations, we reformulate token compression as a lightweight plug-and-play framework that reformulates token compression into an end-to-end learnable decision process. To be specific, we propose VisionSelector, a scorer module decoupled from the backbone MLLM that incorporates a differentiable Top-K mechanism and a curriculum annealing strategy to bridge the training–inference gap, enabling efficient and adaptive token selection various arbitrary compression rates. Remarkably lightweight with only 12.85M trainable parameters, VisionSelector demonstrates  generalization across various compression rates and adaptively identifying critical tokens. This leads to superior performance across all compression budgets, evidenced by preserving 100% accuracy on MME with 30% retention budget, outperforming prior methods by 12.21% at 10% retention budget, and doubling prefill speed.  Code and models will be publicly available.", "tldr": "", "keywords": ["Token Compression; Efficient MLLMs; Long Context"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f3ed197c5c29f07e02f16ddd4790f6def13ef72c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces VisionSelector, an end-to-end learnable visual token compression framework tailored for multimodal large language models (MLLMs). The key idea is to move beyond heuristic or fixed-rule post-processing of visual tokens, proposing instead a plug-and-play module that scores and selects tokens using a lightweight importance scorer, a differentiable Top-K mechanism, and a curriculum-based annealing strategy to bridge the gap between training and inference. VisionSelector is designed to be flexible, efficient, and easily integrated into existing MLLMs; importantly, it generalizes across different compression rates. Extensive experiments on diverse benchmarks—including image, video, and text-rich VQA datasets—demonstrate that VisionSelector achieves better accuracy-efficiency tradeoffs than prior state-of-the-art baselines, sometimes even improving performance beyond the uncompressed model by filtering noise."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Innovative End-to-End Framework:** The method reframes visual token pruning as a fully learnable, task-driven optimization process. By integrating the selection module as a lightweight and modular scorer, it sidesteps many brittleness and generalization issues that plague heuristic and rule-based approaches.\n\n- **Differentiable Top-K with Curriculum Annealing:** Using a differentiable Top-K operator (see Section 3.3 and Equation 3, page 4–5) together with a curriculum annealing strategy (Section 3.4), VisionSelector provides a novel way of bridging the selection gap between soft selection during optimization and hard pruning at inference.\n\n- **Plug-and-Play Integration:** The architecture (see Figure 2, page 3) is clean, minimally invasive, and compatible with existing MLLMs and inference accelerators (like FlashAttention)."}, "weaknesses": {"value": "1. **Missing and Incomplete Related Work Coverage:** The paper omits several relevant, recent works on token compression for multimodal LLMs (see the 'Potentially Missing Related Work' section for specifics), some of which could provide important baselines or complementary perspectives (e.g., LLaVA-Scissor, VoCo-LLaMA, TokenCarve), especially regarding adaptive or explainability-driven approaches. This literature gap is notable for a field moving as fast as MLLMs.\n2. **Limited Theoretical Justification Beyond Empirical Performance:** While the differentiable Top-K mechanism and importance scorer are mechanically sound, theoretical analysis is minimal regarding optimality or generalization (e.g., what regimes guarantee retention of task-relevant tokens; how the annealing weight λ interacts with the main learning objective). There's no formal guarantee on information preservation aside from empirical robustness claims. Section 3.4, for instance, lacks even limited theoretical insight into the properties of the composite loss.\n3. **Insufficient Discussion of Failure Modes and Bias:** The limitations section is brief and does not address corner cases or typical scenarios where hard selection could result in catastrophic output failures (e.g., if a batch of tokens is scored identically, or if rare-but-critical tokens are missed). It also does not analyze if some semantic categories are disproportionately pruned, which is a known failure in pruning/selection literature.\n4. **Ablation Scope is Strong but Not Exhaustive:** Although ablations are included (see Table 3, 5), they mostly focus on data, λ annealing, and projection dimension. There's no targeted ablation on the scorer’s architecture (e.g., two layers vs. deeper/nonlinear networks, effects of near-zero initialization), or versions using only soft selection (removing hard-masking), which would clarify the necessity of each design choice.\n5. **Results Reported Primarily on a Single MLLM Backbone:** The main text emphasizes results on Qwen2.5-VL-7B, with some reference to a 3B model in the appendix. However, the generalizability to other popular MLLM backbones (e.g., LLaVA, OpenFlamingo, GPT4-V, etc.) is not empirically validated, limiting claims of plug-and-play universality."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hiV0KuyYF9", "forum": "uRBKM7Ykeh", "replyto": "uRBKM7Ykeh", "signatures": ["ICLR.cc/2026/Conference/Submission8497/Reviewer_8MrQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8497/Reviewer_8MrQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761651830082, "cdate": 1761651830082, "tmdate": 1762920370172, "mdate": 1762920370172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the explosion of visual tokens in Multimodal LLMs for high-res images, multi-image inputs, and video, which hurts memory and latency. Existing pruning/merging methods rely on heuristics (e.g., attention saliency) and often drop critical content under aggressive compression.\n\nThe authors propose VisionSelector, a lightweight (~12.85M params) plug-in module placed between the vision encoder and the LLM. It scores each visual token via a Learnable Importance Scorer (LIS), and uses a Differentiable Top-K Selection (DTS) scheme during training to learn which tokens to keep under a given budget; inference uses hard Top-K. The base MLLM remains frozen.\n\nA Curriculum Annealing loss encourages the soft training mask to match the hard inference mask, aiming to close the train–inference gap.\nResults claim: keeping only 10–30% of tokens still preserves near-baseline accuracy (sometimes ~100%), while reducing memory and roughly halving prefill latency. The same trained selector is said to generalize across different compression ratios and even to video benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. High Practicality\nPerformance retention remains exceptionally high (the paper claims ≈95% or even close to 100%) when retaining only a small number of visual tokens (10–30%). This approach reduces memory usage and nearly halves prefill inference latency. It offers significant real-world deployment value for high-resolution images, multi-page OCR, and video understanding.\n\n2. Compact and pluggable\nThe backbone LLM requires no retraining whatsoever, training only a small scoring head with approximately 12.85 million parameters plus a Top-K module. This reduces training costs and facilitates direct deployment within existing multimodal large models."}, "weaknesses": {"value": "1. LIS is essentially an attention-based scoring head; differentiable Top-K approximates Top-K as a continuous mask and then uses implicit differentiation; curriculum gradually increases consistency loss. These ideas closely resemble existing sparse selection/differentiable ranking/STE-style pruning. The paper packages them as a new paradigm but fails to clearly articulate what the truly novel theoretical contributions are.\n\n2. Recommend comparing with methods that also require training to highlight the method's effectiveness.\n\n3. It is recommended to add comparisons of more models, such as the llava series and internvl series.\n\n4. Recommend adding a larger-scale model for comparison, such as the Qwen 2.5VL 14B."}, "questions": {"value": "See the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gpOMpnpS4g", "forum": "uRBKM7Ykeh", "replyto": "uRBKM7Ykeh", "signatures": ["ICLR.cc/2026/Conference/Submission8497/Reviewer_zK6q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8497/Reviewer_zK6q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808341414, "cdate": 1761808341414, "tmdate": 1762920369730, "mdate": 1762920369730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new framework for adaptive visual token compression in MLLMs. The authors identify a key inefficiency in current MLLMs that the excessive number of visual tokens from high-resolution or multi-image inputs, and argue that existing heuristic or attention-based compression methods either discard critical information or suffer from “attention sink” biases. \nTo address this, they propose VisionSelector, a lightweight, plug-and-play module that reformulates token compression as a fully learnable decision process. It comprises a Learnable Importance Scorer, a Differentiable Top-K selection mechanism, and a Curriculum Annealing Strategy that bridges the training–inference gap. Trained once at a fixed compression rate, VisionSelector generalizes to arbitrary retention budgets and integrates seamlessly with MLLMs such as Qwen2.5-VL. Extensive experiments across 13 image and video understanding benchmarks demonstrate that VisionSelector achieves superior accuracy retention and substantially outperforming prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed end-to-end learnable token compression mechanism represents a major step forward in solving the problem of visual token inefficiency in MLLMs. It offers a dynamic, adaptive solution as opposed to rigid, heuristic-based methods.\n2. The approach is highly practical. The module is lightweight with only 12.85M params, plug-and-play, and fast to train as it keeps the backbone frozen.\n3. The method is designed to integrate with existing MLLMs without requiring changes to the backbone model, providing a practical and deployable solution for current systems."}, "weaknesses": {"value": "1. While the paper motivates limitations of attention/similarity heuristics, it does not compare to strong learned pruning/gating baselines (e.g., Gumbel‑Softmax token gates, NeuralSort/SoftSort‑style Top‑K relaxations, recent dynamic‑context sparsification for MLLMs). The DiffTop‑K reference is a StackExchange post rather than established differentiable sorting literature. A stronger related‑work discussion and head‑to‑head comparisons to learnable alternatives would clarify novelty and technical significance.\n\n2. All experiments are conducted on the Qwen2.5-VL model (both 7B and 3B variants). Given the claim of “plug‑and‑play” generality, the evidence would be stronger with other popular MLLMs (e.g., InternVL, LLaVA, etc.), especially those without PatchMerger modules. Testing VisionSelector on at least one other major architectural family is necessary to fully validate this claim of general applicability.\n\n3. The ablation study in Table 3. reveals that the model's performance is highly sensitive to the training data mix. For instance, adding the COCO dataset significantly boosts performance on OCR-related tasks (OCRBench score from 701 to 763). This highlights a key trade-off that the paper glosses over: while this learnable method is more powerful, it is also data-dependent and requires careful data curation (144K samples) and training."}, "questions": {"value": "While VisionSelector performs well in training with soft selection, does the transition to hard selection during inference introduce any discrepancies, particularly in edge cases with low compression budgets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qo7l5pIlD5", "forum": "uRBKM7Ykeh", "replyto": "uRBKM7Ykeh", "signatures": ["ICLR.cc/2026/Conference/Submission8497/Reviewer_8uak"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8497/Reviewer_8uak"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762096339250, "cdate": 1762096339250, "tmdate": 1762920369228, "mdate": 1762920369228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}