{"id": "UX6JWRJudg", "number": 10294, "cdate": 1758166355656, "mdate": 1763016972089, "content": {"title": "Rethinking GCNs for the Traveling Salesman Problem: Are We Encoding Effectively?", "abstract": "Graph Convolutional Networks (GCNs) have demonstrated strong potential to address the Traveling Salesman Problem (TSP). However, existing GCN-based TSP solvers still struggle with limited generalization, overfitting, and extending to asymmetric TSPs. To address these challenges, we rethink how to enable models to learn unified and generalizable representations for TSPs. Specifically, we introduce three encoding strategies: global node embedding for **Input Unification**, Min-Max Scaling for universal **Edge Normalization**, and layer-wise expanding views for **Aggregation Enhancement**. These designs culminate in **UNE-GCN**, a model that learns generalizable TSP representations with strong robustness, favorable learning dynamics, and linear scalability. Extensive experiments show that UNE-GCN can guide LKH-3 to perform more efficient searches, and the two-stage framework of UNE-GCN + LKH-3 achieves superior solutions with less search time on both symmetric and asymmetric TSPs. To demonstrate the effectiveness of our encoding strategies, we apply experimental comparison and discuss different encoding schemes to unveil and validate the critical roles of **U-N-E** in the advancement of GCN-based TSP solvers. Experimental results demonstrate that UNE-GCN achieves state-of-the-art performance, with up to a **0.60%** improvement in Gap over plain LKH-3 on large-scale ATSPs and an **83%** reduction in error metric compared to the original GCN backbone on large-scale STSPs, providing insights for the design of more effective graph encoders.", "tldr": "", "keywords": ["Graph Neural Networks", "Traveling Salesman Problem", "Combinatorial Optimization", "Generalization", "Rethinking"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f732b0a0fd181aa13e9e08fdac4cedd29aa05b8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper re-evaluates Graph Convolutional Network (GCN) design for solving TSP, addressing limitations of poor generalization, overfitting, and inability to handle asymmetric TSP (ATSP) in existing solvers. It proposes UNE-GCN, a framework integrating three encoding strategies: Input Unification, Edge Normalization, and Aggregation Enhancement. UNE-GCN is tested in a two-stage pipeline with LKH-3, showing potential towards a unified GCN design for STSP/ATSP and insights into scalable TSP representation learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper focuses on the gap in prior GCN-based TSP solvers (lack of unified STSP/ATSP handling and poor cross-scale generalization), which is important for practical TSP solving by neural methods.\n2. The proposed three encoding strategies are described with clarity. And the layer-wise expanding views are relatively novel adaptations to TSP's structural properties."}, "weaknesses": {"value": "In general, I appreciate the research targets that have motivated the authors as well as the substantial works accomplished and reported thus far in their submission. However, there exist major concerns regarding narrative clarity, technical contribution, and the empirical evaluations.\n\n**1. The novelty and technical contribution are limited.**\n- The k-NN sparsification, learnable node embeddings, and min-max normalization for edge weights seem quite straightforward, already mentioned, or widely applied, which might appear as supplementary introduction or background setting in much of the literature rather than specific components as a primary contribution. The main contribution of this work may lie in proposing a new GCN backbone architecture tailored for TSP solving with a multi-view message-passing mechanism (Sec. 4.3-4.4). I suggest the authors emphasize this part with more explicit comparisons against previous GCNs. It is difficult to identify what's innovative about this work from the current version.\n- As a submission claiming to address flaws in neural architectures, \"*UNE-GCN can guide LKH-3 to perform more efficient searches, and the two-stage framework of UNE-GCN + LKH-3 achieves superior solutions with less search time on both symmetric and asymmetric TSPs*\" is not supposed to be sufficient as a summarized outcome (detailed in point 3 below).\n\n**2. The clarity and readability need to be improved.**\n- The paper organization is not quite reasonable. Substantial critical information is piled in the appendix, e.g, the comparison with neural baselines (Appendix C), distinctive discussions (Appendix E/F/G), etc., while explanations for some straightforward tricks take up much space in Sec. 4.\n\n**3. The experimental evaluations are limited.**\n- Most importantly, the main attention should be on demonstrating the advantage of the proposed enhanced GCN backbone against other neural encoders that produce similar heatmaps with subpar quality.\n    - So the authors need to provide results where solution tours are ***greedily constructed* under the guidance of the heatmaps predicted by baselines with different backbone models, without any post-inference improving techniques (e.g., LKH, MCTS, etc.)**\n    - Currently, the main empirical comparison centers on ablation variants and against LKH, which is somewhat too narrow and deviates from the main targets. I do not see the point involving LKH as a stage-2 partner, because LKH-3 itself is well-known to be powerful enough and shall disguise the (minor) quality divergence of the raw neural output (e.g., the heatmap) by different baseline networks.\n    - I.e., Table 6 and Table 7 should be placed in the main context with fairer comparisons. MCTS, 2-Opt, and LKH-3 have no comparability, and it does not make sense to show that UNE-GCN + LKH3 outperforms methods like DIFUSCO+MCTS or even pure neural approaches like the Transformer-based ones. **A possible and mainstream assessment might be as simple and clear as: 1) UNE-GCN + greedy > GCN + greedy; and 2) UNE-GCN + greedy > baseline (Att-GCN/DIFUSCO/T2T/...) + greedy.** Of course, you are still encouraged to keep comparisons like UNE-GCN + LKH > baselines + LKH, and so on.\n- The evaluation protocol and test datasets are not standard. \n    - Generalization is important, and so are the in-distribution parts. I wish to see ***clear and comparable*** results that the models are trained and tested on uniform (S)TSP instances that have been conventionally adopted by numerous works (e.g., Att-GCN, DIMES, DIFUSCO, Fast-T2T, etc. 1280 instances for TSP-50/100, 128 for TSP-500, etc.), with clear lengths, gaps, and solving time reported. \n    - Could you please provide results on ATSP-[20/50/100/200/500] instances that are generated in a conventional TMAT class (by MatNet, UniCO, GOAL, etc) to ensure the best ***comparability***? Note you may still keep the results on test sets that are perturbed from TSP ones as cross-distribution generalization.\n- Evaluating a GCN backbone only on TSP is somewhat limited. Consider testing on at least more similar graph-based combinatorial tasks is better, e.g., CVRP, MIS, etc."}, "questions": {"value": "For clarity and convenient interaction, please refer to Weaknesses for all concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TkoPXxYdIw", "forum": "UX6JWRJudg", "replyto": "UX6JWRJudg", "signatures": ["ICLR.cc/2026/Conference/Submission10294/Reviewer_N9Zi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10294/Reviewer_N9Zi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760722045509, "cdate": 1760722045509, "tmdate": 1762921647058, "mdate": 1762921647058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenges of generalization, overfitting, and difficulty in adapting GCN-based solvers to the TSP and ATSP. The authors propose the UNE-GCN model, which incorporates three strategies for improving performance: Input Unification (global node embedding), Edge Normalization (Min-Max scaling), and Aggregation Enhancement (layer-wise expanding views). Experimental results demonstrate that these strategies contribute to the generalization ability of the model, enhancing both the accuracy and efficiency of the GCN-based solver."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper includes thorough experimental analysis with extensive ablation studies, providing clear evidence for the effectiveness of each proposed strategy. \n2. The proposed methods are easy to follow."}, "weaknesses": {"value": "1. The individual techniques used—Min-Max scaling, edge aggregation, and pruning—are not entirely new and have been employed in previous works, which limits the overall novelty of the approach.\n2. The paper should demonstrate that the generalization improvement is independent of the LKH solver used, as the observed improvements may be a result of enhancing LKH rather than improving generalization per se.\n3. The mainly compared baseline model (Neurolkh) is from 2021 (although exists some baselines without combining LKH in Appendix). It would be beneficial to compare the UNE-GCN with more recent models based on LKH, such as VSR-LKH, DualOpt, and Select and Optimize, to strengthen the comparative analysis.\n4. While the paper focuses on TSP, it remains unclear whether the proposed approach can be generalized to other combinatorial optimization problems like CVRP or FJSP, which could broaden the impact of the proposed model.\n5. The adopted MR@5 metric cannot fully reflect the effectiveness of the model, because combinatorial optimization problems are not classification tasks."}, "questions": {"value": "Could the authors provide results without the use of LKH to better isolate the contribution of UNE-GCN itself, separate from the post-processing improvements contributed by LKH?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0kvixayv1y", "forum": "UX6JWRJudg", "replyto": "UX6JWRJudg", "signatures": ["ICLR.cc/2026/Conference/Submission10294/Reviewer_2vXk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10294/Reviewer_2vXk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760841981166, "cdate": 1760841981166, "tmdate": 1762921646696, "mdate": 1762921646696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper rethinks the use of Graph Convolutional Networks (GCNs) in solving the Traveling Salesman Problem (TSP). It introduces three encoder strategies: input unification, edge normalization, and aggregation enhancement, to improve representation learning. Extensive ablation studies are conducted to demonstrate the effectiveness of these strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed input unification strategy is well designed, enabling the model to handle asymmetric TSP (ATSP) instances effectively. The experiments on the three encoder strategies are comprehensive, and the model demonstrates strong generalization ability across different size instances."}, "weaknesses": {"value": "1. This paper claims to be a rethinking of GCNs for the TSP. However, the experiments do not demonstrate how this rethinking applies to other GCN-based models. Such experiments are necessary; otherwise, the work reads more like the proposal of a new model rather than a true rethinking. Moreover, the experiments are limited to applications within the LKH framework only.\n\n2. DIFUSCO and T2T are GNN-based models, not standard GCNs. The paper’s description of these two methods is therefore inaccurate and should be corrected.\n\n3. Regarding LKH, it is recommended to use version 3.0.6, since different LKH versions vary in efficiency and implementation details. NeuroLKH was trained based on version 3.0.6, under which its runtime is shorter than that of LKH. However, the paper reports that NeuroLKH is slower, which contradicts the original paper. Comparing under version 3.0.6 would be a fairer and more consistent setup. It would also be preferable to compare under the same iteration count rather than same runtime limits.\n\n4. For DIFUSCO, the original work used a 2-opt search as the default configuration, whereas this paper reports results under MCTS. To ensure consistency, it is recommended to evaluate DIFUSCO using the default 2-opt search, since the model was trained and tuned under that setting.\n\n5. Including comparison with baseline on the 200-node and 500-node datasets is more easier for a more comprehensive evaluation of model scalability and generalization.\n\n6. The paper states that the candidate set size for ATSP is set to 6, but the default in LKH should be 5. Unless there is a specific reason for this adjustment, it is suggested to follow the default setting.\n\n7. The main text contains a large number of tables without corresponding discussion, and readers are forced to consult the Appendix, which is not ideal. It is recommended to move the key descriptions from the Appendix into the main body, possibly moving some tables to the Appendix instead. Furthermore, comparisons with baseline models are particularly important and should be presented in the main text."}, "questions": {"value": "1. Whether this paper is truly a rethinking or simply a new model remains unclear. Although the title claims it to be a rethinking, the content reads more like the proposal of a new LKH-based model.\n\n2. The others are shown in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8oGPw1D782", "forum": "UX6JWRJudg", "replyto": "UX6JWRJudg", "signatures": ["ICLR.cc/2026/Conference/Submission10294/Reviewer_SU2T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10294/Reviewer_SU2T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625988361, "cdate": 1761625988361, "tmdate": 1762921646323, "mdate": 1762921646323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UNE-GCN, a novel approach to solving the Traveling Salesman Problem (TSP) using Graph Convolutional Networks (GCNs). The authors introduce three key encoding strategies—Input Unification, Edge Normalization, and Aggregation Enhancement (U-N-E)—to address the challenges of generalization, overfitting, and scalability in GCN-based solvers. The experimental results show that UNE-GCN outperforms traditional TSP solvers like LKH-3, particularly in handling large-scale instances (up to 100K nodes). The paper makes a significant contribution by improving the generalization ability of GCNs for combinatorial optimization problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The three encoding techniques—Input Unification, Edge Normalization, and Aggregation Enhancement—are well-justified and provide a unified approach to graph encoding that helps overcome the limitations of previous GCN-based solvers. This innovation is a key strength of the paper.\n\n2. The paper presents a thorough set of experiments comparing UNE-GCN with baseline methods, including LKH-3. The results demonstrate a clear advantage in performance, especially on large TSP instances. The ablation studies add depth to the findings and validate the effectiveness of each encoding strategy.\n\n3. UNE-GCN is shown to scale efficiently with larger problem sizes, with performance improvements observed even on instances with up to 100,000 nodes. This scalability is essential for real-world applications where TSP problems can grow quite large.\n\n4. The insights into encoding strategies and generalization in TSP solvers can have broader implications for future research in combinatorial optimization and graph-based neural networks. The framework is easy to extend to other similar problems."}, "weaknesses": {"value": "1.Although UNE-GCN achieves good performance, the added complexity of the model (with respect to parameters and training time) could be a barrier to its practical deployment, particularly for large-scale instances. A more thorough discussion on the computational efficiency and trade-offs involved would be helpful.\n2. While the paper compares UNE-GCN to traditional TSP solvers like LKH-3, it doesn't sufficiently compare it to other state-of-the-art neural network-based methods (e.g., GATs, graph attention networks, or reinforcement learning-based solvers). A more comprehensive comparison would help position UNE-GCN more clearly in the context of neural optimization approaches.\n3. The paper focuses primarily on benchmark datasets, but it could provide more insight into how the model performs in real-world TSP instances, which may have more irregular or noisy data. Further discussion on the generalizability of the model to diverse TSP variants would strengthen the paper."}, "questions": {"value": "1.How does UNE-GCN compare to simpler models in terms of memory usage and inference time for large-scale TSP instances? Could the model be optimized for better computational efficiency?\n\n2.the authors provide a more in-depth comparison with other neural approaches, particularly reinforcement learning-based solvers or GCN variants? How does UNE-GCN's performance compare in terms of optimization stability and convergence speed?\n\n3.The paper provides experimental evidence of the approach's success, but it would be valuable to include some theoretical discussion or guarantees regarding the improvements brought by the proposed encoding strategies. Could the authors provide a theoretical analysis to justify the benefits of the three encoding techniques?\n\n4. While the paper shows great results on benchmark datasets, how does UNE-GCN generalize to more complex real-world TSP instances (e.g., with noisy or incomplete data)? Can the authors show the robustness of the model in more varied settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "X2VtZKidDb", "forum": "UX6JWRJudg", "replyto": "UX6JWRJudg", "signatures": ["ICLR.cc/2026/Conference/Submission10294/Reviewer_2Lz4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10294/Reviewer_2Lz4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838705783, "cdate": 1761838705783, "tmdate": 1762921645676, "mdate": 1762921645676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **UNE-GCN**, a set of three encoding strategies (Input Unification — global node embeddings; Edge Normalization — min–max scaling; Aggregation Enhancement — multi-scale / layered neighborhood aggregation) to improve GCN-based tour-generation for TSP variants. The authors evaluate the approach on a broad suite of experiments (cross-scale synthetic STSP/ATSP, TSPLIB instances) and combine the learned components with LKH-3 in a two-stage pipeline. Results include ablations, scaling-law observations, and comparisons showing time-quality tradeoffs and gains over several neural baselines and, in some settings, over plain LKH-3."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear motivation: focuses on how input/edge/aggregation encoding choices affect cross-scale generalization and ATSP/STSP differences.\n* Simple, implementable modules (global node embedding, row-wise min–max scaling, layered aggregation) with individual contributions validated via ablation.\n* Broad experimental coverage: large-scale synthetic instances, TSPLIB comparisons, and combination with LKH-3 showing practical time/quality tradeoffs."}, "weaknesses": {"value": "* **Limited theoretical justification.** The paper lacks deeper analysis explaining why a global node embedding does not discard essential geometric information (especially for coordinate-based STSP).\n* **Generalization to real-world distributions under-explored.** Training and evaluation are heavily based on synthetic/mixed-scale samples; coverage of realistic instance distributions is limited.\n* **Instability on some TSPLIB cases.** Appendix tables show instances where performance/time gaps are worse — these failure modes are not analyzed in depth.\n* **Reproducibility details missing.** Essential items to reproduce experiments are incomplete (data-generation scripts, exact LKH-3 and JV parameter settings, sampling strategy, random seeds, hardware/parallel setup)."}, "questions": {"value": "* Please provide a deeper, quantitative justification for Input Unification (global node embeddings). How and why does replacing coordinate-based embeddings with a global node embedding retain sufficient geometric/topological information? Can you show per-instance comparisons (coordinate embedding vs. global embedding) on a held-out STSP validation set?\n* How robust is min–max edge normalization to outliers or extreme edge-length distributions? Have you tried robust alternatives (e.g., quantile-based scaling, clipping) and how do they compare?\n* Clarify training details: sampling strategy for the stated 1M training instances (are all seen each epoch?), full training curves (loss, validation metrics) and total training time. This helps assess whether models were trained sufficiently.\n* For TSPLIB instances where UNE-GCN performs worse or inconsistently, can you provide qualitative/quantitative analysis (instance topology, degree distributions, coordinate patterns) to explain failure modes?\n* For ATSP + two-stage LKH-3 pipeline: provide precise details of the Johnson-Ven der (JV) transformation implementation and examine whether JV or any preprocessing changes candidate structure in a way that biases comparisons."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HSYRsSSz2r", "forum": "UX6JWRJudg", "replyto": "UX6JWRJudg", "signatures": ["ICLR.cc/2026/Conference/Submission10294/Reviewer_eBvv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10294/Reviewer_eBvv"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906186154, "cdate": 1761906186154, "tmdate": 1762921645241, "mdate": 1762921645241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}