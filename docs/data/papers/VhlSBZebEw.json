{"id": "VhlSBZebEw", "number": 16099, "cdate": 1758259981618, "mdate": 1763740507939, "content": {"title": "A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models", "abstract": "Test-time prompt tuning (TPT) has emerged as a promising technique for adapting large vision-language models (VLMs) to unseen tasks without relying on labeled data. However, the lack of dispersion between textual features can hurt calibration performance, which raises concerns about VLMs' reliability, trustworthiness, and safety. Current TPT approaches primarily focus on improving prompt calibration by either maximizing average textual feature dispersion or enforcing orthogonality constraints to encourage angular separation. However, these methods may not always have optimal angular separation between class-wise textual features, which implies overlooking the critical role of angular diversity. To address this, we propose A-TPT, a novel TPT framework that introduces angular diversity to encourage uniformity in the distribution of normalized textual features induced by corresponding learnable prompts. This uniformity is achieved by maximizing the minimum pairwise angular distance between features on the unit hypersphere. We show that our approach consistently surpasses state-of-the-art TPT methods in reducing the aggregate average calibration error while maintaining comparable accuracy through extensive experiments with various backbones on different datasets. Notably, our approach exhibits superior zero-shot calibration performance on natural distribution shifts and generalizes well to medical datasets. We provide extensive analyses, including theoretical aspects, to establish the grounding of A-TPT. These results highlight the potency of promoting angular diversity to achieve well-dispersed textual features, significantly improving VLM calibration during test-time adaptation. Our code will be made publicly available.", "tldr": "We propose A-TPT, a novel angular diversity method that improves calibration for test-time prompt tuning of vision-language models by maximizing minimum pairwise angular distance among textual features.", "keywords": ["Model Calibration", "Angular Diversity", "Uniformity", "Vision-Language Models", "CLIP", "Prompt Tuning", "Test-Time Adaptation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e975f3be99006a4b225be798d40e86a8c85c168f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "normalized textual features on the unit hypersphere, ensuring uniform distribution an A-TPT enhances test-time prompt tuning (TPT) by introducing angular diversity calibration. Instead of maximizing dispersion or orthogonality alone, it maximizes the minimum pairwise angular distance between d better"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tElegant mathematical framing grounded in Tammes best-packing problem.\n2.\tClearly articulated motivation: poor calibration under low angular diversity.\n3.\tDemonstrates lower Expected Calibration Error (ECE) with negligible accuracy loss.\n4.\tTheoretically principled and easy to integrate."}, "weaknesses": {"value": "1.\tIncremental relative to O-TPT and C-TPT; lacks major conceptual leap.\n2.\tNo runtime or convergence analysis for numerical optimization.\n3.\tExperiments mostly on classification; unclear utility for generative tasks."}, "questions": {"value": "1.\tHow sensitive is A-TPT to initialization of textual prompts?\n2.\tCan angular diversity loss degrade accuracy for semantically overlapping classes?\n3.\tIs there any theoretical bound on ECE improvement from uniform angular spacing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1EPkPzOZEq", "forum": "VhlSBZebEw", "replyto": "VhlSBZebEw", "signatures": ["ICLR.cc/2026/Conference/Submission16099/Reviewer_5gVD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16099/Reviewer_5gVD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761401679028, "cdate": 1761401679028, "tmdate": 1762926277941, "mdate": 1762926277941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a new criterion for test-time prompt tuning in order to reduce calibration error.  It has previously been noted that calibration error can be reduced by designing a prompt template so that, when the different class labels are inserted into the template, the resulting prompts have maximally dispersed text vectors.  C-TPT measured dispersion using mean L2 distance, which does not guarantee pairwise separation; O-TPT guaranteed pairwise dispersion of 90 degrees if the number of classes is less than twice the number of embedding dimensions, but not otherwise.  The proposed A-TPT minimizes the maximum pairwise cosine similarity of classes, thus maximizing pairwise dispersion."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Derivations are interesting and clear. \n\nEquations and derivations seem correct. The point about \\arccos normalizing gradient magnitudes is quite interesting.  I find multi-letter variable names aesthetically displeasing in general, but the use of \"Cos\" as a variable name does not impair legibility or correctness in this case.\n\nResults show significant consistent reduction in calibration error, with small and inconsistent changes in accuracy, across 15 datasets, in comparison to TPT, C-TPT, and O-TPT."}, "weaknesses": {"value": "Minor: Fig. 3 clearly shows that the prompts with the highest ECE (\"the nearest shape in this image is\" and TPT) are clustered in the center, while other prompts are distributed.  This does not show, however, that the prompts with high ECE have low angular diversity, because t-SNE does not show the angles of vectors: it only shows their cluster structure."}, "questions": {"value": "On p. 4, what does it mean when the same prompt appears in both the list \"Hard prompts\" and the list \"Tuned prompts,\" but with different Accuracy, ECE and AD?\n\nEq. (1) min_{j,j\\ne i} should be min_{i,j\\ne i}"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wk9k2Rhkn7", "forum": "VhlSBZebEw", "replyto": "VhlSBZebEw", "signatures": ["ICLR.cc/2026/Conference/Submission16099/Reviewer_RJNf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16099/Reviewer_RJNf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863711540, "cdate": 1761863711540, "tmdate": 1762926277482, "mdate": 1762926277482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the calibration problems that arise when doing test-time prompt tuning (TPT) on vision-language models and argues that existing fixes (like text feature dispersion (C-TPT) and orthogonality constraints (O-TPT)) don’t actually guarantee that class-wise text features are well separated, especially when the number of classes exceeds the embedding dimension. To address this, the authors propose A-TPT (Angular Test-time Prompt Tuning), which adds an angular diversity regularizer that, for each class embedding, maximizes its minimum angular distance to any other class, encouraging a more uniform packing of text features on the unit hypersphere. According to experiments on fine-grained, distribution-shifted the proposed method reduces expected calibration error (ECE) while largely preserving TPT’s accuracy gains over zero-shot CLIP and prior TPT variants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper clearly identifies the shortcomings of prior text-feature dispersion approaches and uses Figure 2 to illustrate them effectively.\n\n* The reported ECE gains over baselines such as C-TPT and O-TPT are also encouraging.\n\n* The authors show the method also works not only on standard benchmarks used to evaluate CLIP performance, but also on 'calibration critical applications' such as medical domain in Table 4."}, "weaknesses": {"value": "* Although the paper proposes angular diversity regularization as a new metric, the method still operates within the existing C-TPT and O-TPT test-time adaptation paradigm, so the contribution feels more incremental than fundamentally novel in terms of theory or technique\n\n* Could the proposed method be a complementary to previous methods (e.g., C-TPT or O-TPT). That is, could we for example enforce the proposed angular diversity on top of textual dispersion proposed by C-TPT or the orthogonality constraints of O-TPT. It would be interesting to see such an ablation.\n\n* Since the ECE metric could suffer from bias, could the authors report calibration metrics other than ECE as well?"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4SlUXtLP7k", "forum": "VhlSBZebEw", "replyto": "VhlSBZebEw", "signatures": ["ICLR.cc/2026/Conference/Submission16099/Reviewer_r5c7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16099/Reviewer_r5c7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981110138, "cdate": 1761981110138, "tmdate": 1762926276910, "mdate": 1762926276910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Angular Diversity for Test-time Prompt Tuning (A-TPT), instead of pushing text prompts to be dispersed by l2 distance or cosine similarity, it maximizes the minimum pairwise angle between normalized class-wise prompt vectors (a maximin objective using θ=arccos of cosine similarity). This directly spreads prompts on the unit hypersphere to promote more uniform coverage and better calibration during inference. The authors also argue why this angle-based objective has stable gradients even when vectors are very close—unlike the orthogonality loss whose gradient vanishes as angles go to 0."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Rather than optimizing L2 distance or cosine similarity, the paper optimizes the **angle itself,** which better captures geometric separation on the unit sphere and compensates for the shortcomings of previous work. This paper shows the limitation of previous work well.\n- The paper includes extensive analyses that illuminate the method’s behavior from multiple perspectives, aiding interpretation and practical use.\n- It explicitly examines the calibration differences between N > |D| and N ≤ |D|, a case prior work largely overlooks, and clarifies where the proposed method offers the biggest gains over O-TPT."}, "weaknesses": {"value": "- When we increase λ, we understand this as trading some accuracy for improved ECE (better calibration). This trend aligns with Flowers102, but Food101 shows a contrasting pattern. Could you provide insight into why the two datasets behave differently? Also, are these curves averaged over multiple seeds, and how large is the variance across runs?\n- How did you choose the λ term?\n- In the main performance table, could you report results separately or make them explicitly distinguishabl for the N>|D| and N≤|D| regimes? This would help isolate where your method provides the most benefit over O-TPT.\n- How do you ensure numerical stability when computing arccos?\n- While increasing the minimun θ can encourage dispersion, it doesn’t seem sufficient to prevent localized density (clustering) in certain regions. Do you have any guarantees or empirical evidence that your method avoids such partial clustering?"}, "questions": {"value": "See weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cSvBPbM3QT", "forum": "VhlSBZebEw", "replyto": "VhlSBZebEw", "signatures": ["ICLR.cc/2026/Conference/Submission16099/Reviewer_Aoux"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16099/Reviewer_Aoux"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983591747, "cdate": 1761983591747, "tmdate": 1762926276616, "mdate": 1762926276616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "Thank you for providing constructive insights on our paper and for taking the time to review the paper. We have revised the manuscript as outlined below to incorporate reviewers' comments, with the revised manuscript written in blue.\n\n- We have added distinguished A-TPT results for fine-grained, semantically overlapping datasets in Appendix A.21.\n- We have highlighted relevant rebuttal sections, tables, and figures in blue. \n\nAdditional changes will be incorporated in a subsequent revision."}}, "id": "juBfPd8hhd", "forum": "VhlSBZebEw", "replyto": "VhlSBZebEw", "signatures": ["ICLR.cc/2026/Conference/Submission16099/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16099/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission16099/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763740797899, "cdate": 1763740797899, "tmdate": 1763743295925, "mdate": 1763743295925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}