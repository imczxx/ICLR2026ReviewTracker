{"id": "HHsD970kdE", "number": 8192, "cdate": 1758073135302, "mdate": 1759897800859, "content": {"title": "DREAMSTATE: Diffusing States and Parameters for Recurrent Large Language Models", "abstract": "Modern Recurrent Neural Networks (RNNs), such as RWKV, are distinguished by their powerful short-range modeling capabilities and efficient fixed-size states, which constitute a core advantage over standard Transformers. However, there is a significant lack of research into their internal state as an editable knowledge representation. To fill this gap, we first explore the representational properties of the RWKV state by proposing the DREAMSTATE framework. This framework utilizes a conditional Diffusion Transformer (DiT) to directly model the probability manifold of the state, enabling its generation and editing. The structural nature of this representation is validated through t-SNE visualizations and controlled generation experiments. After successfully uncovering and modeling the state's representational potential, we further propose a novel hybrid architecture that combines the local advantages of RNNs with global context adaptability. This architecture features a parallel DiT that processes a variable-length global context to dynamically generate and adjust the core recurrent module's WKV parameters, transforming the fixed recurrence mechanism into a context-aware dynamic function. Experiments demonstrate that this hybrid model can be trained stably via a multi-objective loss, validating its design feasibility. Our work not only opens a new research direction for RNN state representation but also provides a concrete architectural reference for future model design. The code is publicly available at: \\url{https://huggingface.co/2dgx41s/DreamState}.", "tldr": "", "keywords": ["State Representation", "Recurrent Neural Networks (RNNs)", "Diffusion Models", "Hybrid Architecture", "RWKV", "Generative Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/599174631467a11ec2eb9e42646b8e183015a9e4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses a problem known as attention noise, which affects transformers under context shifts. The idea is to learn a context-aware diffusion model that adapts the parameters controlling the RWKV states. The model is trained using a multi-objective loss function combining next-token prediction and diffusion losses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Modelling internal representation is an interesting and novel idea.\n- The application to the domain shift is relevant."}, "weaknesses": {"value": "Weaknesses:\n- The authors could have commented more on related work. For example, they may have mentioned other state-space architectures with input-dependent parameters and clarified the technical differences between their method and differential attention.\n- Training and inference complexity are not addressed. The authors should comment on the possible disadvantages of letting the weights be generated dynamically.\n- Attention noise due to context shifts seems to be a general problem of learning when training and testing distributions differ. What is special about RNN compared to standard transformers?\n- The proof that states can be learned only shows that the state is a vector. Is this enough? What could make the state non-learnable?\n- It is unclear if the authors propose to generate the state using a diffusion model or the linear projection matrices."}, "questions": {"value": "Questions:\n- Is the $[0, 1]$ constraint on the eigenvalue required for stability? What is the intuition behind relaxing it to $[-1, 1]$? How is the stability of the generated states guaranteed in the proposed approach?\n- What is $a_t$ in Eq.1? Is $\\tilde k$ the same as $k$? \n- Would it be possible to replace $P(\\theta|c)$ by a context-dependent deterministic function?\n- Where does $\\theta_{satic}$ come from? Why is enforcing the interpolation needed? What is the value of $\\alpha$ used/observed in the experiments?\n- How do you obtain the samples used to train the diffusion model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9XDtFCXd73", "forum": "HHsD970kdE", "replyto": "HHsD970kdE", "signatures": ["ICLR.cc/2026/Conference/Submission8192/Reviewer_YA6W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8192/Reviewer_YA6W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760962409639, "cdate": 1760962409639, "tmdate": 1762920148736, "mdate": 1762920148736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes a method for generating states and parameters for RWKV models, similar to hypernetworks. The paper frames static weights as a form of structural noise. It provides a diffusion objective for learning to generate states and a multi-objective loss for learning to diffuse parameters and predict tokens simultaneously. A qualitative analysis of the state manifold motivates the generation of states, and an experiment showing that it is possible to reduce the multi-objective loss is provided."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work poses interesting questions regarding the structural noise of fixed parameters. The proposed solution seems feasible and some efforts were made to give evidence for its potential."}, "weaknesses": {"value": "The work lacks empirical evaluation of the performance of the method. Language modeling evaluations are needed. The theoretical contributions are limited to architecture design, but the design is not validated with empirical results. The quantitative evidence for the proposed approach is weak, amounting to a few examples of incoherent text and a low-dimensional visualization of state clusters from similar prompts. No empirical measure of similarity is used to validate the clustering."}, "questions": {"value": "on L136 the flattened state matrix is described as an element of R^HxDxD, which seems like a 3 dimensional tensor. Is this intended to be vector?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ew8HZzKx1Y", "forum": "HHsD970kdE", "replyto": "HHsD970kdE", "signatures": ["ICLR.cc/2026/Conference/Submission8192/Reviewer_KFdZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8192/Reviewer_KFdZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931694295, "cdate": 1761931694295, "tmdate": 1762920148347, "mdate": 1762920148347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to use a diffusion network to model the WKV parameters of RWKV model, the purpose is to explore the representational properties of the RWKV states, and another advantage is that this can combine the local advantage of RNNs with global context adaptability. The experiments show that this hybrid model can be trained via a multi-objective loss.This is a quite unique idea, and can provide a new architectural reference for future model designs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Author provided steps to replicate the results. This is quite awesome.\n2) Using a DiT model to model the RNN parameters unique and interesting. And author showed the model can be stably trained."}, "weaknesses": {"value": "1) The experiments are not complete, with a new model, we need to compare with the standard RWKV model at least. A model can be trained stably is not enough, we also need to show that its performance is at least comparable with other models.\n\n2) Because the new hybrid model can model long context directly, it would be great if it can add benchmarks to show case its strength."}, "questions": {"value": "To control the behaviour of LLM, one standard approach is to add system prompt, or maybe adding some instructions before the user's prompt, the proposed approach seems to be overly complicated?  What's the benefit of your new model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9nLCuElI4a", "forum": "HHsD970kdE", "replyto": "HHsD970kdE", "signatures": ["ICLR.cc/2026/Conference/Submission8192/Reviewer_xPwF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8192/Reviewer_xPwF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061953432, "cdate": 1762061953432, "tmdate": 1762920147908, "mdate": 1762920147908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DREAMSTATE, a framework that applies diffusion models to RWKV recurrent networks in two ways: (1) generating hidden states for controlled inference, and (2) dynamically synthesizing WKV parameters conditioned on global context. The authors validate state manifold structure through t-SNE visualization and demonstrate multi-objective training stability for the hybrid architecture."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Novel application of diffusion to RNN states. The paper introduces diffusion models for generating RWKV hidden states (Section 3.1), which differs from prior work on variational state modeling (VRNN, Chung et al. 2015) or weight generation (Neural Network Diffusion, Wang et al. 2024). The conditional generation framework in Equation 3 enables direct manipulation of the state manifold, validated through clustering in Figure 2 where different personas (code, creative writing, planning) form distinct regions."}, "weaknesses": {"value": "Experimental validation lacks quantitative results and proper baselines. Section 4.3 claims \"training stability\" based solely on Figure 3 showing decreasing loss curves, but provides no numerical results for language modeling performance. Table 1 presents only qualitative text samples without perplexity measurements or accuracy on state-tracking tasks (parity, modular arithmetic) mentioned in Section 1. The paper cites Grazzi et al. (2024) regarding RWKV-7's ability to solve parity problems, but does not demonstrate whether DREAMSTATE maintains or improves this capability. The 0.1B model scale (Section 4.1) is appropriate for proof-of-concept, but the absence of comparisons with RWKV-7 baseline, standard Transformers, or VRNN makes it impossible to assess whether the added complexity provides measurable benefits."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "65iVvsZ2kG", "forum": "HHsD970kdE", "replyto": "HHsD970kdE", "signatures": ["ICLR.cc/2026/Conference/Submission8192/Reviewer_9Qyq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8192/Reviewer_9Qyq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156268986, "cdate": 1762156268986, "tmdate": 1762920147448, "mdate": 1762920147448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}