{"id": "movNwgQtDt", "number": 1554, "cdate": 1756891394840, "mdate": 1763695183957, "content": {"title": "GEPO: Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning", "abstract": "As single-center computing approaches power constraints, decentralized training becomes essential. However, traditional Reinforcement Learning (RL) methods, crucial for enhancing large model post-training, cannot adapt to decentralized distributed training due to the tight coupling between parameter learning and rollout sampling. For this, we propose HeteroRL, a heterogeneous RL architecture that decouples these processes, enabling stable training across geographically distributed nodes connected via the Internet. The core component is Group Expectation Policy Optimization (GEPO), an asynchronous RL algorithm robust to latency caused by network delays or heterogeneity in computational resources. Our study reveals that high latency significantly increases KL divergence, leading to higher variance of importance weights and training instability. GEPO mitigates this issue by using group expectation weighting to exponentially reduce the variance of importance weights, with theoretical guarantees.  Experiments show GEPO achieves superior stability—only a 3\\% performance drop from online to 1800s latency—and reduces the best-to-last gap by 85\\% versus GSPO ($\\Delta$=1.8 vs. 12.0) while attaining the highest scores, highlighting its effectiveness in decentralized, resource-heterogeneous environments.", "tldr": "", "keywords": ["Mathematical Reasoning", "Reinforcement Learning", "Large Language Models", "Decentralized Training", "Heterogeneous Computing"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa62cde0fc492fff64ab5d6c960dda2075e2985c.pdf", "supplementary_material": "/attachment/bd5c24d0370975d4b680f11fd7c10b3ece8628f5.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes HeteroRL, an asynchronous framework for RLHF that decouples rollout sampling from learning to tolerate WAN latency and heterogeneous hardware. Its key algorithm, GEPO, replaces token/sequence importance ratios with a group expectation denominator, theoretically reducing IS variance—especially at high sampler–learner KL—and stabilizing training under policy staleness. Stronger last-epoch performance and robustness vs. GRPO/GSPO are shown on math-reasoning benchmarks under simulated delays."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: It makes sense by clear shift from token/sequence to group-level weighting tied to the real failure mode.\n- Theory: It gives exponential variance reduction of importance weights in high-KL regimes; mechanistic gradient comparison clarifies why updates stabilize. \n- Empirics: GEPO maintains accuracy and avoids collapse across delays; best/last markedly stronger than GRPO/GSPO."}, "weaknesses": {"value": "- Bias–variance trade-off not quantified. The authors should provide MSE curves vs. KL and group size, and analyze the green-region cases where variance may increase.\n  \n- Bound constants/tightness unclear.The theorem invokes constants (e.g., \\(C\\)) but does not estimate or validate their tightness on real training distributions; what about reporting empirical lower/upper bounds and how they vary with vocabulary/sequence length.\n  \n- Systems throughput and realism underreported.The method targets WAN latency, but the authoers do not provide wall-clock throughput (samples/s), utilization, or sensitivity to jitter/packet loss; it is better to add end-to-end performance, synchronization frequency, and utilization metrics.\n\n- Missing off-policy/staleness baselines. I am curious about the comparison omits V-trace/IMPALA-like corrections and AREAL-style staleness controls, which are only mentioned in related works; the authors should add these baselines to separate gains from grouping vs. specific normalization.\n  \n- Sensitivity analyses incomplete.The stability claim depends on grouping and sampling, but the paper does not sweep group size \\(G\\), temperature/top-p, or KL-regularization × delay."}, "questions": {"value": "As shown in weakness.\n\nBtw, why placing the related work section in the final, which is not usual. And the related works seem limited and unorganized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hTLazB3ujV", "forum": "movNwgQtDt", "replyto": "movNwgQtDt", "signatures": ["ICLR.cc/2026/Conference/Submission1554/Reviewer_5yWm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1554/Reviewer_5yWm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889283463, "cdate": 1761889283463, "tmdate": 1762915811722, "mdate": 1762915811722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents HeteroRL, a decentralized reinforcement learning framework designed for training LLMs across distributed and resource-heterogeneous environments. The paper introduces GEPO, including a new group expectation importance weighting mechanism to mitigate the instability caused by policy staleness. Theoretical results show that GEPO exponentially reduces the variance of importance weights under large KL, thereby stabilizing training in asynchronous and high-latency settings. Extensive experiments demonstrate the method's superior performance and stability on reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of reducing the variance of importance weights through group expectation weighting is conceptually elegant and practically powerful. It addresses the instability issue caused by large KL divergence in asynchronous or heterogeneous RL settings.\n1. The motivation, instability and variance explosion in asynchronous or heterogeneous RL due to policy staleness, is well validated by experimental results. The results across both online and heterogeneous RL settings consistently demonstrate that GEPO achieves lower variance, smoother gradients, and higher stability compared to GRPO and GSPO."}, "weaknesses": {"value": "1. The experimental validation appears limited in scope. GEPO is only evaluated on mathematical reasoning datasets (MATH, AIME, AMC) and with relatively small models (up to 8B parameters).\n1. There seems an inconsistency between the formulation and the theoretical analysis. In Section 3.1, the paper explicitly states that \"the vector $(q(y_1|x), …, q(y_G|x))$ does not constitute a valid probability distribution\" since top-K/top-P sampling leads to $\\sum_i q(y_i|x) \\gg 1$. However, in Theorem 1 and its proof, the derivation assumes that $q$ is a normalized discrete probability distribution satisfying $\\sum_i q(y_i|x) = 1$. How the theoretical guarantee holds when $q$ is not a valid probability distribution in practice?"}, "questions": {"value": "1. Could the authors clarify whether the ablation experiments comparing GEPO, GRPO, and GSPO are conducted under the same experimental settings as the main comparison in Section 4? If the setups are identical, how do these ablation results differ conceptually from the earlier baseline comparison?\n2. Why does GEPO remove the normalization when calculating the advantage? In practice, would including this normalization affect the performance or training stability? \n3. It is unclear how the case study in section G is directly connected to the theoretical claim about variance reduction in importance weights."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "c3WdOj4hZP", "forum": "movNwgQtDt", "replyto": "movNwgQtDt", "signatures": ["ICLR.cc/2026/Conference/Submission1554/Reviewer_bBKw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1554/Reviewer_bBKw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910853421, "cdate": 1761910853421, "tmdate": 1762915810983, "mdate": 1762915810983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GEPO, a reinforcement learning algorithm designed for heterogeneous, high-latency distributed training of large language models. It builds on a new framework that decouples sampling and learning across asynchronous nodes. To address instability caused by stale policies, GEPO replaces standard importance weights with group-level expectation weights, theoretically reducing variance when policy divergence is high. Experiments on mathematical reasoning tasks with Qwen models show that GEPO achieves higher accuracy and significantly improved training stability compared to GRPO and GSPO under simulated latency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The visualizations illustrate the main claims of the paper.\n2. The paper targeted on an important bottleneck problem in large-scale distributed reinforcement learning."}, "weaknesses": {"value": "1. The paper lacks experimental comparisons with other asynchronous policy optimization methods [1, 2, 3, 4].\n2. Equation (1) appears very similar to PPO, except that the clipping function is removed.\n3. In line 139, the variable $G$ is undefined; in Equation (1), it is unclear how $A(x)$ is computed, and in line 146, the input of $p$ is not specified.\n4. While Theorem 1 demonstrates a reduction in the variance of the importance sampling coefficient, this result does not guarantee a corresponding reduction in the variance of the weighted term $A \\cdot w$, because $A$ itself is a random variable. In general, even if one random variable has smaller variance than another (e.g., $\\mathrm{Var}(Y)<\\mathrm{Var}(Z)$), it does **not** necessarily follow that $\\mathrm{Var}(XY)<\\mathrm{Var}(XZ)$, unless $X$ is independent of both $Y$ and $Z$ and their expectations are zero.\n5. The claim in lines 178–181 seems questionable. In extreme cases, clipping can suppress unstable tokens by assigning them zero weight, which may be beneficial since such extreme tokens usually correspond to unreliable generations. Thus, clipping might actually serve as a useful safeguard in these scenarios.\n\n [1] Rastogi, Abhinav, et al. \"Magistral.\" *arXiv preprint arXiv:2506.10910* (2025).\n\n [2] Fu, Wei, et al. \"AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning.\" *arXiv preprint arXiv:2505.24298* (2025).\n\n [3] Zhong, Yinmin, et al. \"StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation.\" *arXiv preprint arXiv:2504.15930* (2025).\n\n [4] Han, Zhenyu, et al. \"AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training.\" *arXiv preprint arXiv:2507.01663* (2025)."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0H5McZmBsW", "forum": "movNwgQtDt", "replyto": "movNwgQtDt", "signatures": ["ICLR.cc/2026/Conference/Submission1554/Reviewer_aJa6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1554/Reviewer_aJa6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912243156, "cdate": 1761912243156, "tmdate": 1762915810821, "mdate": 1762915810821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HeteroRL, a decentralized RL framework that decouples rollout sampling from learning to tolerate Internet-scale latency and heterogeneous hardware. Its core algorithm, GEPO (Group Expectation Policy Optimization), replaces token/sequence-level importance weights with a group-level expectation to reduce the variance explosion that arises when the learner’s policy drifts from the samplers due to staleness. A theoretical result shows variance reduction grows (approximately) exponentially with KL divergence between policies, and experiments on Qwen3-1.7B/8B with math-reasoning benchmarks (AIME/MATH/AMC) report higher accuracy and markedly improved stability under simulated delays (up to 1800s)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Clear systems problem + algorithmic handle**. The paper crisply diagnoses policy staleness in decentralized RL and ties it to KL growth → variance blow-ups; GEPO’s denominator uses a within-group expectation to damp variance in precisely that regime.\n\n**Compelling empirical stability**. Under Hetero RL with max delay 64, GEPO improves best accuracy vs. GRPO/GSPO and, crucially, reduces best-to-last degradation by ~85% vs. GSPO (Δ=1.8 vs. 12.0). Curves show lower IW variance and smoother gradients.\n\n**Theoretical insight**. Theorem 1 relates the variance gap (standard IS vs. group-expectation weight) to exp(D_KL), motivating why the method helps exactly when staleness is worst.\n\n**Realistic setting**. The system model explicitly injects stochastic network delay and hetero nodes (Ascend + NVIDIA), aligning with decentralized community compute setups."}, "weaknesses": {"value": "**Bias–variance trade-off left under-quantified in RL objective**. GEPO’s estimator is acknowledged as biased; while lower variance can help optimization, the paper does not quantify end-to-end bias in policy gradients or returns beyond variance plots. A small-bias claim would benefit from controlled ablations where true on-policy gradients are approximated (short-horizon toy MDPs) to measure bias vs. sample efficiency. (GEIW is described as biased but stable.)\n\n**External validity beyond math-reasoning**. Results are limited to math QA on Qwen. It would help to show GEPO under non-text or mixed-modality tasks, or code/data generation tasks where rollouts have different length/entropy profiles. Current related work cites broader systems, but experiments remain narrow.\n\n**Ablations & knobs**. The method depends on group size G and sampling strategy (top-k/p). Sensitivity plots for G, the truncation strategy, and the CPPO-KL coefficient under delay would strengthen the story. (Implementation notes mention CPPO-KL and latency simulation, but tuning studies are light.)\n\n**Fairness to GSPO variants**. Since GSPO is sequence-level IS with clipping, it would be good to include GSPO + stronger clipping / trust-region or defensive mixture baselines to test whether variance spikes can be similarly tamed by tuned GSPO, not just GEPO’s denominator. The paper itself sketches “defensive sampling” as future work—great idea, but makes me wonder how close-tuned GSPO would get."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9TDxNdCPAr", "forum": "movNwgQtDt", "replyto": "movNwgQtDt", "signatures": ["ICLR.cc/2026/Conference/Submission1554/Reviewer_8QcJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1554/Reviewer_8QcJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925597185, "cdate": 1761925597185, "tmdate": 1762915810394, "mdate": 1762915810394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful feedback and valuable suggestions.\n\nReviewers highlighted GEPO's strengths as: \"**Clear systems problem + algorithmic handle**\" (8QcJ), having a \"**conceptually elegant and practically powerful**\" approach to variance reduction (bBKw), and offering clear \"**originality**,\" strong \"**theory**,\" and convincing \"**empirics**\" (5yWm). \n\nFor the weakness, we have incorporated the following key improvements:\n\n### **Update Manuscript**\n- (1) New Appendix Section **J.1**: Add comprehensive bias analysis to quantify the bias $|\\mathbb{E}_p [A] - \\mathbb{E}_q [\\frac{p}{\\mathbb{E}_q q} A]|$ of GEPO's estimator (Response to Weakness-1 of Reviewer 8QcJ)\n- (2) New Appendix Section **J.2**: Extend variance analysis from $Var(\\frac{p}{\\mathbb{E}_qq})$ to the more general $Var(\\frac{p}{\\mathbb{E}_qq} A)$ form for rigorous stability guarantees (Resonse to Weakness-4 of Reviewer aJa6)\n- (3) New Appendix Section **K**: Add comparisons with asynchronous RL baselines (**Truncated IS** technique of **IMPALA/V-trace** [Espeholt et al., 2018], **CISPO** [Chen et al., 2025] and **TOPR** [Roux et al., 2025])  to better contextualize GEPO's advantages\n- (4) New Appendix Section **L**: Conduct thorough hyperparameter studies examining group size, delay distributions, KL-regularization, and top-P/K/temperature sampling strategies\n\n###  **Update Code** (add new policy optimization algotrithms)\n- (1) Truncated IS (1802.01561) as asynchronous baseline,\n- (2) CISPO (2506.13585) as asynchronous baseline,\n- (3) TOPR (2503.14286) as asynchronous baseline,\n- (4) GSPO with trust region (by Reviewer 8QcJ),\n- (5) GSPO with defensive sampling (by Reviewer 8QcJ),\n- (6) GMPO (2507.20673), concurrent work,\n- (7) VL Norm (2509.07558), concurrent work.\n\nDue to time constraints, the new experiments have so far only been completed for the 1.7B model; results for larger model (such as 8B) are still running, and we will update them as soon as possible. Detailed point-by-point responses follow in our individual reviewer replies."}}, "id": "UIHJKYPC5F", "forum": "movNwgQtDt", "replyto": "movNwgQtDt", "signatures": ["ICLR.cc/2026/Conference/Submission1554/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1554/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1554/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763687966319, "cdate": 1763687966319, "tmdate": 1763690648888, "mdate": 1763690648888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}