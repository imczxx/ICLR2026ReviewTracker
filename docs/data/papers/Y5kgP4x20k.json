{"id": "Y5kgP4x20k", "number": 19739, "cdate": 1758298932946, "mdate": 1763645749111, "content": {"title": "SparseSkeleton: Prefill sparse attention by decomposition", "abstract": "Multi-head attention (MHA) and grouped query head attention (GQA) consti-\ntute essential architectural components of modern large language models (LLMs).\nEven though attention computations remain relatively inexpensive for small-scale\ninputs, the computational cost increases quadratically as the input size expands.\nIn long-context scenarios, including tasks such as book-level summarization or\ncode repos analysis, time-to-first-token (TTFT) performance can deteriorate sig-\nnificantly. Although various studies have improved prefill stage performance by\nexploiting sparsity structure, sparsity can still be further increased with structure\nrefinements.\nIn this work, we propose an approximate on-line decomposition of the attention\nmatrix which is able to dynamically identify additional sparsity. The attention\nmatrix is decomposed into three components: a slash component, a vertical com-\nponent, and a horizontal component. Each component requires only linear space,\nthereby enabling more efficient processing compared to the full attention matrix.\nThe decomposition is computed from query and key tokens using a linear-time\nalgorithm. The statistical properties of the decomposition allow generation of the\nmask by merely selecting elements that exceed a threshold. The threshold itself\ncan be chosen to limit the difference with regular dense attention or to respect a\ncertain time-budget.\nWe demonstrate that this technique can be directly applied – without requiring\nretraining – to networks employing standard dense attention mechanisms (MHA,\nGQA) and RoPE. We show that precision is maintained across the ∞Bench and\nPG-19 benchmarks for LLAMA-3-8B-INSTRUCT-1048K. Furthermore, we ob-\nserve substantial increases in sparsity and corresponding speedup compared to\nprevious methods. We halve the number of FLOP relative to State-of-the-Art on\none million tokens.", "tldr": "", "keywords": ["LLM", "Long-context Prefill", "Decomposition", "Sparse Pattern", "Sparse Attention", "Dynamic Sparse Attention."], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f7e377d8282e1d11f339cfdf3eabad5e93d2044.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes SparseSkeleton, a training-free, online prefill sparse-attention method that decomposes the attention matrix into three factors: slash, vertical, and horizontal, and then builds a block-sparse mask by thresholding or by meeting a time-budget target. It supports MHA/GQA with RoPE without retraining and integrates into vLLM with custom kernels for the prefill phase. Experiments on PG-19 and ∞Bench with Llama-3-8B-Instruct-1048K show comparable accuracy to dense and prior sparse-prefill baselines (including MInference) while further reducing prefill PFLOPs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The design is training-free, with no model modification.\n2. The slash/vertical/horizontal factorization is simple, statistically motivated, and maps cleanly to block-sparse execution with either an error-threshold or budget control.\n3. Authors integrated the method into vLLM with custom kernels for prefill, demonstrating the engineering practicality."}, "weaknesses": {"value": "1.  Although well-packaged and more unified, the proposed method appears to be a reformulation and modest generalization of MInference. The core ideas of combining diagonal and vertical structures to sparsify prefill attention are already central to prior work. The “horizontal” term is a reasonable extension, but its essentialness is not convincingly demonstrated.\n2. The paper emphasizes PFLOPs and mask density, but lacks the TTFT evaluation, which is important for real acceleration.\n3. On ∞Bench, the method underperforms MInference on some retrieval-heavy tasks (e.g., Retrieval.KV: 1.40 vs 10.20), indicating possible over-pruning of long-range signals. This raises concerns about the robustness of the sparsity pattern across diverse workloads.\n4. The paper lacks strong ablation studies to justify the necessity of all three components. It remains unclear how much the horizontal term contributes relative to slash + vertical (as in MInference), or whether the added complexity is always worth the compute.\n5. Only evaluated on a single model, single scale. The method is only tested on LLaMA-3-8B-Instruct. It remains uncertain how well this scales to larger or smaller models (e.g., 1.7B, 14B, 32B)."}, "questions": {"value": "1. What is the marginal gain of the horizontal term? Could you provide an ablation where A_h is removed or replaced with a constant, and quantify its effect on FLOPs and accuracy?\n2. What is the actual runtime breakdown and the e2e latency? Could you quantify the proportion of total prefill time spent in decomposition vs attention computation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IDjB7SWYZD", "forum": "Y5kgP4x20k", "replyto": "Y5kgP4x20k", "signatures": ["ICLR.cc/2026/Conference/Submission19739/Reviewer_ywap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19739/Reviewer_ywap"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837780347, "cdate": 1761837780347, "tmdate": 1762931571557, "mdate": 1762931571557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new sparse attention mechanism based on online decomposition. It decomposes the standard attention computation into three components: *slash*, *vertical*, and *horizontal*. The product of these components is used to construct a block-sparse mask during prefill. Experiments validate the FLOPs and performance of the proposed method."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is clearly formulated and well-explained. Alternative designs and approximations are discussed systematically.\n2. The method demonstrates improved average performance and reduced FLOPs compared to existing sparse attention baselines across evaluated benchmarks."}, "weaknesses": {"value": "1. Limited experiments: The paper lacks sufficient empirical validation. Although FLOP reduction is reported, no end-to-end wall-clock speedup is provided, leaving readers uncertain about the practical efficiency gains. Moreover, ablation studies analyzing the contribution of each component are missing. The paper also does not analyze the resulting sparse patterns in detail. The experiment section is the main weakness of the paper and dominates the reviewer's final decision.\n\n2. Presentation: The discussion of the method is somewhat flat, without emphasizing the key principles or design motivations early on. This structure may make it difficult for readers to grasp the main ideas before diving into implementation details."}, "questions": {"value": "1. What is the end-to-end wall-clock speedup achieved by the proposed method compared to dense attention?\n\n2. In which types of tasks or data domains does the proposed method particularly excel compared to other sparse attention methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZS7IWS4Oaf", "forum": "Y5kgP4x20k", "replyto": "Y5kgP4x20k", "signatures": ["ICLR.cc/2026/Conference/Submission19739/Reviewer_mqk9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19739/Reviewer_mqk9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969847184, "cdate": 1761969847184, "tmdate": 1762931570802, "mdate": 1762931570802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SparseSkeleton, a training-free, online prefill sparsification scheme that factorizes the attention matrix into three interpretable components, slash (Toeplitz/diagonal), vertical (columnwise), and horizontal (rowwise), and forms a block-sparse prefill mask by thresholding the product of these factors. The method is designed for MHA/GQA with RoPE and is integrated into vLLM with custom CUDA/Triton kernels for prefill; decoding remains dense (FlashAttention). Experiments on Llama-3-8B-Instruct-1048K report similar PG-19 perplexity to dense and prior sparse-prefill baselines, reduced prefill PFLOPs, and ∞Bench results that are mixed across tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and presents a well-motivated approach to sparse prefill attention through a simple and interpretable factorization into slash, vertical, and horizontal components. This decomposition is intuitive, mathematically grounded, and maps naturally to block-sparse execution. The method is training-free and integrates smoothly into vLLM with custom kernels, demonstrating good engineering practicality. Experimental results show reduced prefill PFLOPs with comparable perplexity to dense and prior sparse baselines, suggesting the approach can achieve meaningful compute savings without retraining. Overall, the paper's clarity, sound formulation, and practical integration are notable strengths."}, "weaknesses": {"value": "While the method is well-formulated, its empirical validation is limited. The paper focuses heavily on theoretical FLOP reduction and mask sparsity but does not provide end-to-end runtime measurements such as TTFT or wall-clock latency. Without this evidence, it remains unclear whether the proposed method offers real-world acceleration beyond simulated efficiency gains. Additionally, the evaluation is restricted to a single model scale (Llama-3-8B-Instruct), leaving questions about generality and scalability unanswered.\n\nThe contribution over prior work, particularly MInference, is also somewhat incremental. The slash and vertical components largely replicate existing ideas, and while the horizontal factor is novel, its necessity and impact are not convincingly demonstrated. The absence of ablation studies isolating this term makes it difficult to assess how much it contributes to performance or sparsity improvements.\n\nFinally, the paper's results on ∞Bench reveal inconsistent behavior, especially in retrieval-heavy tasks, where the method underperforms compared to prior approaches. This suggests potential over-pruning of long-range dependencies and raises concerns about robustness across task types. Together, these limitations weaken the overall empirical strength and make the contribution appear less substantial than it could be with more comprehensive experimentation and analysis."}, "questions": {"value": "* What is the actual end-to-end speedup (TTFT or wall-clock prefill latency) compared to dense attention and existing sparse prefill baselines such as MInference?\n* How much does the horizontal component contribute to accuracy and sparsity? Could you provide an ablation removing or simplifying this term?\n* How does the runtime cost of the decomposition (computing slash, vertical, and horizontal factors) compare to the attention computation itself?\n* Have you tested the method on different model scales or architectures to evaluate generality beyond Llama-3-8B?\n* What factors explain the performance drop on retrieval-heavy tasks in ∞Bench, and could the mask generation be adjusted to preserve long-range attention in those cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Dvnn9cnnqk", "forum": "Y5kgP4x20k", "replyto": "Y5kgP4x20k", "signatures": ["ICLR.cc/2026/Conference/Submission19739/Reviewer_kgAn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19739/Reviewer_kgAn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148222210, "cdate": 1762148222210, "tmdate": 1762931570051, "mdate": 1762931570051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "**All Reviewers**\n\nFirst of all, we want to thank you for spending your time to review our paper. We truly appreciate the opportunity to\nimprove our proposal from your feedback.\n\nWe want to clarify how the proposed solution fundamentally differs from SoTA as such as MInference: with Sparse Skeleton,\nwe are creating an approximated attention map using a fully dynamic method that has linear time complexity. We found the\nneed for 3 components to get a sufficiently accurate approximated attention map. The role of each component is to model a\nparticular feature of the attention head:\n\n - The slash pattern models the behavior of RoPE regardless of the context.\n\n - The vertical pattern models attention to tokens regardless of the query or its distance to it.\n\n - The horizontal pattern models the effect of the row-wise SoftMax, which relates all values of in each row together. In\nparticular, it models how attention is distributed between the Lambda-shape (sink and main diagonals) and the interior\n(anything but the sink and main diagonals) of the attention map.\n\n\nThe major contribution differs from related works in that these 3 sparse patterns are composed multiplicatively, instead of\nadditively with MInference for instance. This is a fundamental difference, since in our case, non-selected verticals for instance\n(the ones that are close to 0) will lead to very small values in the diagonals where they intersect. This allows capturing complex\nsparse patterns. In MInference, for instance, this is not achievable. See figure 1 for an illustrated example. As this prompt is a\nconcatenation of multiple books, it causes a stair case pattern in the attention map of some heads. Whereas MInference is not\nable to provide that particular coverage, our approach, especially thanks to the horizontal component Ah, is capable of doing so.\n\nWe also think this method can be generalized to other positional encoding schemes, in particular the ones that modify RoPE\nfor long context inference (i.e. DCA, YARN). Moreover, we do believe that our method has a more efficient way of sampling\ninformation from each attention head: our method is not limited to a specific part of the attention matrix to determine the mask\n(i.e. the last rows). We expect that it will reduce the amount of data extrapolation and reduce the overall error. In order to\ncompute final output from A.V, The current implementation uses Block mask but we plan to experiment with an element wise\nmask in order to achieve further FLOP reduction."}}, "id": "BMeHoa7BLI", "forum": "Y5kgP4x20k", "replyto": "Y5kgP4x20k", "signatures": ["ICLR.cc/2026/Conference/Submission19739/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19739/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19739/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763647736251, "cdate": 1763647736251, "tmdate": 1763647736251, "mdate": 1763647736251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}