{"id": "kKvEleeIsa", "number": 18918, "cdate": 1758291988072, "mdate": 1759897073632, "content": {"title": "MedCalc-R1: Knowledge-Guided Reward Framework for Medical Mathematical Reasoning", "abstract": "Medical mathematical reasoning is a critical component of clinical decision-making, where accuracy directly affects patient safety and treatment outcomes. However, existing large model approaches, while improving complex reasoning ability, often suffer from knowledge degradation, computational bias, and lack of interpretability. Moreover, commonly used reward mechanisms rely heavily on coarse-grained acceptable ranges, which fail to guarantee stable and precise mathematical outputs. To address these challenges, we propose a knowledge-guided reward framework with two complementary mechanisms. First, a knowledge verification reward enforces explicit formula generation and leverages an independent verification model to check both formulas and results, thereby mitigating knowledge forgetting, enhancing interpretability, and improving reasoning transparency. Second, a hybrid soft–hard reward mechanism incorporates clinical safety thresholds as hard constraints and introduces progressive accuracy-based rewards as soft optimization, simultaneously achieving improvements in both safety and precision. Extensive experiments on medical mathematical reasoning tasks demonstrate that our approach significantly outperforms existing methods in terms of reasoning accuracy, knowledge robustness, and model generalization, thereby validating the effectiveness and broad applicability of the proposed framework.", "tldr": "", "keywords": ["medical mathematical reasoning", "knowledge-guided reward", "complex reasoning", "large language model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ec712828b4870bbf19e24af99edb622440a40bc.pdf", "supplementary_material": "/attachment/9350a4fd7111f44d89c823ce5b8d7996a5a53346.zip"}, "replies": [{"content": {"summary": {"value": "The authors present a reinforcement learning framework tailored for medical mathematical reasoning, which integrates a knowledge verification reward and a hybrid answer reward. Experiments conducted on the MedCalc benchmark demonstrate that the proposed method outperforms existing baselines. Further analyses are provided to investigate the contributions of individual components."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a domain-specific reinforcement learning framework for medical mathematical reasoning, which jointly considers both the validity of formulas and the correctness of final answers in the reward design.\n- Experimental results on MedCalc-Bench show that the proposed method achieves superior performance compared to several baselines."}, "weaknesses": {"value": "- The authors emphasize that medical mathematical reasoning requires high interpretability through transparent reasoning steps, and claim that the proposed knowledge verification mechanism significantly enhances transparency and reliability. However, there is no explicit evaluation or quantitative analysis of interpretability in the experiments, which makes this claim less substantiated.\n- The proposed approach depends on clinically acceptable intervals to compute the hard reward. However, it remains unclear how these intervals are determined. Although MedCalc-Bench provides lower and upper bounds for each question, they are set to 95%–105% of the ground-truth value for equation-based calculators and identical to the ground truth otherwise. Such a setting may not always ensure clinical safety. Therefore, it would be helpful for the authors to clarify how these intervals are obtained—whether they are manually labeled (and if so, at what cost) or automatically generated (and what strategies or validation methods are used). The sensitivity of the model’s performance to potential inaccuracies in these intervals should also be discussed.\n- There are several issues in Table 1. While the caption states that bold font denotes the best performance and underlining indicates the second-best, only the last row actually follows this convention—many other rows lack proper highlighting. Moreover, some bolded values do not correspond to the highest scores; for instance, in the “Avg” column, the R1 model achieves the best result, yet the proposed method is incorrectly highlighted. This might lead to misunderstanding regarding the relative performance of the proposed approach.\n- There are two experiments (Figure 3 and Table 3) comparing different reinforcement learning strategies. However, since this work does not propose a new reinforcement learning algorithm but rather a reward mechanism tailored for medical mathematical reasoning, it is unclear how these experiments demonstrate the advantages of the proposed method."}, "questions": {"value": "- The computation of the format reward is not clearly described and would benefit from additional clarification.\n- It would be valuable to discuss the computational cost of the proposed method, as the knowledge verifier model appears to be substantially larger, potentially increasing the overall computational burden. Additionally, if the method were applied to a larger backbone model, would it require an even larger verifier model? If so, this could raise concerns about scalability.\n- The settings of several hyperparameters are not specified. The values of $\\alpha$, $\\beta$, $\\gamma$, and $\\tau$ are not provided. Moreover, while the paper discusses the influence of sample size in GRPO, it remains unclear how other hyperparameters (e.g., learning rate) are selected—whether through grid search or empirical tuning. The absence of such details may affect reproducibility.\n- Figure 3 shows that RLOO substantially outperforms GRPO, yet the main results are reported using GRPO. It would be helpful to clarify the rationale for this choice.\n- In the conclusion, the authors state that their method significantly outperforms both open-source and closed-source baselines in reasoning accuracy. However, according to Table 1, the proposed approach performs worse than DeepSeek-R1 and o1-mini on average accuracy, which appears inconsistent with that claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BGID1CmHNq", "forum": "kKvEleeIsa", "replyto": "kKvEleeIsa", "signatures": ["ICLR.cc/2026/Conference/Submission18918/Reviewer_dW9S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18918/Reviewer_dW9S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761556813189, "cdate": 1761556813189, "tmdate": 1762930907308, "mdate": 1762930907308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MEDCALC-R1, a framework designed to improve the accuracy, safety, and transparency of Large Language Models (LLMs) in high-stakes medical mathematical reasoning. It addresses existing model failures, such as \"knowledge degradation\" and computational bias, by implementing a novel two-part reinforcement learning (RL) reward system. This system includes a knowledge verification reward, which forces the model to explicitly generate a formula that is then checked by an independent verifier , and a hybrid soft-hard reward, which heavily penalizes answers outside a clinical safety range (hard constraint) while also rewarding answers for their precise proximity to the correct value (soft optimization)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The framework's primary strength is its reward design, which is the first to integrate explicit formula-level verification directly into an RL process and uniquely combines strict clinical safety constraints with fine-grained numerical precision. This approach leads to strong empirical performance, enabling smaller MedCalc-R1 models to significantly outperform much larger open-source baselines, especially in critical tasks like drug dosage calculation."}, "weaknesses": {"value": "The \"Soft Reward\" design is counter-intuitive and appears unjustifiable. Specifically, for the task of calculating medical metrics from a given patient's Electronic Health Record (EHR), there should be a definitive standard answer. The LLM's output is binary—either correct or incorrect. In such a scenario, the introduction of a Soft Reward mechanism seems unnecessary and lacks a strong rationale. Notably, the paper references DeepSeek R1, whose most significant contribution was achieving SOTA performance on analogous calculation tasks (which have ground truths) by employing a Rule-Based Hard Reward. This paper's approach seems to be in direct opposition to the core findings of DeepSeek R1.\n\nIt is unclear why the authors employed a surprisingly weak LLM (Qwen 2.5 32B) as the External Verifier. As illustrated in Figure 4, this verifier appears largely incapable of accurately assessing the correctness of the generated formulas. The authors must justify why they opted against using demonstrably stronger LLMs, such as DeepSeek R1, GPT-o1, Qwen3-235B-A22B, or Qwen2.5-72B, which would presumably yield better verification performance.\n\nThe paper defines an \"acceptable range\" for predicted values (L and U), yet it seemingly fails to explain how these bounds are determined in practice. Furthermore, in Subsection 3.3, the authors use the expression \"belongs to the set of valid formulas Φ(x) defined for the task.\" My understanding is that Φ(x) should represent a single, unique solution. The calculation of a specific medical metric or risk score typically relies on one established formula, not a \"set of functions\" as the notation implies.\n\nThe proposed model's performance is significantly inferior to DeepSeek R1. While this is likely because the model trained in this study is smaller, the fact remains that hospitals can deploy DeepSeek R1 locally to achieve superior results. This reality substantially undermines the practical significance and contribution of this work.\n\nFollowing on from the first point (regarding the reward design), the paper's contribution appears to be limited to a simple modification of the Reward Function applied to an off-the-shelf RL algorithm. Although this modified reward function shows some utility, the paper lacks a more granular or deeper analysis of its effects."}, "questions": {"value": "Why the authors adopted a weak LLM (Qwen 2.5 32B) as the External Verifier?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EATHkW2uSm", "forum": "kKvEleeIsa", "replyto": "kKvEleeIsa", "signatures": ["ICLR.cc/2026/Conference/Submission18918/Reviewer_KHHK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18918/Reviewer_KHHK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657656197, "cdate": 1761657656197, "tmdate": 1762930906526, "mdate": 1762930906526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on medical calculation tasks (e.g., drug dosage, clinical score, and physiological index estimation) and proposes a knowledge-guided training and evaluation framework that integrates formula-based verification and reward design. The method is evaluated only on a single dataset, showing competitive results for small-scale models (1.5B and 3B). Overall, the paper is clearly written and easy to follow."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1  The target task is of high clinical importance since computational accuracy directly affects treatment safety and risk assessment.\n\nS2  The methodology and experiments are presented in a logically coherent manner and can be followed without major difficulty.\n\nS3  The paper emphasizes explicit formula-based reasoning and verifiable outputs, aligning well with the need for traceability."}, "weaknesses": {"value": "W1 The authors use the term Medical Mathematical Reasoning, which is rarely seen in recent literature. The task described essentially overlaps with Medical/Clinical Calculation, a term widely adopted by prior works and benchmarks. If the task is equivalent, adopting the mainstream naming would make the work more discoverable and consistent with community standards.\n\nW2 The paper claims improvements for small models (1.5B/3B) while acknowledging that larger general-purpose models still perform better overall, which is expected. However, the comparison set is limited: It lacks specialized small-scale models that are domain-specific or distilled for medical computation. It therefore cannot demonstrate whether the reported advantage truly arises from the proposed framework or simply from scale limitations. A fair evaluation should include both larger foundation models and smaller, domain-specific models trained under comparable conditions.\n\nW3  The entire study relies on one dataset, without cross-validation on other benchmarks such as CalcQA[1], which represent different task forms. Using only one dataset raises concerns about overfitting and limits the claim of general applicability.\n\nW4 The paper reports only one evaluation metric, which is not rigorous for medical contexts. Optimizing a single objective may bias the model toward that metric while neglecting clinically relevant dimensions. It is crucial to include additional metrics to better reflect real-world reliability.\n\nW5 The paper does not provide details on the prompt templates or sampling parameters. Prompt design significantly affects numerical reasoning, as highlighted in [2], which showed large performance gaps between well-designed and poorly designed prompts. Lack of prompt transparency weakens reproducibility and interpretability.\n\nW6 Recent studies on medical calculation, verifiable rewards, process supervision, and benchmarks are not sufficiently discussed, leaving the positioning of this paper unclear.\n\nW7 The paper omits convergence curves, variance analyses, or computational cost of reward verification, making it difficult to assess training stability or real deployment feasibility.\n\n[1]Zhu, Yakun, et al. \"MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling.\" Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 2025.\n\n[2]Goodell, Alex J., et al. \"Large language model agents can use tools to perform clinical calculations.\" npj Digital Medicine 8.1 (2025): 163."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KQF4QjFFGD", "forum": "kKvEleeIsa", "replyto": "kKvEleeIsa", "signatures": ["ICLR.cc/2026/Conference/Submission18918/Reviewer_zhwt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18918/Reviewer_zhwt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915355320, "cdate": 1761915355320, "tmdate": 1762930905685, "mdate": 1762930905685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MedCalc‑R1, a two‑stage post‑training framework for medical mathematical reasoning. After supervised fine‑tuning (SFT) to learn output format and recall, the method applies reinforcement learning with a knowledge‑guided reward that has three parts: (i) a format reward enforcing outputs that include an explicit formula, step‑wise reasoning, and final value; (ii) a knowledge verification reward that compels the model to generate a valid clinical formula and uses a stronger frozen LLM as an external verifier; and (iii) a hybrid answer reward that combines a hard clinical safety interval with a soft, accuracy‑graded term. The RL stage uses GRPO with KL regularization. On MedCalc‑Bench, a 55‑task benchmark spanning equation‑based and rule‑based clinical calculations , MedCalc‑R1 (1.5B/3B backbones) outperforms open‑source baselines and narrows the gap to proprietary systems; e.g., the 3B model achieves 51.34 average accuracy vs 39.03 for Qwen2.5‑32B‑Instruct. Ablations show both the knowledge verifier and the hybrid reward are necessary, and further analyses examine optimizer choice (RLOO/GRPO), verifier scale, and rollout count. Case studies illustrate transparent, auditable outputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well‑motivated objective: medical calculations require precision, transparency, and safety. The design directly encodes these needs.\n2. Method simplicity and practicality: integrates cleanly with standard RL for LLMs (GRPO) and does not require complex tooling; verifier is plug‑replaceable. \n3. Substantive empirical gains on MedCalc‑Bench, including outperforming much larger open‑source models; strong results on Dosage and Physical subtasks.\n4. Careful analyses: ablations for both mechanisms, optimizer comparison, verifier capacity study, rollout sensitivity, and cross‑task generalization.\n5. Transparency: case studies show explicit formulas and computations, aligning with the paper’s interpretability goal."}, "weaknesses": {"value": "1. Verifier specification and reliability. The formula‑verification reward hinges on an LLM judge. The paper does not quantify verifier accuracy vs. ground‑truth formula annotations, inter‑rater reliability, or the impact of verifier mistakes on training stability. It is also unclear how (\\Phi(x)) is defined and enforced across 55 tasks (e.g., synonymous formulas, unit variants, edge cases). Providing a measured verifier error rate and robustness to noisy judgments would strengthen claims. \n\n2. Format reward under‑specified. \"FORMATREWARD\" is referenced but not defined with an exact scoring schema (regex/spec parsing, weighting, penalties). Reproducibility would benefit from a formal grammar and ablations on this component alone. \n\n3. Safety interval dependence. The hard reward uses per‑item ([L,U]). Although this fits the benchmark (case card shows lower/upper limits), practical deployment may not supply such intervals, and they may be guideline‑dependent. A sensitivity study varying ([L,U]) or learning them from task definitions would clarify generality. \n\n4. Interpretability not quantitatively evaluated. While format and verification aim to enhance transparency, the paper does not include process‑level metrics (e.g., formula correctness rate, step‑consistency checks, auditor agreement). The case studies are illustrative but anecdotal. \n\n5. Limited text‑to‑structure realism. Inputs are structured key patient features rather than raw notes; this reduces complexity and may overstate clinical readiness. An end‑to‑end pipeline with clinical text extraction would be informative.\n\n6. Statistical reporting. Main tables report point accuracies without confidence intervals or multi‑seed variance; some gains are large, but dispersion would help assess stability."}, "questions": {"value": "1. Verifier details: How is (\\Phi(x)) (the set of valid formulas) constructed for each subtask? Are symbolic templates or unit‑normalized forms used? What is the observed false‑positive/false‑negative rate of the LLM verifier relative to human annotations across a held‑out set? \n\n2. Format reward rubric: Please provide the exact scoring (parsing rules, penalties for missing sections, weighting) for \"FORMATREWARD\". Could you add an ablation \"w/o format reward\" to disentangle its effect from the other two components? \n\n3. Clinical intervals: Are ([L,U]) always available in MedCalc‑Bench, and how are they derived? How sensitive are results to widening/narrowing these intervals, or replacing them with published clinical thresholds when multiple guideline variants exist? \n\n4. Verifier robustness: What happens if the verifier is smaller/weaker or adversarially perturbed (e.g., small wording changes)? Can you stabilize training with soft verification scores (confidence/logits) instead of a hard ({+1,-1}) signal?\n\n5. Process metrics: Can you report formula‑correctness rate and step‑consistency metrics (e.g., recomputing the final answer from the emitted steps) to substantiate interpretability gains beyond final accuracy? \n\n6. Variance and significance: Please include multi‑seed means/standard deviations (or CIs) for Table 1 and Table 2, and clarify the number of runs used for the optimizer comparisons in Fig. 3. \n\n7. Scalability: What are the compute and latency overheads of verifier calls during RL and at inference (if any)? Could self‑verification or distilled verifiers reduce cost without hurting accuracy? \n\n8. Distribution shift: Since inputs are structured, do results hold if you prepend short natural‑language case narratives or vary units (mg/dL vs µmol/L)? Any experiments on unit normalization errors? \n\n9. Would you please benchmark state-of-the-art reasoning LLMs, e.g., GPT-5-high, Gemini-2.5-pro, Claude 4.5-thinking, etc.? If their performance is already excellent, that might indicate the problem is largely solved, especially considering that the compute used here is negligible compared with the massive post-training compute invested in those SOTA reasoning models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dZNDh8Fuhz", "forum": "kKvEleeIsa", "replyto": "kKvEleeIsa", "signatures": ["ICLR.cc/2026/Conference/Submission18918/Reviewer_ezs7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18918/Reviewer_ezs7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960070251, "cdate": 1761960070251, "tmdate": 1762930904832, "mdate": 1762930904832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to enhance medical mathematical reasoning capabilities in large language models (LLMs) through a knowledge-guided reward framework. The proposed approach introduces two complementary mechanisms:\n1. Knowledge verification reward based on explicit formula generation, utilizing an independent verification model to assess correctness.\n2. Soft-hard reward mechanism that integrates (i) clinical safety thresholds as hard constraints and (ii) progressive accuracy-based rewards as soft optimization signals.\n\nThe model is trained in two stages, supervised fine-tuning (SFT) followed by reinforcement learning (RL), and evaluated on the MedCalc benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed reward components (format, knowledge, and answer rewards) are well-motivated and align with realistic clinical reasoning and decision-making processes.\n- The evaluation includes a wide range of zero-shot baselines, providing a strong comparative foundation.\n- The ablation studies are comprehensive and effectively demonstrate the contribution of each reward component.\n\nOverall the proposed framework is highly effective."}, "weaknesses": {"value": "- The selection of constants (e.g., values 2 and 3 for the hard reward) should be justified. Are these empirically tuned or theoretically grounded?\n- The proposed framework is only evaluated on Qwen, raising questions about its generalizability to other LLM backbones. Other LLM backbones should be evaluated.\n- The choice of five epochs for RL training should be motivated, especially if performance saturates or diverges beyond that point.\n- Table numbering in the text should follow chronological order.\n- In Table 1, the best results should be boldfaced for easier interpretation.\n- The code is not released, which limits reproducibility and transparency."}, "questions": {"value": "- How was the predefined set of valid formulas constructed?\n- What makes this framework specific to medical reasoning? Could the same reward design not generalize to other mathematical or scientific reasoning tasks? Please clarify the domain-specific assumptions or dependencies that limit its broader applicability"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PBmcsSBifc", "forum": "kKvEleeIsa", "replyto": "kKvEleeIsa", "signatures": ["ICLR.cc/2026/Conference/Submission18918/Reviewer_uCfs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18918/Reviewer_uCfs"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762837509088, "cdate": 1762837509088, "tmdate": 1762930903963, "mdate": 1762930903963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}