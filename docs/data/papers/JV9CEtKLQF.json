{"id": "JV9CEtKLQF", "number": 18352, "cdate": 1758286768885, "mdate": 1759897109130, "content": {"title": "Cut Less, Fold More: Model Compression through the Lens of Projection Geometry", "abstract": "Compressing neural networks without retraining is vital for deployment at scale. We study calibration-free compression through the lens of projection geometry: structured pruning is an axis-aligned projection, whereas model folding performs a low-rank projection via weight clustering. We formalize both as orthogonal operators and show that, within a rank distance of one, folding provably yields smaller parameter reconstruction error, and under mild smoothness assumptions, smaller functional perturbations than pruning. At scale, we evaluate >1'000 checkpoints spanning ResNet18, PreActResNet18, ViT-B/32, and CLIP ViT-B/32 on CIFAR-10 and ImageNet-1K, covering diverse training hyperparameters (optimizers, learning rates, augmentations, regularization, sharpness-aware training). We show that folding typically achieves higher post-compression accuracy, with the largest gains at moderate–high compression. The gap narrows and occasionally reverses at specific training setups. Our results position folding as a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory.", "tldr": "Folding - low-rank, geometry-aware projection - typically outperforms structured pruning for calibration-free compression, supported by a one-rank slack guarantee and results on  >1000 CNN/ViT checkpoints", "keywords": ["model compression", "structured pruning", "model folding", "projection geometry"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e0119596739a340729f496cd89922ca91c8d2db.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper builds on model folding introduced in Wang et al. (2025), comparing it to structured magnitude pruning both theoretically and empirically. The authors show that both structured pruning and folding can be viewed as orthogonal projections in parameter space, and based on that show that folding has leads to a smaller reconstruction error with one-rank slack. Experiments on more than 1,000 checkpoints of CNNs and ViTs show that folding leads to a higher post-compression accuracy than structured magnitude pruning in most cases, especially at moderate to high compression; even without the one-rank slack assumed for the theoretical results. In-depth ablation studies show that the improvement of folding over structured pruning are most pronounced in training settings that promote convergence to flatter local minima."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Overall, the paper is written well, easy to follow, and the reader is not left with many questions. The experiments are executed well, showing results for a multitude of models over four architectures, and in-depth ablation studies are provided. The experiments without post-compression reconstruction show that in most cases, FOLD outperforms MAG, with the gaps increasing with compression ratio, validating the findings in Wang et al. (2025). It is interesting to see how the training procedure impacts the performance of the two post-training compression techniques studied in the paper. Experiments comparing FOLD and MAG with post compression fine tuning of only normalization layers or the full models provide novel insights on how folding and magnitude pruning compare in this setting."}, "weaknesses": {"value": "I have several concerns regarding soundness and contribution of the work, which I explain below. I hope that these remarks are helpful in improving the work and I am happy to discuss my evaluation.\n\n- Theorems 2.1 and 2.2 compare structured magnitude pruning with $k$ retained rows and folding with $k+1$ retained rows and conclude that folding, in theory, outperforms pruning. However, the error inequalities given in the theorems say nothing about the performance of folding compared to pruning as pruning with $k+1$ retained rows also outperforms pruning with $k$ retained rows.\n- The claim of \"folding incurs provably smaller functional deviation\" seems to refer to the output or decision of the model, but is based on the parameter-Lipschitz assumption of the loss. This bound does not control the change in model output directly. \n- The setup is called \"calibration-free\" yet for CNNs, there is no evaluation without data-based post-pruning reconstruction. Even in the no fine-tuning setting, REPAIR is used to re-estimate the batch-normalization. \n- The experiments are conducted only with small scale architectures, the largest being CLIP ViT-B-32 at 151M parameters. The authors point out that calibration-free structured pruning methods collapse in the LLM pruning setting. However, experiments on larger vision transformers like ViT-22B would be interesting.\n- The figures in the paper mostly contain plots comparing model quality after FOLD and MAG compression but not the quality of the uncompressed models. In some cases, this makes it hard to judge if low accuracy is due to compression performing badly or the base model itself having low accuracy - it might be the case that FOLD performs better especially in the cases where the base model itself is weak.\n- Minor typo: In line 146, the loss difference is measured using the Frobenius norm."}, "questions": {"value": "- How does the pre-compression model accuracy relate to delta accuracy? This is especially interesting since augmentation is both known to strongly impact final model performance and has a large impact on delta accuracy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9eAnfk2edt", "forum": "JV9CEtKLQF", "replyto": "JV9CEtKLQF", "signatures": ["ICLR.cc/2026/Conference/Submission18352/Reviewer_kdrS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18352/Reviewer_kdrS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903136248, "cdate": 1761903136248, "tmdate": 1762928063923, "mdate": 1762928063923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper frames structured pruning and model folding as orthogonal projections in parameter space: pruning is an axis-aligned projection, while folding is a low-rank, cluster-structured projection. The authors prove that folding yields smaller parameter reconstruction error and tighter (Lipschitz-based) bounds on functional perturbation than pruning. A large-scale, calibration-free study shows folding usually achieves higher post-compression accuracy, especially at moderate to high compression, and it keeps its edge after lightweight or short fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Casting pruning and folding as orthogonal projections cleanly explains their geometric differences; theorems show folding’s strictly smaller projection error and hence tighter loss perturbation under mild smoothness\n\n- The evaluation spans >1000 checkpoints and multiple architectures/datasets, with consistent wins for folding at moderate to high compression and robustness to training variations.\n\n- Folding’s advantage persists after minimal recalibration (e.g., BN/LayerNorm reset) and 1-5 epochs of fine-tuning, which mirrors realistic deployment where full retraining is expensive."}, "weaknesses": {"value": "- The core guarantee uses a one-rank slack (pruning rank vs. folding rank)  Although the authors state experiments match retained parameters/FLOPs, a theoretical result at exactly matched rank would remove any residual ambiguity, especially at low compression where a single unit can matter. Consider strengthening theory (e.g., conditions under which folding dominates at equal rank) or adding tighter empirical per-layer equality checks to preclude hidden capacity differences.\n\n- Results focus on accuracy; there’s little on real-world latency, memory bandwidth, or kernel fusion performance after folding - despite identical FLOPs at matched size, wall-clock behavior can differ."}, "questions": {"value": "Can you provide per-layer tables verifying that, for every comparison, parameters and FLOPs (and effective activations) are exactly matched between folding and pruning after the next-layer adaptation, not just approximately matched globally? If there are exceptions, quantify their impact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t0xxC0ieIR", "forum": "JV9CEtKLQF", "replyto": "JV9CEtKLQF", "signatures": ["ICLR.cc/2026/Conference/Submission18352/Reviewer_y5Wu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18352/Reviewer_y5Wu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065209553, "cdate": 1762065209553, "tmdate": 1762928063180, "mdate": 1762928063180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies calibration-free model compression by framing pruning and model folding as orthogonal projections in parameter space. The authors argue that folding (weight clustering) preserves functional similarity better than pruning (weight removal) and provide both theoretical guarantees and large-scale empirical validation over 1,000 CNN and ViT checkpoints across CIFAR-10 and ImageNet-1K. Folding is shown to outperform pruning, especially at moderate-to-high compression levels."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The projection-theoretic framing of pruning and folding is reasonable. Framing compression as orthogonal projection provides an solid interpretation.\n\n- The paper is writing well with good visualizations."}, "weaknesses": {"value": "- The main argument is questionable. The paper argues that model folding is better than model pruning with closer distance to the original models. This statement is a little problematic. Since pruning domain has been developed broadly and deeply in the past decade, many pruning methods could produce pruning models even surpassing original models. The logic here is that pruning could eliminate some harmful neurons to make the pruned model to be stronger. Therefore, the distance to original model is not sufficiently strong and convincing enough to support the outperformance.\n\n- Numerical experiment is limited. The paper only compares with basic magnitude pruning schema. Given theoretical support being not sufficient, it should compare and discuss with more recent representative works, e.g., DepGraph, OTO, etc., which contains more pruning protocols."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ciAESUjkoV", "forum": "JV9CEtKLQF", "replyto": "JV9CEtKLQF", "signatures": ["ICLR.cc/2026/Conference/Submission18352/Reviewer_b6Qf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18352/Reviewer_b6Qf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154151618, "cdate": 1762154151618, "tmdate": 1762928059323, "mdate": 1762928059323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified theoretical framework showing that model folding and structured pruning are orthogonal projections onto different subspaces, with folding provably achieving smaller parameter reconstruction error by projecting onto cluster-structured rather than axis-aligned subspaces. The extensive empirical validation (>1,000 models across ResNet18, PreActResNet18, ViT-B/32, CLIP ViT-B/32 on CIFAR-10/ImageNet-1K) confirms folding consistently outperforms magnitude pruning, especially at moderate-to-high compression ratios, with advantages persisting through fine-tuning. While limited to vision models, the work makes a valuable contribution by establishing folding as a principled, geometry-aware alternative to pruning that better preserves network functionality, supported by both rigorous theory and comprehensive ablations identifying when folding excels (moderate learning rates, SAM training) versus when the gap narrows (extreme hyperparameters)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "• **Solid theoretical contribution**: Formalization of pruning and folding as orthogonal projections with provable guarantees on parameter reconstruction error, providing principled understanding of compression methods\n\n• **Extensive empirical validation with thorough ablations**: Over 1,000 checkpoints across diverse architectures (CNNs/ViTs) and datasets, with systematic investigation of learning rates, SAM, augmentation, and regularization effects that clearly identify when folding excels\n\n• **Clear and informative figures**: The scatter plots with color-coded compression ratios and bar plots showing accuracy gaps effectively communicate results across all tested architectures\n\n• **Good organization and presentation**: Paper flows logically from theory to experiments to ablations, with comprehensive appendices providing complete experimental details"}, "weaknesses": {"value": "**Major Weaknesses**\n\n- **Limited Scope to Small-Scale Models and Datasets**: Evaluations are confined to relatively small architectures such as ResNet18 (11M parameters) and ViT-B/32 (~86M parameters) on CIFAR-10 and ImageNet-1K. The absence of experiments on large-scale models like LLMs (e.g., GPT series) or diffusion models limits confidence in the method’s scalability and its relevance to modern deployment settings with billions of parameters.\n\n- **Insufficient Comparisons to State-of-the-Art Methods**: Although the paper benchmarks against basic magnitude pruning (L1/L2), it lacks head-to-head comparisons with stronger recent baselines such as SparseGPT [1] and Wanda [2] approaches, despite referencing them in Appendix F. As a result, it remains unclear whether folding offers advantages over cutting-edge compression methods.\n\n- **Lack of Empirical Validation for Some of the Claims**: The paper offers intuitive rationales for hyperparameter effects (e.g., SAM for flatter minima, augmentations for invariant solutions) but lacks direct measurements like Hessian eigenvalues or sharpness metrics to quantify them. It cites literature on optimizers and learning rates influencing curvature but conducts no targeted experiments to verify causal links, leaving claims as unverified hypotheses.\n\n- **Lack of Predictive Metrics**: The paper's ablation studies spot cases where pruning beats folding (e.g., very high learning rates with Adam or large SAM radii), but it fails to create predictive tools like sharpness thresholds, weight sparsity ratios, or loss curvature metrics—to foresee these without tons of tests. This after-the-fact analysis offers little real-world advice on choosing methods.\n\n- **Calibration-Free Claim**: While described as calibration-free, the approach still depends on REPAIR (BatchNorm reset) for CNNs, LayerNorm resets for ViTs, and short fine-tuning (up to 5 epochs) to reach its best results. These requirements imply partial data access and tuning, which conflict with the notion of a truly data-free or zero-shot compression pipeline.\n\n**Minor Weaknesses**\n\n- **Deferred Related Work**: A detailed discussion of prior art is confined to Appendix F due to space constraints, which may force readers to cross-reference for context on novelty, potentially disrupting flow in the main text.\n\n- **Lack of Runtime Analysis**: Folding involves k-means clustering per layer, which is computationally heavier than magnitude-based pruning's simple sorting, but the paper provides no measurements or discussions of overhead, overlooking efficiency trade-offs.\n\n[1] Frantar, E. \"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot.\" arXiv preprint arXiv:2301.00774, January 2023. \n\n[2] Sun, M. \"A Simple and Effective Pruning Approach for Large Language Models.\" arXiv preprint arXiv:2306.11695, June 2023 (published at ICLR 2024)."}, "questions": {"value": "see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t2TT8UX1q7", "forum": "JV9CEtKLQF", "replyto": "JV9CEtKLQF", "signatures": ["ICLR.cc/2026/Conference/Submission18352/Reviewer_VBA4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18352/Reviewer_VBA4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762237695763, "cdate": 1762237695763, "tmdate": 1762928058410, "mdate": 1762928058410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}