{"id": "e4SvuiVHcW", "number": 14024, "cdate": 1758227193704, "mdate": 1763771848868, "content": {"title": "Resolving Oversmoothing with Opinion Dissensus", "abstract": "While graph neural networks (GNNs) have allowed researchers to successfully apply neural networks to non-Euclidean domains, deep GNNs often exhibit lower predictive performance than their shallow counterparts. This phenomena has been attributed in part to oversmoothing, the tendency of node representations to become increasingly similar with network depth. In this paper we introduce an analogy between oversmoothing in GNNs and consensus (i.e., perfect agreement) in multi-agent systems literature. We show that the message passing algorithms of several GNN models are equivalent to linear opinion dynamics in multi-agent systems, which have been shown to converge to consensus for all inputs regardless of the initial state. This new perspective on oversmoothing motivates the use of nonlinear opinion dynamics as an inductive bias in GNN models. In addition to being more general than the linear opinion dynamics model, nonlinear opinion dynamics models can be designed to converge to dissensus for general inputs. Through extensive experiments we show that our Behavior-inspired message passing (BIMP) neural network resists oversmoothing beyond 100 time steps and consistently outperforms existing continuous time GNNs even when amended with oversmoothing mitigation techniques. We also show several desirable properties including well behaved gradients and adaptability to homophilic and heterophilic datasets.", "tldr": "We propose a continuous-depth GNN inspired by behavioral interaction, which is provably robust to oversmoothing with well behaved gradients and adaptability across homophilic and heterophilic datasets.", "keywords": ["Graph neural networks", "oversmoothing", "decision and control"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5943a64d5e32619a3d53b3dfddfca24e5a03aae6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes BIMP (Behavior-Inspired Message Passing), a new class of graph neural networks inspired by nonlinear opinion dynamics. The authors establish a formal analogy between GNN message passing and multi-agent opinion consensus, theoretically showing that oversmoothing in GNNs corresponds to opinion consensus in social dynamics. Based on this insight, they design a nonlinear dynamical system that maintains diversity among node representations. The proposed BIMP model integrates communication and option graphs, introduces nonlinear saturation and bifurcation-controlled parameters, and achieves strong robustness against oversmoothing while remaining computationally efficient."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Theoretical originality:\n    \n    The paper establishes a clear and elegant analogy between oversmoothing in GNNs and opinion consensus in nonlinear dynamics, offering a novel theoretical lens for understanding message passing.\n    \n2. Well-motivated nonlinear model:\n    \n    The introduction of nonlinear saturation functions, bifurcation-controlled attention, and external inputs provides a principled way to prevent consensus, moving beyond heuristic anti-oversmoothing tricks.\n    \n3. Comprehensive analysis and proofs:\n    \n    Theoretical lemmas and theorems rigorously support the model’s stability, convergence, and dissensus properties.\n    \n4. Strong empirical validation:\n    \n    BIMP demonstrates stable Dirichlet energy over 1000 timesteps and consistent superiority across both homophilic and heterophilic datasets, outperforming recent continuous-depth GNNs such as GRAND, GraphCON-Tran, and KuramotoGNN."}, "weaknesses": {"value": "1. Ablation clarity\n\n   The authors should include an additional Dirichlet energy ablation experiment (on activation functions and inductive bias) to verify the impact of different functions and modules on oversmoothing.\n\nMinor issues\n* Please provide citations for the baseline models in the main text, and ideally include brief descriptions of these baselines."}, "questions": {"value": "1. To what extent is the robustness to oversmoothing due to the nonlinear opinion dynamics itself, versus architectural choices (e.g., residual terms, attention normalization)?\n    \n2. Does the bifurcation-controlled parameter ( $u = \\frac{d}{\\alpha + 3}$ ) require careful tuning for different datasets, or is it generally robust across tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3EVw4P6VzF", "forum": "e4SvuiVHcW", "replyto": "e4SvuiVHcW", "signatures": ["ICLR.cc/2026/Conference/Submission14024/Reviewer_AqGA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14024/Reviewer_AqGA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760584698146, "cdate": 1760584698146, "tmdate": 1762924514705, "mdate": 1762924514705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of oversmoothing in GNNs. It makes two significant contributions. First, it proposes a general framework to determine if a GNN model suffers from oversmoothing. In particular, this framework proves oversmoothing for linear discrete-depth GNNs and continuous-depth Laplacian GNNs. This is made possible by mapping a continuous depth GNN to opinion dynamics. Based on this result, the second contribution is a GNN model that does not suffer from oversmoothing. Additionally, the paper tests the performance of the proposed GNN on various datasets, showing slightly better accuracy than state-of-the-art models using similar computational power."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents an innovative connection between GNNs and opinion dynamics. The proposed GNN model is not susceptible to oversmoothing and outperforms state-of-the-art models."}, "weaknesses": {"value": "Some relevant related works may be missing from the paper. For example, there is no reference to how this work relates to previous work on oversmoothing, such as *A Note on Over-Smoothing for Graph Neural Networks by Chen Cai and Yusu Wang* or *Residual Connections and Normalization Can Provably Prevent Oversmoothing in GNNs by Michael Scholkemper, Xinyi Wu, Ali Jadbabaie, and Michael T. Schaub*, which appeared in ICLR 2025. Additionally, there is insufficient reference to discrete-time opinion dynamics, of which there are many nonlinear examples that may adapt better to describe discrete-depth GNNs. See, e.g., *Consensus Dynamics: An Overview by Luca Becchetti, Andrea Clementi, and Emanuele Natale (SIGACT News 2020)*. The relation to these kind works is the subject of Question 3 below. \nFinally, it is natural to wonder how the external input B in your paper relate to skip connections (see Question 1)."}, "questions": {"value": "1. How does external input B in your paper relate to skip connections, which are known in the literature to prevent oversmoothing? \n2. Could the fact that GNNs based on Laplacian dynamics suffer from oversmoothing, even with external input, be an effect of the Laplacian dynamic rather than the linearity of the dynamic? \n3. How does your work compare to the references mentioned in the Weaknesses section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WpqpcBbzBE", "forum": "e4SvuiVHcW", "replyto": "e4SvuiVHcW", "signatures": ["ICLR.cc/2026/Conference/Submission14024/Reviewer_XBcV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14024/Reviewer_XBcV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738232139, "cdate": 1761738232139, "tmdate": 1762924514176, "mdate": 1762924514176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their time and constructive feedback. We are encouraged that the reviewers found our proposed BIMP model to be innovative, well motivated, and clearly presented (hd9T, XBcV, AqGA); supported by formal proofs and analysis (hd9T, AqGA); resistant to oversmoothing at large depths (hd9T, XBcV); and performant against baselines methods (AqGA, XBcV).\n\nIn response to reviewer questions about the source of robustness to oversmoothing (hd9T, XBcV, AqGA), we highlight our experimental results in Figure 5, Figure 6 and Table 13. Comparisons of classification accuracy (Figure 5) and Dirichlet energy (Figure 6) demonstrate that architectural choices alone (i.e., normalization and skip-connections in Pairnorm and DGN), cannot mitigate oversmoothing at large depth; and our ablation study (Table 13) shows that BIMP becomes susceptible to oversmoothing when the nonlinearity in our aggregation function is removed (even if residuals and attention are retained). This strongly suggests the robustness of our architecture is due to our inductive bias and not conventional architectural choices.\n\nIn response to reviewer hd9T’s request for a concrete real-world experiment where oversmoothing becomes the bottleneck, we highlight our multi-agent trajectory prediction task. We added a new visualization of latent features and their variance in Figure 4, and observe that for baseline models, the latent representations become similar for all agents resulting in similar next state predictions. These results strengthen our motivation for resolving oversmoothing, and highlight our model’s superior performance.\n\nIn response to reviewer hd9T’s question about resilience to oversquashing, we have added new results on the long-range molecular graph benchmark (Peptides-func and Peptides-struct datasets). In Table 3 we show that our model performs on par with state-of-the-art models, while requiring fewer parameters, shorter training time, and lower memory usage. Thank you for pointing out this opportunity to broaden the scope of our paper to over-squashing, which is a fundamental bottleneck that restricts long-range information.\n\nIn response to reviewer AqGA’s question about the robustness of our model to hyperparameter selection, we have added a new sensitivity analysis. In Appendix E.5, Figure 9, we include results on 2 homophilic datasets (Cora, Pubmed) and 2 heterophilic datasets (Chameleon and Squirrel). Thank you for suggesting this opportunity to show that our model is performant for a broad range of damping $d$ and self‐reinforcement $\\alpha$ values. \n\nWe address other reviewer comments below and have incorporated all feedback in our rebuttal revision. We highlight changes to our original manuscript with blue text."}}, "id": "8c3EoMa0kZ", "forum": "e4SvuiVHcW", "replyto": "e4SvuiVHcW", "signatures": ["ICLR.cc/2026/Conference/Submission14024/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14024/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14024/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763771946464, "cdate": 1763771946464, "tmdate": 1763771946464, "mdate": 1763771946464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the oversmoothing problem in graph neural networks (GNNs) by drawing an analogy to consensus formation in multi-agent opinion dynamics. The authors show that several classes of existing GNNs can be interpreted as equivalent to linear opinion dynamics, which necessarily converge to consensus, and therefore inevitably oversmooth. Based on nonlinear opinion dynamics models, the paper proposes a continuous-depth GNN architecture named BIMP (Behavior-Inspired Message Passing), which is theoretically guaranteed to avoid oversmoothing under certain conditions. Experiments are conducted on common node-classification benchmarks to demonstrate resistance to feature collapse at large depths and competitive accuracy against baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The analogy between oversmoothing and consensus dynamics is presented clearly and supported by formal statements, which may help unify prior perspectives on oversmoothing.\n2. The paper states assumptions and propositions explicitly, with proofs delegated to the appendix, making the theoretical section readable.\n3. The empirical plots demonstrate that the proposed model maintains stable Dirichlet energy and accuracy even when simulated for up to 10^3 layers, which supports the main claim of the proposed framework on the oversmoothing side.\n4. The paper is easy to follow, and figures illustrating the bifurcation behavior are helpful in conveying the underlying intuition."}, "weaknesses": {"value": "1. Necessity of focusing on oversmoothing is not fully justified.\n\nThe paper treats oversmoothing as a central obstacle in GNN design, but it is unclear why controlling depth-induced feature collapse is still an impactful problem in practice. Many modern architectures (e.g., graph transformers, attention-based models with residual paths, or shallow-but-expressive methods) already achieve strong performance with 2–4 layers, often outperforming deeper continuous-depth models regardless of oversmoothing behavior. If a 3-layer model can surpass a 128-step ODE-based model, the motivation for pursuing “arbitrarily deep but stable” GNNs needs stronger justification. The paper would benefit from either (a) a concrete real-world setting where depth improves performance and oversmoothing becomes the bottleneck, or (b) evidence that existing expressive architectures still fail due to oversmoothing rather than other factors (e.g., over-squashing, limited expressivity, memory constraints).\n\n2. Benchmark coverage and presentation choices raise questions.\n\nWhile the standard homophilic datasets are reported in the main text, several heterophilic results appear only in the appendix, despite being directly relevant to the claim that the model adapts to both regimes via tunable filtering. In addition, the benchmark suite does not explore domains where deep message passing is naturally required (e.g., long-range molecular graphs, multi-hop reasoning, or large-scale relational systems). Since the paper does not claim improvements in runtime, scalability, or memory, the restriction to small- to medium-scale citation graphs feels limiting.\n\n3. Depth stability is demonstrated, but the practical benefit is unclear.\n\nThe paper shows that BIMP avoids Dirichlet-energy collapse and remains trainable at large depths, but it is not shown why this matters for downstream tasks. In the current benchmarks, deeper baselines typically do not fail catastrophically—they just plateau or degrade moderately. An illustrative failure case, where a strong baseline collapses due to oversmoothing and BIMP succeeds, would make the motivation more concrete.\n\n4. Novelty is mainly conceptual rather than architectural.\n\nAlthough the theoretical analogy is interesting, the empirical model resembles prior continuous-depth GNNs with a particular nonlinear choice and learned adjacency. The extent to which performance gains arise from the opinion-dynamics design rather than standard architectural components is not fully disentangled.\n\n5. Scope limited to oversmoothing, not broader depth-related limitations.\n\nOther known depth challenges—such as over-squashing, expressivity limits, and scaling to large graphs—are not addressed. Since oversmoothing is only one of several depth-related bottlenecks, the contribution feels narrow unless its relevance can be better contextualized."}, "questions": {"value": "1. Can the authors provide a concrete example where oversmoothing is the dominant failure mode in modern GNNs?\n2. How does BIMP perform on domains where depth is required (e.g., long-range molecular interaction graphs, knowledge graphs, large-scale recommender systems)?\n3. Is the nonlinear inductive bias still necessary when skip-connections, normalization, or positional encodings are added? An ablation isolating the source of benefit would be useful.\n4. How does BIMP compare against non-ODE architectures that already mitigate oversmoothing implicitly?\n4. Since nonlinear continuous-depth models are not the only way to address the scenarios tested in the paper, how are the runtime / memory comparisons against strong non-ODE baselines (e.g., GCN with residuals, GraphGPS, GATv2, or GNN-SSM), especially on larger graphs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KLFtsKwmVq", "forum": "e4SvuiVHcW", "replyto": "e4SvuiVHcW", "signatures": ["ICLR.cc/2026/Conference/Submission14024/Reviewer_hd9T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14024/Reviewer_hd9T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975725549, "cdate": 1761975725549, "tmdate": 1762924513805, "mdate": 1762924513805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}