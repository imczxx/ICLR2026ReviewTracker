{"id": "setRMvmjDt", "number": 13853, "cdate": 1758223738700, "mdate": 1759897408578, "content": {"title": "Provably Convergent Nonconvex Algorithm for Volume Optimization-based Latent Component Analyses", "abstract": "We present an algorithm that aims at solving a family of nonconvex optimization problem with convergence guarantee to a global optimum. This family of nonconvex optimization problems share the formulation of maximizing the volume of a matrix subject to linear constraints. Problems of this form have found a lot of applications in unsupervised learning and representation learning, especially if identifiability of the latent representation is important for the task. Specific examples based on the types of constraints include bounded component analysis, sparse component analysis (complete dictionary learning), nonnegative component analysis (nonnegative matrix factorization), and admixture component analysis, to name a few. Computationally, the problem is hard because of the nonconvex objective. An algorithm based on linearized ADMM is proposed for these problems. Although a similar algorithm has appeared in the literature, we note that a small modification has to be made in order to guarantee that the algorithm provably converges even for convex problems. Then we present the main contribution of this work that is convergence guarantee to a global optimum at a sublinear rate. We do assume some mild conditions on the initialization, but our numerical experiments indicate that these initialization conditions are very easy to satisfy.", "tldr": "We show that a family of nonconvex optimization problems related to unsupervised learning and representation learning can be solved by linearized ADMM with convergence guarantee.", "keywords": ["Convergence analysis; nonconvex optimization; matrix volume"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/063a8776a4851ebc3f9709d32f53c162dc3c4436.pdf", "supplementary_material": "/attachment/11097caf107caa3f6ee16209c7a15957214fb9dc.pdf"}, "replies": [{"content": {"summary": {"value": "This paper presents a unified optimization framework for several latent component analysis problems, backed by a novel L-ADMM algorithm with a claimed global convergence guarantee. The unification of these problems under a single volume-maximization principle is a valuable conceptual contribution. However, the validity of the central theoretical claim remains unverifiable due to the omission of a key proof, and the empirical validation is preliminary, lacking comparisons to established baselines and tests on real-world data. These issues currently preclude a full assessment of the method's practical impact."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper effectively unifies several distinct unsupervised learning models (BCA, SCA, NCA, ACA) under a single volume-optimization framework, offering a valuable high-level perspective.\n2. Tackling the global convergence of non-convex problems like complete dictionary learning is a challenging and impactful goal. The proposed L-ADMM variant presents a novel technical pathway."}, "weaknesses": {"value": "1. The global convergence guarantee (Theorem 3.1) relies entirely on Lemma 3.2. The proof of this foundational lemma is relegated to the supplement, making it impossible to assess the validity of the paper's central contribution.\n2. The theoretical guarantee depends on an initialization condition (Eq. 17) that requires knowledge of the unknown ground-truth solution S_♮, rendering it hard to use in practice. The claim that this condition is easily met with random initialization lacks any statistical evidence.\n3.  The empirical evidence is not enough. There are no comparisons to established baselines, validation is confined to idealized synthetic data, and there is no empirical scrutiny of the theoretical assumptions during iterations.\n4. The paper's organization could be improved. Critical flaws include: (a) the lack of a dedicated \"Related Work\" section, failing to contextualize the novelty against prior art; (b) a disjointed introduction with a misplaced technical subsection (1.1); and (c) redundant content. They obscure the paper's narrative and contributions."}, "questions": {"value": "1. The complete proof of Lemma 3.2 must be included in the main text to allow for verification of Theorem 3.1.\n2. Can a practical initialization scheme not relying on S_♮ be proposed? Please provide statistical evidence (e.g., success rate over thousands of random trials) for the claim that condition (17) is easily satisfied.\n3. It is essential to add comparisons to state-of-the-art baselines and include results on real-world datasets. An experiment monitoring the key theoretical inequality during iterations would be highly valuable.\n4. The paper must be restructured. A dedicated \"Related Work\" section is essential. The technical content of Section 1.1 should be moved to a new section, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ik9diJf4a1", "forum": "setRMvmjDt", "replyto": "setRMvmjDt", "signatures": ["ICLR.cc/2026/Conference/Submission13853/Reviewer_H3Ks"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13853/Reviewer_H3Ks"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635904262, "cdate": 1761635904262, "tmdate": 1762924375640, "mdate": 1762924375640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a linearized ADMM variant for volume optimization-based latent component analyses. A convergence result is provided in the convex setting and the same result is extended to a family of volume optimization-based problems, provided the initialization is inside some basin of attraction. Some experiments suggest that the method can converge to the global minimum."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The linearization around $A^\\dagger(c-Bs_t)$ is interesting and this choice is well-motivated by the given convergence proof in the convex setting.\n2. The problem of interest (latent component analyses) is important to the machine learning community."}, "weaknesses": {"value": "1. The initialization condition for convergence of the volume-based latent component analyses depends on the solution. This condition is claimed to be mild, yet I find the motivation lacking. What is missing is either (i) a thorough analysis of this initialization condition for some specific problems with known distributions of the solution or (ii) more convincing empirical evidence (which should also include the fraction of initializations that satisfied the condition) better showcasing how it behaves in practice.\n2. Related to the previous weakness, it is not discussed in the experiments how the initializations were chosen. If this is taken from the same distribution as the true underlying solution, it might explain why the condition is ‘easy’ to satisfy.\n3. The writing quality isn’t great and the figures are also unclear (see issues and questions below).\n4. It is claimed that (4) in the supplementary material is satisfied for $\\rho > |\\eta|$. This is clearly not true by Figure 2 in the supplementary.\n\nMinor issues/suggestions:\n1. The conditions given for the identifiability for each of the four cases feels out of place in the main text, since it is not really relevant to the L-ADMM algorithm, which should be the main focus of the text.\n2. Add dimensions to all matrices (at least in theorem statements) since it often matters whether they are square or rectangular. Also include the rank assumptions more explicitly since this is crucial for the properties of the pseudo inverse. For example line 354 requires $X$ to be full row rank, which I cannot find anywhere in the text.\n3. Plot averages and error bars in figures instead of all trajectories.\n4. Line 135: disk -> cell\n5. Line 259 & 270: should refer to (8), not (5)\n6. Line 308: = -> $\\leq$\n7. Line 317: orthogonizing -> orthogonalizing\n8. Line 345: “the gradient of we know the gradient of the”\n9. Line 382: a $\\dagger$ seems to be missing on $S_t^\\top$\n10. Line 393: is -> its\n11. Line 465: == -> =\n12. Line 220 in the supplementary: “Since we assume $S_t S^\\dagger_\\natural$, so is $(S_tS_\\natural^\\top)^\\top$” What is assumed?"}, "questions": {"value": "1. Provide more details for the simplification to obtain (16).\n2. The experiments show the optimality gap going to 1e-6, yet other methods seem to go to lower values. How do you then know that the found solutions are global optima?\n3. Regarding the figure in the supplementary material: why is time plotted on a log scale? why don’t all methods start at the same point? why does the optimality gap in (c) have a sudden jump upwards for the BCD method?\n4. Please rewrite the proof of Lemma 2, which is a crucial part of the claimed core contribution of the paper, yet difficult to read. I am currently not certain about the correctness of this result, especially in light of the fourth stated weakness.\n5. How are the least squares problems solved? Is the matrix prefactorized? If yes, is this time properly taken into account in the comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7kQHuzKgIa", "forum": "setRMvmjDt", "replyto": "setRMvmjDt", "signatures": ["ICLR.cc/2026/Conference/Submission13853/Reviewer_5sLf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13853/Reviewer_5sLf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948588863, "cdate": 1761948588863, "tmdate": 1762924375226, "mdate": 1762924375226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel algorithm for a family of nonconvex optimization problems common in unsupervised machine learning. The primary contribution is an algorithm based on Linearized Alternating Direction Method of Multipliers (L-ADMM) that is provably guaranteed to converge to a global optimum at a sublinear rate if the initialiaztion is good enough (lies in the basin of attraction), despite the problem's nonconvex and NP-hard nature."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core contribution of the paper mainlly focus on the convergence analysis of equation $(4)$. A key technical contribution is the specific variant of L-ADMM used. Standard L-ADMM linearizes the difficult part of the objective ($f(w)$) at the previous iterate ($w_t$). This paper's variant, however, linearizes $f$ at a different point ($A^\\dagger(c - Bs_t)$)."}, "weaknesses": {"value": "1) Though the convergence of the optimal Lagrangian function value is provided in theorem $2.1$, it is still unknown what is the convergence rate of the primal variable $w_\\star$. I am wondering if the auhors could indicate the corresponding rate.\n\n2) To the best of my knowledge, for general non-convex problem, ADMM like algorithm can only guarantee to converge to the stationary points. I am wondering if the authors could point out what is the key structures that helps to ensure the convergence to the global optimum in this case."}, "questions": {"value": "1) The authors state that the initialization requirement in Equation $(16)$ differs from those of other non-convex algorithms. This claim is unclear and requires further elaboration. The authors should be more precise about this distinction and clarify the significance of the 'gap' between the requirements presented in this paper and those of existing methods.\"\n\n2) In my understanding, the log-det term in the equation $1$ is used to orthogonalize the rows vectors of $W$ for which can be regarded as the penalty functions of constrains $ WW^T= I$. I am wondering what will the case be like if we replace the log-det term with something similar with $\\|WW^T-I\\|_F$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MzgAylZETn", "forum": "setRMvmjDt", "replyto": "setRMvmjDt", "signatures": ["ICLR.cc/2026/Conference/Submission13853/Reviewer_BFQE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13853/Reviewer_BFQE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973529840, "cdate": 1761973529840, "tmdate": 1762924374880, "mdate": 1762924374880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers a unified framework for a family of latent component analysis (LCA) problems under the lens of volume optimization.\nThe authors formulate a general nonconvex optimization problem of minimizing the negative log-determinant (to maximize the “volume” of the mixing matrix) plus a regularization term that encodes task-specific constraints. To address this problem, they propose a Linearized ADMM algorithm and provide theoretical guarantees. Experiments on synthetic and real datasets suggest that the proposed method performs comparably or better than baseline algorithms across several LCA formulations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper connects several LCA variants via a single “maximum volume” optimization formulation, revealing structural commonalities across bounded/sparse/nonnegative/admixture models. The geometric interpretation via log-det volume maximization and identifiability arguments provides some intuition.\n\n2. The linearized ADMM updates rely on standard matrix operations, making the algorithm easy to implement.\n\n3. Experiments demonstrate superior performance, suggesting that the proposed algorithm is effective in practice."}, "weaknesses": {"value": "1. The convergence results presented in Theorems 2.1 and 3.1 are neither practical nor meaningful, as they rely on the assumption that the optimal dual multiplier $\\lambda_*$ is known.\n\n2. The ADMM and linearized ADMM algorithms discussed in this paper are not directly applicable to the LCA optimization models considered here, since they assume the smooth function $f(\\cdot)$ is convex, whereas the smooth term $-\\tfrac{1}{2}\\log\\det(WW^{\\top})$ is non-convex.\n\n3. The optimization problem in (1) is, in general, NP-hard. I remain unconvinced that this problem can be solved with guaranteed convergence to a global optimum."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "og0Rf9UD5u", "forum": "setRMvmjDt", "replyto": "setRMvmjDt", "signatures": ["ICLR.cc/2026/Conference/Submission13853/Reviewer_KMXJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13853/Reviewer_KMXJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999569121, "cdate": 1761999569121, "tmdate": 1762924374261, "mdate": 1762924374261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}