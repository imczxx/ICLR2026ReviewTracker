{"id": "Ksxvz8XLVi", "number": 6931, "cdate": 1758002486745, "mdate": 1759897883312, "content": {"title": "Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling", "abstract": "Reward models are essential for aligning Large Language Models (LLMs) with human values, yet their development is hampered by costly preference datasets and poor interpretability. While recent rubric-based approaches offer transparency, they often lack systematic quality control and optimization, creating a trade-off between scalability and reliability. We address these limitations with a novel, training-free framework built on a key assumption: evaluation rubrics underlying human preferences exhibit significant generalization ability across diverse queries, a property that enables remarkable data efficiency. Our two-stage approach first infers high-quality, query-specific rubrics using a validation-guided Propose-Evaluate-Revise pipeline. Second, it generalizes these granular rubrics into a compact, non-redundant core set by maximizing an information-theoretic coding rate. The final output is an interpretable, hierarchical “Theme-Tips” rubric set. Extensive experiments demonstrate the framework's exceptional data efficiency and performance. Critically, using just 70 preference pairs (1.5\\% of the source data), our method also empowers smaller models like Qwen3-8B to outperform specialized, fully-trained counterparts. This work pioneers a scalable, interpretable, and data-efficient path for reward modeling. Related code and data are available at https://anonymous.4open.science/r/Auto-Rubric-9219/.", "tldr": "", "keywords": ["Reward Modeling", "Rubric Extraction", "Interpretability", "Data Efficiency"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a13ad23ae94c01d46805c6d6f6ca30437f74ba4a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Auto-Rubric, a training-free framework that learns an appropriate rubric \"prompt\" for downstream tasks. First, the framework performs a query-specific rubric generation using an iterative \"Propose-Evaluate-Revise\" pipeline based on a small number of preference pairs. Since this first step introduces a lot of redundancy, the framework further prunes this large pool greedily using an information-theoretic selection algorithm with a final rubric set from only 70 preference pairs (1.5% of the source data). The framework was evaluated using Qwen3-8B, Qwen3-14B, Qwen3-32B, and Qwen3-235B-Instruct-2507 on RewardBench, RewardBench2, JudgeBench, and RM-Bench."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**S1.** The paper introduces a training-free framework, which provides a scalable and accessible alternative to traditional reward modeling by achieving remarkable data efficiency.\n\n**S2.** The framework is evaluated across a wide range of models and four benchmarks, supported by detailed ablation studies.\n\n**S3.** The paper is easy to follow with a clear motivation."}, "weaknesses": {"value": "**W1.** Contrary to the paper's claim, the \"human interpretability\" aspect is debatable. I believe that the framework does not optimize for human-preferred rubrics (or at least directly), but rather it optimizes for prompts that maximize discriminative accuracy on a small seed set. The resulting rubrics are less a general evaluation framework and more a set of highly-tuned prompts. Take 1 rubric that is performing the best according to the metric of coverage, precision, and contribution in accuracy in Table 3:\n\n\n```\n## Theme 3: Clarity and Structured Organization\n\nTheme: Prioritize clarity, conciseness, and structured organization to enhance readability and directness.\n• Tip 1: For a ”Thank you” prompt, respond with a concise acknowledgment and an open invitation for further questions, avoiding assumptions about the user being a student or lawyer.\n• Tip 2: When summarizing steps for building a dropshipping agent business, use bullet points or numbered lists to present key points logically and avoid hallucinated information.\n• Tip 3: In audit findings related to deposit insurance boards, structure responses with precise, actionable items and conclude with a concise summary emphasizing implications.\n• Tip 4: Avoid excessive formatting like bold text or unnecessary punctuation when explaining grammatical correctness, maintaining a straightforward and professional tone.\n```\n\nPhrases such as \"When summarizing steps for building a dropshipping agent business,\" \" In audit findings related to deposit insurance boards,\" etc., are clear overfitting in terms of the prompt that I doubt represents human interpretability of what the rubric should look like for any of the benchmarks. Strangely, why do these instance-specific, overfitted rubrics perform so well on unseen data?\n\n**W2.** Given that this paper is for a \"reward modeling\" framework, there is a lack of evidence that these rubrics can be used to successfully post-train a policy model (RLHF or DPO). High accuracy on static, pairwise-preference only benchmarks does not guarantee better RL training signals.\n\n**W3.** The evaluation methodology lacks some details and is unfair to past baselines. The paper uses different voting strategies for different benchmarks (e.g., voting@10 for RewardBench2, voting@5 for RewardBench, voting@1 for RM-Bench), which would be an unfair comparison to baselines that report voting@1, which I believe the authors should clarify.\n\nFurthermore, the performance gains could be partially attributed to this test-time ensembling rather than the rubrics themselves.\n\nIn addition, there is no information about the experimental setup on the machine and the environment being used, so I wonder if the improvement is simply from different configuration setups. Finally, there is no statistically significant study as well.\n\n**W4.**  Given that the method is a training-free approach that effectively optimizes prompts for an \"LLM-as-a-Judge,\" the set of baselines is incomplete. The paper should compare its framework against other relevant training-free prompt optimization methods that are also used to improve the performance of LLM judges.\n\n**W5.** The claim of \"generalization\" is weak. The rubrics are extracted from general-domain helpfulness datasets (HelpSteer3-General, UltraFeedback) and then evaluated on benchmarks that also primarily measure general-domain helpfulness. This appears to be more a test of in-distribution robustness rather than true generalization. I believe a proper generalization test would involve extracting rubrics from one domain (e.g., helpfulness) and evaluating their ability to judge a completely different domain (e.g., creative writing, code quality, or humor)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qrcc9hjFw0", "forum": "Ksxvz8XLVi", "replyto": "Ksxvz8XLVi", "signatures": ["ICLR.cc/2026/Conference/Submission6931/Reviewer_tuBg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6931/Reviewer_tuBg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655941663, "cdate": 1761655941663, "tmdate": 1762919165138, "mdate": 1762919165138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Reward models are hampered by expensive preference dataset and poor interpretability. The alternative of using rubrics can offer transparency but suffers from quality control considerations. The paper proposes a method of generating high quality query-specific rubrics through a propose-evaluate and revise pipeline. It then generalizes prompt-specific rubrics into a compact core set."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe approach designed to generate prompt specific rubrics and aggregate them into prompt agnostic rubrics is reasonable.\n2.\tThe choice of evaluation benchmarks are diverse and suitable.\n3.\tThe performance of the models on the chosen benchmarks are strong with insightful analysis and ablations done in the paper and the accompanying appendix."}, "weaknesses": {"value": "1.\tThe papers missed substantial discussion on existing work on rubric based reward modelling including DeepSeek-GRM [1], RewardAnything [2] and LMUnit [3] which were released more than 3 months prior to ICLR deadline. These works are important to situate this work’s contribution in. Furthermore, approaches such as the Propose-Evaluate-Revise pipeline/Rubric Aggregation can be discussed in relation to similar approaches [1], [2], [3] and [4]. It does not seem like the paper authors were aware of these works and without further clarification, I believe that the novelty of the approach does not seem clear. Adjacent works on rubrics might also be worthwhile to discuss in relation to this work (e.g. HealthBench [5], PaperBench [6], Rule-based-Rewards [7]).\n2.\tThe chosen baseline model from Skywork-Reward-V2 paper is not the strongest in the series (see their Llama-8B based models), which can potentially mislead readers into thinking the models in the paper is substantially stronger than alternatives. Also in Table 1, reporting averages for models that are missing certain benchmarks might not be fair comparison.\n3.\tThe improvement relative to the base models of the same size in Table 1 is limited (e.g. < 2 points on average). This suggest that much of the gains come from using a stronger base model. Looking at the rubrics in Appendix G.1, I’m not confident that the extracted rubrics are actually “rubrics” instead of merely summarized examples (see Rubrics in Theme 1 discussing specific topics like Hogwarts or Sumerian texts). \n\n[1] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling, 2025. URL https://arxiv.org/ abs/2504.02495.\n\n[2] Zhuohao Yu, Jiali Zeng, Weizheng Gu, Yidong Wang, Jindong Wang, Fandong Meng, Jie Zhou, Yue Zhang, Shikun Zhang, and Wei Ye. Rewardanything: Generalizable principle-following reward models, 2025. URL https://arxiv.org/abs/2506.03637.\n\n[3] Jon Saad-Falcon, Rajan Vivek, William Berrios, Nandita Shankar Naik, Matija Franklin, Bertie Vidgen, Amanpreet Singh, Douwe Kiela, and Shikib Mehri. Lmunit: Fine-grained evaluation with natural language unit tests, 2024. URL https://arxiv.org/abs/2412.13091.\n\n[4] Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Daniel Egert, Ellie Evans, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev. HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks URL https://arxiv.org/abs/2503.04378 \n\n[5] Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quinonero-Candela, ˜ Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, Johannes Heidecke, and Karan Singhal. Healthbench: Evaluating large language models towards improved human health, 2025. URL https://arxiv.org/abs/2505.08775.\n\n[6] Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. Paperbench: Evaluating ai’s ability to replicate ai research, 2025. URL https://arxiv.org/abs/2504.01848.\n\n[7] Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian Kivlichan, Molly Lin, Alex Beutel, John Schulman, Lilian Weng. Rule Based Rewards for Language Model Safety. 2024. URL https://arxiv.org/abs/2411.01111."}, "questions": {"value": "1.\tIt’s not clear how evaluation is done. How are rubrics chosen for each benchmark or do they all use a fixed set of rubrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uwYcaaMY6M", "forum": "Ksxvz8XLVi", "replyto": "Ksxvz8XLVi", "signatures": ["ICLR.cc/2026/Conference/Submission6931/Reviewer_d1UR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6931/Reviewer_d1UR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782530090, "cdate": 1761782530090, "tmdate": 1762919164619, "mdate": 1762919164619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reward model based on rubrics generated from pairwise human feedback data. Their method uses two stages, first creating query-specific rubrics, and then in the next stage aggregating the resulting rubrics to create overall relevant rubrics. The authors test their method on a number of popular reward model/llm-as-a-Judge benchmarks, such as RewardBench (2), RM-Bench, and Judge-Bench."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Information theoretic framing is novel as far as I am aware (and goes beyond), specifically\n2. Diverse experiments across broad set of suitable LLM-as-a-Judge benchmarks (RewardBench (2), RM-Bench, and Judge-Bench)\n3. Overall the paper is well written and structured."}, "weaknesses": {"value": "1. **Lack of diverse LLM-as-a-Judge baselines:** the paper does not appear to compare multiple different LLM-as-a-Judge configurations even though it is known that different configurations with the same model can have a huge effect. This weakness means the LLM-as-a-Judge baseline may be less capable than it could be, and affects the strong claims regarding performance relative to baselines.\n2. **Single-seed results:** the experiments appear to be based on a single sample. With the known variance of LLM judgements, the results would be significantly more robust if more than one seed result would be reported (including variance/error bars!). This more rigorous analysis is especially important as the claimed improvements are relatively small.\n3. **Missing prior work:** work around Inverse Constitutional AI [1,2,3] also extracts \"principles\" from pairwise human feedback seemingly equivalent to the rubrics introduced in this paper, and also uses it similar to reward model as LLM-as-a-Judge instructions. This line of work is not mentioned in the current draft, but very related. Would be interesting to have a discussion/comparison.\n\nOverall I am most concerned by the first two points: the results as they are presented currently are, in my opinion, inconclusive without more rigorous evaluation in terms of diverse baseline prompts and multi-seed statistics.\n\nMinor:\n1. Quite a few times space missing between citations and preceding text e.g. in L43: \"large-scale crowd annotations(Bai et al., 2022)\". I recommend adding spaces where missing.\n2. Figure 2 is difficult to read, text is too small\n\n- [1] Henneking, Carl-Leander, and Claas Beger. \"Decoding Human Preferences in Alignment: An Improved Approach to Inverse Constitutional AI.\" _arXiv preprint arXiv:2501.17112_ (2025).\n- [2] Findeis, Arduin, et al. \"Inverse Constitutional AI: Compressing Preferences into Principles.\" _The Thirteenth International Conference on Learning Representations_ (2025).\n- [3] An, Esther. \"Towards Principled AI Alignment: An Evaluation and Augmentation of Inverse Constitutional AI.\" (2025)."}, "questions": {"value": "1. Would you be able to clarify the baseline LLM-as-a-Judge configuration used, and how that configuration was selected? I did not see relevant information in the paper.\n\t1. Related: Would you be able to test additional LLM-as-a-Judge configurations, in particular I would be interested in an ArenaHard baseline [4].\n2. Would you be able to clarify your contribution relative to the missing related work?\n3. I don't understand the term \"Theme-Tips\" that is used across the paper, what is this term supposed to mean?\n\n[4] https://github.com/lmarena/arena-hard-auto/blob/196f6b826783b3da7310e361a805fa36f0be83f3/utils/judge_utils.py#L1"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iuqcU2gaMA", "forum": "Ksxvz8XLVi", "replyto": "Ksxvz8XLVi", "signatures": ["ICLR.cc/2026/Conference/Submission6931/Reviewer_kdeU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6931/Reviewer_kdeU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822849056, "cdate": 1761822849056, "tmdate": 1762919164238, "mdate": 1762919164238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel training-free framework for building interpretable and data-efficient reward models (RMs). It is motivated by the limitations of existing rubric-based and preference-supervised methods, which suffer from high annotation cost, low interpretability, and scalability issues. The key idea is that evaluation rubrics underlying human preferences possess generalization ability across queries, allowing for efficient reuse and compression.\n\nThe proposed two-stage method first generates query-specific rubrics through a validation-guided Propose–Evaluate–Revise pipeline, then generalizes them into a compact, non-redundant “Theme–Tips” rubric set using information-theoretic coding-rate optimization. The approach produces interpretable, hierarchical rubrics without additional training.\n\nEmpirically, the method achieves strong data efficiency, using only 70 preference pairs (1.5% of the data), while enabling smaller models (e.g., Qwen3-8B) to outperform fully trained, specialized reward models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty of Data-efficient, training-free framework: The proposed two-stage Propose–Evaluate–Revise and information-theoretic selection approach achieves state-of-the-art performance using only a small fraction of preference data.\n\n2. Interpretable and open-source rubrics: The release of query-agnostic rubric datasets promotes transparency and facilitates further research on interpretable alignment.\n\n3. Rubric analysis framework: This is interesting and helpful to get a deeper dive into the rubric utility based on coverage, precision and contribution of individual rubrics.  In general, the analysis in the main paper and appendix are interesting.\n\n4. Results: Demonstrates consistent gains across multiple reward modeling benchmarks, with rubric-enhanced models (e.g., Qwen3-235B, Qwen3-8B) outperforming several fully-trained counterparts."}, "weaknesses": {"value": "Two Core Weaknesses:\n\n1. Limited generalizability across domains: The extracted “Theme–Tips” rubrics from HelpSteer2 and UltraFeedback appear highly similar at the theme level, differing mainly in tip-level nuances. Since both datasets focus on conversational and chat-based skills, it is unclear whether the proposed method would generalize to domains such as math, code, or science. Including experiments on at least one or two non-conversational domains would significantly strengthen the paper’s claims.\n\n2. Clarity and presentation issues: Some sections of the paper lack sufficient clarity, making it difficult to follow the setup, experimental details, and analyses. Specific questions and points of confusion are noted in the following section.\n\nIf the authors can clarify these aspects and extend evaluation to other domain datasets, I would consider increasing my overall score."}, "questions": {"value": "Clarification questions:\n1. Theme tips: Pg 5 second last paragraphs \" Finally, the selected core set is structured into our interpretable “Theme-Tips” hierarchy by a structuring LLM\", what is the structuring LLM ? How is it achieved ?\n2. Information gain is negative for higher batch numbers in Fig 3b, while IG should always be non-negative, can you explain why ?\n3. Training algorithm: can you explain how is it different from regular gradient descent on a set of parameters, pg 7 section 4.3 provides some idea but it is unclear how batch, and epoch work in this case and how it relates to requiring only 70 samples to get the final rubrics"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uRH5lKcIsb", "forum": "Ksxvz8XLVi", "replyto": "Ksxvz8XLVi", "signatures": ["ICLR.cc/2026/Conference/Submission6931/Reviewer_HqJg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6931/Reviewer_HqJg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762127997147, "cdate": 1762127997147, "tmdate": 1762919163857, "mdate": 1762919163857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}