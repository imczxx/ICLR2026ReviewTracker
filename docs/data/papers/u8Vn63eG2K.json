{"id": "u8Vn63eG2K", "number": 20107, "cdate": 1758302556445, "mdate": 1763648964559, "content": {"title": "A Sensitivity Analysis of State-Space Models on Graphs", "abstract": "The recent success of State-Space Models (SSMs) in sequence modeling has inspired their extension to graphs, giving rise to Graph State-Space Models (GSSMs). While effective, existing approaches often rely on sequentializations or spectral decompositions that lack permutation equivariance, message-passing compatibility, and computational efficiency. Moreover, they typically target either static or temporal graphs in isolation and, crucially, provide only loose or qualitative results on information propagation, offering no exact guarantees on challenges such as vanishing gradients and over-squashing.\nIn this work, we revisit the design of GSSMs through the lens of sensitivity analysis. We introduce a principled integration of modern SSM computation into the Message-Passing Neural Network framework, yielding a unified architecture that is computationally efficient, permutation equivariant, and supports fast parallelism. Our formulation admits closed-form Jacobian computations, enabling an exact sensitivity analysis of node-to-node dependencies and rigorous lower bounds on information flow, contrasting sharply with prior heuristic approaches. These theoretical insights clarify when and how stable long-range propagation can be achieved. Finally, we validate our model across a wide range of benchmarks, including node classification, graph property prediction, long-range reasoning, and spatiotemporal forecasting, where it achieves strong empirical performance while preserving the simplicity of message passing.", "tldr": "We revisit GSSMs via sensitivity analysis. We integrate modern SSMs into message passing, yielding a parallel model with Jacobians for exact flow bounds, enabling stable long-range propagation and strong results on static and temporal graphs.", "keywords": ["graph neural networks", "state space models"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6273b767282be786e070c15eae80b98dd60ceb2d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes MP-SSM, a graph model that embeds a linear, tied-weight state-space recurrence inside message passing and applies an MLP afterwards. This yields exact expressions for Jacobian sensitivity analysis, which enables lower bounds on information flow (vanishing gradients / over-squashing) and a parallel implementation (via the closed-form multilayer propagation). The model is designed to jointly handle static and temporal graphs and the authors report competitive results on synthetic graph property prediction, the standard heterophilic benchmarks, and spatio-temporal forecasting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The submission is generally well-written and easy to understand. The design (linear diffusion followed by MLP) is simple, but empirical results compared to many baselines such as MPNNs, Spectral/polynomial filter GNNs, and graph SSMs are strong.\n- The method admits a very clean and simple exact Jacobian sensitivity analysis."}, "weaknesses": {"value": "- It seems to me that the architectural novelty is rather modest. The SSM block is, essentially, a linear multi-hop graph filter with weight sharing, which is very similar to classic GNNs with, e.g., polynomial filters (such as ChebNet or SGC).\n- Importantly, this analogy carries over to most of sec. 3, i.e. the theoretical results are almost exclusively driven by the linearity, *not* the specific design of MP-SSM. Throughout sec. 3, the input term $U_{t+1}B$ does not contribute to the sensitivity, so essentially the same reasoning would apply to any linear GNN/graph filter, modulo the particular filter $p(A)$ (and without weight sharing one would simply obtain a product of the individual weight matrices, or just a single one). While I did not find any works that state the exact Jacobian as directly as here, the derivation is very straightforward. Presenting it as here (esp. together with the lower bounds from Thm. 3.11) is, in my view, a legitimate but rather modest contribution.\n- Specifically, the $2^{-k/2}$ separation in Thm. 3.13 between GCNs and MP-SSM is merely an artifact of removing per-hop nonlinearities and *not* due to the design of MP-SSM. Essentially, this gap hinges on modeling ReLU as an *unscaled* $\\mathrm{Bernoulli}(1/2)$ gate. However, standard practice in most of deep learning is to use variance-preserving initialization/normalization, in which case the post-ReLU activations are multiplied by the *gain* of $\\sqrt{2}$ in every layer. See [1] or the [source code of graphgym](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/graphgym/init.html) (`calculate_gain('relu')`).\n- Similarly, the advertised k-hop parallelization is, again, just the linear-filter trick: it removes depth-wise sequentiality but inflates the activation footprint by the number of “layers”, and, being an artifact of linearity, applies equally to any polynomial/linear GNN, not uniquely to MP-SSM.\n\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.* ICCV 2015"}, "questions": {"value": "- Can you refer to the points from “Weaknesses”?\n- Do you have a concrete (maybe more mechanistic) explanation for the outperformance of MP-SSM on the benchmarks? i.e. is it more linearization of the diffusion, weight sharing that lets you use a larger effective depth within the same parameter budget (e.g. for the LRGB), or sth else that is model specific?\n\nMiscellaneous:\n- line 66: typo \"analisys\"\n- line 106: typo \"of of\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jFsc2XzDEm", "forum": "u8Vn63eG2K", "replyto": "u8Vn63eG2K", "signatures": ["ICLR.cc/2026/Conference/Submission20107/Reviewer_Ngbo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20107/Reviewer_Ngbo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650695313, "cdate": 1761650695313, "tmdate": 1762933003172, "mdate": 1762933003172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Message-Passing State-Space Model (MP-SSM), a novel framework that integrates modern State-Space Models (SSMs) with the Message-Passing Neural Network (MPNN) paradigm. The primary goal is to address the well-known limitations of traditional MPNNs in modeling long-range dependencies, specifically tackling issues like over-squashing and vanishing gradients."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Principled Theoretical Foundation:** The paper is built on a solid theoretical foundation. The use of sensitivity analysis to *guide* the model's design, rather than just analyze it post-hoc, is a strong, principled approach.\n2. **Elegant and Efficient Design:** The core architectural choice—a linear recurrence for graph diffusion (Eq. 2) with nonlinearity isolated to a final, shared MLP (Eq. 4)—is elegant. This design is directly responsible for both the model's theoretical tractability and its potential for a highly efficient parallel implementation (Appendix B).\n3. **Strong Empirical Validation:** The model's effectiveness is demonstrated across a wide and diverse range of tasks (graph property prediction, heterophily, spatiotemporal forecasting). The fact that it achieves competitive or state-of-the-art results (e.g., Tables 1, 3, 4) while retaining the simplicity and efficiency of message passing is impressive."}, "weaknesses": {"value": "1.  **Missing Related Work:** The paper does not discuss or compare with work [1]. [1] shares similar motivation: applying structured state-space models (like S4) to graphs to overcome the \"bottleneck\" (i.e., over-squashing) of traditional MPNNs. [1] also prominently features a *sensitivity analysis* to motivate its design and claims theoretical guarantees on information flow. More importantly, [1] does not break the permutation invariance. This overlap makes it difficult to assess the precise novelty of this paper's core theoretical contributions without a direct comparison.\n\n---\n\n[1] Breaking the Bottleneck on Graphs with Structured State Spaces, CIKM 2024"}, "questions": {"value": "See the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SGmF39b1pd", "forum": "u8Vn63eG2K", "replyto": "u8Vn63eG2K", "signatures": ["ICLR.cc/2026/Conference/Submission20107/Reviewer_erdw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20107/Reviewer_erdw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810569894, "cdate": 1761810569894, "tmdate": 1762933002559, "mdate": 1762933002559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies Message Passing State-Space Models (MP-SSMs) for temporal graphs in the following form: $X_{t+1}=AX_tW+U_{t+1}B$, where $A$ is the adjacency matrix, $X_{t+1}$ is the node hidden representations at time $t+1$ and $U_{t+1}$ is the input node features at time $t+1$. For static graphs, the authors let $U_{t+1}\\equiv U_1$. \n\nThe paper provides a detailed sensitivity analysis, specifically the spatio-temporal correlation $\\partial \\[X_{t}\\]\\_i/\\partial \\[X_{t^\\{\\prime\\}}\\]\\_j$, that quantifies the long-range dependencies chracterized by graph structure.Notably, Theorem 3.13 shows that MP-SSMs maintain temporal correlations over longer ranges than standard GCNs, whose correlations decay more rapidly. On static graphs, time refers to model depth and this implies that MP-SSMs preserve meaningful correlations even as model depth increases. Empirically, the model performs competitively across spatio-temporal and static graph benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The formualtion of MP-SSMs is conceptually clean.\n- The paper is well motivated, well-written, and easy to follow.\n- The experimental results on static and temporal graph benchmarks show consistent improvements over baselines."}, "weaknesses": {"value": "- One key motivation for incorporating state-space models (SSMs) is to better capture long-range dependencies. In the context of static graphs, this typically refers to spatial dependencies, for example Dwivedi et al., “Long Range Graph Benchmark.” However, due to the inherently localized nature of message passing, the proposed MP-SSM still exhibits a limited receptive field and may therefore struggle to fully capture long-range spatial correlations across distant nodes.\n- The authors may elaborate on their claim that “MP-SSM generalizes MPNNs” (Line 173). Under the static graph setting, the MP-SSM seems to reduce to applying multiple message-passing steps without intermediate nonlinearities, which can be viewed as a subset of standard MPNNs. In fact, this is then equivalent to a single-layer spectral GNN implementing a polynomial filter on the graph operator. From this perspective, MP-SSMs appear less expressive than general MPNNs."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZnBseqCN0z", "forum": "u8Vn63eG2K", "replyto": "u8Vn63eG2K", "signatures": ["ICLR.cc/2026/Conference/Submission20107/Reviewer_zRgR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20107/Reviewer_zRgR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886781164, "cdate": 1761886781164, "tmdate": 1762933001914, "mdate": 1762933001914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits Graph State-Space Models (GSSMs) by integrating modern State-Space Models (SSMs) into Message-Passing Neural Networks (MPNNs) via sensitivity analysis, addressing critical limitations of existing graph learning methods: vanishing gradients, over-squashing, lack of permutation equivariance, and disjoint handling of static/temporal graphs. The core proposal is the Message-Passing State-Space Model (MP-SSM). Empirically, MP-SSM is validated across 15 benchmarks: it outperforms SOTA baselines on graph property prediction (diameter, SSSP, eccentricity), heterophilic node classification (e.g., Roman-empire, Amazon-ratings), and spatiotemporal forecasting (e.g., Metr-LA, PeMS-Bay), while matching the simplicity of MPNNs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1, The sensitivity analysis is novel, moving beyond heuristic combinations of GNNs and SSMs. \n\n2, Theoretical results are mathematically sound and experiments are comprehensive—covering both synthetic (e.g., shortest-path prediction) and real-world benchmarks.\n\n3, By addressing vanishing gradients/over-squashing and unifying static/temporal graph modeling, MP-SSM advances the practicality of GSSMs. Its efficiency and simplicity make it a viable alternative to complex models like graph transformers."}, "weaknesses": {"value": "1, More related papers e.g. https://openreview.net/pdf?id=0Z6lN4GYrO should be discussed.\n\n2, The parallel closed-form solution requires storing a tensor of shape (num_steps, n, hidden_dim), which may limit scalability for very large num_steps (e.g., long temporal sequences) or high hidden_dim on memory-constrained GPUs. The paper mentions this tradeoff but does not explore mitigation strategies (e.g., chunked computation for large num_steps).\n\n3, Testing MP-SSM with other GSOs (e.g., random walk, PageRank) would strengthen the claim of generality and reveal how GSO properties (e.g., spectral radius) interact with sensitivity bounds."}, "questions": {"value": "1, Have you explored memory-efficient variants (e.g., chunked unrolling for large num_steps) to mitigate GPU memory constraints? If so, what were the tradeoffs between runtime and memory usage?\n\n2, The paper claims compatibility with any GSO, but experiments use only the symmetric normalized adjacency. Could you provide results (even on a subset of benchmarks) with other GSOs (e.g., random walk, PageRank) to validate generality?\n\n3, Did you ablate the MLP’s depth/activation function?\n\n4, How would you extend MP-SSM to handle dynamic topologies ? \n\n5, For the heterophilic benchmarks (Section 4.2), MP-SSM outperforms models tailored for heterophily (e.g., FAGCN, H2GCN). Could you elaborate on why MP-SSM excels here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T83SLaqXA7", "forum": "u8Vn63eG2K", "replyto": "u8Vn63eG2K", "signatures": ["ICLR.cc/2026/Conference/Submission20107/Reviewer_9CJ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20107/Reviewer_9CJ1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930369812, "cdate": 1761930369812, "tmdate": 1762933001393, "mdate": 1762933001393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}