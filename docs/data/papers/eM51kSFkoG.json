{"id": "eM51kSFkoG", "number": 21406, "cdate": 1758317217985, "mdate": 1759896923632, "content": {"title": "Batch Speculative Decoding Done Right", "abstract": "Speculative decoding speeds up LLM inference by using a small draft model to propose multiple tokens that a target model verifies in parallel. Extending this idea to batches is essential for production serving, but it introduces the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, breaking right-alignment and corrupting position IDs, attention masks, and KV-cache state. We show that several existing batch implementations violate output equivalence—the fundamental requirement that speculative decoding must produce identical token sequences to standard autoregressive generation. These violations occur precisely due to improper handling of the ragged tensor problem. In response, we (1) characterize the synchronization requirements that guarantee correctness, (2)  present a correctness-first batch speculative decoding \\oursb that exposes realignment as consuming 40\\% of overhead,  and (3) introduce \\oursx, which maintains a sliding pool of sequences and dynamically forms same-length groups, to reduce the realignment overhead while preserving per-sequence speculative speedups. On SpecBench dataset, across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B pairs, our approach achieves up to 3× throughput improvement at batch size 8 compared to batch size 1, with efficient scaling through batch size 8, while maintaining 95\\% output equivalence. Our method requires no custom kernels and integrates cleanly with existing inference stacks.", "tldr": "Correct batch speculative decoding = proper position/mask/KV-cache synchronization for ragged tensors + cross-batch scheduling to eliminate wasted overhead.", "keywords": ["speculative decoding", "batch speculative decoding", "llm inference"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c146854abff466edc531f707b9f9fd646814e687.pdf", "supplementary_material": "/attachment/d6d2fbbe920627c1ff014c18d560fe6054908915.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes EQSpec and EXSpec, focusing on the correctness of batch speculative decoding. The paper claims that existing speculative decoding systems fail to preserve output correctness (aligned to the original base model) when batch size > 1, due to corrupted position IDs, attention masks, and KV cache state. To solve this, it proposes a group-and-padding batch scheduling algorithm to correctly synchronizing among the ragged tensors in a batch. Experimental results show that EXSpec can largely preserve the correctness (over 90%) while other systems can merely do so."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The topic is highly related to practical LLM usage. Batch speculative decoding is an important direction, yet there has been few existing works focusing on the correctness, mostly on speed performances.\n\nThe group-then-padding algorithm is practically useful. It can mitigate the length misalignment in an efficient way. Specifically, as stated in Section2, it does not involve modifying position IDs, avoiding re-implementing a whole new kernel, and also preserve the accepted tokens from being cropped.\n\nThe demonstration figures (Fig.3 and 4) are quite informative, making the main design easily understood."}, "weaknesses": {"value": "Main concerns:\n\nAs the paper stated, the problem of current inference systems is about incorrect output, which is caused by, alleged, KV-cache and position-ID errors. I think the root causes should be more specified and quantified. Is it because current systems have not implemented batch SD supports, or the implementation is incorrect, or just float precision is not accurate enough? Specifically, vLLM can achieve high match accuracy on Vicuna, but lower on other models. If the cause is about missing or incorrect implementation, I think the results would be uniformly low. If the authors provide detailed implementation or code examples, this concern will be clarified.\n\nThe paper claims that the output will be corrupted at batch size > 1, but Table 1 shows that BSP and DSD also have significantly low accuracy when batch size=1. That is a misalignment between the claim and evidence. Furthermore, this result also indicates that the problem is not about batch size, but other factors, while the optimization method largely targets at batch size > 1.\n\nMinor concerns:\n\nThe baseline of token throughput in Fig.5(a) is ‘no speculation’. ExSpec only achieves marginal acceleration compared to the original auto-regressive decoding, which is slow. As a speculative method, it should be compared to other speculative-decoding baselines for token throughput.\n\nThe claim that ‘speculative decoding needs to yield identical output’ can be more accurate: it is only true when temperature=0, while for temperature>0 the output of base model is a distribution and the output tokens are sampled from this distribution, so there is basically no ‘correct output’ but only a distribution.\n\nThe introduction and experiment analysis are hard to read. Improvement on writing would be beneficial."}, "questions": {"value": "1. Could you provide details about the cause of corrupted output of existing systems , e.g. missing or incorrect implementation, to further clarify the cause of corrupted outputs?\n2. Does the incorrectness also exist when batch size = 1? If so, how does the proposed method address this issue, given that the modification is only about batch scheduling?\n3. How does the methods perform compared to speculation-based baselines in terms of inference speed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HTD2vd3AXf", "forum": "eM51kSFkoG", "replyto": "eM51kSFkoG", "signatures": ["ICLR.cc/2026/Conference/Submission21406/Reviewer_RaXj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21406/Reviewer_RaXj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830909117, "cdate": 1761830909117, "tmdate": 1762941751115, "mdate": 1762941751115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a correctness-first batch speculative decoding EQSPEC and EXSPEC to accelerate batch speculative decoding. The method is validated on SpecBench dataset, across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B pairs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a detailed analysis of the existing problems in batch speculative decoding and proposes direct solutions to the most critical issues. For example, the EQSPEC design introduces the *unpad–repad* strategy to ensure correctness, while EXSPEC employs a *dynamic scheduling mechanism* to improve efficiency.\n- The paper quantitatively analyzes the actual cost composition and speedup factors in batch speculative decoding, offering an in-depth breakdown of various cost sources and their respective impacts on overall performance.\n- The experiments comprehensively compare the proposed methods with multiple existing batch speculative decoding approaches, and further integrate them into system-level frameworks such as vLLM and SGLang. The paper also provides unique insights into the results of current methods and potential directions for future improvements."}, "weaknesses": {"value": "- The models used for validation in this paper, such as Vicuna and GLM, are relatively outdated and small in scale. Since speculative decoding provides limited acceleration benefits for smaller models, the effectiveness and impact of the proposed methods may be somewhat diminished.\n- The paper does not introduce substantial optimizations for KV cache management. Its realignment process is implemented by re-concatenating a rank-4 KV tensor, which imposes significant memory overhead. In contrast, modern systems such as vLLM and SGLang include specialized optimizations for KV cache handling that could potentially improve efficiency.\n- The proposed methods are primarily designed for offline batch inference, where the distribution of sequence lengths is relatively uniform. However, in real-world speculative decoding scenarios, task lengths often vary widely. Such heterogeneity may cause a noticeable drop in EXSPEC’s grouping success rate and overall throughput performance."}, "questions": {"value": "- How does the proposed method perform on larger-scale LLMs and SOTA LLMs? Testing on more powerful models would strengthen the paper’s practical relevance and applicability.\n- Is it possible to incorporate more advanced scheduling strategies to further improve EXSPEC’s grouping success rate and overall throughput?\n- Since speculative decoding is primarily adopted in online serving environments by major LLM providers, the authors could consider applying their methods in more realistic inference scenarios to better demonstrate their real-world effectiveness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JYB4xqK1u6", "forum": "eM51kSFkoG", "replyto": "eM51kSFkoG", "signatures": ["ICLR.cc/2026/Conference/Submission21406/Reviewer_ajDR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21406/Reviewer_ajDR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906671423, "cdate": 1761906671423, "tmdate": 1762941750526, "mdate": 1762941750526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the core challenge of scaling batch speculative decoding for production use: the disruption of output equivalence. The authors correctly identify that the varying number of accepted draft tokens across sequences in a batch leads to the \"ragged tensor problem.\" This prevents existing batch implementations from guaranteeing the output matches standard autoregressive generation. The paper proposes a \"correctness-first\" framework. First, it rigorously identifies the precise synchronization invariants required to maintain output equivalence. It then presents two implementation strategies: EQSPEC taht strictly enforces these invariants but incurs a high overhead of up to 40% for realignment, and EXSPEC that cleverly avoids the realignment cost entirely by using cross-batch scheduling to dynamically group sequences of the same length. The experimental results show that the proposed method achieves up to 3x throughput improvement (at batch size 8) while successfully maintaining over 95% output equivalence."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. **An important topic:** It addresses a critical correctness vs. performance trade-off in the LLM production environment.\n2. **Innovation:** The clever use of the scheduling mechanism (cross-batch) to resolve a data structure problem (realignment overhead) is a prime example of system-level optimization.\n3. **Significant improvement:** The 3x throughput improvement is achieved while maintaining a high correctness guarantee."}, "weaknesses": {"value": "1. **Compatibility issues:** The compatibility with common modern inference techniques like continuous batching and paged attention remains future work.\n2. **Lack of fully quantified metrics:** While EXSPEC avoids realignment, cross-batch scheduling itself might introduce new scheduling latency. The paper needs to further discuss and quantify EXSPEC's scheduling overhead under realistic high-concurrency workloads."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2z9UB1HhmA", "forum": "eM51kSFkoG", "replyto": "eM51kSFkoG", "signatures": ["ICLR.cc/2026/Conference/Submission21406/Reviewer_jZfw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21406/Reviewer_jZfw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986482875, "cdate": 1761986482875, "tmdate": 1762941750210, "mdate": 1762941750210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}