{"id": "H6fM44DOHP", "number": 19027, "cdate": 1758292831702, "mdate": 1759897065170, "content": {"title": "MMReD: a Cross-Modal Benchmark for Dense Context Reasoning", "abstract": "Despite recent advancements in extending context windows of large language models (LLMs) and large vision-language models (LVLMs), their ability to perform complex multi-modal reasoning over extended contexts remains critically limited. To underline this challenge, we present MMReD, a benchmark specifically designed to assess reasoning abilities within dense, information-rich scenarios where simple retrieval is not enough. Unlike traditional Needle-in-a-Haystack evaluations, MMReD challenges models to identify and interpret global patterns across entire contexts. Our benchmark comprises 24 tasks of varying complexity, ranging from standard passkey retrieval setups to those requiring selective or uniform attention to all context chunks. The evaluation reveals a consistent performance drop across all tested models -- including the most advanced LLMs, LVLMs, and architectures specializing in code and reasoning -- as the number of observations increases. Notably, even the leading reasoning-specialized models achieve 0\\% accuracy on certain tasks at the maximum context length of 128 observations. Conventional fine-tuning techniques, such as SFT and GRPO, also fail to generalize effectively to longer contexts. These observations reveal an inherent limitation in current model architectures, emphasizing the need for innovative approaches to enable competent dense context reasoning in multi-modal AI systems.", "tldr": "We present MMReD, a benchmark revealing that state-of-the-art LLMs and LVLMs struggle with dense multi-modal reasoning over long contexts, highlighting critical architectural limitations and the need for fundamentally new approaches.", "keywords": ["long context", "reasoning", "LLM", "LVLM", "MLLM", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8bc3b3d6f25d5e88b03c93f5bea266e4056ddfd1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MMReD (Multi-Modal REasoning in Dense context), a new benchmark designed to evaluate the ability of large language models (LLMs) and large vision-language models (LVLMs) to reason over long, information-rich sequences. The authors argue that existing benchmarks, which primarily use a \"Needle-in-a-Haystack\" (NIAH) setup, are insufficient as they only test a model's ability to retrieve a specific fact from a large, mostly irrelevant context. MMReD, in contrast, creates scenarios where every piece of the context is important, forcing models to identify and interpret global patterns rather than just locating sparse information.\n\nThe benchmark comprises 24 tasks that test models on tracking entities, spatial relationships, and event-based reasoning over sequences of varying lengths, up to 128 observations.\nThe evaluation revealed a consistent and significant drop in performance across all tested models—including advanced LLMs, LVLMs, and reasoning-specialized architectures—as the sequence length increased. \nThe study demonstrates that state-of-the-art models fail to generalize to dense context reasoning challenges. Furthermore, standard fine-tuning techniques like Supervised Fine-Tuning (SFT) and GRPO were found to be insufficient for enabling this capability.\n\nThe paper concludes that there is an inherent limitation in current model architectures when it comes to reasoning over dense, multi-modal contexts. The MMReD benchmark effectively highlights this gap and underscores the need for new architectural innovations and training paradigms to advance long-context understanding in AI systems."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel Problem Formulation: The paper introduces \"dense context reasoning\" as a distinct capability from standard \"Needle-in-a-Haystack\" (NIAH) retrieval, a significant conceptual shift.\nCreative Benchmark Design: MMReD uses a minimalist visual and linguistic environment to effectively isolate reasoning capabilities from perceptual or language complexity.\nClear Writing and Structure: The paper is well-written and logically organized, with an effective abstract and introduction that clearly motivate the work .\nChallenges Evaluation Paradigms: The work significantly challenges the sufficiency of the dominant NIAH paradigm, showing it is not a reliable indicator for complex reasoning."}, "weaknesses": {"value": "The paper's central conclusion is that the observed performance degradation reveals an \"inherent limitation in current model architectures\". While the extensive experiments show that models fail, the paper provides limited insight into why they fail at a mechanistic level.\n\nThe claim is a high-level one, and the analysis stops short of diagnosing the specific architectural components that are failing. It is unclear if the bottleneck lies in the attention mechanism's ability to handle uniform information density, the decay of information in positional embeddings over long distances, or how representations are processed through successive layers.\n\nThe work could be substantially improved by including more targeted analysis to diagnose the failure modes. An investigation of the models attention patterns on long dense-context (DC) tasks would be highly valuable. Visualizing attention maps could reveal whether attention becomes overly diffuse, incorrectly focuses on recent tokens (recency bias), or fails to integrate information from distant parts of the sequence."}, "questions": {"value": "1. Could the authors comment on this distinction between reasoning \"length\" and \"depth\"? Do you believe the architectural limitations observed are primarily related to failures in long-term memory and information aggregation, or do they also impact the ability to construct complex, multi-step logical inferences? How might the models you tested perform on tasks requiring deeper causal or counterfactual reasoning over the same long contexts?\nThis is not a criticism of the current design but an opportunity to add valuable nuance. A brief discussion on this \"length vs. depth\" axis of complexity in the conclusion could help frame the current results more precisely and outline a clear roadmap for the next generation of dense context benchmarks, which might incorporate tasks with deeper logical requirements.\n\n2. Could you provide more specific insights into the nature of this limitation? For instance, did your preliminary analysis reveal specific failure modes in the self-attention mechanism, such as attention becoming overly diffuse and uniform on Dense Context (DC) tasks compared to Needle-in-a-Haystack (NIAH) tasks? Or is the failure more related to information decay and representational corruption as information is passed through successive layers?\nThe paper's impact could be significantly strengthened by including a qualitative analysis, perhaps in the appendix, that delves into these potential causes. For example, providing attention visualizations for a successful short-context DC task versus a failed long-context one could offer powerful visual evidence to support your central claim and diagnose the failure mode more precisely."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nhArzmwQxO", "forum": "H6fM44DOHP", "replyto": "H6fM44DOHP", "signatures": ["ICLR.cc/2026/Conference/Submission19027/Reviewer_mQFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19027/Reviewer_mQFA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760596539303, "cdate": 1760596539303, "tmdate": 1762931071251, "mdate": 1762931071251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark, MMReD (Multi-Modal Reasoning in Dense context), to evaluate the ability of large LLMs and LVLMs to perform complex reasoning over long sequences of information. The authors argue that existing benchmarks do not adequately test a model's ability to reason in \"dense context\" scenarios where all information is potentially relevant. MMReD gives models a series of images depicting characters in different rooms and asks questions that require tracking entities, understanding spatial relationships, and identifying global patterns across the entire sequence. The study evaluates a wide range of frontier models and reveals a consistent drop in performance as the context length increases. The results show that even the most advanced models struggle with these dense context tasks. Also, common fine-tuning techniques offer little improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper argues that \"Needle-in-a-Haystack\" tasks (i.e., testing retrieval) are fundamentally different from \"dense context\" tasks, which require integration and reasoning. Figure 4 provides evidence that performance on these two task types does not correlate well at longer context lengths. This shows that dense context is fundamentally different from retrieval tasks, such as the needle-in-a-haystack problem.\n\n* The authors simplify the visual and linguistic elements and isolate the  reasoning capabilities they want to measure. This avoids confounding variables and ensures that the results reflect its capacity for dense context reasoning. Furthermore, the measure to ensure unique sequences and balanced answer distributions prevents models from relying on memorization or statistical shortcuts, making the evaluation more robust.\n\n* The paper also conducts a series of ablation studies. By testing the effects of model size, fine-tuning methodologies (SFT and GRPO), multimodal adapter types, and video-specific pooling methods, the authors provide multiple views on the problem. The finding that reasoning-specialized LLMs outperform complex LVLMs on the textual version of the tasks is particularly interesting. Specifically, multimodal instruction tuning and video-centric pretraining may be detrimental to this type of reasoning. This suggests that the bottleneck is not multimodal perception but a limitation in long-sequence reasoning."}, "weaknesses": {"value": "* The benchmark's controlled & minimalist design is a potential weakness. The environment is a highly structured, symbolic \"toy world\" with fixed rules (a 2x3 grid, one character moves per step). This does not reflect the ambiguity and chaotic nature of real-world scenarios, which involve complex simultaneous interactions, occlusions, and less predictable patterns. While an ablation with synthetic \"perceptual noise\" is included, it doesn't capture the true complexity of real-world visual and narrative understanding.\n\n* Because the benchmark is generated in a way that follows clear logical rules, there is a risk that future models could add an ad-hoc module specifically to solve MMReD-like tasks without truly developing generalized dense context reasoning. For example, a model could develop a specialized internal module for tracking entities in a grid.\n\n* The paper categorizes questions into two main buckets: Needle-in-a-Haystack (NIAH) and Dense Context (DC). However, the degree of \"density\" is somewhat ambiguous. An analysis of how performance degrades based on the degree of context density required to solve the question could yield deeper insights."}, "questions": {"value": "* The poor performance of video-oriented LVLMs was quite surprising. Do you have any hypotheses for this? Could it be related to their training strategies or a bias in their training data?\n\n* Do you believe the long-context reasoning is an inherent limitation of the Transformer architecture, or is it a problem that can be addressed with different training data and methodologies?\n\n* The paper focuses on tracking and counting tasks. Have you considered expanding the benchmark to include more abstract forms of reasoning, such as inferring intentions or predicting future states based on observed patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iPyDZaUDuI", "forum": "H6fM44DOHP", "replyto": "H6fM44DOHP", "signatures": ["ICLR.cc/2026/Conference/Submission19027/Reviewer_6qaP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19027/Reviewer_6qaP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761270091662, "cdate": 1761270091662, "tmdate": 1762931070896, "mdate": 1762931070896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new multimodal (vision + text) benchmark for dense context reasoning. Compared to existing benchmarks, it challenges models to identify and interpret global patterns across entire contexts. The tasks vary in difficulty, with a good number being challenging for existing models. Investigation also shows conventional fine-tuning techniques do not help improve the performance and so more advanced strategies will be needed to address the challenge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The benchmark is well motivated and studies a relatively valuable point about dense context reasoning in multimodal tasks.\n* A large number of models is evaluated, including many recent ones.\n* The benchmark has setups that are doable for existing models, but also setups that are too challenging for current models, so the benchmark is likely to be relevant for at least some time.\n* Various related questions are studied, and these help probe the performance of the models on the task further.\n* The paper is clearly written and easy to read."}, "weaknesses": {"value": "* All tasks come from one environment, so one could say that the benchmark is somewhat limited in this sense. A benchmark with e.g. three more-distinct types of setups would enable more rigorous investigation of dense context reasoning.\n* There could be a short discussion where the performances of models for text-only vs multimodal versions of the benchmark are compared. Which one is easier / to what extent is the text-only version easier? (one can infer this from the tables though)\n* In-context learning may potentially help so could be interesting to investigate too given its popularity.\n* Minor: the plots would benefit from larger font sizes where possible."}, "questions": {"value": "* How do the performances of models for text-only vs multimodal versions of the benchmark compare?\n* Does in-context learning help for these sorts of tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JdlNmdu38e", "forum": "H6fM44DOHP", "replyto": "H6fM44DOHP", "signatures": ["ICLR.cc/2026/Conference/Submission19027/Reviewer_MPFi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19027/Reviewer_MPFi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761408570107, "cdate": 1761408570107, "tmdate": 1762931070316, "mdate": 1762931070316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MMReD, a benchmark for evaluating long-context and cross-modal reasoning in large language and vision-language models (LLMs and LVLMs). Unlike traditional “Needle-in-a-Haystack” (NIAH) setups focused on sparse retrieval, MMReD emphasizes dense context reasoning, where all observations are informative and models must integrate global patterns rather than locating single facts."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. MMReD effectively bridges the gap between retrieval-based and integrative reasoning benchmarks by introducing dense, information-rich multi-modal contexts that better reflect real-world reasoning challenges.\n2. The evaluation suite is extensive and well-designed, covering a wide range of strong baseline and state-of-the-art models across relevant modalities and tasks.\n3. Comprehensive ablation studies offer deep insights into model behavior, highlighting the limitations of current approaches in handling dense contextual reasoning."}, "weaknesses": {"value": "1. The benchmark is constructed in an artificial and controlled environment, which may limit the model's ability to generalize the the real world modalities and with more noises.\n2. The close-ended question with exact-match accuracy may limit the insight into different models' performance in dense context reasoning."}, "questions": {"value": "1. How do you envision MMReD transferring to real-world multimodal reasoning, such as video QA? Could introducing controlled perceptual noise or natural imagery affect the benchmark’s conclusions?\n2. Since all contexts are synthetic, do you plan to integrate text–image mixtures or human-annotated long-context tasks to bridge synthetic and natural settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rc56X5tIDA", "forum": "H6fM44DOHP", "replyto": "H6fM44DOHP", "signatures": ["ICLR.cc/2026/Conference/Submission19027/Reviewer_URmZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19027/Reviewer_URmZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618112499, "cdate": 1761618112499, "tmdate": 1762931069822, "mdate": 1762931069822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}