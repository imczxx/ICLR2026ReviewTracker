{"id": "3fIBwnz4Tf", "number": 3856, "cdate": 1757555541230, "mdate": 1762956207363, "content": {"title": "FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control", "abstract": "We present FlexTraj, a framework for image-to-video generation with flexible point trajectory control. FlexTraj introduces a unified point-based motion representation that encodes each point with a segmentation ID, a  temporally consistent trajectory ID, and an optional color channel for appearance cues, enabling both dense and sparse trajectory control.\nInstead of injecting trajectory conditions into the video generator through token concatenation or ControlNet, FlexTraj employs an efficient sequence-concatenation scheme that achieves faster convergence, stronger controllability, and more efficient inference, while maintaining robustness under unaligned conditions.\nTo train such a unified point trajectory-controlled video generator, FlexTraj adopts an annealing training strategy that gradually reduces reliance on complete supervision and aligned condition. Experimental results demonstrate that FlexTraj enables multi-granularity, alignment-agnostic trajectory control for video generation, supporting various applications such as motion cloning, drag-based image-to-video, motion interpolation, camera redirection, flexible action control and mesh animations.", "tldr": "", "keywords": ["Diffusion Models", "Video Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/5056a3fe898aeb7c75602d282983e10896a02566.pdf", "supplementary_material": "/attachment/60f57caf690e0000f7fa8d9349abf5e1f421a7b8.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents an image-to-video generation framework based on point trajectory control. In detail, the proposed FlexTraj leverages a unified point-based motion representation which contains the information of segmentation ID, temporal consistent trajectory ID and color cues. The combined conditions are formed as video and encoded by video VAE. FlexTraj adopts a sequence-concatenation scheme to inject the condition into video diffusion and proposes an annealing training strategy to adapt the model for motion control with different trajectory conditions. Comprehensive experiments conducted on DAVIS verify the efficacy of motion control under the condition of dense, spatially sparse, temporally sparse and unaligned trajectories."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe support of multi-granularity and alignment-agnostic trajectory control can be widely applied into various scenarios. The value in the engineering field is high. \n2.\tGood performances are achieved on the DAVIS and the proposed FlexBench compared to SOTA trajectory control methods.\n3.\tThe generalization ability for unaligned control is impressive which could have much potential for the motion transfer."}, "weaknesses": {"value": "1.\tAlthough the proposed approach supports various motion control scenarios, the technical contribution is limited in my personal opinion. This work is more like an engineering design instead of a research investigation. The point control is widely explored in many previous motion control methods such as DragNUWA and DragAnything. Meanwhile, the sequence concatenation for condition injection in DiT is not new even though it is an effective and efficiency way in real-world implementation. The causal mask and KV cache have no strong relation with the whole network design and the technical is also leveraged from EasyControl. \n2.\tThe technical details about the SegID and TrackID are not clear. From the Figure 2, it is very hard for me to figure out what is the trajectory id and the segmentation id. What is the difference between them? There is no intuitive demonstration for these. Meanwhile, the condition videos shown in Figure 2 is very confusing and there is no meaningful thing. Is the index related with the object instance? Is there any detection or segmentation techniques employed for the target? I have no idea and very confused about these concepts. Nevertheless, I think these concepts are very important sinch they attribute the work a lot for the successful trajectory control. \n3.\tIn the experimental section, most of the baselines are constructed on the UNet. The authors usually claim that the inferior results are caused by the unpowerful architecture. As such, I think the comparison are not fair and the contribution can be brought by the stronger video DiT models.\n4.\tThere should be more quantitative analysis for ablation studies rather than only show the visual cases. It is not convincing in my opinion."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JA4YS4e4j5", "forum": "3fIBwnz4Tf", "replyto": "3fIBwnz4Tf", "signatures": ["ICLR.cc/2026/Conference/Submission3856/Reviewer_ELNB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3856/Reviewer_ELNB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890730951, "cdate": 1761890730951, "tmdate": 1762917069184, "mdate": 1762917069184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "B4ii5YV2gO", "forum": "3fIBwnz4Tf", "replyto": "3fIBwnz4Tf", "signatures": ["ICLR.cc/2026/Conference/Submission3856/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3856/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762956206418, "cdate": 1762956206418, "tmdate": 1762956206418, "mdate": 1762956206418, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FlexTraj for image-to-video (I2V) generation that allows for precise and flexible control over object motion using user-defined point trajectories. FlexTraj couples point trajectories with contextual information like segmentation masks and appearance attributes. This multi-granularity representation is integrated into a pre-trained I2V diffusion model via a tailored condition injection mechanism and an optimized annealing training strategy. The results show an advancement in controllable video generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The combined use of segmentation IDs, temporal IDs, and optional appearance cues is effective.\n- The comparison against ControlNet-style injection indicates that the proposed condition injection method is effective.\n- The annealing training strategy is shown to be crucial for preserving the base model's coherence while ensuring alignment to the control signal.\n- The paper is overall well-written and easy for readers to follow."}, "weaknesses": {"value": "- The paper claimed that FlexTraj is the first framework to support multi-granularity and alignment-agnostic trajectory control. However, in fact, the video generation with 3D point trajectory control has been well studied in previous works, e.g., Diffusion-As-Shader (DAS). And the control signals mentioned in the paper are all variations of 3D point trajectories, which can be obtained through a simple conversion. Therefore, I don't think this can be considered a major contribution.\n- The data processing pipeline used by FlexTraj is highly similar to that of DAS. Furthermore, I believe the methodology involves a combination of several existing and commonly used techniques. i.e., token concatenation and LoRA structures have been widely used and proven effective in controllable generation tasks, causal mask and KV cache are also borrowed from EasyControl. Claiming the efficient sequence-concatenation strategy as a major contribution is relatively weak.\n- The generative capacity remains constrained by the pre-trained base I2V model. Tasks requiring significant outpainting, strict physical constraints, or substantial changes to the scene structure may still be challenging.\n- Some highly related published papers are missing from the related work section.\n\nTrajectory Attention For Fine-grained Video Motion Control, ICLR 2025\n\nGS-DiT: Advancing Video Generation with Dynamic 3D Gaussian Fields through Efficient Dense 3D Point Tracking, CVPR 2025\n\nGEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control, CVPR 2025"}, "questions": {"value": "- What about the performance of FlexTraj when applied to highly non-rigid phenomena, such as fluid dynamics, e.g., water, smoke, fire, or deformable materials, e.g., cloth, while point tracking and segmentation may fail?\n- How does the proposed trajectory representation specifically handle points or segments that become occluded and then re-emerge later in the video? Similarly, how does FlexTraj handle newly appearing objects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KnrexOrkh8", "forum": "3fIBwnz4Tf", "replyto": "3fIBwnz4Tf", "signatures": ["ICLR.cc/2026/Conference/Submission3856/Reviewer_fZAa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3856/Reviewer_fZAa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948981695, "cdate": 1761948981695, "tmdate": 1762917068844, "mdate": 1762917068844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents *FlexTraj*, a unified framework for image-to-video generation that introduces flexible point trajectory control. The method encodes motion as a set of annotated 3D points—each carrying segmentation, trajectory, and optional color attributes—enabling both dense and sparse motion control. A core contribution lies in its *efficient sequence-concatenation* scheme for condition injection, which improves controllability and supports unaligned conditions. The authors further propose a density and alignment *annealing training* strategy, allowing the model to generalize from dense aligned conditions to sparse and unaligned scenarios. Experiments demonstrate consistent improvements in motion controllability and visual quality over a wide range of baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Provides a **comprehensive and unified** approach to motion control across multiple levels of granularity (dense, sparse, unaligned).  \n\n2. The **point-based representation** is elegant and general, bridging various types of control signals in a compact and interpretable form.  \n\n3. The **efficient sequence-concatenation mechanism** is well-motivated and empirically validated, offering practical benefits in training stability and inference speed.  \n\n4. The **annealing curriculum** for handling condition sparsity and misalignment is a thoughtful design and contributes to strong robustness."}, "weaknesses": {"value": "1. The framework seems **computationally heavy**, and real-time feasibility or scalability to longer sequences is not discussed.  \n\n2.  Occasional lack of **theoretical depth**—most claims are empirically observed without deeper analysis (e.g., why sequence concatenation generalizes better).  \n\n3. Computational overhead is mentioned briefly, but training efficiency and inference scalability deserve more quantitative reporting to assess practical feasibility."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wsHyJx0zBQ", "forum": "3fIBwnz4Tf", "replyto": "3fIBwnz4Tf", "signatures": ["ICLR.cc/2026/Conference/Submission3856/Reviewer_2vtv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3856/Reviewer_2vtv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961181511, "cdate": 1761961181511, "tmdate": 1762917068454, "mdate": 1762917068454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FlexTraj, a point-trajectory-based controllable image-to-video framework. The method introduces a unified representation that encodes segmentation IDs, temporal IDs, and optional color cues, and feeds these as tokens into a video diffusion model (CogVideoX-5B) via sequence concatenation with LoRA and a causal mask, similar to EasyControl. The authors further propose a data annealing curriculum to handle dense, sparse, and unaligned inputs. Experiments show improvements over point-trajectory and structure-guided baselines across dense, sparse, temporal, and unaligned settings, with strong motion controllability and competitive visual quality"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper introduces a unified point-trajectory conditioning interface that supports dense, sparse, and unaligned trajectory inputs, offering good flexibility and applicability across diverse video control scenarios.\n\n2.The proposed training strategy that gradually anneals data density and alignment quality helps the model stably learn from heterogeneous trajectory formats, enabling consistent motion control under varied supervision signals.\n\n3.The method is evaluated on DAVIS, FlexBench, and other benchmarks, and the results demonstrate strong motion controllability and competitive visual quality. The system is well-engineered and shows a high level of completeness."}, "weaknesses": {"value": "1. Limited technical novelty: The core design largely extends EasyControl’s token-concatenation and conditioning mechanism to video without introducing new temporal control modeling or control representations. The contribution is mostly an application-level adaptation rather than a fundamental methodological advance for controllable video generation.\n\n\n2.Lack of direct comparison with classical controllable video baselines such as VACE under identical data and training settings. Since EasyControl and VACE are both important backbones in this area, a fair head-to-head evaluation is necessary to convincingly support the claimed improvements in controllability and quality.\n\n\n3.EasyControl’s causal attention mechanism is designed specifically to support multiple control types simultaneously. However, this paper only demonstrates single-control scenarios, failing to reveal the benefit of causal attention or justify its necessity in this setting. As a result, it remains unclear whether the method extends beyond a straightforward single-control EasyControl adaptation."}, "questions": {"value": "What is the motivation for using causal attention? In controllable video generation tasks, what advantages does it offer compared to a bidirectional attention design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8DeEjOXTr6", "forum": "3fIBwnz4Tf", "replyto": "3fIBwnz4Tf", "signatures": ["ICLR.cc/2026/Conference/Submission3856/Reviewer_nZYY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3856/Reviewer_nZYY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975189231, "cdate": 1761975189231, "tmdate": 1762917068120, "mdate": 1762917068120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}