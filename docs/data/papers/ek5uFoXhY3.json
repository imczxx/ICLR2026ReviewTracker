{"id": "ek5uFoXhY3", "number": 11166, "cdate": 1758191800535, "mdate": 1762923576212, "content": {"title": "Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks", "abstract": "Federated learning is vulnerable to poisoning and backdoor attacks under partial observability. We formulate defence as a partially observable sequential decision problem and introduce a trust-aware Deep Q-Network that integrates multi-signal evidence into client trust updates while optimizing a long-horizon robustness–accuracy objective. On CIFAR-10, we (i) establish a baseline showing steadily improving accuracy, (ii) show through a Dirichlet sweep that increased client overlap consistently improves accuracy and reduces ASR with stable detection, and (iii) demonstrate in a signal-budget study that accuracy remains steady while ASR increases and ROC-AUC declines as observability is reduced, which highlights that sequential belief updates mitigate weaker signals. Finally, a comparison with random, linear-Q, and policy gradient controllers confirms that DQN achieves the best robustness–accuracy trade-off.", "tldr": "Trust-aware DQN learns client trust from multi-signal evidence, robustly defending FL against poisoning/backdoors under non-IID and partial observability.", "keywords": ["Federated learning", "adversarial robustness", "robust aggregation", "model poisoning", "backdoor attacks", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0107c88e339ba5a47e9516d9f1b3624b3488c454.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a trust-aware defense mechanism for federated learning (FL), formulating the defense as a partially observable Markov decision process (POMDP). The approach leverages a Deep Q-Network (DQN) to dynamically adjust each client’s trust. Experiments on CIFAR-10 show that the DQN-based defense achieves more stable and adaptive robustness than static aggregation rules."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper reframes FL defense as sequential decision-making under uncertainty, which is a fresh and rigorous departure from static aggregation methods.  \n2. The integration of Bayesian belief tracking with DQN allows the system to reason temporally about client reliability is novel"}, "weaknesses": {"value": "1. Missing baseline comparisons: The experiments compare only RL controllers (DQN, Linear-Q, Policy Gradient, Random) but omit standard FL defense baselines such as FLTrust [2], FedAvg, and Krum [3]. Including these would better contextualize performance gains.\n\n2. Limited attack evaluation: The study focuses mainly on a single backdoor attack. The proposed defense should also be tested against stronger adaptive or RL-based poisoning attacks, e.g., Li et al., NeurIPS 2022 [1].\n\n3. Narrow experimental scale: All evaluations use CIFAR-10 with only ten clients. Results on larger or more realistic FL benchmarks and datasets would strengthen the generality claim.\n\n[1] H. Li et al. Reinforcement Learning-based Model Poisoning Attacks on Federated Learning. NeurIPS, 2022.\n\n[2] X. Cao et al. FLTrust: Byzantine-Robust Federated Learning via Trust Bootstrapping. arXiv:2012.13995, 2022.\n\n[3] P. Blanchard et al. Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent. NeurIPS, 2017."}, "questions": {"value": "1. For the questions regarding experiments, please refer to the weakness part.\n\n2. Is there any theoretical proof on the DQN convergence or performance guarantee? \n\n3. The related-work section could be expanded to include prior trust- or reputation-based defense methods, such as Sun et al., UAI 2023 [1], Xu & Lyu 2021 [2] to better position the contribution within the literature.\n\n[1] X. Sun et al. Pandering in a (Flexible) Representative Democracy. UAI, 2023.\n\n[2] X. Xu and L. Lyu. A Reputation Mechanism is All You Need: Collaborative Fairness and Adversarial Robustness in Federated Learning. arXiv:2011.10464, 2021."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2hNqNY8eIn", "forum": "ek5uFoXhY3", "replyto": "ek5uFoXhY3", "signatures": ["ICLR.cc/2026/Conference/Submission11166/Reviewer_nGbF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11166/Reviewer_nGbF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761686018710, "cdate": 1761686018710, "tmdate": 1762922327812, "mdate": 1762922327812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "wYvOHHtJhh", "forum": "ek5uFoXhY3", "replyto": "ek5uFoXhY3", "signatures": ["ICLR.cc/2026/Conference/Submission11166/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11166/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762923575342, "cdate": 1762923575342, "tmdate": 1762923575342, "mdate": 1762923575342, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to improve the robustness of federated learning against Byzantine attacks. It presents a method that models the defense process as a partially observable sequential decision problem. A trust-aware Deep Q-Network controller is introduced to adjust client weights using Bayesian belief updates of client trust. The framework combines three anomaly indicators: update direction alignment, norm deviation, and validation impact. These indicators are used to estimate each client’s reliability, and the aggregation weights are then updated based on these trust estimates."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "+ The idea of viewing federated defense as a partially observable reinforcement learning problem is conceptually interesting. It connects adaptive control and security under uncertainty.\n+ The proposed workflow of signal extraction, trust estimation, aggregation, and reward design is clear and easy to follow."}, "weaknesses": {"value": "- The reported attack success rates remain high (about 40–47%), and the ROC-AUC values are close to random (around 0.5). These results show that the proposed method does not achieve real robustness improvements. The final test accuracy on CIFAR-10 (around 65%) is also low compared with standard federated learning benchmarks, which limits the practical contribution.\n- The comparison omits standard robust aggregation methods such as Krum, Trimmed Mean and Coordinate-wise Median, FLTrust, and FLARE . The paper only compares several reinforcement learning controllers. Without these baselines, the claims of superiority are not well supported.\n- The method depends on per-client alignment and validation-impact signals. These signals require access to individual client updates, which is not possible under secure aggregation protocols. This assumption makes the method difficult to apply in real federated learning systems where client updates are encrypted or aggregated securely.\n- Important components such as the observation likelihoods $P(o_t^i|s_t^i)$, prior distributions, and the trust update rule are only described in general terms. The paper does not include complete definitions or derivations. The Bayesian belief update cannot be reproduced based on the current description, which reduces transparency and technical rigor.\n- The experiments are conducted with a small convolutional model on CIFAR-10 using 10 clients, 50 communication rounds, and 20 percent malicious clients. This setup is too small to represent real federated learning scenarios that involve large numbers of clients and heterogeneous data. The paper also evaluates only one type of backdoor attack. Other important attack types such as gradient sign, label flipping, and collusion are not tested.\n- The Linear-Q and Policy Gradient baselines perform very poorly. This indicates weak or inconsistent hyperparameter tuning. The imbalance in training makes the comparison unfair and overstates the benefits of the DQN controller.\n- The appendix states that ChatGPT was used to organize and write all sections. The style of writing, with repetitive phrasing and generic explanations, supports this statement. Although the use of language models for assistance is not unethical, the dependence on generated text appears to have replaced original explanation and analysis. The theoretical parts are shallow, and some equations seem copied from previous works without clear reference.\n- All cited papers are real, but many entries contain incorrect information. Several key works are assigned the wrong year or venue. For example, FedAvg was introduced in 2017, not 2023; Yin et al. should refer to ICML 2018, not 2021; and Xu & Lyu should be listed as 2020, not 2021. Such citation errors indicate a lack of careful verification and reduce the credibility of the work."}, "questions": {"value": "1.How are the per-client alignment and validation-impact signals computed under secure aggregation? If raw updates are used, what are the privacy and security implications?\n\n2.What are the explicit mathematical forms of the observation likelihoods and priors used in the belief-update process?\n\n3.How is the trust-accuracy term ($\\tau_A$) in the reward defined without access to ground-truth labels for malicious clients?\n\n4.Why are standard robust aggregation baselines such as Krum, Trimmed Mean, FLTrust, and FLARE not included in the experiments?\n\n5.What hyperparameter tuning strategy was used for the reinforcement learning baselines to ensure fair comparison?\n\n6.Can the controller scale to larger numbers of clients? Please include the computational complexity and runtime analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cJ9Cm7hk7u", "forum": "ek5uFoXhY3", "replyto": "ek5uFoXhY3", "signatures": ["ICLR.cc/2026/Conference/Submission11166/Reviewer_KsZM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11166/Reviewer_KsZM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875735630, "cdate": 1761875735630, "tmdate": 1762922327422, "mdate": 1762922327422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reformulated FL defense as a partially observable Markov decision process. Then it introduced a trust-aware Deep Q-Network that learns to optimize robustness-accuracy trade-offs by adaptively filtering and reweighting client updates. Experimental results validated the effectiveness of the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) It reformulated FL defense as a partially observable Markov decision process. \n\n(2) A trust-aware Deep Q-Network was proposed to optimize robustness-accuracy trade-offs by adaptively filtering and reweighting client updates. \n\n(3) Experimental results validated the effectiveness of the proposed framework."}, "weaknesses": {"value": "(1) The experiment results are weak. More FL datasets and large numbers of clients can be included to validate the effectiveness of the proposed framework in various settings.\n\n(2) More recent FL defense baselines, such as Byzantine-robust aggregation, can be used in the experiments.\n\n(3) It is unclear why DQN enables faster convergence than Linear-Q, Policy Gradient, and Random."}, "questions": {"value": "(1) What is the \"update rule introduced in the thesis\" in line 205?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xCPFaLzVNu", "forum": "ek5uFoXhY3", "replyto": "ek5uFoXhY3", "signatures": ["ICLR.cc/2026/Conference/Submission11166/Reviewer_Vgrh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11166/Reviewer_Vgrh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967287754, "cdate": 1761967287754, "tmdate": 1762922327044, "mdate": 1762922327044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper design a defense mechanism against poisoning and backdoor attacks in FL, particularly addressing the challenge of partial observability introduced by secure aggregation protocols. The core idea is to model the defense as a Partially Observable Markov Decision Process (POMDP) and use a Trust-Aware Deep Q-Network (DQN) to learn a dynamic aggregation policy that optimizes a long-horizon robustness-accuracy objective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The DQN controller achieves the best robustness-accuracy trade-off compared to competitive baselines, including Random, Linear-Q, and Policy Gradient controllers.\n2. The sequential belief updates are shown to effectively mitigate the weaknesses of individual, weaker anomaly signals."}, "weaknesses": {"value": "1. The DQN-based approach introduces additional complexity and training overhead compared to static aggregation rules, which may be a concern in resource-constrained FL settings.\n2. All experiments are conducted on CIFAR-10, performance on more complex or domain-specific datasets (e.g., medical or NLP tasks) remains unverified.\n3. The DQN-based approach needs massive rounds to get covergence so that can get a robust aggregation policy. The communication round 50 might not sufficient, or the task is too simple so that only needs few rounds."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KyD4DpiLzS", "forum": "ek5uFoXhY3", "replyto": "ek5uFoXhY3", "signatures": ["ICLR.cc/2026/Conference/Submission11166/Reviewer_dp8Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11166/Reviewer_dp8Z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994828019, "cdate": 1761994828019, "tmdate": 1762922326518, "mdate": 1762922326518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}