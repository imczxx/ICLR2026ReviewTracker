{"id": "finA00bYJj", "number": 3947, "cdate": 1757571838055, "mdate": 1759898061515, "content": {"title": "Pixel Motion as Universal Representation for Robot Control", "abstract": "We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. \nOur high-level $\\textit{System 2}$, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame and past motion to guide robot control.\nPixel motion—a universal, interpretable, and motion-centric representation—can be extracted from videos in a weakly-supervised manner, enabling diffusion model training on any video-caption data.\nTreating the generated pixel motion as largely embodiment-agnostic $\\textit{universal representations}$, our embodiment-aware $\\textit{System 1}$ module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision.\nSystem 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals.\nThis hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action.\nVisualizations at https://anonymous.4open.science/w/LangToMo.", "tldr": "Learning language tied universal and interpretable motion features that can be mapped to real world actions in robotics tasks", "keywords": ["vision-language-action", "universal motion representations"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0428984d452342e862ae284f91fd1ac368b4a264.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "LangToMo is a dual-system vision-language-action framework that uses text-conditioned pixel motion forecasts (via an image diffusion model) as universal, interpretable intermediate representations. A high-level System 2 generates pixel motion from a single frame and past motion, while a low-level, embodiment-aware System 1 maps this motion to robot actions through motion-to-action functions (hand-crafted or minimally supervised), enabling flexible, scalable, and generalizable control across unsupervised and supervised settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper investigates a new hierarchical policy approach that uses pixel motion to interface between high- and low-level policies. It addresses an important area and offers insightful directions for future research.\n- Writing: The paper is well written, the method is clearly presented, and the figures/tables are complete and easy to read."}, "weaknesses": {"value": "1. **Motivation**: I am uncertain about the authors’ motivation for using pixel motion to construct a hierarchical policy. Prior dual system approaches (e.g., HiRT[1], LCB[2], OpenHelix[3]) leverage interactions between large and small models to increase control frequency, yet the paper does not explain how its hierarchical design relates to or improves upon this line of work. In addition, the authors should compare against these methods and clarify the advantages and drawbacks of using pixel motion versus other latent representations as the interface between System 1 and System 2.\n\n[1] Zhang J, Guo Y, Chen X, et al. Hirt: Enhancing robotic control with hierarchical robot transformers[J]. arXiv preprint arXiv:2410.05273, 2024.\n\n[2] Shentu Y, Wu P, Rajeswaran A, et al. From llms to actions: Latent codes as bridges in hierarchical robot control[C]//2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2024: 8539-8546.\n\n[3] Cui C, Ding P, Song W, et al. Openhelix: A short survey, empirical analysis, and open-source dual-system vla model for robotic manipulation[J]. arXiv preprint arXiv:2505.03912, 2025.\n\n2. Compared to prior approaches that predict visual traces, the novelty of this work appears limited, and the authors have neither adequately justified nor empirically validated the advantages of pixel motion over visual trace/optical flow.\n\n3. The experimental results are weak, especially on simulation benchmarks against advanced baselines. The appendix shows only moderate performance on CALVIN, and results on other environments (e.g., SimplerEnv) are missing."}, "questions": {"value": "Refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wb2NxgBaCR", "forum": "finA00bYJj", "replyto": "finA00bYJj", "signatures": ["ICLR.cc/2026/Conference/Submission3947/Reviewer_YQmb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3947/Reviewer_YQmb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491211033, "cdate": 1761491211033, "tmdate": 1762917107331, "mdate": 1762917107331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Two key clarifications to resolve misunderstanding"}, "comment": {"value": "We thank reviewers for all positive feedback. \n  - *Addresses an important area and offers insightful directions for future research.*\n  - *The paper is well written, the method is clearly presented, and the figures/tables are complete and easy to read.*\n  - *Dual-system design also satisfied the real-time issue of robot policy*\n  - *The writing is clear and easy to follow.*\n  - *The method is able to leverage the unlabeled human data and enable scaled learning.*\n  - *Surpass other baseline methods in real world zero-shot tasks via large-scale pretraining.*\n\n\n&nbsp;\n\nWe are surprised to see the reject ratings given the highly positive feedback. We believe there have been some misunderstandings regarding our work, and focus on the following two points to allow better clarification. We apologize in advance for any lack of clarity on our part that led to this misunderstanding. \n\n&nbsp;\n\n**1. Pixel Motion = Future Optical Flow**\n  - From a single frame, we predict future optical flow (motion for every pixel in image). We realize our choice of wording “pixel motion” is highly unclear. We will use the term optical flow instead.\n  - No prior work explores directly learning this from videos conditioned on text. They use subsets of image pixels, picked using task-specific heuristics, which makes training less scalable.  \n\n&nbsp;\n\n**2. No robot action data for training**\n  - Our real world results (Table 2 & 3) as well as MetaWorld results (Table 4 LTM-H) are not trained on any action trajectory data. \n  - We specifically pick this MetaWorld split because most prior works operating under this “actionless” training settings (e.g. AVDC) use this split.\n  - VLA models like OpenVLA & Pi0 cannot operate under this setting. They need thousands of extensive robot action trajectory data to pretrain followed by hundreds of downstream task robot demos for finetuning. \n  - Even in our downstream finetuning setting (Table 4 LTM-S), we use as few as 10-20 robot demos for finetuning."}}, "id": "JnoKW4jyEG", "forum": "finA00bYJj", "replyto": "finA00bYJj", "signatures": ["ICLR.cc/2026/Conference/Submission3947/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3947/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3947/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763433178153, "cdate": 1763433178153, "tmdate": 1763433178153, "mdate": 1763433178153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents LangToMo, a two-stage framework for predicting robot motion using pixel movement as an intermediate representation.\nLangToMo consists of two systems:\n(i) System 2 employs a diffusion-based model to generate pixel motion (PM), pretrained on the OpenX dataset and fine-tuned on downstream task demonstrations;\n(ii) System 1 maps actions conditioned on the predicted pixel motion.\nExperimental results demonstrate that LangToMo outperforms baseline methods on both Meta-World and real-world robotic manipulation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed two-stage framework preserves the original model’s capabilities while enabling the transformation from vision-language signals to action representations.\n- Compared to related work, LangToMo employs a diffusion model to directly predict pixel motion instead of generating full video sequences.\n- Surpass other baseline method in real world zero-shot tasks via large-scale pretraining."}, "weaknesses": {"value": "- **Longer inference latency**\n  \n  Similar to UniPi, many steps of denoising are required when the diffusion model predicts pixels or PMs, which leads to long inference delays and false closed-loop control, which limits the model to static scenes.\n\n- **Weak evaluation**  \n  Choosing Metaworld benchmark in main text experiment for VLA models is less convincing. Metaworld tasks and scenarios are relatively simple, and accurate action prediction can be achieved using images alone without requiring text. The supplementary material shows that Calvin's experimental results are worse than those of VPP, which only performs pixel predictions. The reasoning seems insufficient ( VPP performs better even without using large amounts of data for cotraining). Therefore, additional ablation experiments are needed to clarify that the poorer performance is due to model size.\n- **Real-world task problems**  \n  The project link cannot be opened and the real-world video results cannot be seen. From the experimental content in the text, it seems that the scenes and tasks in real-world setting are relatively simple."}, "questions": {"value": "1. Is the poor performance of Calvin due to the model size or the pipeline structure? You can add PM prediction channel to the SVD with post-training to make a fair compare with VPP in Calvin.\n2. What is the failure case in the Calvin rollout? Is it due to semantic understanding causing PM prediction errors (wrong movement direction) or inaccurate action head mapping?\n3. Please provide the frequency of your model deployment on real world."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Although directly predicting pixel motion may be more effective than extracting it from generated videos, the inconsistency between the pretraining and fine-tuning stages could lead to the forgetting of the pretrained video model’s capabilities. I am concerned about LangToMo’s high-level semantic understanding ability, which cannot be well reflected in benchmarks such as Meta-World. Therefore, more convincing experiments are needed to demonstrate the effectiveness of the proposed method."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VKxs36nxF9", "forum": "finA00bYJj", "replyto": "finA00bYJj", "signatures": ["ICLR.cc/2026/Conference/Submission3947/Reviewer_CeJz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3947/Reviewer_CeJz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568882709, "cdate": 1761568882709, "tmdate": 1762917107016, "mdate": 1762917107016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents LangToMo, a two-stage framework for predicting robot motion using pixel movement as an intermediate representation.\nLangToMo consists of two systems:\n(i) System 2 employs a diffusion-based model to generate pixel motion (PM), pretrained on the OpenX dataset and fine-tuned on downstream task demonstrations;\n(ii) System 1 maps actions conditioned on the predicted pixel motion.\nExperimental results demonstrate that LangToMo outperforms baseline methods on both Meta-World and real-world robotic manipulation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed two-stage framework preserves the original model’s capabilities while enabling the transformation from vision-language signals to action representations.\n- Compared to related work, LangToMo employs a diffusion model to directly predict pixel motion instead of generating full video sequences.\n- Surpass other baseline method in real world zero-shot tasks via large-scale pretraining."}, "weaknesses": {"value": "- **Longer inference latency**\n  \n  Similar to UniPi, many steps of denoising are required when the diffusion model predicts pixels or PMs, which leads to long inference delays and false closed-loop control, which limits the model to static scenes.\n\n- **Weak evaluation**  \n  Choosing Metaworld benchmark in main text experiment for VLA models is less convincing. Metaworld tasks and scenarios are relatively simple, and accurate action prediction can be achieved using images alone without requiring text. The supplementary material shows that Calvin's experimental results are worse than those of VPP, which only performs pixel predictions. The reasoning seems insufficient ( VPP performs better even without using large amounts of data for cotraining). Therefore, additional ablation experiments are needed to clarify that the poorer performance is due to model size.\n- **Real-world task problems**  \n  The project link cannot be opened and the real-world video results cannot be seen. From the experimental content in the text, it seems that the scenes and tasks in real-world setting are relatively simple."}, "questions": {"value": "1. Is the poor performance of Calvin due to the model size or the pipeline structure? You can add PM prediction channel to the SVD with post-training to make a fair compare with VPP in Calvin.\n2. What is the failure case in the Calvin rollout? Is it due to semantic understanding causing PM prediction errors (wrong movement direction) or inaccurate action head mapping?\n3. Please provide the frequency of your model deployment on real world."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Although directly predicting pixel motion may be more effective than extracting it from generated videos, the inconsistency between the pretraining and fine-tuning stages could lead to the forgetting of the pretrained video model’s capabilities. I am concerned about LangToMo’s high-level semantic understanding ability, which cannot be well reflected in benchmarks such as Meta-World. Therefore, more convincing experiments are needed to demonstrate the effectiveness of the proposed method."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VKxs36nxF9", "forum": "finA00bYJj", "replyto": "finA00bYJj", "signatures": ["ICLR.cc/2026/Conference/Submission3947/Reviewer_CeJz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3947/Reviewer_CeJz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568882709, "cdate": 1761568882709, "tmdate": 1763438992568, "mdate": 1763438992568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to use pixel motion as a control interface. System 1 translate language into pixel motions while system 2 translate the motions into robot actions. The authors show performance gain in MetaWorld experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The writing is clear and easy to follow.\n\nThe method is able to leverage the unlabeled human data and enable scaled learning."}, "weaknesses": {"value": "The novelty is limited. Many papers have explored the idea of extracting universal action representation from videos.\n\nThe performance is only evaluated on MetaWorld and the performance gain is marginal compared to ATM. More evaluations are needed.\n\nSee questions below."}, "questions": {"value": "(1)\tAnother line of works that using latent actions to capture the pixel motions is missing, including works like LAPA, IGOR, Villa-X, UniVLA, etc. Comparison and discussion are needed.\n\n(2)\tAs for the experiments, the author pretrain the models on the robot data (L361) and finetune the models on robot and human data. However, the pixel motions, the setting should be that we have many unlabeled video data (of human) and a small amount of labeled robot data.\n\n(3)\tIn Table 5, why running two systems at the same frequency will lead to a performance drop?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d4pUkZeAVh", "forum": "finA00bYJj", "replyto": "finA00bYJj", "signatures": ["ICLR.cc/2026/Conference/Submission3947/Reviewer_AqBk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3947/Reviewer_AqBk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796523104, "cdate": 1761796523104, "tmdate": 1762917106706, "mdate": 1762917106706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LangToMo, a dual-system vision-language-action (VLA) framework that uses pixel motion (optical flow) as a universal intermediate representation for robot control. System 2 is a language-conditioned diffusion model that predicts pixel motion from a single image and instruction and system 1 is a  mapping function that converts the generated pixel motion into robot actions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a key bottleneck in robot learning from videos: the need for action supervision and embodiment-specific data. The idea of treating pixel motion as a universal, interpretable, and embodiment-agnostic abstraction is good.\n2. Dual-system design also satisfied the real-time issue of robot policy"}, "weaknesses": {"value": "1. Although the idea of using pixel motion as action representation is nice, I feel the idea is widely studied in previous work. The author list the difference with previous works at Table 1. I feel the idea is a little bit incremental.\n2. For the simulation experiments, the author only did experiments on 11 Metaworld benchmarks tasks, which is limited. Many previous works train language conditioned policy on the whole Metaworld benchmark. Also, Metaworld is not designed for language-conditioned tasks, maybe run methods on Calvin or Libera can better verify the effectiveness of the method."}, "questions": {"value": "1. Could you include comparisons with more advanced vision-language-action (VLA) models? Since the proposed approach ultimately produces a language-conditioned policy, it should also be evaluated against general VLA policies, not only motion-based ones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZiBS3ZPDHl", "forum": "finA00bYJj", "replyto": "finA00bYJj", "signatures": ["ICLR.cc/2026/Conference/Submission3947/Reviewer_9Laj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3947/Reviewer_9Laj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948479300, "cdate": 1761948479300, "tmdate": 1762917106342, "mdate": 1762917106342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}