{"id": "u1lUbEDQsU", "number": 7319, "cdate": 1758015707570, "mdate": 1759897860142, "content": {"title": "SORA: Free Second Order Attacks in Fast Adversarial Training", "abstract": "Adversarial Training (AT) is a leading defense against adversarial examples but often suffers from *Catastrophic Overfitting* (CO) in efficient single-step variants, where robustness to multi-step attacks collapses despite high single-step performance. \n    We address this failure mode with two contributions. \n    First, we identify *Epsilon Overfitting* (EO), a previously overlooked phenomenon in which fixed perturbation magnitudes exacerbate CO, and show that introducing perturbation variability significantly improves robust generalization across different architectures and datasets. \n    Second, we propose **PertAlign** (Perturbation Alignment), a theoretically grounded, computationally negligible metric that predicts CO onset by measuring gradient alignment across attack stages. \n    Leveraging these insights, we introduce **SORA**, an adaptive step-size adversarial training method that dynamically adjusts perturbations based on loss-surface geometry. \n    SORA consistently prevents CO, achieves state-of-the-art robustness and clean accuracy, and generalizes across datasets and architectures using a single fixed set of hyperparameters.\n    Extensive experiments on diverse datasets and architectures, show that SORA matches or surpasses the robustness of prior methods while delivering higher clean accuracy and superior efficiency.\n    Code is available at [https://anonymous.4open.science/r/2026_ICLR_SORA](https://anonymous.4open.science/r/2026_ICLR_SORA).", "tldr": "", "keywords": ["Adversarial Robustness", "Adversarial Training", "Fast Adversarial Training", "Catastrophic Overfitting"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/751266ac18af6dd1e68c2d026d7659d23ab416a2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose an efficient metric to measure the non-linearity of loss landscape during adversarial training. By utilizing this metric, the optimal step size for adversarial attack can be calculated. The experiments on diverse datasets and architectures demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is written well\n2. The method is computationally efficient \n3. The theoretical analysis on PerAlign, which is to measure the non-linearity of the loss landscape, is rigorous"}, "weaknesses": {"value": "1. **Marginal improvement:** On CIFAR10 and CIFAR100, SORA only has slight improvement compared to NFGSM and AAER. What are the advantages of your method compared to them?\n2. **Abnormal baseline performance:** The performance of GradAlign, ZeroGrad, ATAS, AAER is surprisingly low on PathMNIST and TissueMNIST. It is well known that AT is sensitive to hyperparameters. Did you tune their hyperparameters to ensure the optimal performance on these datasets?\n3. **Some baselines are missing:** The results of critical baselines, e.g., ATTA [1], Fast-BAT [2], NuAT [3], are missing\n4. **Lack of results on high-resolution datasets:** You only have the results on low-resolution datasets. It is still unknown whether your method can be scaled up to high-resolution datasets, e.g., ImageNet-100.\n\n[1] Haizhong Zheng et al. Efficient adversarial training with transferable adversarial examples\n\n[2] Yihua Zhang et al. Revisiting and advancing fast adversarial training through the lens of bi-level optimization.\n\n[3] Gaurang Sriramanan et al. Towards efficient and effective adversarial training."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fAougUByqz", "forum": "u1lUbEDQsU", "replyto": "u1lUbEDQsU", "signatures": ["ICLR.cc/2026/Conference/Submission7319/Reviewer_PMUw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7319/Reviewer_PMUw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761038382020, "cdate": 1761038382020, "tmdate": 1762919430251, "mdate": 1762919430251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of Catastrophic Overfitting (CO) in single-step adversarial training (AT). The authors identify a phenomenon termed \"Epsilon Overfitting\" (EO), where models overfit to the specific perturbation magnitude used during training. They propose Perturbation Alignment (PertAlign) to predict the onset of CO, and leverage these insights to develop SORA, an adaptive step-size AT method. SORA dynamically adjusts perturbation sizes based on an approximation of the local loss curvature. Extensive experiments across multiple datasets and architectures show that SORA can prevent CO and achieve competitive robust accuracy with minimal computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tComprehensive Evaluation: The paper provides a thorough empirical evaluation across a diverse set of datasets (including challenging medical imaging benchmarks) and model architectures. \n2.\tIdentification of Epsilon Overfitting (EO): The observation that fixed, large perturbation magnitudes can lead to overfitting on specific ε values is an interesting and well-documented analysis. \n3.\tLow-Cost Metric: The proposed PertAlign metric is computationally cheap, as it reuses gradients already computed during the standard training process. \n4.\tPractical Algorithm: The SORA algorithm itself is relatively simple to implement and integrates seamlessly into existing fast AT pipelines."}, "weaknesses": {"value": "1.\tSignificance of the Core Problem: A fundamental question remains: How critical is Catastrophic Overfitting in the broader landscape of adversarial robustness? While CO is a known failure mode in single-step AT, the paper does not sufficiently articulate why CO remains a pressing, unsolved issue that warrants a new solution, especially given that multi-step methods like PGD-10 are still considered the gold standard for high robustness, albeit at a higher cost.\n2.\tNovelty and Depth of Contributions: 1) Epsilon Overfitting (EO): While the term is new, the underlying idea—that fixed-step attacks can lead to non-robust, overfitted decision boundaries—is a core intuition behind many existing adaptive and multi-step methods. The claim that EO is a \"previously overlooked phenomenon\" may be an overstatement; it is more accurately a new and specific characterization of a known class of problems. 2) Perturbation Alignment (PertAlign): The theoretical derivation connects PertAlign to the Hessian, which is a solid contribution. However, from a practical standpoint, PertAlign can be perceived as a relatively trivial trick: it is essentially the cosine similarity between the gradient at a random start and the gradient after one FGSM step. What is it essentially different from GradAlign?\n3.\tMarginal Practical Gains: The experimental results show that SORA's advantages are often marginal. In many tables, SORA's robust accuracy is within less than ~1% of the best-performing single-step baselines (e.g., NFGSM, AAER). While it achieves better clean accuracy on some datasets like PathMNIST, the overall improvement in the trade-off between robustness and accuracy is not dramatic. Hence, the net practical benefit of adopting SORA is not overwhelmingly compelling.\n4.\tTheoretical Grounding Justification: The theoretical analysis in Appendix A is technically sound, deriving the optimal step size under a quadratic loss assumption. However, the justification for this approximation in the context of deep neural networks is weak. The highly non-convex and complex loss landscapes of modern DNNs are far from quadratic, and the paper does not provide evidence that this local approximation holds well enough in practice to be truly meaningful, beyond serving as a heuristic inspiration for the algorithm.\n5.\tComparison to Competitors: The paper positions SORA as a state-of-the-art method. However, when compared to strong baselines like PGD-10 or TRADES, SORA consistently lags in robust accuracy (as expected, given their higher computational cost). Among its single-step peers, it is a strong contender but not a clear dominator. The claim of \"superior efficiency\" is true relative to multi-step methods, but its advantage in time/memory over other single-step methods (see Figure 6) is minimal, and its performance gain is similarly slight."}, "questions": {"value": "See the weaknesses.\n\n1.\tBeyond preventing CO, what specific, significant advantage does SORA offer over other CO-robust single-step methods, given that the accuracy improvements are often marginal?\n2.\tCan you provide empirical evidence validating the quadratic loss assumption in deep networks? How sensitive is SORA's performance to scenarios where this assumption breaks down?\n3.\tThe concept of adapting to local curvature is not new. How is SORA's approach fundamentally different or better than prior adaptive step-size methods like ATAS, beyond the specific heuristic used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8mQyk8lI9K", "forum": "u1lUbEDQsU", "replyto": "u1lUbEDQsU", "signatures": ["ICLR.cc/2026/Conference/Submission7319/Reviewer_pwii"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7319/Reviewer_pwii"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761614484085, "cdate": 1761614484085, "tmdate": 1762919429402, "mdate": 1762919429402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and analyze Epsilon Overfitting, demonstrating its importance for robust generalization and CO. The authors propose PertAlign, a efficient metric for early and reliable CO prediction. The authors verify the effectiveness of their method on different methods and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe motivation and method are clear, and the visualization is helpful.\n2.\tThe authors provide sufficient evidence, including extensive experiments and ablation studies, to support the effectiveness of their proposed method. \n3.\tThis paper is well-written, making it easy to follow."}, "weaknesses": {"value": "1.\tMisleading or Overstated Claims  \n(1) The fact that fixed perturbation magnitudes exacerbate CO has been thoroughly studied in prior work [1,2]. It is already well-known that larger perturbations easily lead to CO [3,4]. This is not a “previously overlooked phenomenon” as the authors claim.  \n(2) The paper argues that existing measures incur a high computational cost. But [4, 5] already provides a time-efficient CO indicator without extra backwards.\n2.\tWeak Novelty Foundation  \n(1)\tThe “epsilon overfitting” part merely repeats existing research on decision boundary distortion. Many works already explain and measure distorted boundaries. The more important insight is to explain: how does boundary distortion dynamically during training, and what causes the spontaneous and initial onset of distortion.  \n(2)\tPertAlign is fundamentally the same as GradAlign, with only a different radius for landscape measurement. Moreover, various efficient landscape-measurement techniques already exist [5]. \n3.\tEffectiveness of the Method  \n(1)\tMost machine learning method requires hyperparameter searching per dataset and architecture. Using “universal hyperparameters” is beneficial but not an excuse to deny baseline performance.  The authors should ensure fair comparisons by tuning baselines on different dataset and architecture, such as SENet and PathMNIST.  \n(2)\tThe method reduces CO by dynamically decreasing step-size, which raises the concern that it simply converges to a trivial solution where the perturbation strength is too small to trigger CO. To rule out this trivial solution, the authors must verify performance under more challenging perturbation budgets (e.g., 32/255), since N-FGSM maintains robustness at 16/255.\n\n[1] Understanding Catastrophic Overfitting in Single-step Adversarial Training  \n[2] Fast Adversarial Training with Adaptive Step Size  \n[3] Fast is better than free: Revisiting adversarial training  \n[4] Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples Regularization  \n[5] Efficient local linearity regularization to overcome catastrophic overfitting"}, "questions": {"value": "Refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lh9Tu1R0i0", "forum": "u1lUbEDQsU", "replyto": "u1lUbEDQsU", "signatures": ["ICLR.cc/2026/Conference/Submission7319/Reviewer_Xi2k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7319/Reviewer_Xi2k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636964051, "cdate": 1761636964051, "tmdate": 1762919428947, "mdate": 1762919428947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the catastrophic overfitting issue in accelerated adversarial training and propose a metric called PertAlign to predict  CO in advance. PertAlign is based on the second order analyses on the input loss landscape and can be used to adaptive choose the step size when applying FGSM in adversarial training. The authors provide both theoretical motivations and the experimental results to validate the method (SORA) proposed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "++ PertAlign is theoretically motivated and is a low-cost indicator predicting CO in advance.\n\n++ The method is simple and in a plug-and-play manner.\n\n++ The experiments are relatively comprehensive on various models and datasets. The time / memory scatter shows SORA in competitive among fast adversarial training methods."}, "weaknesses": {"value": "I have the following concerns about this paper:\n\n1. Limited novelty: the relationship between the craggy loss landscape and catastrophic overfitting is actually not new. \n\n2. The motivation of PertAlign is based on the second order approximation (Lemma 4.1). However, the step size $\\alpha$ is generally quite large in one-step adversarial training, sometimes it can even be larger than $\\epsilon$, I am not sure if the higher-order terms in the proof of Lemma 4.1 can be ignored. If not ignored then the approximation will not be accurate.\n\n3. While the experiments are comprehensive, the performance improvement on AA, the most reliable evaluation, is very marginal (From Table 7 to 24). In most tables, the performance difference between SORA and the strongest baseline among fast adversarial training methods, is very small and smaller than the performance variance.\n\n4. The experiments focus on the $l_\\infty$ attacks and image classification problems, it would be better to include $l_2$ or $l_1$ perturbations and other tasks. In addition, the SORA pseudo-code seems to consider $l_\\infty$ perturbations only, is it applicable to $l_2$ or $l_1$ cases? If not, what modifications do we need to make?\n\n5. In addition to Table 3, more ablation studies are expected, such as sweeping the values of new hyper-parameters ($\\alpha_max$, $\\alpha_0$, $\\beta$ in SORA's pseudo-code)\n\nMinor issues:\n\n1. Some missing literature:\n\n    * About second-order curvature in adversarial training: \"Robustness via Curvature Regularization\" (CVPR 2019)\n    * About fast adversarial training or geometry: \"YOPO: You only propagate once\" (NeurIPS 19), \n\n2. In Table 5's caption: \"The values in each cell correspond to clean, FGSM and PGD-10 accuracies.\" while I see only two results per cell."}, "questions": {"value": "Please address the questions in the weakness section:\n\n1. What is the key difference between PertAlign and existing work? (incl. the missing literature mentioned) If using second-order approximation is the key, then it would be better to demonstrate how tight the second order approximation is in the input loss landscape.\n\n2. Why the improvement is marginal? What is the practical advantages of the proposed method?\n\n3. The method's performance on tasks other than image classification.\n\n4. The adaptation of the proposed methods to $l_2$ and $l_1$ cases and the corresponding experimental results.\n\n5. More ablation studies on the newly introduced hyper-parameters."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U7Q66neuZ1", "forum": "u1lUbEDQsU", "replyto": "u1lUbEDQsU", "signatures": ["ICLR.cc/2026/Conference/Submission7319/Reviewer_ypja"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7319/Reviewer_ypja"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762422467332, "cdate": 1762422467332, "tmdate": 1762919428465, "mdate": 1762919428465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}