{"id": "o88RP8Xqy5", "number": 11837, "cdate": 1758204177668, "mdate": 1762930863523, "content": {"title": "T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models", "abstract": "Recent advances in text-to-video (T2V) diffusion models have significantly enhanced the quality of generated videos. However, their capability to produce explicit or harmful content introduces new challenges related to misuse and potential rights violations. To address this newly emerging threat, we propose unlearning-based concept erasing as a solution. First, we adopt negatively-guided velocity prediction fine-tuning and enhance it with prompt augmentation to ensure robustness against prompts refined by large language models (LLMs). Second, to achieve precise unlearning, we incorporate mask-based localization regularization and concept preservation regularization to preserve the model's ability to generate non-target concepts. Extensive experiments demonstrate that our method effectively erases a specific concept while preserving the model's generation capability for all other concepts, outperforming existing methods.", "tldr": "We propose T2VUnlearning, the first robust and precise method for unlearning specific concepts in text-to-video models.", "keywords": ["Text-to-video", "Machine Unlearning", "Concept Erasure", "Diffusion Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3dfd4e4811a813d29d3f8eeabed53c95959237fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an approach to erase concept from T2V models. They do so using a combination of negatively guided velocity predictions, mask based location and a preservation loss on unrelated concepts. They show results on nudity, object and faces."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors have adopted various T2I concept erasure techniques into T2V models and show that it works. \n- They show promising results on various tasks including nudity, objects and faces. \n- The paper is generally well written and I appreciate that the authors have openly cited works from which they have borrowed particular ideas."}, "weaknesses": {"value": "- The authors seem to be making a distinction between unlearning methods for T2I vs T2V diffusion models. Most methods proposed for T2I models could directly also be applied to T2V models. Thus I think claims such as \"we are the first to propose an unlearning-based concept erasing method for T2V models\" need to be revisited. Especially given methods such as SAFREE have already shown generalizability to T2V models. \n- The proposed method is an adaptation of ESD and Receler for T2V models. \n- The baseline comparisons are lacking. While the paper proposes a weight update based method it only compares itself to inference based methods in Table 1. \n- Since there are so many red-teaming efforts now to bypass concept erasure, I am particularly interested in seeing how this performs on that but this is currently missing from the paper."}, "questions": {"value": "- How does the method perform with multi-concept erasure? \n- Which Ring-A-Bell dataset have you used? Is it specific to T2V models also does it contain adversarial prompts like the original version or the chat-gpt prompts like the version from SAFREE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5SnVAWYknY", "forum": "o88RP8Xqy5", "replyto": "o88RP8Xqy5", "signatures": ["ICLR.cc/2026/Conference/Submission11837/Reviewer_n2NF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11837/Reviewer_n2NF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760987001753, "cdate": 1760987001753, "tmdate": 1762922857358, "mdate": 1762922857358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "XtAK12xUsK", "forum": "o88RP8Xqy5", "replyto": "o88RP8Xqy5", "signatures": ["ICLR.cc/2026/Conference/Submission11837/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11837/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762930163748, "cdate": 1762930163748, "tmdate": 1762930163748, "mdate": 1762930163748, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a negatively-guided framework for concept unlearning in text-to-video (T2V) diffusion models. The method extends text-to-image (T2I) unlearning strategies to the video domain by combining negative velocity guidance, temporal masking, and LLM-based prompt augmentation. Experimental results on nudity and identity removal are presented.\n\nWhile the problem of safe and controllable video generation is important, the proposed approach suffers from several fundamental issues in task formulation, methodological choice, and experimental evaluation. Overall, I find the technical novelty and empirical validation insufficient for acceptance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Addresses an emerging and relevant topic — safety and concept erasure in T2V generation.\n- Attempts to explore an unlearning perspective beyond simple prompt filtering.\n- Provides qualitative examples and limited user study."}, "weaknesses": {"value": "1.\tUnreasonable setting and problematic methodology.\nThe key distinction between T2V and T2I lies in temporal consistency. Video-level concept unlearning could naturally exploit temporal priors such as keyframe guidance or inversion-free conditioning. In contrast, the proposed negative-guidance approach acts purely on the noise/velocity level, which is not a principled choice for video data. This design inherits known instability and over-suppression issues from prior image-level works, leading to numerous ad-hoc “tricks” to mitigate fundamental flaws rather than solving them.\n\n2.\tUnconvincing and low-quality results.\nThe qualitative results are not persuasive.\n- Targeted unlearning often degenerates into object removal (e.g., Fig.8 removes the entire person instead of only nudity).\n- Even after multiple auxiliary tricks, the model still forgets irrelevant semantics.\n- For non-targeted unlearning, results remain almost identical (especially in faces), suggesting weak control ability.\n\n3.\tBiased and insufficient user study.\nThe user study questions are likely leading and lack fine-grained criteria. For example, “nudity removal” should evaluate whether the person remains but becomes clothed, not whether the person disappears. The study therefore cannot reliably support the claimed effectiveness.\n\n4.\tAbsence of video results.\nAs a T2V work, it is unacceptable that no video samples or temporal metrics are shown. The absence of video demonstrations raises strong suspicion that the model’s tricks harm temporal coherence and visual creativity. Quantitative metrics and video demos are essential.\n\n5.\tQuestionable LLM-based prompt augmentation.\nLLM-generated text augmentation is not necessarily aligned with the T2V text-encoder embedding space, and may bring LLM-specific fitting effects. Robustness against different LLMs or jailbreak prompts is untested. A more principled and robust approach would be embedding-space adversarial perturbation rather than text-level augmentation.\n\n6.\tMitigation tricks leading to trivial solutions.\nThe added preservation regularizers and masks appear to suppress removal artifacts but actually encourage trivial “erase-the-object” solutions, harming fine-grained unlearning quality.\n\n7.\tMissing comparisons to keyframe-guided or training-free baselines.\nSince the goal is unlearning without retraining, it is crucial to compare against inversion-free or keyframe-based editing approaches to justify the proposed methodological path. Without this, the claimed advantage over simpler alternatives remains unsubstantiated."}, "questions": {"value": "1.\tCan you provide video samples and temporal quality metrics?\n\n2.\tHow does your method compare to keyframe-guided or training-free editing for concept removal?\n\n3.\tHave you tested LLM augmentation across different LLMs or using adversarial embedding perturbation?\n\n4.\tWhat measures were taken to ensure the user study questions were unbiased?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "o5K0wQgWhM", "forum": "o88RP8Xqy5", "replyto": "o88RP8Xqy5", "signatures": ["ICLR.cc/2026/Conference/Submission11837/Reviewer_HJ6n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11837/Reviewer_HJ6n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761112769351, "cdate": 1761112769351, "tmdate": 1762922856956, "mdate": 1762922856956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes T2VUnlearning, an adapter-based fine-tuning approach to erase undesirable concepts from text-to-video (T2V) diffusion models while preserving non-target capabilities. The method combines:\n(1) negatively-guided velocity prediction (a reparameterization of score-based negative guidance) enhanced with LLM-based prompt augmentation;\n(2) mask-based localization that extracts concept masks from text–video regions in full-attention QK maps to spatially constrain updates; and\n(3) concept-preservation regularization (inspired by prior-preservation in DreamBooth) to mitigate catastrophic forgetting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Comprehensive evaluation: multiple model families, diverse prompt distributions (including human-written), and a mix of automatic and human studies. The SafeSora and VBench analyses are appropriate, and the face-erasure study is a challenging stress test."}, "weaknesses": {"value": "1. VBench Object Class and Subject Consistency are valuable, but broader “video quality” and “text-video alignment” metrics (e.g., aesthetic/FLA, motion fidelity, temporal consistency beyond a single metric) could reveal subtle degradations post-unlearning.\n2. The augmentation is crafted to mirror T2V training prompts; robustness to adversarial or diverse paraphrases outside the LLM’s style remains uncertain. Reporting performance under adversarially optimized prompts (beyond long/refined ones) would be useful.\n3. The thresholding strategy for QK-based masks appears heuristic. Sensitivity to thresholds, layer selection, and temporal consistency of masks is underexplored. Failure modes when concepts are diffuse (styles, activities, or abstract attributes) are not deeply analyzed.\n4. Although adapters are lightweight, the reported 300–500 epochs per target concept may still be costly for practitioners who need to erase many concepts or maintain fleets of models. Comparisons to training-free methods [1,2] in wall-clock and energy would help position the method.\n5. Choosing a single “preserve” concept (e.g., “person” for nudity) is intuitive, but may not cover broader semantic neighborhoods. It is unclear how well this scales when multiple related concepts need protection simultaneously. This is also discussed in ANT [3].\n6. Potential regrowth or circumvention: The paper does not study whether erased concepts re-emerge after continued fine-tuning on benign data, or whether jailbreak-style prompting can bypass the erasure boundary.\n7. The number of compared baselines is too limited. It would be beneficial to include some classic methods (such as MACE [4] and AC [5]) to make the comparison more comprehensive.\n8. Can the proposed method be applied to the FLUX model, similar to what EraseAnything [6] does?\n\n[1] SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation\n\n[2] Concept Corrector: Erase concepts on the fly for text-to-image diffusion models\n\n[3] Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts\n\n[4] MACE: Mass Concept Erasure in Diffusion Models\n\n[5] Ablating Concepts in Text-to-Image Diffusion Models\n\n[6] Enabling Concept Erasure in Rectified Flow Transformers"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DJuCiph4eu", "forum": "o88RP8Xqy5", "replyto": "o88RP8Xqy5", "signatures": ["ICLR.cc/2026/Conference/Submission11837/Reviewer_VAQJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11837/Reviewer_VAQJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570708075, "cdate": 1761570708075, "tmdate": 1762922856502, "mdate": 1762922856502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes T2VUnlearning for text-to-video diffusion using DiT/MMDiT backbones. This method involves inserting lightweight adapters after full-attention layers and fine-tuning them via negatively-guided velocity prediction coupled with mask-based localisation regularisation and concept-preservation regularisation. LLM-based prompt augmentation is also employed to enhance robustness against prompt reformulations. Experiments on CogVideoX-2B/5B and HunyuanVideo demonstrate the effective suppression of targeted concepts (e.g. nudity, object categories and celebrity/identity) while preserving non-target content to a large extent."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Problem significance. The systematic transfer of concept unlearning from T2I to T2V addresses pressing safety and compliance needs in generative video.\n2. Engineering practicality: Plug-and-play adapters are applicable to multiple public T2V backbones, and the approach appears deployment-friendly.\n3. Relatively comprehensive evidence. It covers three sensitive concept families, multiple models and metrics, plus ablations and a user study.\n4. Clear localisation idea. It uses QK interactions with full attention to approximate text–visual alignment regions, thereby reducing unintended forgetting outside the target area."}, "weaknesses": {"value": "1. The paper criticises the reliance on LLM-refined prompts for inference in prior SOTA for undermining defences, yet later adopts LLM-based prompt augmentation in training. Please clarify the distinction in terms of stage, objective and risk, and articulate the novel contribution beyond a combination of known components.\n2. Nudity evaluation largely depends on a single detector (e.g. NudeNet), so bias/misclassification may influence conclusions. There is a lack of multidetector agreement or small-scale human calibration.\n3. The mask relies on text–visual interactions requiring full attention, and its robustness to sparse/block attention, multi-scale attention or variant implementations has not been demonstrated."}, "questions": {"value": "Have you tried multi-detector voting or small human-labeled subsets to validate the stability of the Nudity Rate? Please quantify how much the main conclusions change under different detectors/thresholds."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ewzfq35LlI", "forum": "o88RP8Xqy5", "replyto": "o88RP8Xqy5", "signatures": ["ICLR.cc/2026/Conference/Submission11837/Reviewer_gWkN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11837/Reviewer_gWkN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881154998, "cdate": 1761881154998, "tmdate": 1762922856054, "mdate": 1762922856054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}