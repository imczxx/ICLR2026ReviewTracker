{"id": "GwHtOZsBRV", "number": 4830, "cdate": 1757773824857, "mdate": 1759898010635, "content": {"title": "LAVA: A UNIFIED FRAMEWORK FOR FINETUNING LANGUAGE AND VISION MODELS", "abstract": "LoRA and its variants have attracted considerable attention because of their abilities to tune negligible number of parameters while achieving comparable down-streaming performances. This success is largely attributed to the intrinsic low-rank structure of model parameter spaces, which allows LoRA to train two projection matrices to project weights into a low-dimensional subspace and then map them back. However, it does not consider how to explore this low-rank subspace sufficiently and may lose the expression ability accordingly. Moreover, when using LoRA to tune convolution layers, flatten operation is required to convert tensors into matrices. We argue that this will degrade the model's performance. In this paper, we address this issue from a general parameter sub-space perspective: we present a unified $\\textbf{L}$anguage $\\textbf{A}$nd $\\textbf{V}$ision $\\textbf{A}$daption finetuning framework (called $\\textbf{LAVA}$). Specifically, we verify the existence of low-rank subspaces in convolution layers empirically and propose to parameterize the increment of both convolution kernels and matrices as sum of learnable rank-1 components. To improve training stability, we analyze the optimization dynamics of LoRA and incorporate orthogonal regularization into our parameterization, for which we give theoretical proof that it will help reduce the variance of gradient. We conduct various experiments on different downstreaming tasks to validate LAVA's superiority. For example, when tuning LLaMA2-7b for commonsense tasks, the performance of our LAVA is $\\textbf{+1.9}$% higher than that of LoRA. For metric depth estimation tasks, LAVA only tunes $\\sim$1.5\\% of Depth-Anything\\textsubscript{large} (335.3M), and achieves $\\textbf{+3.1}$% $\\delta_1$ accuracy against that of LoRA and $\\textbf{+5.6}$% $\\delta_1$ accuracy against that of SVDiff.", "tldr": "A Unified Language and Vision Adaption Finetuning Framework", "keywords": ["Finetuning; Large Language Models; Large Vision Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/880a4b75a5263cb45b55626e0ad75230be7a52e8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a parameter-efficient finetuning (PEFT) method named LAVA, which applies Tucker and CANDECOMP/PARAFAC decompositions to reparameterize the weights in convolutional layers. This design preserves the original weight shapes and structural properties of convolutional filters. To enhance training stability, the authors further introduce an orthogonal regularization term in the parameterization. Experiments are conducted on a variety of NLP and vision benchmarks, including language understanding, commonsense reasoning, and image segmentation, showing (limited) improvements over several baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The main idea is interesting: decomposing convolutional weights while maintaining their structural form is non-trivial and practically meaningful.\n\nIncorporating orthogonal regularization into the decomposition framework is a reasonable design choice that could potentially improve training stability and generalization."}, "weaknesses": {"value": "1. In Equation (1), the variables *x* and *y* are not explicitly defined, which makes the formulation hard to follow.\n2. The decomposition described in Section 3 is somewhat unconventional. Typically, for convolutional weights $W'$, the decomposition is applied over dimensions $c_{out}$ and $c_{in} \\times h \\times w$, rather than $c_{out} \\times h$ and $c_{in} \\times w$. The motivation and benefit of this alternative factorization are unclear, and this design choice makes the comparisons in Section 3 less convincing.\n3. The experiments use **LLaMA-2-7B** as the main backbone. Given the availability of more recent models (e.g., LLaMA-3, Mistral, or Gemma), this weakens the empirical support for the claimed generality and advancement of LAVA.\n4. The paper claims that orthogonal regularization improves training stability, but there are no explicit experiments or analyses (e.g., training curves or variance metrics) to support this claim.\n5. The paper does not compare against several relevant and recent PEFT methods, such as **FLoRA**, **DoRA**, and **Conv-LoRA**. Tables 2 and 3 omit many state-of-the-art baselines, making it difficult to assess the true contribution and competitiveness of LAVA."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CWwW6Nx7nQ", "forum": "GwHtOZsBRV", "replyto": "GwHtOZsBRV", "signatures": ["ICLR.cc/2026/Conference/Submission4830/Reviewer_1TBd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4830/Reviewer_1TBd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630694242, "cdate": 1761630694242, "tmdate": 1762917600682, "mdate": 1762917600682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LAVA proposes a unified, parameter-efficient fine-tuning framework aimed at addressing two core issues in existing LoRA-based methods for vision and language tasks:\n\nInsufficient exploration of low-rank subspace: the optimization process of LoRA may lead to redundant dimensions\n\nImproper handling of convolution layers: flattening operations destroy spatial encoding properties\n\nKey contributions include:\n\nTensor-factorization perspective: parameterizes convolutional kernel updates as a sum of rank-1 tensor components, preserving full dimensional integrity\n\nOrthogonal regularization: theoretically shown to reduce gradient variance and stabilize training\n\nUnified framework: applicable to both attention mechanisms and convolutional networks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Conducts the first systematic analysis of low-rank subspace properties in convolution layers within the PEFT paradigm\n\nProvides rigorous theoretical proofs for orthogonal regularization (Theorem 1 & Proposition 1)\n\nEstablishes the mathematical connection that shows LoRA is a special case of LAVA\n\nCovering NLU, commonsense reasoning, semantic segmentation, depth estimation, and text generation"}, "weaknesses": {"value": "The evaluation does not include several recent and competitive PEFT baselines, such as LoRA+, VeRA, DoRA, and NoRA.\n\nThe computational efficiency analysis is incomplete, as it lacks direct comparisons of training time and memory usage.\n\nSuggestion: include comparisons with more state-of-the-art methods in the revised version."}, "questions": {"value": "same as above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dSFR7njOBt", "forum": "GwHtOZsBRV", "replyto": "GwHtOZsBRV", "signatures": ["ICLR.cc/2026/Conference/Submission4830/Reviewer_mXqN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4830/Reviewer_mXqN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805292293, "cdate": 1761805292293, "tmdate": 1762917600218, "mdate": 1762917600218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LAVA (Language And Vision Adaption), a unified framework for the parameter-efficient fine-tuning of large models. The authors identify two primary limitations with the widely-used LoRA method: (1) Subspace Redundancy, where unconstrained training can lead to correlated, inefficient representations in the low-rank update, and (2) Dimension Disorder, where applying LoRA to convolutional layers requires flattening tensors into matrices, thereby disrupting the inherent spatial structure of the weights.\n\nLAVA addresses these issues by introducing (1) a generalized subspace-based adaptation that handles high-order tensors (like convolution kernels) directly by parameterizing the weight update as a sum of rank-1 tensors, thus preserving dimensional integrity. This method naturally reduces to the LoRA formulation when applied to matrices. (2) A column-orthogonal regularization term applied to the trainable low-rank matrices. This encourages the basis vectors to be orthogonal, promoting a more complete exploration of the low-rank subspace and, as the authors show theoretically, stabilizing training by reducing the variance of gradients.\n\nThe authors conduct a comprehensive set of experiments across natural language understanding (GLUE), commonsense reasoning (LLaMA2-7B), semantic segmentation (SAM), depth estimation (Depth-Anything), and text-to-image generation (SDXL). Their results consistently show that LAVA outperforms LoRA and other PEFT baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is built on a very clear critique of LoRA. The concepts of dimension disorder for convolutions and dimension redundancy from unconstrained optimization are well-explained, and represent meaningful limitations in current PEFT approaches. \n\nLAVA is a simple but effective framework that simple addresses the identified problems. Using a tensor decomposition-inspired update for convolutions is a natural fit, and extending LoRA with orthogonal regularization is a principled way to improve subspace exploration. \n\nThe empirical evaluation compares LAVA and LoRA across many tasks, further supporting the efficacy of LAVA."}, "weaknesses": {"value": "The core components of LAVA (tensor decomposition [3,4] and orthogonal regularization) are not novel in isolation. Orthogonal constraints are a well-known tool in machine learning for improving training stability and representation quality and have been previously used for PEFT [1, 2]. The paper would be strengthened by a more detailed discussion of related work that has used similar techniques, even outside the direct context of PEFT, to better contextualize its specific contribution. For instance, the distinction from OFT could be sharpened.\n\nThe work lacks comparison (or integration ?) into more recent PEFT alternatives to LoRA [5,6,7] and others. Do these state-of-the-art PEFT approaches also suffer from subspace redundancy and dimension disorder also or are some of these problems already partly addressed. In this case, is LaVA complementary with these existing solutions ?\n\nThe text-to-image generation experiment (Sec 5.5) feels less thorough than the other experimental sections. The evaluation relies primarily on a single qualitative figure in the main paper and one FID score in the appendix. Given the stochastic nature of generation, strengthening this section with more quantitative metrics (e.g., CLIP scores), a user study, or at least more generated examples in the appendix would make the claims in this domain more robust.\n\n[1] Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, and Xuanjing Huang. 2023. Orthogonal Subspace Learning for Language Model Continual Learning. In EMNLP 2023.\n\n[2] Büyükakyüz, K. OLoRA: orthonormal low-rank adaptation of large language models. arXiv preprint arXiv:2406.01775, 2024\n\n[3] Lebedev, Vadim, et al. \"Speeding-up convolutional neural networks using fine-tuned cp-decomposition.\" arXiv preprint arXiv:1412.6553 (2014).\n\n[4] Yifan Yang, Jiajun Zhou, Ngai Wong, and Zheng Zhang. 2024. LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models. In Proceedings of Association for Computational Linguistics: Human Language Technologies, 2024\n\n[5] Edalati, Ali, et al. \"KronA: Parameter-Efficient Tuning with Kronecker Adapter.\" Enhancing LLM Performance: Efficacy, Fine-Tuning, and Inference Techniques. Cham: Springer Nature Switzerland, 2025. 49-65.\n\n[6] Liu, Shih-Yang, et al. \"Dora: Weight-decomposed low-rank adaptation.\"  International Conference on Machine Learning. 2024.\n\n[7] Albert, Paul, et al. \"RandLoRA: Full-rank parameter-efficient fine-tuning of large models.\" ICLR (2025)."}, "questions": {"value": "The orthogonal regularization is applied to only one of the factor matrices (U in Eq. 3) in the convolutional case. What was the rationale for this specific choice? Have the authors experimented with regularizing all factor matrices or a different combination, and how did that affect performance and training stability?\n\nRegarding the commonsense reasoning results (Table 7), do the authors have any hypotheses for why LAVA might underperform LoRA on specific datasets like SIQA and WinoGrande? Is it possible that for some tasks, the unconstrained subspace exploration of LoRA is accidentally beneficial, or is it more likely noise?\n\nCould the authors quantify the computational overhead of the orthogonal regularization term? Does it introduce a noticeable slowdown in training wall-clock time compared to a standard LoRA implementation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n1cqMSXxNH", "forum": "GwHtOZsBRV", "replyto": "GwHtOZsBRV", "signatures": ["ICLR.cc/2026/Conference/Submission4830/Reviewer_6cEr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4830/Reviewer_6cEr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884913618, "cdate": 1761884913618, "tmdate": 1762917599818, "mdate": 1762917599818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes a method (LAVA) for parameter efficient fine-tuning of vision and language models, particularly focused on convolution layers. Given a tensor, it models the low-rank update as the sum of 'r' CANDECOMP/PARAFAC (CP) rank-1 updates. Each rank-1 update is obtained as an outer-product of learnable vectors. When the tensor is a matrix, this is equivalent to LoRA. Unlike typical application of LoRA to convolution layers by reshaping the weight tensor to be a matrix, the proposed CP rank-1 update in LAVA does not require any reshaping and thus better models spatial information. Additionally, the authors propose orthogonal regularization to ensure the update has maximum possible rank and to improve training stability. The proposed method is shown to outperform baseline approaches including LoRA on diverse vision and language tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using the sum of CP rank-1 updates for convolution layers is interesting. It addresses the specific issue of weight reshaping in LoRA adaptation for convolution.\n2. The idea of orthogonal regularization is well motivated. The authors provide theoretical proof for training stabilization due to the proposed regularization.\n3. The experiments include diverse tasks on both vision and language models and the proposed method consistently outperforms both LoRA and other baseline approaches."}, "weaknesses": {"value": "1. The primary contribution of the paper is an effective PEFT method (LAVA) for convolutional layers. However, there is not much discussion or empirical comparison with related PEFT methods focused on convolution like Lora-C [a], Conv-Adapter [b] and LoRAE [c]. Both Conv-Adapter and LoRAE preserve spatial properties in convolution similar to LAVA. Conv-Adapter can be seen as a generalization of LAVA and reduces to LAVA when the learnable 2-D filter is modified to be a separable filter and removing the non-linearity between the depth-wise and point-wise convolution blocks. The experiments are limited with just results with a single dataset and model on depth estimation and image generation tasks. There is no comparison with SOTA approaches on the image generation task. The results on semantic segmentation (table 12 in A.7.2) are not consistent with the existing literature (LoRA consistently outperforms Conv-Lora while Conv-Lora (Zhong et al., 2024) show the opposite on the same datasets).\n2. For language models, the proposed approach reduces to applying orthogonal regularization atop LoRA. More experimental results and analysis is required to understand whether and why this is helpful. For instance, does the regularization lead to a higher rank than that observed in LoRA? Or, is the stabler training the reason for performance improvements? The provided LLM results are on just two datasets with just one model (small RoBERTa model on one, LLaMA-2 on another) for each. While I understand the resource requirements for larger scale experiments, more experiments are required for a stronger comparison between LoRA and the proposed method. Discussion and comparison with a related work OLoRA [d] is missing. OLoRA performs orthonormal decomposition of the weight matrix before performing LoRA updates. \n\nReferences:\n\n[a] Ding, Chuntao, et al. \"LoRA-C: Parameter-Efficient Fine-Tuning of Robust CNN for IoT Devices.\" arXiv preprint arXiv:2410.16954 (2024). \\\n[b] Chen, Hao, et al. \"Conv-adapter: Exploring parameter efficient transfer learning for convnets.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024. \\\n[c] Wang, Zhixue, Hongyao Ma, and Jiahui Zhai. \"Low-rank adaptation for edge AI.\" Scientific Reports 15.1 (2025): 33109. \\\n[d] Büyükakyüz, Kerim. \"Olora: Orthonormal low-rank adaptation of large language models.\" arXiv preprint arXiv:2406.01775 (2024)."}, "questions": {"value": "1. Provide analysis on the rank of learned weight matrices for the language model experiments for both LoRA and LAVA. \n2. The learning rate plays a very important role in LoRA fine-tuning. Since the learning rate is tuned for a particular model and dataset in the 3. LLM experiments, the comparison with LoRA might be unfair. Provide results on language modeling with lr tuning for LoRA. Also, provide details on the dataset split used to perform the lr tuning for LAVA.\n4. Provide empirical comparison with Conv-adapter either on existing tasks in LAVA or on the tasks in [b]. \n5. Provide discussion on training compute and memory for LAVA. For LLMs, does splitting the update into `r` matrices significantly increase the training compute and memory compared to LoRA? How does this scale to larger models and ranks?\nAdd results for multiplier=0 for the plots in Figure 6 (analysis of \\lambda). The value of \\lambda does not seem to significantly affect results on the NLU tasks and a multiplier of value lower than 1 seems to have the best performance. Why would LAVA then perform significantly better than LoRA?\n6. Why are the results for LoRA (encoder+decoder) so much worse than LoRA (encoder) in depth estimation (table 3)? Is it because of the convolutional decoder? Why do we not observe similar degradation in segmentation and image generation tasks? Does this support the use of non-reshaping technique in LAVA? More such experiments on convolution heavy backbones like ResNet would have made the work stronger (not asking for those expts here)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GMmMDQnVWy", "forum": "GwHtOZsBRV", "replyto": "GwHtOZsBRV", "signatures": ["ICLR.cc/2026/Conference/Submission4830/Reviewer_R3Bj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4830/Reviewer_R3Bj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762069893043, "cdate": 1762069893043, "tmdate": 1762917599311, "mdate": 1762917599311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}