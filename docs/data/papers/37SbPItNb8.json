{"id": "37SbPItNb8", "number": 10960, "cdate": 1758185689180, "mdate": 1759897617910, "content": {"title": "Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision-Language Models", "abstract": "Large Vision-Language Models (LVLMs) often suffer from object hallucination, generating text inconsistent with visual inputs, which can critically undermine their reliability. Existing inference-time interventions to mitigate this issue present a challenging trade-off: while methods that steer internal states or adjust output logits can be effective, they often incur substantial computational overhead, typically requiring extra forward passes. This efficiency bottleneck can limit their practicality for real-world, latency-sensitive deployments.\nIn this work, we aim to address this trade-off with **Residual-Update Directed DEcoding Regulation (RUDDER)**, a low-overhead framework that steers LVLMs towards visually-grounded generation. RUDDER is built on two key innovations: (1) Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence vector extracted from the residual update of a self-attention layer during a *single, standard forward pass*. (2) A Bayesian-inspired adaptive gate that performs token-wise injection, applying a corrective signal whose strength is conditioned on the model's deviation from the visual context.\nExtensive experiments on key hallucination benchmarks, including POPE and CHAIR, indicate that RUDDER achieves performance comparable to state-of-the-art methods while introducing negligible computational latency, validating RUDDER as a pragmatic and effective approach for improving LVLMs' reliability without a significant compromise on efficiency.\nCode is available at https://anonymous.4open.science/r/RrUuDdDdER-1C13/.", "tldr": "RUDDER is a low-overhead framework that reduces object hallucination in LVLMs by adaptively applying a corrective signal extracted during a single forward pass, matching the performance of computationally expensive state-of-the-art methods.", "keywords": ["Large Vision-Language Models", "Multimodal Large Language Model", "Hallucination"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c8cdf2d799782367595f2961e01161cf326f2c93.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces RUDDER, an inference-time intervention technique designed to mitigate object hallucinations in large vision-language models (LVLMs) with minimal computational overhead. The method features two core components: (1) the Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence representation derived from residual updates in a self-attention layer during a single forward pass; and (2) a Beta Gate, a Bayesian-inspired adaptive gating mechanism that dynamically steers generation toward stronger visual grounding. Evaluations on hallucination benchmarks (CHAIR and POPE) and general multimodal assessments (MME) across three LVLM architectures demonstrate that RUDDER achieves  hallucination reduction at lower inference costs compared to existing interventions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1). The CARD vector is efficiently extracted from the standard computation pipeline, while the Beta Gate relies on lightweight vector operations, enabling seamless deployment in latency-constrained real-world applications.\n\n2). The ablation studies and illustrative examples are thorough, providing clear insights into the method's mechanics."}, "weaknesses": {"value": "1). The approach shares conceptual similarities with cross-layer methods like DeCo, which integrates early-layer logits into later layers to address visual forgetting, and RUDDER similarly incorporates attention from early generated tokens into later ones. A direct comparison with such methods would strengthen the novelty claims.\n\n2). The method introduces multiple hyperparameters (e.g., $L$, $k$, and $\\alpha_{\\max}$), raising concerns about its stability and generalizability across diverse VLMs.\n\n3). Performance gains on the test sets are modest, and the method's efficacy diminishes as the underlying VLM's capabilities improve. The baselines are somewhat dated, omitting recent mainstream VLMs like Qwen-VL and InternVL, which questions its applicability to current open-source models. Additionally, comparisons with recent training-free hallucination mitigation techniques, such as DeGF and AGLA, are absent, limiting the benchmarking comprehensiveness.\n\n4). The writing could be refined for clarity and conciseness; for instance, the introduction devotes excessive space to reiterating the need for an \"effective and lightweight\" solution and listing RUDDER's components, without adequately highlighting key insights, motivations for each module, or defining terms. The final three paragraphs overlap significantly, resulting in low information density."}, "questions": {"value": "1). Could you include experimental results on contemporary VLMs such as Qwen-VL and InternVL? Additionally, please add comparisons with current SOTA methods like DeGF and AGLA.\n\n2). To facilitate broader adoption, could you provide a concrete recipe for automated hyperparameter tuning? For example, suggest strategies like grid search, Bayesian optimization, or other efficient approaches for optimizing $L$, $k$, and $\\alpha_{\\max}$ in new deployment scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OURzPQnbGZ", "forum": "37SbPItNb8", "replyto": "37SbPItNb8", "signatures": ["ICLR.cc/2026/Conference/Submission10960/Reviewer_1UYi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10960/Reviewer_1UYi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632783670, "cdate": 1761632783670, "tmdate": 1762922156560, "mdate": 1762922156560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RUDDER, a single‑pass inference‑time steering method for LVLMs that (1) extracts a per‑sample direction (CARD) from self‑attention residual updates during prefill and (2) applies a per‑token Beta‑gate to adapt the steering strength during decoding.  On CHAIR and POPE, across three LVLMs and three decoding strategies, RUDDER typically matches or outperforms strong ITI baselines, while keeping ~baseline latency/throughput. General ability on MME is largely preserved"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Low‑overhead (no extra forwards) with clear efficiency gains versus prior steering methods; quantitative latency/throughput reported. \n\n2. Minimal code hooks; works across three distinct LVLM architectures; integrates with standard decoding loops. \n\n3. Token‑wise gate improves precision vs fixed‑strength steering; ablations show why adaptive > fixed for open‑ended captioning. \n\n4. Solid experiments, including cross‑model, cross‑decoding, efficiency measurements, and layer/parameter sweeps."}, "weaknesses": {"value": "1. The “Bayesian‑inspired” gate is heuristic, there is no formal guarantee that the steering always improves negative log likelihood, though ablations suggest it works well.\n\n2. Layer choice is model‑specific (e.g., late layers for LLaVA/Idefics2; early for InstructBLIP with Q‑former), and final configs differ substantially across backbones. Ablations confirm a sensitive trade-off between CHAIR scores and recall, implying non-trivial parameter search per model/task.\n\n3. The evaluation scope is modest, evaluation on more capabilities like MM-Vet would be beneficial.\n\n4. No diagnostic attribution of why corrections happen. The paper shows outcome metrics and some internal geometry analyses but lacks faithfulness diagnostics that could verify that the method truly reduces language-prior reliance rather than suppressing certain token types."}, "questions": {"value": "1. Exactly which tensors are pooled to form CARD (per‑layer, per‑head residual updates after self‑attention, before MLP)? What pooling (mean, median, head‑weighted)?\n\n2. How were g_min, g_max, and softplus temperature chosen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "07C5xxGb1Q", "forum": "37SbPItNb8", "replyto": "37SbPItNb8", "signatures": ["ICLR.cc/2026/Conference/Submission10960/Reviewer_XUvK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10960/Reviewer_XUvK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885759240, "cdate": 1761885759240, "tmdate": 1762922155850, "mdate": 1762922155850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RUDDER, a lightweight inference-time framework to reduce hallucinations in LVLMs with (almost) no extra computational cost. RUDDER extracts a Contextual Activation Residual Direction (CARD) vector from residual updates during a single forward pass to capture visual evidence, and applies an adaptive Beta Gate to modulate correction strength per token based on visual alignment. Experiments on benchmarks like CHAIR and POPE show that RUDDER matches or surpasses SoTA hallucination mitigation methods while maintaining nearly identical inference speed and general multimodal performance, making it a practical solution for real-world LVLM deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Good Writing.** The writing of the paper is clear and easy to follow (although more high-level intuition and motivation can be expressed in a better way). \n- **Extremely Low Computational Overhead.** It's a smart idea to utilize the intermediate results (embeddings, attention heads, etc.) of the pre-filling phase for later steering of the LVLMs. This indeed avoids typical repetitive computation in the contrasting-based methods. The empirical results on the efficiency analysis perfectly supports this. \n- **Extensive Experimental Results.** Experiments are conducted in various evaluation benchmarks on multiple LVLM backbones, supporting the main claims of the paper."}, "weaknesses": {"value": "+ **Lack of (Sometimes Contradictory) Intuitive Explanation for the Proposed Method.** Despite its practical values in terms of performance and efficiency, I find it hard to understand the rationale behind the proposed method: \n  + What is the meaning of the main body of the steering vector $v\\_{\\text{CARD}}$? \n  + Why pooling the token-wise attention output $\\Delta$ is a good idea, not causing too much information loss?\n  + If the similarity score $g$ between the current token's hidden state $h$ and $v\\_{\\text{CARD}}$ is high, then this hidden state already contains lots of visual information. Why would we want to do stronger steering: $v\\_{\\text{steer}}=g \\cdot v\\_{\\text{CARD}}$ in this case? Shouldn't we put more steering on the ones that loses lots of visual information? \n+ **Over-claims about \"Bayesian\".** It's a bit hard to persuade me to believe the gating mechanism is \"Bayesian\". This is a general training-free strategy, no parameters are updated based on new observations. To me this gating mechanism is at most \"adaptive\". \n+ **Sensitive hyperparameters setting.**\n  + This method introduced many hyperparameters, and they are all adjustable: $L$, $\\alpha\\_{\\text{max}}$, $k$, etc. \n  + The hyperparameters are all set differently for different models and different benchmarks, showcasing the sensitiveness of them.\n  + It's not clear how the author found the optimal setting of the hyperparameters. Is it based on an extra validation dataset?"}, "questions": {"value": "See above (Weaknesses)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D2LzcR73dh", "forum": "37SbPItNb8", "replyto": "37SbPItNb8", "signatures": ["ICLR.cc/2026/Conference/Submission10960/Reviewer_5Rux"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10960/Reviewer_5Rux"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944895165, "cdate": 1761944895165, "tmdate": 1762922155144, "mdate": 1762922155144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RUDDER (Residual-Update Directed DEcoding Regulation), a method to mitigate object hallucination in Large Vision-Language Models (LVLMs). The method introduces two key components: (1) the CARD vector, a per-sample visual steering signal extracted at negligible computational cost during the prefill stage, and (2) the Beta Gate, an adaptive token-wise mechanism that dynamically adjusts intervention strength. While the experimental results appear promising, several aspects require clarification and further validation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies the trade-off in existing methods—existing approaches incur high computational overhead and require multiple forward passes, which limits their practical deployment.\n\n2.  The concept of dynamically adjusting intervention strength based on the model's deviation from visual context is well-motivated. \n\n3. The paper is generally well-structured and clearly written, making it accessible to readers."}, "weaknesses": {"value": "1. The Beta Gate design appears fundamentally counter-intuitive. According to Equation (3):\n\n- When hl,t has high similarity with vCARD (i.e., cos⁡(hl,t, vCARD)≈1), the intervention strength g_t becomes large.\n- When hl,t derivates from vCARD (i.e., cos⁡(hl,t, vCARD) is negative), the intervention strength g_t becomes small.\n- This design contradicts common intuition: one would expect stronger intervention when the model deviates from visual grounding, not weaker. The paper does not adequately justify this seemingly backward design choice.\n\n\n2. While the paper claims Beta Gate is \"Bayesian-inspired,\" it lacks rigorous Bayesian derivation. The connection between the Beta distribution framework and the specific formulation in Equation (3) is unclear.\n\n3. The paper omits several state-of-the-art hallucination mitigation methods. Missing references and performance comparisons (including effectiveness and efficiency): OPERA (Huang et al., 2023), HALC (Chen et al., 2024), ADHH (Yang et al., 2025). \n\nOPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation. CVPR 2023\n\nHALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding. ICML 2024.\n\nUnderstanding and Mitigating Hallucinations in Large Vision-Language Models via Modular Attribution and Intervention. ICLR 2025."}, "questions": {"value": "1. How are k (sensitivity) and c (concentration) determined? Why they are necessary?\n\n2. Do hyperparameters need adjustment for different models (e.g., 7B vs. 13B)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uxCw7IdbhF", "forum": "37SbPItNb8", "replyto": "37SbPItNb8", "signatures": ["ICLR.cc/2026/Conference/Submission10960/Reviewer_56bn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10960/Reviewer_56bn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065382453, "cdate": 1762065382453, "tmdate": 1762922154457, "mdate": 1762922154457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}