{"id": "nVJ43d0g1g", "number": 20009, "cdate": 1758301458545, "mdate": 1759897006488, "content": {"title": "Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search", "abstract": "Large Language Models (LLMs) have achieved significant advances in reasoning tasks. A key approach is tree-based search with verifiers, which expand candidate reasoning paths and use reward models to guide pruning and selection. \nAlthough effective in improving accuracy, these methods are not optimal in terms of efficiency: they perform simple decomposition on the reasoning process, but ignore the planning-execution nature of tasks such as math reasoning or code generation. This results in inefficient exploration of reasoning process. To address this, we propose a dual-phase test-time scaling framework that explicitly separates reasoning into planning and execution, and performs search over the two phases individually.\nSpecifically, we decompose reasoning trajectories and develop reward models for each phase, enabling the search to explore and prune plans and executions separately. We further introduce a dynamic budget allocation mechanism that adaptively redistributes sampling effort based on reward feedback, allowing early stopping on confident steps and reallocation of computation to more challenging parts of the reasoning process. Experiments on both mathematical reasoning and code generation benchmarks demonstrate that our approach consistently improves accuracy while reducing redundant computation.", "tldr": "", "keywords": ["LLM Reasoning", "Test Time Scaling", "Chain-of-Thought"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f7bdc0655622ce795fc42f6cf2882f32f8d6680a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work explicitly decompose the tree-based search steps into planning and execution phases, and use a reward model to guide pruning the selection. It also introduces a dynamic budget allocation mechanism to allocate the sampling effort for each search step. Experimental results demonstrate consistent improvement on both math and coding tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method sounds reasonable for the reasoning tasks\n2. Comprehensive experiments are performed, with various tasks and base models, at a wide range of token budgets. The results are also promising."}, "weaknesses": {"value": "1. The main issue lies in the lack of technical contribution. In short, the only novel part of this work lies in the proposal to explicitly split the search step into planning and decomposition phases. But why it would work, what is the nature of planning and execution from the machine learning perspective, how the paradigm could be generalized to other tasks, seem to be unclear. The authors leave all these questions unanswered, leaving very few takeaways from the reader side. \n2. The proposed method contains a bag of tricks, but the ablation study does not perform a careful evaluation to verify which component makes the main contribution. \n3. There are too many heuristic designs, such as the width and budgets. All these factors make the proposed method hard to generalize."}, "questions": {"value": "1. How the model would behave when no explicitly decomposing the search into planning and execution (paradigm Figure 3(c))?\n2. How do you instruct the model to do planning and execution? Have you trained the model to do it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0ePHfcD5LJ", "forum": "nVJ43d0g1g", "replyto": "nVJ43d0g1g", "signatures": ["ICLR.cc/2026/Conference/Submission20009/Reviewer_B7cR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20009/Reviewer_B7cR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761283624744, "cdate": 1761283624744, "tmdate": 1762932912284, "mdate": 1762932912284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes the DREAM method, which guides large language models to decouple planning and execution steps during reasoning, and to select each step using its corresponding reward model. In addition, a dynamic sampling method is designed to improve the efficiency of reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. By explicitly separating the planning and execution steps, DREAM is expected to further improve sampling efficiency with the help of the PRM.\n\n2. The proposed adaptive sampling budget allocation strategy is reasonable."}, "weaknesses": {"value": "1. The proposed method has limited application scenarios: on one hand, it requires multiple specialized reward models to provide rewards for plan and execution respectively. This adds extra burden to both training (especially considering the need to construct specific datasets) and inference deployment, making practical application difficult. On the other hand, it can only be applied to tasks that can be formatted into plan and execution stages.\n\n2. The proposed idea is not very innovative — essentially, it just breaks down the original steps into finer ones. More simply, even without formatting outputs into plan and execution, one could instruct the model to generate smaller reasoning steps to achieve more controllable, stage-guided generation."}, "questions": {"value": "What I’m more concerned about is: where exactly is the boundary for step decomposition? Or can the steps be continuously subdivided into even smaller units? What additional problems would finer decomposition bring? And how should we account for the computational cost of the reward model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "czgbWaIlw7", "forum": "nVJ43d0g1g", "replyto": "nVJ43d0g1g", "signatures": ["ICLR.cc/2026/Conference/Submission20009/Reviewer_S4PF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20009/Reviewer_S4PF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725253723, "cdate": 1761725253723, "tmdate": 1762932911187, "mdate": 1762932911187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **DREAM**, a framework that explicitly decomposes reasoning into two phases—planning and execution. Separate reward models are trained for planning and for execution to conduct independent tree-structured search and pruning. The authors further propose a dual-threshold adaptive compute budget: terminate early when any candidate surpasses an upper threshold to save computation, and trigger additional sampling when all candidates fall below a lower threshold, thereby reallocating compute from easy steps to harder ones and improving the accuracy–efficiency trade-off. On mathematical reasoning and code generation tasks, DREAM demonstrates strong cross-model and cross-dataset generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly delineates two major shortcomings of current tree-search-based test-time scaling (TTS): (i) planning and execution are evaluated jointly, and (ii) the step budget is fixed. It proposes DREAM, which decouples reasoning into planning and execution phases, equips each phase with a dedicated reward model, and applies dynamic step-level budgeting, thereby reducing ineffective exploration and focusing computation on critical steps."}, "weaknesses": {"value": "1. Lacks theoretical and empirical analysis of the thresholds (e.g., $\\tau_{p1}$ and $\\tau_{p2}$), which are likely to strongly affect resource allocation and accuracy. Paper does not analyze their impact.\n\n2. Experimental details are insufficient. The settings for $\\tau_{p1}$, $\\tau_{p2}$, $m_1$, and $m_2$ are missing, making the experiments difficult to reproduce.\n\n3. Missing evaluations on more challenging mathematical reasoning datasets and stronger reasoning models, which makes it hard to assess the method’s generalization.\n\n4. Typo: in Figure 2, the “Plan–Execution Format” contains two instances of “Q2.”"}, "questions": {"value": "1. In the main results, REBASE underperforms Beam Search on mathematical datasets (especially MATH), despite achieving better compute allocation. Please explain the reasons for this discrepancy.\n\n2. DREAM verifies each step in two phases. Does this increase test-time compute? Please provide a detailed test-time compute report, including FLOPs for the policy model and the PRM, as well as end-to-end inference latency.\n\n3. Is the setting $\\tau_{p1}=\\tau_{e1}$ reasonable? The planning and execution phases differ substantially in sampling distribution and inter-sample dependency, raising concern that DREAM may simply split a step and apply finer-grained rewards without truly distinguishing between planning and execution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xy5YBVTor0", "forum": "nVJ43d0g1g", "replyto": "nVJ43d0g1g", "signatures": ["ICLR.cc/2026/Conference/Submission20009/Reviewer_7wYR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20009/Reviewer_7wYR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884827813, "cdate": 1761884827813, "tmdate": 1762932910257, "mdate": 1762932910257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}