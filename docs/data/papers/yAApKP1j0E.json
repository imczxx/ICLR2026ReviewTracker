{"id": "yAApKP1j0E", "number": 16626, "cdate": 1758266932180, "mdate": 1759897228785, "content": {"title": "COMPOTE: Generating a Dataset of Real-World Binary Level Vulnerabilities", "abstract": "Once a proprietary program written in a compiled language like C is successfully compiled, it is typically distributed as a binary executable. Consequently, security analysis of the program, including vulnerability detection, relies solely on the binary. Binary-level detection methods have been developed over the years, with machine learning (ML)-based methods becoming increasingly popular in the last decade. However, the scarcity of high-quality, publicly available datasets limits the development of ML-based binary vulnerability detectors, as existing binary-level vulnerability datasets are often synthetic and fail to reflect real-world vulnerabilities. At the same time, existing real-world source-code vulnerability datasets cannot be directly compiled, as they typically consist of standalone function snippets rather than compilable programs. To address this limitation, we present Compote, a COMPilation AIâ€‘Orchestrated Transformation Engine that automatically wraps standalone C functions with the minimal scaffolding, such as headers, mocks, and main(), needed for successful compilation of C functions without altering the original code. Applying Compote to real-world functions from ten public datasets of vulnerable code yields a dataset comprising 18K compilable C functions along with their compiled binary versions. Our dataset represents a novel, large-scale, realistic, labeled benchmark spanning both source and binary domains. To evaluate our dataset, we fine-tune state-of-the-art vulnerability detection models. We show that models trained and tested exclusively on existing (synthetic) datasets achieve up to 98.97% F1 but drop to 29.28% when tested on the real-world vulnerabilities in our dataset. This demonstrates the inability of models trained on synthetic datasets to generalize effectively to real-world binary vulnerabilities, resulting in a significant drop in detection performance. We release Compote and our datasets to the research community to support further research on building and evaluating effective and practical binary vulnerability detection models.", "tldr": "We present Compote, an AI tool that converts real-world vulnerable C code into compilable binaries, enabling CompRealVul a large-scale, realistic dataset for training and evaluating binary-level vulnerability detection models.", "keywords": ["AI", "LLM", "Vulnerability Detection", "Binary-Level", "Cybersecurity", "Code Generation", "Dataset Creation", "LLVM-IR", "Compilation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58d09c3f2ad39d506483e1ec9e6f282b4dd6bfd0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the Compote tool and CompRealVul dataset to address the lack of high-quality real-world data in binary vulnerability detection. Through AI-driven code completion technology, Compote can automatically convert non-compilable C code fragments into compilable programs and generate corresponding binary files. This work not only fills the gap in real benchmark data for binary vulnerability detection but also reveals the limitations of current detection technologies in real scenarios, providing important datasets and methodological references for future research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's core contribution lies in filling the long-standing gap of real-world data in binary vulnerability detection. Through innovative AI-assisted compilation technology (Compote), it transforms fragmented source code vulnerability datasets into compilable, executable binary samples. This not only solves the practical engineering challenge of \"source code unavailability, only binaries available\" for detection, but also rigorously reveals the generalization gap between synthetic data and real vulnerabilities through experiments. It provides data support and methodological inspiration for subsequent research, demonstrating significant practical value and academic significance."}, "weaknesses": {"value": "The study presents several critical issues requiring further clarification and improvement. Primarily, while the authors innovatively proposed the Compote tool for processing non-compilable code, there lacks systematic review of existing similar work. In binary vulnerability research, have other teams attempted alternative approaches (e.g., manual completion or semi-automatic conversion) for such data? This necessitates citation and comparative analysis of relevant literature. Secondly, the code completion process may alter the triggering conditions of original vulnerabilities, particularly for multi-threaded or memory-operation vulnerabilities relying on complex contexts. The authors should provide more rigorous verification methods (e.g., dynamic testing or symbolic execution) to demonstrate the semantic integrity of vulnerabilities in completed code. Thirdly, regarding dataset construction, CompRealVul essentially derives from existing datasets, with its innovation mainly reflected in the compilation toolchain rather than the data per se. This \"reprocessing\" paradigm requires clearer positioning in academic contribution evaluation. Furthermore, the experimental design shows notable shortcomings: horizontal comparisons are limited to internal models, failing to benchmark against state-of-the-art code completion systems (e.g., GitHub Copilot or CodeLlama); the baseline models also exclude newer architectures proposed in 2023-2024. Most crucially, certain experimental results (e.g., accuracy degradation) contradict the paper's central claims, demanding deeper analysis to explain these discrepancies. We recommend supplementing more comprehensive comparative experiments and robustness tests to strengthen the conclusions. The authors should particularly address how their method maintains vulnerability authenticity during code completion, and how the compiled binaries compare with manually verified ground truth in terms of vulnerability preservation."}, "questions": {"value": "1.\tThe authors mention that existing real datasets are difficult to directly compile into binary files. Have there been previous attempts to reprocess these datasets to generate binary versions? If so, references and comparisons are needed.\n2.\tWhen completing code, how to avoid changing the trigger conditions of original vulnerabilities due to automatically generated content? Could this potentially transform complex vulnerabilities into simpler ones?\n3.\tThe CompRealVul_C and CompRealVul_Bin datasets are essentially derived from existing datasets (e.g., Devign) rather than originally collected. The authors should clarify that their contribution is the \"compilation toolchain\" rather than the \"data itself,\" and discuss the academic value and innovation of such derived data.\n4.\tMany code completion models (e.g., GitHub Copilot) already exist. The authors should compare Compote's performance in \"compilation-oriented completion\" with these models to demonstrate its irreplaceability. Current experiments only show internal comparisons, lacking horizontal references.\n5.\tIn model comparison experiments, were all models tested with identical hyperparameters, hardware environments, and training durations? If data scales differ (e.g., Juliet's sample size is much larger than CompRealVul's), the authors should explain whether sampling balance was implemented, as this may affect conclusion credibility.\n6.\tAre the baseline models (e.g., BiLSTM) the most advanced in this field? For example, have better architectures been proposed recently (e.g., in 2024)? Additionally, comparisons with commercial large models (e.g., GPT-4, Cursor), which excel in code understanding tasks, are missing. The authors should explain why these weren't included.\n7.\tCode added for compilation may interfere with models' understanding of original function logic, especially for long code segments. The authors should discuss whether this affects models' focus on core vulnerabilities.\n8.\tSome metrics (e.g., Accuracy, Precision) show that models trained on CompRealVul still underperform those trained on Juliet, contradicting the hypothesis that \"real data improves generalization.\" This makes it difficult to demonstrate the dataset's advantages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dfVDzaymrl", "forum": "yAApKP1j0E", "replyto": "yAApKP1j0E", "signatures": ["ICLR.cc/2026/Conference/Submission16626/Reviewer_C4Tu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16626/Reviewer_C4Tu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484623144, "cdate": 1761484623144, "tmdate": 1762926694694, "mdate": 1762926694694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark that includes binary level vulnerabilities. It is based on existin source code vulnerability bechmarks were source code samples are automatically completed to generate compilable code."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Vulnerability detection at the binary level is a challenging problem. Thos new benchmark promises to generate new research in this domain. The benchmark seems well curated and the methods to compile the samples appear sound."}, "weaknesses": {"value": "The benchmark is limitted by existing source code datasets which themselves are limited and may have errors."}, "questions": {"value": "How do you envision applying Compote to repository-level vulnerabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XX6TMowAec", "forum": "yAApKP1j0E", "replyto": "yAApKP1j0E", "signatures": ["ICLR.cc/2026/Conference/Submission16626/Reviewer_raRx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16626/Reviewer_raRx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959395389, "cdate": 1761959395389, "tmdate": 1762926694227, "mdate": 1762926694227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed COMPOTE, an LLM-based to automatically transform standalone C functions into fully compilable programs, which was applied to real-world functions from ten public datasets of vulnerable code, yielding a dataset comprising 18K compilable C functions along with their compiled binary versions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**\nThe paper proposed novel approach of scaffolding source-code vulnerability dataset to construct compilable binary vulnerability dataset.\n\n**Quality**\nThe paper made extensive experiments on various DL models and LLMs, showing quantitative results of vulnerability detection accuracy.\n\n**Significance**\nThe paper proposed a framework which can be used to help further research in the field of machine learning for vulnerability detection."}, "weaknesses": {"value": "**Flawed Problem Setting of Function-Level Vulnerability Detection**\nRecent works have criticized the common practice of treating machine learning for vulnerability detection (ML4VD) as function-level binary-classification problem. As the vulnerability always depends on external context, it's unreasonable to decide if a single function is vulnerable or not. The popular datasets like Devign/BigVul/DisverseVul have problems in the quality of vulnerability labelling. The paper's approach of scaffolding vulnerable/benign functions, despite keeping the same semantic of the given function, may still change the existence of vulnerability in the context of whole program and compiled binary. Without addressing this risk, the quantitative results of the ML4VD score are not reliable.\n- On the Effectiveness of Function-Level Vulnerability Detectors for Inter-Procedural Vulnerabilities (ICSE '24)\n- Top Score on the Wrong Exam: On Benchmarking in Machine Learning for Vulnerability Detection (ISSTA '25)\n- It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a Practical Perspective (APSEC '25)\n\n**Lack of Qualitative Example of Scaffolding**\nScaffolding arbitrary code from real-world software is a non-trivial task which requires correct API and initialization setup. Even if the resulting program is compilable, it does not mean the code is functionally correct. It would be better if the authors can provide at least some example of vulnerable code snippets and resulting compilable code."}, "questions": {"value": "1. What real-world software does the dataset include?\n2. Does the scaffolding consider semantic correctness beyond syntactic compilability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h4Ftcjb8QZ", "forum": "yAApKP1j0E", "replyto": "yAApKP1j0E", "signatures": ["ICLR.cc/2026/Conference/Submission16626/Reviewer_VABE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16626/Reviewer_VABE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002181036, "cdate": 1762002181036, "tmdate": 1762926693790, "mdate": 1762926693790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to propose a LLM-based automated pipeline that takes incomplete or non-compilable real-world C code snippets, automatically wraps, fixes, and compiles them into programs, outputting two datasets:one is compilable C source files, the other is their corresponding compiled binaries, total yielding over 18K compilable programs (approximately 37% success rate from 49K raw functions).\n\nClaimed Contributions:\n\n- They developed a system (COMPOTE) for LLM-guided code wrapping and error correction using iterative compiling feedback.\n\n- They claimed that this is the first large-scale, compilable, real-world binary dataset for vulnerability detection.\n\n- They claimed to empirically have show n that models trained only on synthetic datasets perform poorly on real-world binaries; models trained or fine-tuned on CompRealVul generalize better to real code. \n\n- They claimed that combining both real and synthetic data provides the best generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "They tackled a real data bottleneck in binary-level vulnerability ML.\n\nThey work enables binary-level ML training for models like RNN and ModernBERT - a previously missing benchmark."}, "weaknesses": {"value": "1. Limited novelty: using GPT/LLMs to fix code for compilation has been explored before. The novelty mostly lies in pipeline integration.\n\n2. Evaluation design: No direct CompRealVul vs. Juliet-only comparison - results emphasize \"Combined\" training, not a pure head-to-head. Sample imbalance makes cross-dataset metrics hard to interpret. Does not evaluate whether COMPOTE introduces semantic drift during wrapping."}, "questions": {"value": "1. The paper leaves unanswered whether CompRealVul truly outperforms other \"real-world\" datasets.\n\n2. The paper doesn't test how much the performance improvement comes from realism vs. data curation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gbptXo7Chh", "forum": "yAApKP1j0E", "replyto": "yAApKP1j0E", "signatures": ["ICLR.cc/2026/Conference/Submission16626/Reviewer_gJRB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16626/Reviewer_gJRB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762036482195, "cdate": 1762036482195, "tmdate": 1762926693247, "mdate": 1762926693247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}