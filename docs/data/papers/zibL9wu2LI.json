{"id": "zibL9wu2LI", "number": 22833, "cdate": 1758336011109, "mdate": 1759896844038, "content": {"title": "InSTA: Towards Internet-Scale Training For Agents", "abstract": "The predominant approach for training web navigation agents is to gather human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data is an inefficient resource. We develop a pipeline to facilitate internet-scale training for agents without laborious human annotations. In the first stage, an LLM annotates 150k sites with agentic tasks. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM filters trajectories by judging their success. Language models are powerful data curation tools, identifying harmful content with an accuracy of 97\\%, judging successful trajectories with an accuracy of 82.6\\%, and producing effective data. We train agents based on \\textit{Qwen 3 1.7B} that are competitive with frontier LLMs as web agents, while being smaller and faster. Our top agent reaches a success rate of 56.9\\%, outperforming the data collection policy \\textit{Qwen 3 235B}, a 235 times larger \\textit{Llama 4 Maverick}, and reaching 94.7\\% of the performance of \\textit{Gemini 2.5 Flash}. We will be releasing code, models and data that reproduce the entire pipeline on our website.", "tldr": "", "keywords": ["Deep Learning", "Self-Improvement"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/20f5efb7d7456cfeb09d1176a566acca72086a10.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a 3-stage, LLM-driven pipeline for creating training data for web agents. An LLM task proposer takes websites from the top 1 million in Common Crawl, ensures the domain is safe, and writes a task for the site. An agent then generates trajectories based on these tasks, and the proposer generates new, more difficult tasks based on these trajectories. An LLM judge scores the trajectories. A small agent is then trained using this generated and judged data, and performs comparably to much larger models (on their data). They claim zero-shot transfer to WebVoyager."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The scale is large enough that the data is likely to be useful to others.\n- Train/test separation of websites is stricter than in many other papers.\n- Transfer to WebVoyager slightly mitigates overfitting concerns."}, "weaknesses": {"value": "- Claims about the accuracy of safety filtering and trajectory judging are based on samples of 100 websites and 100 trajectories, respectively. I believe this is too small to make this claim, and unlikely to be representative of your 150k site sample.\n- Headline numbers are based on LLM-as-judge evaluation, without any human verification/sanity checking (beyond that detailed above).\n- The WebLinx and Mind2Web experiments do not demonstrate transfer to existing benchmarks. There is no improvement on their test sets, only on yours, which seems entirely expected.\n- The only external data on which we see improvement is WebVoyager, which is again LLM judged. This does not rule out potential reward hacking."}, "questions": {"value": "- Can you report human-evaluated success on a larger subset of your data, and show that the 1.7B model remains competitive? Alternatively, could you run your agent and compare it with the other models on static, rule-based benchmarks such as WebArena (Zhou, 2023) and others? Either of these would reduce my concern about reward hacking and provide stronger evidence that the method is viable.\n- What is the transfer to Mind2Web and WebLinx without using their human demonstration data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qKMRdhIORp", "forum": "zibL9wu2LI", "replyto": "zibL9wu2LI", "signatures": ["ICLR.cc/2026/Conference/Submission22833/Reviewer_vZ2x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22833/Reviewer_vZ2x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836135703, "cdate": 1761836135703, "tmdate": 1762942403323, "mdate": 1762942403323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a pipeline for collecting web trajectory data. The corresponding dataset is also released. The trajectories are generated by starting with an initial task, followed by exploration on the website, and then generating the task description based on the grounded trajectory. The authors make efforts to filter harmful websites. Trajectory evaluation is done using LLM as a judge. To evaluate the usefulness of trajectories, they evaluate on WebVoyager, WebLINX, and Mind2Web. Further, they show that performance improves with data scale."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The trajectory dataset is a valuable resource.\n2. The performance improvement with scale is a positive indicator of dataset quality."}, "weaknesses": {"value": "1. Missing external baselines for Mind2Web and WebLINX, and WebVoyager. Also, it is not clear how the set of 500 diverse test tasks for Mind2web was chosen.\n2. Missing references - Explorer [1] - similar pipeline for web agent trajectory synthesis\n\n[1] Pahuja, Vardaan, et al. \"Explorer: Scaling exploration-driven web trajectory synthesis for multimodal web agents.\" ACL 2025."}, "questions": {"value": "1. What is the evaluation benchmark used in section 6.1? Is it an intrinsic evaluation on the test set of generated trajectories? Please elaborate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QWoy7RE5oS", "forum": "zibL9wu2LI", "replyto": "zibL9wu2LI", "signatures": ["ICLR.cc/2026/Conference/Submission22833/Reviewer_36Q5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22833/Reviewer_36Q5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961572667, "cdate": 1761961572667, "tmdate": 1762942403056, "mdate": 1762942403056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for training web navigation agents based on (1) selecting a subset of 150K websites from Common Crawl, (2) generating sample tasks on each of these websites, and (3) training a filtered BC model based on the outputs of an LLM judge. The authors run this recipe with a Qwen-3 235B model and distill it into a Qwen-3 1.7B model, outperforming the original 235B model."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. This is a great paper, with really impressive results, showing that frontier agent performance can be achieved with just a few hundred dollars. I expect it  will be of broad interest to agent researchers\n\n2. The task generation setup is clever — for each website, the proposed method first generates a simple task which gives an agent incentive to explore the website, and then generates more complex tasks conditioned on this trajectory.\n\n3. The paper conducts two useful human validations: first, to make sure that the majority of LLM-proposed tasks are in fact achievable by humans, and second to confirm that the LLM judge models produce mostly correct judgments\n\n4. This paper opens up many promising lines of future work, such as scaling the current recipe, experimenting with new approaches to task generation, and expanding to RL methods beyond filtered BC"}, "weaknesses": {"value": "I think the abstract/intro could have been a bit more clear that the headline result requires both the policy and judge models to be 235B models, but given that the final performance of the distilled 1.7B model surpasses the performance of the 235B model, I don’t consider this to be a major weakness."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KZ92OJzAcd", "forum": "zibL9wu2LI", "replyto": "zibL9wu2LI", "signatures": ["ICLR.cc/2026/Conference/Submission22833/Reviewer_4wrC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22833/Reviewer_4wrC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993756527, "cdate": 1761993756527, "tmdate": 1762942402852, "mdate": 1762942402852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes InSTA, an automatic, three-stage pipeline to facilitate internet-scale agent trajectory collection and training. The first stage is to use an LLM-based task proposer to filter 1 million websites for safety, and generate agentic tasks for 150,000 diverse sites. This stage includes a feedback loop where an agent's trajectory is used to generate a harder, more grounded task. In the second stage, an LLM agent (i.e. the data collection policy) is employed to attempt the generated tasks on live websites, producing trajectories of observations and actions. Finally, an LLM-based judge is used to evaluate the success of these trajectories, filtering for high-quality data. \n\nUsing this pipeline, the authors create a large-scale dataset including 2.2M steps. Their key finding is that a small model (Qwen 3 1.7B) trained on this judge-filtered data can outperform much larger, zero-shot frontier models Llama 4 Maverick and Qwen 3 235B. The trained agents also demonstrate strong zero-shot transfer to the Web Voyager benchmark and improve the generalization of agents trained on static benchmarks like Mind2Web and WebLINX."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The pipeline's execution at the scale of 150,000 live websites is a non-trivial engineering and research achievement, far surpassing the ~200 sites in many existing benchmarks. The release of the insta-150k-v2 task dataset and the larger reasoning dataset is a valuable contribution to the community.\n2. The authors prudently integrate safety considerations from the start, rather than treating it as an afterthought. The LLM-based safety filter is shown to be highly effective (up to 97% accuracy) , and the tasks generated are validated by humans as being achievable and verifiable (up to 89%)."}, "weaknesses": {"value": "1. The feedback loop is proposed as a key design in section 4.1. However, the paper explicitly states, \"For this paper, we employ one loop of task generation\". This is a major limitation, which indicates that the full promise of an iterative system where tasks get incrementally harder is not actually realized or evaluated.\n2. The paper claims to generate \"challenging\" tasks. However, the analysis in Appendix F (Figure 14) shows the most solved tasks are simple information retrieval (e.g., \"contact information,\" \"hours of operation,\" \"biographical info\"). Conversely, Figure 15 shows that the least successful categories are more complex tasks like \"product comparison,\" \"downloading reports,\" and \"booking\". This suggests the InSTA pipeline is currently most effective at creating simple lookup tasks, not the complex, multi-step reasoning and interaction tasks that are a key frontier for agent research."}, "questions": {"value": "The paper's feedback loop is a core idea, but the experiments only use one loop. Can the authors provide results using multiple loops of task generation? Is there quantitative evidence that tasks from $loop_{n+1}$ are measurably \"harder\" or lead to better agent training than tasks from $loop_n$? Without this, the dynamic environment claim seems premature."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CwqYDe1Xwe", "forum": "zibL9wu2LI", "replyto": "zibL9wu2LI", "signatures": ["ICLR.cc/2026/Conference/Submission22833/Reviewer_Esoq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22833/Reviewer_Esoq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762084240712, "cdate": 1762084240712, "tmdate": 1762942402657, "mdate": 1762942402657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}