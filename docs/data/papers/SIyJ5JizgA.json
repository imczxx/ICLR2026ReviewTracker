{"id": "SIyJ5JizgA", "number": 23837, "cdate": 1758349163343, "mdate": 1759896794555, "content": {"title": "Reinforcement learning for hierarchical proof generation in Lean 4", "abstract": "Scaling formal theorem proving to more complex problems requires more efficient inference methods than standard whole-proof generation, which struggles with longer proofs due to exponentially decreasing success rates with increasing proof length. In this work, we systematically study how online reinforcement learning can be coupled with a hierarchical (lemma-based) style of proof generation, that has recently gained popularity for inference-time methods. We show a fruitful interaction in two ways: reinforcement learning allows to train proof decomposition policies successfully, and hierarchical inference allows to overcome plateaus in reinforcement learning as its richer distribution favors exploration and generated lemmas can be understood as an online synthetic data generation technique. Overall, hierarchical inference trained with reinforcement learning produces strong numbers on evaluations even at the 7B parameter scale and outperforms standard whole-proof generation setups in terms of sample efficiency and scalability, making it a suitable technique for necessarily data-constrained formalization research efforts.", "tldr": "RL for lemma-based proving gives strong and explorative policies and overcomes RL plateaus.", "keywords": ["reinforcement learning", "hierarchical inference", "theorem porving", "lean", "ai for math"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd72da2fd28299f6220f8832e64b996d376a0ef4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the combination of online reinforcement learning (RL) with hierarchical, lemma-based proof generation for automated theorem proving in Lean 4. The authors identify two primary challenges with current methods: \n- Standard \"whole-proof\" generation suffers from exponentially decreasing success rates as proof length increases.\n- RL-based training in data-scarce domains like formal mathematics quickly \"saturates,\" leading to performance plateaus. \n\nThe authors design a controlled experiment using a small 7B parameter model and a deliberately data-constrained setting (only 6,000 RL training samples) to induce saturation. They compare their hierarchical \"sketch-based\" RL approach against standard whole-proof generation baselines (GRPO). The results show that while the GRPO baselines quickly plateau in both cumulative training solves and evaluation performance, the hierarchical method continues to improve, avoids saturation, and maintains higher entropy."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly articulates two significant and well-known problems: the scalability of long-form proof generation and the saturation of RL in data-constrained environments."}, "weaknesses": {"value": "- Clarity and Experimental Setting: The paper's narrative and experimental design are unclear. The method is not clearly explained; despite reading the methodology section repeatedly, I was unable to fully grasp the approach. Furthermore, the use of limited training data to mimic data scarcity (as in Lean/other environments) seems questionable or an odd choice for demonstrating the method's effectiveness.\n\n- Doubtful Main Results: The main experimental results presented in Table 1 are questionable. All baseline methods are designed for the theorem-proving task. This design inherently gives the sketch methods the advantage of having a natural language proof in advance. The authors claim the performance of the sketch methods without natual language proof is close to their proposed method but never show these results in the paper.\n\n- Misleading Figures: The figures in the paper are somewhat misleading. Figures 1(c) and 1(d) use a smoothed line to suggest the superiority of the sketch method. However, the best result actually comes from the GRPO Baseline in the early steps. The sketch method never clearly surpasses the baseline in either the miniF2F or ProofNet environments across the entire process shown.\n\nOverall, these are just a few of the listed weaknesses; the paper's results are highly doubtful, and I do not recommend this paper for acceptance."}, "questions": {"value": "- In Figures 1(a) and (b), it's recommended to use RL iterations instead of hours to show the progress of the training. And why in figure (b) the time is less compared to the time in (a)?\n\n- It would be better to have a figure illustrating your methodology or a pseudo-code. To clearly illustrate the pipeline of your method. And the overall writing and storytelling should be improved significantly \n\n- Personally speaking, the point of the sketch method can prevent saturation in RL is really weird, since the sketch method has a much lower starting point compared to the GRPO baseline. And normally, when tuned properly, it's not that easy to fail for GRPO in formal tasks. (As we can see in Godel Prover-v2, Kimina-Prover, and Seed Prover)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z1htXhYlZN", "forum": "SIyJ5JizgA", "replyto": "SIyJ5JizgA", "signatures": ["ICLR.cc/2026/Conference/Submission23837/Reviewer_X1h4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23837/Reviewer_X1h4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630132964, "cdate": 1761630132964, "tmdate": 1762942826882, "mdate": 1762942826882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how online reinforcement learning can be coupled with a hierarchical (lemma-based) style of proof generation. The authors report that hierarchical proof generation performs favorably compared to standard whole-proof generation, showing higher sample efficiency and plateauing later in reinforcement learning runs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The hierarchical design of rewards makes sense in the context of interactive theorem proving, especially when one adopts a sketch-based proof generation strategy."}, "weaknesses": {"value": "- The presentation is so confusing that it’s difficult to understand how exactly the authors implemented their framework and carried out the experiments. For example, in the sketch-based experiment, no detail of the model or settings were given. How are drafts or proof skeletons generated? By what model, trained or fine-tuned on what dataset? \n- In general the paper also lacks proper mathematical formalization of the approaches. How exactly are the hierarchical rewards integrated into GRPO? In such a setting, what are the actions and states in terms of an MDP?\n\nMinor: various places where the English is incomprehensible. For example, on line 241: “ .. we obtain promising for its utility in theorem proving in Lean”"}, "questions": {"value": "What is the exact setting of the “sketch-based” proving / experiments? What is the decomposer model and what is the prover model? How are they related to the models in section 4.2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EBvMUNQH9X", "forum": "SIyJ5JizgA", "replyto": "SIyJ5JizgA", "signatures": ["ICLR.cc/2026/Conference/Submission23837/Reviewer_L8s3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23837/Reviewer_L8s3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937207109, "cdate": 1761937207109, "tmdate": 1762942826607, "mdate": 1762942826607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates combining online reinforcement learning with hierarchical (lemma-based) proof generation in Lean 4. The authors propose a two-step approach where a decomposer first generates a proof sketch containing lemmas, which are then individually proved by a prover model. They train this hierarchical system using GRPO-based reinforcement learning applied at each node in the proof tree. The key findings are: (1) hierarchical proof generation can be trained with RL using minimal supervised data, (2) it shows higher sample efficiency and delays plateaus compared to standard whole-proof GRPO training, and (3) it achieves competitive benchmark results despite using only 6,000 training samples and 10,000 RL steps. The approach is evaluated on proof autoformalization tasks using miniF2F and ProofNet benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: This work is the first systematic study combining RL with hierarchical proving in Lean 4, filling a gap between prior inference-only hierarchical methods and RL-only whole-proof approaches. The application of policy gradients at each node in the proof tree rather than treating the entire generation as a single trajectory is creative and the experimental design focusing on the data-constrained regime is thoughtful.\n\n2. Quality: Solid experimental methodology with appropriate baselines (GRPO variants, pass@k training) and proper use of established benchmarks. Clear identification of saturation criteria and systematic measurement and honest acknowledgment of limitations (small scale, \"toy model\").\n\n3. Clarity: Well-motivated problem statement with clear exposition of the scaling challenge (p^n success rate). Good visual presentation (Figure 1 effectively shows the saturation phenomenon). Appropriate background section establishing key concepts and clear description of the two-step hierarchical approach.\n\n4. Significance: This work addresses practically important problem of scaling to complex proofs, and demonstrates feasibility of training hierarchical decomposition policies with RL."}, "weaknesses": {"value": "Major:\n\n1. Unfair computational comparison\n\nHierarchical inference requires nk+1 LLM calls per \"attempt\" while standard generation requires 1. This makes all comparisons fundamentally unfair: the hierarchical method simply uses more compute. The paper compares \"pass@32\" for standard vs. \"pass@8\" for hierarchical, but the hierarchical version uses 59-69 LLM calls (Table 1), making it actually pass@(59-69). A fair comparison would be: standard pass@60 vs. hierarchical pass@8, which is not shown. This undermines the main claim of \"higher sample efficiency\"\n\n2. Limited experimental scale\n\nOnly 6,000 RL training samples (1,500x smaller than DeepSeek-Prover-V1.5), 10,000 RL steps, and 7B parameter models tested. These choices may artificially favor hierarchical methods, perhaps standard GRPO plateaus simply because the dataset is too small. The authors acknowledge this as a \"toy model\" but then draw general conclusions.\n\n3. Missing ablations and baselines\n\n3.1 No ablation on k (number of attempts per lemma), what if k=1 or k=8?\n \n3.2 No comparison to the simplest baseline: just use more samples with standard generation.\n\n3.3 No ablation on reward choice ($R=R_o$ vs. $R=R_a$), why this choice?\n\n3.4 No experiments with larger datasets to see if standard GRPO would also avoid plateaus.\n\n3.5 No testing at larger model scales (13B, 32B).\n\n4. Unclear generalization\n\nResults are on proof autoformalization (conditioned on NL proofs), which is different from de novo theorem proving. The small-scale \"toy model\" may not represent behavior at realistic scales. The saturation phenomenon observed may be specific to this small dataset size.\n\nMinor:\n\n5. Incomplete technical details:\n\nLearning rates not specified (important for RL); batch sizes not given; number of gradient update steps unclear. Wall-clock time comparisons missing (how much slower is hierarchical inference?). No error bars or confidence intervals despite stochastic training.\n\n6. Presentation:\n\nThe advantage formula in Section 3.1 could be clearer. Missing details on how lemmas are extracted programmatically.\nNo examples of actual generated proof sketches. The term \"saturation\" is used loosely before being defined.\n\n7. Limited analysis:\n\nThere is no analysis of what makes a \"good\" decomposition. No investigation of failure modes, or analysis of lemma complexity distribution.\nLimited discussion of when hierarchical methods help vs. hurt. No comparison of proof lengths between methods.\n\n8. Claims:\n\n\"Strong numbers on evaluations\" (abstract), the results are competitive but not state-of-the-art.\n\"Outperforms standard whole-proof generation\". this seems to be only true with the unfair computational budget comparison.\nClaims about \"sample efficiency\" are misleading given the LLM call discrepancy."}, "questions": {"value": "Critical:\n\n1. Fair computational comparison: Could you please provide results for standard GRPO with pass@60 (or the number of LLM calls that matches your hierarchical pass@8)?\n\n2. Scaling behavior: Have you tested whether standard GRPO also avoids plateaus with larger datasets (e.g., 60,000 samples instead of 6,000)? Perhaps the issue is dataset size, not the inference method?\n\n3. Ablation on k: What happens with k=1 (hierarchical structure but no extra samples per lemma) vs. k=8? This would isolate the benefit of decomposition from simply using more samples.\n\n4. Baseline comparison: What if you just do standard GRPO but with 5x more samples per problem (to match the ~5x LLM calls of hierarchical)? Does it also avoid plateaus?\n\nImportant:\n\n5. Reward choice: Why did you choose $R=R_o$ (overall proof success) instead of $R=R_a$ (average proof success)? Did you try both?\n\n6. Larger scales: Have you tested this at 13B or 32B parameter scales? Do the benefits still hold?\n\n7. Have you investigated if this approach work for theorem proving without natural language proof hints?\n\n8. How do you ensure the generated decompositions are \"good\"? Can you show examples of learned decomposition strategies?\n\n9. Failure analysis: What fraction of sketches contain 0 lemmas? What is the distribution of lemma counts? When does hierarchical generation hurt?\n\n10. Can you show training curves (loss, KL divergence, etc.) not just evaluation metrics?\n\nMinor Questions:\n\n11. Implementation: What are the exact learning rates, batch sizes, and number of gradient steps used? How much slower is hierarchical inference in practice (wall-clock time)?\n\n12. Lemma extraction: Can you provide more details on the extract_goal tactic and show examples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5uL0x75W3E", "forum": "SIyJ5JizgA", "replyto": "SIyJ5JizgA", "signatures": ["ICLR.cc/2026/Conference/Submission23837/Reviewer_byhV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23837/Reviewer_byhV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991594709, "cdate": 1761991594709, "tmdate": 1762942826306, "mdate": 1762942826306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}