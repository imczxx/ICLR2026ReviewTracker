{"id": "X4Oo6ovLT3", "number": 11319, "cdate": 1758196197860, "mdate": 1759897591472, "content": {"title": "Libra: Assessing and Improving Reward Model by Learning to Think", "abstract": "Reinforcement learning (RL) has significantly improved the reasoning ability of large language models. \nHowever, current reward models underperform in challenging reasoning scenarios and predominant RL training paradigms rely on rule-based or reference-based rewards, which impose two critical limitations:1) the dependence on finely annotated reference answer to attain rewards; and 2) the requirement for constrained output format. These limitations fundamentally hinder further RL data scaling and sustained enhancement of model reasoning performance. To address these limitations, we propose a comprehensive framework for evaluating and improving the performance of reward models in complex reasoning scenarios. We first present a reasoning-oriented benchmark (Libra Bench), systematically constructed from a diverse collection of challenging mathematical problems and advanced reasoning models, to address the limitations of existing reward model benchmarks in reasoning scenarios. We further introduce a novel approach for improving the generative reward model via learning-to-think methodologies. Based on the proposed approach, we develop Libra-RM series, a collection of generative reward models with reasoning capabilities that achieve state-of-the-art results on various benchmarks. Comprehensive downstream experiments are conducted and the experimental results demonstrate the correlation between our Libra Bench and downstream application, and the potential of Libra-RM to further improve reasoning models with unlabeled data.", "tldr": "", "keywords": ["Generative Reward Model; Reward Model Benchmark; Reinforcement Learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f13a208dc62f4798fd8b7e708c9f36fcb866d93a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a framework for evaluating and improving generative reward models (GRMs) in reasoning tasks. The framework comprises two main components: 1) a reward benchmark curated from outputs of SoTA reasoning models; and 2) a family of GRMs trained using a combination of SFT, rejected sampling, and RLVR. The authors demonstrate that Libra-RM achieves good performance on mainstream reward benchmarks and yields consistent gains on downstream tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The manuscript is well-written and easy to follow. The paper clearly details training data construction, RL reward formulation, and ablation setups.\n- A reliable, reasoning-oriented reward model and benchmark would be highly valuable for advancing RL research in LLM reasoning."}, "weaknesses": {"value": "- While the overall framework is well executed, the technical components—such as rejection sampling, GRPO-based RL, and SFT—are well-established in prior work. The contribution lies more in system integration and evaluation than in algorithmic innovation.\n- The study’s downstream evaluation primarily focuses on advancing mathematical reasoning with DPO. This makes the claimed generality toward “unverifiable reasoning tasks” somewhat speculative. Additional evidence with online RL and domains where correctness is ambiguous would strengthen the claims."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qUhKrheCcJ", "forum": "X4Oo6ovLT3", "replyto": "X4Oo6ovLT3", "signatures": ["ICLR.cc/2026/Conference/Submission11319/Reviewer_V8zg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11319/Reviewer_V8zg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761377737948, "cdate": 1761377737948, "tmdate": 1762922459745, "mdate": 1762922459745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Libra Bench, a corpus designed for reward model learning and evaluation in mathematical reasoning tasks. To construct the dataset, the authors first select questions from existing sources such as AIME 2024 and AIME 2025. Five models (DeepSeek and Qwen variants) then generate responses to these questions, which are subsequently verified through rule-based, model-based, and manual validation methods. The benchmark is used to evaluate various reward modelling approaches, such as discriminative reward models and LLM-as-a-Judge, demonstrating that Libra Bench poses a substantial challenge. Furthermore, the proposed reward modelling method outperforms baseline approaches, and downstream evaluations show that a reward model trained with this method can effectively enhance policy post-training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Comprehensive baseline methods are presented in the experiments, including evaluating 15 different reward models on the proposed dataset and 6 different reward models for downstream applications. \n* More than 200 questions with responses from 5 different DeepSeek and Qwen variants are collected and annotated in the corpus."}, "weaknesses": {"value": "* The experimental results in Table 3 appear incomplete. A statistical significance test is missing, which is crucial for demonstrating that the reported improvements are meaningful. Some performance gains, such as those between Libra-RM-32B-MATH, DeepSeek-R1, and Qwen3-32B in Table 2, are relatively marginal. Without significance testing, it is difficult to assess the contribution of the proposed method.\n* The experimental setup for the unverifiable reasoning scenario (Section 6) is insufficiently described. Based on Figure 2, the experiments seem to be conducted on AIME24/25, which are typically verifiable tasks.\n* An analysis of the impact of different model responses used during training would be highly informative. Although the authors collect responses with a wide range of performance levels, the effect of this diversity remains unclear. For instance, is it more beneficial to train exclusively on responses from stronger models, or does incorporating error-prone responses also provide useful learning signals for reward modelling? Additionally, the diversity of models used for response collection could be improved; currently, only five LLMs from two variants are employed. Incorporating models from different families (e.g., Phi-2, Gemma, or Llama) would enhance dataset diversity and provide deeper insights into the generalizability of the proposed approach."}, "questions": {"value": "* Typo: The reference prefixes of *4* and *E* in L403 are missing. \n* Typo: A space should be removed after \"Libra Bench\" in L412.\n* Typo: A missing space should be inserted before \"As shown\" in L421.\n* Is the $D^{rl}_{rank}$ used by any reward model (mentioned in Section 4.3)?\n* What will the len_penalty be when $L_{max}$ equal to $L_{exp}$ (equation 4)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NkGgbfBDEY", "forum": "X4Oo6ovLT3", "replyto": "X4Oo6ovLT3", "signatures": ["ICLR.cc/2026/Conference/Submission11319/Reviewer_mokn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11319/Reviewer_mokn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578304724, "cdate": 1761578304724, "tmdate": 1762922459119, "mdate": 1762922459119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Libra, a framework for assessing and improving reward models in complex reasoning tasks. It introduces Libra Bench, a reasoning-oriented benchmark built from challenging math problems and advanced reasoning models, and Libra-RM, a generative reward model trained via a “learning-to-think” approach combining rejection sampling and reinforcement learning. Libra-RM achieves state-of-the-art performance across multiple RM benchmarks and shows strong correlation between Libra Bench accuracy and downstream RL improvements."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Proposes a clear and comprehensive framework integrating a new reasoning-oriented benchmark (Libra Bench) and generative reward models (Libra-RM). These benchmark is well-designed with challenging math problems and advanced reasoning models, effectively testing RM correctness.\n2. The method is well-motivated and clearly presented. Experimental settings are thorough, showing strong state-of-the-art results and meaningful correlation with downstream reasoning performance."}, "weaknesses": {"value": "1. The work does not appear highly original to me, as similar directions have been explored in prior studies such as [1] and [2]. It would be helpful if the authors could discuss these related works in more detail or include a comparison in the experiments.\n2. The training data is mainly distilled from several reasoning models, so the generalization to unseen models or broader domains remains uncertain. Providing experiments or analysis on cross-model transferability would strengthen the paper.\n\n[1] GRAM: A Generative Foundation Reward Model for Reward Generalization\n[2] RM-R1: Reward Modeling as Reasoning"}, "questions": {"value": "I’m wondering whether the authors have explored how the ratio between judging and non-judging data affects training. Does varying this proportion influence the model’s stability or final accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9mj74VFqYj", "forum": "X4Oo6ovLT3", "replyto": "X4Oo6ovLT3", "signatures": ["ICLR.cc/2026/Conference/Submission11319/Reviewer_8a9q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11319/Reviewer_8a9q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830945099, "cdate": 1761830945099, "tmdate": 1762922458303, "mdate": 1762922458303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Libra introduces a reasoning-oriented RM benchmark (Libra Bench) for evaluating Reward Models on reasoning tasks with rollouts sampled with the latest reasoning models. The authors also trained 2 generative thinking reward models which they demonstrate to be superior to competing approaches on the Libra Bench. Finally the effectiveness of the Libra-RM is demonstrated by training SLMs on preference pairs graded by different RMs."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is relatively well motivated to investigate how reasoning-style generative RMs may be better judges on reasoning tasks than existing RM or LLM-as-judge. \n\nThe reported performance gains of the RMs on Libra Bench is very significant, indicating the effectiveness of the proposed method."}, "weaknesses": {"value": "The biggest and potentially fatal issue of the current work is data contamination. In particular, both the benchmark (Libra Bench) and the RM (Libra RM) rely on rollouts generated by models in the R1 model facility (DeepSeek-R1, R1-distilled SLMs). As such, it is very difficult to gauge the degree to which the performance gain that Libra RM has over competing method is coming from data distribution or not. \n\nAdditionally, the various choices of Libra-RM experimentations are not ablated. For example, the inclusion of non-judging data and length penalty are both not ablated. As a result it is unclear whether any of such design choices are necessary or specific to the specific model type that Libra-RM is working with (thinking/reasoning generative reward models).\n\nPerhaps a meta question, since these tasks in Libra Bench are verifiable (e.g. AIME), why is a reward model needed? To answer this question will require running RL experiment using Libra-RM vs. verifiable reward."}, "questions": {"value": "For RM benchmarks, I highly encourage a human-in-the-loop approach where the ground truth answers are sampled from human responses rather than model responses to avoid potential issue of data contamination. In the case where data contamination is unavoidable, it is then critical to cleanly separate in distribution performance evaluation from out of distribution evaluations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4aIUyroSvF", "forum": "X4Oo6ovLT3", "replyto": "X4Oo6ovLT3", "signatures": ["ICLR.cc/2026/Conference/Submission11319/Reviewer_77LG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11319/Reviewer_77LG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940504622, "cdate": 1761940504622, "tmdate": 1762922457193, "mdate": 1762922457193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}