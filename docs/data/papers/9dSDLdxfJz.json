{"id": "9dSDLdxfJz", "number": 12555, "cdate": 1758208539789, "mdate": 1759897502054, "content": {"title": "Combinational Backdoor Attack against Customized Text-to-Image Models", "abstract": "Recently, Text-to-Image (T2I) synthesis technology has made tremendous strides. Numerous representative T2I models have emerged and achieved promising application outcomes, such as DALL-E, Stable Diffusion, Imagen, etc. In practice, it has become increasingly popular for model developers to selectively adopt personalized pre-trained text encoders and conditional diffusion models from third-party platforms, integrating them together to build customized (personalized) T2I models. However, such an adoption approach is vulnerable to backdoor attacks. In this work, we propose a \\textbf{C}ombinational \\textbf{B}ackdoor \\textbf{A}ttack against \\textbf{C}ustomized \\textbf{T2I} models (CBACT2I) targeting this application scenario. Different from previous backdoor attacks against T2I models, CBACT2I embeds the backdoor into the text encoder and the conditional diffusion model separately. The customized T2I model exhibits backdoor behaviors only when the backdoor text encoder is used in combination with the backdoor conditional diffusion model. These properties make CBACT2I more stealthy and controllable than prior backdoor attacks against T2I models. Extensive experiments demonstrate the high effectiveness of CBACT2I with different backdoor triggers and backdoor targets, the strong generality on different combinations of customized text encoders and diffusion models, as well as the high stealthiness against state-of-the-art backdoor detection methods. The code is available at: https://anonymous.4open.science/r/COM_backdoor-2404/.", "tldr": "", "keywords": ["Backdoor attack", "Text-to-Image model", "Customized Text-to-Image model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22590f9138b3c5b8b55c3d796a0cf42750087c65.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CBACT2I, a novel combinational backdoor attack targeting customized text-to-image (T2I) models. CBACT2I combines the backdoor encoder and the backdoor conditional diffusion model to build a backdoor text-to-image model, the malicious behavior emerges only when both compromised components are assembled together. This feature makes the attack stealthier and harder to detect. The paper conducted extensive experiments on various models and datasets, demonstrating the attack effectiveness and the stealthiness against existing defense mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. New attack surface: This paper introduces a novel backdoor attack in text-to-image models, considering the combinations of text encoders and conditional diffusion models.\n2. High effectiveness and generality: This paper conducted comprehensive experiments to demonstrate the attack effectiveness with different backdoor triggers and backdoor targets the strong generality on different combinations of customized text encoders and diffusion models.\n3. Defenses discussion: This paper conducted extensive experiments to demonstrate the attack stealthiness against existing defense mechanisms, such as ONION, T2Ishieldis and UFID."}, "weaknesses": {"value": "1. The ASR for “style backdoor target” depends on a simple classifier. The style-ASR is computed via a ResNet-18 trained by the authors (98% acc.), which may introduce bias. Since GPT4o-as-a-judge is introduced in the case study in the real-world scenario, it is suggested also employ GPT4o to judge the ASR of “style backdoor target”.\n2. The idea of using CBACT2I for secret information hiding is interesting. However, there is no experimental validation for the \"secret hiding\" application. The authors should provide some experimental results."}, "questions": {"value": "1.The authors compute style-ASR with a ResNet-18 (≈98% acc.), which may bias results. Since GPT-4o is already used as a judge in your real-world case study, could authors also report GPT-4o–based ASR for the style backdoor target?\n2. Could the authors provide quantitative experimental results to demonstrate the effectiveness of their approach in the secret information hiding application?\n3. For the “pre-set image” backdoor attack, could the authors include additional similarity metrics (e.g., LPIPS) to more comprehensively measure attack effectiveness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ofsA1hn2Ue", "forum": "9dSDLdxfJz", "replyto": "9dSDLdxfJz", "signatures": ["ICLR.cc/2026/Conference/Submission12555/Reviewer_VUYx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12555/Reviewer_VUYx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760680818398, "cdate": 1760680818398, "tmdate": 1762923412264, "mdate": 1762923412264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper implements a Combinational Backdoor Attack by simultaneously optimizing the text encoder and the noise denoising module in the text-to-image model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "This work focuses on the backdoor attack in text-to-image tasks, which is a significant security threat, and proposes a novel threat scenario: \"Combinational Backdoor Attack.\""}, "weaknesses": {"value": "***1. Unclear Threat Model***\n\nThe threat model is somewhat confusing. I understand that the authors aim to jointly tamper with two components (the text encoder and the UNet) to enhance the stealthiness of the backdoor attack. However, this setup raises several concerns:\n(1) How often does such a co-usage scenario occur in real-world settings? As far as I know, on open-source platforms like CivitAI, personalized fine-tuning of text encoders is rare; most community models focus on VAE or UNet modifications (please correct me if I am mistaken).\n(2) Why improving stealthiness necessarily requires backdooring multiple components? This approach seems more like an application of existing methods in a new setting rather than a fundamentally new methodological contribution.\n\n***2. High Similarity to Related Works***\n\n**The proposed method appears overly similar to prior works [1,2].** In particular, Section 4.3 is highly similar to [1] (see Eq. (1) in both papers), and Section 4.4 is highly similar to [2] (see Eq. (4) here with Eq. (7) in [2]). **Given these overlaps, the novelty of the contribution is questionable.**\n\n***3. Insufficient Evaluation Against Backdoor Defenses***\n\nThe experimental evaluation does not include comparisons with recent text-to-image backdoor defense methods[3,4,5], which are essential to validate the claimed stealthiness.\n\n\n[1] Struppek L, Hintersdorf D, Kersting K. Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis[J]. arXiv preprint arXiv:2211.02408, 2022.\n\n[2] Zhai S, Dong Y, Shen Q, et al. Text-to-image diffusion models can be easily backdoored through multimodal data poisoning[C]//Proceedings of the 31st ACM International Conference on Multimedia. 2023: 1577-1587.\n\n[3] Wang Z, Zhang J, Shan S, et al. Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models[J]. arXiv preprint arXiv:2504.20518, 2025.\n\n[4] Zhai S, Li J, Liu Y, et al. Efficient Backdoor Detection on Text-to-image Synthesis via Neuron Activation Variation[C]//ICLR 2025 Workshop on Foundation Models in the Wild.\n\n[5] Xu Y, Zhong N, Li G, et al. Fine-grained Prompt Screening: Defending Against Backdoor Attack on Text-to-Image Diffusion Models[J]."}, "questions": {"value": "While Eq. (4) seems to be designed only for generating “specific images” backdoor, it remains unclear how the method achieves “specific styles” backdoor as claimed in Section 4.5. \n\nDo authors utilize the loss function of Eq. (4) to obtain a “specific styles” backdoor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4f5pYtsns7", "forum": "9dSDLdxfJz", "replyto": "9dSDLdxfJz", "signatures": ["ICLR.cc/2026/Conference/Submission12555/Reviewer_rcc7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12555/Reviewer_rcc7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639184328, "cdate": 1761639184328, "tmdate": 1762923411860, "mdate": 1762923411860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a new security vulnerability in customized text-to-image pipelines, where users mix pretrained text encoders and diffusion models. The authors propose CBACT2I, a combinational backdoor that injects separate triggers into the encoder and decoder: each component appears benign on its own, but together they activate malicious outputs when a triggered prompt is used. The attack preserves normal functionality, works across different encoder–decoder combinations, and evades existing defenses. Experiments show high attack success with strong stealthiness. The work is well-motivated and reveals an overlooked, practical threat in modular T2I model development."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a novel attack, where the backdoor can only be triggered when the text encoder matches the diffusion model.\n\n2. The experiments are both sound and comprehensive, which demonstrates the effectiveness of the proposed method as well as its robustness.\n\n3. The proposed method is straightforward, simple, and effective.\n\n4. Good writing, easy to follow."}, "weaknesses": {"value": "1. **Scope of generalization.** Experiments focus on a few open-source diffusion models, and all of them are variants of stable diffusion model family; transferability to other architectures, tokenizers, or deployed commercial stacks (closed-source encoders/decoders) is not shown. I therefore recommend more experiments on different text encoders and diffusion models, including the newest SD models and the earliest LDM, whose text encoder is based on BERT.\n\n2. **Limited defense evaluation.** Only a few detectors (ONION, T2IShield, UFID) are evaluated; the paper lacks study against preprocessing (normalization), model-editing defenses, or newer detection methods tailored for modular pipelines. Moreover, I also suggest that the author examine how the fine-tuning would affect the injected backdoor.\n\n3. **Confusing attack significance.** The proposed combinational design indeed improves stealth, but it also substantially reduces the probability of accidental triggering: only a very specific combination of a poisoned encoder, a poisoned text encoder, a backdoored diffusion model, and a triggered prompt will activate the backdoor. This raises important questions that the paper does not sufficiently justify: *What's the point of conducting such an attack?* and, as it seems only the attack can trigger the backdoor, *Why does the attacker need to attack himself?*, and as a result, *What real-world significance does this attack actually have?*  Particularly, I do not fully agree with the author on the discussion in Sec 6, where the attack targets in the real-world scenario were described as producing bias, harmful, and advertisement contents. Since the attacker has full access to the original model, they can always obtain an exclusive model for these malicious tasks by just fine-tuning it. As for the positive phase, I also doubt that the proposed method can have any advantage over existing watermarking methods or backdoor methods."}, "questions": {"value": "see in weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NfNhpHh0R3", "forum": "9dSDLdxfJz", "replyto": "9dSDLdxfJz", "signatures": ["ICLR.cc/2026/Conference/Submission12555/Reviewer_hpav"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12555/Reviewer_hpav"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907668181, "cdate": 1761907668181, "tmdate": 1762923411530, "mdate": 1762923411530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}