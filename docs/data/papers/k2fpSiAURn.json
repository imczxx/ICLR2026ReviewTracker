{"id": "k2fpSiAURn", "number": 1523, "cdate": 1756889292350, "mdate": 1763060988214, "content": {"title": "Mixture of Neuron Experts", "abstract": "In this work, We explore whether the parameters activated by the MoE layer remain highly sparse at inference. We perform a sparsification study on several representative MoE models. For each expert, we rank parameters by the magnitude of their activations from the gate projection and progressively prune the activated subset. Pruning up to $60\\%$ of parameters within that subset causes only negligible task-performance degradation; substantial drops occur only after more than $90\\%$ are removed. We further decompose experts into neuron granular MoE and visualize their activation values, finding that most neuron activations are near zero. This observation motivates us to select only high-activation neuron experts during pretraining. Based on this insight, we propose \\emph{Mixture of Neuron Experts} (MoNE). MoNE applies a simple top-$k$ selection within each expert, incurs negligible latency, and requires no additional routing parameters or inter-expert communication. Extensive experiments demonstrate that MoNE matches standard MoE performance while activating only $50\\%$ of the MoE-layer parameters, and it consistently outperforms traditional MoE when compared at equal numbers of activated parameters. These results suggest that MoNE is a practical approach to improving parameter utilization and inference efficiency in MoE-like models.", "tldr": "We achieve neuron granular expert selection on Mixture of Experts.", "keywords": ["Mixture of experts", "Large language model", "Pretraining"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7f96edd974ee656fafdef2bec2caa381b88a64f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a technique to further sparsify the expert parameters in MoE models. Their technique is based on the observation that MoE models which use GLU style experts often have sparse activations in the Gated part of the GLU up-projection which means a subset of the expert weights effectively have null computation. The paper then proposes a technique to only select the top-k activations in the GLU to compute the actual expert output which they call MoNE. They have results on several pretrained MoEs showing this to be the case. Furthermore, they pretrain some models with MoNE and show that this improves performance compared to baseline MoEs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "I think the main contribution of the paper is in identifying the effective sparsity in GLU style experts and then being able to decompose the expert computation such that they can run another sparse computation within the expert by only using the outputs of the experts which have high activation values. They were also able to demonstrate this sparse behavior in GLU experts of several pretrained MoE models like Qwen and Mixtral. Finally, they also demonstrated, albeit on a small scale, the benefit of pretraining MoEs with MoNE and showed how it improves upon standard MoE models."}, "weaknesses": {"value": "One of the main weakness of this method is the presentation of the paper. The writing was not effective and I was genuinely confused at several subsections of the paper. The list of errors / confusions I was able to spot are listed below. \n\nLine 18 - achieve neuron granular expert select \n\nLine 47 - magnitude of their activations weights, which calculated by the gate projection. \nWhat are activation weights? I think the authors mean activations values.\n\nLine 256 - experts that associated with high absolute gate\n\nLine 448 - The Effect of Different Activate function inside the Expert\n\nAlso, I am fairly confused as to how this is only applicable to MoE models. GLU styled FFNs are common in dense transformers and their technique should be applicable to standard dense models which I believe would serve as a simpler baseline. \n\nThe other thing I don't see in the paper is an analysis of the overhead introduced by MoNE. They briefly mention that they do not see any overhead between MoNE and standard MoEs but I don't see how that is happpening. Their is an additional loss computation at each layer, an additional top-k and I am not sure how exactly they have implemented the MoNe computation but since it is token specific and very granular sparsity, I would expect an overhead for that too. However, the details are not provided."}, "questions": {"value": "The authors mention  : \"The fundamental concept of Mixture-of-Experts (MoE) in large language\nmodels entails partitioning a large feed-forward network (FFN) into several smaller subnetworks\nreferred to as experts\" .This is only true in the case of fine-grained experts. For most other implementations, it just means we replicate the standard expert N times i.e MoEs are Flop-matched to Dense models during inference / training. Could you please clarify?\n\nAlso can you please explain why this technique was studied in the context of MoEs only and not dense transformer models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5Ako8FiCuF", "forum": "k2fpSiAURn", "replyto": "k2fpSiAURn", "signatures": ["ICLR.cc/2026/Conference/Submission1523/Reviewer_rRiW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1523/Reviewer_rRiW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760565109796, "cdate": 1760565109796, "tmdate": 1762915792577, "mdate": 1762915792577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates sparsity in Mixture-of-Experts (MoE) architectures at inference and introduces Mixture of Neuron Experts (MoNE), a neuron granular variant of MoE. The authors empirically show that even within the activated experts, most neuron activations are near zero, indicating under-utilization of parameters. MoNE addresses this by applying a top-k neuron selection within each expert, activating only high magnitude neurons while maintaining similar accuracy. They further propose a Neuron-Granular Load-Balance Loss (NG-LBL) to encourage uniform neuron utilization. Experiments on models ranging from 0.9 B to 2.8 B parameters demonstrate that MoNE matches or exceeds the performance of traditional MoE while activating roughly 50 % fewer parameters and incurring negligible latency overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Comprehensive ablations on activated-parameter ratio (KN), activation functions, and load-balance effects\n2. Achieves neuron-granular routing without increasing router complexity or latency.\n3.  Visual and quantitative confirmation of intra-expert sparsity.\n4. Addresses pressing scalability challenges in sparse-activation LLMs such as Mixtral and DeepSeek."}, "weaknesses": {"value": "1.  The paper does not formalize why top-k neuron selection preserves representational capacity.\n2. Experiments rely mainly on NeMaTron-CC (50 B / 100 B tokens); additional pretraining corpora would strengthen generalization claims.\n3. Downstream tasks are limited to 10 LM-Eval benchmarks; results on reasoning or multimodal tasks could show broader utility.\n4. Some results (e.g., NG-LBL vs baseline) overlap; statistical significance is not reported."}, "questions": {"value": "1. How stable is top-k neuron selection during training—does neuron dropout harm convergence?\n2. Could NG-LBL introduce over-regularization for layers with naturally sparse activations?\n3. How does MoNE perform when fine-tuned on smaller downstream datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "klIoZib1ee", "forum": "k2fpSiAURn", "replyto": "k2fpSiAURn", "signatures": ["ICLR.cc/2026/Conference/Submission1523/Reviewer_n4j5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1523/Reviewer_n4j5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623417443, "cdate": 1761623417443, "tmdate": 1762915791571, "mdate": 1762915791571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies sparsity in the activations of Mixture-of-Experts language models. The authors find that the activation patterns are very sparse i.e. most activations are near zero magnitude. The Mixture of Neuron Experts architecture is proposed, introducing sparsity in the activations of experts along with the standard sparse selection of experts themselves in MoEs. The MoNE approach selects the top K neurons from the activations and computes the expert output using only those neurons. The authors show the pruning of these activations has small effect on performance. They also develop a neuron granular load balancing loss to improve the uniformity of expert activations. The authors justify the efficacy of their method with pretraining experiments comparing MoNE to traditional MoE; under the same computational cost they find MoNE outperforms MoE in language modeling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The algorithm is clearly outlined and easy to follow. Diagrams such as Figure 3 are very helpful in visualizing the method.\n\nEmpirical findings are well presented, and they evidently motivate the method. The MoNE method is simple and adds relatively little overhead to the MoE architecture.\n\nThe Neuron Level Load Balancing Loss is clearly shown to improve pretraining from benchmarks in Table 3. Moreover, even without the load balancing loss the MoNE outperforms standard MoE with the same active parameter count."}, "weaknesses": {"value": "In tables 1-4 the authors do not report statistical uncertainty around the benchmark scores. This is essential to claim the differences are statistically significant.\n\nThe method would be more convincing if the authors demonstrated the MoNE preserves performance when fine-tuned on downstream tasks.\n\nThe authors show throughput and latency numbers in Table 5, but it is unclear which model scale these numbers are reported for. \n\nThe method introduces new hyperparameters, which would require additional tuning in practice"}, "questions": {"value": "Could you demonstrate how the throughput and memory are affected by scale? Specifically, does the throughput relative to baseline MoE change as you vary model scale? \n\nCould you provide more detail on hyperparameter selection? How much training is required to select for the optimal hyperparameters?\n\nCould you provide some intuition as to why the activation function affects the results so significantly? Also, are these activation functions compared against the baseline MoE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "63uO9bFSg4", "forum": "k2fpSiAURn", "replyto": "k2fpSiAURn", "signatures": ["ICLR.cc/2026/Conference/Submission1523/Reviewer_R3z4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1523/Reviewer_R3z4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868613947, "cdate": 1761868613947, "tmdate": 1762915790730, "mdate": 1762915790730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Mixture of Neuron Experts (MoNE), which improves the model's parameter efficiency by introducing an additional sparse routing at the neuron level within the MoE experts. The motivation for this approach is derived from observing the sparse patterns within the MoE experts, and the paper introduces a corresponding load balance loss for auxiliary training. Experimental results show that MoNE outperforms MoE in terms of performance with the same number of parameters and active parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The motivation of this paper is clear, and the presentation is easy to follow.\n\n+ The experiments are comprehensive and demonstrate that there is still redundancy in the active parameters within the MoE."}, "weaknesses": {"value": "+ The paper contains factual error in its description of previous methods, which reduces the credibility of the proposed approach. According to both earlier literature [1] and recent work [2], Equation 5's $P_i = \\frac{1}{T} \\sum_{x \\in B}Act(topK(Router(x)))[i]$ should actually be $P_i = \\frac{1}{T} \\sum_{x \\in B}Act(Router(x))[i]$. This error also extends to the description of the proposed method in the paper, which makes it difficult to verify whether the experimental implementation is correct.\n\n+ The paper introduces a new load balance loss, but the analysis of this loss is insufficient. For example, it is necessary to analyze the distribution of token routing when the loss reaches its minimum in order to better illustrate its effect. In fact, since MoNE's neuron experts use point-wise SiLU or sigmoid as activation functions, the sum of $G_i$ for each token within each expert is not equal to 1. This means that the MoE's load balance loss does not achieve the expected effect in this case. As long as $G_i$ is small, $P_i$ will also be small, and the load balance loss will be minimized, regardless of whether the distribution is balanced. (DeepSeek v3 also uses sigmoid as the router's activation function, but it applies additional normalization when calculating the load balance loss.)\n\n+ The concept of neuron experts in this paper was already introduced in earlier work [3] and scaling attempts were made in [4]. The approach presented in this paper can be seen as a nesting of MoE and Product-Key Models (PKMs), but this is not discussed in the paper.\n\n\n[1] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Eﬃcient Sparsity\n\n[2] DeepSeek-V3 Technical Report\n\n[3] Large Memory Layers with Product Keys\n\n[4] Ultra-Sparse Memory Network"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dXSR0oQgRM", "forum": "k2fpSiAURn", "replyto": "k2fpSiAURn", "signatures": ["ICLR.cc/2026/Conference/Submission1523/Reviewer_REoY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1523/Reviewer_REoY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916900832, "cdate": 1761916900832, "tmdate": 1762915790371, "mdate": 1762915790371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}