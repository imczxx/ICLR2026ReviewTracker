{"id": "QU8rk0eZgp", "number": 1835, "cdate": 1756950599258, "mdate": 1763021628070, "content": {"title": "Can Monocular Single-Depth Foundation Models Generate Multi-Depth Hypotheses?", "abstract": "Monocular depth foundation models underpin modern 3D perception, yet they are mainly trained under a restrictive paradigm that predicts a single deterministic depth value per pixel. This formulation assumes that every image ray intersects only one surface, an assumption that fails in transparent or multi-layer scenes. This raises our central question: can models built for single-depth prediction generate multi-depth hypotheses? We find that they can. Beneath their deterministic outputs lies latent multi-layer structure, a hidden geometry obscured by the training objective rather than absent from the model itself. To uncover it, we introduce Laplacian Visual Prompting (LVP), a lightweight input-space perturbation that elicits complementary depth hypotheses from a frozen model without retraining. On our new MD-3k benchmark of ambiguous scenes, LVP consistently decouples foreground and background layers, showing that a single off-the-shelf depth foundation model can be reprogrammed into a multi-hypothesis estimator. These results reveal that the geometric capacity of depth foundation models is richer than their standard outputs suggest, and open a new path toward probing and harnessing hidden representations for more complete 3D understanding under ambiguity.", "tldr": "Depth models trained for single outputs hold latent multi-layer cues, unlockable via Laplacian Visual Prompting.", "keywords": ["Depth Foundation Model; Visual Prompting; 3D Spatial Understanding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/118b67b98551c2d1f3afded4e592299ab9588a9d.pdf", "supplementary_material": "/attachment/f303d61bf6689c986d708aa8797547719b6aa8a2.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors study how to enable monocular depth foundation models to understand multi-layered 3D scenes with transparent glasses. Without finetuning monocular depth models, authors find that simply prompting a pre-trained model with a Laplacian of the input image, i.e., Laplacian Vision Prompting, can improve the background depth estimation. Alongside with the paper, a new benchmark named MD-3k is introduced for evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper explores how to understand multi-modal spatial relationship in the scenes with transparent glasses, which is challenging and meaningful for the community. \n\n2. This paper introduces MD-3k, a new benchmark to systematically evaluate the spatial relationships (near vs. far) of multi-layer depth estimation. \n\n3. Without finetuning any models, Laplacian Visual Prompting is a plug-in module for different models. It is simple and proven effective on the MD-3k benchmark."}, "weaknesses": {"value": "1. The evaluation metrics only evaluate whether the orders of depth is correct. More depth map metrics, such as relative depth difference, should be used to show the accuracy of depth map. \n\n2. It is not guaranteed that pre-trained monocular depth models always output foreground depth, e.g. depth of transparent glass. Therefore, the claim that ``biased single-layer estimate from vanilla monocular depth model and prediction with LVP-transformed input are complementary ‘’ may not be correct. Both of them may output the background depth. This is also illustrated in Fig. 6. \n\n3. Using edge information, estimated with Laplacian operator, only will cause ambiguity in depth prediction, resulting in low accuracy. For example, if a checkboard (black & white) is placed on a table, the depth map from Laplacian visual prompting should not be flat since there is rich edge information on the checkboard. This problem also appears in the visualization of Fig. 10. For the fourth example, there is a dark-gray carpet on the light-gray floor, resulting in an edge in Laplacian response. As shown in 2nd depth image, there exists depth inconsistency between carpet and floor. In addition, Laplacian operator is a high-pass filter, filtering out lots of information. However, to accurately estimate depth maps, intensity information is very important, e.g. shadow or slight intensity change in a local window."}, "questions": {"value": "1. Instead of Multi-Layer Monocular Depth Estimation, I think two-layer MDE would be more accurate because the paper only discusses the scene with two depth layers (transparent glass + objects in the back). The current pipeline may not support more depth layers, e.g. 3. \n\n2. Would the pipeline work for curved transparent surfaces?\n\n3. L118: Diffusion-based monocular depth models can also output multi-modal depth maps by changing the random seeds of input Gaussian noise. This should be clarified. \n\n4. L314: Could authors explain the reason why LVP reduces 2nd layer depth preference for some models. Intuitively, LVP provides information for second layer depth and should increase 2nd layer depth preference, e.g. DA v2-L."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1WCWwzNylP", "forum": "QU8rk0eZgp", "replyto": "QU8rk0eZgp", "signatures": ["ICLR.cc/2026/Conference/Submission1835/Reviewer_YVLM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1835/Reviewer_YVLM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761494450615, "cdate": 1761494450615, "tmdate": 1762915904525, "mdate": 1762915904525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a training-free approach for generating a second, alternative depth layer for a single input image by feeding to the monocular depth model at hand the Laplacian-filtered version of the image. Moreover, the authors curate the new MD-3k benchmark for two-layer depth estimation which is sourced from the existing GDD dataset, by annotating it with pairwise depth ordering labels across two different depth layers, and introduce new metrics to evaluate two-layer depth predictions. Comparisons and ablations on the new benchmark analyze the depth-layer biases of several central monocular relative depth estimators and show that the proposed input-space visual prompting can yield decent two-layer depth predictions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The examined problem of two-layer monocular depth estimation is challenging, interesting and of practical use to computer vision practitioners.\n\n2. The core idea of input-space visual prompting to allow a training-free second depth layer prediction is clever and well-motivated.\n\n3. The compilation of a new benchmark to quantitatively evaluate methods' performance on the newly considered task is an important contribution, besides the methodological aspect of the paper."}, "weaknesses": {"value": "1. Soundness of central hypothesis. The core hypothesis of the LVP-based method that amplifying high frequencies will prioritize the prediction of the second, further depth layer with strong textures breaks in the cases where the depth model at hand is already biased towards the second depth layer. This is the case for models such as DAv1 and DAv2-O (L. 426), for which the employed LVP-based two-layer estimation unintuitively deteriorates performance as model size is increased. My perception is that the proposed method works well only when the depth estimator at hand is indeed biased to originally predict the first, low-frequency depth layer, and fails otherwise. I would expect that ideally such a training-free method should first determine which bias the estimator has and adaptively construct the visual prompt either as a low-pass or as a high-pass signal for obtaining the second depth layer accordingly. Table 4 is not fully helpful in examining this setting, as it only shows the \"Reverse\" subset comparison of low-pass vs. high-pass prompts, while omitting the \"Same\" subset, which might reveal an opposite picture to the former subset. The soundness of the high-pass second-layer hypothesis is also challenged by the finding (L. 344-345) that DPT, ZoeDepth, and DAv2-SB flip their predictions from the second layer to the first when prompted with the Laplacian, which is against the provided intuition.\n\n2. Performance trails that of easy baselines. While the proposed training-free method exhibits decent performance for two-layer prediction, Table 2 reveals that the simple baseline of prompting the depth model with semantic predictions results in even better performance. Note here that the boldface notation in this table is inconsistent, highlighting the results of the proposed method although for \"Overall\" and \"Reverse\" it is another method that scores the highest. Given that the current state of the art in segmentation is quite advanced, with models such as SAM delivering accurate masks for diverse monocular inputs which capture well edges that correspond to depth gradients, the utility and relevance of the proposed Laplacian filtering, which performs below-par to segmentation-based prompts, is questioned. This comparison also contains a vague aspect: the authors mention that \"extra semantic priors\" are used as prompts. How are the Laplacian and the semantic prompts combined, if they are? If they are, what is the comparison of using semantic priors rather as drop-in replacement of Laplacian prompts - which prompt alone is better?\n\n3. Quantitative comparison of single-layer vs. multi-layer methods and respective baselines. The proposed evaluation can accurately capture the comparative performance of different two-layer methods. However, I do not find how the proposed framework can actually tell whether a two-layer prediction does not introduce more errors compared to a single-layer prediction than the correctly predictions it makes for alternative-layer pixels. I also understand that the current evaluation is restricted to the ground-truth masks that indicate the presence of two layers in the respective image regions. What about the consistency of the two different predictions in single-layer regions of the image? Moreover, it is not clear to me what the performance of a dummy, e.g. random baseline would be in the setting of the main comparison in Table 1. Such a measure could serve as reference to better interpret the \"Overall\" and \"Same\" figures in that table. In my understanding, simply copying the primary depth prediction to the secondary one would yield a score of 100% for \"Same\" and 0% for \"Reverse\". I would like to thus see what the respective \"Overall\" score would be for that baseline, in order to better understand how much the presented models improve the \"Overall\" score comparatively.\n\n4. Ordinal-only evaluation. While the presented benchmark is useful for evaluating the ordinal correctness of predicted multi-layer depth maps, it ignores the continuously-valued accuracy of the predictions, with measures such as $\\delta_1$ etc. Moreover, only the relative depth estimation scenario is experimentally examined. Foundational monocular metric depth estimators such as UniDepth [A], UniK3D [B], UniDepthV2 [C], Metric3D v2 [D], and Depth Pro [E] are ignored, even though both their outputs are compatible with the relative depth estimation setting which is examined and they could serve as baselines for metric two-layer depth estimation experiments using the proposed LVP approach.\n\n[A] Piccinelli et al.: UniDepth: Universal monocular metric depth estimation. In CVPR, 2024.\n\n[B] Piccinelli et al.: UniK3D: Universal camera monocular 3D estimation. In CVPR, 2025.\n\n[C] Piccinelli et al.: UniDepthV2: Universal monocular metric depth estimation made simpler. IEEE T-PAMI, 2025 (to appear).\n\n[D] Hu et al.: Metric3D v2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation. IEEE T-PAMI, 2024.\n\n[E] Bochkovskii et al.: Depth Pro: Sharp monocular metric depth in less than a second. In ICLR, 2025."}, "questions": {"value": "Cf. my questions in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Y8uUTDF13l", "forum": "QU8rk0eZgp", "replyto": "QU8rk0eZgp", "signatures": ["ICLR.cc/2026/Conference/Submission1835/Reviewer_m2ax"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1835/Reviewer_m2ax"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814573680, "cdate": 1761814573680, "tmdate": 1762915904319, "mdate": 1762915904319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that monocular depth foundation models, although trained to output a single deterministic depth per pixel, actually encode latent multi-layer geometry. \n\nThe authors introduce Laplacian Visual Prompting (LVP), a training-free input perturbation that feeds the model a Laplacian-filtered image. This high-frequency “prompt” suppresses smooth foreground surfaces (e.g., glass) and coaxes the frozen model to produce a complementary depth map for the background. \n\nEvaluated on the proposed MD-3k benchmark of ambiguous, multi-layer scenes, LVP separates foreground and background, turning an off-the-shelf depth model into a multi-hypothesis estimator without any retraining or extra parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Reveals a previously unreported property: single-depth foundation models implicitly encode multi-layer geometry. This reveal that the geometric capacity of depth foundation models is richer than their standard outputs suggest, and open a new path toward probing and harnessing hidden representations for more complete 3D understanding.\n\nIntroduces Laplacian Visual Prompting (LVP)—a simple, training-free input transform that converts any frozen depth model into a multi-hypothesis estimator. The idea of using a classical Laplacian filter as a “visual prompt” is elegant and novel.\n\nProvides MD-3k, the dedicated benchmark for transparent / multi-layer depth evaluation.\n\nLVP is tested across several foundation models and consistently produces complementary depth maps."}, "weaknesses": {"value": "The authors argue that the Laplacian prompt suppresses low-frequency foreground glass signals and highlights high-frequency background details (from the first layer to the second). However, Fig. 6 shows that for some models (DPT, ZoeDepth, DAV2-SB) the behavior flips: the Laplacian prompt draws the prediction toward the foreground glass rather than the background (from the second layer to the first). The paper does not explain why a single high-pass perturbation drives different models toward opposite posterior modes, nor how practitioners can anticipate which layer a given model will favor.\n\nThe manuscript did not discuss several studies that likewise expose hidden capabilities of pretrained vision models, such as “Emergent Correspondence from Image Diffusion,” “Cross-View Completion Models are Zero-shot Correspondence Estimators,” “Easi3R: Estimating Disentangled Motion from DUSt3R Without Training,” and “Video Models are Zero-shot Learners and Reasoners.” A clearer comparison would help position LVP’s novelty and clarify how it complements these zero-shot 3D reasoning approaches."}, "questions": {"value": "Why does a Laplacian-filtered image make sense to the model? The prompt is clearly OOD: the network was never trained on these high-pass inputs.\n\nInconsistent layer preference across models (Fig. 6). For DPT, ZoeDepth, and DAV2-SB, the Laplacian prompt highlights the foreground instead of the background. Can you give practitioners a simple heuristic for predicting whether a given model will swap layers or not?\n\nImpact on the original (first-layer) prediction. When LVP is applied, the original layer is degraded. In applications, one may need both layers: how should we decide where and when to apply LVP? Please quantify the accuracy drop on the first layer and discuss possible mitigations such as spatially selective prompting or blending strategies.\n\nCan the method be used to automatically combine two layers prediction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ti7l4a7vT9", "forum": "QU8rk0eZgp", "replyto": "QU8rk0eZgp", "signatures": ["ICLR.cc/2026/Conference/Submission1835/Reviewer_sERC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1835/Reviewer_sERC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900029812, "cdate": 1761900029812, "tmdate": 1762915904073, "mdate": 1762915904073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method can predict the scene's multi-layer structure, where the ambiguity is induced by the transparent surfaces. The proposed Laplacian Visual Prompting (LVP) predicts structure behind the transparent surface, where the Laplacian of the image indicates the high-frequency structure (i.e., the structure behind the often homogeneous surface). To evaluate this, the authors introduced MD-3k, which is a benchmark of the scenes with the ambiguity induced by transparent surfaces. The authors demonstrated the LVP on various MDE models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper addresses a novel problem setting with significance in monocular depth estimation.\n\nThe proposed Laplacian Visual Prompting is intuitive and effective.\n\nThe application of the LVP (especially conditioning the generated geometry) is interesting."}, "weaknesses": {"value": "The assumption of the scene with the transparent surface is a bit strong. In detail, the transparent surface should be fully transparent and smooth, and the structure of the scene behind the surface needs to be sufficiently high-frequency."}, "questions": {"value": "Would the proposed method be generalized to the scenes violating the assumption? For example,\n(1) The scene with a smooth glass surface, and the scene behind the surface has only a homogeneous wall.\n(2) A glass surface with the patterns (a mosaic or an embossment) and the scene behind the glass surface only includes a smooth, homogenous wall.\n\nIf not, what could be the possible way to address this?\n\nAlso, the benchmark only includes the scenes following the overall assumption of this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nXYAQaxkDU", "forum": "QU8rk0eZgp", "replyto": "QU8rk0eZgp", "signatures": ["ICLR.cc/2026/Conference/Submission1835/Reviewer_8tiM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1835/Reviewer_8tiM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985106781, "cdate": 1761985106781, "tmdate": 1762915903888, "mdate": 1762915903888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Changes for Our Manuscript Revision"}, "comment": {"value": "**Dear Reviewers, Area Chairs, and Program Chairs**,\n\nWe sincerely thank the reviewers, ACs, and PCs for the time and effort devoted during this review. We especially appreciate our reviewers for offering valuable comments, providing positive feedback, and drawing insightful suggestions.\n\n---\n\nWe are encouraged that our reviewers recognize this work:\n* **Reviewer 8tiM:** finds our paper addresses a \"novel problem setting with significance,\" notes the \"Laplacian Visual Prompting is intuitive and effective,\" and that the \"application... is interesting.\"\n* **Reviewer sERC:** recognizes that our work \"Reveals a previously unreported property,\" finds the \"idea... is elegant and novel,\" and highlights that it \"Provides MD-3k, the dedicated benchmark.\"\n* **Reviewer m2ax:** states the \"problem... is challenging, interesting and of practical use,\" the \"core idea... is clever and well-motivated,\" and the \"compilation of a new benchmark is an important contribution.\"\n* **Reviewer YVLM:** notes that our paper \"explores... a challenging and meaningful\" problem, \"introduces MD-3k, a new benchmark,\" and finds LVP \"is simple and proven effective.\"\n\n\nAs suggested by our reviewers, we have revised the manuscript accordingly. All major revisions in the paper are **highlighted in blue** for your convenience.\n\n---\n\n\nWe present a **summary of changes** as follows:\n\n### 1. Experimental Analysis & Baselines:\n* As suggested by **Reviewer m2ax**, we have added a **\"dummy\" baseline** (copying the RGB prediction) to **Sec. 4.2** and **Table 2**. This new ideal baseline (56.4% ML-SRA) provides a much stronger reference and highlights our method's **+19.1%** absolute improvement.\n* We have clarified the comparison to the \"semantic\" baseline in **Sec. 4.2** (as suggested by **Reviewer m2ax**), correcting the bolding and emphasizing that our single-model LVP achieves comparable performance *without* the multiple extra models and geometric assumptions required by the baseline.\n* As requested by **Reviewer m2ax**, we have added the \"Same\" subset results for the Gaussian (low-pass) prompt to **Table 5**, confirming that high-pass (not low-pass) signals are key to switching modes in ambiguous *Reverse* cases.\n* To address **Reviewer YVLM**'s question, we have clarified our Canny edge comparison in **Sec. 4.5**, highlighting that LVP's non-binary, intensity-rich signal provides more robust information than simple binary edges (Table 4).\n\n### 2.  Scope & Future Work:\n* In response to **Reviewers 8tiM and YVLM**, we have added a new **Fig. 11** and a detailed discussion in our **\"Open questions\" (Sec. 5)** to explicitly address failure cases, including homogeneous backgrounds and patterned/semi-transparent foregrounds.\n* As suggested by **Reviewers m2ax and YVLM**, we now acknowledge the ordinal-only evaluation as a limitation (due to the lack of metric GT, a well-known unsolved data problem) and have added a full list of the suggested SOTA metric-depth models (UniDepth, Metric3D, etc.) to our **\"Open questions\" (Sec. 5)** as a key direction for future work.\n* To answer **Reviewer YVLM**'s question on curved surfaces, we have added **Fig. 11a**, which explicitly demonstrates LVP's successful generalization to this case.\n* As suggested by **Reviewer YVLM**, we have revised the text throughout the paper (e.g., **Sec. 1, 3.1, 5**) to be more precise, referring to our method as \"two-layer\" where appropriate.\n\n### 3. Elaboration & Writing:\n* As suggested by **Reviewers sERC, m2ax, and YVLM**, we have clarified our statement of our core hypothesis. We have replaced the \"high-pass = background\" intuition with a more robust **\"mode-switching\" hypothesis** in **Sec. 3.5**. This new justification now consistently explains:\n    * Why LVP flips different models to different layers (the \"inconsistent layer preference\" from **Reviewer sERC**).\n    * Why scaling *hurts* some models but *helps* others, resolving the paradox raised by **Reviewer m2ax** (detailed in **Sec. 4.3**).\n    * Why LVP sometimes *reduces* 2nd-layer preference, as noted by **Reviewer YVLM** (clarified in **Sec. 4.1**).\n* As suggested by **Reviewer sERC**, we have added the cited works on emergent properties to our **Introduction (Sec. 1)**, framing our work within this important context.\n* We have added a clarification to **Sec. 2** (per **Reviewer YVLM**) distinguishing our deterministic LVP from the non-deterministic, slow sampling of diffusion models.\n* To address **Reviewer sERC**'s practical question, we have added a discussion on blending/combining hypotheses to our **\"Open questions\" (Sec. 5)** as a key future direction.\n\n---\n\nFor detailed responses regarding each of the above aspects, please kindly refer to the individual rebuttal windows in the review section.\n\nLast but not least, we sincerely thank our reviewers, ACs, and PCs again for the effort devoted and the constructive suggestions provided. **We welcome further discussion and are happy to address any remaining concerns**!"}}, "id": "1m2cPRQY97", "forum": "QU8rk0eZgp", "replyto": "QU8rk0eZgp", "signatures": ["ICLR.cc/2026/Conference/Submission1835/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1835/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1835/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763023128443, "cdate": 1763023128443, "tmdate": 1763023128443, "mdate": 1763023128443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}