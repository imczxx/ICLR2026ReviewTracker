{"id": "nc28mSbyVG", "number": 23817, "cdate": 1758348816453, "mdate": 1759896795541, "content": {"title": "Swap-guided Preference Learning for Personalized Reinforcement Learning from Human Feedback", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a widely used approach to align large-scale AI systems with human values. However, RLHF typically assumes a single, universal reward, which overlooks diverse preferences and limits personalization. Variational Preference Learning (VPL) seeks to address this by introducing user-specific latent variables. Despite its promise, we found that VPL suffers from posterior collapse. While this phenomenon is well known in VAEs, it has not previously been identified in preference learning frameworks. Under sparse preference data and with overly expressive decoders, VPL may cause latent variables to be ignored, reverting to a single-reward model. To overcome this limitation, we propose Swap-guided Preference Learning (SPL). The key idea is to construct fictitious swap annotators and use the mirroring property of their preferences to guide the encoder. SPL introduces three components: (1) swap-guided base regularization, (2) Preferential Inverse Autoregressive Flow (P-IAF), and (3) adaptive latent conditioning. Experiments show that SPL mitigates collapse, enriches user-specific latents, and improves preference prediction. Our code and data are available at https://anonymous.4open.science/r/SPL-0111", "tldr": "We identify posterior collapse in preference learning and introduce SPL—swap-guided encoding and adaptive latent conditioning—to learn expressive, user-specific posteriors. SPL reduces collapse and improves preference prediction.", "keywords": ["Ranking and Preference Learning", "Latent Variable Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d7de278ba786a94b660450a52a03c087b4bf583.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new method for personalized RLHF. It includes an experimental evaluation on two Llama variants (3B, 8B), with the proposed method comparing favorably to the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and easy to understand. It addresses an important and underserved niche of LLM personalization through diverse preferences. The method is well-justified and includes solid theoretical foundations."}, "weaknesses": {"value": "1. In Section 3, the datasets are not yet introduced in any way, which makes it quite confusing to read.\n2. No confidence intervals in Table 1.\n3. Evaluations are based on completely synthetic preferences, not actual individual human preferences.\n4. The paper does not provide the Pets dataset in meaningful detail, it also seems to be missing from the code release.\n5. The performance improvement is relatively modest, which might be worth it depending on the computational cost trade-off."}, "questions": {"value": "1. Can you run additional experiments to obtain tighter confidence bounds in Table 2? Some of the values are quite close to one another, making it hard to draw conclusions.\n2. What is the computational overhead of using this method, over the baselines?\n3. How exactly was the Pets dataset constructed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RRkZILcSSe", "forum": "nc28mSbyVG", "replyto": "nc28mSbyVG", "signatures": ["ICLR.cc/2026/Conference/Submission23817/Reviewer_cHBi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23817/Reviewer_cHBi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23817/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580007561, "cdate": 1761580007561, "tmdate": 1762942819663, "mdate": 1762942819663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of posterior collapse in Variational Preference Learning for personalized reinforcement learning from human feedback, where user-specific latent variables often become uninformative and default back to a single reward model. \nThis paper propose Swap-guided Preference Learning (SPL), which leverages the mirroring property of swapped preferences and introduces three components: swap-guided base regularization, P-IAF, and adaptive latent conditioning. \nExperiments show that SPL avoids collapse, stabilizes latent representations, and achieves higher preference-prediction accuracy than baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies posterior collapse in personalized preference learning and introduces the intuitive idea of swap-guided mirroring, where swapping preferences flips the latent mean but keeps variance invariant, offering a novel and insightful diagnostic lens.\n2. SPL integrates swap-guided base regularization, P-IAF, and adaptive latent conditioning into a coherent framework, directly addressing collapse while preserving user-specific information. The design is principled, interpretable, and builds effectively on established variational methods."}, "weaknesses": {"value": "1. SPL introduces many additional hyper parameters, like $\\beta, \\gamma, \\eta$, but does not analyze how robust these hyperparameters are or how much tuning would cost."}, "questions": {"value": "1. How would you use the personalized reward model in policy model training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3VmMidSY5x", "forum": "nc28mSbyVG", "replyto": "nc28mSbyVG", "signatures": ["ICLR.cc/2026/Conference/Submission23817/Reviewer_QmDu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23817/Reviewer_QmDu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23817/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855390684, "cdate": 1761855390684, "tmdate": 1762942819349, "mdate": 1762942819349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles an important problem of personalizing RLHF to diverse users. It builds on existing paradigm that uses variational inference to model different users. The main focus of this paper is on the widely known “posterior collapse” problem with VAEs. The authors introduce swap annotators to the preference dataset i.e. effectively data augmentation by flipping the context labels and the target user preference order to create more data that voids posterior collapse. Further, they introduce a more expressive prior with a normalizing flow based model, and run experiments on preference modelling across two datasets. The experiments show that the final method is robust to posterior collapse and achieve a mean higher accuracy on preference modelling over the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written, with the motivation and background work sufficiently laid out.\n- The authors clearly expose the posterior collapse problem with experimental evidence. This is a strong motivation towards explaining the issues with priors works and motivating the solutions introduced.\n- The data augmentation technique for regularisation seems to be very effective and low-overhead, which potentially makes it very efficient.\n- The P-IAF architecture introduced ensures that the regularisation and the mirror property hold after the transformations. Also present a theoretical justification for the approximation with the swap-invariant and swap-depedent variables.\n- The authors show the validity of the method on multiple datasets againts multiple baselines."}, "weaknesses": {"value": "- An issue with the method is that it assumes that the user-context is provided via binary preference labels. This doesnt seem to be scalable as more recent works [1], have focused on expanding user context to muti-turn dialogue. It would be interesting if the authors could discuss the applicability of the introduced regularisation to other forms of context.\n- The swap based data augmentation seems to be a very interesting contribution. If the authors could include a baseline that trains a VPL based model with only the additional mirrored data, it could further show the benefit of the additional contributions beyond the swap guided pairs.\n- In Table 3:, The adaptive latent conditioning and base regularisation provide negligible improvement to the modelling accuracy, which makes it hard to justify their contribution to the overall performance.\n- Overall, the paper introduces interesting and know-techniques to improve preference modelling under diverse users. While the problem setting and the motivation of the solution is completely justified, the individual components introduced and the ablations over them provide relatively weak signals. If the authors are able to answer my questions, and provide additional ablations or experiments to strengthen their claims I would be happy to increase my score.\n\n[1] Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward. Yanming Wan, Jiaxing Wu, Marwa Abdulhai, Lior Shani, Natasha Jaques"}, "questions": {"value": "- In Figure 3b is the non-collapsed posterior trained via VPL or SPL? I.e does the sign reversal happen under the original dataset or the augmented dataset?\n- The swap based regularisation assumes that the dataset contains context and preference pairs where the preference order is always dependent on the user context. But this would introduce wrong labels for preference pairs that are independent of the context i.e. same label regardless of the context. How do the authors resolve this issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VR2ALcQm3i", "forum": "nc28mSbyVG", "replyto": "nc28mSbyVG", "signatures": ["ICLR.cc/2026/Conference/Submission23817/Reviewer_YXX7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23817/Reviewer_YXX7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23817/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955852339, "cdate": 1761955852339, "tmdate": 1762942819134, "mdate": 1762942819134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that a leading personalization method, Variational Preference Learning, suffers from posterior collapse, causing user latents to be ignored. It proposes Swap-guided Preference Learning (SPL), which exploits a mirroring property of preference pairs by constructing fictitious “swap” annotators and enforcing sign-reversal of posterior means while keeping variances invariant. SPL combines (1) swap-guided base regularization, (2) a Preferential Inverse Autoregressive Flow (P-IAF) to disentangle swap-reversal and swap-invariant signals, and (3) adaptive latent conditioning for the reward decoder. Experiments demonstrate that SPL reliably prevents collapse across KL weights and improves preference-prediction accuracy over strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper surfaces posterior collapse in preference learning (especially for VPL), provides detailed diagnostics of why the user latent is ignored, and introduces a swap-guided remedy centered on the mirrored “swap” property to keep the latent informative.\n2. SPL is derived from a clear ELBO with a swap-guidance regularizer. Analyses show how base regularization and the proposed P-IAF reduce swap-mismatch and prevent cross-context leakage, which explains why the posterior should not collapse.\n3. Across KL weights, SPL maintains substantially higher Active Units than VPL (particularly on UF-P-4), indicating non-collapsed, informative user latents."}, "weaknesses": {"value": "1. Some recent methods reported results on UF-P and are discussed by the authors but not included in experiments (e.g., Nam et al., 2025), making it hard to situate SPL’s gains against the newest alternatives. Adding these would strengthen claims.\n2. The approach adds flow-based inference (P-IAF) and swap-guidance terms, which plausibly increase compute and memory, but the paper does not report training time, inference latency, or budget-constrained comparisons. Reporting these costs would clarify practicality."}, "questions": {"value": "1. What are SPL’s training and inference compute overheads relative to VPL/VPL-IAF?\n2. Is SPL robust to noisy labels (e.g., label-flip noise) which is common in real-world settings?\n3. How well does SPL generalize to unseen but in-distribution user contexts (i.e., train/test contexts are disjoint but user classes remain fixed)?\n4. How does SPL compare to more recent personalized-reward baselines on UF-P (e.g., Nam et al., 2025)?\n5. How were key hyperparameters chosen, and how did their values influence Active Units and accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aYqTtkC6Zl", "forum": "nc28mSbyVG", "replyto": "nc28mSbyVG", "signatures": ["ICLR.cc/2026/Conference/Submission23817/Reviewer_yo5D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23817/Reviewer_yo5D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23817/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106174198, "cdate": 1762106174198, "tmdate": 1762942818929, "mdate": 1762942818929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}