{"id": "Bi7CPaOMHC", "number": 23996, "cdate": 1758351442033, "mdate": 1759896787319, "content": {"title": "RAID: A Benchmark Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors", "abstract": "AI-generated images have reached a quality level at which humans are incapable of reliably distinguishing them from real images. To counteract the inherent risk of fraud and disinformation, the detection of AI-generated images is a pressing challenge and an active research topic.\nWhile many of the presented methods claim to achieve high detection accuracy, they are usually evaluated under idealized conditions. In particular, the $\\textit{adversarial robustness}$ is often neglected, potentially due to a lack of awareness or the substantial effort required to conduct a comprehensive robustness analysis. In this work, we tackle this problem by providing a simpler means to assess the robustness of AI-generated image detectors. We present $\\textbf{RAID}$ ($\\textbf{R}$obust evaluation of $\\textbf{AI}$-generated image $\\textbf{D}$etectors), a benchmark dataset of 72k diverse and highly transferable adversarial examples.\nThe proposed dataset is created by running attacks against an ensemble of seven state-of-the-art detectors and images generated by four different text-to-image models.\nExtensive experiments show that our methodology generates adversarial images that transfer with a high success rate to unseen detectors, which can be used to quickly provide an approximate yet still reliable estimate of a detector's adversarial robustness. Our findings indicate that current state-of-the-art AI-generated image detectors can be easily deceived by adversarial examples, highlighting the critical need for the development of more robust methods.", "tldr": "", "keywords": ["AI-generated image detection", "adversarial robustness"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb180285427136e5e3f299bda03b1123222f382f.pdf", "supplementary_material": "/attachment/78dcb03990af91de0b32e0e079009e81a18473ee.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides a new benchmark on evaluating the Adversarial Robustness of AI-generated image Detectors based on ensemble transfer-based adversarial examples. A diverse set og text-to-image models and detection methods are considered."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written, and the technical details are sufficient for basic understanding.\n2. Multiple detectors and generative models are tested.\n3. White-box scenarios are also presented for comparison."}, "weaknesses": {"value": "1. No detectors from 2025 are considered, for both ensemble and evaluation. Considering that AIGI detection is quite a hot topic, it is necessary to test methods published recently. \n\n2. Only PGD with 10 iterations and a step size of 0.05 is tested. Although APGD and CWA are discussed in the appendix, the experiments are not thorough. More importantly, only one transfer technique, CWA, is considered. This is insufficient because this paper focuses on transfer attack settings. The authors should further consider more representative transfer techniques, such as MI, DI, TI, and SI. Please see https://xiaosenwang.com/transfer_based_attack_papers.html for a curated list of transfer attack papers. It is also weird that CWA, which is explicitly designed for transferability, performs even worse than PGD.\n\n3. No comparisons to existing attacks specifically designed for AIGI detection, especially those mentioned in Section 4 Related Work: Adversarial Robustness of AIGI Detectors.\n\n4. The authors claim that “The adversarial robustness is often not investigated in works proposing synthetic image detectors.” The reviewer does not think this is a problem, but a reasonable common practice. This is just like every paper that proposes a new technique (e.g., model architecture, loss, etc) for improving the general performance in a specific task would not test the adversarial robustness. Adversarial robustness is a quite independent problem that just considers the ideal, worst-case scenarios.\n\n5. The authors state that “each generated image is post-processed such that the image format and compression strength match that of the real distribution present in the corresponding real image.” Please provide more details about this. In particular, were the generated images originally generated with the same size or post-resized to the same size?\n\n6. Fooling the detector for misprediction \"from fake to real\" and \"from real to fake\" are both considered. The former makes sense and was commonly studied in the literature because it allows bypassing the detector with a fake image. However, the real-world meaning of the latter should be discussed.\n\n7. The first paragraph is too long. Starting the paper directly with the AIGI detection is enough. Instead, it makes more sense to use more content to introduce more details about RAID. Now, even the statistics of the dataset are not mentioned in the Introduction, making the significance of the contribution unclear. More specifically, it is better to have a table to show the experimental setting, e.g., the data splits and selection of generative models."}, "questions": {"value": "See the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RRKJCQBfs3", "forum": "Bi7CPaOMHC", "replyto": "Bi7CPaOMHC", "signatures": ["ICLR.cc/2026/Conference/Submission23996/Reviewer_vxxp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23996/Reviewer_vxxp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789455015, "cdate": 1761789455015, "tmdate": 1762942890600, "mdate": 1762942890600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a practical but often overlooked issue in evaluating AI-generated image (AIGI) detectors: most current evaluations assume ideal, non-adversarial settings and lack a standard way to test robustness against attacks. To address this, the authors introduce RAID, a benchmark dataset built to measure how well AIGI detectors withstand adversarial perturbations.\nRAID includes 96,000 images, among which 72,000 are adversarial examples generated using the PGD method with three perturbation levels (ε = 8/255, 16/255, 32/255). These are created from four popular text-to-image diffusion models (such as the Stable Diffusion family and DeepFloyd IF) and produced by attacking seven representative AIGI detectors with diverse architectures.\nExperiments show that RAID’s adversarial samples transfer well to unseen detectors and can effectively reveal weaknesses in robustness. Even the best-performing models, such as the ConvNeXt-based detector from Chen et al. (2024a), suffer large drops in F1 score under attack. RAID also generalizes across ten additional generative models, four baseline detectors, and two commercial detectors (Sightengine and HIVE), demonstrating its broad applicability. While the main contribution lies in dataset and benchmark construction rather than new algorithms or theory, the work provides a valuable tool for systematic robustness evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper fills a clear gap in current research: there has been no standardized dataset for testing the adversarial robustness of AIGI detectors. RAID provides a reproducible and comparable benchmark that can guide future studies.\n2. Well-designed dataset\n(1) By combining seven different detector architectures (ResNet-based, CLIP-based, and patch-level models) during attack generation, the resulting adversarial samples are not tied to any single model, making them more general and closer to real-world black-box scenarios.\n(2) The dataset covers 96,000 images from four major generators, carefully matching real-image compression and format to reduce bias. Three perturbation budgets allow for both imperceptible and stronger attacks, fitting different robustness evaluation settings.\n(3) The dataset provides lightweight evaluation scripts, and images are stored in PNG format to avoid additional compression loss.\n3. Comprehensive and careful experiments\n(1) RAID is tested on a range of models, including extra baseline detectors and commercial systems, confirming its robustness and generality.\n(2) The ensemble PGD approach is compared with other attacks such as APGD and CWA, showing stronger and more stable performance. \n(3) Using F1, accuracy, and AUROC avoids bias from any single measure and provides a fuller view of robustness degradation."}, "weaknesses": {"value": "1. The contribution is mainly engineering-focused. It builds on established methods like PGD and ensemble attack concepts without proposing new theoretical ideas or insights into the mechanisms behind detector robustness.\n2. While the authors note that RAID should evolve with new generative models, they do not describe how future updates will be handled. The dataset relies on diffusion-based generators, which may limit its relevance once new architectures emerge. No clear plan for versioning or community maintenance is mentioned.\n3. The adversarial examples are not optimized for robustness under image operations such as cropping or JPEG compression. Although the authors focus on “worst-case” scenarios, real-world AIGI often goes through such transformations, which can reduce attack effectiveness.\n4. Missing details:\n(1)The results for smaller perturbations (ε = 8/255) are only briefly discussed and not well analyzed, leaving the reasons for weaker transfer unexplored.\n(2)The evaluation on commercial detectors is based on a very small sample (50 real and 50 generated images per model), which makes the results less reliable.\n5. The study does not suggest any possible defense methods or insights drawn from RAID. Even a short discussion on how RAID’s findings might inspire robustness improvements would add value."}, "questions": {"value": "1. How was the “matching compression level” between generated and real images determined? Was it derived from LAION metadata or measured automatically? The criteria for selecting the 4,800 new real images (e.g., scene diversity, resolution) should also be clarified.\n2. What is the rationale for using 10 PGD iterations and a 0.05 step size? Was this combination empirically optimized or simply adopted from prior work?\n3. Visual perceptibility of adversarial samples: The paper relies solely on ε values to claim imperceptibility, but does not conduct human perceptual validation (e.g., user studies). If ε = 32/255 produces visible noise, its practicality as a realistic adversarial example is questionable.\n4. The authors describe RAID as “engineering infrastructure,” but do they plan to extract theoretical insights from RAID’s results (e.g., correlations between robustness and architecture design)? Clarifying this could enhance the paper’s academic contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0KsuiQ3Kl1", "forum": "Bi7CPaOMHC", "replyto": "Bi7CPaOMHC", "signatures": ["ICLR.cc/2026/Conference/Submission23996/Reviewer_fGJU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23996/Reviewer_fGJU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812538317, "cdate": 1761812538317, "tmdate": 1762942890423, "mdate": 1762942890423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RAID, a large benchmark of precomputed adversarial examples for AI-generated image detectors. RAID is produced by attacking an ensemble of seven detectors on images synthesized by multiple text-to-image models and saved at three perturbation levels. The authors claim these adversarial examples are highly transferable and that RAID can serve as a fast standardized test for a detector’s adversarial robustness. They provide white-box, black-box, and ensemble attack experiments and proposed benchmark dataset\nconsists of 72,000 adversarial examples – 24,000 adversarial examples for each attack parameter $\\epsilon(\\frac{8}{255}, \\frac{16}{255}, \\frac{32}{255})$ – in addition to original images, for a total of 96,000 images."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Evaluating adversarial robustness of AIGI detectors is timely and societally relevant as image generation quality improves; providing a standardized benchmark addresses a real community need.\n\n2. The authors evaluate multiple published detectors and multiple generative models, improving the generality of the findings.\n\n3. The dataset consists 96k files which is a substantial engineering contribution that will help the researchers to test detectors."}, "weaknesses": {"value": "1. The authors used cross-entropy loss toward a target class (Eq. (1)–(3)) but the text does not clearly state whether attacks are targeted or untargeted in practice nor how the target label is chosen. This is extremely important as targeted and untargeted attacks have different success/transferability properties and different real-world relevance. The threat model needs a precise, explicit statement. \n\n2. The paper re-trains weaker detectors on the D3 training set while leaving three detectors with original weights. The stated reason is to avoid creating adversarial examples from detectors that already perform poorly on D3. However, re-training detectors on the same D3 distribution which is also used to craft the attacks and to build RAID risks data / attack circularity and may bias the benchmark toward vulnerabilities specific to that dataset/ensemble. The mix of re-trained and original weights also complicates interpreting transferability. If ensemble and attack generation are closely related to D3 then RAID might overestimate transferability to detectors trained on different datasets or real deployed detectors. \n\n3. The adversarial perturbations are saved as PNG tensors (lossless) and the authors explicitly do not optimize for robustness to real post-processing (JPEG, rescaling, social-media recompression). They acknowledge this limitation but nevertheless claim RAID is a reliable estimate of robustness. Generally, AIGIs are shared through pipelines. Attacks that fail after common transformations (or that become visible/obvious) are less relevant in practice. The authors show some transformation experiments in appendix but leave out important real pipelines\n\n4. The authors use ensemble PGD as their main attack and compare adaptions of APGD and CWA. However, the adversarial robustness literature emphasizes diverse, adaptive, and strong attacks plus gradient-free approaches for transferability. Some of these are only briefly discussed in Appendix and the APGD adaptation is argued to be ineffective without deeper diagnostics. A benchmark claiming to reflect “strong transferable attacks” should show RAID withstands other strong attack classes. \n\n5. The paper reports single number digits in the main tables. They have mentioned variance over 5 seeds in the appendix only for a few tables and the main text mostly uses point estimation. For an empirical dataset/benchmark, error bars are necessary."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "The ethics statement acknowledges dual-use but largely dismisses the danger by stating 'our work does not introduce any new attack methodologies'. This is insufficient according to me because publishing a large scale, easy to use attack corpus lowers the bar for malicious acotrs."}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZdeHNK5jLJ", "forum": "Bi7CPaOMHC", "replyto": "Bi7CPaOMHC", "signatures": ["ICLR.cc/2026/Conference/Submission23996/Reviewer_jmcu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23996/Reviewer_jmcu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824794189, "cdate": 1761824794189, "tmdate": 1762942889828, "mdate": 1762942889828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}