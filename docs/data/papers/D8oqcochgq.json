{"id": "D8oqcochgq", "number": 3395, "cdate": 1757417898548, "mdate": 1759898092379, "content": {"title": "Product-Quantised Image Representation for High-Quality Image Synthesis", "abstract": "Product quantisation (PQ) is a classical method for scalable vector encoding, yet it has seen limited usage for latent representations in high-fidelity image generation.\nIn this work, we introduce PQGAN, a quantised image autoencoder that integrates PQ into the well-known vector quantisation (VQ) framework of VQGAN. \nPQGAN achieves a noticeable improvement over state-of-the-art methods in terms of reconstruction performance, including both quantisation methods and their continuous counterparts. We achieve a PSNR score of 37dB, where prior work achieves 27dB, \nand are able to reduce the FID, LPIPS, and CMMD score by up to 96\\%. \nOur key to success is a thorough analysis of the interaction between codebook size, embedding dimensionality, and subspace factorisation, with vector and scalar quantisation as special cases. We obtain novel findings, such that the performance of VQ and PQ behaves in opposite ways when scaling the embedding dimension. Furthermore, our analysis shows performance trends for PQ that help guide optimal hyperparameter selection.\nFinally, we demonstrate that PQGAN can be seamlessly integrated into pre-trained diffusion models. This enables either a significantly faster and more compute-efficient generation, or a doubling of the output resolution at no additional cost, positioning PQ as a strong extension for discrete latent representation in image synthesis.", "tldr": "", "keywords": ["Vector Quantisation", "Representation Learning", "Diffusion Models", "Generative Models", "Image Synthesis"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8b1a93fa64e6457afe7b3b842389b5f654e3d1eb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PQGAN, a novel quantised autoencoder that uses product quantisation (PQ) instead of the traditional vector quantisation (VQ) used in models like VQGAN. The simple yet effective idea is to splits the vector into S subspaces, each quantized separately. This allow a codebook which is exponentially bigger than the VQ codebook, depending on the dimension of the subspaces (with one subspace only, PQ reduces to VQ). \n\nThe motivation behind PQGAN is that standard VQ suffers from training sparsity, codebook collapse, and redundancy: PQGAN addresses this by factorising each latent vector into subspaces, quantising each independently.\n\nOn ImageNet 256×256, PQGAN outperforms the  baselines and achieves high fidelity with small codebooks (K = 128–512).\n\nThe paper carries on a detailed analysis on codebook usage (in terms of Perplexity and Entropy) and metrics are evaluated against size of the codebook, number of subspaces and latent embedding size.\n\nFinally, PQGAN was integrated into Stable Diffusion 2.1, in three variants improving generation results."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Novelty: despite being simple, the idea is novel and effective.\n\n- The paper conducts a systematic analysis of how embedding dimension, number of subspaces, and codebook size interact, covering the full spectrum between scalar and vector quantisation.\n\n- Extensive experiments on ImageNet, FFHQ, and LSUN establish both quantitative and qualitative superiority. Reported improvements are substantial and consistent.\n\n- Integration with Stable Diffusion is a useful addition to the method and is carefully validated, showing also computational benefit.\n\n- I have appreciated the analysis of codebook utilisation, with entropy and perplexity evaulations. Codebook utilization is an issue in standard VQ-VAEs \n\n- The paper is clearly written and well-structured. The presentation of results is well-designed"}, "weaknesses": {"value": "I think the main weakness of the paper is the substantial lack of theoretical foundation: PQ seems to be a well-established technique in signal processing and vector encoding [1], and its application to autoencoders has appeared in other context contexts (e.g., El-Nouby et al., 2023; Mentzer et al., 2020) as highlighted in the paper itself.\n\nA deeper theoretical argument for why PQ improves latent scalability and codebook utilization (rather than relying only on empirical evidence) may strengthen the paper.\n\nThe paper also claims that PQ and VQ behave “in opposite ways when scaling the embedding dimension,” but this is again presented empirically without theoretical insight.\n\n[1] Product Quantization for Nearest Neighbor Search, Jegou et al. - TPAMI 2011"}, "questions": {"value": "Please refer to the \"Weakness\".\n\nI think the paper would benefit from more insights (possibly theoretical) on the reasons why PO obtains such successful results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bUDiknsLXL", "forum": "D8oqcochgq", "replyto": "D8oqcochgq", "signatures": ["ICLR.cc/2026/Conference/Submission3395/Reviewer_rVnn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3395/Reviewer_rVnn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578581184, "cdate": 1761578581184, "tmdate": 1762916704392, "mdate": 1762916704392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PQGAN, which incorporates product quantisation (PQ) into the VQ-VAE framework. PQGAN achieves state-of-the-art reconstruction performance, significantly improving metrics such as PSNR, FID, LPIPS, and CMMD compared to existing methods. Furthermore, the paper demonstrates that PQGAN can be seamlessly integrated with pre-trained diffusion models, resulting in faster or higher-resolution image generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- PQGAN demonstrates significant improvements in image reconstruction compared to state-of-the-art methods."}, "weaknesses": {"value": "- The novelty of the overall approach is very limited. PQ has been used for some time in the field of VQ-VAE, e.g., UniTok [a]. This paper is more like a technical report than a research paper, and it proposes few new ideas or inspirations.\n\n- The proposed latent adaptation is also very straightforward; reducing the spatial resolution of the VAE and increasing the number of channels are common techniques for reducing computation when training diffusion models. This application does not necessarily demonstrate that PQ brings any additional benefits.\n\n- The comparison with previous works is unfair, as this method uses significantly more tokens than other VQ-VAEs.\n\n[a] Unitok: A unified tokenizer for visual generation and understanding. C Ma, Y Jiang, J Wu, J Yang, X Yu, Z Yuan, B Peng, X Qi. arXiv preprint arXiv:2502.20321"}, "questions": {"value": "- The authors are suggested to highlight the contribution of this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GDErQ99zqQ", "forum": "D8oqcochgq", "replyto": "D8oqcochgq", "signatures": ["ICLR.cc/2026/Conference/Submission3395/Reviewer_Jhoa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3395/Reviewer_Jhoa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749546649, "cdate": 1761749546649, "tmdate": 1762916703947, "mdate": 1762916703947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PQGAN, a novel way to quantize the latents of the auto encoders using product quantized learning. This method factorizes the high dimensional latent channel into many smaller, independent subspaces, each quantized with their own codebook. This method improve the reconstruction quality of the VAE and achieves a state-of-the-art 37.4 dB PSNR. The authors argue that the spatial resolution is the main bottleneck and through their method, they can operate at a lower spatial resolution but with a much higher channel dimension. This enables the method to generate larger resolution images or get speed up upto 4x."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1)  PQGAN achieves very high reconstruction fidelity, 37.4 dB PSNR, which is higher than 25.3 dB of the standard Stable Diffusion VAE and other methods.\n\n2) The paper demonstrates a novel finding the product quantization improves the reconstruction quality in VAEs\n\n3) The method can either double the output image resolution or achieving a 4x generation speedup at the same resolution, This is achieved with the same cost."}, "weaknesses": {"value": "1) The paper fails to provide  quantitative comparison (e.g., FID, CLIP Score) between the generations of its adapted PQSD model and the original Stable Diffusion.\n\n2) The independent learning of codebooks might not learn complex correlations as the number of subspaces increase. In the limit it is as if sampling independently from each dimension. The tend in the paper also shows that.\n\n3) Since multiple indices are associated with the same pixel, it is incompatible with the autoregressive models. Only Flow based models and diffusion models can benefit from this."}, "questions": {"value": "1) The paper does not provide quantitative generative metrics like FID or CLIP score to compare their with other methods. This is the main limitation and it would be good to see these scores to validate the claims in the paper. Why are these evaluations not in the paper? \n\n2. Discuss more about the tradeoff in weakness 2). What is the cut-off?\n\n3. 50% increase in the inference cost seems high. What is the increase in the training time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q7PD6JQUcW", "forum": "D8oqcochgq", "replyto": "D8oqcochgq", "signatures": ["ICLR.cc/2026/Conference/Submission3395/Reviewer_gjYk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3395/Reviewer_gjYk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966952045, "cdate": 1761966952045, "tmdate": 1762916703728, "mdate": 1762916703728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PQGAN, a novel quantised image autoencoder that utilizes Product Quantisation (PQ) to achieve state-of-the-art fidelity for latent image representations, especially for high-quality image synthesis. PQGAN is integrated into pre-trained diffusion models, like Stable Diffusionl, which provides significantly faster and more compute-efficient generation, and improve the output resolution. \nThe paper also reports difference behavior of VQ and PQ with respect to embedding dimension."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "PQGAN surpasses both existing quantisation methods and continuous autoencoders in reconstruction quality"}, "weaknesses": {"value": "This work is not the first to consider PQ (or RQ) for image compression, which is mentioned in the paper, yet the phrasing is still misleading in some places (abstract). That's being said, the method of this paper is much better. It would be worth ablating and analyzing more why. \n\nFor the mundane reader, it would be worth citing the paper that has introduced/popularized product quantization (\"Product quantization for nearest neighbor search\"). \n\ntypo L329: benifits"}, "questions": {"value": "From Table 1, I understand that the big improvement in PSNR comes from using a resolution of 32x32, while the 16x16 patch size is only offering a PSNR of 28.3. \nHave you tried larger patch sizes? \n\nSimilarly, RVQ is not evaluated in the context of compression, and only reported with patch size of 8x8. So I wonder if the better performance of PQ in this context is simply due to a better hyper-parameter choice of the patch size. Have you tried replacing PQ by a RVQ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9G6aJLsUiW", "forum": "D8oqcochgq", "replyto": "D8oqcochgq", "signatures": ["ICLR.cc/2026/Conference/Submission3395/Reviewer_Nh49"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3395/Reviewer_Nh49"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762187739301, "cdate": 1762187739301, "tmdate": 1762916703500, "mdate": 1762916703500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}