{"id": "xc9gn0fd19", "number": 11608, "cdate": 1758202450025, "mdate": 1759897564907, "content": {"title": "Strategema: Probabilistic Analysis of Adversarial Multi-Agent Behavior with LLMs in Social Deduction Games", "abstract": "Social deduction games, like Mafia, are rich testbeds for adversarial multi-agent interactions, featuring deception, coalition formation, and reasoning under uncertainty. While Large Language Models (LLMs) have shown promise in modeling human-like behavior, their use as a laboratory for \\textit{quantitative}, \\textit{probabilistic} analysis of adversarial strategies remains underexplored. We introduce \\textbf{Strategema}, a simulation framework that leverages LLMs to power agents who maintain explicit Bayesian belief models about other players' roles and use them to make informed decisions. Through extensive experiments (400 games across four configurations) varying player counts and adversary ratios, we uncover fundamental patterns in deception, trust dynamics, and strategic convergence. We move beyond descriptive analysis to show that the \\textit{trajectory of an agent's belief state} is a powerful predictor of game outcomes. Furthermore, we identify systematic biases in LLM-based reasoning, including confirmation bias that impedes belief updating. Our framework provides a novel paradigm for benchmarking strategic reasoning and offers insights into the mechanics of deception in multi-agent systems, with implications for AI safety and multi-agent interaction research.", "tldr": "", "keywords": ["Adversarial", "Multi-Agent Behavior", "LLM", "SDG"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d9fe7881baf02a54014dfe7c06755d307de1db1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Strategema, a framework to play the game Mafia, which is also known as Werewolf. The framework aims at leveraging LLMs to maintain explicit Bayesian belief models about other players’ roles and use them to make informed decisions. Benchmarking-wise, it also introduces some metrics specifically designed for strategic reasoning, e.g., Deception Score."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper studies strategic reasoning from two new perspectives through Deception Score, and Strategic Consistency, and provides statistical significance over them, which may benefit relevant studies\n2. Most of the content of this paper is clear"}, "weaknesses": {"value": "1. The method of \"reasoning with an explicit belief model\" that the paper introduced is not novel [1], and lacks an in-depth study. For example, how reliable is it to rely solely on an LLM for belief updating?\n2. Lack of baseline methods. Based on the experiment section, the enemy agents are unknown. Are they simple ReAct agents? And also it seems that the enemies are always the same, which is insufficient.\n3. Lack of case study. How is a single round of the game look like? This also relates to the game setting -- which characters are included and even what are the game rules? A brief intro may help better understand the context, especially for those who are unfamiliar with the game\n4. How generalizable is the framework? In a much more complicated game, for instance one with more character types and a larger number of players, how would the performance change accordingly?\n\n[1] Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search. J Light, M Cai, W Chen, G Wang, X Chen, W Cheng, Y Yue, Z Hu. ICLR 2025"}, "questions": {"value": "Please see weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7dbvTPQ8VS", "forum": "xc9gn0fd19", "replyto": "xc9gn0fd19", "signatures": ["ICLR.cc/2026/Conference/Submission11608/Reviewer_8rQB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11608/Reviewer_8rQB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552225749, "cdate": 1761552225749, "tmdate": 1762922685003, "mdate": 1762922685003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Strategema, a reproducible framework for social-deduction games in which LLM agents engage in natural-language interaction while maintaining explicit Bayesian beliefs over hidden roles and selecting actions via belief-guided policies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper proposes to pair natural-language play with explicit Bayesian belief states and auditable generation trace in Mafia scenario. This enables principled analysis of deception, coordination, and strategy.\n\n2. The framework is well specified, the experiments are systematic with targeted ablations, and the reporting is clear—overall the paper is easy to follow."}, "weaknesses": {"value": "1. The social-deduction setting (e.g., Werewolf, Avalon) and the use of Bayesian deduction [4] with multiple LLM agents are already well studied and closely mirror this work; however, prior research is neither emphasized nor compared here [1,2,3,5,6], which limits the claimed novelty.\n\n2. The main experiments rely primarily on a single closed-source model (GPT-4.1). The supplementary check on Claude (two settings, 50 games) is informative but limited in scope, underscoring the need for a more robust cross-model evaluation. It would also strengthen the study to include sensitivity analyses (e.g., how token budgets affect deception and consistency scores, and how results vary with prompt/sampling parameters).\n\n3. Self-reported probabilities are directly used as “Bayesian beliefs” without calibration, which can induce over/under-confidence.\n\n[1] Meta Fundamental AI Research Diplomacy Team (FAIR)†, et al. \"Human-level play in the game of Diplomacy by combining language models with strategic reasoning.\" Science 378.6624 (2022): 1067-1074.\n\n[2] Xu, Yuzhuang, et al. \"Exploring large language models for communication games: An empirical study on werewolf.\" arXiv preprint arXiv:2309.04658 (2023).\n\n[3] Lan, Yihuai, et al. \"Llm-based agent society investigation: Collaboration and confrontation in avalon gameplay.\" arXiv preprint arXiv:2310.14985 (2023).\n\n[4] Rahimirad, Shahab, et al. \"Bayesian Social Deduction with Graph-Informed Language Models.\" arXiv preprint arXiv:2506.17788 (2025).\n\n[5] Light, Jonathan, et al. \"Avalonbench: Evaluating llms playing the game of avalon.\" arXiv preprint arXiv:2310.05036 (2023).\n\n[6] Golechha, Satvik, and Adrià Garriga-Alonso. \"Among us: A sandbox for measuring and detecting agentic deception.\" arXiv preprint arXiv:2504.04072 (2025)."}, "questions": {"value": "Please refer to the weakness section. In addition, it would be beneficial to have a takeaway section of insights that could help in building multi agent LLM system."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iDfEIBRYtB", "forum": "xc9gn0fd19", "replyto": "xc9gn0fd19", "signatures": ["ICLR.cc/2026/Conference/Submission11608/Reviewer_n1qz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11608/Reviewer_n1qz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618555039, "cdate": 1761618555039, "tmdate": 1762922684371, "mdate": 1762922684371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Strategema, a simulation environment that uses large language models (LLMs) to play the social deduction game Mafia. Unlike prior work, the agents in this framework maintain explicit Bayesian belief models over other players’ roles, which guide their probabilistic decision-making. Through this novel framework, the paper studies fundamental patterns in deception and strategic decision-making in multi-agent settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper introduces a novel simulation environment for studying LLM behavior in social deduction games. The framework is valuable for examining how deception and decision-making patterns emerge in LLM agents and for eventually comparing those patterns to human behavior in similar settings. The contributions are important for understanding practical risks that could arise from agentic applications of LLMs and for exploring how their integration into human systems might help or harm other participants. The paper appears technically sound, and I find the experiments, analysis, and overall presentation clear and engaging."}, "weaknesses": {"value": "I did not identify any major weaknesses in the paper. However, I believe the manuscript would benefit from another round of editing to address several typos and instances of repetitive phrasing. I have highlighted these and a few related clarity issues in my detailed comments below."}, "questions": {"value": "Major comments:\n• In a typical Mafia game, the Mafia players know who one another are. Is this also the case in your implementation? I could not find this specifically addressed anywhere, but it seems it would have implications for the Belief State of those Mafia agents.\n\n• You specifically note coordination between Villagers. If Mafia members are aware of each other, do you also observe any coordination between them? Does this potentially have broader implications on coordination between LLMs and connections to Theory of Mind?\n\n• The action space is not explicitly defined anywhere, besides a mention of an action being a message. I think it would be helpful to clarify what actions different agents can take and how these are represented.\n\n• In Sec. 3.4 (Experimental Design), when you mention the possible Player and Mafia counts, it is unclear that you are only testing a subset of all possible combinations. I think this should be more clearly communicated.\n\n• In Sec. 3.4 you mention running 100 games (25 for each of four configurations) using Claude 3 Sonnet, but in Section 4.1 you say you conduct 25 independent games for two configurations (so 50 total). Could you clarify this discrepancy?\n\n• The difference in Mafia success rates between ChatGPT and Claude are reported to be 5-7%. Would it be possible to report whether this result is statistically significant given the relatively limited number of experiments with Claude (25 vs 100 with ChatGPT)?\n\n• Could you offer any insight into why Claude 3 Sonnet performs worse than GPT-4.1? Have you tried newer Claude or OpenAI models?\n\n• In Sec. 4.3 you mention “established benchmarks from silent Mafia simulations,” but no reference is provided. Are you referring to prior work or establishing these benchmarks yourself?\n\n• Could you elaborate on why the deterministic approach represents a more realistic game balance? Is there literature that indicates humans may not sample probabilistically in such settings? Are there other biases that could offset purely deterministic reasoning?\n\n• Do the LLM agents appear more competent as Mafia or as Villagers? If one team were replaced by humans, which side would you expect to perform better?\n\nMinor comments:\n\n• “From economic negotiations to cybersecurity...” : this statement would benefit from citations.\n\n• Several citations are cited with an in-line style, whereas I believe they should have parentheses. For example, “(Bard et al., 2020)” rather than “Bard et al. (2020)”.\n\n• Many sections contain repetition which disrupts the flow of the paper. For example, in Sec. 3.3 example, the “fallback to uniform distribution” is mentioned both in Structured Response Parsing and again under Fallback Mechanisms. This repetitiveness was the most prevalent stylistic issue throughout the paper.\n\n• Table 1 includes redundant columns: “Ratio” can easily be computed from “Players” and “Mafia.” Furthermore, “Win Rate” could be renamed to “Mafia Win Rate,” which would make both make it more clear and also imply the Villager Win Rate.\n\n• In Table 2, the label “Reduction” is confusing since Mafia success rates increase in the “Without Speaking” condition.\n\n• At the end of Sec. 4.3 there appears to be a small typo as you end with “(22”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n20Biu49YQ", "forum": "xc9gn0fd19", "replyto": "xc9gn0fd19", "signatures": ["ICLR.cc/2026/Conference/Submission11608/Reviewer_Qin8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11608/Reviewer_Qin8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670223655, "cdate": 1761670223655, "tmdate": 1762922683951, "mdate": 1762922683951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces stratagema --- a simulation based framework, built on LLMs, for studying multi-agent dynamics in social deduction games. The LLM based agents utilise explicit belief models and the paper investigates how agent beliefs develop over time, finding that they exhibit certain biases."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper seems to be decently motivated and a good idea."}, "weaknesses": {"value": "Overall the main limitations is that the methods and results are not presented clearly or compellingly enough. \n\nThe introduction is quite short. It could / should give broader motivation and more detail on the contributions. \n\nAn explanatory figure would go a long way in helping the reader quickly understand the core components of the paper / framework. \n\nThe related work feels too long and sometimes tangential, and not always connected enough to the paper's contributions. I think it should also be moved to later in the paper. Sometimes claims are made without citation eg \" Computationally, previous work has analyzed game dynamics using heuristic AI players and through online human studies.\" This section also doesn't seem well strucutred and there is some repetition. \n\nSomewhat limited evaluation, e.g., with only two LLMs used. Also, IIUC, any given game only uses one model, so they don't study games with different models interacting with each other. \n\nThe results regarding the win rates (Table 1) do not seem very interesting to me, or informative of LM based or agent based interaction --- it's obvious that if the ratio of mafia players is greater then the mafia team will win more frequently...\n\nThe methodology says \"we collect comprehensive data to enable both quantitative and qualitative analysis:\" including, e.g, agent reasoning traces and voting history. But the results do not discuss these at all --- it stands out that there are not example reasoning traces from the agents, which can help to relate the quantitative results to intuitive model reasoning. \n\nThe result about ADVANCED DECEPTION ANALYSIS: SEMANTIC COHERENCE AND STRATEGIC DECEPTION are not well explained enough and some of this content should go in the methods section. Again, examples would help. \n\n\nminor \n\nthe citations are not properly formatted in parentheses \n\nfor deception cite: https://arxiv.org/abs/2312.01350"}, "questions": {"value": "How does your agent architecture relate to existing formalisms? Is it a type of bayesian game or POMDP for example?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2sO54gGnyj", "forum": "xc9gn0fd19", "replyto": "xc9gn0fd19", "signatures": ["ICLR.cc/2026/Conference/Submission11608/Reviewer_KbwB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11608/Reviewer_KbwB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934036599, "cdate": 1761934036599, "tmdate": 1762922683448, "mdate": 1762922683448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}