{"id": "HM7OkY1jFw", "number": 12266, "cdate": 1758206746523, "mdate": 1763716767222, "content": {"title": "Shape of Thought: When Distribution Can Matter More than Correctness in Reasoning Tasks", "abstract": "We present the surprising finding that a language model's reasoning capabilities can be improved by training on synthetic datasets of chain-of-thought (CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to the language model's own distribution, making it more amenable to learning. Second, these \"incorrect\" traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use a language model to paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawed CoT traces and study to what extent models are tolerant to these flaws. We demonstrate our findings across the MATH, GSM8K, and Countdown datasets using various language models ranging from 1.5B to 9B across Qwen, Llama, and Gemma models. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process.", "tldr": "Training on synthetic CoT traces, even with wrong final answers, improves reasoning due to aligning with the model's distribution and leveraging partial reasoning steps, outperforming human-annotated data.", "keywords": ["LLM", "Reasoning", "Synthetic Data", "Chain of Thought"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/32c2531e39ef99634427137ea1df543cd1ab83ac.pdf", "supplementary_material": "/attachment/97ffd36233d970214baf8ebbd988ff906601214d.zip"}, "replies": [{"content": {"summary": {"value": "This work explores how distillation from the reasoning of larger models can provide useful training signal to smaller models even when the distillation data is noisy (the reasoning from the larger model yields wrong answers).  Surprisingly, this noisy data can train the model to perform better on the task than human-curated data (human-annotated \"chains-of-thought\").  This is shown by having Gemma-2-27B-IT generate traces for reasoning benchmarks such as GSM8k, MATH500, and Countdown. These traces are separated into two datasets, one where the model got the answer correct, \"G\", and one where the model got the answer incorrect, \"W\". The authors then do SFT on these traces, including an additional dataset of human-curated data, \"H\".  The authors then evaluate these tasks and show that datasets with incorrect traces sampled from stronger models often outperform human-curated datasets. Additionally the authors show that by paraphrasing the human-curated data by the larger model, most of this performance gain can be explained, suggesting that much of the weakness from human curated data is from it being so far out of distribution from the model being trained.\n\nThe authors additionally show that as traces becoming more incorrect (i.e. corrupting traces with models to be increasingly more incorrect), the performance gains slowly degrade. This is paired with a small qualitative analysis suggesting that the reason incorrect traces provide strong signal is because each trace may contain individual steps that are correct and thus provide signal to the weaker model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- I like the hypothesis and the comparison to human-curated CoTs is fascinating! The results for GSM8k and Math500 are also pretty compelling (the main figures).\n- Good set of models are being tested and evaluated."}, "weaknesses": {"value": "1. **The tables and figures feel either cherry-picked or are missing important entries (see below)**\n- Figure 2, why is there no human baseline for the Qwen2.5-1.5B model?\n- Why is there no human dataset for countdown? Without it, the paragraph \"Countdown as an Out-of-distribution Task.\" means little to me -- I don't think this paper is arguing that incorrect traces teach the model just as much as using correct traces. So of course correct traces will yield improvements over \"W\". \n- Figure 5 in the appendix shows more results but I think \"H\" is still it's own line when, if I understood correctly, \"H\" should be 1 line per model (you cannot compare \"H\" trained on G-2B with the Qwen model because that's not a fair comparison).\n- (minor) please put in the actual performances and accuracies, not deltas. (i.e. table 2 and 3)\n\n2. **Most of these findings have been found in \"LLMs Can Easily Learn to Reason from Demonstrations\nStructure, not content, is what matters!\" and I would encourage the authors to cite this work**. It's fairly well known and does a far more indepth analysis on the corruption of long chains-of-thought.  A significant difference to this papers credit is the inclusion of human data and that this paper seems to be looking at \"CoT\" pre-R1 (so not \"long cots\" as the paper I mentioned above explores).\n\n3. **The main conclusions drawn are weakly supported.**  \n- a. \": (i) synthetic traces are closer to the student model’s output distribution and are thus easier to learn from\", there was not an experiment that directly tested this. Table 4 is showing how \"G\", \"W\", and \"H\" all effect models at scale (looking only at G-2B vs G-9B for Math500, so this may not be supported on the other models or tasks). And Table 5 shows lower starting training loss, but again is only on Math500 and G-2B.  *The core finding of this paper is that human curated data is often weaker than noisy model generated data*, these other findings need controlled experiments with multiple tasks and models to be established.\n- b. \" (ii) “wrong” traces frequently contain reusable, partially correct reasoning steps.\" - this is supported by the findings in section 5.5 but the authors only look at a total of 40 traces to draw this conclusion. I don't think this was substantially found. The notion of \"completely incorrect\" in section 5.4 is also strenuous at best, \"We prompt the G-27B-IT model to generate completely flawed CoTs using\nMATH training problems.\" -- what does this mean? A \"completely flawed CoT\" could mean utter gibberish, or it invented a whole new problem to solve, or it could be exactly the same as those that were in \"W\". So what should we gather from this finding? That your distillation data doesn't have to be correct but does need to be incorrect in some specific way to be useful? I think more analysis and more controlled experiments are needed, check \"LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!\" for examples of the type of controlled errors I am thinking of. And to be clear, I saw the appendix section too. While explaining the current setup would be helpful, I think this whole experiment needs more control on what the \"errors\" are, otherwise I cannot detangle these from the previous experiments (\"W\" datasets, etc.) prompting for completely inaccurate traces seems far to open ended to draw any useful conclusions.\n\n\n\n**minor notes**: significance testing would be nice to see here because the deltas can be so small, please use \"Figure 2\" or \"Appendix H\" etc. not just \"2\" or \"H\" etc. Why was 4-shot ICL included in the evaluations? It felt completely out of place. Please show all results for all models and datasets, it may be a large table, but would make a lot of what is being said clearer."}, "questions": {"value": "See my weaknesses for questions per point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "khvBQdt5p7", "forum": "HM7OkY1jFw", "replyto": "HM7OkY1jFw", "signatures": ["ICLR.cc/2026/Conference/Submission12266/Reviewer_BDoS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12266/Reviewer_BDoS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546071380, "cdate": 1761546071380, "tmdate": 1762923204937, "mdate": 1762923204937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors show that, across MATH, GSM8K and Countdown, supervised fine-tuning (SFT) on chain-of-thought traces that reach a wrong final answer can outperform training on correct human-written traces. Experiments are extensive (3 tasks × 4 model sizes × 3 data types) and reproducibility is good (code, prompts, checkpoints promised)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "At multiple model families & scales, trends hold across all, indicating generality within the math domain.\n\nThe paper offers a careful ablation of distribution vs. correctness."}, "weaknesses": {"value": "The core finding of this paper is no longer novel.\n\nAll experiments on math word problems; no evidence for code, commonsense, or scientific reasoning.\n\nNo step-level F1 or human-verified correctness; claims about “faithfulness” rest on 20 hand-checked examples.\n\nHyper-parameter sensitivity and statistical significance are not explored.\n\nOnly one 9B model; no 30B+ or frontier-scale teacher.\n\nNo comparison with RL or verifier-based filtering"}, "questions": {"value": "Run identical pipeline on CodeForces (code) and ARC-c (science); report macro-averaged gain.\n\nImplement R1-style RL with outcome reward on same compute; compare final accuracy and wall-clock cost."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "evCunQ9UQF", "forum": "HM7OkY1jFw", "replyto": "HM7OkY1jFw", "signatures": ["ICLR.cc/2026/Conference/Submission12266/Reviewer_Vexn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12266/Reviewer_Vexn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655819958, "cdate": 1761655819958, "tmdate": 1762923204545, "mdate": 1762923204545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a hypothesis that training language models on incorrect chain-of-thought (CoT) reasoning traces can still improve reasoning performance, sometimes even outperforming training on human-annotated CoTs. The authors conduct extensive experiments across three reasoning benchmarks (MATH, GSM8K, Countdown) and multiple model families (Gemma, Llama, Qwen). They find that synthetic CoTs generated by stronger models often yield better downstream performance than human-written ones. They attribute this to distributional alignment and  partial correctness . They further validate these hypotheses via paraphrasing human traces to align their style with model-generated ones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "a.This paper emonstrates that distributional closeness can outweigh final-answer correctness is both surprising and practically significant for data curation strategies.\n\nb.Multiple models with different parameters are evaluated.\n\nc.Results consistently show that synthetic CoTs outperforms human-written CoTs.\n\nd.The authors validate their hypothesis by paraphrasing the human-written CoTs."}, "weaknesses": {"value": "a. The use of abbreviations like L-8B and W (for wrong CoTs) hinders immediate comprehension. The authors should adopt more explicit and standard notation, such as $\\mathbf{Llama-8B}$ and \"Incorrect CoTs\" or \"Wrong CoTs\", to improve readability.\n\nb. The training dataset is relatively small (under 1,000 examples) and restricted to the mathematical domain. This raises questions about the generalizability of the conclusions to other reasoning domains (e.g., commonsense, logical deduction) and larger-scale SFT.\n\nc. The comparison between human-written and synthetic CoTs may be confounded by stylistic differences (e.g., length, verbosity, level of detail) rather than solely correctness. Human-written traces are noted as being significantly shorter, which could be an independent variable.\n\nd. It remains unclear if this strong conclusion holds true across other benchmarks or against human-written CoTs that are confirmed to be of a much higher baseline quality."}, "questions": {"value": "How does the quality of wrong CoTs influence performance after SFT? Potential factors of quality are response length, proportion of correct reasoning steps and clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EwXELQLbOE", "forum": "HM7OkY1jFw", "replyto": "HM7OkY1jFw", "signatures": ["ICLR.cc/2026/Conference/Submission12266/Reviewer_KHFM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12266/Reviewer_KHFM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809074464, "cdate": 1761809074464, "tmdate": 1762923204142, "mdate": 1762923204142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the conventional assumption that correctness is the primary determinant of data quality in reasoning tasks for LLMs, proposing instead that distributional similarity between synthetic data and the model's own output can be more critical. Through experiments on benchmarks like MATH, GSM8K, and Countdown, the authors demonstrate that training on model-generated chain-of-thought (CoT) traces with incorrect final answers (Category 3 data) can outperform human-annotated correct traces. The study also explores mechanisms like paraphrasing human CoTs to align with model distributions and gradually introducing flawed CoTs to quantify error tolerance, providing insights into how models learn from partially correct reasoning steps."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1）The paper directly questions the \"correctness equals quality\" dogma, suggesting that distributional proximity may outweigh final-answer accuracy. This idea opens new avenues for data curation, such as leveraging incorrect CoTs to avoid discarding useful signals, which is particularly valuable for resource-constrained scenarios in data-centric LLM research.\n\n2）This paper designs rigorous experiments to validate their hypotheses. For instance, the paraphrasing experiment—where human CoTs are rewritten by an LLM to better match the model's distribution—shows improved performance, directly supporting the distribution similarity claim. Additionally, the error tolerance experiment progressively introduces fully flawed CoTs, revealing that models tolerate up to 25% errors without significant performance drop, highlighting their ability to extract signals from localized mistakes."}, "weaknesses": {"value": "1）Experiments are confined to models ranging from 1.5B to 9B parameters, while modern LLMs often scale to hundreds of billions. Larger models may exhibit different sensitivities to noise or greater reliance on correctness, and the scaling behavior remains unexplored, limiting the generalizability of the findings.\n\n2）Synthetic CoTs are generated by stronger models (e.g., Gemma-27B-IT), which could introduce biases. If the teacher model has systematic errors, the student model might amplify them, potentially compromising the reliability of the approach."}, "questions": {"value": "1）On the Countdown task, G data (correct answers) clearly outperforms W data (incorrect answers), as shown in Figure 2, but the paper does not deeply investigate why distribution similarity fails to dominate in this case."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h9rxBJOzUL", "forum": "HM7OkY1jFw", "replyto": "HM7OkY1jFw", "signatures": ["ICLR.cc/2026/Conference/Submission12266/Reviewer_QXaD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12266/Reviewer_QXaD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951600991, "cdate": 1761951600991, "tmdate": 1762923203723, "mdate": 1762923203723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the effect of model-generated vs human-provided chains-of-thought when fine-tuning LMs on mathematical reasoning tasks. They find that fine-tuning using synthetic CoTs leads to better model performance as compared to human-labeled CoTs. This still holds even if the synthetic CoTs are mistaken. The authors hypothesize that this is due to the fact that synthetic CoTs are closer in terms of distribution to the model's pretraining, which may make them more amenable for fine-tuning. They further show that using an LM to paraphrase human-provided CoTs can lead to further performance gains when fine-tuning models on the resulting dataset."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors make an interesting finding that synthetic CoTs that lead to incorrect answers are more effective in SFT as compared to human-provided CoTs.\n - Experiments are conducted on a wide range of open language models, from different model families."}, "weaknesses": {"value": "- The scope of the contributions is rather limited.\n - There are some methodological concerns (see below). For example, the statistical significance of the results is not reported.\n - There are some issues with the writing and the structure of the paper (see below). For example, there are numerous claims that are made without supporting references."}, "questions": {"value": "There is a lot of background material that is not necessary to include for an ARR submission. For example, the background on autoregressive language modeling and SFT may be omitted or moved to the appendix. This would free up space for additional more novel contributions (such as the results referenced by Section 5.4).\n\nThe margins between the accuracies are quite small, and it is difficult to interpret the statistical significance of the results. Confidence intervals/bands would help to address this.\n\nWhy is Countdown considered out-of-distribution? Countdown is very similar to an earlier task called \"24Game\" which was mentioned in [https://arxiv.org/abs/2305.10601]. There, they used examples from the website [https://www.4nums.com/game/difficulties/] (which has existed since at least 2014 according to [web.archive.org]). It is very possible that the basic task in the Countdown game has been included in LLM pretraining data for many years.\n\nAlthough the authors report starting training loss as a proxy for the difference in the distribution of the SFT data and the model's pretraining data, I am curious about the loss value *after* fine-tuning.\n\nMore detailed comments follow. While the paper is largely well-written, there are a handful grammatical and stylistic errors. I include some of them from the first three sections in the list below. This list is *not* comprehensive, so I encourage the authors to carefully read through the paper and correct all such errors.\n\nLine 132: (minor) Missing space before citation.\n\nLine 140: \"This has motivated extensive study of how neural networks learn from noisy labels.\" This claim is missing support from references. Which studies?\n\nLine 144: Use inline citations if the citation functions as a grammatical component in the sentence. In this case \"Havrilla & Iyer (2024)\" functions as a noun. In natbib, for example, \\citet can be used for inline citations.\n\nLine 153-155: The claims \"It is observed that children do not merely mimic the linguistic data they are exposed to; instead, they often regularize it, producing a grammatical system that is more consistent and systematic than their input\" and \"Case studies, such as children of non-native speakers acquiring a more regular grammar than their parents\" need support from references. Which studies?\n\nLine 157: \"evidences\" -> \"evidence\"\n\nSection 2.3 reads more like related work than as background.\n\nMinor: \"work\" is traditionally an uncountable noun, and so it has no plural form.\n\nLine 186: \"Cots\" -> \"CoTs\"\n\nIt is difficult to read the highly abbreviated names of the models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oOPkDPOQJI", "forum": "HM7OkY1jFw", "replyto": "HM7OkY1jFw", "signatures": ["ICLR.cc/2026/Conference/Submission12266/Reviewer_aeqo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12266/Reviewer_aeqo"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969029745, "cdate": 1761969029745, "tmdate": 1762923203350, "mdate": 1762923203350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response by Authors"}, "comment": {"value": "We thank the reviewers for their helpful comments. We are happy that the reviewers find our work “interesting”, “valuable”, “surprising”, with our experiments being “rigorous”.\n\nWe would like to clarify some general points and reiterate the contributions of our paper for all the reviewers, ACs, and SACs.\n\nWe believe our work offers a useful insight to the community by demonstrating that distributional alignment is critical in reasoning tasks. Contrary to standard pipelines, which assume correctness is the primary determinant of data quality, we empirically establish that supervised finetuning (SFT)  on synthetic CoT traces, even those leading to incorrect final answers, can improve downstream performance, outperforming human-annotated \"gold\" data. We isolate this effect by showing that paraphrasing human data to become closer to the model’s output distribution induces better gains, confirming that the distributional gap in human text often hinders learning. The other factor is the existence of useful reasoning sub-steps in all the CoTs, even the ones with final incorrect answers, which provide the models enough information for reasoning improvement. Our findings reveal that the widely discarded \"negative\" data in current pipelines actually contains high-value, valid reasoning primitives, and we quantify model robustness to show tolerance for up to 25% completely flawed data before performance degrades. Our work is novel in prioritizing the distribution of the CoTs as measured using the model being finetuned over final-answer correctness, offering a scalable paradigm for improving reasoning that reduces the reliance on expensive human annotation.\n\n### Significance Testing\n\nWe report mean $\\pm$ standard deviation and 95\\% confidence intervals on the mean for robustness metrics over **5 random seeds**. The experiments were run using **Gemma 2B (G2B)** over the datasets: **H**, **G27B-IT-G**, and **G27B-IT-W**.\n\n| Run | **Mean $\\pm$ Std Dev** | **95% CI on Mean** |\n| :--- | :--- | :--- |\n| G2B on H | $0.162 \\pm 0.005$ | $0.162 \\pm 0.006$ |\n| G2B on G27B-IT-W | $0.183 \\pm 0.008$ | $0.183 \\pm 0.009$ |\n| G2B on G27B-IT-G | $0.223 \\pm 0.005$ | $0.223 \\pm 0.006$ |\n\n\n### Ablations Across Different Hyperparameters\n\nWe perform a detailed ablation of the main hyperparameters - batch size (BS) and learning rate (lr) - over the Gemma-2B model on the MATH related datasets to show that the trends of results that synthetic **G and W CoTs outperform H CoTs** remain consistent across all runs. We provide **maximum accuracy on MATH500 test set** for G-2B for all these runs.\n\n\n| Model | **H** | **G-27B-IT-W** | **G-27B-IT-G** |\n| :--- | :--- | :--- | :--- |\n| **BS 256 lr 2e-5** | 0.17 | +0.02 | +0.04 |\n| **BS 256 lr 1e-6** | 0.14 | +0.03 | +0.05 |\n| **BS 64 lr 2e-5** | 0.17 | +0.02 | +0.06 |\n| **BS 64 lr 1e-6** | 0.15 | +0.02 | +0.04 |\n| **BS 16 lr 2e-5** | 0.15 | +0.02 | +0.05 |\n| **BS 16 lr 1e-6** | 0.17 | +0.02 | +0.02 |\n\nWe have added additional paraphrasing experiments (Figure 13) and loss curves (Figure 14) to strengthen our findings."}}, "id": "t8iGAhYd1r", "forum": "HM7OkY1jFw", "replyto": "HM7OkY1jFw", "signatures": ["ICLR.cc/2026/Conference/Submission12266/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12266/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission12266/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763716356381, "cdate": 1763716356381, "tmdate": 1763717002794, "mdate": 1763717002794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}