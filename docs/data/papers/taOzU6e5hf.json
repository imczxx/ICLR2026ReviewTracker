{"id": "taOzU6e5hf", "number": 2901, "cdate": 1757299223110, "mdate": 1763760725586, "content": {"title": "Metric-Normalized Posterior Leakage (mPL): Attacker-Aligned Privacy for Joint Consumption", "abstract": "Metric differential privacy (mDP) strengthens local differential privacy (LDP) by scaling noise to semantic distance, but many ML systems are consumed under joint observation, where model-agnostic, per-record guarantees can miss leakage from evidence aggregation. We introduce metric-normalized posterior leakage (mPL)—an attacker-aligned, distance-calibrated measure of posterior-odds shift induced by releases—and show that for single or independent releases, uniformly bounding mPL is equivalent to mDP. Under joint observation, however, satisfying mDP may still leave mPL high because learned aggregators compound evidence across correlated items. To make control practical, we formalize probabilistically bounded mPL (PBmPL), which limits how often mPL may exceed a target budget, and we operationalize it via Adaptive mPL (AmPL), a trust-and-verify pipeline that perturbs, audits with a learned attacker, and adapts parameters (with optional Bayesian remapping) to balance privacy and utility. In a word-embedding case study, neural adversaries violate mPL under joint consumption despite per-record mDP perturbations, whereas AmPL substantially lowers the frequency of such violations with low utility loss, indicating PBmPL as a practical, certifiable protection for joint-consumption settings.", "tldr": "We introduce mPL, show per-record mDP can fail under joint observation, and use AmPL to curb violations with minimal utility loss.", "keywords": ["Metric differential privacy", "posterior leakage", "joint observation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/db185a6a54a9722eab0f55634f962fd4544003f4.pdf", "supplementary_material": "/attachment/721e51872096cf0953ce583f83174d0a5b13ee87.zip"}, "replies": [{"content": {"summary": {"value": "This paper identifies a significant gap in metric-style privacy for released embeddings: although metric differential privacy (mDP) protects each record in isolation, it can fail when an attacker jointly observes multiple, correlated releases. The authors formalize this joint posterior leakage (mPL) and propose an adaptive pipeline that uses learned adversaries to audit and adjust perturbation mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper demonstrates that per-record mDP can be insufficient under *joint* consumption of multiple perturbed records. This is an important and well-motivated observation: attackers who aggregate correlated releases can substantially increase posterior confidence, undermining traditional mDP guarantees."}, "weaknesses": {"value": "1. **Unspecified attacker capabilities (threat model).**  \n   - The threat model lacks a concrete specification of attacker knowledge and resources (e.g., access to candidate sets, ability to query models, prior distribution knowledge). A clearer, more explicit threat model is needed to interpret the empirical results and to understand when the reported mPL violations are realistic.\n\n2. **Training data requirement for the learned adversary.**  \n   - How much paired data \\((x, M(x))\\) does the adversarial model require to reach the reported attack performance?  \n   - Practically, where would an attacker obtain sufficiently many *original* records and their corresponding perturbed releases to train such a model? If training requires a large amount of supervised pairs, the real-world applicability of the learned-adversary threat is reduced. The authors should report learning curves (attack accuracy / mPL violation vs. number of training pairs) and discuss plausible data-collection scenarios for the attacker.\n\n3. **Assumed knowledge of the embedding method (line 388).**  \n   - The manuscript appears to assume the attacker knows the victim’s word-embedding method (L388). This is a strong assumption that strengthens the attacker considerably. Please clarify and justify this assumption: is it necessary for the attack to succeed, and how sensitive are results if the attacker uses a mismatched embedding model?"}, "questions": {"value": "1. **Generality beyond text.**  \n   - The experiments are limited to text embeddings. Do analogous joint-leakage risks arise for other modalities (images, tabular data, audio)? The paper should discuss whether the mPL phenomenon and the AmPL countermeasure generalize to non-text embeddings, or explicitly limit scope to textual embeddings.\n\n2. **Perturbation model: embedding-level vs. token-level.**  \n   - The defense and attack operate on word embeddings (Exponential Mechanism over candidate embeddings). Would the learned-adversary attack still be effective if perturbations are applied directly on text (e.g., token deletion, insertion of noise characters, synonym substitution) rather than on embeddings? The paper should evaluate or at least discuss this alternative threat axis and its implications for attack success and practical defenses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lZicEUrWDS", "forum": "taOzU6e5hf", "replyto": "taOzU6e5hf", "signatures": ["ICLR.cc/2026/Conference/Submission2901/Reviewer_ynkJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2901/Reviewer_ynkJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720453146, "cdate": 1761720453146, "tmdate": 1762916435587, "mdate": 1762916435587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the problem of metric differential privacy (mDP). Since mDP mechanisms do not account for joint observation, privacy leakage may occur when only individual records are considered. To address this, the authors formalize metric-normalized posterior leakage (mPL) and propose PBmPL as a framework to control it. However, since mPL cannot be computed directly, they introduce an attacker model to approximate it and adapt the noise level accordingly to prevent violations, thereby balancing privacy and utility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* They formalized the concept of metric-normalized posterior leakage (mPL) and investigated its properties.\n* They observe that existing metric differential privacy (mDP) mechanisms fail to adequately protect privacy under joint observation.\n* They propose a method to control and reduce metric-normalized posterior leakage (mPL) under joint observation."}, "weaknesses": {"value": "* The violation rate does not decrease significantly compared to the baseline when using the proposed method.\n* The paper estimates mPL by training an adversarial model and averaging the results over multiple sampled instances. However, this approach may introduce errors both from the adversarial model itself and from sampling variance, especially when the sample set is large. There is no analysis provided to quantify or bound these potential sources of error.\n* The paper lacks a clear analysis or visualization of the trade-off between utility and violation rate. Presenting this relationship with a graph would make the results more intuitive and convincing.\n* The proposed method appears to require more time for sampling and training compared to the baseline. A comparison of computational cost or runtime would therefore be necessary."}, "questions": {"value": "* I am not very familiar with this area, but I wonder whether there are other approaches that consider individuals separately. If such methods exist, a comparison with them would be necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "b7WfT7WJxp", "forum": "taOzU6e5hf", "replyto": "taOzU6e5hf", "signatures": ["ICLR.cc/2026/Conference/Submission2901/Reviewer_bgUX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2901/Reviewer_bgUX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732467164, "cdate": 1761732467164, "tmdate": 1762916435388, "mdate": 1762916435388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces metric-normalized posterior leakage, mPL, which is a distance-calibrated measure of how much an output shifts posterior odds between candidate secrets. mPL addreses leakage that arises when perturbed outputs are jointly consumed by the adversary, which is a setting where per-reccord metric mDP can be misleading. \n\n\nIn addition, this paper propose PBmPL, which bounds the frequency with which mPL may exceed a budget and supports estimation through sampling with a concentration guarantee. Moreover, they operationalize a trust-and-verify pipeline (AmPL) that (i) applies level-wise perturbations, (ii) trains a learned attacker to approximate posteriors and audit mPL, (iii) adapts mechanism strength from audit feedback, and (iv) optionally performs Bayesian remapping as pure post-processing. \n\nIn a word-embedding case study, they show that standard mDP mechanisms can still exhibit notable mPL violations under neural attackers, while AmPL substantially lowers the violation rate with comparable utility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed metric-normalized posterior leakage (mPL) is a novel privacy notion. mPL establishes basic properties, such as post-processing invariance, and proves that for single or independent releases, a uniform mPL bound is equivalent to mDP.\n\n\nThe adaptive mPL (AmPL) provides a concrete recipe, which is a creative combination of known pieces that makes an otherwise intractable problem operational for practitioners. \n\nBy centering joint observation of correlated items, the work hits a practically important gap that affects text embedding pipelines and other modern ML settings.\n\nThe case study with word embeddings demonstrates that mechanisms tuned for per-record mDP can still suffer non-trivial mPL violations under learned joint attackers, while AmPL materially reduces the violation frequency at comparable utility."}, "weaknesses": {"value": "My main concern is as follows.\n\nThe paper's \"certificate\" relies on a **learned posterior surrogate** and a **sampling-based audit** (their PBmPL). It lacks (i) a **surrogate-to-truth transfer bound** (uniform over outputs) on likelihood ratios/privacy loss, (ii) **attacker model generalization control** (i.e., validity under worst-of-many attackers with proper multiple-comparison corection and hold-out evaluation), and (iii) any **composition** accounting across multiple tokens in a sequence of multiple releases. As a result, it seems that the claimed \"certifiable protection under joint consumption\" does not extend beyond the audited samples and the specific attacker or beyond a single run, even with infinite data/samples. \n\n\nThe post-processing invariance does not rescue the missing bounds. Remapping preserves whatever bound you already have. It does not creat a sequence-level or worst-case gurantee on its own.\n\n\n\n# W-1: (i) \n\nIn this paper, they train a model to approximate posteriors (or distances mapped through a temperature-scaled softmax) and compute mPL from that surrogate.\n\nWithout a **uniform approximation bound** between the **true likelihodd ratio** (or the privacy-loss random variable) and its **surrogate**, the audit can under-estimate leakage even with infinite samples. Bayesian posterior-odds guarantees follow directly from likehood-ratio bounds. If those are only approximated, we must quantify the approximation error.\n\n\n# W-2: (ii) \n\nIn this paper, they audit privacy leakage against one or a few learned attackers, with hyperparameter tuning, then report low violation rates. This might be a problem: Security claims should hold against a **class** of attack models, not just the one trained. Hyper/architecture search introduces multiple comparions and adaptivity to the audit set, which can hide violations (overfitting to the evaluation).\n\n\n\n# W-3: (iii)\n\nIn this paper, they report per-mechanism or per token violation rates and improvements after the adaptive audit loop. Then discuss joint consumption. \n\nThe theoretical results are for single release (and independent releases), which are mathematically correct. The composition is not needed to validate those theoretical results.\n\nHowever, the paper's central promise is privacy under joint consumption/repeated use. To claim that, we n**eed a composition argument** (or an explicit reduction to a composed privacy-loss bound). Without it, the paper's main claim is **under-justifed**. So, composition is not some add-on to paper's contribution; instead, it is required to elevate the scopr from single/independent to the realistic seting the paper cares about, especially under dependence."}, "questions": {"value": "Please refer to the comments under Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QjQfKqinrd", "forum": "taOzU6e5hf", "replyto": "taOzU6e5hf", "signatures": ["ICLR.cc/2026/Conference/Submission2901/Reviewer_UW4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2901/Reviewer_UW4E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789542240, "cdate": 1761789542240, "tmdate": 1762916435221, "mdate": 1762916435221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the assumption in metric differential privacy (mDP) that secrets are independent across records. The authors introduce metric-normalized posterior leakage (mPL), which quantifies how an adversary’s posterior belief shifts when observing multiple correlated releases. They prove that mPL is equivalent to mDP when secrets are released individually or independently distributed, but it can expose violations under joint observation.\n\nTo enforce mPL in practice, the authors propose Adaptive mPL: a trust-and-verify pipeline where a learned neural adversary estimates leakage and informs parameter adaptation to achieve probabilistically bounded mPL. Experiments on text embedding protection show that per-record mDP can fail under joint observation, while AmPL reduces such leakage with minimal utility loss."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The work identifies a real gap in mDP deployment assumptions: joint inference over correlated records is typical in modern systems.\n* mPL is clearly defined and theoretically grounded, recovering mDP under independent settings.\n* The experiments provide useful evidence that per-record privacy guarantees do not prevent aggregate leakage when records are correlated."}, "weaknesses": {"value": "* mPL application depends entirely on a learned adversary. While a strong adversary is assumed (with access to the noise mechanism and auxiliary data distribution), it does not necessarily represent an upper bound - a more efficient or capable adversary could still be possible. The claimed guarantees are therefore empirical rather than theoretical.\n* Experiments focus solely on correlated records belonging to a single user. While I understand the general threat from correlated data, for the scenarios presented in the experimental section a per-user privacy budget could be sufficient to eliminate violations. I believe it should be at least incorporated as a baseline.\n* The privacy budget range (0.3-0.5) is narrow, and results appear qualitatively similar across values. More extreme privacy regions would make trends clearer.\n* Paper presenetation:\n\t* Initial empirical results (line 258) appear too early and are hard to follow at that stage of the paper.\n\t* Some figure labels are too small to read comfortably (e.g. Figure 2 and Figure 4).\n\t* Units for epsilon (km^-1) should be introduced earlier and explained clearly.\n\t* Sensitivity tiers are introduced without explanation.\n\t* The paper would benefit from a related works section"}, "questions": {"value": "What is the intuition for higher epsilon corresponding to lower violation rates? (Table 1)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fsl54Iu4AY", "forum": "taOzU6e5hf", "replyto": "taOzU6e5hf", "signatures": ["ICLR.cc/2026/Conference/Submission2901/Reviewer_xef9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2901/Reviewer_xef9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937404653, "cdate": 1761937404653, "tmdate": 1762916434944, "mdate": 1762916434944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}