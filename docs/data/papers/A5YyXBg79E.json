{"id": "A5YyXBg79E", "number": 17455, "cdate": 1758276256045, "mdate": 1759897174403, "content": {"title": "Language-Specific Latent Process Hinders Cross-Lingual Performance", "abstract": "Large language models (LLMs) are demonstrably capable of cross-lingual transfer, but can produce inconsistent output when prompted with the same queries written in different languages. To understand how language models are able to generalize knowledge from one language to the others, we measure representation similarity across languages by centered kernel alignment (CKA) and cosine similarity. We also apply the logit lens to interpret the implicit steps taken by LLMs to solve multilingual multi-choice reasoning questions. We find LLMs predict inconsistently and are less accurate because they rely on representations of individual languages, rather than working in a shared semantic space. While larger models are more multilingual, we show their hidden states are more likely to dissociate from the shared representation compared to smaller models, but are nevertheless more capable of retrieving knowledge embedded across different languages. Finally, we demonstrate that knowledge sharing in small models can be facilitated by steering their latent processing towards the shared semantic space. This improves the models’ multilingual reasoning performance, as a result of more knowledge transfer from, and better output consistency with English.", "tldr": "We show that larger LLMs process multilingual queries differently than smaller LLMs, and that shared latent semantic space facilitates cross-lingual transfer and consistency.", "keywords": ["multilingual", "consistency", "cross-lingual transfer", "knowledge sharing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0577db5b5db7d1358e9484089b93ef1b3c3483d8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates why LLMs show inconsistent behavior when handling the same tasks across different languages. The authors analyze the internal representations of models such as Gemma 2 and Qwen 2.5 using techniques like centered kernel alignment (CKA), cosine similarity, and logit lens. They find that although larger models achieve higher multilingual accuracy, their internal representations become increasingly language-specific rather than shared, causing inconsistent reasoning across languages. Smaller models rely more on a shared, English-centric semantic space that supports more stable cross-lingual knowledge transfer.\n\nTo address this, the authors propose cross-lingual activation steering, a technique that nudges model activations toward English-aligned latent representations. This intervention improves multilingual reasoning and consistency in smaller models by enhancing their use of shared semantic structures. The work introduces a framework for quantifying cross-lingual alignment and offers an explanation for how model scale affects multilingual reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Introduced cross-lingual activation steering to enhance reasoning performance.\n- Combined CKA, cosine similarity, and the logit lens for multilingual analysis.\n- Demonstrated a correlation between representation similarity and multilingual accuracy."}, "weaknesses": {"value": "- The study relied primarily on multiple-choice tasks, which limited generalization to open-ended reasoning.\n- Benefits are skewed toward languages written in Latin script.\n- The validity of the findings is limited to two model families.\n- There is no runtime or latency analysis conducted for steering."}, "questions": {"value": "- Did you test how the steering method scales for larger or more complex models?\n- How sensitive are results to the selection of layers?\n- Could multilingual prompting (e.g., CoT in the target language) close the gap similarly?\n- Did you try to construct shared-space vectors for non-English languages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NiutqgboX6", "forum": "A5YyXBg79E", "replyto": "A5YyXBg79E", "signatures": ["ICLR.cc/2026/Conference/Submission17455/Reviewer_v76E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17455/Reviewer_v76E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761599284237, "cdate": 1761599284237, "tmdate": 1762927340923, "mdate": 1762927340923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how LLMs process multilingual inputs and they often fail to maintain consistent reasoning across languages with a novel framework of repurposing CKA as a metric to study structure similarity between language representations. The paper studies the latent representations of models such as Gemma2, and Qwen2.5 across scales using multiple benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Build a framework for analysing cross-lingual transfer with well-selected metrics such as CKA, cosine-similarity and logit-lens to quantify language representation overlap, which provides a clear and interpretable way to study cross lingual transfer.\n- The analysis across models and layers are comprehensive.\n- The paper reframes multilingual reasoning as a latent-space alignment problem, providing a clear direction for multilingual output consistency."}, "weaknesses": {"value": "- The task is completely limited to multi-choice reasoning questions. This is a clever choice that is easy to measure the cross-lingual transfer with unambiguous labels. However, it also limits the generalisation to open-ended and generative multilingual reasoning.\n- “Humans have an innate ability to apply common knowledge and perform reasoning skills consistently across different languages” itself is a very contentious claim. This paper also doesn’t need Jerry Fodor’s nativism as motivation.\n- As shown in Figure4, there is a huge disparity between transfer effects among languages that are not very related, such as from Arabic to Swahili. The averaged performances illustrated in the main body of the paper is obviously inflated. Without modelling the language relatedness in the analysis, it is difficult to assess the generalisability of the findings in the paper. At least the specific language’s relatedness with English.\n- Also, in terms of language relatedness, the paper investigates steering vectors only towards English. The motivation seems to be that the training data is primarily English. On that note, the training data volume per language is also not taken into consideration at all. Various factors that can be impactful to the results are not considered.\n- Overall, the paper is novel in repurposing a useful metric such as CKA for cross-lingual analysis, however, the study remains Anglocentric and linguistically shallow."}, "questions": {"value": "- Consider language relatedness, and training data volume per language, instead of simply taking average results across languages, which can inflate the results because of higher resource latin script languages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wl6DiUNt83", "forum": "A5YyXBg79E", "replyto": "A5YyXBg79E", "signatures": ["ICLR.cc/2026/Conference/Submission17455/Reviewer_oQ2d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17455/Reviewer_oQ2d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991704272, "cdate": 1761991704272, "tmdate": 1762927340516, "mdate": 1762927340516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper highlights the lack of consistency of outputs across languages, despite LLMs being capable of cross-lingual transfer. To measure generalization across languages, the paper measures representation similarity between languages by centered kernel alignment (CKA) and cosine similarity. They introduce three evaluation metrics to measure performance across languages: consistency, positive transfer and negative transfer.Their analysis show that LLMs don't use a shared semantic space which leads to low performance.They finally use steering vector to do steering towards English and find that it’s effective for smaller models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides useful insights on how knowledge is represented and shared internally across languages in LLMs.  The authors investigate how LLMs transfer knowledge across languages. Through their experiments, they establish the usefulness of a shared semantic space for cross-lingual transfer. \n2. The paper proposes a cross lingual steering approach to improve cross lingual transfer for smaller models. \n3. Evaluation method is robust: The authors use ranking order for MCQ-styled questions across languages to measure consistency. They also measure positive and negative transfer."}, "weaknesses": {"value": "1. Steering evaluation can include cross-dataset generalization to strengthen the claims. The current results only include effects on the same dataset."}, "questions": {"value": "1. Do you notice any trends in consistency scores based on the linguistic distance of the language from English?\n2. Does adding a steering vector towards English deteriorate performance on cultural/region-specific questions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xVzNBCz3ar", "forum": "A5YyXBg79E", "replyto": "A5YyXBg79E", "signatures": ["ICLR.cc/2026/Conference/Submission17455/Reviewer_LmGN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17455/Reviewer_LmGN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762022777821, "cdate": 1762022777821, "tmdate": 1762927340054, "mdate": 1762927340054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies cross-lingual consistency for the output when inputting the same query in different languages. The authors consider three factors: knowledge transfer (positive or negative), representation similarity, and activation steering. Then, the authors examine both small and large models, offering the key finding that a large model tends to handle each language independently, and a small model pushes all languages to a shared space."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Cross-lingual consistency is a recent topic. It evaluates the fairness in LLMs, which is very important in reall applications."}, "weaknesses": {"value": "There are some concerns.\n\n1.\tExisting work [1] overshadows the novelty and contribution of this paper. For example, this paper follows a similar experimental design to [1], including CKA and Logits Lens examinations, layer-wise analysis, and activation steering. \n\n2.\tExperiments are limited, which makes the paper not conclusive. The authors only conducted experiments on multiple-choice datasets. How about generation tasks?\n\n3.\tWhile the paper is clear, the authors spend too many spaces on introducing existing works, e.g., CKA, Logits Lens, and methods for activation steering. I would like to see more in-depth analyses, which make the paper more fruitful. \n\n[1] Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?   https://arxiv.org/abs/2507.12838"}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "auwa5lidXB", "forum": "A5YyXBg79E", "replyto": "A5YyXBg79E", "signatures": ["ICLR.cc/2026/Conference/Submission17455/Reviewer_y8RG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17455/Reviewer_y8RG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762398002164, "cdate": 1762398002164, "tmdate": 1762927339056, "mdate": 1762927339056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}