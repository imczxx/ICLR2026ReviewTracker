{"id": "qRbCkTk9ZR", "number": 6896, "cdate": 1758000747452, "mdate": 1759897885362, "content": {"title": "Learning What Matters Now: Dynamic Preference Inference under Contextual Shifts", "abstract": "Human decision-making is guided by dynamically shifting value priorities rather than fixed objective functions. Existing computational models often assume static optimization targets, overlooking phenomena such as preference drift, context-sensitive trade-offs, and goal re-evaluation. In this work, we introduce a cognitive-inspired framework that treats preferences as latent states inferred under uncertainty. The agent maintains a Bayesian belief over preferences, continuously refines this belief from past experience, and acts according to the most probable preference configuration. Crucially, inference and control are trained jointly, ensuring that preference adaptation remains aligned with long-term utility while grounded in interpretable value dimensions. This formulation enables agents to autonomously re-balance goals as circumstances change, without externally specified weight schedules. Experiments demonstrate superior adaptability to preference shifts and changing constraints, highlighting both the practical effectiveness and cognitive plausibility of our approach.", "tldr": "", "keywords": ["Dynamic Value Preference Inference", "Cognitive Modeling", "Adaptive Decision-Making"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f02cda35fae245fa6a34f326021dbac414634a53.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "A cognitively inspired RL framework is proposed for dynamically inferring agent preferences in non-stationary, multi-objective environments. The method jointly trains preference inference and control via variational Bayesian optimization, achieving interpretable and adaptive behavior. Experiments in two environments demonstrate the effectivenss of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The central idea of treating value preferences as latent, context-sensitive cognitive states inferred through Bayesian variational methods is novel and interesting.\n\n2. The manuscript is clearly organized and flows logically from motivation to methodology to experiments.\n\n3. The experimental design is good, featuring comprehensive analyses and ablation studies that offer valuable insights into the model’s behavior."}, "weaknesses": {"value": "1. The experiments are confined to relatively small, synthetic domains (toy symbolic and 2D Maze). Although cognitively illustrative, these environments may not fully demonstrate the scalability or generalizability of the proposed DPI framework. More complicated continous environments or real-world cases that better reflect the changing preferences are needed.\n\n2. Although the paper introduces a principled ELBO-based training objective with regularization terms, it lacks sufficient analysis of training stability, including convergence behavior and sensitivity to hyperparameters."}, "questions": {"value": "1. In Lines 93–94, the definition of $\\omega_t$ is unclear. Please clarify its meaning and justify why it is denoted in bold.\n\n2. The choice of a unit Gaussian prior lacks sufficient motivation. It would be helpful to provide a rationale or empirical justification for this assumption.\n\n3. In Line 203, the derivation of the presented formula is not fully explained. Additional details or intermediate steps would improve clarity.\n\n4. In the Maze environment, the authors claim that “without dynamic preference adjustment, the agent cannot succeed.” Please elaborate on how this statement was verified or implemented in practice.\n\n5. In Table 1, the success rate (SR) should also be reported with the mean and 95% confidence interval (CI) across $N$ episodes, consistent with the other metrics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I3oARGquYx", "forum": "qRbCkTk9ZR", "replyto": "qRbCkTk9ZR", "signatures": ["ICLR.cc/2026/Conference/Submission6896/Reviewer_9DQE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6896/Reviewer_9DQE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638160488, "cdate": 1761638160488, "tmdate": 1762919139420, "mdate": 1762919139420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of Multi-Objective Reinforcement Learning in dynamic environments where preference weights are not fixed or explicitly given, but are instead latent and subject to change based on contextual shifts. The authors propose a cognitive-inspired framework called Dynamic Preference Inference (DPI). DPI consists of two main components: a Value Appraisal Module that uses variational inference to maintain a posterior distribution over latent preference weights based on recent history, and an Action Selection Module that employs a preference-conditioned actor-critic to execute policies based on sampled preferences. The framework is validated on two synthetic environments (Queue and Maze), showing that it can adapt to sudden changes in task dynamics (e.g., \"deadline shocks\" or \"hazard surges\") better than static or heuristic baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel Problem Formulation: The paper identifies and formalizes a significant gap in current MORL research: treating preference weights as latent and dynamic states that must be inferred online, rather than static inputs. This is well-motivated by cognitive theories of human decision-making.\n\n- Principled Framework: The DPI method offers a principled probabilistic approach, synthesizing variational inference for preference estimation (using ELBO) with established preference-conditioned RL techniques (like the envelope operator).\n\n- Demonstrated Adaptivity: Empirical results on the test environments clearly demonstrate the method's ability to recover performance (measured by Post-Shift Performance, PS@K) rapidly after unobserved environmental shifts, significantly outperforming fixed-preference baselines.\n\n- Interpretability: The inclusion of alignment analysis (cosine similarity between inferred preferences and instantaneous reward vectors) provides evidence that the learned latent preferences meaningfully track shifting task semantics."}, "weaknesses": {"value": "- Limited Experimental Scope: The evaluation is restricted to two relatively simple synthetic environments: a symbolic queue task and a 2D grid-world. While illustrative, these do not fully demonstrate the method's scalability to complex, high-dimensional, or continuous control problems often seen in practical MORL settings.\n\n- Potentially Weak Oracle Baseline: The ENVELOPE baseline, described as having \"oracle access to event-dependent preference weights\" , performs surprisingly poorly (e.g., only 0.01% success rate in Maze ). This baseline appears to use sparse, static weights triggered by events. A true \"oracle\" in such highly dynamic environments should likely use dense preference updates at every timestep (e.g., exponentially increasing urgency as a deadline approaches, not just a flat jump in weight when a \"shock\" occurs). The large performance gap might simply reflect poorly tuned static weights for the baseline rather than the absolute necessity of inference.\n\n- Complexity of Objective: The total training objective is a composition of many terms: PPO loss, dual critic loss, ELBO, directional alignment ($\\mathcal{L}\\_{dir}$), and self-consistency ($\\mathcal{L}\\_{stab}$). The ablation study confirms standard components alone (e.g., w/o $\\mathcal{L}\\_{dir}$ or $\\mathcal{L}\\_{stab}$) perform significantly worse, suggesting the method might be highly sensitive to the hyperparameters balancing these diverse loss terms ($\\lambda, \\gamma, \\xi, \\beta$).\n\n- Dependence on Return Vectors for Alignment: The directional alignment loss relies on vector returns $\\vec{G}\\_t$ estimated by the critic. In environments with very sparse rewards or difficult exploration challenges, these return estimates might be highly inaccurate early in training, potentially leading to poor preference inference that is hard to recover from."}, "questions": {"value": "1. Dense Oracle Baseline: Could you compare DPI against a \"Dense Oracle\" baseline that has access to analytically derived optimal preferences at every timestep (e.g., analytically derived from exact remaining time and energy)? This would provide a stronger upper bound to gauge the true effectiveness of DPI's inference module compared to standard envelope methods.\n\n2. Hyperparameter Sensitivity: Given the composite nature of the loss function, how sensitive is the model's performance to variations in the auxiliary loss coefficients $\\lambda$ (alignment) and $\\gamma$ (stability)?\n\n3. Cold-Start Dynamics: How does DPI behave early in training when the critic's vector return estimates $\\vec{G}_t$ are likely noisy or uninformative? Does this lead to early collapse into sub-optimal preference modes?\n\n4. Theoretical vs. Practical Gap: The generative assumption is $\\omega\\_{t}^{*}\\sim p(\\omega\\_{t} | \\omega\\_{t-1},\\xi\\_{t})$, but the implementation uses a static prior $p\\_{0}(z)=\\mathcal{N}(0,I)$. While the recurrent encoder handles history, did you experiment with strictly enforcing the temporal dependency in the ELBO (e.g., a learned transition prior)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wRXT6aRfZh", "forum": "qRbCkTk9ZR", "replyto": "qRbCkTk9ZR", "signatures": ["ICLR.cc/2026/Conference/Submission6896/Reviewer_6Kvv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6896/Reviewer_6Kvv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958461307, "cdate": 1761958461307, "tmdate": 1762919139021, "mdate": 1762919139021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of the shifted preference as time and condition/state change. This problem is plural and complex, and the authors proposed a computational agent that comprises the Value Appraisal and Action Selection to model the dynamic decision-making systems."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The paper identifies an interesting problem that may not be fully aware of in the machine learning literature. This problem itself has connections with reinforcement learning and MDPs.\n- The proposed framework is quite interesting, and the Bayesian view of this problem is natural to follow. I like how the value shift is integrated in the framework, and I also feel that using the sample example for explaining the framework helps a lot."}, "weaknesses": {"value": "- The paper starts with a lot of terminology but lacks an explanation of these terms. Although I'm quite familiar with the choice model/psychological literature, it is hard to tell if these terms are made up/invented by the authors. I strongly suggest avoiding overuse or misuse of terminology and approaching from an easy-to-understand tone. Besides, proper references for the well-defined terms are needed.\n-- I pointed out some points in the Questions section, but there are many more confusing points.\n\n- The paper does not highlight the computational contribution and what makes any other approach hard to address the research question.\n\nI find it quite hard to raise the score to a positive one because of the whole writing and idea delivery, but I would not mind giving a borderline reject if the author could clarify and improve the flow."}, "questions": {"value": "Introduction:\n- The statement of Line 28 needs some (psychological) literature to support. The instance given afterward does not fully support the claim \"rarely pursuing goals with fixed and immutable priorities\".\n- In line 36-38, I don't see a clear necessity between the modeling of such dynamic value adaptation and artificial intelligence. Why use AI in particular; why not some other type of modeling? The motivation for using AI is not clear in the writing. You may want to expand a bit on why current literature fails in the computational aspect and why AI can be helpful.\n- In line 32-34, I don't think \"psychology\" and \"cognitive science\" are two distinct/mutually exclusive terms. They might refer to similar stuff. \n- In line 50/51, what does \"decision-making models\" exactly mean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N6AEWol2bH", "forum": "qRbCkTk9ZR", "replyto": "qRbCkTk9ZR", "signatures": ["ICLR.cc/2026/Conference/Submission6896/Reviewer_b9am"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6896/Reviewer_b9am"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976612993, "cdate": 1761976612993, "tmdate": 1762919138703, "mdate": 1762919138703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers dynamically changing preferences. It proposes an algorithm that leverages a signal providing information about these signals to modify its actions accordingly. Empirical results are presented on two settings (waiting in a queue, a maze problem) comparing the method to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper considers an interesting under-explored area. There is strong empirical evidence that humans switch their stated goals over time. There is value in understanding this phenomenon and in designing algorithms that support humans despite these changes."}, "weaknesses": {"value": "The motivation for the paper is unclear (see questions #1, #2, #3 below).  It is also unclear how to apply it to a practical problem (questions #1, #2, #4).  The experiments should have included stronger RL-based baselines that maximize the performance metrics reported (question #5)."}, "questions": {"value": "#1. In the \"waiting in line\" example used to motivate the paper and used in the numerical experiments, the phenomenon described can be modeled without the need to change goals.\n  The goal would be to maximize the expected value of the utility function 1{waiting_time < T} + c*cut_in_line.\n  The optimal strategy is to wait until T and, if we haven't gotten to the front of the line, cut.\n  The thing that is changing is not the goal but the optimal action (cut or not cut) given the current state (how long we've been waiting in line).\n \nLet me explain my example in more detail so that the authors can respond.  My example is a discrete time problem where there is a random variable W that is the time we would wait if we chose not to cut in line. This is drawn from a known probability distribution.  In each time period t, if we haven't gotten to the front of the line yet (i.e., t < W), then we would decide whether to cut in line or not based on t.  Any decision rule (a way to decide the action, i.e., whether to cut, based on the current state, i.e., t) results in a binary random variable cut_in_line, which is 1 if we eventually choose to cut in line and 0 if not. It is a function of W.  A decision rule also results in a waiting time, which is the minimum of W and when we choose to cut in line, if ever.  Using a dynamic programming argument where the state is the current time t, you can show that if c is large enough, an optimal strategy will be to wait until time T-1 and, if we haven't gotten to the front of the line, cut.\n\nThese same arguments extend to utility functions of the form f{waiting_time} + c*cut_in_line for a general function f. For most functions f and distributions over W, the optimal strategy will be to wait until some finite time that is strictly larger than 0 and then cut.\n\nThis is an optimal stopping problem.\n\n#2. The maze problem (the second numerical example) also seems to be better modeled by a fixed goal. What the paper positions as changes in goal really just seem like changes in the state in a Markov decision process (MDP) or partially observable MDP. For example, when trying to get out of a maze, a reasonable fixed goal is to minimize the expected time to get out of the maze. Over time, our knowledge about the maze changes. The maze itself may also change. As a result, the optimal action may change.\n\n#3. I am having trouble understanding the motivation for the paper.  I agree that there is a large body of evidence showing that humans \"switch goals\" over time. But this doesn't imply that algorithms should do this.\n\nIs the goal to develop a decision-making algorithm that is somehow better able to provide value to humans, in light of this empirically-observed goal switching?  Or is it to improve our understanding of human decision-making?\n\nMy sense is that the paper's goal is the not the second --- if it were, then we would presumably want to include human data in the experiments and make comparisons to other hypotheses.\nPresumably then it is the first.  But if this is the case, then it isn't clear that we want our algorithms to \"swich goals\".\n\nFirst, for settings like my comments #1 and #2 where human \"goal switching\" can be explained as the use of decision-making shortcuts to perform well against a larger goal that is static, wouldn't human utility be better maximiized by doing a good job of optimizing against the larger goal with a gool RL strategy?  More broadly, if \"goal switching\" can be explained by humans' bounded rationality because of biological limitations, then wouldn't a human prefer that an algorithm making decisions on their behalf be more rational than they are?\n\nI am not philosophically opposed to developing an algorithm for settings where humans switch goals but to be publishable at ICLR there would need to be a stronger set of examples where it is demonstrated that the new algorithm provides value to a human. To demanstrate that it provides more value to a human, it is important to somehow bring human data in to the paper, e.g., by having a human assess the quality of the outcomes provided by the proposed algorithm and baselines in a real-world setting.\n\n#4. Practically, how should a practitioner interested in using this method estimate p(s_t | omega_t) and p(omega_t | omega_{t-1}, xi_t)?\n\n#5. In the experiments, methods are evaluated relative to static success measures --- mean episodic return, success rate, and post-shift performance at K. If I want to maximize some combination of these (a weighted combination, or maximizing one subject to constraints on the others), then I should use a reinforcement learning algorithm designed for this task. The Queue environment seems particularly simple and I would expect that an RL algorithm could accomplish this. At a minimum, the paper should include as a baseline an RL algorithm that optimizes each of the single metrics reported.\n\n#6. Please help me understand some of the notation in section 3.1.\n- In equation 1, which is s_t not on the right-hand side of the equation?\n- In equation 1, presumably omega is a function of z_t even though the notation from equation 2 indicating this is not used here.  This notation from equation 2 is useful --- it should be introduced and defined in the text.\n- In equation 2, why does s_{t-H+1:t} not appear on the right-hand side?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eV2ryos14R", "forum": "qRbCkTk9ZR", "replyto": "qRbCkTk9ZR", "signatures": ["ICLR.cc/2026/Conference/Submission6896/Reviewer_tNvs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6896/Reviewer_tNvs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762089271642, "cdate": 1762089271642, "tmdate": 1762919138229, "mdate": 1762919138229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}