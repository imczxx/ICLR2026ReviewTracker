{"id": "PWChrnrw7Z", "number": 1959, "cdate": 1756971494043, "mdate": 1759898176633, "content": {"title": "SHE-LoRA: Selective Homomorphic Encryption for Federated Tuning with Heterogeneous LoRA", "abstract": "Federated fine-tuning is critical for improving the performance of large language models (LLMs) in handling domain-specific tasks.\nHowever, prior work has shown that clients' private data can actually be recovered via gradient inversion attacks.\nExisting privacy preservation techniques against such attacks typically entail performance degradation and high costs, making them ill-suited for clients with heterogeneous data distributions and device capabilities.\nIn this paper, we propose SHE-LoRA, which integrates selective homomorphic encryption (SHE) and low-rank adaptation (LoRA) to enable efficient and privacy-preserving federated tuning of LLMs in cross-device environments.\nBased on model parameter sensitivity assessment, heterogeneous clients adaptively negotiate and select a subset of model parameters for homomorphic encryption. \nTo ensure accurate model aggregation, we design a column-aware secure aggregation method and customized reparameterization techniques to align the aggregation results with the heterogeneous device capabilities of clients.\nExtensive experiments demonstrate that SHE-LoRA maintains performance comparable to non-private baselines, achieves strong resistance to the state-of-the-art attack, and significantly reduces communication overhead by 99.71\\% and encryption time by 99.87\\%, compared to HE baselines.\nOur code is accessible at https://anonymous.4open.science/r/SHE-LoRA.", "tldr": "SHE-LoRA: Privacy FL for edge devices via selective homomorphic encryption & adaptive LoRA. Matches non-private performance while cutting comms 93.7% and encryption compute 99.8%, comparing to full encryption.", "keywords": ["Foundation models", "LoRA", "Homomorphic Encryption"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9e14a9c6e8fdb23c7dc59462598da710adb53dec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new framework for privacy-preserving federated fine-tuning of large language models (LLMs) that balances efficiency, privacy, and heterogeneity among client devices. The proposed SHE-LoRA integrates SHE with LoRA to allow clients with diverse computational capacities and data distributions to securely participate in federated fine-tuning. The method adaptively encrypts only the \"most sensitive\" parameters based on a parameter sensitivity analysis, using a global negotiation mechanism to determine a subset of the parameters to be encrypted. Besides, the adaptive aggregation (on server side) and reparametrization (on client side) techniques are proposed to enable the FL cycle."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper proposes a framework that provides a strong privacy protection guarantee for the models.\n2) The proposed algorithm can save computation cost significantly compared to the existing methods.\n3) The proposed algorithm provides some insight about how to define and select important parameters."}, "weaknesses": {"value": "1) The parameter subset selection and negotiation may remain unclear for readers. For example,\n* Why is $S_j$  defined in the paper a reasonable metric to evaluate the importance? \n* What does \"features aggregated across L tokens\" mean in the context of the paper? \n* If the importance of the parameters changes in the training process, does the selection dynamically adapt to such changes? \n2) It is unclear what privacy protection strength this algorithm can provide in the worst case.\n* Do the most sensitive parameters equivalent to those that should be protected for privacy protection?\n* It seems there is no guarantee that the most sensitive parameters of a client will remain after negotiation. Does it mean the privacy of such a client, if it exists, is more vulnerable than that of others?\n* The attack experiments only consider reconstruction attack, but no membership inference attack (which is more closely related to the privacy definition people commonly believe).\n3) Given that the encrypted parameters are only a very small fraction, it is unclear whether directly removing those parameters from training can also produce similar model performance. If so, it may become arguable that the \n* Some ablation studies about the parameter may help.\n4) The writing of the paper can be further improved.\n* Some notations and definitions are missing in the paper. For example, the notation of \"batch\" uses the same notation as the matrix $B$ in LoRA.\n* Too many details are omitted in the main text, which makes readers hard to follow."}, "questions": {"value": "Please refer to the question in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bo6cmcLiBY", "forum": "PWChrnrw7Z", "replyto": "PWChrnrw7Z", "signatures": ["ICLR.cc/2026/Conference/Submission1959/Reviewer_sB8q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1959/Reviewer_sB8q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761515987948, "cdate": 1761515987948, "tmdate": 1762915974300, "mdate": 1762915974300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SHE-LoRA, a framework that combines selective homomorphic encryption (SHE) with low-rank adaptation (LoRA) for cross-device federated fine-tuning of LLMs. The core ideas are: (1) negotiation across heterogeneous clients using order-preserving encryption to choose columns to encrypt based on channel-wise sensitivity; (2) column swapping to cluster encrypted vs. plaintext columns for efficient batching and to obfuscate positions; (3) column-aware adaptive aggregation that aggregates plaintext and ciphertext parts separately; and (4) reparameterization via SVD so each client recovers LoRA factors at its local rank. Experiments show accuracy comparable to non-private heterogeneous LoRA while reducing communication by up to 99.71% and encryption time by up to 99.87% vs. full-HE baselines, and strong robustness to DAGER gradient inversion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Massive reductions in HE time and bandwidth vs. full HE and MaskCrypt, with stable per-client times due to column clustering and budget control.\n2. Comparable to non-private Flex-LoRA across GLUE/MMLU and vision tasks; sometimes better on subsets."}, "weaknesses": {"value": "1. OPE preserves order information; while only rankings are revealed, the paper does not quantify leakage from order disclosure or compare with order-revealing encryption alternatives.\n2. For 30B/70B models, ciphertext size/time per parameter increases notably and requires larger key sizes. While still workable at small budgets, the practicality at higher budgets or longer runs is unclear."}, "questions": {"value": "1. How does SHE-LoRA fare against membership/property inference or reconstruction attacks that use auxiliary priors? Any reasons to expect similar robustness?\n2. How should a,b,c be chosen in practice? Could you provide an adaptive rule and an ablation on real datasets?\n3. For highly dynamic Non-IID clients, how often should renegotiation occur? Can you report end-to-end time or energy overheads for different periods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6dXAHRk5vc", "forum": "PWChrnrw7Z", "replyto": "PWChrnrw7Z", "signatures": ["ICLR.cc/2026/Conference/Submission1959/Reviewer_pYmU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1959/Reviewer_pYmU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832701692, "cdate": 1761832701692, "tmdate": 1762915974008, "mdate": 1762915974008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed method SHE-LoRA integrates selective homomorphic encryption (SHE) with LoRA-based federated fine-tuning under heterogeneous client resources and Non-IID data. It aims to tackle the challenge of adaptive SHE for heterogeneous clients and the expansion of encrypted subsets on SHE. Specifically, this is done through the proposed HE subset negotiation mechanism and selective encryption and column-aware aggregation. Empirical results have shown that SHE-LoRA is resistant to privacy attacks, communication-efficient, and performant."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is empirically effective, which shows advantages in privacy-preserving, communication cost, and model performance.\n- The problem of vulnerable and heterogeneous LoRA updates is motivated well.\n- Principled design of the proposed algorithm."}, "weaknesses": {"value": "- Limited novelty in the adoption of SHE methods to LLM LoRA fine-tuning.\n- Column-wise weighted averaging is proposed, but the choice of weights (e.g., proportional to client data size or sensitivity) is not formally justified or compared.\n- The negotiated global HE subset is claimed to optimally balance privacy and HE overhead per client, but lacks formal optimality guarantees or approximation bounds (e.g., submodular coverage, budgeted max coverage).\n- The paper argues that encrypting A is sufficient, but the threat model and Section 2.4 discuss expanded encrypted subsets due to BA multiplication. It is not clear whether encrypting only Aâ€™s sensitive columns is enough under all considered attacks.\n- Presentation could be improved by fixing the inline citations, defining the exact weighting scheme in column averaging, and reporting the Non-IID partitioning method."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ijqcc93zcr", "forum": "PWChrnrw7Z", "replyto": "PWChrnrw7Z", "signatures": ["ICLR.cc/2026/Conference/Submission1959/Reviewer_i41f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1959/Reviewer_i41f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995659708, "cdate": 1761995659708, "tmdate": 1762915973771, "mdate": 1762915973771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a privacy-preserving method for fine-tuning large language models in federated learning. Traditional LoRA-based methods risk data leakage through shared gradients, while full homomorphic encryption is too costly. SHE-LoRA introduces selective homomorphic encryption and designs a column-level negotiation and aggregation mechanism to handle heterogeneous clients with different computing resources and privacy needs. The approach includes four key stages: (1) clients privately report column sensitivity to jointly select a global subset for encryption; (2) encrypted columns are reordered for efficient HE operations; (3) aggregation aligns plaintext and ciphertext parts column-wise; (4) results are re-factorized into new low-rank parameters. Experiments on NLP tasks show SHE-LoRA maintains model accuracy while reducing encryption and communication overhead by up to 99% compared with full HE, and it effectively defends against gradient inversion attacks like DAGER. The system scales to large models and non-IID data, proving that partial, structured encryption can offer strong privacy with minimal cost in federated LoRA training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The framework explicitly supports clients with different hardware capabilities, network conditions, and privacy budgets.\n2. This paper introduces selective homomorphic encryption at the column level of LoRA matrices, encrypting only the most privacy-sensitive components.\n3. Experiments on NLP and vision datasets demonstrate that SHE-LoRA achieves accuracy comparable to or better than state-of-the-art methods (e.g., Flex-LoRA) under heterogeneous and Non-IID conditions."}, "weaknesses": {"value": "1. The privacy guarantees of selective encryption, the convergence behavior of federated training under mixed plaintext/ciphertext updates, and the optimality of the HE subset negotiation are not formally proved.\n2. SHE-LoRA method can't adapt to heterogeneous LoRA approaches like FLoRA.\n3. The experiments rely on relatively small base models and simple benchmark tasks, which limits the generalizability of its results to large-scale or more complex real-world scenarios. It is recommended to evaluate the method on stronger base models such as Qwen3 and Llama 3.2, as well as on more challenging benchmark tasks including MMLU-Pro, GPQA, MuSR, MATH, IFEval, and BBH."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tkDryGNMoG", "forum": "PWChrnrw7Z", "replyto": "PWChrnrw7Z", "signatures": ["ICLR.cc/2026/Conference/Submission1959/Reviewer_9N3M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1959/Reviewer_9N3M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240240132, "cdate": 1762240240132, "tmdate": 1762915972397, "mdate": 1762915972397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}