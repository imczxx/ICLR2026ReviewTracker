{"id": "TRM3GP3u2O", "number": 8365, "cdate": 1758080147224, "mdate": 1759897789482, "content": {"title": "PSRT: Accelerating LRM-based Guard Models via Prefilled Safe Reasoning Traces", "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on tasks such as mathematics and code generation. Motivated by these strengths, recent work has empirically demonstrated the effectiveness of LRMs as guard models in improving harmful query detection. However, LRMs typically generate long reasoning traces during inference, causing substantial computational overhead.\nIn this paper, we introduce $\\textbf{PSRT}$, a method that replaces the model's reasoning process with a $\\textbf{P}$refilled $\\textbf{S}$afe $\\textbf{R}$easoning $\\textbf{T}$race, thereby significantly reducing the inference cost of LRMs. Concretely, PSRT prefills \"safe reasoning virtual tokens\" from a constructed dataset and learns over their continuous embeddings. With the aid of indicator tokens, PSRT enables harmful-query detection in a single forward pass while preserving the classification effectiveness of LRMs.\nWe evaluate PSRT on 7 models, 13 datasets, and 8 jailbreak methods. In terms of efficiency, PSRT completely removes the overhead of generating reasoning tokens during inference. In terms of classification performance, PSRT achieves nearly identical accuracy, with only a minor average F1 drop of 0.015 across 7 models and 5 datasets", "tldr": "We replace the LRM-based guard model’s reasoning process with a prefilled safe reasoning trace, thereby preserving its capability while significantly reducing the computational overhead.", "keywords": ["AI Safety", "LRM", "Inference acceleration", "Guard Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f218a7e487949c3ad404b649c20da93e0f4f942b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission pertains to large reasoning models (LRMs) used as guard models for detecting harmful queries. It proposes PSRT, a method that replaces the generation of reasoning traces with a fixed, prefilled \"reasoning trace\" composed of optimized embedding vectors. The paper presents an extensive evaluation of PSRT in terms of models (reasoning guard models, non-reasoning guard models, non-guard models trained to become reasoning guard models) and datasets (harmful, jailbreak, harmless, mixed). The overall finding is that PSRT preserves the detection performance of reasoning guard models while avoiding generation of reasoning traces, thereby substantially reducing the number of generated tokens."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The main idea is a clever application of p-tuning (prefix tuning, prompt tuning) to reasoning traces, specifically to optimize a fixed reasoning trace for harmful query detection.\n- Demonstrates that generation of reasoning traces is not needed to achieve almost the same detection performance (and in a few cases even better performance) compared to reasoning guard models"}, "weaknesses": {"value": "1. The most important shortcoming for me is the use of number of generated tokens as a proxy for computational cost. With PSRT (as I understand it), since the query varies and occurs before the fixed reasoning trace, it is still necessary to perform a forward pass on the tokens of the reasoning trace (computing all their internal representations, etc.). Simply reporting the number of generated tokens does not measure the cost of this forward pass, nor whatever computational savings are achieved by performing this forward pass on fixed tokens rather than generating a similar number of new tokens.\n1. I am unsure about the significance of including the SFT-only models (Qwen3-8B, Llama-3.1-8B-Instruct, etc.), as well as the components related to them, namely the dataset construction and SFT in Section 3.1. Figure 1 shows that these models are Pareto-dominated by GuardReasoner (lower F1 score, more generated tokens). Moreover, it is not clear to me how novel is the method in Section 3.1 for training guard models, or how specialized it is for harmful query detection (please see the next point). Thus, the main significance that I see is to show the \"generality of PSRT across diverse model architectures,\" but I am not sure that this warrants so much space in the main paper. I would have been more interested in seeing the additional results on GuardReasoner (Appendix A.1) in the main paper and discussed in greater depth, since GuardReasoner is a stronger model.\n1. The paper limits itself to detecting harm in the query/model input and not in the model output. The reason for this limitation is not clear.\n1. Section 2 cites prior work on shortening reasoning traces. It would have been good to see one of these methods used as an experimental comparison because it would be an intermediate approach that does not avoid generating reasoning traces completely.\n1. The paper does not provide deeper insight into why PSRT works. Can the virtual reasoning trace be interpreted somehow? What are the relative contributions of the averaging initialization and subsequent fine-tuning?\n\nMore minor:\n1. I find the term \"safe reasoning trace\" confusing because the predominant reading of this term is \"a reasoning trace that is safe,\" i.e., free from harmful content, not \"reasoning about safety.\" I think \"safety reasoning trace\" would be better.\n1. Section 3.1 implies that DeepSeek-V3.1 is used as the judge of harmfulness. If this is correct, then this dependence on a single LLM could be acknowledged as a limitation.\n1. Lines 261-262: Sections 3.1 and 3.2 are not experiment sections. Perhaps wrong references?\n1. It would be good to perform the second ablation (omitting the average initialization) for GuardReasoner models also.\n1. Line 724: Should Table 4 be Table 5? Table 4 is on mixed datasets."}, "questions": {"value": "1. Number of generated tokens after PSRT: For the GuardReasoner models, is the number of generated tokens still around e.g. 17 in Table 2 because the model generates that many as answer tokens? Why are the corresponding numbers for the SFT-only models much higher, in the 70s or 80s or even higher?\n1. Lines 360, 363: I do not see the exact numbers quoted here (99.26%, etc.) in Table 2. Are these numbers averages over the three sizes of GuardReasoner models?\n1. In the ablation study, what initialization is used instead of the average embeddings?\n1. In Table 5, why are the TPRs of the GuardReasoner models so uneven, and in particular, why is the original 8B one so poor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "While I do not think that an ethics review is needed, I do think the ethics statement is a little too quick to assert that no concerns are raised. This is because the work deals with harmful content. It is also not clear what is meant by \"sensitive terms\" in Section 4.1, \"Dataset for training\" paragraph."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hKp4JThCR6", "forum": "TRM3GP3u2O", "replyto": "TRM3GP3u2O", "signatures": ["ICLR.cc/2026/Conference/Submission8365/Reviewer_vQT3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8365/Reviewer_vQT3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760738415003, "cdate": 1760738415003, "tmdate": 1762920276321, "mdate": 1762920276321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PSRT, a method to accelerate Large Reasoning Model (LRM)–based guard models by eliminating explicit reasoning token generation during inference. PSRT introduces Prefilled Safe Reasoning Traces, represented as optimized “safe reasoning virtual tokens” in the embedding space, allowing the model to perform harmful-query detection with a single forward pass. The authors demonstrate the method’s generality across 7 models, 13 datasets, and 8 jailbreak attacks, showing comparable detection performance (≤0.015 average F1 drop) while completely removing reasoning overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel and practical contribution: The paper addresses a real bottleneck in LRM deployment, inference latency due to reasoning traces, and proposes an elegant solution by embedding “prefilled reasoning” directly into model inputs.\n\n- Strong empirical validation: Extensive experiments across diverse models (Qwen, Llama, ChatGLM, Mistral, GuardReasoner) and datasets (StrongReject, JBB, SimpleSafetyTest, AdvBench, etc.) show consistent performance with drastically reduced computational cost.\n\n- Well-motivated theoretical grounding: The connection between reasoning trace averaging and point-estimate optimality (Proposition B.5), and the ELBO interpretation for training objective, make the approach conceptually sound."}, "weaknesses": {"value": "- Limited conceptual novelty: The idea is closely related to p-tuning and prompt embedding averaging, which have been explored for efficiency. The main novelty lies in the specific application to guard models rather than a fundamentally new optimization principle.\n\n- Ablation insufficiently deep: The ablation (Fig. 3) focuses mainly on SFT and averaging initialization; it would be valuable to test different trace lengths, embedding dimensions, or virtual token counts to probe robustness.\n\n- Evaluation scope: The paper is entirely focused on binary harmful query detection. Demonstrating that PSRT is effective on multi-class classification or structured safety reasoning tasks (e.g., toxicity type detection) is more impactful"}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tJg72hzsBj", "forum": "TRM3GP3u2O", "replyto": "TRM3GP3u2O", "signatures": ["ICLR.cc/2026/Conference/Submission8365/Reviewer_93No"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8365/Reviewer_93No"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931042117, "cdate": 1761931042117, "tmdate": 1762920274817, "mdate": 1762920274817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Prefilled Safe Reasoning Trace (PSRT), a novel method designed to enhance the efficiency of Large Reasoning Models (LRMs) used for safety detection (e.g., harmful or jailbreak query classification). Traditional LRM-based guard models achieve strong safety performance but suffer from significant inference-time overhead due to long reasoning traces. PSRT addresses this issue by replacing explicit reasoning generation with prefilled “safe reasoning virtual tokens”, effectively compressing the reasoning process into a single forward pass.\nThe proposed framework introduces three key components:\n\n1. Safe Reasoning Dataset Construction: A curated reasoning dataset is built using DeepSeek-V3.1 to generate reasoning traces and safe/unsafe labels for queries.\n\n2. Safe Reasoning Token Initialization: Prefilled “safe reasoning tokens” r_s are initialized in the embedding space by averaging reasoning embeddings, replacing explicit reasoning sequences.\n\n3. Single-Pass Binary Classification: The model leverages the prefilled r_s to directly classify queries as safe or unsafe without generating reasoning tokens."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper has two notable strengths:\n\n**First,** it provides a clear and practical solution for accelerating safety reasoning in Large Reasoning Models (LRMs). By introducing Prefilled Safe Reasoning Traces (PSRT), the authors successfully remove the need for explicit reasoning generation while maintaining nearly the same detection performance. This represents a meaningful step toward efficient and deployable LRM-based safety systems, especially in latency-sensitive scenarios.\n\n**Second,** the experimental evaluation is extensive and convincing. The authors validate PSRT across multiple model families (e.g., Qwen, Llama, GLM, Mistral) and a wide range of datasets (including harmful and jailbreak benchmarks), with detailed quantitative analysis and qualitative visualization. This comprehensive setup provides strong empirical evidence for the method’s robustness and general applicability."}, "weaknesses": {"value": "Some concerns arise regarding the scalability and generalization of using a single r_s.\n\n**First**, the current dataset construction heavily relies on existing safety reasoning datasets (e.g., GuardReasoner, ReNeLLM), which raises questions about the model’s cross-distribution generalization, whether it has truly learned generalizable safety reasoning logic or merely memorized dataset-specific patterns.\n\n**Moreover**, as the scope and diversity of safety-related datasets continue to grow, it remains unclear whether a single global r_s can adequately cover the full spectrum of safety requirements, and whether its generalization performance can be maintained under larger and more diverse settings."}, "questions": {"value": "See questions in the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "78MMC61bry", "forum": "TRM3GP3u2O", "replyto": "TRM3GP3u2O", "signatures": ["ICLR.cc/2026/Conference/Submission8365/Reviewer_3png"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8365/Reviewer_3png"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958048490, "cdate": 1761958048490, "tmdate": 1762920274082, "mdate": 1762920274082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The topic of this paper is the LRM-based guardrail model. And the authors aim to reduce the computational costs of this kind of guard model by replacing the model's reasoning process with a prefilled safe reasoning trace. The comprehensive experiments demonstrate the effectiveness of the proposed method. The computational overhead of generating reasoning tokens is removed yet the performance doesn't drop."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The experiments are very comprehensive, e.g., the PSRT is evaluated on 7 models, 13 datasets, and 8 jailbreak methods. \n\n2. The code is provided, which ensures reproducibility. \n\n3. The paper is well-motivated and the topic is practical."}, "weaknesses": {"value": "1. The color in Figure 1 is confused. For example, for the Qwen3-8B model, the line is blue, but the delta and the circle are black. Besides, it seems to be hard to identify GuardReasoner-3B and GuardReasoner-8B. In addition, it is not clear why these instruct models like LLaMA-3.1-8B-Instruct or base models like Qwen3-8B will generate more tokens than the LRM-based guardrail model, i.e., GuardReasoner.\n\n2. The idea of the proposed method is similar to Coconut [1]. Please discuss it and identify the novelty.  \n\n3. The efficiency experiments are missing, i.e., time costs and GPU memory costs of the LRM-based guard models and the proposed models. Please detail the inference process of the proposed method. Does it support vLLM?\n\n4. Although the authors claim the proposed method can reduce the reasoning tokens significantly, it seems to reduce the explainability of the LRM-based models since the prefilled embeddings of safe reasoning traces are not readable.\n\n5. Minor: missing discussion on an LRM-based guard model [2] in the related work part. The notation table is missing.\n\n\n[1] Training Large Language Models to Reason in a Continuous Latent Space\n\n[2] GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning"}, "questions": {"value": "1. How's the inference process of the proposed method? Does it support vLLM?\n\n2. How can the proposed method keep the explainability of the LRM-based models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "v56KOxMeMh", "forum": "TRM3GP3u2O", "replyto": "TRM3GP3u2O", "signatures": ["ICLR.cc/2026/Conference/Submission8365/Reviewer_1p4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8365/Reviewer_1p4m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006085203, "cdate": 1762006085203, "tmdate": 1762920273660, "mdate": 1762920273660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}