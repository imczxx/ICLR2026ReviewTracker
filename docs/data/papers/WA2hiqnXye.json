{"id": "WA2hiqnXye", "number": 6922, "cdate": 1758002153984, "mdate": 1762941617877, "content": {"title": "Towards Non-destructive Privacy Protection for LVLMs via node-level localized editing", "abstract": "Large Vision-Language Models (LVLMs) have shown astonishing potential in various vision tasks and are broadly used in sectors like finance and medicine. However, the risk of abuse exists, where attackers may leverage these models to steal private information, creating security vulnerabilities for their deployment. Studies show that LVLMs struggle to consistently refuse privacy-compromising instructions from users. Current privacy protection research primarily focuses on safeguarding training data, aiming to prevent models from leaking sensitive information contained within it. However, privacy leakage can extend beyond training data, where models may be misused to extract private information from images or infer sensitive location details. The protection of such external privacy has received little attention.\nTo address this, we introduce PRN-Edit, a privacy risk mitigation method based on model editing. Our method improves a model's privacy protection by increasing its rate of refusal to answer privacy-related questions, and it can generalize to novel sensitive questions not seen during the mitigation process. PRN-Edit works by using a learnable feature mask to locate privacy risk nodes in the feature encoding of user instructions, which then precisely guides the update of model parameters. Through comprehensive experiments on MiniGPT-4 and LLava-1.5, we show that our algorithm significantly boosts the model's privacy protection while maintaining its utility.", "tldr": "A privacy risk mitigation algorithm based on localized feature model editing.", "keywords": ["Large Vision-Language Model", "Model Editing", "Privacy Protection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7cc8dc5018e6cbfc017d00d5cbf4deb17befec4e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to mitigate the privacy leakage problem in large vision-language models (LVLMs). The authors propose PRN-Edit, a two-stage localised editing method designed to modify the models’ responses to predefined privacy-sensitive queries."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- **Novel Problem Definition** The paper proposes to use a distinct editing problem to address the privacy risk, which seems to be practical and relevant for real-world deployment of LVLMs\n\n- **Methodological Design and Motivation**  The proposed neuron-level localisation and editing pipeline presents a technically interesting approach to fine-grained model intervention, offering a reasonable degree of innovation, particularly in aligning privacy mitigation to preserve model generality. The emphasis on non-destructive editing—maintaining task performance while suppressing privacy sensitive behaviours is well motivated and supported by empirical results.\n\n- **Evaluation Design and Analysis** The authors conduct extensive experiments by constructing a paired privacy dataset and further evaluating their approach against recent model-editing baselines on the ScienceQA and MLLMGuard benchmarks."}, "weaknesses": {"value": "1. **Scope Alignment and Relevance to LVLMs** The proposed technique does not appear to be tightly coupled with the LVLM-specific problem setting described in the paper. While the method is evaluated on multimodality models, its core mechanism, localised parameter editing, could in principle be applied to any LLMs and LVLMs. Moreover, most of the compared baselines (e.g., MEMIT, DINM) were originally designed for text-only LLMs, which raises concerns about whether the claimed contributions are genuinely tailored to the multimodality privacy challenge.\n\n2. **Clarity, Generalisation, and Sensitivity Issues** Certain parts of the current version lack conceptual clarity and suffer from an overly complex narrative. The proposed framework seems to rely heavily on a specific definition of privacy-sensitive questions and requires two additional optimisation stages. As shown in Table 5, the method is quite sensitive to hyperparameters and the choice of the edited network layer(s). These factors collectively cast doubt on the robustness and generalisability of the approach beyond the tested settings.\n\n3. **Overlap with LLM Guardrails and Missing Discussion** The problem addressed in this paper overlaps conceptually with LLM safety guardrails, which both aim to enforce safe or privacy-preserving model behaviour. However, the paper does not provide a clear discussion or experimental comparison with guardrail-based approaches. Including such analysis would help clarify whether PRN-Edit offers complementary advantages or merely reimplements an internalised version of existing behavioural defences."}, "questions": {"value": "1. Could the authors provide more detailed explanations of the loss functions defined in Equations (3) and (4)?\n\n2. On page 5, lines 262–267, referring to $M_l$ as a mask seems somewhat inaccurate, since it functions more like an additional linear layer. Moreover, the relationship between the elements of $M_l$ and the cosine-based interpretation is not clearly justified. Simply stating that negative elements correspond to privacy-risky nodes makes more sense."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JQhgfvFz3P", "forum": "WA2hiqnXye", "replyto": "WA2hiqnXye", "signatures": ["ICLR.cc/2026/Conference/Submission6922/Reviewer_uawP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6922/Reviewer_uawP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761598149390, "cdate": 1761598149390, "tmdate": 1762919157835, "mdate": 1762919157835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an important problem of safeguarding the large vision language models that struggle to consistently refuse the privacy-compromising instructions from the user. The paper proposes PRN-Edit, which is a privacy risk mitigation method based on model editing. It improves the model's privacy protection by increasing the rate of refusal to answer privacy-related questions. It can also generalize well to unseen samples. It uses a learnable feature mask to locate the privacy risk nodes in the feature encoding of the user instructions, which then precisely guides the update of the model parameters. Experimental results on LVLMs such as MiniGPT-4 and LLava-1.5 further verify the claims."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem taken up is important and useful for the community (though not being solved for the first time).\n\n2. The major advantage is that the method generalizes well to unseen samples, which is the outcome of the better dataset creation for training the model."}, "weaknesses": {"value": "1. The paper is not very well written, and there are multiple missing details (which are there in appendix), making the solution confusing; for instance, the authors did not even explain what $\\mathcal{L}_{sen}$ looks like in the main text (which is the main contribution) and deferring it to the Appendix, and there is no information on what is frozen and what is being learned during the module 2 training. There is no detailed description of why the values of $\\alpha$ and $\\beta$ are chosen as such.\n\n2. Although the problem is being phrased as LVLM privacy protection, the underlying solution only works in the LLM part, where the input query is being observed and decisions are made based on that. For instance, the example given in the paper, that given a passport attacker can ask for the passport number, hence the problem is only in the language part, as per my understanding. This makes all the existing privacy protection methods candidates as baselines.\n\n3. The paper does not discuss anything about why the existing LLM privacy protection methods cannot be used. As the major processing is performed on the LLM part of the backbone why the existing LLM-based methods for privacy protection are not applicable, and are not being considered as a baseline."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YTXe5yYhST", "forum": "WA2hiqnXye", "replyto": "WA2hiqnXye", "signatures": ["ICLR.cc/2026/Conference/Submission6922/Reviewer_hdga"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6922/Reviewer_hdga"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649060280, "cdate": 1761649060280, "tmdate": 1762919157530, "mdate": 1762919157530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "2gOBdjPlkI", "forum": "WA2hiqnXye", "replyto": "WA2hiqnXye", "signatures": ["ICLR.cc/2026/Conference/Submission6922/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6922/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762939528964, "cdate": 1762939528964, "tmdate": 1762939528964, "mdate": 1762939528964, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses privacy risks in Large Vision-Language Models, focusing on scenarios where models may follow sensitive user instructions, potentially exposing private information even if it was not in the training data. To mitigate this, the authors propose PRN-Edit, a localized feature-level model editing method that identifies privacy-risk nodes in the feed-forward layers of Transformers and selectively updates them to increase refusal rates for sensitive queries. They also construct a paired-sample dataset covering six privacy categories to help models distinguish between sensitive and benign questions. Experiments on MiniGPT-4 and LLaVA-1.5 demonstrate that PRN-Edit improves privacy protection while maintaining model utility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper introduces a precise method for mitigating privacy risks in LVLMs by targeting specific feature nodes, which allows fine-grained control without affecting overall model performance. \n2.\tThe paper introduces a paired-sample dataset which provides a valuable resource for training and evaluating privacy-aware models, enhancing the generalizability of their approach.\n3.\tThe paper conducts comprehensive experiments to validate their method, evaluating both on the proposed paired-sample dataset and on unseen datasets to test generalization. Additionally, they perform ablation studies for layer selection, demonstrating the effectiveness and contribution of each component in their approach."}, "weaknesses": {"value": "1. The identification of privacy-risk nodes relies purely on gradient signals from cross-entropy losses rather than any causal or interpretable link to privacy-sensitive representations. Consequently, the learned mask may capture features correlated with output refusal behavior rather than truly encoding private information.\n2.\tAlthough the authors note that their two-module method is computationally less efficient than single-stage optimization in discussion, they do not provide any experimental data to support this claim. Without quantitative comparisons of runtime or resource consumption against baselines, the discussion on efficiency remains speculative and lacks empirical validation.\n3.\tAlthough PRN-Edit achieves higher refusal rates than baseline methods on unseen datasets (ScienceQA and MLLMGuard), the evaluation does not analyze the cases where the model still fails to prevent privacy leakage. It remains unclear whether these failures are due to insufficiently learned mask features, unseen privacy attributes during training, or other limitations? Therefore, the actual generalization capability may still be constrained by the diversity and coverage of privacy-related attributes in the training data, and this aspect warrants further discussion.\n4.\tThe learnable mask is trained on a paired-sample dataset. The success of generalization likely depends on how well the privacy categories in training cover real-world sensitive information.\nIs there a risk that the mask overfits to the specific question phrasing or token locations seen in training?\n5.\tThe evaluation of model utility primarily focuses on response rates to benign questions, without assessing whether the model’s reasoning quality or task-specific accuracy is preserved after PRN-Edit. Adopting comprehensive utility metrics is necessary to validate that the method maintains overall performance, which is crucial for broader real-world applications, especially when considering efficiency concerns noted in Weakness 1."}, "questions": {"value": "please check the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KsTnLzj53y", "forum": "WA2hiqnXye", "replyto": "WA2hiqnXye", "signatures": ["ICLR.cc/2026/Conference/Submission6922/Reviewer_V8rL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6922/Reviewer_V8rL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822658752, "cdate": 1761822658752, "tmdate": 1762919157138, "mdate": 1762919157138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}