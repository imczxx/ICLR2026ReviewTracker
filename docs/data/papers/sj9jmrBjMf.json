{"id": "sj9jmrBjMf", "number": 15554, "cdate": 1758252634795, "mdate": 1759897299785, "content": {"title": "VRPRM: Process Reward Modeling via Visual Reasoning", "abstract": "Process Reward Model (PRM) is widely used in the post-training of Large Language Model (LLM) because it can perform fine-grained evaluation of the reasoning steps of generated content. However, most PRMs lack long-term reasoning and deep thinking capabilities. On the other hand, although a few works have tried to introduce Chain-of-Thought capability into PRMs, the annotation cost of CoT-PRM data is too expensive to play a stable role in various tasks. To address the above challenges, we propose VRPRM, a process reward model via visual reasoning, and design an efficient two-stage training strategy. Experimental results show that using only 3.6K CoT-PRM SFT data and 50K non-CoT PRM RL training data, VRPRM can surpass the non-thinking PRM with a total data volume of 400K and achieved a relative performance improvement of up to 118\\% over the base model in the BoN experiment. This result confirms that the proposed combined training strategy can achieve higher quality reasoning capabilities at a lower data annotation cost, thus providing a new paradigm for PRM training with more efficient data utilization.", "tldr": "", "keywords": ["Process Reward Model", "Reinforcement Learning", "Chain-of-Thought"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1307374b37dec12b62d517a0d4dddeb893fd0692.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes VRPRM, a multimodal Process Reward Model (PRM) with explicit Chain-of-Thought (CoT) capabilities, trained via a data-efficient, two-stage paradigm: SFT on a small, high-quality CoT dataset to elicit reasoning, followed by RL amplification on large-scale non-CoT data. Used as a critic for Best-of-N selection, VRPRM significantly improves multimodal reasoning benchmarks and outperforms the SOTA (e.g., VisualPRM) on VisualProcessBench (FEI/AEI) with only ~13.4% of the training data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method achieves modest performance improvements with better data efficiency"}, "weaknesses": {"value": "The techniques are well-explored in the text-only domain; neither CoT-PRM nor RL is novel. The paper also omits discussion of relevant related work. For instance, R-RPM[1] discussed a CoT-based PRM with further optimization using DPO, achieving high data efficiency, and DeepSeek-GRM[2] explored a generative universal reward model in an RL context. Overall, the paper's novelty is insufficient.\n\nThe improvement on recent models like MiMo appears marginal. This raises questions about the method's effectiveness, especially given that \"thinking\" capabilities are now prevalent in models such as Qwen3-4B-VL-Thinking.\n\n[1] R-PRM: Reasoning-Driven Process Reward Modeling\n[2] Inference-Time Scaling for Generalist Reward Modeling"}, "questions": {"value": "I suggest reporting Major@8 and Pass@8 in Table 2 to better clarify the magnitude of the improvement and its position relative to the upper bound. I also recommend reporting the performance of the teacher model used for distillation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CXwE5LEcMW", "forum": "sj9jmrBjMf", "replyto": "sj9jmrBjMf", "signatures": ["ICLR.cc/2026/Conference/Submission15554/Reviewer_jxEj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15554/Reviewer_jxEj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794408707, "cdate": 1761794408707, "tmdate": 1762925828231, "mdate": 1762925828231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VRPRM, a multimodal process reward model that inserts explicit CoT reasoning into PRM training and then scales it with reinforcement learning. The authors motivate PRMs as step-level evaluators used for RL and test-time scaling, noting that existing visual PRMs lack deep reasoning; they position VRPRM as a two-stage alternative: supervised fine-tuning on a small CoT-PRM set followed by RL on larger non-CoT PRM data. In the abstract they claim strong data efficiency, 3.6K CoT-PRM SFT + 50K non-CoT RL surpasses a 400K non-thinking PRM and yields up to 118% relative gains over a base model under Best-of-N selection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Simple two-stage recipe combining CoT and RL for easy adoption.** The paper first uses SFT on a small, structured CoT-PRM dataset to seed reasoning, then applies RL on larger non-CoT PRM data to scale. CoT and RL are explicitly integrated in a multimodal PRM, and rewards cover both format and process, keeping implementation straightforward.\n- **Stronger process supervision with clear empirical gains.** On *VisualProcessBench* it surpasses prior PRMs on process metrics. ablations show removing CoT degrades performance while adding RL yields further gains, supporting the training recipe.\n- **Practical test-time scaling.** Used as a BoN critic at inference, it consistently improves results across multiple multimodal benchmarks and model sizes."}, "weaknesses": {"value": "- My main concern is the inconsistency between the experimental settings of Table 1 and Table 2. In Table 1, the authors evaluate VRPRM using Qwen and MiMo backbones to analyze the model’s process reasoning ability, but in Table 2, they switch to the InternVL2.5 family as the policy model for BoN testing without specifying which version of VRPRM serves as the critic, and they omit the MiMo results entirely. This inconsistency makes the two tables difficult to compare and raises questions about the robustness of the approach across backbones. In addition, the inclusion of results from external leaderboards further weakens the fairness and reproducibility of the reported improvements.\n- Another concern is the narrow scope of the evaluation datasets.\n  Most benchmarks used in the paper (such as VisualProcessBench, MathVista, MathVision, MathVerse-VO, WeMath, and LogicVista) are all visual–mathematical or logic reasoning datasets.\n  While these settings highlight the model’s performance on step-by-step reasoning tasks, they do not demonstrate its generality to broader multimodal understanding domains (e.g., commonsense reasoning, visual QA, or instruction following).\n  As a result, the claimed contribution of a “general multimodal PRM” currently lacks sufficient empirical support beyond the math reasoning domain.\n- Please clarify what is methodologically new beyond combining CoT-SFT with RL under process/format rewards. In particular, position VRPRM against ATHENA[1] and MM-PRM[2]: what is distinct in your reward signal/design, two-stage training protocol, or test-time critic use? A brief comparison (and, ideally, head-to-head results under a unified pipeline) would resolve concerns that the contribution is primarily an engineering recipe.\n\n[1]  ATHENA: ENHANCING MULTIMODAL REASONING WITH DATA-EFFICIENT PROCESS REWARD MODELS\n\n[2] MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision"}, "questions": {"value": "Please address the identified weaknesses in your response, and correcting the noted typos and inconsistencies may further improve the paper’s clarity and readability.\n\n- PROMBLEM FORMULATION → PROBLEM FORMULATION.\n\n- The bold and underline annotations in Table 1 appear inconsistent with the numerical values (e.g., some smaller numbers are highlighted while larger ones are not).  Please double-check the highlighting to ensure that the bold font truly marks the best result and the underline the second best.\n\n- The paper introduces many abbreviations (e.g., RM/ORM/PRM/CoT, SFT/RL, BoN, FEI/AEI, MMMU, VO, etc.) and not all are defined at first mention or used consistently across sections/tables.  Please define every acronym upon first use in the main text.\n\n- In the Introduction, Figure 1 is mistakenly referred to as Table 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HOZIloAGYy", "forum": "sj9jmrBjMf", "replyto": "sj9jmrBjMf", "signatures": ["ICLR.cc/2026/Conference/Submission15554/Reviewer_SHdC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15554/Reviewer_SHdC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899065052, "cdate": 1761899065052, "tmdate": 1762925827706, "mdate": 1762925827706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VRPRM, a multimodal Process Reward Model trained with a two-stage scheme: (i) cold-start SFT on ~3.6K CoT-PRM data generated by Claude-3.7-Sonnet to teach formatting and step-wise judging, and (ii) RL scaling on 50K non-CoT PRM data. The PRM predicts a binary step label and uses the probability of token “1” at each step as the step reward (Eq. 9), then aggregates over steps for BoN selection. On VisualProcessBench (FEI/AEI) and multiple reasoning benchmarks, VRPRM reportedly outperforms VisualPRM and yields strong BoN gains with InternVL2.5 policies."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Clear, practical PRM pipeline: The two-stage training design (CoT-PRM SFT → RL on non-CoT) is well-motivated and carefully specified, with strict format/quality checks.\n\nStrong empirical results on process evaluation: On VisualProcessBench, VRPRM-7B variants outperform prior work (incl. VisualPRM) on both FEI/AEI."}, "weaknesses": {"value": "Cross-family generalization not fully established: Most experiments pair VRPRM with InternVL2.5 policies for test-time scaling, with no results on other families (e.g., Qwen-VL or GPT-class).\n\nRL training dynamics are underreported: The paper does not show reward trajectories or response-length curves during RL, making it hard to diagnose the training process.\n\nHigher inference overhead for CoT-PRM: Compared to PRMs that output a single {+/-} token, the CoT-PRM requires structured reasoning and stepwise judgments, which substantially increases per-candidate evaluation time."}, "questions": {"value": "1. Please address the key weaknesses we raised.\n\n2. Strong-policy stress test on HLE: In light of VRPRM’s strong performance with InternVL models, it would be very helpful to also report BoN results with a state-of-the-art non-InternVL policy (e.g., GPT-5-mini or a comparable model you can access) on Humanity’s Last Exam (HLE, with images), to see whether VRPRM could further advance SOTA.\n\n3. Concrete example of VRPRM scoring:\nPlease include one full worked example (preferably from your test set) showing: (1) The policy’s generated candidate. The corresponding VRPRM's generated judgments. (2) For the selected candidate, the step list and the per-step probability of token “1” (your positive label)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oBP6fSdtyM", "forum": "sj9jmrBjMf", "replyto": "sj9jmrBjMf", "signatures": ["ICLR.cc/2026/Conference/Submission15554/Reviewer_T1Q6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15554/Reviewer_T1Q6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953820797, "cdate": 1761953820797, "tmdate": 1762925827261, "mdate": 1762925827261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VRPRM (Visual Reasoning Process Reward Model), a new Process Reward Model (PRM) designed to perform fine-grained, step-by-step evaluation of long-term multimodal reasoning processes. The work addresses the limitation of existing PRMs, which often lack the capability for deep, Chain-of-Thought (CoT) reasoning, especially in multimodal settings. VRPRM proposes an efficient two-stage training strategy: 1) initial Supervised Fine-Tuning (SFT) on a small amount of CoT-PRM data (3.6K samples) to establish reasoning foundations, followed by 2) Reinforcement Learning (RL) fine-tuning (GRPO) on a larger set of non-CoT PRM data (50K samples) to enhance robustness and scalability. The model demonstrates strong performance, with results indicating it can significantly surpass non-thinking PRMs trained on much larger datasets, achieving a high relative performance improvement over the base model in the BoN (Best-of-N) experiment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Effective Multi-stage Training for Data Efficiency:** The core methodology, combining a small, high-quality SFT CoT dataset with a larger, less costly RL dataset, is highly practical. The results strongly suggest that the initial SFT phase effectively primes the model for complex reasoning, allowing the subsequent RL stage to generalize this capability robustly and efficiently, overcoming the high annotation cost typically associated with CoT-PRM data.\n\n2. **Demonstrated Performance Gains:** The model achieves impressive empirical results, notably achieving a relative performance improvement of up to 118% over the base model in the BoN experiment and surpassing non-thinking PRMs trained on 400K data points using only a fraction of that data. This validates the effectiveness of the process reward modeling approach guided by visual reasoning.\n\n3. **Relevant Application of Existing Techniques:** The paper successfully adapts established and robust techniques—specifically SFT and GRPO (General Reinforcement Policy Optimization)—to the multimodal PRM domain, providing a clean, reproducible framework for advancing step-wise reward modeling in VLMs. The design of the PRM response format to capture step-by-step judgments is clear and effective."}, "weaknesses": {"value": "1. **Increased Computational Overhead:** The explicit nature of the Process Reward Model, which generates a step-by-step judgment (a reasoning trace of its own) for the solver's output, inevitably introduces significant computational overhead compared to non-reasoning, end-to-end reward models (which only generate a single score). The paper does not provide a quantitative analysis of this overhead (e.g., token generation time or total inference latency) relative to simpler PRM or reward model baselines.\n\n2. **Limited Technical Novelty in Methodology:** The core technical components of the method—Supervised Fine-Tuning (SFT) followed by GRPO (a common RL fine-tuning variant)—are standard practices in LLM post-training. The paper's novelty lies primarily in its successful application and empirical demonstration on Multimodal PRMs with a specific, designed output format (process judgment blocks), rather than in the development of a fundamentally PRM-specific new training algorithm.\n\n3. **Unverified SFT Data Quality:** The CoT-PRM SFT dataset, which is critical for establishing the model's fundamental reasoning capability, is generated by a powerful proprietary model (Claude) and only subjected to a format check. There is a significant concern that the step-wise correctness or fine-grained judgment labels assigned by Claude may contain subtle errors or biases that are not caught by a simple format validation. This introduces uncertainty regarding the true quality and correctness of the most expensive and foundational data used in the training process."}, "questions": {"value": "1. Verification of SFT Step-Labels: Given the reliance on Claude for generating the CoT-PRM SFT data, what verification process, beyond a simple format check, was used to ensure the correctness and accuracy of the step-wise judgment labels themselves?\n\n2. Quantitative Overhead Analysis: Given that the VRPRM generates a detailed step-wise reasoning trace, could the authors provide a quantitative analysis of the computational overhead?\n\n3. Reward Model  Generalization: How robust is the final model's step-wise judgment when evaluating reasoning traces for unseen or out-of-distribution multimodal tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G45hMtbheu", "forum": "sj9jmrBjMf", "replyto": "sj9jmrBjMf", "signatures": ["ICLR.cc/2026/Conference/Submission15554/Reviewer_bYma"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15554/Reviewer_bYma"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762182141562, "cdate": 1762182141562, "tmdate": 1762925826432, "mdate": 1762925826432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}