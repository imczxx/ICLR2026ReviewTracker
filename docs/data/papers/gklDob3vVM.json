{"id": "gklDob3vVM", "number": 12311, "cdate": 1758207020862, "mdate": 1759897518252, "content": {"title": "RsGCN: Subgraph-Based Rescaling Enhances Generalization of GCNs for Solving Traveling Salesman Problems", "abstract": "GCN-based traveling salesman problem (TSP) solvers face two critical challenges: poor cross-scale generalization for TSPs and high training costs. To address these challenges, we propose a Subgraph-Based Rescaling Graph Convolutional Network (RsGCN). Focusing on the scale-dependent features (i.e., features varied with problem scales) related to nodes and edges, we design the subgraph-based rescaling to normalize edge lengths of subgraphs. Under a unified subgraph perspective, RsGCN can efficiently learn scale-generalizable representations from small-scale TSPs at low cost. To exploit and assess the heatmaps generated by RsGCN, we design a Reconstruction-Based Search (RBS), in which a reconstruction process based on adaptive weight is incorporated to help avoid local optima. Based on a combined architecture of RsGCN and RBS, our solver achieves remarkable generalization and low training cost: **with only 3 epochs of training on a mixed-scale dataset containing instances with up to 100 nodes, it can be generalized successfully to 10K-node instances without any fine-tuning**. Extensive experiments demonstrate our advanced performance across uniform-distribution instances of 9 different scales from 20 to 10K nodes and 78 real-world instances from TSPLIB, while requiring **the fewest learnable parameters and training epochs** among neural competitors.", "tldr": "We propose a new GCN-based TSP solver with powerful generalization and low training cost.", "keywords": ["Traveling Salesman Problems", "Combinatorial Optimization", "Graph Convolutional Networks", "Rescaling", "Generalization"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5430d1c6690367936cb0a334a01de1c50fcbc060.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces RsGCN, a new Graph Convolutional Network (GCN) model designed to enhance generalization in solving Traveling Salesman Problems (TSP) across varying problem scales. The authors address the issue of poor cross-scale generalization in GCN-based TSP solvers by incorporating a subgraph-based rescaling technique. This approach involves constructing subgraphs using k-Nearest Neighbors (k-NN) and rescaling subgraph edges using Uniform Unit Square Projection to standardize the edge lengths. RsGCN is combined with a Reconstruction-Based Search (RBS) post-search strategy, which further refines the solution by enhancing the diversity of the search space and avoiding local optima. The model demonstrates significant improvements in both performance and training efficiency, achieving state-of-the-art results across TSP instances ranging from 20 to 10,000 nodes while requiring fewer parameters and training epochs compared to previous methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The introduction of subgraph-based rescaling to adaptively normalize edge lengths is a novel idea that addresses the key challenge of generalizing across different TSP scales. This approach allows the model to learn universal patterns from smaller TSP instances, which can then be successfully applied to larger-scale problems without fine-tuning.\n2.The proposed method achieves impressive results with only 3 epochs of training on a mixed-scale dataset containing instances up to 100 nodes. This level of efficiency, combined with the ability to generalize to 10K-node instances, is a significant strength, as it reduces training time and resources compared to other neural methods.\n3.The experimental results demonstrate the superiority of RsGCN + RBS over other methods, including both classical solvers (Concorde, LKH-3) and recent neural network-based approaches. The model’s generalization ability is highlighted by its performance across various test sets, including uniform-distribution instances and real-world TSP instances from TSPLIB."}, "weaknesses": {"value": "1.While RsGCN handles large-scale problems better than previous methods, the model still requires full passes for each decoding step, which might be computationally intensive for extremely large instances (e.g., 100K nodes). Optimizations in terms of decoding efficiency and inference time could be explored.\n\n\n2.The experiments primarily focus on uniform-distribution instances, and the paper acknowledges that RsGCN has not been specifically trained on non-uniform distributions. Although the model generalizes well to real-world instances, a more detailed exploration of how RsGCN adapts to non-uniform distributions (e.g., city clustering) would be valuable.\n\n\n3.The choice of k-NN and k2 for the RBS algorithm is critical, and while the ablation studies show solid results, the sensitivity of the model to these parameters could be more clearly quantified. A more detailed analysis of how varying these parameters impacts model performance would strengthen the study."}, "questions": {"value": "1.While the model performs well on instances with up to 10,000 nodes, how does RsGCN perform on even larger-scale instances (e.g., 100K nodes)? Are there any optimizations or trade-offs that can be made to further improve scalability?\n2.Since RsGCN was trained primarily on uniform-distribution datasets, how well does it generalize to non-uniform TSP distributions (e.g., cities with clustered or irregular distances)? Can the model be easily adapted to handle real-world TSP instances with more complex structures?\n3.The paper introduces RBS, but how does this algorithm compare to other common post-search methods, such as Monte Carlo Tree Search (MCTS) or 2-Opt in terms of performance, flexibility, and computational cost? Is RBS consistently superior in all cases, or could another post-search method be more effective for specific TSP instances?\n4.The paper mentions the selection of k1 and k2 as hyperparameters in the subgraph and RBS design. How sensitive is the performance of RsGCN to changes in these parameters, and can automated methods be used to select the optimal hyperparameters for different TSP instance scales?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kwU1e2ZsKM", "forum": "gklDob3vVM", "replyto": "gklDob3vVM", "signatures": ["ICLR.cc/2026/Conference/Submission12311/Reviewer_R5e5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12311/Reviewer_R5e5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840160947, "cdate": 1761840160947, "tmdate": 1762923237741, "mdate": 1762923237741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RsGCN, a subgraph-based rescaling method to enhance the cross-scale generalization of GCNs for solving Traveling Salesman Problems (TSPs). The key innovation is subgraph construction via k-NN and edge-length rescaling using Uniform Unit Square Projection, normalizing node/edge distributions across scales to allow training on small instances (≤100 nodes) while generalizing to large instances (up to 10K nodes). Combined with a novel Reconstruction-Based Search (RBS) that uses adaptive-weight reconstruction to escape local optima, the framework achieves state-of-the-art results on 9 uniform-distribution scales and 78 real-world TSPLIB instances."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Subgraph-based edge normalization effectively eliminates scale-dependent distortions, enabling robust zero-shot generalization.\n2. Training on mixed-scale instances (≤100 nodes) for only 3 epochs suffices for 10K-node generalization.\n3. RsGCN uses the fewest parameters among neural baselines (416K parameters vs. 1.4M–11M) and scales linearly in inference.\n4. Outperforms GCN/Transformer baselines on both synthetic and real-world TSPs."}, "weaknesses": {"value": "1. The idea of using k-NN to prune a neural architecture is not really novel, although it is suitable to solving TSP problems.\n2. The idea of reconstruction-based search is not really novel either, similar to the improvement-based solvers introduced before."}, "questions": {"value": "1.  If the results of rescaling mechanism are processed by LKH3 for heuristic search, would it be helpful to LKH-3?\n2. I am particularly curious about whether the author have any novel perspectives on neural network applications for TSP solving beyond the proposed improvement-based (or reconstruction-based) paradigm. How might these insights inform future enhancements to RSGCN?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4ZMbGbXdYR", "forum": "gklDob3vVM", "replyto": "gklDob3vVM", "signatures": ["ICLR.cc/2026/Conference/Submission12311/Reviewer_WuKt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12311/Reviewer_WuKt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923431282, "cdate": 1761923431282, "tmdate": 1762923237295, "mdate": 1762923237295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ``RsGCN``, which normalizes edge distances at the node level to eliminate scale-sensitivity across instances of different sizes. Additionally, it proposes ``RBS``, a novel K-OPT algorithm based on ``MCTS``. After only three rounds of supervised training on small-scale TSP instances, the combined ``RsGCN+RBS`` framework generalizes to large-scale TSP problems and achieves good performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method incurs low training costs and, according to experimental results, delivers good performance and generalization.\n\n2. The paper conducts a thorough ablation study on the use of edge normalization and node K-NN."}, "weaknesses": {"value": "1. Clearly, the sole focus on TSP is a major limitation of this paper. The proposed ``RsGCN+RBS`` is an improved variant of the earlier ``Att-GCN+MCTS`` pipeline. The latter, over the past few years, has not demonstrated any clear trend toward being extended to other combinatorial problems.\n\n2. The authors do not report the results right after ``STATE INITIALIZATION``. Similar to ``Att-GCN``, the performance gains may largely come from the post-processing steps. The underlying idea of these post-processing techniques is essentially derived from ``LKH``, i.e., iteratively applying K-OPT to improve solution quality. This is also why these approaches struggle to extend to problems where LKH performs poorly, such as the ``CVRP``."}, "questions": {"value": "1. Scaling edges differently for each node preserves their relative ordering, yet because every node has its own scaling factor the transformed problem is no longer equivalent to the original one; indeed, the optimal tour may change. So ``RsGCN`` is only used to screen the candidate set for ``RBS``, rather than training the ``GCN`` to learn to predict the solution as in previous works?\n\n2. Figures 5 and 7 partly demonstrate the performance gains of ``RsGCN``, but all these experiments are tightly coupled with ``RBS``. I understand the authors’ intention to treat ``RsGCN+RBS`` as an integrated whole; nevertheless, I would still like to see the results when ``RBS`` is completely removed, i.e., after only ``STATE INITIALIZATION``.\n\n3. Unlike MCTS, RBS does not rely on a heatmap and can thus be regarded as a generic post-processing strategy. Could the authors supplement results for other methods combined with RBS, e.g., ``Fast-T2T + RBS`` and ``DIFUSCO + RBS``?\n\n4. The authors mention that future work will extend to ``ATSP`` and ``CVRP``. I would like to know how the proposed ``RsGCN+RBS`` pipeline could be applied to these problems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DaAb7yHPSM", "forum": "gklDob3vVM", "replyto": "gklDob3vVM", "signatures": ["ICLR.cc/2026/Conference/Submission12311/Reviewer_S3J9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12311/Reviewer_S3J9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945036032, "cdate": 1761945036032, "tmdate": 1762923236910, "mdate": 1762923236910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses two critical and persistent challenges in neural combinatorial optimization (NCO) for the Traveling Salesman Problem (TSP): poor cross-scale generalization and high training costs. The authors identify that GCNs are highly sensitive to \"scale-dependent features,\" particularly the distribution of edge lengths, which changes as problem instances grow larger even when normalized in a unit square. The paper proposes RsGCN and RBS to deal with the two challenges. Strong empirical results are presented."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Simple but effective: The core idea of the RsGCN model is refreshingly straightforward. The authors correctly identify that as TSP instances grow, the nodes in the unit square get denser, and the average edge lengths shrink. GCNs, being sensitive to the distribution of their input features (edge weights), fail to generalize. The solution of subgraph-based rescaling isn't a massive new architecture, a complex attention mechanism, or a heavy generative decoder.\n\n- Exceptional training efficiency: The model achieves SOTA performance after training for only 3 epochs on small instances , which the authors report takes ~30-50 minutes on an A100/H20 GPU. This is orders of magnitude faster than competing diffusion-based and RL methods (e.g. fast t2t) that require hundreds of hours of training and fine-tuning. Combined with its minimal parameter count (0.417M), this makes RsGCN a highly practical and scalable solution."}, "weaknesses": {"value": "- Limited contributions on the scope of studied problems. Only tsp is evaluated. More problems e.g. cvrp should be included to demonstrate the effectiveness.\n\n\n- RBS vs. Classical Heuristics: The RBS algorithm is a sophisticated, multi-stage local search (destroy, repair, 2-opt, adaptive weighting). It is a strong heuristic in its own right, which is good, but it also means the performance lift is not purely from the learned GCN. Table 1 shows that RBS (using 5-NN, no GCN) is already a top-tier solver. This is a common feature of NCO, but it's worth noting that a significant part of the performance gain comes from a well-engineered (non-learned) search heuristic."}, "questions": {"value": "- how to set k the number of nearest neighbors for instances of different scales? Would a different k from training cases when applied on testing cases affect the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7JtdleVvBI", "forum": "gklDob3vVM", "replyto": "gklDob3vVM", "signatures": ["ICLR.cc/2026/Conference/Submission12311/Reviewer_GShj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12311/Reviewer_GShj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999113047, "cdate": 1761999113047, "tmdate": 1762923236671, "mdate": 1762923236671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}