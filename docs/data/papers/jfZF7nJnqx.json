{"id": "jfZF7nJnqx", "number": 16831, "cdate": 1758269214018, "mdate": 1763184393699, "content": {"title": "MergeMoE: Efficient Compression of MoE Models via Expert Output Merging", "abstract": "The Mixture-of-Experts (MoE) technique has proven to be a promising solution to efficiently scale the model size, which has been widely applied in recent LLM advancements. However, the substantial memory overhead of MoE models has made their compression an important research direction. In this work, we provide a theoretical analysis of expert merging, a recently proposed technique for compressing MoE models. Rather than interpreting expert merging from the conventional perspective of parameter aggregation, we approach it from the perspective of merging experts' outputs. Our key insight is that the merging process can be interpreted as inserting additional matrices into the forward computation, which naturally leads to an optimization formulation. Building on this analysis, we introduce MergeMoE, a method that leverages mathematical optimization to construct the compression matrices. We evaluate MergeMoE on multiple MoE models and show that our algorithm consistently outperforms the baselines with the same compression ratios.", "tldr": "", "keywords": ["Mixture-of-Experts", "Model Compression"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/488802f715d090288e92df38431f3291259492b3.pdf", "supplementary_material": "/attachment/75daaddc5f9fee4b4e65cd41d3dd4ab2d8a26f5f.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the substantial memory overhead issue of Mixture-of-Experts (MoE) models by proposing MergeMoE, an efficient compression method. Unlike conventional expert merging techniques that approach the problem from parameter aggregation perspective, the authors innovatively reframe expert merging from the viewpoint of merging expert outputs, reinterpreting the merging process as inserting additional matrices into forward computation, which naturally leads to a mathematical optimization formulation. Specifically, the algorithm first clusters experts based on the similarity of their weight matrices, then leverages mathematical optimization to construct the merged experts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors provide comprehensive theoretical proofs and mathematical derivations for their proposed merging theory. The method is grounded in solid mathematical foundations.\n- The paper demonstrates relatively clear writing that makes the authors' insights and concepts more accessible and easier to understand.\n- The authors appropriately provide model size after compression and computational cost analysis in the experimental section, which enables better evaluation of the practical utility and efficiency of the proposed compression technique."}, "weaknesses": {"value": "- The paper only evaluates on Qwen and DeepSeek model families, making it unclear whether the conclusions would hold for other MoE model architectures (e.g., GPT-OSS), which raises questions about the generalizability of the proposed method across different model designs and architectures.\n- The paper only evaluates on language tasks, lacking evaluation on important domains such as mathematics and code generation tasks, which limits the assessment of the method's effectiveness across the full spectrum of capabilities that modern MoE models are expected to handle.\n- The paper lacks discussion of current quantization techniques, such as the performance when quantization is applied alongside expert merging, despite quantization being widely used in practice and orthogonal to model compression approaches.\n- MergeMoE's similarity-based clustering approach and frequency-based merging strategy are similar to prior works (e.g., [1,2]), suggesting that the novelty is somewhat incremental.\n\n[1] Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy\n\n[2] Retraining-Free Merging of Sparse MoE via Hierarchical Clustering"}, "questions": {"value": "1. What would be the performance of MergeMoE on other MoE architectures beyond Qwen and DeepSeek families?\n2. What is the performance of the compressed models on other tasks?\n3. What is the performance when MergeMoE is combined with quantization techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bo3605FW0z", "forum": "jfZF7nJnqx", "replyto": "jfZF7nJnqx", "signatures": ["ICLR.cc/2026/Conference/Submission16831/Reviewer_Dpo1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16831/Reviewer_Dpo1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760938169914, "cdate": 1760938169914, "tmdate": 1762926857360, "mdate": 1762926857360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "xoEvZkLPD7", "forum": "jfZF7nJnqx", "replyto": "jfZF7nJnqx", "signatures": ["ICLR.cc/2026/Conference/Submission16831/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16831/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763184392943, "cdate": 1763184392943, "tmdate": 1763184392943, "mdate": 1763184392943, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper revisits expert merging for MoE models from an **output-space** perspective rather than parameter averaging. It formalizes merging as inserting linear maps into the forward pass and proposes a framework with clustering/weighting ((A,B)) and three projection matrices $((T_1,T_2,T_3))$. In practice, the method fixes $(T_2,T_3)$ as **frequency-based merges** within clusters and then computes $(T_1)$ via a closed-form least-squares solution $(T_1 = QP^{\\dagger})$. Experiments on several MoE models report improvements over prior merging baselines at comparable compression ratios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**1. Originality:** The output-merging view is conceptually clear and helps unify and reinterpret prior parameter-space approaches (e.g., showing them as special cases).\n\n**2. Quality:** The framework is well presented; the role of $(A,B,T_1,T_2,T_3)$ in approximating the original forward pass is easy to follow.\nPracticality: Computing $(T_1)$ with a least-squares closed form is simple and efficient, making the procedure easy to reproduce.\n\n**3. Significance:** If broadly applicable, the output-space perspective could influence how MoE compression methods are designed beyond naive weight averaging."}, "weaknesses": {"value": "**1. Core conclusions hinge on a strong assumption:** \n\nAll theory and performance gains rely on **fixing $(T_2,T_3)$ to frequency-merge** (cluster-wise usage-frequency weighting). The “optimality” claims are therefore **conditional**—they hold under this parameterization (and additional assumptions such as independence between router logits and expert outputs), not for more general or learnable $(T_2,T_3)$. This limits generality and reduces the paper’s contribution to “optimal within a restricted family.”\n\n**2. Design space underexplored:**\n\nThere is no systematic comparison against alternative choices for $(T_2,T_3)$ (TIES, Fisher, DARE merge in model merging methods), nor against other weighting schemes beyond frequency.\n\n**3. Operational metrics missing:** \n\nInference-time effects (throughput, latency, memory) after merging are not reported, making practical value harder to assess beyond parameter counts."}, "questions": {"value": "My only major consideration is this: the key results and optimality are conditional on $(T_2,T_3)$ being frequency merges. Demonstrating effectiveness with more general $(T_2,T_3)$—or providing strong evidence that frequency weighting is uniquely preferable—would significantly strengthen the contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NO or VERY MINOR ethics concerns only"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IjsrZyCIVg", "forum": "jfZF7nJnqx", "replyto": "jfZF7nJnqx", "signatures": ["ICLR.cc/2026/Conference/Submission16831/Reviewer_Hg2e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16831/Reviewer_Hg2e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665025942, "cdate": 1761665025942, "tmdate": 1762926856954, "mdate": 1762926856954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MergeMoE, a post-training compression approach for Mixture of Experts (MoE) models. The method identifies redundant experts through cosine similarity of their output activations and merges similar experts using low-rank approximation with singular value decomposition. A small residual MLP is optionally added to compensate for the approximation error. The authors evaluate the method on several public MoE architectures such as OLMoE and DeepSeek-V2-Lite and report moderate compression with limited degradation on general understanding benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The motivation of reducing redundancy among experts is reasonable and supported by observations from over-parameterized MoE models.\n- The proposed merging scheme is simple and easy to integrate into existing systems, which could facilitate efficient deployment without retraining.\n- Experiments are conducted on multiple MoE backbones and datasets, providing a relatively broad empirical view."}, "weaknesses": {"value": "- The core idea of expert merging has been explored in prior literature. Although this work uses singular value decomposition for merging, the contribution beyond existing methods remains limited.\n- The reported results show noticeable performance degradation under stronger compression. The paper would benefit from a clearer analysis of when merging is beneficial and when it becomes detrimental.\n- The evaluation focuses mainly on understanding-oriented classification tasks such as HellaSwag. It remains unclear how the proposed method performs on more generation-intensive or reasoning-based tasks, where MoE architectures may behave differently.\n- The practical inference efficiency gains are not clearly analyzed, making it difficult to assess the real deployment advantages given the observed performance loss."}, "questions": {"value": "- Since the method relies on post-hoc distillation to recover performance, it would be useful to separate the effects of merging and distillation.\n\n- The paper could be improved by including more challenging benchmarks such as code generation, mathematical reasoning, or long-context modeling, which would make the study more relevant to large-scale model evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "juNGFTrlJb", "forum": "jfZF7nJnqx", "replyto": "jfZF7nJnqx", "signatures": ["ICLR.cc/2026/Conference/Submission16831/Reviewer_DVn9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16831/Reviewer_DVn9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770109700, "cdate": 1761770109700, "tmdate": 1762926856667, "mdate": 1762926856667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present MergeMoE, a compression technique for Mixture-of-Experts (MoE) models. The work reframes expert merging from the conventional parameter-aggregation view to an output-merging perspective. This insight formulates compression as a quantitative optimization problem: finding matrices ($A, B, T_1, T_2, T_3$) that minimize the output approximation error. The method uses a decoupled optimization strategy, setting $T_2$ and $T_3$ via weighted averaging and solving for $T_1$ analytically using a least-squares solution on sampled data. The authors also provide theoretical justification (Theorem 1) for using expert usage frequency as the optimal weighting scheme."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The reformulation of expert merging as an output-space optimization problem is a strong theoretical contribution, moving beyond prior heuristic-based parameter-space approaches\n- The paper provides a novel theoretical proof (Theorem 1) justifying the optimality of using usage frequency for weighting, a method previously adopted by M-SMOE based only on empirical performance\n- The proposed method, particularly the use of least-squares to solve for the $T_1$ compression matrix, demonstrates consistently superior performance over baselines (M-SMOE, ZipIt) across multiple MoE architectures and benchmarks"}, "weaknesses": {"value": "- The optimization is decoupled and incomplete. The method defaults to a simple heuristic (weighted averaging) to set $T_2$ and $T_3$, which the paper notes is identical to the M-SMOE baseline's approach. This weakens the claim of a fully optimization-based approach, as the non-linear components are not optimized\n- The main experimental evaluation (Tables 1-3) is weak, as it presents only a single, fixed compression ratio for each model (e.g., 128->64 for Qwen3, 60->30 for Qwen1.5, 64->28 for DeepSeek). This makes it impossible to assess how the methods compare at different, more aggressive compression levels (e.g., 4x, 8x)\n- The ablation study on compression ratio impacts (Figure 2) is insufficient. It is limited to a single model (Qwen 1.5) and a single dataset (WinoGrande), preventing generalizable conclusions about the performance/compression trade-off\n- The method's performance is shown to be highly sensitive to the number of input samples, failing completely (random-guess performance) below a \"critical threshold\". This sensitivity (Figure 4) is a significant practical limitation, and its dependency on sample quality or distribution is not explored\n- The comparison is limited to other expert-merging techniques. The lack of comparison against other established MoE compression methods (e.g., expert pruning based on usage frequency, or quantization) makes it difficult to contextualize the method's practical utility"}, "questions": {"value": "- The optimization of $T_2$ and $T_3$ is avoided. What is the performance impact of learning these matrices, for instance, via gradient descent on the least-squares objective, rather than using the M-SMOE heuristic?\n- Can the authors provide benchmark comparisons (vs. M-SMOE, etc.) at more aggressive compression ratios (e.g., 4x, 8x expert reduction) in Tables 1-3?\n- Given the method's high sensitivity to sample size (Figure 4), how stable is the result based on the distribution of the (e.g.,) 36+ samples? Does a poorly chosen batch of samples lead to catastrophic failure?\n- How does MergeMoE compare to a simple expert-pruning baseline that removes the M-N least-frequently-used experts and fine-tunes the router?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tuv7ytmRYx", "forum": "jfZF7nJnqx", "replyto": "jfZF7nJnqx", "signatures": ["ICLR.cc/2026/Conference/Submission16831/Reviewer_BGKX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16831/Reviewer_BGKX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014768385, "cdate": 1762014768385, "tmdate": 1762926856265, "mdate": 1762926856265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}