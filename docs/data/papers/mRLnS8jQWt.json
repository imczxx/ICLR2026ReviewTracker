{"id": "mRLnS8jQWt", "number": 14566, "cdate": 1758239045701, "mdate": 1759897362003, "content": {"title": "Rethinking Layer Relevance in Large Language Models Beyond Cosine Similarity", "abstract": "Large language models (LLMs) have revolutionized natural language processing. Understanding their internal mechanisms is crucial for developing more interpretable and optimized architectures. Mechanistic interpretability has led to the development of various methods for assessing layer relevance, with cosine similarity being a widely used tool in the field. On this work, we demonstrate that cosine similarity is a poor proxy for the actual performance degradation caused by layer removal. Our theoretical analysis shows that a layer can exhibit an arbitrarily low cosine similarity score while still being crucial to the model's performance. On the other hand, empirical evidence from a range of LLMs confirms that the correlation between cosine similarity and actual performance degradation is often weak or moderate, leading to misleading interpretations of a transformer's internal mechanisms. We propose a more robust metric for assessing layer relevance: the actual drop in model accuracy resulting from the removal of a layer. Even though it is a computationally costly metric, this approach offers a more accurate picture of layer importance, allowing for more informed pruning strategies and lightweight models. Our findings have significant implications for the development of interpretable LLMs and highlight the need to move beyond cosine similarity in assessing layer relevance.", "tldr": "", "keywords": ["LLMs", "Layer Relevance", "Mechanistic Interpretability", "Structured Pruning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b95da9c196031198963fbe5f9e7403b1e93c203c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper critiques cosine similarity as a metric for layer relevance in transformers and proposes an accuracy-based alternative that directly measures performance degradation from layer removal. The authors provide theoretical proof that cosine similarity can arbitrarily misestimate layer importance and demonstrate their accuracy-based metric achieves better results in structured pruning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The theoretical contribution (Theorem 1) formally proves that layers with arbitrarily low cosine similarity can still be critical for performance, providing rigorous justification for questioning this widely-used metric. The proof construction cleverly exploits the \"snowball effect\" where subtle layer modifications are amplified by downstream layers, offering valuable theoretical insight into transformer dynamics.\n  - The empirical evaluation is thorough, covering multiple models (LLaMA3-8B, Mistral-7B, OLMo) and eight diverse benchmarks, with careful ablations comparing iterative versus one-shot pruning and task-dependent versus task-independent settings. The visualizations effectively illustrate the discrepancies between cosine similarity and actual relevance, particularly the striking example of OLMo's layer 16."}, "weaknesses": {"value": "- The computational cost of the proposed accuracy-based metric is a major practical limitation that receives insufficient treatment. While Section 6.1 mentions the method requires N × T forward passes compared to T forward passes for cosine similarity (where N is the number of layers), the paper lacks concrete wall-clock time comparisons, memory requirements, or strategies to make the approach tractable for large-scale models. For a 70B parameter model with 80 layers, computing layer-by-layer relevance becomes prohibitively expensive. The brief mention of \"parallel computation\" and future work on cost reduction does not adequately address whether this metric can realistically replace cosine similarity in practice.\n  - The assumption that accuracy on a calibration dataset is the \"ground truth\" for layer relevance may itself be problematic in ways not fully explored. The paper acknowledges strong sensitivity to calibration data choice (Table 6 shows performance varying from 56.34% to 63.18% depending on which task's training set is used), yet does not investigate: (1) whether small calibration sets produce stable relevance estimates, (2) how relevance rankings change with different calibration sizes, (3) whether the metric captures generalizable layer properties or just task-specific overfitting patterns. The task-independent results (Table 5) where cosine similarity outperforms the accuracy metric (59.69% vs 50.23%) raise questions about which metric truly captures layer importance.\n  - The connection between the theoretical worst-case construction (Theorem 1) and practical transformer behavior remains unclear. The proof constructs a contrived 3-layer model that perfectly overfits a dataset, but the paper does not demonstrate that similar pathological patterns actually occur in real trained transformers. Missing are analyses showing: (1) whether real transformer layers exhibit the \"snowball effect\" mechanism from the proof, (2) what proportion of layers in practice have low cosine similarity but high accuracy-based relevance, or (3) quantitative measures of the correlation between the two metrics across different model families and training stages. The empirical sections jump from the theorem to pruning experiments without bridging this gap.\n  - The evaluation methodology has potential confounds that could inflate the apparent superiority of the accuracy-based metric. In the task-dependent setting (Table 1), the calibration data is the training set of the target task, which may give the accuracy metric an unfair advantage since it directly optimizes for performance on data from the same distribution. A more rigorous evaluation would use separate calibration and evaluation splits from each task. Additionally, the paper does not control for the possibility that the accuracy metric simply overfits to the calibration data distribution. The finding that some layers have negative relevance (green in Figure 1A) — meaning accuracy improves upon their removal — suggests the metric may be capturing dataset-specific artifacts rather than fundamental layer importance. I will reconsider my score in the rebuttal."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HI5H7Vq4ZP", "forum": "mRLnS8jQWt", "replyto": "mRLnS8jQWt", "signatures": ["ICLR.cc/2026/Conference/Submission14566/Reviewer_5sPj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14566/Reviewer_5sPj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760494521908, "cdate": 1760494521908, "tmdate": 1762924956634, "mdate": 1762924956634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses how to assess layer relevance in large language models (LLMs) and challenges the widespread use of cosine similarity between layer inputs and outputs as a proxy for layer importance. Through a theoretical worst-case analysis and broad empirical evidence, the authors show that cosine similarity can fail to capture true layer relevance. They propose an alternative metric based on the actual drop in model accuracy after layer removal and demonstrate its advantages for structured pruning.\nHowever, the practical feasibility of the proposed approach and certain implementation details of the numerical comparisons remain unclear. If these concerns were addressed, the paper would make a meaningful contribution to understanding layer relevance in large models and is likely to stimulate further work in this area."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies a widely used heuristic (cosine similarity) as a proxy for layer relevance, and demonstrates its unreliability. This would be an important contribution given the prevalence of cosine-based analysis in interpretability work. The paper is clearly written and well organized, making the arguments easy to follow."}, "weaknesses": {"value": "W1. **Computational practicality**\n\nThe proposed accuracy-drop metric, while conceptually sound, is computationally intensive, as it requires re-evaluating model performance after removing each layer. However, the paper provides no quantitative assessment of this cost relative to cosine similarity or other baselines. Without such analysis, readers are left uncertain whether the approach is feasible for large-scale pruning or limited to research-scale evaluation.\n\nW2. **Effect of layer removal**\n\nIt is unclear whether the observed accuracy drops truly reflect layer relevance or artifacts of the chosen pruning method. The authors note that \"no healing or post-processing was applied.\" This raises the possibility that the measured performance degradation arises partly from architectural disruption rather than intrinsic irrelevance. Since cosine similarity ignores activation magnitude, a pruning operator without residual rescaling can make cosine-guided deletions appear disproportionately harmful. Therefore, the claim that \"cosine-based pruning drops performance more\" may hold only for some pruning methods and might not generalize to pruning frameworks that include rescaling or post-hoc recovery steps.\n\nIf the authors can address the two main concerns, I would be happy to raise my overall score."}, "questions": {"value": "Q1. Could you quantify the computational cost of the proposed accuracy-drop metric relative to cosine similarity and other baselines?\n\nQ2. Have you tested alternative ablation mechanisms? If not, could differences in ablation strategy explain part of the observed performance drops?\n\nQ3. Relatedly, could the apparent weakness of cosine-guided pruning partly arise because cosine ignores magnitude, and your ablation protocol does not re-normalize downstream layers? In that case, the observed degradation might reflect an uncompensated magnitude mismatch rather than a true failure of cosine similarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SyQMS5KfNP", "forum": "mRLnS8jQWt", "replyto": "mRLnS8jQWt", "signatures": ["ICLR.cc/2026/Conference/Submission14566/Reviewer_a5Py"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14566/Reviewer_a5Py"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665331028, "cdate": 1761665331028, "tmdate": 1762924956145, "mdate": 1762924956145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work discusses the issues of layer removal based on cosine similarity and proposes an alternative method which directly measures the impact of layer removal on the downstream performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Good motivation. Directly measuring the impact of removing layers on accuracy is ideal.\n- Outperforms prior methods.\n- Relatively simple."}, "weaknesses": {"value": "- The main issue I have with the method is that it only measures the impact of removing a single layer. In the scenario where multiple layers are removed, the metric does not identify which layers are dependent to one another. The authors state: \"our method prunes blocks iteratively, re-evaluating the model after each step\". This can result in the removal of a layer which on its own does not result in any drastic drop in performance, but the joint removal of this layer and a subsequent cause drastic drop in performance. In contrast, there could be another layer which if removed first causes a greater drop in performance, but other layers not being as dependent on it, further removals might result in a lesser total performance drop. This is not something that is addressed in the paper, even just in discussion.\n- As the authors mentioned themselves \"when the calibration set is restricted to a single benchmark, performance varies significantly\". Hence this method might not be applicable in scenarios where the downstream application is unknown or very general.\n- The computational cost of different methods are reported as equations, but actual numbers in the tables would help."}, "questions": {"value": "- line 15: \"On this work,\" -> \"In this work,\"\n- Line 294 \"a random baseline in the dataset\" is not the clearest way to express that the model is a random predictor. I assume that this is what the authors meant."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HZ1caPY1Km", "forum": "mRLnS8jQWt", "replyto": "mRLnS8jQWt", "signatures": ["ICLR.cc/2026/Conference/Submission14566/Reviewer_BfoK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14566/Reviewer_BfoK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826924089, "cdate": 1761826924089, "tmdate": 1762924954125, "mdate": 1762924954125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether cosine similarity between a layer's input and output is a reliable proxy for that layer's relevance in large language models. The authors prove a theoretical worst case showing a layer can have arbitrarily low cosine-similarity yet be crucial to performance because small changes it introduces are amplified downstream. They then empirically compare cosine-similarity rankings to a ground-truth relevance obtained by measuring the actual drop in task accuracy after ablating layers. Across multiple models and tasks the correlation is weak to moderate and cosine similarity misranks layers in the vast majority of cases. Motivated by these findings, the paper advocates using an accuracy-based relevance score for mechanistic interpretability and for structured pruning, and shows that pruning guided by this metric outperforms several established baselines in both task-dependent and task-independent settings, albeit at higher computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important and timely problem for both interpretability and model compression: how to evaluate which layers actually matter. The theoretical result is clear and convincing in demonstrating the possibility of pathological cases that invalidate cosine-similarity as a universal proxy. The empirical evaluation is broad and carefully presented, spanning multiple model families, many datasets, and both task-dependent and task-independent pruning regimes. The manuscript also provides actionable methodology: a fully defined accuracy-based relevance score, iterative pruning procedures, and cost accounting that clarifies the trade-offs between fidelity and compute. The analyses of variance across tasks, the examples where layer removal improves performance, and the distillation of insights back into pruning recommendations are all valuable for practitioners who want dependable diagnostics instead of relying on a cheap but misleading proxy."}, "weaknesses": {"value": "While compelling, the work leaves several practical and methodological gaps that, if addressed, would strengthen the contribution. First, the accuracy-based metric is expensive. Please quantify wall-clock runtime and monetary cost for representative experiments and provide more detail on how costs scale with model size, number of layers, and calibration set size. Second, task-independent performance is sensitive to the calibration dataset. Please report more systematic ablations that identify which properties of a calibration set make it effective and whether simple ensemble calibration strategies mitigate sensitivity."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xhC87lDcWQ", "forum": "mRLnS8jQWt", "replyto": "mRLnS8jQWt", "signatures": ["ICLR.cc/2026/Conference/Submission14566/Reviewer_J9R4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14566/Reviewer_J9R4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762109211886, "cdate": 1762109211886, "tmdate": 1762924953567, "mdate": 1762924953567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}