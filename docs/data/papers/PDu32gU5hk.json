{"id": "PDu32gU5hk", "number": 7494, "cdate": 1758024580014, "mdate": 1759897849497, "content": {"title": "CAFÉ: Coverage-Aware Self-Distillation to Mitigate Forgetting in Deep Networks", "abstract": "Deep neural networks rarely exhibit global overfitting in the classical sense, yet they often suffer from a less visible problem - forgetting of previously learned patterns. This phenomenon, which was termed local overfitting, degrades performance in specific regions of the input space even as overall accuracy improves. To address this problem, we propose CAFÉ (Coverage-Aware Forgetting Elimination) - an online, validation-aware, single model method, which mitigates forgetting during training while exploiting self-distillation. CAFÉ identifies and prioritizes checkpoints that uniquely recover forgotten validation samples, dynamically weighting their contributions to form evolving soft labels for each epoch of training. Our experiments show that CAFÉ consistently outperforms both standard training and recent self-distillation SOTA methods under clean and noisy labels, across CIFAR-100 and TinyImageNet, with and without data augmentation. Beyond raw accuracy gains, our results provide quantitative evidence of the substantial impact of forgetting on deep learning performance, and demonstrate that targeted mitigation yields measurable robustness.", "tldr": "CAFÉ is a coverage-aware self-distillation method that tracks validation and dynamically reuses past checkpoints to prevent local overfitting, yielding consistently higher accuracy and robustness under clean and noisy labels", "keywords": ["Overfitting", "Double Descent", "Knowledge Distillation", "Self Distillation", "Label Noise", "Checkpoint Ensembles"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5faf6064b1a1c834d2776fc94e8dc1d710ae03b5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CAFÉ (Coverage-Aware Forgetting\nElimination), a method aimed at reducing local forgetting in deep network training. The approach constructs a teacher distribution by weighting past checkpoints according to their *marginal validation coverage*, thus emphasizing models that uniquely contribute to correctly predicted validation samples. This self-distillation mechanism helps preserve useful past knowledge without ensemble inference or additional model parameters. Experiments on CIFAR-100, CIFAR-100N, and TinyImageNet show consistent gains over strong baselines such as SAT and early stopping, and an efficient variant (Light CAFÉ) maintains similar performance with much lower storage cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed CAFÉ framework is simple, effective, and model-agnostic, requiring no architectural changes or additional inference time cost.\n2. The paper addresses an important problem (i.e., local forgetting during standard supervised training) with clear motivation and empirical evidence."}, "weaknesses": {"value": "1. The method requires storing multiple checkpoints during training (even if Light CAFÉ mitigates this), which could be burdensome for large-scale models.\n2. The experiments are limited to CNN architectures (ResNet, DenseNet). It remains unclear whether the observed local forgetting phenomenon also occurs in Transformer-based models such as ViT or Swin Transformers, which are now dominant in vision tasks. If such forgetting exists, would the proposed CAFÉ still be effective, or would it require adaptation to the Transformer training dynamics?\n3. The analysis of “local forgetting” is primarily demonstrated on CIFAR-100 (and its noisy variant). It is unclear whether the same phenomenon appears on large-scale datasets such as ImageNet, where the data distribution is more diverse and the training is typically longer and more stable. Is local forgetting a general property of deep network training, or is it amplified by small datasets and limited data diversity? Moreover, would CAFÉ still provide benefits when the model already achieves high coverage on large datasets?\n4. The paper lacks a direct comparison to other forgetting mitigation techniques in continual learning outside self-distillation (e.g., rehearsal-based or regularization-based methods)."}, "questions": {"value": "Please see the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NKqFzKjig1", "forum": "PDu32gU5hk", "replyto": "PDu32gU5hk", "signatures": ["ICLR.cc/2026/Conference/Submission7494/Reviewer_A8Am"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7494/Reviewer_A8Am"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750806491, "cdate": 1761750806491, "tmdate": 1762919607357, "mdate": 1762919607357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CAFÉ,  a self-distillation method that mitigates local forgetting in deep neural networks. CAFÉ weights previous training checkpoints based on their marginal validation coverage, a metric that measures how many validation examples a checkpoint gets right that others miss. With respect to other methods that solve the problem by using an ensemble of previous checkpoint, at the end Cafè is able to produce a single model. This coverage-weighted knowledge is distilled into the current model, forming adaptive soft targets that preserve useful information over time. Experiments are conducted on CIFAR-100 and TinyImageNet where the proposed method is compared against self-distillation and ensemble methods, under both clean and noisy labels."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of using previous checkpoints during training is interesting, as it allows the method, unlike other approaches that rely on checkpoint ensembles, to produce a single model at the end of training.\n- The paper also proposes a lightweight version that reduces storage requirements by limiting the number of checkpoints, without compromising performance.\n- As shown in Fig. 2, the method effectively mitigates the problem of local forgetting during training, whereas other methods address it only post hoc"}, "weaknesses": {"value": "- The results are somewhat incremental with respect to KF in Tables 1 and 2, and with respect to PS-KD in Table 3.\n- Style of the paper: The paper lacks clarity in the presentation of results. Table 3 shows several rows missing. Figure 4 was obtained by overlaying results on top of an image taken from another paper (as clearly stated in lines 378–381). The last row of Tables 1 and 2 is misleading, as it initially appears to show improvement over the state of the art, while it actually compares against ERM + early stopping. Line 79 includes the phrase “see review below” without directly referencing the relevant section.\n- Experiments using more recent vision backbones (e.g., ViT) are missing. see questions.\n- Figure 5b is unclear and difficult to interpret. Please clarify the main message in the caption, even if it is already discussed in the main text.\n- The title can be misleading as usually forgetting refers to forgetting of previous knowledge in the Continual Learning setting. I would clarify that."}, "questions": {"value": "- What is the main difference between CAFE and FK? As far as I understand, FK also uses a similar strategy involving previous checkpoints. The main difference (and advantage) of CAFE is that the proposed method does not require an ensemble during inference.\n- Since Table 3 shows that PS-KD achieves results similar to CAFE, it would be useful to highlight the main differences between the two methods.\n- In the KF paper, experiments were also conducted using more recent visual backbones such as ViT. Why are these missing from the proposed work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IQr5xAADl2", "forum": "PDu32gU5hk", "replyto": "PDu32gU5hk", "signatures": ["ICLR.cc/2026/Conference/Submission7494/Reviewer_PEp2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7494/Reviewer_PEp2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843424794, "cdate": 1761843424794, "tmdate": 1762919606945, "mdate": 1762919606945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of local overfitting in deep learning, where previously learned patterns are forgot within training. The proposed method keeps all previous checkpoints and aggregates their predictions to build pseudo labels. Experimental results show the effectiveness of the proposed methods on small image datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Addressing local overfitting in deep learning is an interesting research topic.\n\n+ The proposed method is easily understandable.\n\n+ The performance gain is consistent throughout experiments."}, "weaknesses": {"value": "- No ablation study on the design choices, including hyperparameter tuning. For example, is the schedule for beta in L254 optimal?\n\n- As the proposed method requires to keep all previous checkpoints and runs them to get their outputs, an analysis on the computational cost compared with baseline methods is required.\n\n- In Figure 5, why the marginal coverage of Vanilla peaks in the middle? Following the idea in STEP 2: MARGINAL COVERAGE SWEEP, the best performing model should be chosen at first, which usually appear around the end of training. If not, then it implies that the learning rate schedule is simply suboptimal, and could be better by hyperparameter tuning. In other words, the comparison might not be fair, as the optimization of Vanilla appears to be not properly done.\n\n- Citation format is problematic in some places, e.g., L329.\n\n- Accuracy is not sufficient to catch the degree of forgetting."}, "questions": {"value": "Please address concerns in Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5hSnlVfZZx", "forum": "PDu32gU5hk", "replyto": "PDu32gU5hk", "signatures": ["ICLR.cc/2026/Conference/Submission7494/Reviewer_FxrM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7494/Reviewer_FxrM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932961960, "cdate": 1761932961960, "tmdate": 1762919606541, "mdate": 1762919606541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CAFÉ (Coverage-Aware Forgetting Elimination), an online, validation-aware self-distillation method to mitigate local overfitting and forgetting in deep neural networks. CAFÉ dynamically identifies checkpoints with unique validation coverage and forms soft teacher targets for subsequent training epochs based on their marginal contributions. Extensive experiments demonstrate CAFÉ’s robustness to clean and noisy labels, outperforming standard ERM, prior self-distillation baselines, and specialized ensembles (such as Knowledge Fusion) on CIFAR-100, TinyImageNet, and CIFAR-100N. The work also includes in-depth ablations and theoretical analysis of its claims and complexity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Interesting approach to handle forgetting with self distillation from previous checkpoints based on validation accuracy.\n2. Relatively easy to understand approach.\n3. Good performance improvement on the CIFAR datasets, especially in high noise cases.\n4. Significant improvements shown in Table 3 for clean data."}, "weaknesses": {"value": "1. Reliance on a validation set that is representative of the test set. In cases where the test set may be significantly different, this approach will not be effective.\n2. Very low improvement on TinyImageNet for low (symmetric) or zero noise cases. (Table 2).\n3. Very few compared methods in Table 1 and 2. Reduces the confidence in the overall effectiveness of the approach.\n4. Could not find any forgetting measure or metric.\n5. Since the objective is dealing with the \"forgetting of previously learned patterns.\", wont incremental learning experiments be a better judge of how good the approach is in dealing with forgetting."}, "questions": {"value": "1. Discuss the reliance on a validation set when it is not fully similar to the test set\n2. Why is the improvement very low on TinyImageNet for low (symmetric) or zero noise cases. (Table 2).\n3. Are there no more recent papers that can be compared with in Table 1 and 2?\n4. Could not find any forgetting measure or metric.\n5. Wont incremental learning experiments be a better judge of how good the approach is in dealing with forgetting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Iqi6LckjDv", "forum": "PDu32gU5hk", "replyto": "PDu32gU5hk", "signatures": ["ICLR.cc/2026/Conference/Submission7494/Reviewer_kdK9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7494/Reviewer_kdK9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146593864, "cdate": 1762146593864, "tmdate": 1762919606127, "mdate": 1762919606127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}