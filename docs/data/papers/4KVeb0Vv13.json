{"id": "4KVeb0Vv13", "number": 24791, "cdate": 1758360366286, "mdate": 1759896748068, "content": {"title": "(Token-Level) \\textbf{InfoRMIA}: Stronger Membership Inference and Privacy Assessment for LLMs", "abstract": "Machine learning models are known to leak sensitive information, as they inevitably memorize (parts of) their training data. More alarmingly, large language models (LLMs) are now trained on nearly all available data, which amplifies the magnitude of information leakage and raises serious privacy risks. Hence, it is more crucial than ever to quantify privacy risk before the release of LLMs. The standard method to quantify privacy is via membership inference attacks, where the state-of-the-art approach is the Robust Membership Inference Attack (RMIA). In this paper, we present InfoRMIA, a principled information-theoretic formulation of membership inference. Our method consistently outperforms RMIA across benchmarks while also offering improved computational efficiency.\n\nIn the second part of the paper, we identify the limitations of treating sequence-level membership inference as the gold standard for measuring leakage. We propose a new perspective for studying membership and memorization in LLMs: token-level signals and analyses. We show that a simple token-based InfoRMIA can pinpoint which tokens are memorized within generated outputs, thereby localizing leakage from the sequence level down to individual tokens, while achieving stronger sequence-level inference power on LLMs. This new scope rethinks privacy in LLMs and can lead to more targeted mitigation, such as exact unlearning.", "tldr": "We introduce a new state-of-the-art membership inference attack, InfoRMIA that dominates RMIA on all benchmarks. We also propose a token-level attack framework that has high power and can pinopint info leakage to token levels.", "keywords": ["membership inference attack", "mia", "privacy", "llm privacy", "memorization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/631257434707e4096a5c97b3c52c9c11a8076282.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies data memorization issues of LLMs through membership inference attacks. The paper proposes InfoRMIA, a membership inference attack stronger than the Robust Membership Inference Attack (RMIA). InfoRMIA extends membership inference attacks from the sentence level to the token level, providing a more fine-grained analysis of privacy.  Experimental results suggest that their method achieves new state-of-the-art performance and offers a more detailed perspective on memory leakage in large language models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper proposes studying privacy from a token-level perspective, which offers finer granularity compared to the traditional sequence-level analysis.\n\n2. Experimental results show that  InfoRMIA achieves a new state-of-the-art performance, surpassing prior baseline RMIA."}, "weaknesses": {"value": "1.\tThe theoretical derivations in the paper are difficult to follow and would benefit from clearer explanations and additional intuition. For example, in Section 4.2, it is hard to understand what the authors are trying to emphasize, as the section directly presents a formula without sufficient explanation or intuition.\n\n2.\tThe paper lacks a sufficient explanation of the experimental setup. While Section 5 introduces the analysis tools used, Section 6 directly presents the results and conclusions without clarifying the chosen baselines or model configurations.\n\n3.\tIt would be better to include more experimental results to substantiate the claimed SOTA performance. Although Tables 1 and 2 show that InfoRMIA generally outperforms RMIA, the advantages appear marginal in several cases (e.g., AUC 0.822 vs. 0.821 on the ai4privacy dataset). Moreover, some results in the appendix suggest that InfoRMIA’s performance is not entirely stable. Providing additional experiments and analyses would help strengthen the paper’s main claim."}, "questions": {"value": "1. I may miss some paper details. Could the authors clarify the experimental setup in more detail, including dataset splits, model architectures, and training configurations? How are the auditor tools used?\n\n2. Could the authors include additional experiments to better demonstrate the robustness and consistency of InfoRMIA’s performance?\n\n3. Could the authors discuss cases where InfoRMIA underperforms or performs similarly to RMIA? Will LiRA be considered as one extra baseline?\n\n4. Could the authors provide more intuition behind the theoretical derivations, especially in Section 4.2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ipmci0Wka0", "forum": "4KVeb0Vv13", "replyto": "4KVeb0Vv13", "signatures": ["ICLR.cc/2026/Conference/Submission24791/Reviewer_pY12"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24791/Reviewer_pY12"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761401162746, "cdate": 1761401162746, "tmdate": 1762943199562, "mdate": 1762943199562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InfoRMIA, a novel membership inference attack (MIA) method for large language models (LLMs) that significantly improves upon the original Range Membership Inference Attack (RMIA). The authors propose a theoretically grounded approach based on information theory principles, using a continuous test statistic that eliminates the need for the hyperparameter γ required in RMIA. The paper demonstrates that InfoRMIA provides higher precision in membership scores and reduces computational overhead. Additionally, the authors conduct token-level analysis, revealing that personally identifiable information (PII) such as names of people and artworks is disproportionately more memorized by LLMs compared to other token types. The paper shows that sequence-level privacy metrics like AUC may overestimate privacy risks, as they often reflect memorization of non-private content, making them poor indicators of true privacy risk."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper provides a strong theoretical foundation for InfoRMIA, deriving a continuous test statistic based on information theory principles that is more precise than the discrete statistic used in RMIA.\n\nS2: The token-level analysis is a significant contribution that reveals insights not captured by aggregate metrics like AUC, showing that PII (personally identifiable information) is disproportionately more memorized by LLMs.\n\nS3: The paper effectively demonstrates that sequence-based frameworks overestimate privacy risks by showing that the most memorized sequences often contain no private tokens (Figure 2).\n\nS4: The experimental evaluation is comprehensive, showing InfoRMIA's superiority over RMIA across multiple standard datasets (Purchase100, CIFAR-10, AG News) and models.\n\nS5: The paper clearly explains why AUC alone is a poor indicator of true privacy risk in LLMs, providing a valuable insight for the privacy research community."}, "weaknesses": {"value": "W1: The paper lacks sufficient analysis of InfoRMIA's performance across different LLM architectures and sizes. The experiments are limited to a few standard datasets and models, which may not fully represent the diversity of real-world LLMs.\n\nW2: While the paper claims \"reduced computational complexity,\" it does not provide specific quantitative comparisons of computational costs between InfoRMIA and RMIA, making it difficult to assess the practical significance of this claim.\n\nW3: The token-level analysis is limited to only two datasets (AG News and ai4privacy), which may not be sufficient to generalize the findings about PII memorization patterns across different domains and languages.\n\nW4: The paper does not discuss potential defense mechanisms against InfoRMIA or how the insights from the token-level analysis could inform the development of more privacy-preserving training methods.\n\nW5: The counterintuitive finding that private tokens sometimes show lower membership scores than non-private tokens (in ai4privacy) is not sufficiently explored, leaving an important observation underdeveloped."}, "questions": {"value": "Q1: Could you provide more detailed computational complexity analysis comparing InfoRMIA and RMIA, including execution time and memory usage measurements across different dataset sizes? This would strengthen the claim about reduced computational overhead.\n\nQ2: Would it be possible to expand the token-level analysis to include more diverse datasets across different languages and domains to better generalize the findings about PII memorization patterns?\n\nQ3: Could you investigate the reason behind the observation that private tokens sometimes show lower membership scores than non-private tokens in ai4privacy? This could have important implications for privacy risk assessment.\n\nQ4: How might the insights from your token-level analysis be used to develop more effective privacy-preserving training methods or defenses against membership inference attacks?\n\nQ5: Could you compare InfoRMIA with other state-of-the-art membership inference attacks beyond RMIA to better position the contribution of your work in the broader field?\n\nQ6: The paper mentions \"token-level framework\" but doesn't clearly explain the methodology for token-level analysis. Could you provide more details on how the token-level scores are calculated and how they relate to the sequence-level scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KPdoBkzV7h", "forum": "4KVeb0Vv13", "replyto": "4KVeb0Vv13", "signatures": ["ICLR.cc/2026/Conference/Submission24791/Reviewer_zEAx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24791/Reviewer_zEAx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813519238, "cdate": 1761813519238, "tmdate": 1762943199272, "mdate": 1762943199272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a fine-grained member inference attack, specifically at the token level, called InfoRMIA, which calculates the leakage of individual tokens and aggregates them to the sequence level to achieve stronger inference capabilities.This paper conducts an in-depth analysis of the existing best MIA (RMIA), and then proposes a more principled statistical test method (InfoRMIA), and analyzes its advantages over RMIA This paper proposes a token-level MIA framework to better quantify the memory and information leakage of LLMS with finer granularity and more meaningful analysis"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear theoretical framework construction\n- Clear and concise formula and algorithm expressions"}, "weaknesses": {"value": "- Lack of more effective aggregation methods\n- No method for distinguishing between sensitive and non-sensitive tokens was proposed\n- The truly sensitive information is diluted or concealed"}, "questions": {"value": "The article proposes a new attack method, InfoRMIA, and the author clearly explains the reasons why it is superior to RMIA, reducing the dependence on the size of the dataset Z and improving the accuracy by using Bayesian factors. The author pointed out the shortcomings of the traditional MIA, such as information loss caused by global averaging, and proposed token-level member signal analysis, which is of great practical significance and value. The author pointed out that under the traditional MIA test, tokens with high member scores are more likely to contain less sensitive information, thereby highlighting the limitations and flaws of the AUC metric, that is, AUC to a greater extent reflects the member signals of non-sensitive tokens. These issues are all very valuable and worthy of in-depth consideration. Moreover, the author expounds these issues with clear logic and evidence, which is highly persuasive \n\nHowever, I do have the following comments:\n \n- Lack of more effective aggregation methods\nOn page 2, the author claims that aggregating from token-level metrics to sequence-level metrics would make it less significant for privacy assessment. However, the method used when migrating from the token-level to the sequence-level in the author's proposed approach is also a common aggregation method. Is this somewhat contradictory\n- Lack of a mechanism to distinguish tokens from the opponent's perspective\nOn page 5, the author states that the work of this paper can precisely locate information leakage, and the scenario proposed by the author is that users observe the member scores of each token and check whether these tokens are sensitive. I don't think this is the most appropriate approach, because in member inference attacks, the adversary needs to filter and confirm which tokens are truly sensitive, and this process should be completed automatically, as the amount of data could be extremely large. This requires the formulation of some kind of recognition algorithm to complete the screening of truly sensitive tokens. For tokens that will not disclose privacy, even if their member scores are high, they should still be excluded\n- The truly sensitive information is diluted or concealed(The same as the first point)\nOn page 6, the author mentioned better aggregation methods, but glossed over them due to their limitations and used common aggregation methods such as averaging and min-k. However, before screening out the truly sensitive tokens, simply averaging the tokens in the sequence obviously leads to information loss. The author also mentioned the shortcomings of the averaging method, but did not use a better aggregation method\n•No method for distinguishing between sensitive and non-sensitive tokens was proposed(The same as the second point)\nIn many places in the text, the author points out that the traditional sequence-level MIA has shortcomings because it ignores fine-grained member signals, and the signals of non-sensitive tokens may dilute or mask those of sensitive tokens. On page 7, the author points out the deficiency of the AUC metric, that is, AUC may reflect the signal of non-sensitive tokens to a greater extent. However, throughout the entire article, the author did not provide any effective methods for distinguishing sensitive tokens from non-sensitive ones. They merely claimed that users could check by themselves whether a token was sensitive, but this was clearly not from the perspective of an adversary. Therefore, I think the author should provide an automated algorithm to distinguish between sensitive tokens and non-sensitive tokens, making the article more persuasive\n\nMinor issues：\nDespite pointing out the limitations and flaws of the AUC metric, the results presented on page 8 show that the attack in this paper achieved a higher AUC value, which sounds somewhat strange"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wQE1zt1bQU", "forum": "4KVeb0Vv13", "replyto": "4KVeb0Vv13", "signatures": ["ICLR.cc/2026/Conference/Submission24791/Reviewer_hMRp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24791/Reviewer_hMRp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961507117, "cdate": 1761961507117, "tmdate": 1762943198975, "mdate": 1762943198975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents InfoRMIA an information theory centric method for membership inference. There are two key contributions: a continuous test static which is superior to a discrete statistic that RMIA uses. This improves AUC and TPR, at low FPR. The second contribution is a token level member inference framework. This is distinct from currently used sequence level methods. The results with a token level framework show that sequence level scores are not accurate due to conflation of private and non-private tokens within a sequence.\n\nThe work shows that this method outperforms RMIA and is less sensitive to population size. In addition the token level membership inference results that are stronger than sequence level membership inference. They especially point to sequences which are memorized with no private tokens over estimating the privacy risk, while also pointing to the challenges of using AUC as a privacy metrics"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Better test statistic: Current work reframes RMIA’s setup as a composite hypothesis test, propose a log-likelihood ratio that decomposes into per-sample info gain term and a KL divergence term. This decomposition into memorization and generalization term provides a good interpretability/diagnostic lens,  importantly eliminates gamma, and removes sensitivity of populations set Z. \nThe derivation is sound and statistically rigorous. Computationally efficient and avoids scaling with large population sets.\n\nThe equation turns is smoother, information theory centric relaxation of RMIA equation, there by turns discrete counting into a counting BayesFactor score. This makes sense conceptually and is mathematically sound. \n\n2. Empirically the work shows that this improves AUC and TPR, at low FPR on tabular, image and text datasets. The overall results show that this superior to RMIA.\n\n3. Token level MIA enables a more granular signal and thereby enables more precise decision making related to membership inference. The work also argues and shows sequence level MI conflates private and non private tokens. Thus token level MIA yields a stronger MI."}, "weaknesses": {"value": "1. The KL term assumes normalized distribution over data. However details around estimator or smoothing do not seem fully specified.  This could impact the KL term i.e. how sensitive is KL term to these choices\n\n2. RMIA seems to be the baseline comparison. Are there other MIAs with which the model can be compared to make SOTA claims?\n\n3. Token to sequence aggregation: performance depends on type of aggregation like min-k, mean. While the paper acknowledges this it will be good to show additional aggregators like trimmed mean and also try tuning free calibrations to reduce dependence on min-k"}, "questions": {"value": "1. How is p(z/theta) instantiated over the LLM implementations. Is this treated as an empirical distribution or logits over some token set? Please provide details around normalization, smoothing and detail how is KL term impacted by these?\n\n2. What is the runtime overhead when comparing sequence level methods on longer inputs and large vocabularies?\n\n3. For MIMIR experiments how sensitive are results to an early checkpoint, domain mismatch between reference and target?\n\n4. Would it be possible to include other reference based MIAs to support SOTA claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HadQuG2mqE", "forum": "4KVeb0Vv13", "replyto": "4KVeb0Vv13", "signatures": ["ICLR.cc/2026/Conference/Submission24791/Reviewer_Kseq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24791/Reviewer_Kseq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972256737, "cdate": 1761972256737, "tmdate": 1762943198736, "mdate": 1762943198736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents InfoRMIA, a membership inference attack (MIA) designed specifically for large language models (LLMs), specifically based on the existing RMIA framework. Then the authors argue that sequence-level MIAs are fundamentally flawed for LLMs, as the privacy risk is localized to individual tokens rather than entire sequences. To address this, they propose a token-level privacy assessment framework that can more accurately identify privacy leakage at the token level."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Membership inference attack on large language models is a critical area of research.\n2. The proposed InfoRMIA method is effective."}, "weaknesses": {"value": "1. The organization of the paper could be improved, particularly in providing sufficient context for RMIA and clearly defining the threat model.\n2. The threat model needs further discussion, especially regarding the assumption of having access to a dataset drawn from the same distribution.\n3. The idea of token-level membership inference needs further discussion, particularly regarding the definition of privacy leakage.\n4. Experimental results in Table 1 are hard to interpret."}, "questions": {"value": "1. The organization of the paper could be improved. Specifically, Section 3.1 briefly introduces RMIA, without much detail, and even the problem statement and threat model are not clearly defined. The authors directly jump into the technical details of InfoRMIA without providing sufficient context. RMIA is a relatively new method, and readers may not be familiar with it, even those who are well-versed in membership inference attacks. \n\n2. Another issue linked to the unclear threat model is that it is not clear what the attacker's capabilities are. For example, I checked Appendix A, and saw RMIA requires a dataset that is drawn from the same distribution. Does InfoRMIA have the same requirement? And although this assumption is common in traditional MIAs, it needs further discussion in the context of LLMs. Specifically, for traditional MIAs, the models are usually trained on relatively small and homogeneous datasets, like training images on CIFAR-10. So it's clear what \"the same distribution\" means. However, LLMs are typically trained on massive and diverse datasets, making it less clear what \"the same distribution\" means in this context. For example, if we want to infer whether a medical text is part of the training data, are we supposed to sample other medical texts from the same distribution as the target text? And many sensitive data are quite rare, making it difficult to find similar data points from the same distribution. Therefore, I would like to see a more detailed discussion on this assumption and its implications for the practicality of InfoRMIA.\n\n3. The idea of token-level membership inference needs further discussion. I agree that certain tokens in a sequence may pose greater privacy risks than others. However, the definition of privacy leakage for individual tokens requires further discussion. The privacy risk of a token are also dependent on its context within the sequence. For example, the name of the patient in a medical report may be sensitive, but if it is appears in an author list of a research paper, it may not be sensitive at all. Similarly, a password is highly sensitive if it's surrounded by the corresponding login information, but if it appears in a random sentence, it may not be sensitive. Therefore, claiming sentence-level MIAs are fundamentally flawed may be too strong without further discussion.\n\n4. Experimental results in Table 1 are hard to interpret. I would expect that sampling more data would lead to better attack performance, and it's true for RMIA. However, for InfoRMIA, the attack performance remains identical even to four decimal places. It would be helpful to provide some explanation for this phenomenon."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XRn3o6RVOq", "forum": "4KVeb0Vv13", "replyto": "4KVeb0Vv13", "signatures": ["ICLR.cc/2026/Conference/Submission24791/Reviewer_Vt48"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24791/Reviewer_Vt48"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976135768, "cdate": 1761976135768, "tmdate": 1762943198569, "mdate": 1762943198569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}