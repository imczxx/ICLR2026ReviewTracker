{"id": "epC4DKfP2I", "number": 16044, "cdate": 1758259063569, "mdate": 1763719513800, "content": {"title": "SENTINEL: StagewisE iNtegriTy verification for pIpeliNe parallEL decentralized training", "abstract": "Decentralized training introduces critical security risks when executed across untrusted, geographically distributed nodes. While existing Byzantine-tolerant literature addresses data parallel (DP) training through robust aggregation methods, pipeline parallelism (PP) presents fundamentally distinct challenges. In PP, model layers are distributed across workers where the activations and their gradients flow between stages rather than being aggregated, making traditional DP approaches inapplicable. We propose SENTINEL, a verification mechanism for PP training *without computation duplication*. SENTINEL employs lightweight momentum-based monitoring using exponential moving averages (EMAs) to detect corrupted inter-stage communication. Unlike existing Byzantine-tolerant approaches for DP that aggregate parameter gradients *across replicas*, our approach verifies sequential activation/gradient transmission *between layers*. We provide theoretical convergence guarantees for this new setting that recovers classical convergence rates when relaxed to standard training. Experiments demonstrate successful training of billion-parameter LLMs across untrusted distributed environments with hundreds of workers while maintaining model convergence and performance.", "tldr": "SENTINEL provides a worker verification mechanism for pipeline parallel decentralized training by monitoring inter-stage activations and activation gradients that workers communicate.", "keywords": ["Pipeline Parallelism", "Decentralized Training", "Adversarial Machine Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1313da45a3ecf51951a5e3f96bfb8047ec28355b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses an underexplored yet relevant threat model unique to PP training, with clear formulation and empirical validation (Tabs. 1–4). The proposed SENTINEL method demonstrates strong detection performance and low overhead, supported by theoretical reasoning (Sec. 3; Sec. 3.2 and Appendix). Theoretical analysis regarding convergence under undetected bounded corruption supports the approach (Thm. 1). However, several aspects require clarification or extension: limited empirical baselines against existing PP security methods (No direct evidence found in the manuscript), potentially strong reliance on trusted verifiers (Sec 2.1), and incomplete exploration of complex adaptive or stealthy attacks (adaptive attack limited to single scenario). The paper would benefit from deeper discussion on system overhead, scalability trade-offs, and interactions with DP-level Byzantine defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-motivated identification of a critical security gap in PP decentralized training:\n>> Clear distinction between DP and PP vulnerabilities, including cascading activation corruption (Sec. 2; Fig. 2).\n>> Importance supported by large-scale LLM training requirements (Sec. 1).\n\n- Comprehensive attack taxonomy and evaluation:\n>> Seven activation/gradient attack types, including delay and invisible-noise attacks (Sec. 2.1).\n>> Mixed-attack experiments better match real-world adversaries (Table 2).\n>> Detection performance measured via precision, recall, F1, detection speed, and final validation loss (Tabs. 1–3).\n\n- Theoretical support with assumptions stated explicitly:\n>> Convergence guaranteed to a bounded neighborhood under undetected bounded perturbations (Sec. 3.2; Thm. 1).\n>> Honest majority conditions formally quantified (Lemma 1).\n\n- Large-scale distributed experiments and SWARM deployment:\n>> Demonstrates end-to-end robustness in real decentralized environment (Fig. 4).\n>> Ablation studies explore warm-up duration, collusion, and gradient delay impact (Fig. 3).\n\nClear articulation of limitations and threat boundaries:\n>> Notes vulnerability to other ML attack types, e.g., backdoor, privacy attacks (Conclusion).\n>> No assumption that >50% malicious workers can be tolerated (Lemma 1)."}, "weaknesses": {"value": "- Limited baseline comparison with Byzantine robust methods\n>> Despite claims of incompatibility, no empirical or conceptual comparison with adapted DP defenses (Sec. 1), which would negatively impact the novelty claim and related practical justification.\n\n- Insufficient quantification of false positives and their impact\n>> Precision degradation noted in collusion experiments (Fig. 3b), but consequences such as reduced worker availability are not analyzed.\n>> Validation loss alone may not reveal long-term optimization harm (Tabs. 2–3).\n\n- Partial mathematical clarity and missing definitions\n>> Distance measures in Eq. (2) was referenced in App. D, but no summary or concrete examples in main text.\n>> Threshold adaptation method references App. Alg. 5 without high-level stability discussion (Sec. 3).\n>> Overall, it feels that the lack of clarity weakens interpretability and reproducibility directly from the paper.\n\n- Threat model assumptions not fully explored\n>> Assumes no collusion between malicious workers yet collusion only tested up to 60% among attackers (Fig. 3b), without theoretical support.\n>> First and last stage assumed to be honest (footnote 1), limiting generality for end-to-end secure decentralization.\n>> Broader adversarial coordination strategies remain unexplored."}, "questions": {"value": "In Section 3.2, Theorem 1 states that the convergence neighborhood size is proportional to ( \\tau ). Could the authors provide a more precise relationship (e.g., a constant factor or specific bound) between ( \\tau ) and the convergence error? This would help in tuning ( \\tau ) for desired performance.\n\nIn Section 5.1, when integrating with SWARM, the paper mentions \"32 trainer nodes with verification capability\" but doesn't detail how the EMAs are synchronized across trainers. Could the authors clarify the synchronization mechanism and its impact on detection accuracy?\n\nThe paper uses multiple distance metrics (Appdix) but doesn't clarify how they are combined (e.g., majority vote, weighted average). Could the authors specify the combination strategy and its impact on detection performance? This would improve reproducibility and understanding of the method's robustness.\n\nThe ablation studies in Fig. 3 focus on warm-up steps, collusion, and delay, but do not explore the impact of varying EMA decay rates ((\\beta_{h}) and (\\beta_{g})). Could the authors provide additional experiments varying these hyperparameters to understand their effect on detection accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Nil"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PuaEzMdKZL", "forum": "epC4DKfP2I", "replyto": "epC4DKfP2I", "signatures": ["ICLR.cc/2026/Conference/Submission16044/Reviewer_ux6h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16044/Reviewer_ux6h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728861840, "cdate": 1761728861840, "tmdate": 1762926241129, "mdate": 1762926241129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We would like to thank the reviewers and the area chair for their academic service and feedback on our work. We are glad that the reviewers:\n\n- *found our setting interesting* as we are addressing an underexplored yet important security issue within pipeline parallel-based decentralized training (reviewers gMeg, Byyi, bjpd, and ux6h),\n- appreciate our *lightweight and practical verification* approach (reviewers gMeg and bjpd)\n- value our *comprehensive attack taxonomy* and evaluation (reviewers bjpd and ux6h),\n- appreciate the work that went into *real-world integration of SENTINEL with SWARM* for decentralized training (reviewers gMeg, Byyi, and ux6h)\n- found our paper presentation *clear and well-written* (reviewers gMeg, Byyi, and ux6h)\n\n-----------------------------------------------------------------------\n\n## **Summary of Contributions**\n\nWe want to summarize key contributions of our paper:\n\n- A suite of adversarial threats for training-interruption attacks with **more than 7 attacks**. To the best of our knowledge, the only prior work on pipeline parallel based attacks only considered negation and scale attacks (Lu et al. 2024)\n- **Identifying cascading effect** within pipeline parallel attacks and formalizing this concept\n- A practical, **lightweight verification** solution using EMA and IQR thresholds with adaptive thresholding that adjust itself based on changing training dynamics\n- **Theoretical analysis** of the core method:\n\t1. Convergence analysis in the presence of training-interruption attacks demonstrating that small, undetected adversaries have limited impact on network convergence in the non-convex setting (formal statement: Theorem 3 in Appendix E.1.4)\n\t2. Showing that the above would recover well-known convergence bounds when the malicious actors are removed (Appendix E.1.5)\n\t3. Analyzing the relationship between the honest majority in the initial pool of workers and conditions under which random worker assignment to each stage would result in an honest majority (>50%) (formal statement: Lemma 1 in Appendix E.2)\n-  A comprehensive, step-by-step adaptation guide for **integrating SENTINEL into SWARM** (Appendix G)\n- **Comprehensive evaluation benchmark** involving (Section 5 and Appendices F & G)\n\t- Three different datasets (C4, FineWeb, OpenWebText)\n\t- Two different architectures (Llama-3 and NanoGPT) and adding two more MoEs (Llama-4 and DeepSeek-V3)\n\t- Seven initial activation AND activation gradient attacks + mixed attacks + adaptive EMA attacks\n\t- Ablation study on the impact of \"warm-up\", \"collusion\", \"delay\", \"EMA\", \"distance metrics\", and \"randomness\" (first three in the main paper Section 5 and the second three in Appendix G.3)\n\t- Real-world experiment on a fully functional decentralized training environment using SWARM involving *128 AWS GPU separate instances* showcasing the novel integration of SENTINEL into SWARM\n\n-----------------------------------------------------------------------\n\n## **Revised Version**\n\nFollowing the discussions and reviewers' requests, we have made the following changes to the manuscript:\n\n- Moved our detailed discussion on our distance metrics and how they are combined (in response to reviewer ux6h) to the main paper\n- Added discussions on the worker availability in real-world eco-systems to our set of FAQs in Appendix A (following questions from bjpd and ux6h)\n- Added a clear visualization on the difference between fixed mesh DP+PP vs. stochastic DP+PP (a.k.a. SWARM) to Appendix G.3.1 and discussions around this\n- Added the following additional experiments:\n\t- Ablation study on the sensitivity to $\\beta$ (requested by bjpd and ux6h)\n\t- A large-scale experiment on Llama-3 with 4B parameters (requested by bjpd)\n\t- MoE experiments on two different architectures, Llama-4-0.4B and DeepSeekV3-1B (requested by reviewer bjpd)\n\t- Impact of adding robust aggregation alongside SENTINEL (requested by gMeg and bjpd)\n\n> LEGEND: we use the **color red** for any material (including tables and figures) added to this revision or moved from Appendix to main paper. We also use the **color blue** to highlight the questions that were answered in the original version of the paper.\n\nBelow, we address all reviewers’ comments and concerns. We hope this clarifies any misunderstandings. We remain available and committed to resolving any further points to ensure a thorough evaluation of our work.\n\n### References\n\nLu et al. \"Position: Exploring the robustness of pipeline-parallelism-based decentralized training.\" *ICML*. 2024."}}, "id": "v0gBzpBt8C", "forum": "epC4DKfP2I", "replyto": "epC4DKfP2I", "signatures": ["ICLR.cc/2026/Conference/Submission16044/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16044/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16044/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763717854518, "cdate": 1763717854518, "tmdate": 1763717854518, "mdate": 1763717854518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SENTINEL, a lightweight verification mechanism for ensuring computational integrity in pipeline-parallel (PP) decentralized training, where traditional Byzantine-robust aggregation methods are inapplicable. SENTINEL introduces verifier nodes that monitor inter-stage activations and gradients using Exponential Moving Averages (EMAs) and IQR-based adaptive thresholds to detect anomalies caused by malicious workers. Experiments with Llama-3-0.6B and 1.2B models on decentralized frameworks (e.g., SWARM) show high (>90%) F1 scores in detecting various attack types with minimal overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel threat model: Addresses pipeline-parallel decentralized training security — an underexplored but increasingly relevant setting.\n\n- Lightweight design: Verification via EMAs and statistical tests avoids costly redundancy or gradient aggregation.\n\n- Comprehensive evaluation: Covers numerous attack types (activation, gradient, mixed, and adaptive attacks) across large-scale distributed setups."}, "weaknesses": {"value": "- Trusted verifier assumption: SENTINEL depends critically on verifier nodes being honest and reliable. If a verifier node is compromised, it can both hide attacks and falsely flag benign workers, effectively collapsing the system’s security. The paper does not discuss mechanisms such as rotating verifiers, distributed verification, or cryptographic attestation to mitigate this.\n\n- Incomplete threat model: The approach targets activation/gradient corruption but ignores broader adversarial behaviors such as data poisoning, backdoor insertion, or sybil collusion across multiple stages. These are common in decentralized systems and could bypass SENTINEL entirely.\n\n- Limited evaluation scope: Experiments are conducted on medium-scale Llama-3 models (≤1.2B parameters) in simulated decentralized settings. It remains unclear how SENTINEL scales to truly large (>10B) models, heterogeneous networks, or high-latency cross-institutional environments where EMA synchronization could become costly.\n\n- Parameter sensitivity and calibration cost: The verification relies on several empirically chosen hyperparameters (EMA decay rates, window size, IQR threshold k). These may require manual tuning per dataset and model. There is little analysis of robustness to these choices or automated adaptation beyond the initial “warm-up” period.\n\n- False positives and training stability: While the paper reports high detection rates, it gives limited insight into false positive rates and the resulting training slowdowns or disruptions. Misidentification of honest nodes could degrade throughput or cause partial divergence in long training runs.\n\n- Assumption-heavy theoretical guarantees: The proofs rely on simplified assumptions (e.g., independent random worker assignment, fixed detection thresholds). These are difficult to ensure in real decentralized networks, where collusion or heterogeneous bandwidth can break such guarantees."}, "questions": {"value": "- How would the system behave if one or more verifier nodes are compromised or unavailable?\n\n- Could the verification function be decentralized (e.g., through rotating verifiers or majority voting among neighboring nodes)?\n\n- What is the quantitative computation and communication overhead of SENTINEL relative to redundancy-based baselines?\n\n- How sensitive is performance to hyperparameter tuning (βₕ, βg, k, warm-up length)?\n\n- Can the same framework detect semantic or backdoor-style attacks where activations remain statistically normal but maliciously biased?\n\n- How generalizable is SENTINEL to other architectures (e.g., MoE models or non-transformer networks)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B704oT0TAk", "forum": "epC4DKfP2I", "replyto": "epC4DKfP2I", "signatures": ["ICLR.cc/2026/Conference/Submission16044/Reviewer_bjpd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16044/Reviewer_bjpd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868018701, "cdate": 1761868018701, "tmdate": 1762926240664, "mdate": 1762926240664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed SENTINEL, a verification mechanism for PP training without computation duplication. They provide theoretical convergence guarantees for this new setting that recovers classical convergence rates when relaxed to standard training. Experiments demonstrate successful training of billion-parameter LLMs across untrusted distributed environments with hundreds of workers while maintaining model convergence and performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is claimed as the first comprehensive study of vulnerabilities unique to decentralized training with hybrid data–pipeline parallelism, and introduce a suite of training-interruption attacks that serve as benchmarks for evaluating the security of future systems.\n\n2. The theoretical analysis demonstrates that undetected malicious workers have a negligible impact on the convergence properties. \n\n3. The authors integrate our method with SWARM parallelism to demonstrate its remarkable versatility in real-world decentralized training ecosystems."}, "weaknesses": {"value": "1. The authors claimed that \"the paper considered the first comprehensive exploration of secure and verifiable PP decentralized training\nby identifying\". As for me, In this setting we can see that we need to train billionparameter LLMs through internet-scale communication among distributed nodes.  The paper does not discuss all possible the topology of the inter-connected distributed notes.\n\n2.  Due the issue listed above, the advantage of using decentralized training with hybrid data–pipeline parallelism is largely weaken. And the proposed EMAs is not persuasive. Please explain.\n\n3. The generality of \"Data and Pipeline Parallel Threat Model\" is not justified. I am not sure why it is typical in real practice. \n\n4. Plus, the advantage of combing your scheme with SWARM needs to be discussed in many different scenarios."}, "questions": {"value": "Please see in the weakness parts."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Discrimination / bias / fairness concerns"]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FWkfYnTUqI", "forum": "epC4DKfP2I", "replyto": "epC4DKfP2I", "signatures": ["ICLR.cc/2026/Conference/Submission16044/Reviewer_Byyi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16044/Reviewer_Byyi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957025507, "cdate": 1761957025507, "tmdate": 1762926240296, "mdate": 1762926240296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Decentralized training is an emerging field that makes LLM training more accessible by allowing research groups and volunteers to pool together available compute and, potentially, match the performance of centralized GPU clusters. This paper discusses the problem of making such system Byzantine-tolerant, i.e. robust to the presence of malicious participants that contribute incorrect results (e.g. to disrupt the training run or get the incentive from participation without actually contributing their compute).\n\nWhile prior work addressed this problem in case of using data parallelism, it is crucial that modern LLMs have a large number of parameters and require some form of model parallelism to be trained. This paper addresses Byzantine tolerance in case of pipeline parallelism, which is known to be one of the most practical forms of model parallelism in decentralized setups due to its low bandwidth requirements.\n\nThe paper proposes to use EMA-based metrics to detect anomalies in activations and gradients passes through the pipeline stages. The authors prove that these metrics catch attacks that are large enough to significantly affect the validation loss. They also report experiments showing that this approach withstands multiple simple attacks that might be applied by malicious workers."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Significance.** The paper discusses decentralized training, a promising approach to make LLM training accessible for small research labs, academic and individual researchers that don't have access to massive centralized clusters. The authors address the problem of Byzantine tolerance, which is known to be a major roadblock to adopting decentralized training.\n2. **Originality.** The paper goes beyond most prior work and addresses Byzantine tolerance in case of using pipeline parallelism, which is known to be one of the most practical approaches to model parallelism for decentralized training systems due to its low bandwidth requirements.\n3. **Practical solution.** Unlike prior work, the proposed solution doesn't require to allocate a substantial share of GPU compute to verification. Instead, it suggests to use cheap CPU nodes to detect anomalies in activations and gradients passes through the pipeline stages. The authors propose a straightforward way of integrating their method into existing decentralized training frameworks.\n4. **Clarity.** The paper is well-written and describes the proposed algorithm in a clear way.\n5. **Realistic training setup.** The authors report experiments with various simple attacks on a realistic distributed training setup."}, "weaknesses": {"value": "1. **No results for adversarially designed attacks.** The authors only evaluate common generic attacks (L162-173, L480-481), such as sending constants, random values, or transformations of true activations/gradients. They don't evaluate adversarial attacks specifically designed to bypass the proposed method (e.g. by sending random data mimicking the tracked EMA-based metrics). It is difficult to infer bounds on their validation loss impact from the provided theoretical derivations.\n2. **No results for medium-strength attacks.** Figure 1 features only strong attacks (F1 score > 0.8) that get caught and weak attacks (F1 score < 0.2) that don't impact training, with only one datapoint in between. This suggests that medium-strength attacks (F1 score ≈ 0.5) might still slip through and significantly impact the validation loss.\n3. **Too strong, less realistic assumptions.** The paper assumes that malicious workers don't collude with each other (L161) and only perturb activations/gradients while sending them through pipeline stages, not during gradient aggregation (L321). It also assumes a small enough number of malicious nodes so that majority of workers holding each pipeline stage are honest with a high probabilty (L285).\n4. **Accounting for gradient aggregation attacks.** While the authors claim that protecting from gradient aggregation attacks is a \"complimentary axis\" (L321), they don't discuss how to combine their method with protecting from such attacks, and how this affects the method's assumptions (e.g. for the number of malicious workers)."}, "questions": {"value": "1. How does the proposed method withstand specially designed adversarial attacks, e.g. if attackers send random data mimicking the tracked EMA-based metrics or use a small MLP instead of the proper pipeline stage to save compute?\n2. What is the effect of medium-strength attacks (e.g. with F1 scores 0.3, 0.4, 0.5, 0.6) on the validation loss?\n3. Given the effect of weak and medium-strength attacks, does decentralized training still make sense? (e.g. if we get the validation loss of a smaller model with 10x training compute, the participants might rather choose to train the smaller model locally)\n4. How can we combine the proposed method with methods to protect from gradient aggregation attacks? How would this impact the maximum number of malicious workers the system can withstand?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CmS3fWjURv", "forum": "epC4DKfP2I", "replyto": "epC4DKfP2I", "signatures": ["ICLR.cc/2026/Conference/Submission16044/Reviewer_gMeg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16044/Reviewer_gMeg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980920554, "cdate": 1761980920554, "tmdate": 1762926239719, "mdate": 1762926239719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}