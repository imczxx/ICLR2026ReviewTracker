{"id": "0zIcPe4CtY", "number": 14362, "cdate": 1758233649380, "mdate": 1763438230216, "content": {"title": "Sublinear Time Quantum Algorithm for Attention Approximation", "abstract": "Given the query, key and value matrices $Q, K, V\\in \\mathbb{R}^{n\\times d}$, the attention matrix is defined as $\\mathrm{Att}(Q, K, V)=D^{-1}AV$ where $A=\\exp(QK^\\top/\\sqrt{d})$ with $\\exp(\\cdot)$ applied entrywise, $D=\\mathrm{diag}(A{\\bf 1}_n)$. The attention matrix is the backbone of modern transformers and large language models, but explicitly forming the softmax matrix $D^{-1}A$ incurs $\\Omega(n^2)$, motivating numerous approximation schemes that reduce runtime to $\\widetilde O(nd)$ via sparsity or low-rank factorization.\n\nWe propose a quantum data structure that approximates any row of $\\mathrm{Att}(Q, K, V)$ using only row queries to $Q, K, V$. Our algorithm preprocesses these matrices in\n$\\widetilde{O}\\left( \\epsilon^{-1} n^{0.5} \\left( s_\\lambda^{2.5} + s_\\lambda^{1.5} d + \\alpha^{0.5} d \\right) \\right)$\ntime, where $\\epsilon$ is the target accuracy, $s_\\lambda$ is the $\\lambda$-statistical dimension of the exponential kernel defined by $Q$ and $K$, and $\\alpha$ measures the row distortion of $V$ that is at most $d/{\\rm srank}(V)$, the stable rank of $V$. Each row query can be answered in\n$\\widetilde{O}(s_\\lambda^2 + s_\\lambda d)$\ntime.\n\nTo our knowledge, this is the first quantum data structure that approximates rows of the attention matrix in sublinear time with respect to $n$. Our approach relies on a quantum Nystr{\\\"o}m approximation of the exponential kernel, quantum multivariate mean estimation for computing $D$, and quantum leverage score sampling for the multiplication with $V$.", "tldr": "We develop the first quantum algorithm to approximate the attention matrix and output any row of the attention matrix in sublinear in context length time.", "keywords": ["quantum computing", "attention approximation", "numerical linear algebra"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10b2149101d3fe6584e6d8b3148fc1208203cd20.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies quantum algorithms (in the QRAM model) to approximate attention in transformers. Given query-key-value matrices $Q,K,V$ of sizes $n\\times d,d\\times n,$ and $n\\times d$, respectively, the goal is to approximate the attention matrix $A=softmax(QK^\\top)V$. This is achieved by constructing a data structure that, given any index $i\\in[n]$, it returns $\\widetilde r_i\\in\\mathbb{R}^{d}$, which approximates the  $i$-th row of $A$. The total complexity to construct the data structure  depends on $n,d$ and other parameters such as the accuracy, statistical dimension, and row-distortion, and Frobenius norms (I expand further below). If the additional parameters besides $n,d$ are treated as constants, then the proposed approach provides quadratic speed-up with respect to the large dimension $n$, against known algorithms for the problem at hand. Importantly, the complexity guarantees are also accompanied with approximation bounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper is well-written and concise, which makes it enjoyable to read. \n2) It targets one of the currently most popular computational problems in deep learning, the \"quadratic curse\" of attention.\n3) It provides a novel approach to the problem, combining (non-trivially) techniques from random Linear Algebra and quantum computing.\n4) It achieves a complexity that scales as $\\sqrt{n}$, with respect to the large dimension $n$. This is impressive, even if it only holds for certain parameter regimes. To my knowledge, classical algorithms require $\\Omega(n^2)$ time to achieve sharp element-wise approximations (ref [2] below), or $\\Omega(n)$ for less strict approximations. \n5) The related work discussion is thorough.\n6) The mathematical analysis is very rigorous. I did not read all the details in the proofs, but I was not able to \"break\" any of them, they seem correct, and well-written.\n\n### References\n- [1] Demmel, James, Ioana Dumitriu, and Olga Holtz. \"Fast linear algebra is stable.\" Numerische Mathematik 108.1 (2007): 59-91.\n- [2] Alman, Josh, and Zhao Song. \"Fast attention requires bounded entries.\" Advances in Neural Information Processing Systems 36 (2023): 63117-63135."}, "weaknesses": {"value": "I have only two \"major concerns\" to raise at this stage. Below in \"Questions\" I provide specific questions that would help me understand the details better and clarify these points.\n1) **Classical model of computation**: From what I understand, there are subroutines in the main algorithm that rely on classical computations. For example, line 274 assumes a subroutine to compute the pseudoinverse in $O(n^\\omega)$. How? From what I know, finite precision algorithms can only return approximate solutions (see e.g. ref [1] below). Things might be easier in \"exact arithmetic\", but I am not sure that infinite-precision is compatible with QRAM.\n2) **Approximation/complexity trade-off**: I am a bit sceptical about the complexity / approximation trade-off. In Theorem 3.1, line 362, there is a $\\sqrt{n}$ term, and two more \"hidden\" in the Frobenius norms. The former can be absorbed by setting e.g. $\\lambda=1/\\sqrt{n}$, but it is a bit unclear how this affects complexity. Now, if we were to upper bound the $||\\cdot||_F$-norms with $||\\cdot||_2$-norms, which is commonly the desired type of bound, they would intruduce another $\\sqrt{n}$ factor. This factor would have to be absorbed inside $\\epsilon$, e.g., by setting $\\epsilon'=\\epsilon/\\sqrt{n}$. But they this would introduce an additional $\\sqrt{n}$ factor in the complexity of line 365, and therefore it would no longer be sublinear in $n$.\n\n**Minor concerns:**\nI have the following two minor comments (but they did not influence my recommendation).\n\n- The QRAM model is mostly of theoretical interest (at least at the time of this writing). This might be a limitation for practical implementations in the future. \n- The authors recognize in line 377 that the reported approximation guarantee is for a \"symmetrized version\" rather than the classic norm-wise approximation. I think this is fine, I am more concerned with the Frobenius-versus-spectral norm topic, as I mentioned above. But it certainly speaks in favor of the authors that they explicitly mention this topic."}, "questions": {"value": "My current assessment is slightly leaning towards reject, due to the two main concerns that I raised above. At this stage it is not clear to me if the final, end-to-end complexity achieves the reported sublinear time, or, if it does, what are the corresponding parameter regimes. My recommendation is not final. I will take into consideration the authors responses as well as the comments from the other reviewers.\nHere I mention some questions that I would like answered to help me clarify my understanding of the paper and provide the additional evidence for my final assessment.\n### Questions\n1) Regarding concern 1): What is the \"classical\" model of computation followed here? Is it \"compatible\" with QRAM? Could you provide references/discussion on the precise complexity / approximation guarantees of the assumed classical subroutines?\n2) Regarding concern 2): Could you provide a small paragraph discussing further how the choice of the different parameters affects the total complexity? If someone wants spectral-norm bounds, how can they be achieved?\n3) Can we replace QRAM with something simpler (e.g., QROM)? Which parts are currently the \"bottleneck\"? \n4) Are there any quantum / classical lower bounds for Frobenius-norm type of approximations, e.g., in similar spirit to [2] below. I do not expect the authors to prove lower bounds at this stage, but a relevant discussion would be helpful.\n5) If we were to use the proposed algorithms to  approximate the entire attention matrix, what would be the complexity and how does it compare with existing attention algorithms? I think that the $\\Omega(n^2)$ lower bounds of [2] leave quite some room for improvements. E.g., if the final complexity of the proposed algorithm is $O(n^{1.5})$ to achieve the same (or similar) bound as [2], then this would already be a nice improvement, and would significantly strengthen the presentation. Could you provide some insights? \n6) Could the authors comment on how to choose the $\\lambda$ parameter?\n\n### Additional Feedback\nHere I provide additional feedback with the aim to improve the paper. These points are here to help, and not necessarily part of the decision assessment.\n\n- The main result, Theorem 3.1, is in page 7. It would be nice to either move it earlier, e.g., in the introduction, or at least a more explicit statement of the main result in the introduction.\n- A table with the main result compared to existing algorithms could be helpful. E.g., to compare complexity, approximation guarantees (if any), the model of computation, or other properties that the authors consider important (again, this is not a request for the rebuttal, just potentially nice-to-have)\n- Some paragraphs are a bit long, e.g., the first paragraph of Section 3.3. \n- Using colored references can be helpful, and I think it is allowed by ICLR template.\n- In line 206, $O(s^2)\\cdot \\mathcal{T}_K+s^\\omega$ should be $O(s^2\\cdot \\mathcal{T}_K+s^\\omega)$. The hidden constants in fast matrix product are quite large. There might be other places in the paper where this applies.\n- When mentioning algorithms/subroutines with $O(n^\\omega)$  complexity, a reference or proof should be given. I know that some theory papers tend to take them for granted, but often they are highly non-trivial to prove, or even to find the corresponding bibliography.\n- Between lines 368-369: The sentence \"...achieving a quadratic speedup over any classical algorithm\" should probably be \"...achieving a quadratic speedup with respect to $n$ over any classical algorithm that we know of\". \n- In line 372, it is mentioned \"when $a=o(n)$\". Based on Definition C.2 in the Appendix, I think $a(V)$ is always at most $d$. Take the SVD of $V=U\\Sigma W^\\top$, and let $v_i$ be the $i$-th row of $V$. It holds that \n$||v_i||_{2}^{2}=||e_i^{\\top}U\\Sigma W^{\\top}||_2^2\\leq  ||e_i^{\\top} U||_2^2||\\Sigma W^\\top||_2^2=\\tau_i||V||_2^2.$ Replacing this in Definition C.2 gives \n$\\frac{d}{||V||_F^2}\\cdot \\max_i\\frac{||v_i||_2^2}{\\tau_i} \\leq d$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LB4mcYJhXY", "forum": "0zIcPe4CtY", "replyto": "0zIcPe4CtY", "signatures": ["ICLR.cc/2026/Conference/Submission14362/Reviewer_zcdq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14362/Reviewer_zcdq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760975099397, "cdate": 1760975099397, "tmdate": 1762924783813, "mdate": 1762924783813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response"}, "comment": {"value": "We thank all the reviewers for your helpful and constructive comments and we have significantly improved the results obtained in this work. We summarize them below, and they have been revised accordingly in the manuscript (marked in red).\n\n* Approximation guarantee without symmetrization: in the original draft, we obtain an approximation in terms of the symmetrization of the $A:=\\exp(QK^\\top)$ matrix in the following form: $\\\\|\\widetilde D^{-1} (\\widetilde A+\\widetilde A^\\top)/2 \\widetilde V - D^{-1} (A+A^\\top)/2 V\\\\|_F$, in the revision, we successfully obtain the approximation guarantee in terms of the true attention: $\\\\|\\widetilde D^{-1} \\widetilde A \\widetilde V - {\\rm Att}(Q, K, V)\\\\|_F$ where ${\\rm Att}(Q, K, V)=D^{-1}AV$. Our main observation is that to obtain the desired guarantee, it is enough to bound the spectral and Frobenius norm error $\\\\|A-\\widetilde A\\\\|$ and $\\\\|A-\\widetilde A\\\\|_F$ while in the original version, we have to rely on a much stronger spectral approximation guarantee $A+A^\\top\\preceq \\widetilde A+\\widetilde A^\\top \\preceq A+A^\\top+2\\lambda I$. Our new guarantee is much more natural and lets one directly interpret the quality with respect to the attention matrix.\n\n* Relaxed condition on the matrix $D^{-1}$: in the original draft, in order to obtain a meaningful approximation guarantee, we require $\\\\|D^{-1}\\\\|\\leq \\frac{1}{\\epsilon \\\\|E\\\\|+\\lambda\\sqrt{n}}$ where $E$ is the exponential kernel over the dataset $Q\\cup K$. In the revision, we improve our analysis and relax the requirement to $\\\\|D^{-1}\\\\|\\leq \\frac{1}{\\epsilon \\\\|A\\\\|+\\lambda \\sqrt{n}}$, this requirement is much weaker as $A$ is the off-diagonal block of $E$ and typically has a much smaller spectral norm than $E$. To realize this new bound, we developed a tailored analysis of the quantum matrix-vector product approximation algorithm that replaces the dependence on $\\\\|E\\\\|$ by $\\\\|A\\\\|$.\n\n* Refined upper bound on the row distortion $\\alpha$: in the original draft, we didn’t provide an upper bound on $\\alpha$, which appears in our runtime bound. This naturally raises the question whether $\\alpha$ can be in the order of $n$ and hence diminishes the quadratic advantage we obtain. Thanks to Reviewer zcdq who points out that $\\alpha$ admits an upper bound of $d$, we refine the argument and prove that $\\alpha(V)\\leq \\frac{d}{{\\rm srank}(V)}$ where ${\\rm srank}(V)=\\frac{\\\\|V\\\\|_F^2}{\\\\|V\\\\|^2}$ is the stable rank of $V$. This means that for the setting we are interested in where the sequence length is much larger than the key-value dimension, $\\alpha$ is strictly sublinear.  \n\n* Empirical verification on the assumptions of parameters: while our algorithm requires QRAM and thus is hard to empirically verify its efficiency, we instead check several assumptions on the parameters. These include (1) the assumption that $\\\\|D^{-1}\\\\|\\leq \\frac{1}{\\epsilon \\\\|A\\\\|+\\lambda \\sqrt{n}}$, we see that this condition is satisfied in most cases for any $\\epsilon\\leq 0.18$, (2) Frobenius norm vs spectral norm, specifically, we check the Frobenius and spectral norms of $A$ and $V$. For $A$, we empirically show that the Frobenius norm is typically within a factor of 2 of the spectral norm, while for $V$, it does have the $\\sqrt{d}$ gap predicted by the theory. We note that to cancel out the $\\sqrt{d}$ factor, we can scale down $\\epsilon$ by a factor of $\\sqrt{d}$, causes a $\\sqrt{d}$ runtime blowup, maintaining the sublinear behavior, (3) the upper bound on the row distortion factor $\\alpha(V)$, $d/{\\rm srank}(V)$, which we observe is a small constant less than 3, (4) infinity norm vs spectral norm for $A$, which we again observe that they are only a constant factor ($<3$) off rather than $\\sqrt{n}$. Note that the $\\lambda \\sqrt{n}$ factor in the approximation guarantee largely comes from converting the spectral norm bound to infinity norm bound. In experiments, we show these two quantities are very close for $A$ and do not exhibit a $\\sqrt{n}$ gap. These experiments are performed on the pretrained OLMo2 1B and 7B models and evaluated on the pretraining datasets. These models have sequence length $n=4096$ and $d=128$. We aggregate the statistics over all heads and across all layers, and report their means. We record the results in the following table. For more details, see Appendix E in the revised manuscript.\n\n| Metric                                                        |  OLMo2-1B |  OLMo2-7B |\n|:-------------------------------------------------------------|----------:|----------:|\n| Largest $\\epsilon$ s.t. $\\|D^{-1}\\|\\leq \\frac{1}{\\epsilon \\|A\\|}$ |   0.1708  |   0.1685  |\n| $\\frac{\\|A\\|_F}{\\|A\\|}$                                     |   1.3769  |   1.3586  |\n| $\\frac{\\|V\\|_F}{\\|V\\|}$                                     |  11.3137  |  11.3137  |\n| $\\frac{d}{{\\rm srank}(V)}$                                  |   1.7126  |   2.1345  |\n| $\\frac{\\|A\\|_{\\infty}}{\\|A\\|}$                              |   2.7439  |   2.7439  |"}}, "id": "9nojt7rLeJ", "forum": "0zIcPe4CtY", "replyto": "0zIcPe4CtY", "signatures": ["ICLR.cc/2026/Conference/Submission14362/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14362/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14362/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763182629023, "cdate": 1763182629023, "tmdate": 1763183135270, "mdate": 1763183135270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies quantum algorithms (in the QRAM model) to approximate attention in transformers. Given query-key-value matrices $Q,K,V$ of sizes $n\\times d,d\\times n,$ and $n\\times d$, respectively, the goal is to approximate the attention matrix $A=softmax(QK^\\top)V$. This is achieved by constructing a data structure that, given any index $i\\in[n]$, it returns $\\widetilde r_i\\in\\mathbb{R}^{d}$, which approximates the  $i$-th row of $A$. The total complexity to construct the data structure  depends on $n,d$ and other parameters such as the accuracy, statistical dimension, and row-distortion, and Frobenius norms (I expand further below). If the additional parameters besides $n,d$ are treated as constants, then the proposed approach provides quadratic speed-up with respect to the large dimension $n$, against known algorithms for the problem at hand. Importantly, the complexity guarantees are also accompanied with approximation bounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper is well-written and concise, which makes it enjoyable to read. \n2) It targets one of the currently most popular computational problems in deep learning, the \"quadratic curse\" of attention.\n3) It provides a novel approach to the problem, combining (non-trivially) techniques from random Linear Algebra and quantum computing.\n4) It achieves a complexity that scales as $\\sqrt{n}$, with respect to the large dimension $n$. This is impressive, even if it only holds for certain parameter regimes. To my knowledge, classical algorithms require $\\Omega(n^2)$ time to achieve sharp element-wise approximations (ref [2] below), or $\\Omega(n)$ for less strict approximations. \n5) The related work discussion is thorough.\n6) The mathematical analysis is very rigorous. I did not read all the details in the proofs, but I was not able to \"break\" any of them, they seem correct, and well-written.\n\n### References\n- [1] Demmel, James, Ioana Dumitriu, and Olga Holtz. \"Fast linear algebra is stable.\" Numerische Mathematik 108.1 (2007): 59-91.\n- [2] Alman, Josh, and Zhao Song. \"Fast attention requires bounded entries.\" Advances in Neural Information Processing Systems 36 (2023): 63117-63135."}, "weaknesses": {"value": "I have only two \"major concerns\" to raise at this stage. Below in \"Questions\" I provide specific questions that would help me understand the details better and clarify these points.\n1) **Classical model of computation**: From what I understand, there are subroutines in the main algorithm that rely on classical computations. For example, line 274 assumes a subroutine to compute the pseudoinverse in $O(n^\\omega)$. How? From what I know, finite precision algorithms can only return approximate solutions (see e.g. ref [1] below). Things might be easier in \"exact arithmetic\", but I am not sure that infinite-precision is compatible with QRAM.\n2) **Approximation/complexity trade-off**: I am a bit sceptical about the complexity / approximation trade-off. In Theorem 3.1, line 362, there is a $\\sqrt{n}$ term, and two more \"hidden\" in the Frobenius norms. The former can be absorbed by setting e.g. $\\lambda=1/\\sqrt{n}$, but it is a bit unclear how this affects complexity. Now, if we were to upper bound the $||\\cdot||_F$-norms with $||\\cdot||_2$-norms, which is commonly the desired type of bound, they would intruduce another $\\sqrt{n}$ factor. This factor would have to be absorbed inside $\\epsilon$, e.g., by setting $\\epsilon'=\\epsilon/\\sqrt{n}$. But they this would introduce an additional $\\sqrt{n}$ factor in the complexity of line 365, and therefore it would no longer be sublinear in $n$.\n\n**Minor concerns:**\nI have the following two minor comments (but they did not influence my recommendation).\n\n- The QRAM model is mostly of theoretical interest (at least at the time of this writing). This might be a limitation for practical implementations in the future. \n- The authors recognize in line 377 that the reported approximation guarantee is for a \"symmetrized version\" rather than the classic norm-wise approximation. I think this is fine, I am more concerned with the Frobenius-versus-spectral norm topic, as I mentioned above. But it certainly speaks in favor of the authors that they explicitly mention this topic."}, "questions": {"value": "My current assessment is slightly leaning towards reject, due to the two main concerns that I raised above. At this stage it is not clear to me if the final, end-to-end complexity achieves the reported sublinear time, or, if it does, what are the corresponding parameter regimes. My recommendation is not final. I will take into consideration the authors responses as well as the comments from the other reviewers.\nHere I mention some questions that I would like answered to help me clarify my understanding of the paper and provide the additional evidence for my final assessment.\n### Questions\n1) Regarding concern 1): What is the \"classical\" model of computation followed here? Is it \"compatible\" with QRAM? Could you provide references/discussion on the precise complexity / approximation guarantees of the assumed classical subroutines?\n2) Regarding concern 2): Could you provide a small paragraph discussing further how the choice of the different parameters affects the total complexity? If someone wants spectral-norm bounds, how can they be achieved?\n3) Can we replace QRAM with something simpler (e.g., QROM)? Which parts are currently the \"bottleneck\"? \n4) Are there any quantum / classical lower bounds for Frobenius-norm type of approximations, e.g., in similar spirit to [2] below. I do not expect the authors to prove lower bounds at this stage, but a relevant discussion would be helpful.\n5) If we were to use the proposed algorithms to  approximate the entire attention matrix, what would be the complexity and how does it compare with existing attention algorithms? I think that the $\\Omega(n^2)$ lower bounds of [2] leave quite some room for improvements. E.g., if the final complexity of the proposed algorithm is $O(n^{1.5})$ to achieve the same (or similar) bound as [2], then this would already be a nice improvement, and would significantly strengthen the presentation. Could you provide some insights? \n6) Could the authors comment on how to choose the $\\lambda$ parameter?\n\n### Additional Feedback\nHere I provide additional feedback with the aim to improve the paper. These points are here to help, and not necessarily part of the decision assessment.\n\n- The main result, Theorem 3.1, is in page 7. It would be nice to either move it earlier, e.g., in the introduction, or at least a more explicit statement of the main result in the introduction.\n- A table with the main result compared to existing algorithms could be helpful. E.g., to compare complexity, approximation guarantees (if any), the model of computation, or other properties that the authors consider important (again, this is not a request for the rebuttal, just potentially nice-to-have)\n- Some paragraphs are a bit long, e.g., the first paragraph of Section 3.3. \n- Using colored references can be helpful, and I think it is allowed by ICLR template.\n- In line 206, $O(s^2)\\cdot \\mathcal{T}_K+s^\\omega$ should be $O(s^2\\cdot \\mathcal{T}_K+s^\\omega)$. The hidden constants in fast matrix product are quite large. There might be other places in the paper where this applies.\n- When mentioning algorithms/subroutines with $O(n^\\omega)$  complexity, a reference or proof should be given. I know that some theory papers tend to take them for granted, but often they are highly non-trivial to prove, or even to find the corresponding bibliography.\n- Between lines 368-369: The sentence \"...achieving a quadratic speedup over any classical algorithm\" should probably be \"...achieving a quadratic speedup with respect to $n$ over any classical algorithm that we know of\". \n- In line 372, it is mentioned \"when $a=o(n)$\". Based on Definition C.2 in the Appendix, I think $a(V)$ is always at most $d$. Take the SVD of $V=U\\Sigma W^\\top$, and let $v_i$ be the $i$-th row of $V$. It holds that \n$||v_i||_{2}^{2}=||e_i^{\\top}U\\Sigma W^{\\top}||_2^2\\leq  ||e_i^{\\top} U||_2^2||\\Sigma W^\\top||_2^2=\\tau_i||V||_2^2.$ Replacing this in Definition C.2 gives \n$\\frac{d}{||V||_F^2}\\cdot \\max_i\\frac{||v_i||_2^2}{\\tau_i} \\leq d$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LB4mcYJhXY", "forum": "0zIcPe4CtY", "replyto": "0zIcPe4CtY", "signatures": ["ICLR.cc/2026/Conference/Submission14362/Reviewer_zcdq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14362/Reviewer_zcdq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760975099397, "cdate": 1760975099397, "tmdate": 1763555321484, "mdate": 1763555321484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a quantum data structure for approximating rows of the attention matrix in sublinear time with respect to the sequence length n. The method combines quantum Nyström approximation, multivariate mean estimation, and leverage score sampling to approximate the components of the attention mechanism. This is the first quantum algorithm to achieve sublinear dependence on n in the row-query model without structural assumptions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work is the first to achieve sublinear-in-n row queries for attention approximation using quantum methods.\n2. The approach makes no  structural assumptions making it widely applicable."}, "weaknesses": {"value": "1. Parameter dependence: The runtime depends on s  and α , which may be large in practice, limiting practical speedups.\n2. Norm of D−1 assumption: The guarantee requires ∥D−1∥<(ϵ∥E∥+λn)−1, which may not hold in all settings."}, "questions": {"value": "1. Can you give numberical experiments to show the time cost, errors and the assumptions on parameters.\n2. How does the statistical dimension s behave in practice for typical transformer inputs, and does it remain small enough to yield meaningful speedups?\n3. Is the row distortion parameter α bounded in real-world value matrices, and are there cases where it becomes large enough to negate the sublinear advantage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7PhIPzUwmO", "forum": "0zIcPe4CtY", "replyto": "0zIcPe4CtY", "signatures": ["ICLR.cc/2026/Conference/Submission14362/Reviewer_eeim"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14362/Reviewer_eeim"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761054853435, "cdate": 1761054853435, "tmdate": 1762924783405, "mdate": 1762924783405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal of the paper is to study approximation algorithms for self-attention computation in the transformer architecture. The inputs to self-attention are $Q,K,V \\in \\mathbb{R}^{n\\times d}$ and the goal is to output $Att(Q,K,V) = D^{-1}A V$ where $A= exp(QK^T/\\sqrt{d})$ and $D^{-1} = diag(A\\mathbb{1})$. Past works for provable attention approximation need at least $\\Omega(nd)$ time, which is the input and output size, and the paper focuses on quantum algorithms that can achieve a better runtime. If one insists on outputting the entire $Att(Q,K,V)$ matrix, $\\Omega(nd)$ time is inevitable, however this can be avoided by formulating the problem as a data structure problem. In particular the goal is preprocess $Q,K,V$ into a data structure that then allows, for any index $i\\in [n]$, the return an approximation to the $i^{th}$ row of $Att(Q,K,V)$. Even then since each row of $Att(Q,K,V)$ is a convex combination of rows of $V$, achieving sublinear in $n$ time is hard. \n\nTheir main contribution is a quantum data structure that access the input matrices only using row queries, performs preprocessing in $\\widetilde{O}(\\epsilon^{-1} n^{0.5} poly(d,s_{\\lambda},\\alpha)$ time, and answers output row queries in time $\\widetilde{O}(s_{\\lambda}^2 + s_{\\lambda}d)$ (here $s_{\\lambda}$ is the statistical dimension of $exp(QK^T/\\sqrt{d})$. Their approach uses techniques such as Grover search, Quantum Nystrom approximation, and Quantum multivariate mean estimation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of the paper is to present a sublinear time algorithm that answers row queries for attention approximation in the quantum model. The techniques are very interesting and conceptually simple."}, "weaknesses": {"value": "Perhaps one minor weakness is that there are few previous works on attention approximation that achieve spectral norm approximation guarantees and it would be to prove such a guarantee here as well."}, "questions": {"value": "The first question is that the authors make a statement that achieving sublinear in $n$ dependence for the row query model seems intractable for classical algorithms since each row of the output is a convex combination of rows of $V$. Is there a formal claim to show this ? If yes then since there are past works on attention approximation that make structural assumptions on the input matrices, is it possible to prove classical sublinear in $n$ guarantees under plausible assumptions ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "70IbOKd9R9", "forum": "0zIcPe4CtY", "replyto": "0zIcPe4CtY", "signatures": ["ICLR.cc/2026/Conference/Submission14362/Reviewer_ojWR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14362/Reviewer_ojWR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761492722248, "cdate": 1761492722248, "tmdate": 1762924782958, "mdate": 1762924782958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a quantum data structure for approximating the Transformer attention mechanism under the row query model, where only individual rows of the attention output are queried. The key contribution is a theoretical framework that achieves sublinear preprocessing time complexity of $\\tilde{O}(\\epsilon^{-1} n^{0.5}(s_\\lambda^{2.5} + s_\\lambda^{1.5} d + \\alpha^{0.5} d))$, providing a quadratic speedup over the best known classical algorithms.\nThe method embeds the non-symmetric attention matrix $A = \\exp(QK^\\top / \\sqrt{d})$ into a larger symmetric exponential kernel matrix over the combined dataset $(Q, K)$, and applies a combination of quantum Nyström approximation, quantum multivariate mean estimation, and quantum leverage score sampling to approximate the attention normalization factor, kernel matrix, and value multiplication components, respectively.\nThe resulting data structure allows approximating any attention row in time $\\tilde{O}(s_\\lambda^2 + s_\\lambda d)$, without assumptions on $Q, K, V$. This is, to the authors’ knowledge, the first quantum algorithm achieving sublinear dependence on sequence length $n$ for attention approximation. The authors also provide theoretical guarantees in Frobenius norm for the symmetrized attention matrix $(A + A^\\top)/2$, along with detailed parameter dependence on the kernel’s statistical dimension $s_\\lambda$ and value distortion factor $\\alpha$."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: High Originality and Theoretical Significance: This work, to the best of my knowledge, is the first to propose a sublinear-time quantum algorithm for approximating the standard Transformer attention mechanism in the row-query setting. Achieving a preprocessing complexity of $\\tilde{O}(n^{0.5})$, the method provides a potential quadratic speedup over classical algorithms. This represents a meaningful theoretical advance and offers a new perspective on overcoming the quadratic bottleneck in large-scale attention computation.\n\nS2: Sophisticated Theoretical Framework: The paper demonstrates strong technical depth by systematically combining several advanced quantum tools—Nyström kernel approximation, multivariate mean estimation, and leverage score sampling—into a coherent data structure for attention approximation. The approach of embedding the non-symmetric attention matrix into a symmetric exponential kernel over the joint query–key space is both elegant and conceptually novel. Moreover, the framework is general, requiring no structural assumptions on $Q$, $K$, or $V$, which enhances its theoretical robustness and potential applicability."}, "weaknesses": {"value": "W1: Lack of Empirical Validation: The paper is entirely theoretical and does not provide any numerical simulation or small-scale experiment to illustrate the potential practical impact of the proposed method. While this is acceptable for a theoretical contribution, even a simple empirical demonstration (e.g., simulated quantum runtime scaling or synthetic kernel approximation) would help substantiate the claimed sublinear advantages.\n\nW2: Symmetrization Limitation: Because the algorithm approximates the attention matrix through a symmetric kernel on the combined $(Q, K)$ dataset, it effectively provides guarantees only for the symmetrized form $(A + A^\\top)/2$. This design choice limits its direct interpretability as an approximation to the true attention matrix, and it remains unclear whether the same speedup can be achieved without this symmetrization."}, "questions": {"value": "Q1:Regarding Empirical Validation (W1):\nCould the authors provide any empirical or simulated evidence to illustrate the practical implications of the proposed algorithm? For example, could a small-scale classical simulation or synthetic experiment demonstrate the expected sublinear scaling behavior or approximation quality?\n\nQ2:Regarding Symmetrization Limitation (W2):\nThe current framework provides guarantees only for the symmetrized attention matrix $(A + A^\\top)/2$. Do the authors believe the same quantum speedup could be achieved without this symmetrization? If not, could they elaborate on the fundamental technical barriers that make direct approximation of the asymmetric attention matrix more challenging?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FDcq4OC0zm", "forum": "0zIcPe4CtY", "replyto": "0zIcPe4CtY", "signatures": ["ICLR.cc/2026/Conference/Submission14362/Reviewer_ppDN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14362/Reviewer_ppDN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724966579, "cdate": 1761724966579, "tmdate": 1762924782246, "mdate": 1762924782246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}