{"id": "57N2jCINEP", "number": 5440, "cdate": 1757909949374, "mdate": 1759897975070, "content": {"title": "CoDe: Semantic Color Reasoning and High-Fidelity Detail Synthesis for Underwater Image Enhancement", "abstract": "Underwater images often suffer from severe color distortion and texture degradation due to light absorption and scattering, posing huge challenges for visual perception and restoration. Recent diffusion-based underwater image enhancement (UIE) methods have shown remarkable performance, but most rely on customized architectures trained from scratch or lack auxiliary guidance beyond image-level inputs, which limit the model generalization and controllability. In this work, we propose a semantic Color reasoning and high-fidelity Detail synthesis UIE framework (CoDe), which fully leverages the synergy of diffusion models and vision-language models. It explicitly disentangles color and texture of underwater images: a fine-tuned LLaVA provides domain-invariant semantic color cues for robust color correction, while an SDXL-based generator restores high-frequency details for sharp reconstruction. Furthermore, we design an adaptive degradation-aware feature modulation module that fuses underwater and clean-domain representations, effectively suppressing noise interference during the denoising diffusion process. Extensive experiments on multiple underwater benchmarks demonstrate that CoDe achieves superior performance, significantly improving both color fidelity and texture preservation.", "tldr": "", "keywords": ["Diffusion model", "Underwater image enhancement", "VLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a34790a1458b50fee231e9e0fa3f2a06a957b9b9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a multi-modal decoupling framework for underwater image enhancement that disentangles color and texture restoration. The approach fine-tunes LLaVA to extract semantic color descriptions from degraded underwater images, uses these descriptions to guide SDXL for texture recovery, and introduces an ADFM module to fuse underwater features with diffusion latents. The method achieves SOTA results on multiple UIE benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies that color correction and texture restoration are fundamentally different tasks in UIE, with color distortions requiring semantic-level reasoning while texture degradation needs fine-grained pixel-level modeling.\n\n2. The pipeline elegantly connects color semantic extraction (fine-tuned LLaVA) → text-guided generation (SDXL) → adaptive feature fusion (ADFM), where each component addresses a specific aspect of the problem.\n\n3. The evaluation spans both reference-based (PSNR/SSIM) and no-reference metrics (UIQM/UCIQE) across four underwater benchmarks, with consistent improvements and extensive ablations."}, "weaknesses": {"value": "1. The method uses GPT-4 to generate color captions as supervision for LLaVA fine-tuning, but at inference time LLaVA must estimate these captions without access to clean references, creating a train-test mismatch. The paper lacks quantitative analysis of caption quality degradation and provides no evidence that fine-tuned LLaVA truly learns domain-invariant semantics rather than memorizing GPT-4's specific language patterns.\n\n2. While the ADFM module design is structured, the paper provides weak justification for why this specific combination effectively suppresses noise artifacts during iterative denoising. Critically, the gating mechanism (Eq. 3) lacks explicit constraints to guarantee adaptive reduction of underwater feature influence as denoising progresses.\n\n3. The gradual transition from clean captions to LLaVA captions during training (Section 3.4) lacks crucial implementation details—what is the transition schedule? How is the ratio configured?—yet this curriculum learning strategy is presented as essential without ablation or justification compared to direct LLaVA caption training.\n\n4. This paper does not provide data on inference time, GPU memory consumption, or speed comparisons against CNN/GAN baselines. This makes it impossible to assess practical feasibility for time-critical applications like underwater robotics.\n\n5. On UIEB, CoDe's PSNR gain over CLIP-UIE is only +0.22 dB (Table 1), and qualitative differences in Figure 3 are subtle."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PN6its36Zh", "forum": "57N2jCINEP", "replyto": "57N2jCINEP", "signatures": ["ICLR.cc/2026/Conference/Submission5440/Reviewer_PeZr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5440/Reviewer_PeZr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761449332253, "cdate": 1761449332253, "tmdate": 1762918063697, "mdate": 1762918063697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CoDe, a multi-modal framework for underwater image enhancement (UIE), which leverages a vision-language model (LLaVA) to extract semantic color cues and a diffusion model (SDXL) for high-fidelity texture recovery. An Adaptive Degradation-aware Feature Modulation (ADFM) module is introduced to fuse underwater and clean features during denoising, effectively suppressing noise propagation. The authors carried on the experimental demonstration in the reference datasets and the non reference datasets, and has a great performance in the color fidelity and texture recovery."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tNovel decoupling of color and texture restoration: LLaVA is employed for semantic color reasoning and SDXL for texture synthesis, which is a innovative multi-modal strategy.\n\n2.\tWell-designed modulation module: ADFM adaptively fuses features without directly injecting degraded information, enhancing generation quality.\n\n3.   Comprehensive experiments: The method is thoroughly evaluated on multiple datasets, showing consistent improvements in both full-reference and no-reference metrics."}, "weaknesses": {"value": "1.  Effectiveness of color deviation captions: the method of LLaVA training relies on paired underwater clean images to generate color difference captions, which limits the generalization to unpaired scenes. Meanwhile, whether the color deviation captions can be quantified from the RGB three channels of image features, rather than using the GPT-4 model. The captions provided by GPT-4 model contains the color information and object information of the image, but the underwater image does not contain only one object. When there are too many objects, such as multiple fish and multiple objects, these situations are not fully discussed in the paper.\n\n2.  Slow inference speed: Real time is necessary for underwater exploration, monitoring and other applications. Despite using LoRA for efficient fine-tuning, the SDXL-based diffusion model requires 20 sampling steps and 40000 iterations, and the two-stage model training hinders real-time applications.\n\n3.  Sensitivity to LLaVA-generated captions: Inaccurate color descriptions from LLaVA may misguide the diffusion process and degrade enhancement quality. There is no quantitative discussion on the generated text description, which is relatively an ill-posed problem"}, "questions": {"value": "1.\tThe authors provided “the proposed framework can be readily extended to other crossdomain enhancement tasks such as low-light image enhancement and dehazing”, but without any experiments? How generalizable is the framework?\n2.\tThe direct use of GPT-4 as the source of ground truth lacks justification. Other models like GPT-5, GPT-4 Plus, Deepseek, Gemini and so on.\n3.\tIn the ADFM module, were other feature fusion mechanisms (e.g., cross-attention) explored? Why was the current combination (Channel Gating + Spatial Attention + FiLM) chosen? It remains unclear whether other feature fusion mechanisms (e.g., latest attention) were explored. The justification for the current combination of Channel Gating, Spatial Attention, and FiLM appears insufficient without ablations.\n4.\tThe visual comparisons provided in the paper primarily showcase enhancements of single, isolated subjects. The lack of results on complex scenes (e.g., scenes with multiple coral species, fish schools, or intricate backgrounds) raises concerns about the model's robustness and generalization capability in more realistic and challenging underwater environments.\n5.\tAs reflected in the loss function and the weaknesses mentioned earlier, the method still relies on paired images, making it difficult to demonstrate whether the disentanglement of color and texture actually contributes to the enhancement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b2zvp3P6JY", "forum": "57N2jCINEP", "replyto": "57N2jCINEP", "signatures": ["ICLR.cc/2026/Conference/Submission5440/Reviewer_TEkn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5440/Reviewer_TEkn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820646554, "cdate": 1761820646554, "tmdate": 1762918063435, "mdate": 1762918063435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a multimodal underwater image enhancement approach by leveraging a fine-tuned LLaVA model for textual color description extraction and a diffusion-based generator for high-frequency and texture enhancement. The proposed method achieves high-quality image restoration on multiple benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a multimodal approach for underwater image enhancement, which utilizes a language model to extract textual information from images as supervisory guidance for image restoration."}, "weaknesses": {"value": "The paper lacks a detailed introduction to the text extraction module (Figure 2). Relying solely on numerical experiments is insufficient to convince readers of the reliability and generalizability of the text color representations extracted by the LLaVA model."}, "questions": {"value": "1. The paper lacks mathematical modeling of the textual feature representation. There is no verification of whether the textual information effectively describes the color characteristics of underwater images, nor how such validity should be evaluated.\n\n2. What is the inference time of the diffusion probabilistic model-based texture enhancement method? The paper should provide comparative analysis with other approaches to demonstrate its computational efficiency advantages.\n\n3. Underwater images lack ground truth; the so-called \"ground truth\" in Figure 3 cannot actually be used to evaluate the algorithm's effectiveness. From a visual perspective, the advantages of the proposed method are not significant.\n\n4. The paper fails to provide essential explanations regarding \"lightweight alignment\": its specific meaning, implementation methodology, and evaluation criteria. This omission hinders comprehensive understanding and validation of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bfbN9Ckp6U", "forum": "57N2jCINEP", "replyto": "57N2jCINEP", "signatures": ["ICLR.cc/2026/Conference/Submission5440/Reviewer_b4Ai"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5440/Reviewer_b4Ai"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890155543, "cdate": 1761890155543, "tmdate": 1762918063137, "mdate": 1762918063137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed CoDe (Semantic Color Reasoning and High-Fidelity Detail Synthesis), an underwater image enhancement framework that decouples color correction and detail restoration to address color distortion and texture degradation. CoDe fine-tunes the LLaVA vision–language model to generate semantic color captions, guiding domain-invariant color correction, while the SDXL diffusion model restores high-frequency textures. An Adaptive Degradation-aware Feature Modulation (ADFM) module fuses underwater and latent features through attention and FiLM modulation to suppress artifacts, and LoRA enables efficient fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduced a novel color–texture decoupled underwater image enhancement framework that combined diffusion models with vision–language models, representing a creative and effective approach to handling complex underwater degradations.\n2. By fine-tuning LLaVA to generate color-related textual descriptions, the method performed semantic color correction rather than relying solely on pixel-level cues, enabling domain-invariant and context-aware enhancement.\n3. The integration of SDXL for texture synthesis ensured realistic and sharp detail restoration, outperforming traditional CNN- and GAN-based methods in perceptual quality.\n4. The proposed ADFM module effectively fused underwater and latent diffusion features, using attention and FiLM modulation to suppress degradation artifacts and stabilize the denoising process."}, "weaknesses": {"value": "1. The enhancement relied on LLaVA-generated color captions, which might be inaccurate or inconsistent under severe degradation, and the paper lacked evaluation of their reliability.\n2. The study focused on visual quality but did not test whether or not enhancement improved downstream tasks, such as detection or segmentation, limiting its practical relevance.\n3. Although LoRA reduced the training cost, SDXL inference remained computationally heavy and slow, restricting real-time or on-device deployment.\n4. The network architecture figure lacked sufficient detail and were overly generalized; some key modules (e.g., FiLM and ResFuse) were not illustrated, making it difficult to understand the implementation process."}, "questions": {"value": "1. It remains unclear how the fine-tuned LLaVA specifically focuses on color semantics while avoiding entanglement with texture or structural cues during caption generation. The paper should clarify what training constraints or supervision ensure this selective color reasoning.\n2. The study only employed LLaVA for color caption generation without comparing it to other vision–language or large language models (e.g., BLIP-2, or GPT-4V). Such comparisons would better validate whether LLaVA is indeed the most suitable choice for semantic color extraction.\n3. After introducing the color captions into the diffusion model, the paper did not provide attention maps or feature visualizations to illustrate how textual color guidance interacts with image features. Visual evidence would help confirm that the semantic cues effectively influence color correction and texture synthesis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PN389LshOn", "forum": "57N2jCINEP", "replyto": "57N2jCINEP", "signatures": ["ICLR.cc/2026/Conference/Submission5440/Reviewer_GiWM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5440/Reviewer_GiWM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927205765, "cdate": 1761927205765, "tmdate": 1762918062861, "mdate": 1762918062861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}