{"id": "WzLjwv8KAn", "number": 24027, "cdate": 1758351918808, "mdate": 1762943117044, "content": {"title": "Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs", "abstract": "Large language models (LLMs) have unlocked a wide range of downstream generative applications. \nHowever, we found that they also risk perpetuating subtle fairness issues tied to culture, positioning their generations from the perspectives of the mainstream US culture while demonstrating salient externality towards non-mainstream ones.\nIn this work, we identify and systematically investigate this novel **culture positioning bias**, in which an LLM’s default generative stance aligns with a mainstream view and treats other cultures as \"outsiders\".\nWe propose the ***CultureLens*** benchmark with 4,000 generation prompts and 3 evaluation metrics for quantifying this bias through the lens of a *culturally situated interview script generation* task, in which an LLM is positioned as an on-site reporter interviewing local people across 10 diverse cultures. \nEmpirical evaluation on 5 state-of-the-art LLMs reveals a stark pattern: while models adopt insider tones in over 88\\% US-contexted scripts on average, they disproportionately adopt mainly outsider stances for less dominant cultures.\nTo resolve these biases, we propose *2 inference-time mitigation methods*: a baseline prompt-based **Fairness Intervention Pillars (FIP)** method, and a structured **Mitigation via Fairness Agents (MFA)** framework consisting of 2 pipelines:\n(1) **MFA-SA (Single-Agent)** introduces a self-reflection and rewriting loop based on fairness guidelines.\n(2) **MFA-MA (Multi-Agent)** structures the process into a hierarchy of specialized agents: a Planner Agent(initial script generation), a Critique Agent (evaluates initial script against fairness pillars), and a Refinement Agent (incorporates feedback to produce a polished, unbiased script).\nEmpirical results demonstrate that agent-based MFA methods achieve outstanding and robust performance in mitigating the culture positioning bias: \nFor instance, on the CAG metric, *MFA-SA reduces bias in Llama model by 89.70 \\% and MFA-MA mitigates bias in Qwen by 82.55\\%*.\nThese findings showcase the effectiveness of agent-based methods as a promising direction for mitigating biases in generative LLMs.", "tldr": "", "keywords": ["Bias", "Culture", "LLM", "Generation", "Agent"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/49b0992d0cfd44b80f70f8ee3f5e43809fa6bef4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper analyzes culture bias for large language models (LLMs), focusing on an insider vs outsider stance when generating content about different cultures. The authors propose benchmarks with new metrics (CEP, CPD, CAG) to quantify this bias and present qualitative and quantitative analyses showing clear insider vs outsider asymmetries across multiple models. They further propose two prompt-based mitigation frameworks (FIP and MFA) to reduce such biases.\n\nSince culture bias in LLMs have been explored by several prior papers, the main contribution of this paper is the insider vs outsider framing and the associated metrics. Although it's great that the authors also proposed mitigation methods in addition to bias detection, the analysis is only done on the proposed metrics and hard to make comparison with prior work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novel framing via the insider vs outsider idea:\nThe insider vs outsider distinction provides a clear and intuitive way to think about cross-cultural asymmetry. Even though conceptually simple, this framing could help future work reason about whose voice a model takes when discussing cultural contexts. Compared to detecting differences on same given small dataset, this approach theoretically has deeper implications to downstream tasks and model understanding in general. \n\nStrong execution and coverage: \nThe study evaluates multiple models across ten cultures, offering a broad snapshot of how cultural positioning manifests. The qualitative examples and lexical analyses are accessible and help ground abstract claims in concrete evidence.\n\nReasonable dataset and metric design: \nThe proposed benchmark and derived metrics (CEP, CPD, CAG) provide a structured way to quantify the insider vs outsider phenomenon. All of the ideas introduced are smart and well-designed despite not super technically innovative. \n\nBias mitigation in addition to bias detection: \nIn addition to introducing a challenge, the authors also made an attempt to solve the problem using the two prompt-based mitigation frameworks (FIP and MFA) and the results show decent improvements compared to the baseline."}, "weaknesses": {"value": "Too many different ideas and contributions but not enough depth: \nThe paper touches on three distinct areas: bias detection, metric design, and bias mitigation. As a result, the work reads as three partial contributions rather than one cohesive advance. The cultural bias framing, metric proposal, and agent-based mitigation could each justify a separate study, but none are developed deeply enough to stand alone at ICLR level in terms of standard of innovation or analytical rigor.\n\nLimited novelty: \nCultural bias and Western centrism in LLMs have been widely studied. The insider vs outsider framing adds rhetorical clarity but not a fundamentally new conceptual or analytical dimension. Prior work has already characterized similar problems and it's not obvious how the proposed framing compares or improves upon prior findings. Since the insider vs outsider is perhaps the most important contribution, deeper analysis would greatly help. For example, how it affects downstream applications, what we can learn from this, how likely is this going to transfer to existing harmful cases etc. \n\nWeak technical depth and comparisons in mitigation: \nThe mitigation section (FIP and MFA) is underdeveloped. Both are high-level prompting or agentic reformulations evaluated only against the authors’ own metrics, with no comparison to established debiasing or alignment baselines. The methods lack algorithmic substance and do not yield generalizable insights about how to mitigate cultural bias beyond prompt engineering."}, "questions": {"value": "Core contribution on insider vs outsider framing: \nHow do you see the insider vs outsider framing as conceptually distinct from prior discussions of cultural bias and Western centrism in LLMs? Can you articulate what new understanding this framing provides that was not already captured by “cultural alignment” or “representational disparity” studies?\nDid you conduct human evaluations to assess whether these quantitative scores correlate with human judgments of insider vs outsider stance?\nWhat drives the insider vs outsider asymmetry observed? Is it primarily data imbalance, instruction tuning bias, or cultural salience in the training corpus?\nHave you analyzed whether the same patterns hold for multilingual or region-specific models trained outside Western datasets?\n\nOn bias mitigation: \nThe mitigation strategies (FIP, MFA) are tested only on your benchmark. How do they perform on existing cultural or social bias benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z2wDEviiDv", "forum": "WzLjwv8KAn", "replyto": "WzLjwv8KAn", "signatures": ["ICLR.cc/2026/Conference/Submission24027/Reviewer_waHg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24027/Reviewer_waHg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760931419531, "cdate": 1760931419531, "tmdate": 1762942903257, "mdate": 1762942903257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "u32YtVNXtg", "forum": "WzLjwv8KAn", "replyto": "WzLjwv8KAn", "signatures": ["ICLR.cc/2026/Conference/Submission24027/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24027/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762943116193, "cdate": 1762943116193, "tmdate": 1762943116193, "mdate": 1762943116193, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors identify the problem of culture positioning bias, in which LLMs default to adopting an insider lens for certain cultures, but an outsider lens for other, often not as well-resourced, cultures. They introduce an interview script generation task to evaluate this bias across different LLMs. They find that all LLMs are biased toward an American cultural lens, but that certain prompt-based mitigations can reduce this bias on the interview task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies an important direction in cultural alignment that has been relatively neglected. While lots of work has analyzed LLM default behavior in MCQ settings, as well as biases related to cultural steering in open-ended settings, they consider default behavior in an open-ended setting through the lens of culture positioning bias.\n- Extensive analyses reveal that culture positioning bias is an issue in language models, and effective prompt-based mitigations are identified to address this for the task in question."}, "weaknesses": {"value": "- The paper focuses on a very narrow set of interview script generation tasks, which are an uncommon use case for LLMs - due to the narrow task focus, it’s unclear whether the results shown would generalize to more realistic real-world tasks in ways that would perpetuate the representational or allocational harms discussed.\n- The analysis of the qualitative results in Section 4.3.2 don’t seem to be well-grounded in past work on stereotype mitigation. In particular, the rationale behind the color-coded labels in tables 2-3 is not explicitly given, and labels seem to be ad-hoc (e.g. it doesn’t seem problematic for “soviet” and “orthodox” to be associated with Russia, and “Punjab” is a region in Pakistan, so it’s unclear why it’s highlighted but American states are not).\n- The proposed mitigations, such as FIP, seem somewhat ungrounded as well - the FIP prompt given to the model is zero-shot GPT-4o output."}, "questions": {"value": "- To what extent are the results explained by the United States being the only country in the list of 10 where English is the most commonly used language? The Rystrøm 2025 work cited uses language as a cultural control - one hypothesis for the effect seen in this work is that LLMs form associations between the language used and the insider/outsider status of a country. For example, if we prompted in Urdu and the model switches to insider status when generating Pakistani transcripts, this might suggest that the effects seen are related to model inference of insider/outsider status based on the prompt given, rather than unawareness of task-specific norms.\n- What happens if you LLMs the ability to critique/reflect without any task-specific guidelines? It would be useful to know if the multi-agent gains are attributable to task-specific knowledge, or just the ability to reflect in general."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cyxpqqWrUF", "forum": "WzLjwv8KAn", "replyto": "WzLjwv8KAn", "signatures": ["ICLR.cc/2026/Conference/Submission24027/Reviewer_G1gt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24027/Reviewer_G1gt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761423097890, "cdate": 1761423097890, "tmdate": 1762942902813, "mdate": 1762942902813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how large language models reflect cultural bias by asking, \"through which cultural lens do these models see the world?\" Focusing on interview script generation, the authors present CULTURELENS, a benchmark of 4,000 prompts spanning ten culturally diverse contexts. It assesses whether LLMs take an insider or outsider stance when producing culturally grounded content. Three quantitative metrics: Cultural Externality Percentage, Cultural Perspective Deviation, and Cultural Alignment Gap, are used to measure bias systematically. Experiments with several leading LLMs show a clear US-centric tilt, with non-dominant cultures like Papua New Guinea often framed from an outsider view. To mitigate this, the paper introduces Fairness Intervention Pillars, a targeted strategy leveraging both single-agent and multi-agent setups to meaningfully narrow cultural positioning disparities."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **S1:** The paper introduces CULTURELENS, a well-designed benchmark addressing cultural positioning bias in depth for the first time.\n- **S2:** It uses three clear and interpretable metrics (CEP, CPD, CAG) that systematically measure cultural bias.\n- **S3:** The Fairness Intervention Pillar (FIP) offers a practical, effective way to reduce bias, making the work actionable.\n- **S4:** The Mitigation via Fairness Agents (MFA) framework is well-structured, with two pipelines: MFA-SA (Single-Agent) and MFA-MA (Multi-Agent)."}, "weaknesses": {"value": "- **W1**: Only five models were tested, mostly small ones (7B), and major families like Gemini, Gemma, or Claude were missing. Also, using smaller models likely skews results, making the findings less representative of actual model capabilities, as larger models perform better most times. Overall, in this sense, some findings of the current evaluation results can be misleading and inappropriate in general; and the findings do not provide a clear picture in terms of evaluation.\n- **W2:** The evaluation focuses only on the interviewer’s (LLM-generated) questions and ignores the interviewee responses, which limits depth and misses key aspects of cultural reasoning. As cultural understanding is a very complex topic, in this case, this method doesn’t seem reliable (as shown in agreement scores) and it questions the credibility of the whole study.\n- **W3:** Why did the evaluation focus solely on the interviewer’s (LLM-generated) questions rather than also assessing interviewee responses, as it seems that judging from questions which do not have enough content to judge this reliably (even human agreements are low)? Without analyzing the interviewee role, the results feel surface-level and don’t truly assess cultural understanding. Including the interviewee role may provide a deeper or more accurate view of the models’ cultural reasoning abilities.\n- **W4:** The inter-annotator agreement (κ = 0.60) and human–AI agreement (κ = 0.53) indicate only moderate consistency, raising legitimate concerns about the reliability and clarity of human judgments. As noted in the literature [1], Fleiss’ kappa values in this range suggest moderate agreement on a scale from –1 to 1, where 1 represents perfect consensus and 0 corresponds to chance-level agreement. Given this, I find the use of an LLM as a judge questionable, especially considering the task’s design flaws, since the evaluation centers on the interviewer’s perspective (the questions only), the generated responses (interview questions) are unlikely to exhibit strong cultural nuances. Moreover, even human experts show considerable difficulty in maintaining consistency, with only 35–40% of the annotated data deemed reliable [2], as reported in lines 937–940. Together, these issues cast doubt on the robustness and reliability of the presented results.\n- **W5:** The criteria for “insider” or “outsider” cultural positioning are not well-defined, leaving the human evaluation guidelines vague. Were any written guidelines or examples provided? The prompt (C.3) says, \"Does the interviewer use language that suggests they are not part of the interviewee’s culture? Answer only in yes or no.\" There is no clear instruction about that in B.7 too.\n- **W6:** The claim that cultural nuances were “manually verified” is unclear—no mention of checklists, documentation, or how intra-cultural variation was handled. What process was used for the “manual verification” of cultural nuances mentioned in Line 494? Was there a checklist, documentation, or standardized approach?\n- **W7:** The lack of structured verification steps makes the manual validation process seem weak and unreliable. How did the authors ensure that intra-cultural variations (regional, ethnic, or social) were considered during the manual verification process?\n\n[1] Hartling L, Hamm M, Milne A, et al. Validity and Inter-Rater Reliability Testing of Quality Assessment Instruments [Internet]. Rockville (MD): Agency for Healthcare Research and Quality (US); 2012 Mar. Available from: https://www.ncbi.nlm.nih.gov/books/NBK92293/ https://www.ncbi.nlm.nih.gov/books/NBK92287/table/executivesummary.t2/?report=objectonly\n\n[2] McHugh M. L. (2012). Interrater reliability: the kappa statistic. Biochemia medica, 22(3), 276–282."}, "questions": {"value": "Please address the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y3EEXg7ckY", "forum": "WzLjwv8KAn", "replyto": "WzLjwv8KAn", "signatures": ["ICLR.cc/2026/Conference/Submission24027/Reviewer_ngpY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24027/Reviewer_ngpY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748763066, "cdate": 1761748763066, "tmdate": 1762942902482, "mdate": 1762942902482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work identifies a novel culture positioning bias in large language models (LLMs), where generations default to mainstream U.S. cultural perspectives and marginalize other cultures. To measure this bias, the authors introduce CultureLens, a benchmark with 4,000 prompts and 3 metrics that evaluate cultural stance through interview-style text generation across 10 global cultures. They further propose Fairness Intervention Pillars (FIP) and an agent-based Mitigation via Fairness Agents (MFA) framework, showing that MFA methods dramatically reduce cultural bias—by up to 89.7%—and offer a robust path toward fairer generative LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes CultureLen to evaluate culture positioning bias problem.\n2. It also proposes a baseline prompt-based Fairness Intervention Pillars (FIP) method, and a structured Mitigation via Fairness Agents (MFA) framework to mitigate culture positioning bias problem."}, "weaknesses": {"value": "1. I think this experiment in Sec 5.1 is not rigorous. There are lots of cultural knowledge, covering different aspects. The paper just did experiments on Reddit and Wikipedia and claims that culture-specific knowledge can't improve fairness performance. To get this conclusion, the authors need to do large-scale experiments.\n2. I don't think the paper proposes some novel findings. For the culture positioning bias, it seems not new.\n3. For the Fairness Intervention Pillars, I still don't think the method is novel."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ks50sMgkwg", "forum": "WzLjwv8KAn", "replyto": "WzLjwv8KAn", "signatures": ["ICLR.cc/2026/Conference/Submission24027/Reviewer_N7qu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24027/Reviewer_N7qu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948415103, "cdate": 1761948415103, "tmdate": 1762942902265, "mdate": 1762942902265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}