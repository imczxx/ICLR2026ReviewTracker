{"id": "zA2LbsUMDd", "number": 640, "cdate": 1756760416434, "mdate": 1763575093603, "content": {"title": "VideoNSA: Native Sparse Attention Scales Video Understanding", "abstract": "Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models.  **Our method, VideoNSA, adapts Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We employ a hardware-aware hybrid approach to attention, preserving dense attention for text, while employing NSA for video.** Compared to token-compression and training-free sparse baselines, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. Further ablation analysis reveals four key findings: (1) reliable scaling to 128K tokens; (2) an optimal global–local attention allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4) the learnable combined sparse attention help induce dynamic attention sinks.", "tldr": "VideoNSA shows that native sparse attention can scale video-language models reliably and efficiently.", "keywords": ["Efficient Video Understanding", "Sparse Attention"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe913da7396c0934269a078bd5a85e87b4932f50.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper adapts Native Sparse Attention (NSA) to video-language models, demonstrates its potential in tasks including long video understanding, temporal reasoning, and spatial intelligence, and conducts extensive and thorough ablation and analytical experiments. Additionally, to mitigate attention sinks in long-term visual contexts, this paper further proposes to dynamically integrate global and local attention via three complementary branches, effectively addressing this issue."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper attempts to apply Native Sparse Attention (NSA) to VideoMLLMs for the first time, and effectively demonstrates its potential in tasks such as long video understanding.\n\n2. The ablation experiments in this paper are quite comprehensive and rigorous, which analyze the performance of NSA in video understanding tasks from multiple perspectives."}, "weaknesses": {"value": "1. This paper appears to lack an analysis of the training and inference efficiency of VideoNSA. For instance, regarding Table 1, it would be desirable to know the comparisons between various methods and VideoNSA in terms of inference efficiency, latency, and FLOPs. For training efficiency, a comparison between VideoNSA and full attention under the same context length is also expected.\n\n2. Regarding the application of NSA in video tasks, the primary video-related modification in this paper seems to be employing standard GQA for text while adopting NSA for video attention. It is of interest to understand the impact of this operation on the final performance. Does \"Dense-NSA\" in Table 3 refer to the use of NSA for all modalities? It is suggested that the authors further elaborate on the performance differences of NSA across different modalities (i.e., video and text)."}, "questions": {"value": "1. Does \"Dense-NSA\" in Table 3 refer to the use of NSA for all modalities?\n2. Regarding Figure 2, does the performance under the 64k context length appear to be better than that under 128k? Does this indicate that training under a short context length (i.e., 36k) is insufficient to unlock the full 128k performance of VideoNSA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3FJmyNnEPj", "forum": "zA2LbsUMDd", "replyto": "zA2LbsUMDd", "signatures": ["ICLR.cc/2026/Conference/Submission640/Reviewer_awwy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission640/Reviewer_awwy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760607854839, "cdate": 1760607854839, "tmdate": 1762915573803, "mdate": 1762915573803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the challenge of long-video understanding in MLLMs, which is constrained by the quadratic complexity of standard attention. The authors propose VideoNSA, a hybrid attention mechanism that adapts Native Sparse Attention for video-language models. The core of the method is to apply standard Grouped-Query Attention to text tokens while using a learnable, three-branch sparse attention mechanism for the video tokens. This video-specific NSA dynamically combines a Token Compression branch, a Token Selection branch, and a Sliding Window branch using learnable gates. The model, which is an adaptation of Qwen2.5-VL , is trained end-to-end on a 216K video instruction dataset. The authors present experiments showing that VideoNSA achieves competitive performance on long-video understanding, temporal reasoning, and spatial benchmarks. The paper also provides a very detailed analysis of the model's scaling properties, attention budget allocation, internal branch usage, and its effect on mitigating attention sinks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a highly significant and timely problem: scaling video-language models to handle long contexts (e.g., thousands of frames or 128K tokens) efficiently.\n\n2. The proposed hybrid attention mechanism is well-motivated. Preserving dense attention for text tokens while applying aggressive, learnable sparsity to the highly redundant video tokens  is a sensible architectural choice.\n\n3. The paper's main strength is its extensive analysis section. The authors provide commendable, deep insights into the model's behavior, including:\n- A study of information scaling (spatial vs. temporal trade-offs) .\n- A detailed breakdown of attention budget allocation (global vs. local).\n- An analysis of the dynamic gate usage across layers.\n- A novel and valuable investigation into how different sparse branches uniquely contribute to or mitigate attention sinks."}, "weaknesses": {"value": "1. My most significant concern is the flawed \"Dense-SFT\" baseline. This baseline, which should serve as the primary control, was fine-tuned on the same 216K dataset as VideoNSA. However, this Dense-SFT model performed worse than the original, pre-trained Qwen2.5-VL on most benchmarks (e.g., LVB, TimeScope, Tomato). The authors attribute this to the \"limited quality of the training data\". This admission severely confounds the paper's central claim. We cannot know if VideoNSA's architectural improvements are genuine or if the VideoNSA architecture is simply more robust to this specific, low-quality training data than a dense model. The experiment fails to demonstrate that VideoNSA is better than a properly trained dense model.\n\n2. While the paper claims \"improved performance,\" the results in Table 1 are more accurately described as \"competitive\" or \"on par\" rather than a significant step forward.\n- On Long VideoBench (LVB), VideoNSA (60.0) is outperformed by Video-XL-2 (61.0).\n- On $MLVU_{test}$, VideoNSA (51.8) is outperformed by Video-XL-2 (52.2) and InternVL2.5-8B (55.8).\n- On Long TimeScope (LTS) and TimeScope, its scores (44.4 and 83.7) are effectively tied with other sparse attention methods like MInference and XAttention.\n- Given the confounding baseline (Weakness 1), these marginal gains are not sufficient to robustly claim superiority.\n\n3. The paper is motivated by efficiency, but its own analysis (Finding 5) identifies the compression (CMP) branch as the dominant latency bottleneck as context length grows. The paper concludes that \"the prefill stage remains the primary bottleneck\". While the analysis is transparent and appreciated, the paper identifies a critical practical limitation of its own method without offering a solution. This undermines the practical efficiency claims of the work."}, "questions": {"value": "1. Could you elaborate on the \"Dense-SFT\" baseline's performance drop? If the training data is of limited quality, how can you be sure it isn't also limiting VideoNSA's potential? Conversely, do you hypothesize that VideoNSA's gains over the dense baseline would be larger or smaller if trained on a much larger, higher-quality video instruction dataset?\n\n2. Given the CMP branch is the bottleneck, do you have concrete suggestions for optimizing it? The paper states the block-level representation is obtained by \"averaging all tokens\" 29, but the preliminary definition (Eq. 2) mentions a \"learnable MLP\" ($\\varphi$)30.\n- First, please clarify this: is the learnable MLP simply performing a weighted average, or is it a more complex, non-linear projection?\n- Second, if it is a simple average, the latency should be low. If it's a learnable MLP, did you experiment with replacing it with a fixed (non-learnable) pooling operation to see if the performance/latency trade-off improves?\n\n3. The authors note the \"strange behavior\" in the last layer (L27) where all three branch gates become fully active. Do you have any hypothesis for this? Is it a learned behavior for final-layer aggregation, or could it be an artifact of training (e.g., the gates for that layer not receiving a strong gradient)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WEKi1WRvSJ", "forum": "zA2LbsUMDd", "replyto": "zA2LbsUMDd", "signatures": ["ICLR.cc/2026/Conference/Submission640/Reviewer_mLuS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission640/Reviewer_mLuS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760974170938, "cdate": 1760974170938, "tmdate": 1762915573670, "mdate": 1762915573670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VideoNSA, a method for scaling video understanding models to very long contexts by adapting NSA. The core idea is to apply a hybrid attention mechanism to a Qwen2.5-VL-7B. Specifically, text tokens are processed with standard GQA, while video tokens are handled by NSA, which dynamically combines three complementary sparse attention branches: CMP for global aggregation, SLC for salient information, and SWA for local context. The authors fine-tune this model on a 216K video instruction dataset. The resulting model scales effectively to 128K tokens and performs well at a series of challenging long-video benchmarks,"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Extensive Evaluation: The paper evaluates VideoNSA across a diverse set of challenging long-video benchmarks, demonstrating competitive or SOTA performance. The inclusion of strong baselines and thorough ablations validates the design choices.\n- In-depth Analysis: The analysis in Section 4 is a standout feature. The structured \"Findings\" provide clear, actionable insights into how sparse attention behaves when scaled. The study of information scaling, budget allocation, and attention sinks goes far beyond a typical model performance paper and offers significant value to the research community.\n- Efficiency and Scalability: The paper shows that VideoNSA scales effectively to 128K context lengths, far beyond its training regime. The finding that it achieves top-tier performance with only 3.6% of the dense attention budget is a powerful demonstration of the method's efficiency."}, "weaknesses": {"value": "Novelty: The primary weakness is that the core technical component, NSA, is adapted from a previous work (Yuan et al., 2025b). The novelty is in the application, specific architectural choices for video, and the extensive analysis, rather than a new algorithm."}, "questions": {"value": "In \"Finding 5,\" you identify the token compression (CMP) branch as the main latency bottleneck. Given its importance (as shown in the gate analysis in \"Finding 4\"), what are your thoughts on potential avenues for optimizing this branch to further improve the model's overall efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UXm8nj5itU", "forum": "zA2LbsUMDd", "replyto": "zA2LbsUMDd", "signatures": ["ICLR.cc/2026/Conference/Submission640/Reviewer_evHH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission640/Reviewer_evHH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963693347, "cdate": 1761963693347, "tmdate": 1762915573545, "mdate": 1762915573545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends Native Sparse Attention (NSA), originally designed for long-context models, to the multimodal domain, proposing VideoNSA. This method applies NSA's three-branch hierarchical sparse structure and gating mechanism to video tokens, while employing Grouped Query Attention for text tokens. Furthermore, the paper provides a scalability analysis, offering new insights into the behavior of sparse attention in multimodal models."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Insightful Analysis: The scalability analysis provides a degree of interpretability for sparse attention, while also clarifying the advantages of the sparse mechanism (e.g., extensibility to contexts longer than those seen during training, control over attention sinks).\n2. Comprehensive Experiments: The experimental validation is extensive, covering ablation studies for each branch, visualizations of gating distributions, context extension curves, and performance evaluations on multiple benchmarks.\n3. Practical Guidance: The empirical findings offer practical guidance for deploying hardware-aligned sparse attention in long-context multimodal systems."}, "weaknesses": {"value": "1. Limited Algorithmic Novelty: The core framework for the vision component (the three-branch sparse structure + learnable gating) is nearly identical to that of NSA (Yuan et al., 2025), meaning the method lacks fundamental innovation.\n2. Lack of Discussion on Modality-Specific Sparsity: The paper does not discuss the differences in sparsity between text and video. In text, sparse attention primarily filters information at the syntactic and semantic levels, where token dependencies are relatively stable and one-dimensional. In video, however, sparsity involves spatiotemporal locality and motion redundancy, implying that the definition of semantic redundancy differs across modalities. This raises questions about whether it is appropriate to directly reuse a text-based sparse attention mechanism for the video modality.\n3. Insufficient Theoretical or Analytical Depth: The scalability analysis is predominantly empirical. While it helps users better utilize the model, it lacks theoretical explanations or modeling to elucidate the underlying causes of the observed trends."}, "questions": {"value": "1. The paper lacks a strong motivation for applying NSA, a text-modality method, to the video modality. Many methods exist for long-context modeling; why is NSA a good choice? In other words, do the characteristics of NSA offer unique advantages in the context of video?\n2. The authors state that the model utilizes the selection and sliding-window branches in shallow layers to capture fine-grained details and local information, while relying more on the compression branch in deep layers to integrate and refine high-level global semantics. Do the sparse distributions and gating activations differ across various types of videos (e.g., high-speed motion, shot transitions, static scenes), or is the gating behavior solely dependent on layer depth? If the gating behavior is only correlated with layer depth and is independent of input features, does this imply that the model has limited adaptability to different types of video content?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ccWbbuW2sS", "forum": "zA2LbsUMDd", "replyto": "zA2LbsUMDd", "signatures": ["ICLR.cc/2026/Conference/Submission640/Reviewer_8ijG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission640/Reviewer_8ijG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079566446, "cdate": 1762079566446, "tmdate": 1762915573413, "mdate": 1762915573413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Clarifying Core Contributions and Efficiency Analysis"}, "comment": {"value": "We sincerely thank all reviewers for their valuable feedback and constructive comments.\nBelow, we summarize and address several frequently raised concerns before providing reviewer-specific responses.\n\n**Q1: Core contribution (Reviewer 8ijG and Reviewer evHH).**\n\nOur goal is not to create yet another handcrafted sparse attention module. Instead, **we conduct the first systematic analysis of learnable sparsity for long-context video understanding based on Native Sparse Attention (NSA).** We reveal modality-specific routing behavior, demonstrate how sparsity adapts differently for videos versus text, and show that NSA can automatically learn effective sparse patterns without any manually designed branch combinations.\n\n**Q2: Efficiency analysis (Reviewer evHH, Reviewer mLuS, Reviewer awwy).**\n\nWe do not claim that VideoNSA introduces the most efficient kernel. Although VideoNSA has very low theoretical FLOPs, our experiments reveal that the actual latency bottleneck (CMP) primarily arises from current NSA kernel implementations rather than the sparse mechanism itself. We further provide breakdown analyses, and cross-kernel comparisons, indicating that **efficiency can be substantially improved through kernel-level designs rather than architectural changes.**\n\nWe again appreciate the reviewers’ comments and provide detailed responses in the following sections. We have also updated the PDF and highlighted all revisions in orange for clarity, and we are happy to address any additional clarifications or follow-up questions."}}, "id": "S0doGsXNBC", "forum": "zA2LbsUMDd", "replyto": "zA2LbsUMDd", "signatures": ["ICLR.cc/2026/Conference/Submission640/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission640/Authors"], "number": 15, "invitations": ["ICLR.cc/2026/Conference/Submission640/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763573749687, "cdate": 1763573749687, "tmdate": 1763574551624, "mdate": 1763574551624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}