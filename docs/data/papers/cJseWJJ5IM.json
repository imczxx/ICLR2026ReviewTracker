{"id": "cJseWJJ5IM", "number": 9609, "cdate": 1758129866807, "mdate": 1759897709322, "content": {"title": "Efficient Reasoning with Balanced Thinking", "abstract": "Large Reasoning Models (LRMs) have shown remarkable reasoning capabilities, yet they often suffer from overthinking, expending redundant computational steps on simple problems, or underthinking, failing to explore sufficient reasoning paths despite inherent capabilities. These issues lead to inefficiencies and potential inaccuracies, limiting practical deployment in resource-constrained settings. Existing methods to mitigate overthinking, such as suppressing reflective keywords or adjusting reasoning length, may inadvertently induce underthinking, compromising accuracy. Therefore, we propose \\textsc{ReBalance}, a training-free framework that achieves efficient reasoning with balanced thinking. \\textsc{ReBalance} leverages confidence as a continuous indicator of reasoning dynamics, identifying overthinking through high confidence variance and underthinking via consistent overconfidence. By aggregating hidden states from a small-scale dataset into reasoning mode prototypes, we compute a steering vector to guide LRMs’ reasoning trajectories. A dynamic control function modulates this vector’s strength and direction based on real-time confidence, pruning redundancy during overthinking, and promoting exploration during underthinking. Extensive experiments conducted on four models ranging from 0.5B to 32B, and across nine benchmarks in math reasoning, general question answering, and coding tasks demonstrate that \\textsc{ReBalance} effectively reduces output redundancy while improving accuracy, offering a general, training-free, and plug-and-play strategy for efficient and robust LRM deployment. Code and models will be made publicly available.", "tldr": "", "keywords": ["Large Reasoning Models", "Efficient Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/baa401c20de6bd71f7cf5a44f22a35e559168d81.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes REBALANCE, a training-free framework that dynamically balances overthinking and underthinking in reasoning models by steering hidden representations based on confidence and variance signals. The method constructs a steering vector from a small calibration set (e.g., 500 samples) and adaptively adjusts the model’s internal states during inference. Experiments across multiple reasoning domains show improved efficiency (i.e., reducing the number of reasoning tokens) and good accuracy without additional parameter updates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**[S1]** Using confidence to address overthinking and underthinking is interesting. Although the overall framework is complex, the method works surprisingly well in practice, which makes the idea conceptually appealing.\n\n**[S2]** The fact that the approach operates without any parameter updates and can be instantiated with only a few hundred samples (e.g., 500) is impressive. It suggests that the proposed mechanism captures a general behavioral signal of the model rather than relying on large-scale retraining.\n\n**[S3]** The method is applied not only to mathematical reasoning tasks but also to multiple domains, which demonstrates a degree of robustness and general applicability beyond a single task family.\n\n**[S4]** The paper is overall well written and clearly structured, with extensive empirical validation and thoughtful analysis. The presentation is careful, and the experiments are relatively thorough."}, "weaknesses": {"value": "**[W1]** While Figure 5 presents a reasonable ablation on the number of samples used for mean and variance estimation, the transferability of these statistics remains questionable. The current analysis is limited to sampling from MATH and evaluating within the similar domain. It would be more convincing to test more rigorously (than the current analysis in Figure 5) whether the mean/variance computed from easier datasets (e.g., GSM8K) transfer to harder domains (e.g., AIME), or whether math-derived statistics generalize to different modalities such as code reasoning. Without this, the generalization claim remains somewhat weak.\n\n**[W2]** The method feels overly complex due to the number of hyperparameters involved. Decisions such as which layer to select, how strongly to apply steering, or what window size to use are not clearly motivated. It remains unclear whether there exists a single configuration that works consistently across datasets or domains, or whether each model and domain effectively requires its own tuning. This undermines the claim of universality and adds to the engineering-heavy impression.\n\n**[W3]** The paper is somewhat engineering-driven rather than conceptually driven. Most design choices (confidence thresholds, gating, control surfaces) are empirically fitted, and it is not clear why or when the method should work beyond the tested settings. Thus, the approach lacks principled understanding of why such steering in hidden space improves reasoning. The steering direction is defined by empirical prototypes, but the underlying mechanism or geometry of reasoning states remains speculative. From my experience with steering-based approaches, this line of work is still at a very early exploratory stage. The paper demonstrates that something “can” work, but not really why it works. While the results are decent, the contribution feels somewhat ad-hoc and might not generalize to models or tasks with different internal confidence landscapes.\n\n**[Overall]** I don’t find a strong reason to reject this paper. However, it also doesn’t give strong conceptual insights; it reads as an engineering effort that happens to work well rather than a principled step forward in understanding reasoning control. I would lean toward a weak accept, mostly for its empirical completeness and potential to inspire future, more principled research in steering-based reasoning."}, "questions": {"value": "I think it is good to add the exact throughput analysis and memory usage analysis for the efficiency claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "w7ZEA7wXVC", "forum": "cJseWJJ5IM", "replyto": "cJseWJJ5IM", "signatures": ["ICLR.cc/2026/Conference/Submission9609/Reviewer_4rRS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9609/Reviewer_4rRS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761198818013, "cdate": 1761198818013, "tmdate": 1762921149944, "mdate": 1762921149944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free solution that balances overthinking and underthinking in reasoning models.\nBased on the key observation that confidence values and their variances help distinguish these two reasoning modes, the authors introduce a representation steering method that dynamically adjusts the model’s reasoning behavior.\nExperimental results demonstrate that the proposed method improves performance while reducing reasoning length."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation is clear, and the addressed problem is crucial in the reasoning field.\n- The paper is well-written and easy to follow.\n- The experimental results, from initial observations to ablation studies, strongly support the proposed method.\n- The method is simple, easy to implement, and can be plug-and-play across various reasoning models."}, "weaknesses": {"value": "No major weaknesses are observed, but clarification on the following questions would strengthen the paper and may affect my final score."}, "questions": {"value": "**[Q1] Extraction of the Steering Vector** \nAs I understand, the authors extract the steering vector using confidence-defined sets in Eq. (5). However, directly using the definition in Eq. (3) appears simpler. Could the authors report the performance gap between these two methods?\n\n**[Q2] Observation Across Models** \nWhat type of model is used in Figure 2 (b)? I am curious whether confidence values and variances serve as general indicators across different models.\n\n**[Q3] Inference Overheads** Appendix I provides only a brief latency analysis. Could the authors include a quantitative throughput evaluation (e.g., tokens per second) comparing their method with the original model?\n\n**[Q4] Number of Samplings** How many samplings are used per test sample? Since challenge datasets such as AIME and AMC contain relatively few examples (< 100), multiple samplings may be required for more reliable evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a3xjRAccOO", "forum": "cJseWJJ5IM", "replyto": "cJseWJJ5IM", "signatures": ["ICLR.cc/2026/Conference/Submission9609/Reviewer_4rhm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9609/Reviewer_4rhm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713571895, "cdate": 1761713571895, "tmdate": 1762921149537, "mdate": 1762921149537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issues of \"Overthinking\" (redundant computational steps) and \"Underthinking\" (insufficient reasoning paths) in Large Reasoning Models. The authors propose REBALANCE, a training-free framework that aims to achieve efficient and \"balanced thinking.\" The core idea is to use model confidence and confidence variance as real-time indicators to dynamically steer the model's hidden states. The framework uses a pre-computed \"steering vector,\" derived from reasoning mode prototypes extracted in an offline pass, to guide the model's reasoning trajectory. A dynamic control function modulates this vector's strength and direction, pruning redundancy during overthinking and promoting exploration during underthinking. Experimental results show that the proposed method outperforms existing baselines and achieve better efficiency."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear Optimization Objective: The paper defines Overthinking and Underthinking based on numerical features of internal states (stepwise confidence and confidence variance) derived from empirical observations. This semantic-agnostic definition, independent of specific keywords, provides a very clear and quantifiable optimization objective.\n\n2. Extensive Experimentation: The paper provides comprehensive experiments and ablation studies. The REBALANCE framework is validated across multiple models of varying scales and tested on diverse domains beyond mathematics, demonstrating strong generalization. Furthermore, thorough ablation studies are conducted for key design choices, offering justifications for the method's components.\n\n3. Demonstrated Effectiveness: The experimental results demonstrate that the proposed approach can successfully balance the trade-off between Overthinking and Underthinking. It outperforms previous baselines in terms of task performance and token efficiency."}, "weaknesses": {"value": "1. Missing Citation to Key Related Work: The paper focuses significantly on the problem of \"Underthinking\". However, it fails to cite an important and highly relevant recent study on this specific phenomenon [1]. This omission is a significant gap in the related work section.\nReference:\n[1] Wang Y, Liu Q, Xu J, et al. Thoughts are all over the place: On the underthinking of o1-like llms[J]. arXiv preprint arXiv:2501.18585, 2025.\n\n2. Practical Application Burden: Although the method is presented as \"training-free,\" it remains highly dependent on hyperparameter tuning. Furthermore, it requires an additional \"seen dataset\" and a model-specific calibration step (to extract steering vectors and fit the control function). This introduces an extra implementation and tuning burden for each new backbone model, which may complicate its practical application."}, "questions": {"value": "1. The intervention method is specifically developed to target the paper's definitions of Overthinking (low-confidence/high-variance) and Underthinking (high-confidence/low-variance). However, could this direct manipulation of internal reasoning states have other unintended side effects on reasoning, beyond just mitigating these two defined issues? For instance, could it negatively impact creativity or the naturalness of the expressions?\n\n2. While the quantitative results are strong, have the authors analyzed the semantic changes in the model's reasoning process post-intervention? For example, did the REBALANCE framework lead to a measurable change in the frequency of keywords associated with reflection or hesitation (e.g., \"wait,\" \"alternatively,\" \"let me check\")?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JfaJhF63Fb", "forum": "cJseWJJ5IM", "replyto": "cJseWJJ5IM", "signatures": ["ICLR.cc/2026/Conference/Submission9609/Reviewer_HMxB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9609/Reviewer_HMxB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991856427, "cdate": 1761991856427, "tmdate": 1762921149221, "mdate": 1762921149221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key challenge in LRMs: the trade-off between overthinking and underthinking. The authors propose ReBalance, a training-free and plug-and-play framework that dynamically controls reasoning behavior via internal confidence signals. Experiments on 4 models and 9 reasoning benchmarks show that ReBalance reduces reasoning token length significantly while improving or maintaining accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. An important problem that is of strong interest to the LRM community.\n\n2. The paper uses stepwise confidence and confidence variance to detect reasoning modes, with a motivation that is well-explained and intuitively convincing.\n\n3. The method achieves superior results across a wide range of reasoning benchmarks."}, "weaknesses": {"value": "1. The main text lacks a concise introduction to the baselines, which makes the paper more difficult for readers to follow.\n\n2. Novelty of this paper is limited. Steering vector has been used before. How does ReBalance differ from SEAL? Both of them leverage the steering vector. I want to see an apple-to-apple comparison. \n\n3. ReBalance is similar in spirit to adaptive halting or early-exit reasoning models, such as TrimR [1] and FlashThink [2]. I noticed that the authors cited these two papers, but neither compared them as baselines nor discussed them in the Related Work section of the appendix or other places.\n\n[1] Weizhe Lin et al. TrimR: Verifier-based training-free thinking compression for efficient test-time scaling. arXiv preprint arXiv:2505.17155, 2025.\n\n[2] Guochao Jiang et al. FlashThink: An early exit method for efficient reasoning. arXiv preprint arXiv:2505.13949, 2025. (edited)"}, "questions": {"value": "see the weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DZqhzqzxLw", "forum": "cJseWJJ5IM", "replyto": "cJseWJJ5IM", "signatures": ["ICLR.cc/2026/Conference/Submission9609/Reviewer_RNpb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9609/Reviewer_RNpb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762085625044, "cdate": 1762085625044, "tmdate": 1762921148902, "mdate": 1762921148902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}