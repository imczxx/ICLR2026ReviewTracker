{"id": "jDQyU5j8pn", "number": 18560, "cdate": 1758289130876, "mdate": 1759897095707, "content": {"title": "Meta-Learning Contextual Time Series Forecasting with Neural Processes", "abstract": "Neural Processes (NPs) are a powerful class of meta-learning models that can be applied to time series forecasting by formalizing it as a probabilistic regression problem. However, conventional NPs base their predictions only on observations from a single time series, which limits their ability to leverage varied contextual information. In this paper, we introduce a novel NP architecture that, in the spirit of meta-learning, is designed to incorporate context information from multiple related time series. To this end, our approach treats related time series as conditionally independent context examples of a shared underlying data-generating process corresponding to a specific meta-task. A sequence encoder aggregates a variable number of such context time series into a latent task description, which then conditions a sequence decoder, enabling accurate forecasting of unseen target time series. We evaluate our approach on challenging time series forecasting problems, demonstrating that our architecture performs favorably compared to a range of competitor approaches.", "tldr": "We propose a novel meta-learning Neural Process architecture that integrates context from multiple related time series to perform robust time series forecasting.", "keywords": ["Meta Learning", "Neural Processes", "Latent Variable Models", "Time Series Forecasting"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58592b2ea06e6a95b2c8cbf0a8e0d37951d42fd0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper the authors propose a meta learning framework that utilizes Bayesian neural processes for time series forecasting with context examples. The key novelty is the use of Bayesian aggregation to merge the hidden representation of an indefinite amount of context examples into the formulation of conditional NPs. In the empirical study section the authors demonstrate the effectiveness of their proposal over a few ablation baselines on timestamped traces collected from RL environments."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies a practically meaningful question of time series forecasting with few-shot examples. The proposed method of bayesian context aggregation is a clean formulation in the bayesian NP framework."}, "weaknesses": {"value": "While the overall theoretical idea is clean, the writing of the paper is rather hard to follow with many choices made in the paper not explained. The empirical study is also remotely convincing, that it lacks other baseline methods and the benchmark tasks are arguably not about time series. See questions."}, "questions": {"value": "1. Regarding the theoretical perspective, is there equivalence between the Bayesian formulation and the method of MLP on top of the deep set aggregation of context example encodings concatenated with the representation of the task history? \n\n2. Regarding writing:\n(1) Some key details are missing in the main body: e.g., a clean formulation of the exact question and data generative distribution used in the study, the exact model architecture, a sketch of a forward pass, stepwise summary of the training algorithm, to name a few. \n(2) The writing is not well organized in its current shape, which makes it quite hard to see the contributions of the paper. For example, section 1 through 4 are too dense touching topics not in order while have little information to distinguish the proposed method from existing ones.\n\n3. The tasks involved in the empirical studies are new traces from dynamic systems generated by RL environment. In my humble opinion these are not the go to tasks when one thinks of time series forecasting which usually involves some degree of trend and seasonality on top of autocorrelation. Consider including other established time series benchmarks which would also help demonstrate the generalizability of the proposed method.\n\n4. Similarly there are no other time series forecasting methods involved as baselines to justify the advantage of the proposed method. If the context examples are the limiting factor, at least consider testing in the uninformed manner, or apply multivariate models treating contexts as other variates after conditioning on the number of context examples."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EJtanx1eZa", "forum": "jDQyU5j8pn", "replyto": "jDQyU5j8pn", "signatures": ["ICLR.cc/2026/Conference/Submission18560/Reviewer_oXKk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18560/Reviewer_oXKk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632613551, "cdate": 1761632613551, "tmdate": 1762928273856, "mdate": 1762928273856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a neural process-based model designed for meta-learning in probabilistic time series forecasting. Compared with previous works that make predictions only based on previous observations, this work incorporates context information from multiple related time series to form a meta task. A sequence encoder is used to encode context series into a latent representation. Multiple representations are aggregated via Bayesian context aggregation to condition a decoder to generate predictions. Experiments on simulated reinforcement trajectories validate the effectiveness of the proposed model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The issue studied, leveraging multiple related time series for improved forecasting performance, is fascinating and highly relevant.\n2. The proposed model is simple and easy to understand."}, "weaknesses": {"value": "1. The presentation needs improvement:\n  - Too much space is used to introduce the less critical background technical details, well, the core problem statement and methods only appear on page 5 and are very brief.\n  - The notations used are too complicated, making readers confused.\n\n2. Despite that, utilizing related series to improve forecasting is interesting; the result model that encodes each series using an encoder and aggregates representations with Bayesian context aggregation is a little bit trivial.\n\n3. Experiments are only conducted on simulated MuJoCo state trajectories; broader experiments on real-world time series datasets are required.\n\n4. Only some ablated versions of the proposed model are compared, lacking comparison with previous works.\n\n5. An ablation study is required to validate the advantages of: 1)utilizing related series vs. predicting based only on past observation; 2) Bayesian context aggregation v.s. Simple aggregation like mean pooling."}, "questions": {"value": "In real-world datasets other than controllable RL environments, how to construct a meta-task dataset with related series?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "85jQo5BDAN", "forum": "jDQyU5j8pn", "replyto": "jDQyU5j8pn", "signatures": ["ICLR.cc/2026/Conference/Submission18560/Reviewer_Qe2b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18560/Reviewer_Qe2b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709202418, "cdate": 1761709202418, "tmdate": 1762928272952, "mdate": 1762928272952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a meta-learning approach for contextual time series forecasting using neural processes. The work introduces a neural processing framework for multivariate time series that employs in-context learning to address the problem of limited data and enable few-shot learning. The paper appears to present a Bayesian variant neural network that modifies standard neural processes to tackle meta-learning for multivariate time series. According to the abstract and introduction, a key motivation is addressing a limitation in existing methods that focus on individual time series forecasting rather than leveraging information from multivariate time series collectively. The authors propose alternative conditional neural process architectures specifically designed to operate on time series, potentially using recurrent neural networks or causal transformers."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "## Strengths\n\n- The third paragraph of the introduction provides a helpful overview of neural processes as neural networks designed to learn stochastic processes.\n- The related work on neural processes suggests this is a promising research direction.\n- The experimental results appear favorable based on the highlighted outcomes presented."}, "weaknesses": {"value": "## Weaknesses\n\n**Clarity and Presentation:**\n- The core contribution and essential technique are unclear even after reading the introduction (lines 60-70). An enumerated list of contributions would significantly improve clarity.\n- The presentation is inaccessible to readers outside this specific domain. The paper assumes too much specialized knowledge without providing sufficient intuitive explanation.\n- The paper lacks concrete examples or running examples that would help readers understand the approach.\n- Critically, there are no graphical illustrations of the proposed model, making it extremely difficult to understand the architecture and data flow.\n\n**Technical Exposition:**\n- By page 4, halfway through the paper, the proposed method remains unclear. The background material is overly introductory and could be shortened, yet it fails to build toward a clear understanding of the contribution.\n- Section 4 (Method) is very difficult to follow. The equations are presented at a high level without clearly conveying the actual model architecture or implementation details.\n- The relationship between mentioned components (e.g., recurrent neural networks or causal transformers in line 258) and the mathematical notation (e.g., equation in line 265) is unclear and too briefly explained.\n- Where and how neural networks are incorporated into the neural process framework is never made explicit.\n\n**Notation and Mathematical Rigor:**\n- Notation inconsistencies cause confusion. For example, d_tc appears related to f_t (line 152), but f_t's definition is unclear, and the notation d_tc should possibly be d_ft given earlier definitions (line 134).\n- Different symbols appear similar or are confusing (calligraphic T in line 154 vs. line 133, which turn out to be τ and calligraphic T respectively).\n- The loss function is not clearly specified. It is unclear whether cross-entropy or log-likelihood is used.\n\n**Logical Consistency:**\n- Line 213 contains confusing or potentially contradictory statements about using univariate individual time series versus incorporating information from related time series at previous time steps.\n- The Bayesian context aggregation section (around line 173) lacks clarity about what probabilistic quantities are being computed.\n\n**Overall Impact:**\n- After carefully reading through Section 4, I remained unable to understand the proposed method despite significant effort. This prevented meaningful evaluation of the experimental results.\n- The paper does not meet the reasonable expectation that a reader with a PhD should be able to grasp at least the intuition behind the proposed approach.\n\n**Recommendation:** The paper requires major revisions focusing on clarity of presentation, concrete examples, visual aids, explicit architectural details, and clearer mathematical exposition before it can be properly evaluated."}, "questions": {"value": "I have listed structured questions (with help of LLM) in the above weakness part. I am going to say here my honest thoughts when reading the paper as it presents, and hopefully this can help you understand how a new reader perceives your paper. These raw feelings are genuine and I hope they provide a more human-to-human communication and contexts for the structured question above.\n\n# Review: Meta Learning Contextual Time Series Forecasting with Neural Process (Paper 18560)\n\nI am reviewing a paper on meta-learning contextual time series forecasting with neural processes. The paper appears to introduce a neural processing framework for multivariate time series. From the abstract, it seems to be using context in meta-learning, though the specific mechanism is unclear after this initial reading.\n\n## Introduction\n\nAfter reading the first two paragraphs of the introduction, I understand this is a meta-learning Bayesian variant neural network for few-shot learning. The third paragraph is helpful regarding neural processes, which are essentially neural networks designed to learn stochastic processes. Line 55 discusses a limitation in the formulation of existing methods that seem to focus on individual time series forecasting rather than leveraging multivariate time series collectively.\n\nLines 60 to 70 appear to describe the contribution, but the presentation is unclear. An enumerated list would help here. My vague intuition is that this involves meta-learning and some form of in-context learning. However, after reading this paragraph, I still do not understand the essential technique that achieves the proposed improvements.\n\n## Related Work\n\nLine 82 clearly implies that the method will employ in-context learning to solve the problem of limited data and enable few-shot learning. After reading the related work section, it is clear that the paper proposes some modification of neural processes to tackle meta-learning for multivariate time series and few-shot learning. The related follow-up work on neural processes makes this direction promising.\n\n## Background (Section 3)\n\nWhile I intend to read Section 3 in detail, glancing through the structure of the paper, I realize that up to page 3, I still do not know what the authors are actually proposing. The first two paragraphs on stochastic processes, time series, and meta-learning are very introductory and could be shortened. I still do not have a concrete understanding of what the authors are trying to accomplish.\n\nAfter reading all of the background material, I am now up to page 4, halfway through the paper. The approach is still not crystal clear or intuitive to me. A running example would help significantly. After reading about the neural process family, I have a high-level understanding of the concept, but the roles of context and targets, and where the neural network comes into play, remain confusing. One reason for this confusion may be that the description has been very generic.\n\nI find myself staring at certain parts for a long time. For instance, in line 152, there is a predictive distribution of d_τ or T (calligraphic T) of c. This d, denoted as d_tc, is related to f_t. However, where is that f_t defined? Earlier, there is a d_t notation in line 134, and I understand that t corresponds to the function mapping between y and x. Therefore, d_tc should really be d_ft. Additionally, I assume the calligraphic T in line 154 is different from the T in line 133 and has a very different meaning. Upon closer inspection, I see that in line 154, it is actually τ (tau), and in line 133, it is calligraphic T. (You are really pushing the limit of my eyes.)\n\nAfter reading the neural process family section (lines 150-176), I still do not understand where the neural network comes into play. The section on Bayesian context aggregation leaves me very confused. My understanding is that it relates to a posterior distribution, but I am uncertain. As of line 173, having read the first three sections, I do not have a concrete example or concrete mathematics to understand what is exactly happening. The description has been high-level, but not in a way that builds intuition. I also lack intuition about the underlying mechanism.\n\n## Method (Section 4)\n\nIn line 228, I encounter a sentence that concretely addresses the proposal: the authors are proposing alternative conditional neural process architectures specifically designed to operate on time series. However, I still do not know what this entails—the sentence feels hollow.\n\nLine 213 is confusing. It seems to say that the method uses only a univariate individual time series from its own past, yet there might be other information from the past for other related time series. I would be very careful here. For related time series, the method should also use information from those related time series at previous time steps, which does not align well with the boldface sentence in line 213.\n\nI find Section 4 very difficult to read. The equations are high-level and do not clearly convey the architecture or the actual model. All I can discern is a few-shot learning approach for a conditional distribution. Surprisingly, there is not a single graphical illustration of the model to aid understanding. Without an intuitive example or exact mathematical formulation, how am I supposed to understand what the authors are proposing?\n\nFor example, the authors mention using a recurrent neural network or a causal transformer in line 258. How does that relate to the notation in the equation at line 265? The explanation is too brief. Line 265 appears to already involve Bayesian aggregation, but I do not understand what is actually being done.\n\nAt line 270, I see that there is an output (y_hat) that is a Gaussian distribution, with the model outputting the mean and variance. This yields a normal distribution. What is the loss function? Cross-entropy? If the authors specify a log-likelihood parameterization decoder, are they using log-likelihood as the loss function?\n\nRegarding meta-training in line 280, the authors state it is similar to that of standard neural process variants during the training step. At this point, I am simply lost. I have tried very hard to understand the method, but I cannot.\n\n## Experiments and Results\n\nI glanced through the experiments and results. Because I have so little understanding of the methods, I cannot properly evaluate the results. The outcomes appear favorable based on the boldface highlights, but there is not much I can meaningfully assess.\n\n## Overall Assessment\n\nUnfortunately, because I did not truly understand the method, I would not be able to rate this paper highly. I believe it is reasonable to expect that anyone with a PhD should at least understand the intuition behind a proposed method. The presentation is not friendly to an audience outside this particular domain—to say the least. The paper would benefit significantly from clearer explanations, concrete examples, graphical illustrations, and more explicit mathematical formulations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OD8MPvTdb3", "forum": "jDQyU5j8pn", "replyto": "jDQyU5j8pn", "signatures": ["ICLR.cc/2026/Conference/Submission18560/Reviewer_3dpa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18560/Reviewer_3dpa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869091654, "cdate": 1761869091654, "tmdate": 1762928272142, "mdate": 1762928272142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of Neural Processes (NPs) in time series forecasting, namely their inability to leverage context from multiple related time series and their simplistic handling of temporal structure. The authors propose a Conditional Neural Process (CNP) variant that treats each entire time series as a context example, aggregating information with a sequence-based encoder and Bayesian Aggregation. The approach is evaluated on challenging reinforcement learning-based simulation tasks, with extensive experiments comparing against strong baselines, including a transition-based CNP, a transformer-concat model, an oracle, and an uninformed baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The central idea of treating an entire time series as a single context point within a meta-learning task is a conceptually elegant and powerful extension of the NP framework. This approach provides a more natural and structured way to handle collections of related time series, better aligning the model's inductive biases with the problem structure often found in real-world scenarios.\n2. The paper is exceptionally logically structured and easy to follow. The authors clearly motivate the problem, formalize their approach, and situate it within the existing literature. The experimental design is sound, featuring appropriate and insightful baselines, which effectively establish upper and lower performance bounds and help to contextualize the results."}, "weaknesses": {"value": "1. Experiments are confined to structured physics simulations (MuJoCo) and do not test noisy, heterogeneous real-world time series (e.g., finance, energy, retail), where task boundaries are less explicit. Claims of general applicability remain unverified on real data.\n\n2. The method assumes all context series come from the same meta-task. The paper does not evaluate cases with irrelevant or out-of-distribution series, or show that Bayesian Aggregation effectively down-weights them, leaving reliability under realistic noise conditions uncertain."}, "questions": {"value": "1. Real-world time series data often lack the clean \"meta-task\" structure present in the simulation environments. Could the authors comment on the potential applicability and challenges of their method on broader real-world time series benchmarks (e.g., from finance, energy, or demand forecasting)? \n\n2. How does the model behave when the context set includes irrelevant or out-of-distribution series? Does Bayesian Aggregation actually down-weight such sequences via learned variances? Were any experiments conducted to explicitly validate this behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qLJqxKCc58", "forum": "jDQyU5j8pn", "replyto": "jDQyU5j8pn", "signatures": ["ICLR.cc/2026/Conference/Submission18560/Reviewer_3tMr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18560/Reviewer_3tMr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994320253, "cdate": 1761994320253, "tmdate": 1762928268634, "mdate": 1762928268634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}