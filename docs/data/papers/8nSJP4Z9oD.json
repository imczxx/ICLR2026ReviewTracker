{"id": "8nSJP4Z9oD", "number": 3721, "cdate": 1757505668762, "mdate": 1762941485599, "content": {"title": "Rethinking Reward Miscalibration of GRPO in Agentic RL", "abstract": "Building autonomous agents capable of solving long-horizon, real-world tasks has garnered significant research interest. But outcome based rewards may cause reward miscalibration which means it might mistakenly allocate positive reward to flawed middle steps which is regarded as the key reason making the bad actions being reinforced during training.\n    However we reveal that outcome based reward ensures expected negative advantage for those flawed middle steps, which means the flawed actions should be punished during training. Even accounting for the ``squeezing effect\", the probability mass of good actions should increase and the actor should gradually get rid of harmful actions. This shows that flawed actions should be punished during training. \n    We further identify gradient coupling between similar samples as a key issue in agentic RL, the input prompt is extremely similar and the output action space is limited, therefore during training, gradients from well-performing samples can inadvertently strengthen suboptimal or incorrect actions due to similar input observation and output actions. We show that with gradient coupling, some flawed actions might be enhanced.\n    To address this, we propose training the actor to classify good or bad actions to separate the embedding of good/bad actions and alleviate the gradient interference, extensive experiments shows its effectiveness.", "tldr": "Explaining why some flawed actions might be enhancedg during GRPO", "keywords": ["Agent", "GRPO", "RL", "LLM"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b9522b9bc1d750a6e4e4a7416699abbc8d1901e7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper challenges the idea that GRPO struggles due to reward miscalibration, arguing instead that the real issue is gradient coupling: shared neural representations cause updates from successful trajectories to inadvertently reinforce similar but flawed actions, especially early in training. To mitigate this, the authors propose Generative Classification Disentanglement (GCD), which adds an auxiliary task where the policy classifies actions as good or bad to disentangle their embeddings, along with lightweight prompt corrections. Experiments on ALFWorld and ScienceWorld show that GCD consistently improves performance, particularly in out-of-domain settings, highlighting that addressing representational interference is more effective than simply adjusting reward structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Reframes “reward miscalibration” and identifies gradient coupling as the true failure mode, with a clear, intuitive narrative grounded in the RL context.\n\n- I really like how the authors present their approach: the coupling diagnosis is clean, falsifiable, and measurable."}, "weaknesses": {"value": "- the “gradient coupling” story here is a specific LLM-flavoured instance of a very common RL pain point: function-approximation spillover. Neural nets are smooth; many control problems are effectively discrete. When we push up the log-prob for a “good” behaviour, nearby representations get dragged along. That’s just shared features + shared parameters doing their thing. In another word, this is a problem causes by using of NN, but not specific to GRPO. \n\n- This paper use a strong cold start pushes the policy into the “safe regime”, where self-correction already works; this shrinks the headroom for the proposed fix, so gains reflect pretraining quality more than the method. In many deployments you don’t have rich supervised/preference data to seed the policy; if the method shines mainly with good seeds, its robustness from scratch is uncertain.\n\n- The “negative expected advantage” argument is intuitive but glosses over off-policy data, partial observability, and non-stationary judges. I can understand that for theoretical proof, you can somehow assume perfect condition. However, you need to show more experiments to validate the performance across different scenarios, two is not enough."}, "questions": {"value": "- Does your mitigation still help if you replace the policy head with per-action (decoupled) heads or small per-skill adapters?\n- A cold start lifts you out of the “danger zone,” so training self-corrects faster and looks more stable. However, this making the proposed fix appear less critical. Based on my understanding, GCD disentanglement should helps the most when starts the danger zone, where gradient coupling dominates and errors amplify; Why you decided to do the experiments with cold starts as this seems not the best way to present the effectiveness of your method?\n-I think you need to define the loss of GCD in the main text. L_gcd should be properly defined somewhere.\n- is GCD a token level approach as GRPO is at the trajectory level or both is at the trajectory level? In figure 6, it seems that it is all at a trajectory level...which is different with you claim"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e9vcYpOFWm", "forum": "8nSJP4Z9oD", "replyto": "8nSJP4Z9oD", "signatures": ["ICLR.cc/2026/Conference/Submission3721/Reviewer_eAwJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3721/Reviewer_eAwJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760938793973, "cdate": 1760938793973, "tmdate": 1762916947060, "mdate": 1762916947060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely thank the reviewers for their valuable feedback. We apologize for any lack of clarity in the original manuscript and will try to improve the presentation and address the concerns in the revised version"}}, "id": "X4RO9weTvW", "forum": "8nSJP4Z9oD", "replyto": "8nSJP4Z9oD", "signatures": ["ICLR.cc/2026/Conference/Submission3721/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3721/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762938690296, "cdate": 1762938690296, "tmdate": 1762938690296, "mdate": 1762938690296, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the failure of GRPO in agentic RL tasks. It (may) argue that the true cause is not reward miscalibration, but gradient coupling between similar samples. This coupling allegedly causes gradients from good trajectories to incorrectly reinforce similar bad actions. The authors propose an auxiliary classification task, Generative Classification Disentanglement (GCD), to separate the embeddings of good and bad actions and mitigate this issue."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "While the paper may tackle an important problem, it is difficult to discuss the strengths with the current manuscript now."}, "weaknesses": {"value": "(1) Severe Presentation Issues: The paper is nearly un-reviewable due to its extremely poor writing quality. I feel that the manuscript might be somehow automated or randomly generated. It is filled with severe grammatical errors, non-existent words (e.g., \"extrsome\", \"emely\", \"simplicify\"), and logically broken sentences. For example, a single sentence in the Related Work section is contradictory and incoherent:\n\n>Reinforcement Learning (RL during reinforcement learning) algorithms like PPO (Schulman et al., 2017) is growing extrsome emely popular which would weaken the performancebecause it can greatly help the performance through Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022a).\n\nSimilar issues pervade the entire manuscript. The current version is not above the bar for publication or even review at ICLR.\n\n(2) Incomprehensible Methodology: Because the writing is so unclear, the paper's central claims are difficult to understand, let alone verify. The precise distinction between \"gradient coupling\" and the \"squeezing effect\" is not clearly articulated, and the technical implementation of the proposed \"Generative Classification Disentanglement (GCD)\" is opaque. It is impossible to judge the soundness of the method when it is not described coherently."}, "questions": {"value": "."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lwVR9lU5Pf", "forum": "8nSJP4Z9oD", "replyto": "8nSJP4Z9oD", "signatures": ["ICLR.cc/2026/Conference/Submission3721/Reviewer_wZax"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3721/Reviewer_wZax"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761011743223, "cdate": 1761011743223, "tmdate": 1762916946519, "mdate": 1762916946519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors want to address a bottleneck in improving the performance of LLMs on agentic tasks: some failure modes increase after GRPO. They show this is not justifiable with reward miscalibration or \"squeezing effect\" and instead hypothesize that it's because some good actions are similar to bad actions and the positive gradient for the good action makes the bad action more probable too."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- An interesting new perspective on LLM training dynamics in a simple model of: probability of the wrong actions and their risk\n\n- Improved empirical performance over baseline GRPO"}, "weaknesses": {"value": "- The reward miscalibration hypothesis doesn't assume that we are taking the expectation over all the trajectories. We only cover a part of the trajectory space in a GRPO run and this subspace may reinforce the bad actions that are present in successful trajectories. So the part of the argument that dismisses it is not accurate.\n\n- The learning dynamic in section 3.3 is not mathematically rigorous or backed by empirical data. Providing intuition about the dynamics is great, but it should only be an introduction to the actual proof or evidence. \n\n- I spent more time than needed to understand section 3.3 and the interesting idea behind it; it could be a lot simpler with a set of equations over time $t$. Right now, it's pretty badly written to be honest. Its Figure 4 is also rushed. Also section 4.1 is very vague about $\\mathcal L_\\text{GCD}$. I've decreased my score by 1 only because of the writing of these two sections.\n\n- Missing ablations about the classification task of GCD. I suspect that an LLM can decide whether an intermediate action is good or bad since it needs planning and rollouts to find out. It's not just LLM judging, say, the quality of a math solution while already knowing the answer."}, "questions": {"value": "- What is the setup for Figure 3b? How are the probabilities in the heatmap measured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D0IkXQVjpm", "forum": "8nSJP4Z9oD", "replyto": "8nSJP4Z9oD", "signatures": ["ICLR.cc/2026/Conference/Submission3721/Reviewer_Y7oa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3721/Reviewer_Y7oa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785483970, "cdate": 1761785483970, "tmdate": 1762916946098, "mdate": 1762916946098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies the similarity between prompts across sequences of observations collected by an agentic LLM interacting with an environment to be the main reason which causes bad actions (actions which have a negative effect in the long-term performance) to have positive advantages. Authors relate this issue with subsequent observation-actions pairs having similar gradients although very different consequences in the environment. The authors then propose a method in which the agent LLM also acts as a critic, self-reflecting about the failure modes when interacting in the environment to modify the prompt to more effectively separate observation-action tuples.\n\nUnfortunately, the paper is full of incoherences, syntactical, grammatical and lexical mistakes which render the paper unevaluable. (see Section 2 - paragraph 1 for the clearest example)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "-"}, "weaknesses": {"value": "Unfortunately, the paper is full of incoherences, syntactical, grammatical and lexical mistakes which render the paper unevaluable. (see Section 2 - paragraph 1 for the clearest example).\n\nThe method and ideas are not clearly presented. The results presented are not clearly discussed, and conclusions supporting the claims are unclear as well. Additionally, the results do not represent a significant improvement over existing algorithms."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "The paper is written so poorly that it seems likely an LLM was used, and used incorrectly. An example paragraph illustrates this:\n\n\"LLM Reinforcement Learning Reinforcement Learning (RL during reinforcement learning) algorithms like PPO (Schulman et al., 2017) is growing extrsome emely popular which would weaken the performancebecause it can greatly help the performance through Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022a).\"\n\nGenerally, the paper is full of incoherences, syntactical, grammatical and lexical mistakes which render the paper unevaluable."}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K3Kx4sQdDT", "forum": "8nSJP4Z9oD", "replyto": "8nSJP4Z9oD", "signatures": ["ICLR.cc/2026/Conference/Submission3721/Reviewer_AYh5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3721/Reviewer_AYh5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836736041, "cdate": 1761836736041, "tmdate": 1762916945861, "mdate": 1762916945861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}