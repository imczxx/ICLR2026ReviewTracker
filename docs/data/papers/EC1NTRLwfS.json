{"id": "EC1NTRLwfS", "number": 3413, "cdate": 1757420436425, "mdate": 1759898091011, "content": {"title": "FEDEMOE: IMPROVING PERSONALIZATION ON HET- EROGENEOUS FEDERATED LEARNING VIA ELASTIC MIXTURE OF EXPERTS ARCHITECTURE", "abstract": "Heterogeneous federated learning (HtFL) has emerged as a promising approach to address heterogeneity in local computational resources and data distribution, as is common in the real world. However, existing methods cause performance degradation of model personalization because personalized and generalized knowledge are either intertwined or dominated by one of them. To address this issue, we propose a novel Elastic Mixture of Experts (EMoE) architecture on HtFL, namely FedEMoE, decoupling personalization from generalization. In detail, FedEMoE employs a multi-scale feature extraction mechanism via personalized experts to enrich personalized knowledge. Furthermore, we design an elastic shared expert to break the transferred knowledge bottleneck across heterogeneous client models. The elastic shared expert can adaptively expand or shrink according to the status of each expert by the weight spectrum analysis, respectively. Moreover, the\nsparsity of mixture of experts (MoE) alleviates the loss of personalized knowledge that typically results from dense model aggregation. Extensive experiments across statistical and model heterogeneity settings demonstrate that FedEMoE significantly outperforms state-of-the-art federated learning methods on the performance of each heterogeneous model over diverse datasets.", "tldr": "", "keywords": ["Heterogeneous federated learning", "elastic mixture of experts", "personalization", "knowledge transfer", "statistical heterogeneous"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ffb2e428b5db0cd63af514901708a12cf562d0b2.pdf", "supplementary_material": "/attachment/1d07e7c398326077f77865da7e506c582d304763.zip"}, "replies": [{"content": {"summary": {"value": "Existing heterogenous FL methods often lead to performance degradation of model personalization because personalized and generalized knowledge are either intertwined or dominated by one of them.\nThis paper proposed  Elastic Mixture of Experts (EMoE) architecture on HtFL, referred to as FedEMoE.\nThis method decouples personalization from generalization on HtFL. To this end, it develops a multi-scale knowledge extraction and a knowledge exchange mechanism to break the bottleneck of knowledge transfer. \nIt initially deploys an EMoE model architecture to highlight multiple personalized knowledge. Accoridng to the results, the proposed solution outperforms existing methods in a wide range of statistical and model heterogeneity settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strength\nDecoupling personalization from generalization is an interesting idea which is well-executed both in the proposed architecture as well as implementation.\nAnother strength is identifying an important problem. Personalized expert group (PEG) distills knowledge from the shared expert (SE) on small scale data. The SE and PEG are unable to learn local personalized knowledge from the local data. This paper does not only highlight this issue, but also solved it by introducing a knowledge exchange mechanism between SE and PEG on every client."}, "weaknesses": {"value": "The generalized to personalized knowledge transfer seems to be based on several prior works on this topic. It is not clearly which component is novel, proposed by this paper; and which part is based on the literature. \nThere are some generic claims such as higher accuracy and less resource overhead without pointing the metric used for evaluation, i.e., what type of resources this refer to."}, "questions": {"value": "-Why did the paper use “weight spectrum analysis technique” as a diagnosis tool? What are the alternatives? What did prior works use for this purpose?\n-How did the paper verify this statement? “The EMoE architecture maintains high accuracy, reduces resource overhead, and guarantees convergence by alternating expansion and shrinkage”. Currently it is not clear which metrics have been deployed to verify this claim theoretically and empirically.\n-How does assumption 2, Strong Convexity, affect the practicality of proposed solution and the proofs? What are the key challenges in relaxing/generalizing  this assumption?\n-What is the implication of Assumption 7, MoE boundedness  for the resource efficiency of the proposed solution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EcLbzXmfWH", "forum": "EC1NTRLwfS", "replyto": "EC1NTRLwfS", "signatures": ["ICLR.cc/2026/Conference/Submission3413/Reviewer_4TSh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3413/Reviewer_4TSh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761006112504, "cdate": 1761006112504, "tmdate": 1762916712516, "mdate": 1762916712516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FedEMoE, an Elastic Mixture of Experts framework for heterogeneous federated learning that decouples personalization from generalization. It uses personalized experts for multi-scale feature extraction and an elastic shared expert that adapts through weight spectrum analysis to enhance cross-client knowledge transfer. Empirical validations under statistical and model heterogeneity show that FedEMoE outperforms SOTA baselines across diverse datasets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes an interesting idea of decoupling personalization and generalization through an elastic mixture of experts, addressing a key challenge in heterogeneous federated learning."}, "weaknesses": {"value": "- Lack of hyperparameter analysis. The proposed method heavily depends on several hyperparameters, such as KD temperature ($T$), initial expansion threshold ($v_o$), smoothing factor ($\\gamma$), shared experts’ top-K, and the number of shared experts ($N$). However, no hyperparameter sensitivity analysis is provided. This omission makes the method appear highly engineered and potentially sensitive to specific settings.\n\n- Unrealistic assumption on model heterogeneity. The paper assumes heterogeneity only in the number of experts, but real-world heterogeneous FL scenarios typically involve models with different architectures, depths, or dimensions. Moreover, the experiments only use simple CNN-based models. It is unclear whether the proposed method would work well for more complex architectures such as ViTs, LLMs, or MLLMs.\n\n- Limited experimental scale. Experiments are conducted only on CIFAR and Tiny-ImageNet, which are small-scale datasets. Demonstrating scalability on larger benchmarks (e.g., ImageNet) would strengthen the paper. Additionally, it would be valuable to show results for homogeneous clients as well, since real-world scenarios may include both heterogeneous and homogeneous settings.\n\n- Computational overhead. As shown in Appendix D.6, FedEMoE incurs significant additional computational overhead (e.g., from double KD), which may limit practicality in large-scale real-world settings where both model size and the number of clients are large.\n\n- Lack of detailed explanation of Fig. 1. The caption should include a brief explanation of what the authors intend to convey in Fig. 1. I believe terms $D_i$ and $W_i$ refer to local data and local model for client $i$, respectively. However, those terms are used without definition, which reduces readability. Defining them directly in the caption would improve clarity. (Defining them only in Appendix Table 5 is insufficient for readability.)\n\n- Inconsistent terminology. The paper describes multiple personalized experts as a “personalized expert group,” but the shared experts (which are also multiple) are simply referred to as a single “shared expert.” This was confusing until I reached page 6. Clarifying this terminology would prevent misunderstanding.\n\n- Redundant and unclear statements. The paper states that sparse aggregation “retains whole client-specific weights,” but this claim is unclear and needs elaboration. Moreover, in Lines 122–123, the argument about the heavy computational overhead of prototype-based FL methods is duplicated.\n\n- Ambiguity in Eq. (4) and expert consensus. In Sec. 3.3, each sample in $D_i$ may select different experts, so the consensus output should be based on per-sample activation. However, Eq. (4) defines $a_n$ using the total selection frequency across all samples, which seems inconsistent. Why is the same $a_n$ used for all samples? The denominator of $a_n$ also seems incorrect. If MoE selects top-K experts per sample, the denominator should include $K$, not $D_i \\times \\text{top} K$. Additionally, when the SE consists of multiple experts, how is the consensus $R_s$ computed? Is it the same as Eq. (4)? Clarification is needed.\n\n\n- Readability and precision issues. The paper is difficult to follow, with weak logical connections between paragraphs (especially in the Introduction and Approach sections) and several imprecise definitions. Examples:\n    - When computing $A_g$, the paper sums over $A_i$, but it is defined as a set of $a_{i,j}$ (Line 237). How is the summation over sets performed? If $A_i$ varies per expert, $A_g$ should be written as $A_{g,i}$​.\n    - In Line 243, $W(l)$ seems to refer to the SE weight, but this is unclear.\n    - Lines 240–241 are also disconnected from the previous paragraph, making the purpose of computing $A_g$ unclear."}, "questions": {"value": "Please refer to the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AGtx6JqI3i", "forum": "EC1NTRLwfS", "replyto": "EC1NTRLwfS", "signatures": ["ICLR.cc/2026/Conference/Submission3413/Reviewer_ubJ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3413/Reviewer_ubJ1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571600492, "cdate": 1761571600492, "tmdate": 1762916712295, "mdate": 1762916712295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FedEMoE, an elastic mixture-of-experts architecture for heterogeneous federated learning (HtFL). The central idea is to decouple personalization and generalization through a personalized expert group (PEG) and an elastic shared expert (SE). The SE dynamically expands or shrinks via weight spectrum analysis, aiming to avoid overfitting or underfitting. The authors claim that this design mitigates the dilution of personalization during aggregation. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet are presented, showing improvements over several baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is ambitious in combining MoE with FL, with a relatively detailed methodology.\n2. Experiments cover multiple datasets and include ablation studies.\n3. The idea of adaptive expansion/shrinkage of shared experts is intuitively appealing."}, "weaknesses": {"value": "- The method strongly resembles existing PFL decoupling approaches. The paper does not convincingly justify why a new HtFL-specific method is needed.\n- The paper emphasizes HtFL but does not design specifically for this setting. Why not evaluate in the more natural PFL scenario, where many papers already study decoupling?\n- The design assumes a shared feature extractor and elastic shared expert, which contradicts true model heterogeneity (where architectures differ substantially). The “heterogeneity” considered is mostly variations in experts’ numbers, which is limited.\n- Comparisons omit key decoupling-based PFL methods (e.g., DualFed [1], GPFL [2], FedDecomp [3], FedCAC [4]), making the superiority claim weak.\n\n[1] Dualfed: enjoying both generalization and personalization in federated learning via hierachical representations, ACMMM24\n[2] Gpfl: Simultaneously learning global and personalized feature information for personalized federated learning, ICCV23\n[3] Decoupling general and personalized knowledge in federated learning via additive and low-rank decomposition, ACMMM24\n[4] Bold but cautious: Unlocking the potential of personalized federated learning through cautiously aggressive collaboration, ICCV23"}, "questions": {"value": "1. Why is this work framed under HtFL rather than PFL? Wouldn’t FedEMoE be more naturally positioned as a PFL approach? What is the unique HtFL-specific design?\n2. What is the true extent of model heterogeneity in experiments? Are the models fundamentally different in architecture (e.g., ResNet vs VGG), or only in the number of experts?\n3. Can the authors clarify how the elastic expansion/shrinkage significantly differs from existing dynamic capacity adjustment works in MoE literature?\n4. Many gains reported are large (30–40%). Can the authors explain why such dramatic improvements appear, when prior PFL/HtFL works typically show smaller margins?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yLy34dwEee", "forum": "EC1NTRLwfS", "replyto": "EC1NTRLwfS", "signatures": ["ICLR.cc/2026/Conference/Submission3413/Reviewer_WZWB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3413/Reviewer_WZWB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833670843, "cdate": 1761833670843, "tmdate": 1762916711934, "mdate": 1762916711934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to improve the personalization performance on heterogeneous federated learning (HtFL) by exploiting an Elastic Mixture of Experts (EMOE) architecture. The authors try to address the limitations of existing HtFL methods without relying on dense model aggregation that dilutes personalized knowledge. The authors then propose the EMOE framework to decouple two complementary knowledge representations, including personalized knowledge via personalized experts and generalized knowledge via an elastic shared expert. The proposed method extracts the multi-scale features and adaptively evolves the shared expert through the weight spectrum analysis mechanism. Based on this design, the authors utilize the Kullback-Leibler divergence loss for knowledge exchange and the standard local training loss to train the neural model for the proposed method. Evaluations on diverse datasets show that the proposed method outperforms the baselines in terms of prediction accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, this paper represents a meaningful problem in heterogeneous federated learning. The whole idea seems to be effective. The authors find that existing HtFL methods rely on dense model aggregation or intertwined parameters, missing fine-grained personalized knowledge and forcing compromises on local specialization critical for effective personalization. The authors then capture the global generalized knowledge from an elastic shared expert and utilize the fine-grained, multi-scale personalized knowledge from personalized experts to improve the personalization performance."}, "weaknesses": {"value": "**Major Weaknesses:**\n\nOverall, this paper has some merits, but there are a few weaknesses that stop me from giving a higher rating. My major concerns are as follows.\n\n(1) How to determine whether the changes in the singular-value decay rate really reflect an expert's knowledge redundancy or underfitting? Does this change in the state represent a change in expert ability?\n\n(2) During bidirectional distillation, both distributions are in a constantly changing state, which may lead to gradient oscillations or knowledge drift. Has the stability of bidirectional KL optimization been analyzed? In addition, will the asymmetric capacity between PEG and SE affect the convergence of bidirectional distillation?\n\n(3) MoE's performance heavily relies on the accuracy of expert selection. If the gating network frequently activates different experts across clients in a highly non-identical data setup, how can the model ensure that global knowledge is still consistently aggregated?\n\n(4) Why did the authors choose to scale capacity by adding or merging sub-experts instead of increasing or decreasing the parameter width of experts? After expansion or merging, how is the new expert set kept compatible with the current gating structure and activation policy?\n\n(5) Due to the uneven activation frequency of experts, there may be experts who have not been visited for a long time. Do these experts need to participate in the decision-making process of expansion/shrinkage?\n\n\n**Minor Weaknesses:**\n\nHere are two minor questions:\n\n(1) There seems to be no clear definition of \"multi-scale\" in the multi-scale feature extraction mechanism that PEG is responsible for. The authors could give more details on this part.\n\n(2) In Table 3, as the number of clients increases, why is the gap between the proposed method and other methods narrowing on the CIFAR-100 task, while the gap is widening on the Tiny-Imagenet task? The author should provide a brief explanation of the reasons for the experimental results in Section 4.6."}, "questions": {"value": "Please clarify my concerns in the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fHA8yeLEGa", "forum": "EC1NTRLwfS", "replyto": "EC1NTRLwfS", "signatures": ["ICLR.cc/2026/Conference/Submission3413/Reviewer_6cws"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3413/Reviewer_6cws"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841607576, "cdate": 1761841607576, "tmdate": 1762916711555, "mdate": 1762916711555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}