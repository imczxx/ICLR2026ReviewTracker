{"id": "zZ2uCL4Ixt", "number": 17413, "cdate": 1758275709840, "mdate": 1759897176842, "content": {"title": "Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised Molecular Graph Pretraining", "abstract": "High-quality molecular representations are essential for property prediction and molecular design, yet large labeled datasets remain scarce. While self-supervised pretraining on molecular graphs has shown promise, many existing approaches either depend on hand-crafted augmentations or complex generative objectives, and often rely solely on 2D topology, leaving valuable 3D structural information underutilized. To address this gap, we introduce C-FREE (Contrast-Free Representation learning on Ego-nets), a simple framework that integrates 2D graphs with ensembles of 3D conformers. C-FREE learns molecular representations by predicting subgraph embeddings from their complementary neighborhoods in the latent space, using fixed-radius ego-nets as modeling units across different conformers. This design allows us to integrate both geometric and topological information within a hybrid Graph Neural Network (GNN)-Transformer backbone, without negatives, positional encodings, or expensive pre-processing. Pretraining on the GEOM dataset, which provides rich 3D conformational diversity, C-FREE achieves state-of-the-art results on MoleculeNet, surpassing contrastive, generative, and other multimodal self-supervised methods. Fine-tuning across datasets with diverse sizes and molecule types further demonstrates that pretraining transfers effectively to new chemical domains, highlighting the importance of 3D-informed molecular representations.", "tldr": "C-FREE is a simple self-supervised framework that jointly leverages 2D graphs and 3D conformers to learn molecular representations without negatives, achieving state-of-the-art results and strong transfer across chemical domains.", "keywords": ["Pretraining", "Graph", "Self", "Supervised", "Non", "Contrastive", "SSL", "Self", "Predictive", "Molecular", "Foundation", "Model", "Multimodal", "Conformers"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e700e7c5c8258c6586c19040b57496f1f7183154.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a contrast-free predictive framework for molecular self-supervised pretraining that jointly learns from 2D molecular graphs and 3D conformer ensembles.\nThe main idea is to:\n\n- Divide each molecule into multiple k-EgoNet subgraphs,\n- Construct context–target pairs for latent-space prediction, and\n- Integrate 3D conformer information through a hybrid GINE + SchNet + Transformer backbone.\n\nThe proposed method reports state-of-the-art results on benchmarks including MoleculeNet, Kraken, and Drugs-75K."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation and simplicity\n\nThe paper clearly identifies the weaknesses of existing contrastive or generative molecular SSL approaches—namely, difficulties in defining negatives, instability in discrete reconstruction, and inconsistency in augmentation design. The proposed contrast-free predictive formulation provides a simple and effective alternative that avoids these issues.\n\n2. Theoretical grounding\n\nThe use of DeepSets-based subgraph aggregation provides an expressive model comparable to ESAN.\nThe paper includes a theoretical argument showing that the proposed framework is more expressive than 1-WL.\n\n3. Comprehensive experimentation\n\nThe experiments span a wide range of evaluations, including frozen backbone probing, full fine-tuning, label-efficiency analysis, ablations, and expressiveness studies. The design maintains strong consistency between pretraining and fine-tuning (EgoNet pretraining combined with DeepSets fine-tuning). The ablation on the predictor component effectively demonstrates its role in avoiding representation collapse."}, "weaknesses": {"value": "1. Limited technical depth\n\nThe C-FREE framework closely follows existing contrast-free latent prediction paradigms such as BYOL, I-JEPA, and BGRL. Its novelty mainly lies in the use of EgoNet-based subgraph sampling and the integration of 2D and 3D modalities.\nWhile applying such modern self-supervised techniques to the molecular domain is reasonable and well executed, the paper lacks deeper analysis of what kind of chemical or structural knowledge the model actually captures — for example, whether the learned representations encode functional groups, chirality, or spatial interaction patterns.\nIncluding such analysis would have strengthened the scientific novelty of the work, showing that the model learns chemically meaningful concepts rather than only demonstrating performance gains.\nOverall, the contribution feels more like a well-engineered domain adaptation than a fundamentally new conceptual advance.\n\n2. Baseline comparability\n\nIt is not entirely clear whether all baseline models were trained with the same data, number of conformers, or backbone settings, which makes the performance comparisons less transparent.\n\n3. Choice of encoders not well justified\n\nThe method relies on GINE for 2D graphs and SchNet for 3D conformers, even though newer and more expressive GNN architectures—such as GNS, EGNN, SphereNet, GemNet, or SE(3)-Transformer—are now available.\nThe authors do not explain why these relatively older encoders were chosen or whether using stronger graph networks might yield further gains."}, "questions": {"value": "1. Could the authors elaborate on what specific chemical or structural knowledge the learned embeddings capture (e.g., functional groups, chirality, or geometric motifs)?\n\n2. Were all baseline models trained on identical data and conformer setups? If not, how might differences affect the reported comparisons?\n\n3. Why were GINE and SchNet chosen as encoders instead of more modern architectures (e.g., EGNN, SphereNet, GemNet, or SE(3)-Transformer)?\n\n4. How sensitive is the model’s performance to the choice of k in the EgoNet subgraph partitioning?\n\n5. Could the authors provide more insight into how the model’s latent predictions avoid collapse beyond the predictor ablation results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lim3exPTov", "forum": "zZ2uCL4Ixt", "replyto": "zZ2uCL4Ixt", "signatures": ["ICLR.cc/2026/Conference/Submission17413/Reviewer_cps9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17413/Reviewer_cps9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636807111, "cdate": 1761636807111, "tmdate": 1762927310362, "mdate": 1762927310362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a simple SSL framework, C-FREE, which integrates 2D topology and 3D structural information for learning high-quality molecular representations. It models fixed-radius ego-nets as context and their complementary subgraphs as targets, then predicts one latent embedding from the other via a hybrid GNN-Transformer backbone—no negatives, positional encodings, or heavy preprocessing required. Pretrained on GEOM, C-FREE sets superior performance on MoleculeNet, outperforming contrastive, generative, and other multi-modal SSL methods. Fine-tuning across varied downstream datasets confirms its enhanced transferability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear limitations: This paper clearly points out the main weaknesses of previous molecular graph SSL methods and gives a thorough review of contrastive and generative approaches.\n- Novelty: This paper introduces a latent-predictive SSL framework for molecular graphs that provides a refreshing perspective in this field beyond traditional contrastive or generative approaches. Its core design—using ego-net to define context subgraphs and treating the remaining parts as targets, and training the model to bring their embeddings closer together—offers a distinctive method."}, "weaknesses": {"value": "- Lack of clear motivation and justification: This paper does not sufficiently explain why the embeddings of the context and target ego-nets should be close, leaving the theoretical and empirical motivation unclear. Similarly, while the target encoder is updated via EMA for stability, there is no comparison with alternative methods or evidence showing that EMA is particularly effective for the proposed architecture.\n- Inconsistent and limited experimental setup: The experimental evaluation has several inconsistencies. Table 1 (linear probing) and Table 2 (full finetune) use different sets of baselines, and some baselines, especially in Table 2, exclude more recent or multimodal models, weakening the fairness of comparisons. In addition, the \"4.2.2 label-efficient fine tune\" and \"4.2.3 modality ablation\" experiments use different datasets, making it difficult to reliably verify the proposed model's effectiveness.\n- Ambiguity: The model naming is somewhat confusing. Table 1 includes \"C-FREE_MM_LIN\" and \"C-FREE_MM_DS\", whereas Table 2 reports \"C-FREE_MM_FULL\". This inconsistency makes it unclear whether \"C-FREE_MM_FULL\" uses whole-molecule or subgraph-aggregated embeddings.\n- Limited interpretation: This paper mainly reports numerical results without deeper discussion of why the proposed method performs well.\n- Notations: References to \"Table10\" in Section 4.2.2 should be \"Table 4\", and in Table 3 the bold on \"3D-FINE-TUNED\" highlights a non-best result. These should be corrected."}, "questions": {"value": "- This paper does not clearly explain why the context and target should have similar embeddings. The motivation for bringing them closer in latent space is unclear.\n- The baseline models differ between linear probing and full finetune experiments. If there is no specific reason for this, it needs to be aligned the baseline between them. Especially, linear probing experiments should incorporate more recent baseline models, including multimodal models, to ensure a fair and comprehensive comparison.\n- The “4.2.2 label-efficient fine tune” and “4.2.3 modality ablation” experiments use different datasets, which makes it difficult to evaluate the effectiveness of the proposed model.\n- The naming “C-FREE_MM-FULL” in table 2 is unclear whether it uses whole-molecule or subgraph-aggregated embeddings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R7y0EJcf04", "forum": "zZ2uCL4Ixt", "replyto": "zZ2uCL4Ixt", "signatures": ["ICLR.cc/2026/Conference/Submission17413/Reviewer_JgrY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17413/Reviewer_JgrY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639465865, "cdate": 1761639465865, "tmdate": 1762927309352, "mdate": 1762927309352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "summary:\nThe paper proposes C-FREE, a non-contrastive, non-generative multimodal self-supervised molecular pretraining framework that integrates 2D topology and 3D conformations, replacing traditional contrastive/generative objectives with a JEPA-style latent predictive alignment.\nI have the following questions and concerns:\n\n1. Dependence on 3D conformations.\nThe best-performing variant relies on the combination of 2D molecular graphs and 3D conformers. However, obtaining high-quality 3D conformations can be computationally expensive, which may limit the practical applicability of this method to large datasets. How can this dependency on costly 3D data be mitigated? Are the 3D conformers in this work generated directly using tools such as RDKit, or do they require more precise yet expensive methods like molecular dynamics simulations or quantum chemical optimizations?\n2. Chemically meaningful subgraph partitioning.\nThe proposed approach uses fixed-radius k-hop EgoNets to generate subgraphs, analogous to image patches, but does not incorporate chemical domain knowledge such as functional groups or bond types. Have the authors experimented with chemically informed subgraph partitions (e.g., Murcko scaffolds or functional-group-based segmentation)? If so, how do the results compare?\n3. Missing stronger baselines.\nThe paper lacks comparisons with more recent and powerful baselines such as SCAGE, Uni-Mol and VideoMol which have demonstrated strong performance in molecular representation learning. Including such comparisons would help clarify the relative advantages of C-FREE.\n- Qiao J, Jin J, Wang D, et al. A self-conformation-aware pre-training framework for molecular property prediction with substructure interpretability[J]. Nature Communications, 2025, 16(1): 4382.\n- Zhou, G., Gao, Z., Ding, Q., Zheng, H., Xu, H., Wei, Z., ... & Ke, G. (2023). Uni-mol: A universal 3d molecular representation learning framework.\n- Xiang, H., Zeng, L., Hou, L., Li, K., Fu, Z., Qiu, Y., ... & Cheng, F. (2024). A molecular video-derived foundation model for scientific drug discovery. Nature Communications, 15(1), 9696."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It proposes a predictive objective based on k-EgoNet subgraphs. The model learns representations by predicting the embedding of a subgraph (target) from its complementary neighborhood (context). This integrates 2D and 3D information, and is valuable because it requires neither negative samples (avoiding the difficulties of contrastive learning) nor input graph reconstruction (avoiding the discrete reconstruction problem of generative tasks)."}, "weaknesses": {"value": "1. Dependence on 3D conformations.\nThe best-performing variant relies on the combination of 2D molecular graphs and 3D conformers. However, obtaining high-quality 3D conformations can be computationally expensive, which may limit the practical applicability of this method to large datasets. \n\n2. Chemically meaningful subgraph partitioning.\nThe proposed approach uses fixed-radius k-hop EgoNets to generate subgraphs, analogous to image patches, but does not incorporate chemical domain knowledge such as functional groups or bond types. \n\n3. Missing stronger baselines.\nThe paper lacks comparisons with more recent and powerful baselines such as SCAGE, Uni-Mol and VideoMol which have demonstrated strong performance in molecular representation learning. Including such comparisons would help clarify the relative advantages of C-FREE."}, "questions": {"value": "1.\tHow can this dependency on costly 3D data be mitigated? Are the 3D conformers in this work generated directly using tools such as RDKit, or do they require more precise yet expensive methods like molecular dynamics simulations or quantum chemical optimizations?\n2.\tHave the authors experimented with chemically informed subgraph partitions (e.g., Murcko scaffolds or functional-group-based segmentation)? If so, how do the results compare?\n3.\tCould some comparative experiments be added?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bsuMPS8ytO", "forum": "zZ2uCL4Ixt", "replyto": "zZ2uCL4Ixt", "signatures": ["ICLR.cc/2026/Conference/Submission17413/Reviewer_8F9v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17413/Reviewer_8F9v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895417244, "cdate": 1761895417244, "tmdate": 1762927308873, "mdate": 1762927308873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces C-FREE (Contrast-Free Representation learning on Ego-nets), a self-supervised molecular graph pretraining framework that integrates both 2D topology and 3D conformational information without using contrastive or generative objectives. The key idea is to predict the embeddings of subgraphs (ego-nets) from their complementary neighborhoods, trained in a latent predictive space using a hybrid GINE + SchNet + Transformer architecture. The approach avoids negative sampling, clustering (e.g., METIS), and expensive data augmentations. Empirical evaluations on MoleculeNet, Kraken, and Drugs-75K show strong performance in both classification and regression, often surpassing contrastive and generative baselines. The method also demonstrates label efficiency in low-data regimes and supports multimodal transfer between 2D and 3D settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The model achieves competitive or superior performance across several benchmarks (MoleculeNet, Kraken, Drugs-75K), including both frozen and fine-tuned evaluations. Notably, even the 2D-only variant performs strongly, indicating robust representation learning.\nComprehensive evaluation\n- The experiments are extensive: frozen vs. full fine-tuning, multimodal vs. unimodal ablations, low-label settings, and theoretical expressiveness studies. This provides a well-rounded validation of the approach.\n- The paper is well-organized, and implementation details, dataset choices, and ablations are clearly presented. Promises to release code and checkpoints further strengthen reproducibility."}, "weaknesses": {"value": "- While the paper emphasizes “contrast-free multimodal learning,” the overall framework is a relatively straightforward adaptation of existing JEPA/BYOL principles with molecular-specific encoders. The main novelty lies in applying ego-nets rather than METIS clusters and fusing 2D/3D modalities, a valuable but not significant step.\n- The paper does not sufficiently dissect how 3D information improves over 2D or why multimodal fusion works. Results indicate 3D contributes more, but mechanistic or representational analyses (e.g., embedding alignment, modality redundancy) are absent.\n- Some benchmarks (e.g., MoleculeNet [1]) are relatively small and saturated.\n- More recent large-scale baselines (like ChemBERTa-2 [2] , Uni-Mol 2 [3], GEM-2 [4], MolFM [5]) are not compared.\n- Theoretical results (expressiveness > 1-WL) are interesting but neither novel nor essential to the main empirical contributions. They don’t directly translate into new design insights or explain performance improvements.\n- While generally clear, some methodological sections are dense and formula-light. Diagrams (e.g., Fig. 1–2) could better illustrate modality fusion and prediction flow. “Contrast-free” might be slightly overstated, as the predictor/target asymmetry still performs implicit alignment akin to self-distillation.\n\n[1] Wu, Zhenqin, et al. \"MoleculeNet: a benchmark for molecular machine learning.\" Chemical science 9.2 (2018): 513-530.\n\n[2] Ahmad, Walid, et al. \"Chemberta-2: Towards chemical foundation models.\" arXiv preprint arXiv:2209.01712 (2022).\n\n[3] Ji, Xiaohong, et al. \"Uni-mol2: Exploring molecular pretraining model at scale.\" arXiv preprint arXiv:2406.14969 (2024).\n\n[4] Liu, Lihang, et al. \"GEM-2: next generation molecular property prediction network by modeling full-range many-body interactions.\" arXiv preprint arXiv:2208.05863 (2022).\n\n[5] Luo, Yizhen, et al. \"Molfm: A multimodal molecular foundation model.\" arXiv preprint arXiv:2307.09484 (2023)."}, "questions": {"value": "- Although the method claims simplicity, using multiple conformers and subgraph sampling could still be computationally heavy. The paper does not quantify pretraining cost or training speed relative to baselines like GraphMVP or MoleBlend.\n- Evaluation on more chemically relevant or 3D-heavy datasets (e.g., QM9 [6], OC20[7], PCQM4Mv2 [8], TDC [9], or SPICE [10]) would strengthen claims of geometric generalization.\n\n[6] Ramakrishnan, Raghunathan, et al. \"Quantum chemistry structures and properties of 134 kilo molecules.\" Scientific data 1.1 (2014): 1-7.\n\n[7] Chanussot, Lowik, et al. \"Open catalyst 2020 (OC20) dataset and community challenges.\" Acs Catalysis 11.10 (2021): 6059-6072.\n\n[8] Hu, Weihua, et al. \"Ogb-lsc: A large-scale challenge for machine learning on graphs.\" arXiv preprint arXiv:2103.09430 (2021).\n\n[9] Huang, Kexin, et al. \"Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development.\" arXiv preprint arXiv:2102.09548 (2021).\n\n[10] Eastman, Peter, et al. \"Spice, a dataset of drug-like molecules and peptides for training machine learning potentials.\" Scientific Data 10.1 (2023): 11."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K3YAJY63Hq", "forum": "zZ2uCL4Ixt", "replyto": "zZ2uCL4Ixt", "signatures": ["ICLR.cc/2026/Conference/Submission17413/Reviewer_FBXr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17413/Reviewer_FBXr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762114445310, "cdate": 1762114445310, "tmdate": 1762927308231, "mdate": 1762927308231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}