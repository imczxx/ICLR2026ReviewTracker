{"id": "wONGmN1hXL", "number": 11132, "cdate": 1758190104187, "mdate": 1759897606072, "content": {"title": "CLUE-VAD: Structured Semantic Clues for Understanding Explainable Events in Video Anomaly Detection", "abstract": "Weakly Supervised Video Anomaly Detection (WSVAD) aims to identify rare and abnormal events in long untrimmed videos using only video-level labels. While recent approaches have leveraged multimodal learning and pretrained language models, they often treat scenes holistically, failing to provide fine-grained or interpretable insights into the source of anomalies. In this paper, we introduce CLUE-VAD, a novel framework that explicitly decomposes each video segment into three semantically grounded components—Action, Environment, and Object—termed as Textual CLUEs. This structured decomposition enables the model to disentangle overlapping contextual cues and reason about anomalies in a human-aligned and interpretable manner. Our approach comprises three key modules: (i) the Witness Module, which automatically generates dense, clue-specific captions and CLUE-based features using a large-scale video-language model; (ii) the Detective Module, which employs a learnable clue-aware fusion mechanism to dynamically quantify the importance of each semantic clue for anomaly prediction; and (iii) the Reporter Module, which provides fine-grained explanations by attributing anomaly scores to specific keywords and clues. We also construct the CLUE-VAD Benchmark, an enriched evaluation resource with structured segment-level captions for existing WSVAD datasets. Experiments on UCF-Crime and XD-Violence demonstrate that CLUE-VAD achieves strong performance in text-only settings while offering transparent and context-aware anomaly reasoning. Our framework bridges the gap between machine prediction and human interpretation, making it a practical and trustworthy solution for real-world surveillance.", "tldr": "", "keywords": ["Video Anomaly Detection"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d34359915281862850feedd228d8e996508069c9.pdf", "supplementary_material": "/attachment/132b30978935a1f72381c405a9ef01aff64fc8e9.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes CLUE-VAD, a weakly supervised video anomaly detection framework designed to improve interpretability through structured semantic reasoning. It introduces a clue-based decomposition that represents each video segment in terms of Action, Environment, and Object cues, allowing the model to reason in a more human-aligned way. In addition, the authors construct the CLUE-VAD Benchmark by augmenting UCF-Crime and XD-Violence with structured captions and clue-specific keywords, supporting both detection and interpretability evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a novel interpretability framework for weakly supervised video anomaly detection by explicitly decomposing each segment into Action, Environment, and Object clues. Through category-aware clue fusion and clue-level attribution, the method provides fine-grained, human-understandable explanations of why an anomaly occurs.\n\n2. The proposed CLUE-VAD Benchmark augments existing datasets (UCF-Crime, XD-Violence) with structured, segment-level captions and clue-specific keywords.\n\n3. The method is organized into three intuitive modules—Witness, Detective, and Reporter, each with a clearly defined function."}, "weaknesses": {"value": "1. The paper presents an interesting clue-based interpretability design. However, the evaluation of interpretability remains purely qualitative, limited to visualizations without human or quantitative validation.\n\n2. The entire framework depends heavily on captions generated by MLLMs, which serve as the primary features for anomaly detection. However, the paper does not analyze the quality, stability, or bias of these captions. If the generated descriptions are inaccurate or temporally misaligned, anomaly scores may become unreliable.\n\n3. Although the core novelty lies in text-based reasoning, the results in Table 1 show that adding visual features substantially improves performance. However, there are no ablation studies quantifying the contribution of visual versus textual modalities.\n\n4. The main text presents only two quantitative tables, leaving the experimental validation relatively limited. Additional experiments, such as ablations, robustness tests, or interpretability metrics, would further strengthen the paper."}, "questions": {"value": "1. How to quantitatively evaluate the interpretability of the proposed clue-based framework beyond qualitative visualizations?\n\n2. How sensitive is the framework to caption quality and noise, and how would performance change with alternative captioning models?\n\n3. What is the individual contribution of textual and visual modalities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zOuy0wqNBP", "forum": "wONGmN1hXL", "replyto": "wONGmN1hXL", "signatures": ["ICLR.cc/2026/Conference/Submission11132/Reviewer_vbUG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11132/Reviewer_vbUG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706795900, "cdate": 1761706795900, "tmdate": 1762922303093, "mdate": 1762922303093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed a interesting framework that generates three kinds of Textual CLUEs (Action, Environment and Object) for each video segment. Such contextual cues based structure helps the understanding about anomalies in a human-aligned and interpretable manner. Three key modules: (i) the Witness Module,  (ii) Detective Module and (iii) Reporter Module are designed to utilize the anomaly clues. They also constructed another Benchmark, providing structured segment-level captions for existing WSVAD datasets. The experiments show promising performance in both standard and text-only settings, while offering transparent and context-aware anomaly reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper’s most compelling aspect is the explicit A/E/O semantic decomposition that aligns model reasoning with how humans assess anomalous events. And the corresponding three-stage architecture, i.e. Witness, Detective and Reporter, also maps naturally to the steps of observe, infer and explain.  \n2. The Detective module’s category-aware fusion learns per-class importance over A/E/O, leading the model to emphasize the most relevant clue (e.g., Environment for explosions, Object for stealing), and the paper visualizes these learned patterns across categories and over time.\n3. Empirically, the method achieves competitive accuracy against explainable VAD baselines while keeping transparency."}, "weaknesses": {"value": "1. The paper presents CLUE-VAD as “a novel and interpretable framework for WSVAD,” supported by several qualitative figures to support it. However, the current evidence for its interpretability is predominantly visual rather than quantitative.\n2. Another limitation involves the reliability of automatically generated captions that both the method and benchmark rely on. Since each video segment is processed through InternVideo 2.5 to generate Action, Environment and Object descriptions, there is a risk that noise or generic or incorrect wording could propagate into training data and subsequently affect testing outcomes. Especially many wording may not be directly related to the anomaly as presented in Table S3.\n3. The efficiency of inference is not addressed in the discussion. Steps such as multi-caption prompting, clue encoding and attribution are likely to introduce latency and increase memory usage.\n4. In Table 1, compared methods seem to utilize different backbone capacities or feature stacks (e.g., ViT-B vs. ViT-L), which can blur distinctions between architectural improvements and model size benefits. This suggests potential unfair comparisons that require further clarification.\n5. The CLUE-wise score decomposition in Fig. 5 does not seem to align well with the anomaly events depicted in Fig. 4.\n6. The process for scoring anomalies is unclear, particularly concerning the roles of the detective and reporter modules. What distinguishes the category-aware learnable weights in the detective module from the attention score alpha? Both are utilized as weights for features of different clues. Similarly, the role of fused feature E_r is not well described, while z_t is used for the scorer. Some steps are better to defined by formulations. It would be helpful to label these elements in Fig. 2 for clarity.\n7. The clue weights are trained and stored within a case log, necessitating a category classifier to predict them during inference stages. However, details regarding this aspect are missing both in the main text and Appendix. Moreover, the performance of such category classifier will also affect the final performance.\n8. Overall, the experiments and ablation studies are insufficient."}, "questions": {"value": "1. Could the authors provide an intuitive explanation for why minimizing the negative entropy of the attention weights in Eq. 6 is beneficial?\n2. What are the specific training and testing procedures for a text-only configuration?\n3. How are visual features incorporated during both the training and inference stages?.\n4. Please discuss the differences and connections between the category-aware learnable weights in the detective module and the attention score alpha in the reporter module?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UGimwPMQXH", "forum": "wONGmN1hXL", "replyto": "wONGmN1hXL", "signatures": ["ICLR.cc/2026/Conference/Submission11132/Reviewer_3NYN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11132/Reviewer_3NYN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807305125, "cdate": 1761807305125, "tmdate": 1762922302721, "mdate": 1762922302721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CLUE-VAD introduces a structured semantic decomposition for weakly supervised video anomaly detection (WSVAD). Each video segment is decomposed into Textual CLUEs (Action, Environment, Object) using a large video-language model. The framework includes: (i) Witness Module for dense clue-specific captioning and feature extraction; (ii) Detective Module with learnable clue-aware fusion; and (iii) Reporter Module for keyword-level anomaly attribution. A new CLUE-VAD Benchmark with segment-level structured captions is proposed. Results on UCF-Crime and XD-Violence show competitive performance in text-only settings with improved interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Interpretability: First to decompose anomalies into Action/Environment/Object CLUEs — highly human-aligned and novel.\nReporter Module enables keyword-level attribution, a major step toward trustworthy VAD.\nCLUE-VAD Benchmark fills a critical gap in structured evaluation for WSVAD.\nStrong text-only performance on UCF-Crime (↑2.1% AUC) and XD-Violence (↑1.8%).\nClear motivation and modular design; figures effectively show attribution heatmaps."}, "weaknesses": {"value": "Witness Module relies heavily on pretrained video-language models (e.g., CLIP-ViP) — no ablation on frozen vs. fine-tuned VLM or robustness to domain shift.\nDetective fusion uses simple MLP weighting; lacks comparison to attention-based or graph-based clue interaction modeling.\nBenchmark construction not detailed: how are CLUE captions generated/verified? Human annotation cost? Inter-annotator agreement?\nNo real-world deployment analysis (e.g., latency, false positive breakdown by clue type).\nLimited failure case analysis — e.g., when all CLUEs are normal but combination is anomalous (e.g., \"person + knife + playground\")."}, "questions": {"value": "How are CLUE captions in the benchmark generated? Please report annotation protocol, cost, and Fleiss’ κ for inter-annotator agreement.\nCan the Detective Module model inter-clue interactions (e.g., Action × Object)? Ablate against bilinear pooling or GNN.\nHow robust is the Witness Module to VLM domain shift (e.g., indoor vs. dashcam)? Test on out-of-distribution clips."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kHJb6KVxem", "forum": "wONGmN1hXL", "replyto": "wONGmN1hXL", "signatures": ["ICLR.cc/2026/Conference/Submission11132/Reviewer_TmEn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11132/Reviewer_TmEn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905366452, "cdate": 1761905366452, "tmdate": 1762922302365, "mdate": 1762922302365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}