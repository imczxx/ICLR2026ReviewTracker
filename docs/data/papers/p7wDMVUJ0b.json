{"id": "p7wDMVUJ0b", "number": 4688, "cdate": 1757746746740, "mdate": 1759898019934, "content": {"title": "Rethinking Defense for Computer-Use Agents: Context Deception Attacks are Simple to Defend", "abstract": "Computer-use agents powered by vision-language models (VLMs) have significantly advanced human-computer interaction. However, they remain vulnerable to \\textit{context deception attacks}, an emerging threat where adversaries embed misleading content into the agent's operational environment (such as a malicious pop-up window) to hijack agent behavior. As recent benchmarks highlight the severity of these attacks, initial studies have shown that conventional defenses like direct prompting are largely ineffective, fostering a perception that these attacks are a difficult and unsolved challenge. In this paper, we challenge this conclusion, arguing that the perceived difficulty is an artifact of the defense paradigms studied, not an inherent property of the attacks themselves. We introduce in-context defense, a surprisingly simple paradigm that leverages in-context learning to prove our claim. By augmenting the agent’s context with a minimal set of exemplars, we guide it to perform explicit defensive reasoning before action planning, effectively immunizing it from deception. Experiments show this method is remarkably effective, reducing up to 91.2\\% of pop-up window attacks and achieving near-perfect defense on some other attacks, a stark contrast to the failures of prior approaches. Our work delivers two critical insights: (1) the problem of context deception is far more tractable than previously believed, and (2) teaching an agent a reasoning process (defense-first analysis), rather than just giving it a rule, is the key to robust defense.", "tldr": "", "keywords": ["Computer-use agents"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0cb7af0151aa1893ae28dce3ecdec65aa0e7301.pdf", "supplementary_material": "/attachment/ffa186e3fa96cbe2f905006b100b7c4293aa280b.zip"}, "replies": [{"content": {"summary": {"value": "This paper challenges the prevailing assumption that context deception attacks (e.g., fake pop-ups, deceptive HTML attributes) in a computer-use agent’s observation space are inherently difficult to defend against. This paper argues that prior failures stem not from the sophistication of the attacks but from the inadequacy of abstract instruction-based defenses (e.g., “ignore pop-ups”). It proposes in-context defense, a method that prepends a small set of few-shot exemplars demonstrating a defense-first CoT reasoning process before action planning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the critical and highly relevant problem of securing computer-use agents, a key barrier to their safe and reliable deployment in the real world.\n2. The experiments on reasoning order, exemplar quantity, and OOD generalization provide valuable mechanistic insights into why the method works.\n3. The defense requires no model retraining, fine-tuning, or external modules, making it easy to deploy.\n4. The paper is well-written."}, "weaknesses": {"value": "1. The proposed method is a direct application of well-established techniques (few-shot ICL + CoT) to a specific attack surface, though claimed as a new \"paradigm\", which makes it more like a case study. Regarding attack sources, methodologies, algorithms, or architectures, there is little novel contribution, such as how to select example samples or quantify sample quality.\n2. The effectiveness of the method is dependent on manually curated exemplars, but the selection and creation process lacks a principled framework, failing to specify how exemplars were chosen or validated. The description of being \"randomly sampled\" and then subject to \"human quality control\" is vague and not reproducible. This raises concerns about potential \"cherry-picking\" and makes it difficult to generalize the findings.\n3. The paper claims that these attacks are \"simple to defend.\" This is misleading. While the implementation is simple (adding text to a prompt), the creation of effective defense content requires domain expertise and careful manual labor for each attack type. The approach does not offer a \"simple\" scalable solution for new or evolving threats."}, "questions": {"value": "1. The core contribution appears to be the application of few-shot ICL with CoT, a well-established technique. Could you please clarify what you view as the fundamental conceptual novelty of your work beyond demonstrating that a standard method is effective in the CUA security domain?\n2. Could you provide a detailed protocol for exemplar selection beyond the high-level description? Specifically: \n(a) What criteria determined whether a sampled input was “representative”? \n(b) How many candidate exemplars were screened before selecting the final three per attack type? \nProviding a more systematic representation method will significantly improve readability and reproducibility.\n3. The paper's title and main argument assert that these attacks are \"simple to defend.\" However, your method relies on a non-trivial, manual, and expert-driven process of crafting exemplars for each attack category. How does this manual workload align with the claim of simplicity, and how do you envision this approach scaling to a dynamic landscape with new, unforeseen deception attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vbeo9j2nP4", "forum": "p7wDMVUJ0b", "replyto": "p7wDMVUJ0b", "signatures": ["ICLR.cc/2026/Conference/Submission4688/Reviewer_pxv9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4688/Reviewer_pxv9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994569029, "cdate": 1761994569029, "tmdate": 1762917511990, "mdate": 1762917511990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an in-context defense paradigm for computer-use agents (CUAs). Specifically, rather than only giving abstract instructions, the defense provides agents with a set of exemplars that includes defensive CoT (detection → justification → mitigation) before action planning. Although the technique is simple, the defense shows effectiveness against current context deception attacks (e.g., ~91% ASR reduction on pop-ups, ~75% on environment injection, and near-perfect performance on environmental distractions)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-motivated towards the current threat to the CUAs.\n- The defense is easy to deploy and is effective against current attacks."}, "weaknesses": {"value": "- The methodology contribution of this defense is limited as it only uses in-context + CoT. Some other regular methods, such as whether including some few-shot defensive learning could improve the performance, is not discussed.\n- The defense is strongly limited by that the defender must have knowledge of the possible attacks. Although Figure 9 and Table 4 show that it can generalize to OOD pop-ups, however, whether this defense knowledge is transferable between different types of attacks is unclear. As newer attacks emerge, this method will need to collect them and the longer the provided context length, make it less scalable.\n- The method is strongly limited by using exemplars per attack family. Although Figure 9 and Table 4 show that it can generalize to OOD pop-ups, it is unclear whether defense knowledge transfers across attack types. As new attacks emerge, the defense may require frequent exemplar updates, which might not be scalable.\n- There is no ablation on CoT (w/o CoT, or simplified version of current CoT)."}, "questions": {"value": "- Please update all references to their published versions.\n- What are the false cases, and what will affect the defense performance? How would factors such as UI changes (dark mode, responsive layouts) affect robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lC9L6l10vN", "forum": "p7wDMVUJ0b", "replyto": "p7wDMVUJ0b", "signatures": ["ICLR.cc/2026/Conference/Submission4688/Reviewer_oWnt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4688/Reviewer_oWnt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994571972, "cdate": 1761994571972, "tmdate": 1762917511797, "mdate": 1762917511797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reveals that the perceived difficulty of defending Computer-Use Agents against context deception attacks stems from ineffective defense paradigms, and that these attacks are in fact simple to defend. The authors systematically challenge this perception, which arose from the failure of prior defenses like abstract instructions , by introducing a new \"in-context defense\" paradigm. This method leverages in-context learning, augmenting the agent's context with a minimal set of exemplars to guide it to perform explicit defensive reasoning before action planning. Experiments show this method is remarkably effective, reducing up to 91.2% of pop-up window attacks and achieving near-perfect defense on other deception attacks , in stark contrast to prior defenses. The authors also identify that the key to this defense is teaching the agent a sequential \"defend-then-act\" reasoning process. Overall, their work provides a fundamental shift in perspective, underscoring that context deception attacks are far more tractable than previously believed and that success hinges on demonstrating a reasoning process rather than issuing an abstract rule.\nI have also provided some suggestions that I hope the authors find constructive. I am very open to adjusting my score upon a satisfactory response."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The failure of abstract instructions to secure agents has been a significant bottleneck, creating a widespread perception that context deception attacks are an inherently difficult problem. This work reveals this perception is an oversight, emphasizing the imperative for agent safety to focus on teaching how to think rather than just providing rules of what to do. \n1.The paper is very well written, clear, and I enjoyed reading it.\n2.The defense method is not only effective but also surprisingly simple and data-efficient, requiring no complex fine-tuning.\n3.Along the way of showcasing their findings, the authors also make other important contributions:\n(1)They precisely identify the key mechanism of the defense by ablating the reasoning order, showing that a \"defense-first\" analysis is critically more effective than a \"planning-first\" analysis.\n(2)They also test the limits of the method's data efficiency, finding that a single defensive exemplar is sufficient to reduce attack success by 96.2%, which is a very powerful and interesting result."}, "weaknesses": {"value": "The claim that the agent learns a \"portable reasoning skill\" that generalizes to unseen attacks warrants further elucidation. The defense's effectiveness is predicated on the agent identifying \"atypical\" or \"inconsistent\" UI elements . Conversely, i worried that a sophisticated attacker could craft a \"perfectly camouflaged\" attack that is visually indistinguishable from the benign UI. If the agent is presented with such an attack, its \"defense-first\" reasoning (which guide the agent to find anomalies) would conclude \"Nothing atypical identified,\" then the attack success rate would be far higher than reported.\nThe claim that teaching agent how to think rather than just providing rules of what to do is the key to an effective defense. And the finding that a single exemplar can reduce ASR by 96.2% is striking, yet the paper does not deeply analyze why the model's cognitive process is so fundamentally altered by this one example. An in-depth analysis of the model's internals—for instance, how this exemplar changes the model's attention patterns when processing a malicious input—would be highly valuable for truly understanding how this cognitive shift is induced at a fundamental level."}, "questions": {"value": "1.The \"defense-first\" reasoning process is used to detect \"atypical\" UI . How would this defense mechanism perform against a \"perfectly camouflaged\" attack where a malicious element is intentionally designed to be visually indistinguishable from the benign UI (e.g., using identical fonts and button styles)?\n2.What is the practical overhead of this method? The paper mentions that adding exemplars \"raises the cost of initial inference\" but does not quantify the extra token count or the impact on inference latency. How significant is this computational cost for deployment?\n3.The paper shows the defense is effective on open-source models like Qwen. Table 11 attributes this to larger models having \"stronger comprehension abilities\" . Is there an in-depth root cause analysis for why this paradigm is so effective on these models? For example, how do the exemplars mechanistically alter the model's attention patterns when it processes the malicious input?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LkIXaaBVpJ", "forum": "p7wDMVUJ0b", "replyto": "p7wDMVUJ0b", "signatures": ["ICLR.cc/2026/Conference/Submission4688/Reviewer_BpQb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4688/Reviewer_BpQb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762178007708, "cdate": 1762178007708, "tmdate": 1762917511570, "mdate": 1762917511570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}