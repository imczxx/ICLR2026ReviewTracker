{"id": "53eIDko6N5", "number": 8317, "cdate": 1758078462997, "mdate": 1759897792220, "content": {"title": "Disentangled Hierarchical VAE for 3D Human-Human Interaction Generation", "abstract": "Generating realistic 3D Human-Human Interaction (HHI) requires coherent modeling of the physical plausibility of the agents and their interaction semantics. Existing methods compress all motion information into a single latent representation, limiting their ability to capture fine-grained actions and inter-agent interactions. This often leads to semantic misalignment and physically implausible artifacts, such as penetration or missed contact. We propose Disentangled Hierarchical Variational Autoencoder (DHVAE) based latent diffusion for structured and controllable HHI generation. DHVAE explicitly disentangles the global interaction context and individual motion patterns into a decoupled latent structure by employing a CoTransformer module. To mitigate implausible and physically inconsistent contacts in HHI, we incorporate contrastive learning constraints with our DHVAE to promote a more discriminative and physically plausible latent interaction space. For high-fidelity interaction synthesis, DHVAE employs a DDIM-based diffusion denoising process in the hierarchical latent space, enhanced by a skip-connected AdaLN-Transformer denoiser. Extensive evaluations show that DHVAE achieves superior motion fidelity, text alignment, and physical plausibility with greater computational efficiency.", "tldr": "", "keywords": ["Human Motion", "Human-Human Interaction", "3D CV", "Motion Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e3bb5758509b5da66e2fd03ff8aa830c1585452.pdf", "supplementary_material": "/attachment/63723f80a0cf9fd9822aea2090c5dfdb72be91f6.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a novel approach to generate two person interaction motion from text called DHVAE. DHVAE obtain encoding for both motion then fuse these encoding to obtain an interaction encoding. Then a positive/negative pair of encoding is created to help learning the interaction trought a triplet loss. Finally, diffusion is used to denoise the interaction encoding that can then bit split into two motion encoding for both humans. Extensive experiments show that the method beat SOTA on two commonly used datasets quantitatively and qualitatively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- In the context of the datasets used creating positive and negative encoding is a good idea to limit body penetration.\n- method and implementation are clear and well explained \n- extensive experiment and ablation show the superiority of the method\n- Qualitative results are convincing."}, "weaknesses": {"value": "- It is not clear how \"contact\" is determined. Is it siply trought distance between the different body part ?\n- While the idea of building positive and negative encoding is nice it is limited to direct physical interaction or simple non physical interaction (which comprise nearly all current interaction datasets). But at the same time all lot of human interaction are indirect  (e.g a person point somewhere and the other turn to look) and a lot more subtle. How would this method works in those contexts ? Especially how will the negative encoding work for non contact action when 45–90cm translation might not \"break the interaction ?\n- In table 1 we see that the proposed method has the lowest multimodality on interhuman while the results are not as bad on interx but there is no discussion of this in the paper. How do these results affect the generation qualitatively ?\n- In the supplementary video we can see that penetration still happens in the sample generated by the proposed method. How could this be mitigated using the positive/negative encoding. An ablation on different value for the various  \"σ's\" used  and the on how contact are decides could have been interesting to see how much the penetration issue can been mitigated.\n- I might be wrong but in in the appendix the authors consider InterLDM [1] \"to be the first application of latent diffusion models to the HHI task\" but [2] appear to be older.\n\n[1]Boyuan Li, Xihua Wang, Ruihua Song, and Wenbing Huang. Two-in-one: Unified multi-person interactive motion generation by latent diffusion transformer. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\n[2] CHOPIN, Baptiste, TANG, Hao, et DAOUDI, Mohamed. Bipartite graph diffusion model for human interaction generation. In : Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2024"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "C5qtfkuJTB", "forum": "53eIDko6N5", "replyto": "53eIDko6N5", "signatures": ["ICLR.cc/2026/Conference/Submission8317/Reviewer_7SSr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8317/Reviewer_7SSr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760524988078, "cdate": 1760524988078, "tmdate": 1762920243937, "mdate": 1762920243937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes encoding the motions of two people into an interaction-aware variable $z_{o}$ in the latent space, and employs a contrastive learning strategy to improve the quality of interactions during reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The use of contrastive learning for two-person interaction generation is theoretically sound and has been validated in other motion generation domains (e.g., sign-language synthesis).\n2. The paper’s core contributions, the global interaction latent variable $z_{o}$ and the CoTransformer are both empirically verified through experiments.\n3. The manuscript is clearly written; the authors’ intentions can be readily followed from the text, and the Contrastive Learning algorithm is presented in a transparent and unambiguous manner."}, "weaknesses": {"value": "1. The core contribution of the paper is rather narrow: encoding two-person motions into a single latent variable $z_{o}$ improves generation quality, but it does not yield creative or novel motion combinations.\n2. The effectiveness of the contrastive-loss term is not reflected in the numerical metrics; the authors attempt to demonstrate its role through PCA visualizations in Appendix 6.5 (Figs. 7 & 8) and qualitative renderings. **Nevertheless, further evidence is required, as the high-quality results may primarily stem from the KL-divergence regularization.**\n3. Although the method enhances human-human interaction generation, **it offers little insight into scaling motion synthesis to more than two agents**. Moreover, the adopted contrastive-learning strategy poses new challenges for defining multi-person interactions: **how should “contact” and “away” be characterized among multiple individuals engaged in sustained interaction?**"}, "questions": {"value": "1. The Contrastive Learning itself may destroy the original distance information between the two actors; if in a sequence their inter-distance changes drastically (from far to close or vice-versa), will the contrastive loss still work satisfactorily?\n2. The authors provide abundant visual results to demonstrate the effectiveness of Contrastive Learning, yet **an additional user study would make the claim more convincing**.\n3. In view of the concerns about the scalability of Contrastive Learning and its actual contribution to quality, I tentatively assign a rating of 4. **If the authors supply strong evidence that contrastive learning brings substantial quality gains, can be extended to multi-person scenarios, or notably improves cases with large distance variations, I will raise my rating**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w3qmLzQp36", "forum": "53eIDko6N5", "replyto": "53eIDko6N5", "signatures": ["ICLR.cc/2026/Conference/Submission8317/Reviewer_p1j7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8317/Reviewer_p1j7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638803735, "cdate": 1761638803735, "tmdate": 1762920243498, "mdate": 1762920243498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an improved VAE structure, namely DHVAE, for better Human-Human Interaction (HHI) synthesis. Specifically, the individual motions in HHI are separately encoded into latent space and then fused, resulting in two latent variables representing individual motions and one latent variable $z_o$ representing global information. These latent variables are used to train a denoiser that learns to generate HHI-representative latent variables from text. Additionally, the paper proposes Interaction Contrastive Learning to train $z_o$ with the goal of achieving physically plausible results. The denoiser design also incorporates corresponding techniques to adapt to the proposed VAE structure. Quantitative results demonstrate advantages over many existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The figures and tables are well-presented, and the appendix is relatively comprehensive.\n\n2. The method shows advantages in quantitative metrics on both InterHuman and InterX datasets."}, "weaknesses": {"value": "**Major**\n\n1. **The contribution of the main component—DHVAE—has not been sufficiently discussed.** Compared to \"a flat, unified latent representation\" VAE, DHVAE uses three latent variables to represent an HHI sequence. Does this mean that the dimensionality of DHVAE's latent variables is three times that of a standard VAE? For example, on the InterHuman dataset, is D(DHVAE) : D(VAE) = 3×256 : 1×256? If so, are the comparisons in Table 2 and Table 3 fair?\n\n2. **The paper's presentation and formulations lack rigor.** For example, at Line 178, the paper states that Equation 1 \"ignores the conditional dependency between the agents.\" However, Equation 1 actually models the joint probability between $x_a$ and $x_b$. \n\n   Furthermore, **Equation regarding the ELBO of $\\log p(x_a, x_b)$ lacks rigorous derivation.** Without a formal proof showing that this objective is indeed a valid lower bound of the log-likelihood, it should be treated as a heuristic training objective rather than a theoretically grounded ELBO. The authors should either provide the complete derivation or clarify the assumptions under which this bound holds.\n\n3. **Insufficient evidence for the contribution of Interaction Contrastive Learning (ICL).** Table 3 shows that ICL does not significantly help with FID and R-Prec metrics; Table 5 demonstrates that ICL can reduce penetration. However, a critical experiment is missing: proving that ICL does not reduce penetration simply by pushing people further apart—it should be demonstrated that the introduction of ICL at least does not decrease the contact ratio, i.e., maintaining the contact ratio while reducing penetration.\n\n4. **Despite achieving certain quantitative results, the generated effects still exhibit many artifacts based on the provided supplementary materials,** such as penetration, foot sliding, poor contact, and other physically implausible behaviors. This undoubtedly undermines the paper's claim that \"DHVAE achieves superior physical plausibility.\"\n\n**Minor**\n\n5. **The proposed Interaction Contrastive Learning does not demonstrate its contribution in the main experimental section.** Its contribution to physical plausibility is only shown in the appendix (Table 5 in appendix). As a contribution point mentioned in the abstract, is this arrangement appropriate?"}, "questions": {"value": "1. The paper's introduction to the denoiser is relatively brief, but mentions the use of AdaLN, SPE, skip connections, and other techniques. Could a framework diagram of the denoiser be provided in the appendix to more clearly illustrate these techniques?\n\n2. Section 6.7 presents experiments on the layer number and latent size of DHVAE. However, no analysis of the latent size impact is provided in that section. Could the authors explain why the FID metric worsens as latent size increases?\n\n3. Why is the parameter count for TIMotion in Table 2 inconsistent with the data in Table 5 of TIMotion's original paper?\n\n4. Based on weakness 4, given that existing HHI generation methods (including the proposed approach) still suffer from poor physical plausibility, why not introduce post-processing strategies [1, 2] or physics-based simulation [3], as has been successfully adopted in human-object interaction works?\n\n\n**References:**\n\n[1] Wang Z, Wang J, Li Y, et al. InterControl: Zero-shot Human Interaction Generation by Controlling Every Joint[J]. Advances in Neural Information Processing Systems, 2024, 37: 105397-105424.\n\n[2] Li J, Clegg A, Mottaghi R, et al. Controllable human-object interaction synthesis[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024: 54-72.\n\n[3] Wu Z, Li J, Xu P, et al. Human-object interaction from human-level instructions[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025: 11176-11186."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FXFxPD7HjO", "forum": "53eIDko6N5", "replyto": "53eIDko6N5", "signatures": ["ICLR.cc/2026/Conference/Submission8317/Reviewer_h5hK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8317/Reviewer_h5hK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657858527, "cdate": 1761657858527, "tmdate": 1762920243056, "mdate": 1762920243056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel DM-based architecture for interaction generation. Unlike previous approaches that implicitly encode global interactions within the VAE latent space, the proposed method employs a hierarchical encoding scheme to decouple global interaction information from individual motion, thereby explicitly modeling interactions between two agents. To further constrain and refine the learned interactions, contrastive learning is integrated into the VAE training process. Experiments conducted on two commonly used datasets demonstrate that the proposed method outperforms existing approaches across all evaluation metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to follow. The motivation underlying the proposed methodology is clearly articulated and logically developed.\n2. The approach is conceptually simple yet effective. Both the disentanglement of representations and the incorporation of contrastive learning to promote physical plausibility are well justified.\n3. The experimental evaluation is thorough, and the ablation studies effectively demonstrate the contribution of each component."}, "weaknesses": {"value": "1. The design of the contrastive learning component requires further clarification. It is intuitive to generate negative samples for contact cases; however, for non-contact cases, Algorithm 1 suggests that negative samples are synthesized by artificially increasing the distance between agents. In this scenario, the agents become even more separated. Why should such samples be considered valid negatives for non-contact interactions?\n2. It would be informative to report inference latency for interaction generation in comparison with InterMask and TIMotion. Does the proposed architectural modification introduce additional time delay during inference?\n3. The sensitivity of the method to the latent code dimensionality is not discussed. Would increasing the latent space dimension improve performance, or is the method robust to this hyperparameter?"}, "questions": {"value": "1. For non-contact cases, contrastive learning in Algorithm 1 suggests that negative samples are synthesized by artificially increasing the distance between agents. In this scenario, the agents become even more separated. Why should such samples be considered valid negatives for non-contact interactions?\n2. The model seems smaller than sota methods. Does the proposed architectural modification introduce additional time delay during inference?\n3. Would increasing the latent space dimension improve performance, or is the method robust to this hyperparameter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DrpO80HUtj", "forum": "53eIDko6N5", "replyto": "53eIDko6N5", "signatures": ["ICLR.cc/2026/Conference/Submission8317/Reviewer_MZDm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8317/Reviewer_MZDm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878212109, "cdate": 1761878212109, "tmdate": 1762920242706, "mdate": 1762920242706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}