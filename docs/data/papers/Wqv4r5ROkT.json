{"id": "Wqv4r5ROkT", "number": 19561, "cdate": 1758297277192, "mdate": 1759897032448, "content": {"title": "Exogenous Distribution Learning for Causal Bayesian Optimization", "abstract": "Maximizing a target variable as an operational objective within a structural causal model is a fundamental problem. Causal Bayesian Optimization (CBO) approaches typically achieve this either by performing interventions that modify the causal structure to increase the reward or by introducing action nodes to endogenous variables, thereby adjusting the data-generating mechanisms to meet the objective. In this paper, we propose a novel method that learns the distribution of exogenous variables-an aspect often ignored or marginalized through expectation in existing CBO frameworks. By modeling the exogenous distribution, we enhance the approximation fidelity of the data-generating structural causal models (SCMs) used in surrogate models, which are commonly trained on limited observational data. Furthermore, the ability to recover exogenous variables enables the application of our approach to more general causal structures beyond the confines of Additive Noise Models (ANMs) and single-mode Gaussian, allowing the use of more expressive priors for context noise. We incorporate the learned exogenous distribution into a new CBO method, demonstrating its advantages across diverse datasets and application scenarios.", "tldr": "", "keywords": ["Causal Bayesian Optimization", "Causal Inference", "Bayesian Optimization"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a87d4a5ef5dd29b109104efd354ace1038b5ce5f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes EXCBO, a Causal Bayesian Optimization method that explicitly learns the distribution of exogenous noise variables in structural causal models rather than marginalizing them out. The authors use an encoder-decoder framework to recover exogenous variables from observational data and model their distribution with Gaussian Mixture Models. They introduce the Decomposable Generation Mechanism (DGM) as a generalization of Additive Noise Models (ANM), prove exogenous recovery under DGM, and provide regret analysis. Experiments on synthetic and real-world datasets show EXCBO can outperform baselines when noise is multimodal with moderate variance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses a gap in existing CBO methods by explicitly modeling exogenous distributions rather than marginalizing them out or assuming simple Gaussian noise.\n\n2. The encoder-decoder framework is intuitive and practical, using standardization to recover exogenous variables from observational data. The DGM formulation is more general than the ANM."}, "weaknesses": {"value": "1. The authors use 2-component Gaussian Mixture Models to model $p(\\hat{U})$ without any justification. Could the authors elaborate on a) why GMMs were chosen over other flexible density estimators? b) Why exactly 2 components? c) How sensitive are the results to this choice?\n\n2. Regarding the theoretical analysis part:\n\na) The authors mention that there exists a constant $a$ in Theorem 4.1, but according to Equation (16), $a = \\mathrm{sign}[f_b(z)/c]$ actually depends on $z$. Please clarify whether a is truly constant or the independence claim needs modification.\n\nb) For BGM (Theorem F.2), $\\hat{U} \\perp\\perp Z$ is explicitly assumed as a premise. For DGM (Theorem 4.1), it is claimed to be proven as a conclusion. However, the DGM proof relies on $a$ being constant (which is a concern in 2(a)).\n\nc) In line 1037, the authors stated that $\\sigma_{\\phi}(z) = c|f_b(z)|$. Can the authors elaborate on whether this equation is correct?\n\n3. Regarding the experiments:\n\na) MCBO is missed in the Dropwave experiments in Figure 4. I wonder if the authors could provide justification for it? From Figure 9, it seems that MCBO performs comparably with EXCBO on the Dropwave experiments.\n\nb) Figure 4 shows EXCBO's advantage decreases as $\\lambda$ and $\\sigma$ increase. Can the authors elaborate more on it and provide more explanation?\n\n4. The paper is notation-heavy with many symbols, which makes it a little bit hard to understand and follow. It would be better if the authors could add more illustrative examples to improve clarity\n\n5. (Minor) The indentation and margin of the beginning of many paragraphs (e.g., the first paragraph in Section 3, Section 3.4) should be adjusted."}, "questions": {"value": "Please see the questions in the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4P39UHS5TE", "forum": "Wqv4r5ROkT", "replyto": "Wqv4r5ROkT", "signatures": ["ICLR.cc/2026/Conference/Submission19561/Reviewer_V2tA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19561/Reviewer_V2tA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892641557, "cdate": 1761892641557, "tmdate": 1762931442449, "mdate": 1762931442449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EXCBO, a causal Bayesian optimization framework that relaxes the restrictive additive Gaussian noise assumption by estimating the exogenous variable distribution from data.\nUsing an encoder decoder surrogate (EDS), EXCBO recovers latent residuals, models their distribution with a Gaussian Mixture Model, and integrates the estimated p(U) into the optimization process.\nEmpirical results on synthetic and real-world structural causal models show improvements over standard CBO baselines under multimodal or non-Gaussian noise."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\nClear motivation: Identifies a genuine limitation in prior CBO frameworks that assume additive Gaussian noise, a well-motivated problem in causal optimization.\n\nIntuitive methodology: The encoder-decoder formulation for exogenous recovery is conceptually sound and connects structural causal modeling with modern regression techniques.\n\nReasonable empirical evidence: Experiments demonstrate that EXCBO improves performance in settings with multimodal or non-Gaussian noise distributions, aligning with its theoretical motivation.\n\nPractical relevance: Learning a more realistic noise model can be useful for real-world decision-making tasks"}, "weaknesses": {"value": "Weaknesses\nConceptual and Theoretical\nIncremental novelty: The paper combines known ideas from heteroscedastic Gaussian Processes, nonlinear ICA, and latent-variable Bayesian optimization rather than introducing fundamentally new theory or algorithms.\n\nLimited theoretical depth: The recoverability theorem is a restatement of standard residual properties under independence. The regret bound simply inherits results from GP-UCB without considering estimation uncertainty from the exogenous step.\n\nNo analysis of identifiability or robustness: The paper does not explore what happens when the DGM assumption fails, when noise is correlated with parents, or when the graph is misspecified.\nAlgorithmic\nMinor procedural change: The algorithm is essentially FNBO plus a residual normalization and GMM fitting step. The “learning” component is non-iterative and computed once before optimization.\n\nOmission of MCBO: The most relevant baseline (MCBO) is missing from Figures 4 and 5. The justification (“computationally expensive”) is weak and unsupported by runtime data.\n\nUnexplained high initial rewards: In the reward progression plots, EXCBO starts significantly higher than other methods. The paper does not explain whether EXCBO uses pretraining or a different initialization, raising concerns about comparability.\n\nExperimental\nInconsistent reporting: The number of experimental runs or seeds is clearly stated (four) only for the Dropwave dataset. Other benchmarks have uncertainty bars but no run count.\n\nLimited noise diversity: Only Gaussian and two-component Gaussian mixture noises are tested. There are no experiments with heavy-tailed, skewed, or heteroscedastic noise beyond the DGM structure.\n\nNo robustness or ablation studies: The effects of the encoder-decoder, the GMM modeling, or independence violations are not separately tested.\n\nPartial tabular reporting: Tables 1 and 2 summarize results for small-scale tasks, but not for larger experiments or real-world cases.\n\nWriting / Presentation\nClear overall, but contains typographical errors ( “STATMENT”, “LLMS”, extra parenthesis in “do(XI := f(ZI, A, UI) )”.\n\nRelated works is missing critical prior work related to Heteroskedastic Gaussian Processes, Latent Variable Bayesian Optimisation and Non Linear ICA, all closely related to this work\n\nMinor formatting inconsistencies in math expressions and figures.\n\n Minor Comments\nEnsure consistent reporting of the number of runs/seeds across all experiments.\n\nAdd a clear explanation for EXCBO’s higher starting reward to rule out unfair initialization.\n\nInclude MCBO results (even partial) or provide runtime justification with quantitative data.\n\nStandardize reference formatting and include missing citations to heteroscedastic GP and nonlinear ICA literature.\n\nProofread for typographical errors listed earlier."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BXXh006v6F", "forum": "Wqv4r5ROkT", "replyto": "Wqv4r5ROkT", "signatures": ["ICLR.cc/2026/Conference/Submission19561/Reviewer_G7dR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19561/Reviewer_G7dR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938774965, "cdate": 1761938774965, "tmdate": 1762931441790, "mdate": 1762931441790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors built on top of the Sussex MCBO/Aglietti CBO work, incorporating ideas from the exogenous variable learning literature. \n\nThey have successfully incorporated previous reviewer feedback, it seems, and improved their presentation and results.\n\nI am happy to increase my score once my three questions are addressed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The authors present an interesting, novel contribution to the CBO literature, placing it well in context and current literature.\n- Exploring the incorporation of EX in CBO is valid and this contribution is therefore relevant to readership."}, "weaknesses": {"value": "- See questions"}, "questions": {"value": "1. Why do you not benchmark against CBO by Aglietti et al as a baseline? Is this conceptually incompatible? The code is available and runs without code changes, though requires careful specification of initialisation points, AFAIK.\n2. Why do the convergence plots in Figure 6 seem to have different starting points? Presumably, they were initialisation with the identical random sample. Please clarify!\n3. “Each figure presents the mean performance over four random seeds” Why can’t you run more seeds? Four seeds seems to be enough for distinguishable error bars, but I am just curious.\n\nThanks in advance!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0UrAaM7oUp", "forum": "Wqv4r5ROkT", "replyto": "Wqv4r5ROkT", "signatures": ["ICLR.cc/2026/Conference/Submission19561/Reviewer_XTdz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19561/Reviewer_XTdz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762186808930, "cdate": 1762186808930, "tmdate": 1762931441302, "mdate": 1762931441302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EXCBO, a causal Bayesian optimization method that learns the distributions of exogenous variables to better model multimodal noise, in the decomposable generation mechasm. The authors incorporated these learned exogenous distributions into the Bayesian optimization process to improve sample efficiency and regret performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written and easy to follow, and the proposed method is conceptually straightforward."}, "weaknesses": {"value": "### Weak real-world motivation / failure case.\nWithout demonstrating or discussing the failure mode of existing works, it's hard to be motivated why we need the proposed method. \n\n### Restrictive design (one action per node).\n\nThe method assumes a known mapping where each intervened variable $X_i$ has its own continuous action $A_i$ that directly enters its mechanism. I think it's too restrictive, since in the system, either we want to know the best hard-intervention, or best soft-intervention (find the optimal action values AND their parents). It's hard to come up with a real-world scenario that will be matched with this assumption.\n\n### \\tau-SCM appears to rebrand a standard Markovian assumption.\n\nThe \\tau-SCM is simply an SCM with X = f(Z,U) where Z and U are independent. This is a usual Markovian SCM assumption. I don't see any reasons why authors create a new terminology for already existing notions. Also, it's unclear what \\tau stands for. \n\n### Definition 3 (EDS) is not mathematically rigorous.\n\nDefining the encoder/decoder via “a regression model such that E[X] exists and ϕ() can model the conditional mean µϕ() and variance σϕ().\", as in Def. 3, is not considered as a mathmatical definition with no rigoursness. \n\n### Extra smoothness assumptions vs prior CBO.\n\nTheir identification results require differentiability, whereas prior GP-based CBOs don't assume differentiable structural f in the model statement. Therefore, the claimed “generalization” depends on added smoothness conditions.\n\n### Contradiction: $\\hat U=h(Z,X)$ but $\\hat U\\!\\perp\\! Z$\n\nTheorem 4.1. mentioned that $\\hat U \\perp  Z$, while $\\hat U$ is a function of $Z$; i.e., $\\hat U=h(Z,X)$. I think this is contradictory."}, "questions": {"value": "1. If we infer $U$, we can actually recover the SCM. Then a better optimization algorithm can be found. Please discuss. \n\n2. This model's performance depends on the performance of the encoder models. Please take this account in your errro analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y3IdyMOJ9h", "forum": "Wqv4r5ROkT", "replyto": "Wqv4r5ROkT", "signatures": ["ICLR.cc/2026/Conference/Submission19561/Reviewer_Wa3C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19561/Reviewer_Wa3C"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762559367353, "cdate": 1762559367353, "tmdate": 1762931440787, "mdate": 1762931440787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}