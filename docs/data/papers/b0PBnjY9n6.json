{"id": "b0PBnjY9n6", "number": 7495, "cdate": 1758024615650, "mdate": 1759897849452, "content": {"title": "SimuPhy: Towards Physical Understanding, Reasoning, and Evaluation via Code Generation", "abstract": "Large language models (LLMs) have achieved remarkable progress in mathematics and code generation, yet their ability to reason about the physical world remains underexplored. Unlike mathematical reasoning, which can be expressed symbolically in text, physical reasoning is inherently tied to motion and dynamic processes. In this paper, we present SimuPhy, a novel task and dataset for evaluating LLMs’ understanding, reasoning, and coding-based representation of physical laws. In SimuPhy,  a model is given a motion description and tasked with generating code that simulates it. The resulting simulation is executed into a video, which is then evaluated by a vision–language model with predefined verification questions. SimuPhy contains 7,625 motion descriptions, including a curated 300-example test set with human verification. We evaluated 10 advanced LLMs, and find that even the strongest model, Deepseek-671B achieves only 20.6\\% pass rate, highlighting the difficulty of the task and the lack of physical law reasoning in current models. Building on this setup, we explore reinforcement learning with verifiable rewards (RLVR), pairing it with supervised fine-tuning (SFT) to improve models’ ability to generate physically consistent simulations. Together, SimuPhy and our verifiable reward training pipeline provide a foundation for bridging language models toward genuine physical understanding.", "tldr": "We introduce SimuPhy, a benchmark dataset and a closed-loop RL framework that teaches LLMs to understand and simulate physical processes by linking code, video, and VLM-based validation.", "keywords": ["Large Language Models", "Physical Understanding", "Benchmark", "Reinforcement Learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50aade50bff8cf49666fa29e08c83262b29e0de5.pdf", "supplementary_material": "/attachment/91150fc50de60b7fd40288c0f90c2d07c01634da.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SimuPhy, a novel benchmark and dataset designed to evaluate and improve the ability of LLMs to understand, reason about, and simulate physical laws through text-to-code generation. The authors construct a dataset of 7,625 dynamic scenarios across five physics domains using an automated pipeline. Each scenario is paired with visual verification questions, and the generated code is executed to produce videos, which are then evaluated by a VLM for physical consistency. The authors also propose a reinforcement learning approach with verifiable rewards (RLVR) that leverages VLM-based judgments to align model outputs with physical correctness. Experiments on 10 state-of-the-art LLMs reveal significant limitations in physical reasoning, with the best model achieving only 20.6% accuracy. Through fine-tuning and RL, the authors demonstrate that a 7B model can achieve performance competitive with much larger models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Combine text-to-code generation with visual verification for physical reasoning. The RLVR framework is innovative and leverages VLM judgments without human labels.\n\n2. The paper is well-organized, with clear explanations of methodology, metrics, and results.\n\n3. Provides a new benchmark for physical reasoning and a training methodology that improves model alignment with physical laws."}, "weaknesses": {"value": "1. The synthetic dataset may not fully capture the complexity of real-world physics, such as 3D interactions and dynamic lighting effects. Furthermore, it primarily consists of one-dimensional cases, which limits its scope.\n\n2. While the VLM judge is effective, its judgments may inherit biases from its base model. A more fundamental concern is the potential circularity of using a VLM's own physical reasoning capability as a reward signal to further improve itself via reinforcement learning.\n\n3. The dataset underrepresents certain physical concepts, such as optics, which could hinder the model's ability to generalize to these domains.\n\n4. The RL training process is computationally intensive, requiring 576 GPU-hours, which may limit the accessibility and reproducibility of this approach."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "om1K4g9OUy", "forum": "b0PBnjY9n6", "replyto": "b0PBnjY9n6", "signatures": ["ICLR.cc/2026/Conference/Submission7495/Reviewer_8Q8p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7495/Reviewer_8Q8p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760576614304, "cdate": 1760576614304, "tmdate": 1762919609123, "mdate": 1762919609123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SimuPhy, a novel benchmark and training methodology designed to address the significant limitations of Large Language Models (LLMs) in reasoning about dynamic physical processes. The authors argue that existing models fail to generate code that can accurately and consistently simulate physical scenarios described."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- A Novel Evaluation Pipeline: The core of their approach is a \"text-to-code-to-video-to-VLM adjudication\" loop. An LLM's generated Python simulation code is executed, rendered into a video, and then evaluated by a Vision-Language Model (VLM) judge (Qwen-2.5-72B-VL) against a set of verification questions.\n- Comprehensive Experiments: Authors revaluated 10 advanced LLMs and provided detailed experiments"}, "weaknesses": {"value": "- Circular Logic in Benchmark Design: The paper's most fundamental flaw is the \"closed-loop\" nature of its benchmark. The data is AI-generated (by GPT-4o), AI-validated (by DeepSeek-R1), and AI-judged (by Qwen-VL). This \"AI-generation, AI-verification, AI-adjudication\" system raises immediate concerns about circularity. It is unclear whether the benchmark measures an LLM's \"genuine physical understanding\".\n- Misleading and Unfair Comparisons: A key conclusion—that the authors' trained 7B model (45.0% Pass@8, Table 3) surpasses powerful models like Gemini-2.5-pro (40.0% Pass@8, Table 1)—is methodologically unfair and misleading. The authors' 7B model was specifically trained (both SFT and RLVR) on the SimuPhy dataset itself. The frontier models (Gemini, Qwen, etc.) were evaluated in a zero-shot setting without any specific training on the SimuPhy domain or reward signal. This is a clear case of \"training on the test set's domain.\" This \"victory\" does not prove the 7B model has superior, generalizable physical reasoning. It only proves that it can be effectively optimized to \"take the test\" and \"please the referee\" (the VLM) for this specific benchmark."}, "questions": {"value": "- Have you tested the 7B model (trained with SFT + RLVR) on other physics benchmarks (e.g., text-based QA like PhysBench or other simulation tasks)? This would be essential to determine if the \"understanding\" gained by optimizing for the SimuPhy VLM judge is generalizable or if it is an overfitted, task-specific skill."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dqkd8oRPJJ", "forum": "b0PBnjY9n6", "replyto": "b0PBnjY9n6", "signatures": ["ICLR.cc/2026/Conference/Submission7495/Reviewer_ATNw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7495/Reviewer_ATNw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725267541, "cdate": 1761725267541, "tmdate": 1762919608688, "mdate": 1762919608688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SimuPhy, a text-to-code-to-video benchmark testing LLMs’ physical reasoning. Models generate Python code from motion descriptions to render videos verified by a VLM. The dataset spans 7,625 physics scenarios. Top models reach 54.7% Pass@8. Using reinforcement learning with vision-verifiable rewards (RLVR), a 7B model achieves 45.0% Pass@8, showing progress toward physical understanding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Substantial dataset spanning five classical physics domains, plus a curated, human-checked test split.\n- Practical evaluation pipeline with multiple operational metrics (Executable, Render, Playable, Accuracy), revealing distinct failure modes beyond simple correctness.\n- Broad baseline sweep over 10 strong LLMs, the low accuracies highlight a real gap and the benchmark’s difficulty.\n- RL with verifiable vision reward is a sensible training strategy that improves scenario-level Pass@8 notably for a small model, reward ablations (VLM-acc, VLM-binary, LLM-as-judge) are informative.\n- Initial judge-validation against human labels provides some evidence that the VLM judge is usable."}, "weaknesses": {"value": "- Physical grounding is weak: the task permits hand-crafted 2D OpenCV animations without any requirement to use a physics engine or enforce conservation/constraints, so “simulation of physical laws” can degenerate into scripted drawing that merely looks plausible. This risks measuring graphics scripting rather than physical reasoning. Without constraining the toolset to bona fide physics engines (e.g., PyBullet, Box2D, MuJoCo, differentiable optics/EM solvers) or quantitatively checking laws, the task primarily assesses the model’s ability to draw plausible trajectories. Qualitative VLM questions cannot distinguish a kinematically scripted spiral from one generated by correct force integration. This undermines the stated goal of measuring \"physical-law reasoning.\"\n\n- Single-judge dependence and potential reward hacking: all core results depend on one VLM (Qwen-2.5-72B-VL) and a pass rule that treats low-confidence “Not sure” as correct (with up to two allowed). RL directly optimizes this judge, making overfitting to its idiosyncrasies or ambiguity-seeking behavior likely. No cross-judge or multi-judge aggregation is reported for the final scores.\n\n-Limited human evaluation where it matters most: the 93.2% agreement is reported on pre-RL data. There is no targeted human audit of the RLVR-trained model’s outputs to check for reward hacking or degradation in true physical fidelity.\n\n- Evaluation rule design is permissive and may inflate scores: counting low-confidence “Not sure” as correct and allowing up to two such answers to pass can be exploited, there is no sensitivity study showing that results are stable when tightening this criterion.\n\n- Small test set and class imbalance: only 300 test items across 52 concepts and 5 domains, with long-tailed coverage. \n\n- Even by the paper’s own numbers, consistent single-sample performance remains weak (Avg@8), again suggesting that improvements are largely in best-of-N sampling rather than genuine robustness.\n\n- Novelty of “verifiable rewards” is incremental: RL from automated judges (including VLMs) is rapidly becoming standard. \n\nMy recommendation: Reject. Key reasons: (1) The benchmark’s core task allows non-physical, scripted animations to pass, so the central claim of advancing “physical law understanding” is not convincingly supported (2) Heavy reliance on a single VLM judge and permissive pass criteria invites reward hacking and overfitting, with no cross-judge/human verification of the RL improvements."}, "questions": {"value": "- What prevents a model from simply scripting 2D animations that look like the described motion without integrating the relevant equations of motion or laws (e.g., Snell’s law, Lorentz force)? Can you enforce the use of physics engines or provide automated checks for conserved quantities where applicable? Add constrained baselines that must use a designated physics engine per domain, plus a “pure animation” baseline. This would directly test whether the benchmark discriminates \"physics-based\" simulation from scripted graphics.\n\n- Do you have any quantitative verifications (e.g., energy/momentum preservation in elastic collisions, angle ratios for refraction) on a subset of scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s8OIABOSiZ", "forum": "b0PBnjY9n6", "replyto": "b0PBnjY9n6", "signatures": ["ICLR.cc/2026/Conference/Submission7495/Reviewer_r5GR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7495/Reviewer_r5GR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768555460, "cdate": 1761768555460, "tmdate": 1762919608149, "mdate": 1762919608149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SimuPhy, a large-scale benchmark and dataset designed to evaluate large language models’ (LLMs) ability to understand and simulate physical processes through code generation. In SimuPhy, an LLM receives a natural-language description of a motion scenario (e.g., “a spinning disk on a frictionless table”) and must generate Python code that produces a corresponding simulation video. The resulting video is then automatically evaluated using a vision–language model (VLM), which answers predefined verification questions to determine physical consistency between the textual description and the generated simulation. The authors evaluate 10 state-of-the-art LLMs, finding that even the strongest model (DeepSeek-R1-0528, 671B) achieves only 20.6% accuracy. The paper also introduces the RLVR scheme, which uses visual verification signals from a VLM as training feedback. Combined with supervised fine-tuning, RLVR substantially improves model performance: a 7B model trained with this approach reaches 45% pass rate."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark has a large scope, covering a wide range of physics domains and dynamic scenarios.\n - The study includes comprehensive experiments with multiple leading LLMs, highlighting consistent performance patterns across models.\n - The availability of the dataset and the RLVR baseline supports reproducibility and provides a foundation for future research."}, "weaknesses": {"value": "-  It is unclear whether the benchmark truly measures physical understanding or rather the ability to produce correct simulation code. Even a model (or human) with solid conceptual understanding could fail due to coding mistakes such as axis misalignment, suggesting the latter may dominate. The actual human score would reveal that.\n - The data generation pipeline heavily depends on multiple LLMs for scenario creation and filtering, yet the paper’s own results indicate that current LLMs struggle with physical reasoning. \n - The VLM-as-a-judge component is vulnerable to reward hacking and may produce inconsistent or biased judgments; prior works have shown that visual verification models can be exploited or yield false positives. \n - Since Avg@8 effectively corresponds to pass@1 with eight samples used for estimation, adopting standard pass@k notation would improve clarity and comparability."}, "questions": {"value": "- Could you clarify which parts of the data pipeline underwent human evaluation? Was it applied only to the verification questions?\n - Have you considered conducting a human evaluation of the RLVR-trained model outputs to verify that improvements reflect genuine physical correctness rather than reward hacking or overfitting to the VLM judge?\n - If it is possible, please provide the human score for comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Jp54rmKfF", "forum": "b0PBnjY9n6", "replyto": "b0PBnjY9n6", "signatures": ["ICLR.cc/2026/Conference/Submission7495/Reviewer_9j6y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7495/Reviewer_9j6y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926243884, "cdate": 1761926243884, "tmdate": 1762919607764, "mdate": 1762919607764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}