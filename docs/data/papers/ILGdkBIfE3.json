{"id": "ILGdkBIfE3", "number": 370, "cdate": 1756736821866, "mdate": 1763118463850, "content": {"title": "FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction", "abstract": "The integration of new modalities enhances the capabilities of multimodal large language models (MLLMs) but also introduces additional vulnerabilities.\nIn particular, simple visual jailbreaking attacks can manipulate open-source MLLMs more readily than sophisticated textual attacks.\nHowever, these underdeveloped attacks exhibit extremely limited cross-model transferability, failing to reliably identify vulnerabilities in closed-source MLLMs.\nIn this work, we analyse the loss landscape of these jailbreaking attacks and find that the generated attacks tend to reside in high-sharpness regions, whose effectiveness is highly sensitive to even minor parameter changes during transfer.\nTo further explain the high-sharpness localisations, we analyse their feature representations in both the intermediate layers and the spectral domain, revealing an improper reliance on narrow layer representations and semantically poor frequency components.\nBuilding on this, we propose a Feature Over-Reliance CorrEction (FORCE) method, which guides the attack to explore broader feasible regions across layer features and rescales the influence of frequency features according to their semantic content.\nBy eliminating non-generalizable reliance on both layer and spectral features, our method discovers flattened feasible regions for visual jailbreaking attacks, thereby improving cross-model transferability.\nExtensive experiments demonstrate that our approach effectively facilitates visual red-teaming evaluations against closed-source MLLMs.", "tldr": "Understanding and improving the transferability of optimisation-based visual jailbreaking attacks.", "keywords": ["Visual Jailbreaking Attack", "Multimodal Large Language Models", "Red-teaming Evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/22ab5774afbc112bd4ca446035e65ec34c3cb3aa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper focuses on transferable visual jailbreaking attacks. It provides a detailed analysis from the perspectives of loss landscape, feature representation, and frequency-domain features to explain why current optimization-based attacks suffer from poor transferability. To address this issue, the authors propose a new approach, Feature Over-Reliance Correction (FORCE), which enhances the transferability of adversarial attacks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The focus on transferable visual jailbreaking addresses an important topic in the safety community.\n\n\n2. The paper provides a thorough analysis of why optimization-based attacks exhibit poor transferability and generalization.\n\n\n3. The paper is clearly written and easy to follow.\n\n\n4. The proposed method improves transferability, with especially strong gains on adapter-based MLLMs."}, "weaknesses": {"value": "1. The proposed method shows clear improvement on adapter-based MLLMs, but for early-fusion and commercial MLLMs, although it performs slightly better than the baseline, the results are still far from being practically useful.\n\n\n2. There are no experiments on defenses, which makes it difficult to assess the robustness of the proposed attack.\n\n\n3. Transferable adversarial attacks have been well-studied in traditional CV and NLP domains. However, the discussion of these prior approaches, especially regarding their applicability to MLLMs, is missing from this paper."}, "questions": {"value": "1. As the authors analyzed, current adversarial attacks are highly sensitive to weight perturbations. A straightforward idea would be to introduce weight perturbations during the optimization process. Do the authors have any insights on this?\n\n\n2. In the proposed method, the authors choose to enlarge the feature representation region, which I think I understand the motivation behind. However, an opposite idea, finding adversarial examples whose neighboring regions in the feature space are more similar, might also reduce sensitivity. I’m curious how the authors view this opposite alternative.\n\n\nOverall, the paper is well-motivated. If the authors can adequately address the concerns mentioned above, I would be inclined to raise my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kj6dznZITM", "forum": "ILGdkBIfE3", "replyto": "ILGdkBIfE3", "signatures": ["ICLR.cc/2026/Conference/Submission370/Reviewer_Gebp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission370/Reviewer_Gebp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761596603798, "cdate": 1761596603798, "tmdate": 1762915505570, "mdate": 1762915505570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "Dear Reviewers,\n\nWe sincerely thank all reviewers for their thorough and constructive feedback. We are pleased that you found our work to address an important research question, offer clear and underlying insights into transferability, provide an effective method, andour paper is well-written and easy to follow.\n\nWe would like to highlight that our study is one of the first studies to identify the inherently limited transferability of optimisation-based visual jailbreaking attacks. Our work makes a non-trivial contribution by both deepening the understanding of the underlying causes and improving transfer performance. Since our goal is to gain principled insights into the transferability problem, we intentionally exclude performance-oriented techniques such as prompt modification, model ensembling, or loss-based selection.\n\nWe also acknowledge that research on visual jailbreaking remains at an early stage. Nevertheless, the continued strengthening of textual alignment and the increasing practical importance of multimodal evaluation suggest that visual jailbreak attacks will become an important and promising direction for future investigation.\n\nFinally, we would like to express our gratitude once again. Your comments have substantially improved the clarity and quality of our manuscript. We will include additional results, covering more baselines, different optimised source models, defences, and computational cost, in the revised version.\n\nBest regards,\\\nAuthors"}}, "id": "kXNC9kc9Uh", "forum": "ILGdkBIfE3", "replyto": "ILGdkBIfE3", "signatures": ["ICLR.cc/2026/Conference/Submission370/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission370/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission370/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763118283638, "cdate": 1763118283638, "tmdate": 1763118283638, "mdate": 1763118283638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "wUZCVNL3Hz", "forum": "ILGdkBIfE3", "replyto": "ILGdkBIfE3", "signatures": ["ICLR.cc/2026/Conference/Submission370/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission370/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763118462899, "cdate": 1763118462899, "tmdate": 1763118462899, "mdate": 1763118462899, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates transferable adversarial attacks on MLLMs, first examining their effectiveness in representation space and then proposing a solution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper focuses on an important research question\n2. The paper proposes an intriguing method and validates its performance on different benchmarks."}, "weaknesses": {"value": "1. Clarity of writing (Section 3.1 Motivation).\nThe exposition is unclear and significantly hinders comprehension. In Section 3.1 (motivation), please clarify:\n(a) What is the purpose of adding random noise?\n(b) Why does adding adversarial noise increase the optimization loss? If the objective is to elicit “Sure, here it is,” this seems contradictory.\n(c) In the bottom figure, what is the purpose of perturbing the model weights? In addition, the claim that “the attack is trapped in a local optimum of the source MLLM” is not evident from the visualization. The figures should be redrawn and more clearly explained to support the stated motivation. This ambiguity also propagates to later sections (e.g., Sec. 3.2).\n\n2. Interpretation of Figure 3.\nThe explanation provided is insufficient. In Figure 3, layers 1, 6, 16, and 21 exhibit almost identical feature sensitivity, i.e., the required interpolation rate is around 0.2 for all. This observation does not convincingly support the claim that lower layers have worse generalization. Please reconcile this discrepancy or provide additional evidence/analysis.\n\n3. Experimental setup and baselines.\n(a) All noises are generated using LLaVA-1.5-7B, lacking cross-model validation across different open-source MLLMs.\n(b) The method is not compared against other transferable MLLM adversarial attacks. such as [1,2], which is necessary to contextualize performance and transferability.\n\n[1] A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1 NeurIPS 2025\n\n[2] M-Attack-V2: Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting. Arxiv 2025"}, "questions": {"value": "Please first refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EDSOawdiZ1", "forum": "ILGdkBIfE3", "replyto": "ILGdkBIfE3", "signatures": ["ICLR.cc/2026/Conference/Submission370/Reviewer_WwTk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission370/Reviewer_WwTk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805589077, "cdate": 1761805589077, "tmdate": 1762915505210, "mdate": 1762915505210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the **Feature Over-Reliance CorrEction (FORCE)** method to enhance the cross-model transferability of visual jailbreaking attacks on multimodal large language models (MLLMs). The method addresses the issue of attacks relying on model-specific features by incorporating layer-aware regularization and spectral rescaling. Extensive experiments demonstrate that FORCE significantly improves the transferability and efficiency of visual attacks across various MLLM architectures, including commercial models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper offers a clear and well-supported analysis of the underlying reasons for poor transferability in visual jailbreaking attacks. By examining the loss landscape of these attacks, the authors identify that attacks often reside in high-sharpness regions, making them highly sensitive to small changes in model parameters.\n\n2. FORCE effectively addresses the problem of over-reliance on model-specific features in visual jailbreaking attacks. The method combines layer-aware regularization and spectral rescaling to improve the generalizability of the attacks across different models.\n\n3. The experiments conducted demonstrate the effectiveness of FORCE in improving attack transferability across various MLLM architectures. The method is shown to outperform baseline approaches, achieving higher attack success rates and reducing the number of queries required for successful attacks."}, "weaknesses": {"value": "1. The paper compares FORCE mainly with standard PGD and a few basic methods, but it does not evaluate the proposed method against more recent state-of-the-art transferable adversarial attack techniques. \n\n2. The experiments do not assess how FORCE-generated attacks perform when models are equipped with defense mechanisms."}, "questions": {"value": "1. The paper should provide concrete measurements of the computational cost and memory footprint for the proposed FORCE method, as the additional operations for layer regularization and spectral rescaling likely impose significant overhead.\n\n2. The number of baselines compared in your work is relatively limited. Several existing white-box attack methods, including textual attacks like GCG, offer transferable attack strategies by learning a universal suffix for transferability. It would be beneficial to understand why your approach does not include comparisons with these methods.\n\n3. It is important to evaluate how FORCE performs against defended models, particularly testing its effectiveness when target models employ common input-level defenses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7O7GMVPtC8", "forum": "ILGdkBIfE3", "replyto": "ILGdkBIfE3", "signatures": ["ICLR.cc/2026/Conference/Submission370/Reviewer_9Z2k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission370/Reviewer_9Z2k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890319694, "cdate": 1761890319694, "tmdate": 1762915505069, "mdate": 1762915505069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the reason of optimization-based visual jailbreaking attacks on multimodal large language models (MLLMs) show poor cross-model transferability. Through detailed analyses of the loss landscape, layer-wise feature representations, and spectral dependencies, the authors find that existing attacks over-rely on model-specific shallow features and semantically weak high-frequency components, leading to sharp and non-transferable adversarial regions. Extensive experiments across both open-source and commercial MLLMs (e.g., LLaVA, InstructBLIP, Qwen, Claude, Gemini, GPT-5) demonstrate consistent and substantial improvements in cross-model transferability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a clear and multi-perspective diagnosis of the transferability issue, combining loss landscape visualization, layer interpolation, and Fourier analysis. And the proposed method seems practically effective in improving cross-model robustness of visual jailbreaks.\n\n2. The method is tested on a wide variety of MLLMs (adapter-based, early-fusion, and commercial) and three benchmark datasets, showing consistent ASR gains (up to 20%) and reduced query costs. Component analysis, frequency influence plots, and feature interpolation experiments convincingly support the claimed mechanisms.\n\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The weighting rule w_m = \\min(1, \\ell_{m-1}/\\ell_m) is somewhat ad-hoc and not derived from optimization principles; an adaptive or learning-based variant could be more convincing.\n\n2. While the proposed FORCE demonstrates clear gains over PGD, the evaluation omits comparisons with other state-of-the-art transferable attack methods. Including these would strengthen the empirical evidence and contextualize the contribution.\n\n3. “In this challenging setting, our method substantially improves transferability, achieving nearly a 100% increase over the baseline ASR”, corresponds to the ASR increase from 1% to 2%. I still think the performance can be limited. While I think the authors can briefly discuss why Early-Fusion MLLMs achieved limited attack performance."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The paper focuses solely on improving attacks, without exploring how defenders might detect or mitigate such transferable jailbreaks. I suggest the author to briefly discuss how to mitigate the jailbreaks."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LDM0xsTYzP", "forum": "ILGdkBIfE3", "replyto": "ILGdkBIfE3", "signatures": ["ICLR.cc/2026/Conference/Submission370/Reviewer_t3iQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission370/Reviewer_t3iQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902195138, "cdate": 1761902195138, "tmdate": 1762915504828, "mdate": 1762915504828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}