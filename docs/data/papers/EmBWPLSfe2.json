{"id": "EmBWPLSfe2", "number": 19843, "cdate": 1758299899875, "mdate": 1759897016341, "content": {"title": "DeepResearchGym: A Free, Transparent, and Reproducible Evaluation Sandbox for Deep Research", "abstract": "Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensive and well-supported reports to complex queries. However, most existing frameworks rely on dynamic commercial search APIs, which pose reproducibility and transparency challenges in addition to their cost. To address these limitations, we introduce DeepResearchGym as an open-source sandbox that combines a reproducible search API with a rigorous evaluation protocol for benchmarking deep research systems. The API indexes large-scale public web corpora, namely ClueWeb22 and FineWeb, using a state-of-the-art dense retriever and approximate nearest neighbor search via DiskANN. It achieves lower latency than popular commercial APIs while ensuring stable document rankings across runs, and is freely available for research use. To evaluate deep research systems' outputs, we extend the Researchy Questions benchmark with automatic metrics through LLM-as-a-judge to measure alignment with users' information needs, retrieval faithfulness, and report quality. Experimental results show that systems integrated with DeepResearchGym achieve performance comparable to those using commercial APIs, with performance rankings remaining consistent across evaluation metrics. A human evaluation study further confirms that our protocol aligns with human preferences, validating the framework's ability to support the automated assessment of deep research systems.", "tldr": "", "keywords": ["Information Retrieval", "Deep Research", "Web Search"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0edf0147deba662169e08e18ff5a8a43b6006ff7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces DeepResearchGym, a benchmark for evaluating deep research systems. The benchmark provides standardized search and fetch APIs to ensure a fair and transparent environment for comparing various systems. Although the benchmark utilizes an 'LLM-as-a-judge' approach, its metrics are validated by human annotation and high inter-annotator agreement. A comparative analysis of two commercial, three white-box, and two baseline deep research systems was conducted using this benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Provide a transparent and fair environment to compare various deep research systems.\n* Include extensive metrics during evaluation"}, "weaknesses": {"value": "* Novelty: The current corpus and queries of DeepResearchGym are from existing datasets (ClueWeb22, FineWeb, and Researchy Questions). I am very willing to raise my score if there are any missing innovative details in the methodology.\n\n* A few missing details in the benchmark construction and evaluation. Please see the details in the questions."}, "questions": {"value": "* Line 145: The authors state that low-quality and spam pages were filtered during sampling. Could you clarify if this filtering was part of the original ClueWeb22 dataset's preprocessing, or if an additional filtering step was applied? If it was an additional step, please provide more details on the process.\n* The paper mentions that users can switch between corpora (Lines 200-202), yet all evaluations are conducted on either commercial search engines or the ClueWeb22 dataset. The justification for including the FineWeb corpus is unclear. To demonstrate its value and analyze the system's performance across different retrieval corpora, please consider adding experiments based on FineWeb, perhaps in the appendix.\n* Figure 1 indicates high efficiency for the search API/retriever in DeepResearchGym. A discussion of the factors contributing to this efficiency would be a valuable addition. Explaining the underlying design choices could further highlight the innovative aspects of the system.\n* The evaluation is limited to a small number of systems. While the benchmark aims for fairness, two of the systems evaluated do not use the provided search and fetch APIs, making direct comparison difficult. To provide a more direct and comprehensive comparison, have the authors considered evaluating other generative models within the same standardized pipeline?\n* The design of DeepResearchGym is well-suited for efficiency comparisons between different systems (e.g., measuring the number of searches, fetches, or conversational turns). However, the paper currently lacks a discussion on these efficiency metrics. Including an analysis of efficiency would significantly strengthen the evaluation.\n* It's always good to include human evaluation as justification for the LLM judges. However, the authors did not discuss the background of the annotators and the instructions for their annotation. Since the task requires expert-level knowledge, it is necessary to have more details regarding human evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JWNXExpFh1", "forum": "EmBWPLSfe2", "replyto": "EmBWPLSfe2", "signatures": ["ICLR.cc/2026/Conference/Submission19843/Reviewer_FX47"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19843/Reviewer_FX47"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760995182919, "cdate": 1760995182919, "tmdate": 1762932017186, "mdate": 1762932017186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We thank the reviewers for their feedback. In individual answers, we have addressed concerns raised by each reviewer, including clarifications on filtering, evaluation protocols, static versus live corpora, judge model robustness, efficiency, and novelty. We added requested experiments, including evaluation using the FineWeb search corpus, alternative LLM judges to confirm robustness, and additional correlation analysis.\n\n**A- Summary of Contribution**  \nWe introduce a sandbox for reproducible research of deepresearch systems, including  a free and stable search API intended to compliment commercial search services. We illustrate its utility by building a full evaluation protocol for deep research systems grounded in ClueWeb22, which provides a controlled environment for measuring search quality and end-to-end system performance. We aim for the platform to be flexible enough to support multiple research needs, so, besides ClueWeb22, we provide searching FineWeb for researchers who need more recent crawl. This setup broadens applicability, enables comparisons across corpora, and supports diverse experimental needs while maintaining full reproducibility.\n\n**B- DeepResearchGym for Model Training**  \n\nWe take this global answer to further demonstrate the utility of DeepResearchGym with another use-case beyond evaluation. Specifically, we explored its use as a training environment for agentic search systems. Training agents with commercial search APIs is costly, often requiring hundreds of thousands of queries across multiple trajectories. For example, a modest reinforcement learning setup with 10 thousand queries, 16 trajectories per rollout, and 4 searches per trajectory will cost up to 640\\\\$ with Serper, and 5000\\\\$ with Tavily. Note that this is for a single training run, with prices quickly piling up under experimental setups that require multiple training runs. In turn, leveraging our API is free, while maintaining latencies similar to the commercial approaches as described in the paper. \n\nWe start by synthesizing a training dataset grounded in ClueWeb22. Queries were generated from Wikipedia subset of ClueWeb22, following state-of-the-art synthetization approaches such as ASearcher’s [1], and were designed to include multi-hop reasoning and entity abstraction to encourage generalization beyond surface-level patterns.\n\nWe trained two agents to evaluate DeepResearchGym as a training environment. Both agents follow the same simple design [2]: backbone is Qwen3-1.7B, and the model’s action space is search, answer, or summarize context. The first agent was trained on our synthetic dataset (5 thousand queries) grounded in ClueWeb22 with access only to the DeepResearchGym search API. The second agent was trained on a state-of-the-art synthetic dataset [3] (5 thousand queries) with access to a commercial search API. Both agents were then evaluated on standard short-answer benchmarks using commercial search. This choice avoids coverage gaps in ClueWeb22 that can affect open-web evaluation and provides a direct test of generalization. The setup measures whether an agent trained within a confined and fully reproducible environment can transfer its learned search policy to a commercial engine at inference time.\n\nThe table below presents success rates on GAIA, WebWalkerQA, and HLE, computed according to each dataset's guidelines. The agent trained with DeepResearchGym achieves competitive performance despite never accessing commercial search during training, matching or exceeding the commercial-trained baseline on GAIA and HLE despite showing lower performance on WebWalkerQA. These results demonstrate that DeepResearchGym provides a cost-effective and reproducible training environment where learned search strategies can transfer across both search engines and evaluation distributions. We will shortly provide a new version of the manuscript pdf, with these experiments included in the main body.\n\n| Method                                  | Search Train | Search Infer | GAIA | WebWalker | HLE |\n|-----------------------------------------|--------------|--------------|------|-----------|-----|\n| **Before RL**                           |              |              |      |           |     |\n| Qwen3-1.7B [1]                        | None         | Serper       | 8.7  | 19.1      | 6.2 |\n| **After RL**                            |              |              |      |           |     |\n| Qwen3-1.7B + RL (Open Web)              | Serper       | Serper       | 18.4 | 35.4      | 6.9 |\n| Qwen3-1.7B + RL (DeepResearchGym)       | ClueWeb      | Serper       | 20.3 | 29.7      | 7.0 |\n\n\n[1] Jin et al. (2025) Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them.   \n[2] Gao et al. (2025) Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL.  \n[3] Li et al. (2025) Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL."}}, "id": "eUX3HOw3Hj", "forum": "EmBWPLSfe2", "replyto": "EmBWPLSfe2", "signatures": ["ICLR.cc/2026/Conference/Submission19843/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19843/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19843/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763731362078, "cdate": 1763731362078, "tmdate": 1763731429008, "mdate": 1763731429008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DEEPRESEARCHGYM, an open, reproducible sandbox for evaluating deep research systems that generate long-form, evidence-grounded reports. It provides a free search API over public web corpora (ClueWeb22 A/B and FineWeb), implemented with a dense retriever (MiniCPM-Embedding-Light) and DiskANN, plus endpoints to search and to fetch archived page snapshots for stable, auditable evidence. The authors pair this with a tri-part evaluation protocol on Researchy Questions that scores (i) coverage via Key-Point Recall/Contradiction, (ii) retrieval faithfulness via citation recall/precision, and (iii) report quality (clarity, insight) using LLM-as-judge prompts. Empirically, systems keep similar rankings when swapping commercial search for this API, and the API achieves sub-0.5s median latency, suggesting it is a practical, research-grade substitute for proprietary search backends."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes DeepResearchGym, an open-source benchmarking framework specifically designed to enable transparent and reproducible evaluation of deep research systems. Being free and open-source makes this work a valuable resource to the community.\n\n2. Empirical evaluations show that the system achieves strong retrieval quality with minimal loss from approximate search, as well as maintaining response times below those attained by commercial APIs.\n\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. Although DeepResearchGym helps the reproduction of deep research systems with a higher response speed and lower cost, using static corpora may under-serve very time-sensitive queries.\n\n2. Despite showing high correlation with human evaluation, the empirical results only on Researchy Questions using GPT-4.1 as a judge might not comprehensively and robustly reflect the system's actual performance to serve as a replacement of search engine. Additionally, more fine-grained comparison between the proposed API vs different commercial API would be much appreciated. What's the fundamental difference between different search APIs, and how would that affect the performance of deep research systems?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NInKUBLScf", "forum": "EmBWPLSfe2", "replyto": "EmBWPLSfe2", "signatures": ["ICLR.cc/2026/Conference/Submission19843/Reviewer_8hFt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19843/Reviewer_8hFt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761613948048, "cdate": 1761613948048, "tmdate": 1762932016626, "mdate": 1762932016626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **DeepResearchGym**, an open-source benchmarking framework for evaluating deep research systems. The framework addresses critical reproducibility and transparency issues in current evaluation practices that rely on proprietary, dynamic commercial search APIs. The main contributions are: (1) A **free search API** built on large-scale public web corpora (ClueWeb22 and FineWeb) with dense retrieval (MiniCPM-Embedding) and approximate nearest neighbor search (DiskANN), achieving lower latency than commercial alternatives while ensuring stable rankings; (2) An **evaluation protocol** extending the Researchy Questions benchmark with LLM-as-a-judge metrics measuring report relevance (Key Point Recall/Contradiction), retrieval faithfulness (citation precision/recall), and report quality (clarity/insightfulness); (3) **Empirical validation** showing systems maintain comparable performance when switching from commercial to DeepResearchGym APIs, with human evaluation confirming metric reliability ($\\kappa = 0.72$-$0.89$)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Addresses critical reproducibility gap**: The field urgently needs standardized benchmarking infrastructure. The paper directly tackles cost, transparency, and reproducibility issues with commercial APIs.\n2. **Comprehensive multi-dimensional evaluation**: The three-faceted framework (relevance, faithfulness, quality) captures different aspects of report generation quality, going beyond simple surface-form metrics.\n3. **Nuanced analysis**: Query-level correlation analysis (Figure 2) and query log analysis (Appendix A) provide insights beyond system-level aggregates."}, "weaknesses": {"value": "1. **Heavy reliance on a single judge model without robustness analysis**: All automatic evaluations use GPT-4.1-mini exclusively as the judge, with no ablation studies using alternative models such as GPT-4o, Claude Sonnet, or open-source alternatives. This creates potential concerns about judge-specific biases, particularly since some evaluated systems (like OpenAI's deep research) are based on GPT models.\n2. **Insufficient examination of static corpus limitations and ground-truth quality**: While the paper demonstrates that systems maintain performance when switching to static corpora, it does not analyze when this approach is adequate versus problematic. There is no quantitative assessment of which query types are temporally sensitive and might suffer from the 2022-2024 snapshot. Additionally, the evaluation assumes that clicked documents from search logs constitute comprehensive ground truth, but this assumption is never validated. Users may click tangential documents or miss important sources, yet the paper provides no analysis of click quality or relevance distribution."}, "questions": {"value": "1. What is the correlation between report length and KPR? Are higher-scoring systems simply more verbose?\n2. What are the most common failure modes? Can you provide qualitative analysis of queries where systems perform poorly vs. well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ewS1d8X7q4", "forum": "EmBWPLSfe2", "replyto": "EmBWPLSfe2", "signatures": ["ICLR.cc/2026/Conference/Submission19843/Reviewer_8rKV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19843/Reviewer_8rKV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682663276, "cdate": 1761682663276, "tmdate": 1762932016194, "mdate": 1762932016194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces DeepResearchGym, an open-source sandbox for deep research agents. The sandbox features a search API implementation that is reproducible to facilitate future research and agent development. Based on the sandbox, the authors further evaluated several state-of-the-art deep research agents and presents insight into their performance along different axes, including relevance, faithfulness, and quality. Results show that agents can achieve comparable performance when using DeepResearchGym compared to using commercial search APIs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper contributes an open-source sandbox that can potentially facilitate reproducible research in deep research agents.\n2. The authors deliver some insight into fine-grained performance of popular deep research agents."}, "weaknesses": {"value": "1. line 154-155: While the authors use recent data sources to construct the sandbox, it is unclear from the paper how to keep the sandbox \"up-to-date\".\n2. The contribution beyond the sandbox is limited, as the authors mostly follow existing work in their evaluation protocol. \n3. While I understand the importance of having a reproducible environment for benchmarking deep research agents, I fail to see the discussion on the actual benefit from this paper. What are the evaluation nuances that DeepResearchGym helps to capture which are infeasible with time-varying search APIs?\n4. In section 4.3, the authors use the variability observed in query-level analysis to justify the importance of using a standard retrieval API. I think the logic is flawed. Doesn't this show that the evaluation metrics, which heavily relies on textual overlap, are sensitive the search APIs used?\n5. For section 4.4, I don't see the meaning of this pairwise human evaluation when the main results in the paper are pointwise. Why not letting human evaluators follow the same protocol as LLM-as-a-judge and compute score correlations?"}, "questions": {"value": "1. line 144, how do you define \"low quality\"?\n2. line 178-179, how do you define \"minimal loss\"?\n2. Section 4.2, why are subjective report quality metrics (Clarity, Insight) comparable to objective information coverage metrics (KPR)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DTusDh9CBy", "forum": "EmBWPLSfe2", "replyto": "EmBWPLSfe2", "signatures": ["ICLR.cc/2026/Conference/Submission19843/Reviewer_Rq91"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19843/Reviewer_Rq91"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978396170, "cdate": 1761978396170, "tmdate": 1762932015701, "mdate": 1762932015701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}