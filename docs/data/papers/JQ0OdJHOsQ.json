{"id": "JQ0OdJHOsQ", "number": 19926, "cdate": 1758300657636, "mdate": 1759897011926, "content": {"title": "Lightweight Latent Verifiers for Efficient Meta-Generation Strategies", "abstract": "Verifiers are auxiliary models that assess the correctness of outputs generated by base large language models (LLMs). They play a crucial role in many strategies for solving reasoning-intensive problems with LLMs. Typically, verifiers are LLMs themselves, often as large (or larger) than the base model they support, making them computationally expensive. In this work, we introduce a novel lightweight verification approach, LiLaVe, which reliably extracts correctness signals from the hidden states of the base LLM. A key advantage of LiLaVe is its ability to operate with only a small fraction of the computational budget required by traditional LLM-based verifiers. To demonstrate its practicality, we couple LiLaVe with popular meta-generation strategies, like best-of-n or self-consistency. Moreover, we design novel LiLaVe-based approaches, like conditional self-correction or conditional majority voting, that significantly improve both accuracy and efficiency in generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of extracting latent information from the hidden states of LLMs, and opens the door to scalable and resource-efficient solutions for reasoning-intensive applications.", "tldr": "We show that it is possible to cheaply and reliably extract correctness signal from the hidden states of LLMs; this is useful in practice to enhance accuracy and compute-efficiency on reasoning tasks.", "keywords": ["reasoning", "large language models", "verification", "uncertainty estimation", "interpretability"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1b7b23d933c927aa777313018dc464981b951e5c.pdf", "supplementary_material": "/attachment/db59ff4c45faafa94e114241cb16ca62461e5f2d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces LiLaVe (Lightweight Latent Verifier), a novel approach for verifying the correctness of LLM-generated solutions to reasoning-intensive tasks by extracting signals from the hidden states of the base language model. The key innovation is training a lightweight XGBoost classifier on hidden states extracted from specific layers and token positions during generation, labeled by answer correctness. The authors demonstrate that LiLaVe achieves competitive verification performance with large LLM-based verifiers while requiring only approximately 5k training examples and running efficiently on CPU. The paper further introduces practical meta-generation strategies leveraging LiLaVe, including conditional majority voting and conditional self-correction, which improve both accuracy and computational efficiency on mathematical reasoning benchmarks (GSM8K, GSM-Symbolic, MATH, algebra_linear_1d).\nThe work provides both theoretical insights into what information is encoded in LLM hidden states and practical contributions toward resource-efficient verification for reasoning tasks. The authors conduct extensive ablation studies to determine optimal extraction locations (layers and tokens) and demonstrate effectiveness across multiple base models (Llama 3.1 8B, Gemma 2 2B, Phi-3.5-mini)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a practically important problem of expensive LLM-based verification with a simple and effective solution that achieves comparable performance while being orders of magnitude faster.\n- The experimental evaluation is comprehensive, covering four different benchmarks, three base LLMs, and multiple meta-generation strategies with clear performance improvements demonstrated in Figures 3-5.\n- The systematic analysis of which hidden states (layers and tokens) contain optimal correctness signals in Section 3.2 and Figure 1 provides valuable insights into the internal mechanics of language models.\n- LiLaVe requires only approximately 5k training examples compared to 250k+ for LLM-based verifiers, making it highly practical for data-scarce scenarios and quick adaptation to new datasets.\n- The novel conditional majority voting and conditional self-correction strategies demonstrate practical utility and achieve better accuracy-efficiency tradeoffs than baselines, as shown in Figures 4-5.\n- Training on CPU in under 15 minutes and achieving 350× inference speedup compared to LLM-based verifiers provides significant practical advantages for deployment.\n- The paper demonstrates good transferability across different base models (Llama, Gemma, Phi) as shown in Table 2, suggesting the approach is not overly model-specific."}, "weaknesses": {"value": "- The core novelty is limited as probing hidden states for information extraction is well-established, and Bui et al. (2025) presents a similar approach for code generation, making this work somewhat incremental.\n- The comparison with process reward models is insufficient, appearing only in Appendix B.10 with mixed results where Math-Shepherd sometimes outperforms LiLaVe on GSM-style tasks.\n- Several design choices appear ad-hoc despite ablation studies: the specific temperature mixture (0, 0.25, 0.5, 0.75, 1.0), the choice of layers (-1, -2, -4, -8, -16), and tokens (-1 through -16) lack strong theoretical justification.\n- The paper lacks analysis of failure modes or deeper investigation into what patterns the XGBoost model learns from hidden states and why these representations contain correctness information.\n- Cross-dataset transfer results in Table 4 show significant performance degradation (e.g., MATH-trained verifier achieving only 0.53 AUC on algebra_linear_1d), limiting claims about generalization.\n- The verifier-oracle gap shown in Figure 10 reveals substantial room for improvement, with oracle selection achieving much higher accuracy than any verifier-based strategy.\n- Some experimental choices lack justification: why these specific four datasets, why these particular numbers of votes in majority voting experiments, and how sensitive is performance to hyperparameter variations.\n- The MATH results with Gemma 2 2B (AUC 0.53 in Table 2) suggest the approach may struggle when the base model has very low accuracy (approximately 5%), indicating potential limitations."}, "questions": {"value": "1. Can you provide more analysis of what the XGBoost model learns? For example, feature importance analysis or investigation of which hidden state dimensions are most predictive of correctness?\n2. How does LiLaVe performance scale with base model size? Would the same approach work with larger models (70B+) or smaller models (1B-3B)?\n3. In the conditional self-correction experiments (Figure 4), some datasets show minimal improvement. Can you explain why conditional self-correction works well for algebra_linear_1d but provides limited benefit for MATH?\n4. The cross-dataset transfer results (Table 4) show significant drops. Have you investigated why MATH-trained verifiers transfer poorly, and could ensemble approaches improve transfer?\n5. How does the computational overhead of extracting and storing hidden states during generation compare to the verification speedup? What is the end-to-end latency comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rbbWGyDiWV", "forum": "JQ0OdJHOsQ", "replyto": "JQ0OdJHOsQ", "signatures": ["ICLR.cc/2026/Conference/Submission19926/Reviewer_KwTJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19926/Reviewer_KwTJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760649125717, "cdate": 1760649125717, "tmdate": 1762932093060, "mdate": 1762932093060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes learning a simple lightweight verifier from LLM hidden weights, to guide inference-time scaling methods. The method, LiLaVe, is shown to be reasonably well-aligned with oracle correctness, and boosts performance in various reasoning-intensive benchmarks for some small-mid-size open-source language models (2-8B). The authors detail the impact of different inference parameters on LiLaVe."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Overall: The paper dives into the verification and test-time scaling, topics of particular interest today. Instead of token-level verifier methods, the authors work with hidden states, using methods more aligned with those from the mechanistic interpretability communities. The method is well-motivated, experiments well-executed, and results well-detailed. I think that people in the research community would find this an enjoyable interesting read.\n\n## Thorough experimentation\n\nThe experimentation with different design decisions, from architecture to inference parameters, makes sense and is reasonably well-documented. \n\n## Well-done analysis offers generalizable insights\n\nIn particular section 3 is quite strong. I appreciate that it offers interesting signal outside of latent verifiers itself, such as the role of temperature in simple test-time scaling methods and the interaction between sample budget and score threshold."}, "weaknesses": {"value": "## Impact of generation length?\n\nOne might wonder whether correctness signal is largely correlated with something as simple as reasoning length. LiLaVe uses fixed-relative indices to extract signal for accuracy (i.e. -1, -2, -4, -8...)-- it is possible that the latent verifier largely extracts something as simple as absolute position from the hidden state, and uses this to inform the prediction. It would be nice to see some comparisons between LiLaVe prediction and reasoning length. I am also curious how, given its simplicity, LiLaVe performs as a verifier when tested on reasoning lengths outside that observed in its training. \n\n\n## Specificity of the method to the model and task\n\nIt appears that LiLaVe demands a verifier to be trained for each combination of model and task. While each learned verifier is relatively lightweight, this could present a barrier to scalability. It would be interesting to test more general latent verifiers across a family of similar tasks for a given model. Or, at the least, show how the existing trained verifiers perform when tested on related but different tasks."}, "questions": {"value": "* nit: L049 \"Wu et al Wu et al\"\n* What were the lengths of the reasoning chains across tasks? \n* Did you observe \"a-ha\" moments in the reasoning chains used in this experiment, where the chain-of-thought transitions away from confusion or incorrectness? If so, how does the LiLaVe prediction change before and after those? i.e. is there correlation between token-level appearance of correctness and correctness predicted from hidden state?\n* Do LiLaVe verifiers generalize across tasks? In the appendix it is noted that the Phi-2B model struggled in the MATH domain where the model may not have generated enough correct generations to train LiLaVe. What happens if the GSM8k Phi LiLaVe verifier was used in the MATH domain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "taAFLWzgOq", "forum": "JQ0OdJHOsQ", "replyto": "JQ0OdJHOsQ", "signatures": ["ICLR.cc/2026/Conference/Submission19926/Reviewer_ZZw5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19926/Reviewer_ZZw5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761671681857, "cdate": 1761671681857, "tmdate": 1762932092615, "mdate": 1762932092615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Typical verifiers are LLMs that are trained to assess the correctness of responses. However, these can be very expensive to train and deploy, especially in scenarios where one needs to verify many responses (e.g., scaling test-time compute through repeated sampling). This paper proposes an alternative approach where instead of post-training an LLM to verify, they simply train a dataset-specific classifier using the hidden states of the base LLM. This lightweight approach, LiLaVe, obtains high AUC in terms of its predictions on whether a response is correct or not and can be used in various strategies such as best-of-n and weighted majority vote."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Originality:\n- Using internal LM representations to train a verifier is a conceptually simple and underexplored idea.\n\nQuality:\n- Detailed justification for the method is provided (e.g., ablations across token index, layer depth, and temperature)\n- The appendix provides many extra details and results, such as comparison to oracle selection, transfer experiments, and using lilave on responses from different models (which allows LiLaVe to apply to closed-source models).\n\nClarity\n- Paper is generally well-written and ideas are presented clearly.\n\nSignificance\n- The method could have practical value in OOD, personalized, or domain-specific settings where off-the-shelf verifiers may struggle."}, "weaknesses": {"value": "Quality\n- Would be interesting to also train LiLaVe using either the base model's embeddings or BERT embeddings. \n- Some key baselines are lacking. While Table 1 compares AUC to existing reward models, the BoN and weighted MV performance of these reward models is missing. I was expecting to see Figure 3 also have curves for ORM-Mistral (BoN and WMV) and ORM-Deepseek (BoN and WMV). \n- My main concern is that the gains are modest overall and fade at larger test-time scales; most improvements appear only in low-sample regimes. The gap between LiLaVe and oracle verifier remains large.\n- Moreover, the poor asymptotic performance of BoN as N increases raises concerns about LiLaVe. In particular, I wonder if high AUC/a classifier is the best thing to learn, since selecting among N may be more of a process that requires high precision and ranking ability.\n\nSignificance\n- Despite being well-executed, the empirical impact is modest and highly dataset-specific. The method’s dependence on labeled data and limited scaling make it less practical for large-scale deployment. In general settings, I suspect that off-the-shelf verifiers still obtain the best accuracy (although I agree at a higher cost too)."}, "questions": {"value": "1. How would an embedding-based classifier compare to LiLaVe?\n2. What are the Best-of-N and Weighted Majority Vote curves like for reward models like ORM-Mistral and ORM-Deepseek?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RNVY8Baxnl", "forum": "JQ0OdJHOsQ", "replyto": "JQ0OdJHOsQ", "signatures": ["ICLR.cc/2026/Conference/Submission19926/Reviewer_8DqU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19926/Reviewer_8DqU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135751259, "cdate": 1762135751259, "tmdate": 1762932092195, "mdate": 1762932092195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LiLaVe, a light-weight logits-based classifier for math verification. First, the paper analyzes the position and layer-dependence of the hidden-states. Then it trains a classifier to verify LLM solutions of math reasoning tasks. Lastly, the trained classifier is applied in adaptive majority voting and self-correction to improve the answer quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "the paper is well-written, and analysis is comprehensive."}, "weaknesses": {"value": "* authors should add analysis on other reasoning datasets, e.g. AIME, AMC, GPQA-diamond etc. and perform in-distribution out-of-distribution analysis, i.e. adding part of the new data into training or not, and then compare the verification accuracy of different variants. this is to check the practicality of the proposed approach.\n\n* accuracy-efficiency analysis is also important to better understand lilave's implications on test-time-scaling. figure 1 in ref [1] is a good example. authors should compare the efficiency between conditional majority voting, self-correction and conventional majority voting approach; and also compare verifier-based and verifier-free methods, since [1] mentioned that SC (majority voting) may be more efficient without verifiers in realistic inference budgets. this analysis should be done on a wide range of datasets, MATH500, GSM8K, AIME, AMC, GPQA-diamond etc.\n\n* more advanced classifier-based verifiers need to be added and compared. e.g. the sota ones on rewardbench leaderboard[2,3].\n\n* line 399 mentioned the tuning of tau to optimize the verifier performance, but this is less practical in realistic use cases when no ground truths are available.\n\n[1] https://arxiv.org/abs/2504.01005\n\n[2] https://arxiv.org/abs/2403.13787\n\n[3] https://arxiv.org/abs/2506.01937"}, "questions": {"value": "* Figure 5 appears confusing to me: how is each dot generated? more explanation will be appreaciated\n\n* line 248: “*On one hand, for reasoningintensive problems, low temperatures typically result in better performance*” why is this true? for example, both Qwen3[1]  and DeepSeek R1[2] models suggest using non-0 sampling temperature\n\n* line 538: “*Llama 3.1 8B achieves on it results*” should be “*Llama 3.1 8B achieves results*”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "V7yDkjYEhs", "forum": "JQ0OdJHOsQ", "replyto": "JQ0OdJHOsQ", "signatures": ["ICLR.cc/2026/Conference/Submission19926/Reviewer_5yvv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19926/Reviewer_5yvv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762139631150, "cdate": 1762139631150, "tmdate": 1762932091719, "mdate": 1762932091719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}