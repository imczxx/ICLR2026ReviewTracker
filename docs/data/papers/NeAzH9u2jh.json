{"id": "NeAzH9u2jh", "number": 24123, "cdate": 1758353030401, "mdate": 1759896780883, "content": {"title": "Universal Value-Function Uncertainties", "abstract": "Estimating epistemic uncertainty in value functions is a crucial challenge for many aspects of reinforcement learning (RL), including efficient exploration, safe decision-making, and offline RL. While deep ensembles provide a robust method for quantifying value uncertainty, they come with significant computational overhead. Single-model methods, while computationally favorable, often rely on heuristics and typically require additional propagation mechanisms for myopic uncertainty estimates. In this work we introduce universal value-function uncertainties (UVU), which, similar in spirit to random network distillation (RND), quantify uncertainty as squared prediction errors between an online learner and a fixed, randomly initialized target network. Unlike RND, UVU errors reflect policy-conditional $\\textit{value uncertainty}$, incorporating the future uncertainties $\\textit{any policy}$ may encounter. This is due to the training procedure employed in UVU: the online network is trained using temporal difference learning with a synthetic reward derived from the fixed, randomly initialized target network. We provide an extensive theoretical analysis of our approach using neural tangent kernel (NTK) theory and show that in the limit of infinite network width, UVU errors are exactly equivalent to the variance of an ensemble of independent universal value functions. Empirically, we show that UVU achieves equal performance to large ensembles on challenging multi-task offline RL settings, while offering simplicity and substantial computational savings.", "tldr": "We introduce a single-model method for directly estimating value-function uncertainty for any given policy.", "keywords": ["Uncertainty Quantificaiton", "Epistemic Uncertainty", "Exploration", "Offline RL", "Neural Tanget Kernel", "Multitask RL"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f0e55fbf0685ebae2ab588af97faeacc66a54ea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Universal Value-Function Uncertainties (UVU), a single network method for estimating policy-conditioned value uncertainty. The key idea is to train an online predictor via semi-gradient TD learning on synthetic rewards generated by a fixed random target network, and interpret the prediction error as an uncertainty measure. Using neural tangent kernel (NTK) theory, the authors derive post-training distributions of TD trained functions and show that, under the infinite-width and gradient-flow limit, the expected UVU prediction error is equivalent to the ensemble variance of universal $Q$ function. A multi-head version further yields a finite-sample equivalence to a scaled $\\chi^2$ distribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Conceptual simplicity**: UVU replaces RND’s myopic novelty signal with policy-conditioned, future-aware value uncertainty. The concept of modeling TD error between learner and fixed random teacher provides an intuitive mechanism for epistemic uncertainty estimation.\n- **Theoretical grounding**: The paper rigorously analyzes TD learning dynamics under NTK, presenting explicit mean-covariance forms for post-training predictions and proving the equivalence between UVU’s prediction error and ensemble variance.\n- **Empirical effectiveness**: The proposed multi-head network achieves ensemble-level uncertainty quality while reducing computational cost by roughly an order of magnitude."}, "weaknesses": {"value": "1. **Idealized stability condition**: Theoretical TD learning convergence condition (i.e., $\\Theta_{XX}-\\gamma\\Theta_{X'X}\\succ0$) depends on the transition structure and the choice of $\\gamma$, which is difficult to verify or enforce in practice. Although the appendix suggests using Gershgorin’s theorem and gradient-norm conditioning to ensure diagonal dominance, it remains unclear whether this condition holds empirically or how violation manifests as instability (e.g., divergence of TD loss).\n2. **Theory–implementation gap**: The theory assumes gradient flow, full-batch optimization, and stop-gradient TD, whereas the implementation employs mini-batches, target networks, and finite training steps. These deviations could introduce bias or affect convergence, yet no quantitative analysis is provided. In particular, the multi-head version uses a shared trunk, violating the independence assumption in Corollary 2 and potentially leading to systematic underestimation of uncertainty.\n3. **Limited empirical generality**: Experiments are limited to a single offline MiniGrid setting. Evaluation on continuous control, stochastic, or partially observable environments would strengthen generalizability and robustness."}, "questions": {"value": "1. Is there any observed correlation between the smallest eigenvalue of $\\Theta_{XX}-\\gamma\\Theta_{X'X}$ and TD loss divergence?\n2. How does the implementation gap such as inter-head correlations affect the uncertainty calibration?\n3. Do you have plans to extend UVU to continuous control (e.g., D4RL) or stochastic gridworld environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tall26eLcE", "forum": "NeAzH9u2jh", "replyto": "NeAzH9u2jh", "signatures": ["ICLR.cc/2026/Conference/Submission24123/Reviewer_1jtb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24123/Reviewer_1jtb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622974352, "cdate": 1761622974352, "tmdate": 1762942947033, "mdate": 1762942947033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces universal value-fucntion uncertainties (UVU), quantifying uncertainty as squared prediction errors between an online learner and a fixed, randomly initialized target network. The paper also provides a theoretical analysis of the proposed approach using neural tangent kernel (NTK) theory, showing that  UVU errors are equivalent to the variance of an ensemble of independent universal value functions in the limit of infinite network width."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper aims to address an important issue in RL, and the theoretical analysis is strong."}, "weaknesses": {"value": "The central concern with the paper is the numerical study. Suppose that the true values are known for the simulated environment, the experiments should include coverage rates (and related calibration metrics). Reporting only the best policy’s returns with confidence intervals does not fully characterize estimator performance or uncertainty."}, "questions": {"value": "My major concern is about the coverage rate of the true values. The authors may consider a simulated environment for which the true value functions are exactly known and demonstrate that the confidence intervals constructed by the proposed method can achieve the nomial coverage rate for the true value. Such an evaluation criterion is critically important for real decisions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "idSKGs6nSk", "forum": "NeAzH9u2jh", "replyto": "NeAzH9u2jh", "signatures": ["ICLR.cc/2026/Conference/Submission24123/Reviewer_4xcK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24123/Reviewer_4xcK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877926318, "cdate": 1761877926318, "tmdate": 1762942946730, "mdate": 1762942946730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an approach for general uncertainty estimation of a value function, inspired by random network distillation. The authors present a theoretical motivation for their approach and showcase the effectiveness in an empirical analysis with a gridworld RL task.\n\nThis is a well-crafted paper, presenting a convincing and rigorous theoretical analysis. While strictly speaking, the novelty is limited, and the empirical analysis is restricted to a specialized setting, I still think it is a valuable contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper is crafted very well, with high-quality writing, figures, and appendices\n+ The topic of uncertainty estimation for RL (including for critics/value functions) remains relevant, with no widely accepted approaches. Existing techniques like deep ensembles or MC Dropout are available, but are not widespread (e.g., due to the additional computational demands or potential training instability), and MC Dropout in particular has been shown to provide mixed results\n+ The motivation and preliminary sections are comprehensive and seem to cover all important aspects\n+ The theoretical analysis utilizing NTK to show equality of uncertainty estimation to ensembles is convincing, and provides a strong justification of the approach\n\nNeutral: The paper draws on existing work (RND and NTK). Compared to RND, it provides a theoretically grounded analysis and extension; compared to work on NTK, it is a novel application domain."}, "weaknesses": {"value": "- The empirical analysis is somewhat limited (although it is an illustrative use case). I would love to see some additional experiments beyond this scenario. As the method is generalizable, I feel it should be possible to apply it to common benchmark environments (similar to the original RND paper)\n- The lack of any implementation limits impact, and also makes it difficult to fully assess reproducibility."}, "questions": {"value": "- You have only demonstrated the performance of UVU on an offline RL task. Do you expect UVU to behave differently in an online RL setting? During online training, the state distribution shifts, leading to a shifting critic as well (the critic network might be subject to catastrophic forgetting). Can this be accurately captured by UVU?\n- During reading, i was strugling a bit with the introduction of the multi-head variant of UVU (maybe just due the clarity of writing). Could you strenghten the reasoning behind this analysis? In which case would a single UVU network no be sufficient as an estimator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wcoxtQ36GK", "forum": "NeAzH9u2jh", "replyto": "NeAzH9u2jh", "signatures": ["ICLR.cc/2026/Conference/Submission24123/Reviewer_DHG9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24123/Reviewer_DHG9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925089812, "cdate": 1761925089812, "tmdate": 1762942946404, "mdate": 1762942946404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel and computationally efficient single-network method (called \"UVU\") for estimating epistemic uncertainty in reinforcement learning. This approach measures uncertainty as the prediction error between an \"online\" network and a fixed, randomly initialized \"target\" network. The online network is trained using temporal difference learning on a synthetic reward signal generated by the target network itself, allowing the error to capture long-term value uncertainty. The paper provides a strong theoretical proof showing that, under infinite-width network assumptions, this single-model error is mathematically equivalent to the variance of a large, computationally expensive ensemble."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Improving the computational efficiency of epistemic uncertainty quantification is a key objective for RL algorithms.\n\n2. The paper provides a formal proof, grounded in Neural Tangent Kernel (NTK) theory, that its single-network uncertainty estimate is mathematically equivalent to the variance of an infinite ensemble, moving it beyond a simple heuristic methods like Random Network Distillation (RND).\n\n3. The policy-conditioned nature of the value function enables UVU to capture long-term, plan-dependent uncertainty. By making the policy representation an explicit input, the model is forced to learn a \"universal\" value function that understands how outcomes change under different plans. This design is crucial because it allows the temporal difference (TD) learning process to properly propagate uncertainty over time. If a specific policy leads to novel or data-sparse states far in the future, the TD updates naturally \"back up\" that high uncertainty to the agent's current state, providing a holistic, long-term risk assessment for that specific plan, rather than just a myopic \"novelty\" score of the current state."}, "weaknesses": {"value": "**Lack of empirical results** \n\nA potential weakness is the paper's limited evaluation of the intrinsic quality of the uncertainty estimates. The experiments focus on a binary task-rejection benchmark, which demonstrates the utility of the uncertainty but doesn't thoroughly analyze its calibration, its correlation with true value error across different states, or how it behaves in simpler, more controlled environments.\n\n1. The paper compares its approach to Random Network Distillation (RND) and ensemble-based methods for epistemic uncertainty estimation, but it does not provide results on standard online RL benchmarks, where these methods have traditionally been evaluated. Including such results is crucial to demonstrate the practical utility of the learned uncertainty for efficient or safe exploration.\n\n2. Because the method estimates policy-conditioned value uncertainty, its applicability in single-task settings—where the policy continuously evolves as the agent learns about the environment—remains unclear. This raises questions about the method’s feasibility in such scenarios."}, "questions": {"value": "1. Could the authors provide results demonstrating that the method improves sample efficiency in online RL algorithms or facilitates safer exploration?\n\n2. Is the method inherently limited to offline data and multi-task settings—where the number of policies is constrained and policy conditioning is easier to learn—compared to online RL, where the policy continually evolves as the agent learns the task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DmBV0hloRf", "forum": "NeAzH9u2jh", "replyto": "NeAzH9u2jh", "signatures": ["ICLR.cc/2026/Conference/Submission24123/Reviewer_wiBU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24123/Reviewer_wiBU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993897966, "cdate": 1761993897966, "tmdate": 1762942946126, "mdate": 1762942946126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}