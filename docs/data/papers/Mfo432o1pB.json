{"id": "Mfo432o1pB", "number": 10314, "cdate": 1758166777082, "mdate": 1759897659395, "content": {"title": "Automatic visual concept rankings for large multimodal models", "abstract": "Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. While traditional audits range from costly clinical trials to automatic benchmark evaluations, recent advances in automatic interpretability use AI systems to explain other AI models at scale. We introduce an algorithm for identifying salient visual concepts within large multimodal models (LMMs) and demonstrate that leveraging model internals yields more causally relevant insights than black-box approaches. Applying our method to two medical tasks (skin lesion classification and chest radiograph interpretation), we both uncover verifiable conceptual dependencies of LMMs and identify ways in which automatic concept labels may be misleading, highlighting both the promise of automatic interpretability for auditing and the continued importance of expert-in-the-loop oversight.", "tldr": "An automatic method for identifying important visual concepts used by large multi-modal models", "keywords": ["interpretability", "lmm", "vlm", "cav", "probes", "concept", "explainable ai", "xai", "multimodal", "llm", "vision"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/34b8877bf0379d0b8d67edd06b8c84de03444b8e.pdf", "supplementary_material": "/attachment/615121d751000960cc263d498a9d0ad8f7b40f1f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Visual Concept Ranking (VCR), an interpretability method designed to explain the output of large multimodal models (LMMs). The core idea is to identify concepts that are causally relevant to the model's output. The method first learns Concept Activation Vectors (CAVs) by probing an LMM's internal activations and then ranks these concepts by calculating the directional derivative of the model's output log-probabilities with respect to these CAVs. The authors demonstrate their method in medical tasks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's primary strength lies in its principled approach to identifying causal concept influence. By using gradient-based directional derivatives, it provides a more robust measure of concept importance.\n- The VCR algorithm is clearly explained in four distinct steps (Fig 1) and appears technically sound. The scalability analysis (Fig 4) is also a useful addition."}, "weaknesses": {"value": "- Although mentioned in the article, the key \"shortcut\" finding (Fig. 9) was not identified by the algorithm itself, but required manual human inspection of the surfaced images. This seems to severely contradict the author's claim of \"automatic interpretability.\"\n- Since there are so many existing methodologies related to concept bottleneck, it seems that there should be more comparisons with existing methodologies."}, "questions": {"value": "- It would be helpful to compare your research with Kim et al [1]. This seems essential, as it's similar to a prior study that used llm and clip to automatically generate concepts.\n- It would be better if we could see the quality evaluated directly by experts.\n\n\n[1] Kim, Injae, et al. \"Concept bottleneck with visual concept filtering for explainable medical image classification.\" International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VDoghnX8NQ", "forum": "Mfo432o1pB", "replyto": "Mfo432o1pB", "signatures": ["ICLR.cc/2026/Conference/Submission10314/Reviewer_Ey7F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10314/Reviewer_Ey7F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573945383, "cdate": 1761573945383, "tmdate": 1762921656754, "mdate": 1762921656754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VCR, a method for automatically interpreting large multimodal models by identifying which visual concepts actually drive their predictions. It uses a vision-language model to label image concepts, maps those to the model's internal activations, and measures how changes in each concept affect the model's output. The authors tested it on medical image datasets to demonstrate its effectiveness, though it still depends on the quality of automated labelling and requires human oversight for final interpretation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The work presents a gradient-based concept-activation analysis for LMMs, extending LG-CAV. \n2) It proposes a label-free interpretability pipeline using OpenCLIP for automated concept generation, improving scalability for large concept sets. \n3) The work provides a scalable, generalisable framework for concept-level interpretability in multimodal models.\n4) The method is mostly rigorous and reproducible. \n5) The paper is well-written and logically structured; it is reasonably accessible. \n6) The testing demonstrates practical value on real-world medical datasets, offering insight into model reasoning and shortcut behaviours in safety-critical domains."}, "weaknesses": {"value": "1) Lack of stronger baselines: While the work mentions using methods such as MA-MONET, it is unclear exactly what that entails. Second, the work explicitly states that it extends Language-Guided CAVs (LG-CAV); therefore, it would be more convincing to include LG-CAV as a baseline as well.\n\n2) Lack of understanding of the model size effect: While the work compares 3B and 4B models, we do not know how well the model will work with larger models. The comparison between the 3B and 4B OpenFlamingo models provides minimal insight into scaling behaviour. It remains unclear how the method performs on larger or different LLMs.\n\n3) Human-in-the-loop: While the authors acknowledge the value of expert oversight to mitigate labelling errors, the paper lacks specifics on when and how human review would be incorporated, what criteria experts would apply, or how their input might quantitatively improve results. Clearer workflow definitions and evaluation of inter-rater reliability would strengthen this argument.\n\n4) Lack of investigation of components: The framework relies exclusively on OpenCLIP for automatic concept labelling, but the authors do not explain why it was selected or whether they tested other vision-language models.\n\n5) Causal inference is claimed but not fully established: This phrase kind of weakens the claim: \"it's likelihood of calling a radiograph abnormal, which is another classic example of a 'shortcut.\" The statement implicitly acknowledges that the findings may reflect internal correlations rather than true causal reasoning. The method remains valuable for diagnostic interpretability, but causal claims should be moderated/toned down.\n\n- Minor comment: There is no need to define LMMs multiple times."}, "questions": {"value": "1) Why do you think the t-test is appropriate for the tests you have done? Could you just make the choice briefly in the paper/appendix?\n2) Why were the significance tests not discussed in the paper? \n3) Are there any assumptions being made about the correlation of the concepts? If so, please list these alternatives and briefly discuss their implications.\n4) What were the results like when tested with < 500 concepts, e.g., 20, 30, 50, 100, etc? Was there a specific reason for starting at 500? Please include the justification in the paper. \n5) What were the specific reasons for just testing 3B and 4B models? Are there plans to test with other LLMs?\n6) Could you elaborate exactly how human oversight would solve the interpretability problem you are trying to address in the paper?\n7) The work uses OpenCLIP. Did the authors explore other options? Why/why not?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "py3WjfWkft", "forum": "Mfo432o1pB", "replyto": "Mfo432o1pB", "signatures": ["ICLR.cc/2026/Conference/Submission10314/Reviewer_T1q8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10314/Reviewer_T1q8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897535747, "cdate": 1761897535747, "tmdate": 1762921656414, "mdate": 1762921656414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Visual Concept Ranking (VCR), an algorithm for auditing large multimodal models by identifying which visual concepts causally influence their outputs. The method learns concept activation vectors (CAVs) by mapping LMM activations to concept scores from an external VLM (like OpenCLIP), operating without expert labels. VCR then ranks these concepts based on the gradient of the LMM's log-probability output with respect to each CAV. It is tested on OpenFlamingo-3B-Instruct and OpenFlamingo-4B on two medical datasets, with better performance than correlation-based concept selection and R^2-based concapt ranking"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a critical extension of concept-based interpretability to LMMs\n- VCR is \"automatic\" and does not require expensive, expert-annotated concept datasets\n- The visualizations are clear and informative, making the audience understand the concepts quickly"}, "weaknesses": {"value": "- One major shortcoming is that the interpretability audit is only as reliable as the concept labels provided by the external VLM. The VLMs might have spurious correlation (like the purple ink marking example in line418-431), implicit bias, or lack the nuance for specialized domains. It's unclear how to mitigate this potential risk, especially given the main application is in safety-critical areas.\n- The method relies on a predefined set of textual concepts and images. It's unclear how to select the set of text and images, and the effect of size and domain relevance.\n- The title (\"Automatic Visual Concept Rankings for Large Multimodal Models\") suggests a general-purpose method, but all experiments are confined to the medical domain. While the authors suggest it could be applied to general data using vocabularies like Google's Trillion Word Corpus, no such experiments are provided. It is unclear how well this method performs on more abstract or general-domain tasks without this validation."}, "questions": {"value": "- Could the authors elaborate on the novelty of VCR compared to LG-CAV? The CAV-learning pipeline seems to be a direct application of LG-CAV (without the three additional modules). \n- What is the exact definition of activation in step 2 (l118-126)?\n- As general VLM might lack the nuance for specialized domains, how would VCR's findings change if using a VLM trained in medical data?\n- The experiments would be significantly enhanced if more models (such as llava or qwen-vl) can be included.\n- Could the authors add visual comparison of activating images of concepts for VCR and baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BcAXEMHe0v", "forum": "Mfo432o1pB", "replyto": "Mfo432o1pB", "signatures": ["ICLR.cc/2026/Conference/Submission10314/Reviewer_8v6i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10314/Reviewer_8v6i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141023695, "cdate": 1762141023695, "tmdate": 1762921656005, "mdate": 1762921656005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}