{"id": "bVyg58mjhF", "number": 24281, "cdate": 1758354906248, "mdate": 1763256395734, "content": {"title": "Robust Latent Neural Operators through Augmented Sparse Observation Encoding", "abstract": "Neural operator methods have achieved significant success in the efficient simulation and inverse problems of complex systems by learning a mapping between two infinite-dimensional Banach spaces. However, existing methods still exhibit room for optimization in terms of robustness and modeling accuracy. Specifically, existing methods are characterized by sensitivity to noise and a tendency to overlook the importance of multiple sparse observations in new domains. Therefore, we propose a robust latent neural operator based on the variational autoencoder framework. In this method, an encoder utilizing recurrent neural networks effectively captures sequential patterns and dynamical features from domain-specific sparse observations. Subsequently, a neural operator in latent space, followed by a decoder, enables the effective modeling of the original system. Additionally, for certain higher-dimensional complex systems, opting for a lower-dimensional latent space can reduce task complexity while still maintaining satisfactory modeling performance. We evaluate our approach on multiple representative systems, and experimental results demonstrate that it achieves superior modeling accuracy and enhanced robustness compared to state-of-the-art baseline methods.", "tldr": "We propose a robust latent neural operator based on a VAE framework with an RNN encoder, enhancing modeling accuracy and noise resilience in operator learning.", "keywords": ["neural operator", "complex system", "variational inference"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3019fff0a3eda600da05b4eaafa3aca5c036f130.pdf", "supplementary_material": "/attachment/1c135cc04072039cac32be1aadcfd78e9d126067.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a model called RLNO with VAE framework for operator learning. The proposed model has a RNN-based encoder, neura operator in the latent space, and decoder. This framework enables us to deal with noisy and sparse input data with low computational costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed model is noise robust, computationally efficient, and can deal with temporal time interval inputs. They empirically show the performance of the proposed model with various situations."}, "weaknesses": {"value": "The presentation should be improved. For example, please define $b_k$ and $c_k$ in Eq. (2) and what is $x_1$ and $x_2$ in Eq. (10)? Also, in line 289, equation  (Eq. equation 8) shoud be equation (8)."}, "questions": {"value": "- In Section 3.2, the authors insist that an advantage of the proposed model over the ODE-RNN method is the computational cost. Comparing the computational time and the MSE of the proposed model and the ODE-RNN model would be interesting.\n- The authors insist that the proposed model accepts time series with nonconstant time intervals. Does the performance changes between the constant and nonconstant time interval cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lAP833RPG2", "forum": "bVyg58mjhF", "replyto": "bVyg58mjhF", "signatures": ["ICLR.cc/2026/Conference/Submission24281/Reviewer_KSwG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24281/Reviewer_KSwG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799593951, "cdate": 1761799593951, "tmdate": 1762943029427, "mdate": 1762943029427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response:"}, "comment": {"value": "We would like to thank the reviewers for your time, efforts, and valuable comments and suggestions, which do help us to significantly improve the quality of this work. Here, we succinctly summarize the novelty and contributions of this work and then list the main revisions implemented in the manuscript. \n\n**Point 1: the novelty and contributions of this work**\n\n(1) A Novel Fusion Framework. RLNO represents a novel approach grounded in VAE framework, incorporating an RNN-based encoder, latent neural operator and a decoder. This framework effectively harnesses the strengths of RNNs and neural operators, enabling more accurate operator learning. Specifically, the core idea of the proposed framework is to uncover the underlying low-dimensional manifold structure in observed data through latent space transformations. On this basis, the dynamical laws on this manifold can be learned in the latent space, and subsequently operator modeling can be achieved through a decoder. Experimental results demonstrate that this approach can more efficiently model a family of dynamical systems and achieve operator learning.\n\n(2) Utilizing More Dynamic Information. The proposed OPERATOR-RNN encoder extracts and leverages more domain-specific information and dynamic evolution patterns from the sequential observational data in new domains. It remains applicable under non-uniform sampling conditions, thereby improving the robustness and accuracy of the RLNO method. Specifically, current neural operator methods, including DeepONet and FNO, often only input the sampled functions (such as parameter functions, initial or boundary conditions) when tested in new domains. However, in many practical applications, there may exist prior observational data at several moments in this new domain, which is often overlooked in previous methods. Our research finds that fully leveraging these prior observational states has a very significant effect on promoting the robustness and accuracy of neural operator modeling. This benefit arises from domain-specific observations, which not only provide more state information but also extract specific dynamic information from these sequential observations. \n\n(3) Efficiency and Scalability. RLNO inherits the low computational costs of neural operators, significantly outperforming RNN and Neural ODE-based methods. And it can select a smaller latent space dimension, which reduces learning complexity and data requirements, enabling easier extension to high-dimensional complex systems. In addition, the framework presented in this study is general and flexible, and can be extended to other neural frameworks such as diffusion models, neural ordinary differential equations, and graph neural networks. This adaptability enables more robust and accurate dynamical predictions by effectively leveraging sparse observational data in new domains.\n\nFinally, we select a total of five systems for experimental validation, encompassing a synthetic dataset with periodic orbits of varying frequencies; two 1D PDE systems: the Diffusion-Reaction equation and the Kuramoto‚ÄìSivashinsky equation; as well as two 2D PDE systems: the Navier‚ÄìStokes equations and Rayleigh‚ÄìB√©nard convection. Additionally, we select 8 baseline methods (including approaches based on RNNs, classical neural ODEs, classical neural operators and latent neural operators), and conducted 4 ablation studies to affirm the significant contribution of each component in our methodology. To enhance the reliability and reproducibility of our experimental results, we also provide all the experiment codes in the supplementary materials. \n\n\n**Point 2: the main revisions implemented in the manuscript**\n\n(1) We supplement the Key Notation table in Appendix A.2 to enhance the clarity and readability of the article. \n\n(2) We meticulously review the entire text and tone down overly strong claims. Examples include the conclusion of the second paragraph in the Introduction section and the second paragraph of Section 4.1.\n\n(3) We refine the statement of Theorem 1 and carefully verify its proof in Appendix A.3. Furthermore, we supplement the corresponding discussions following the proof.\n\n(4) In Section 4.3, we further elaborate on the complex nature of the 2D Navier‚ÄìStokes and Rayleigh‚ÄìB√©nard PDE systems, thereby demonstrating that the selected experimental systems can be used to validate the effectiveness of the proposed method.\n\n(5) We supplement several references on ‚Äúlatent flow approaches‚Äù in the introduction section.\n\n(6) We carefully review the technical details in the paper to enhance readability. This includes supplementing and refining the notation explanations for Eqs. (2) and (10), and correcting the typographical errors in Line 289 of the original text.\n\nFinally, we thank all the reviewers again for your valuable and insightful comments. We hope that our General Response as well as the individual responses for each reviewer adequately addresses the reviewers‚Äô concerns."}}, "id": "RPovbpbwL4", "forum": "bVyg58mjhF", "replyto": "bVyg58mjhF", "signatures": ["ICLR.cc/2026/Conference/Submission24281/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24281/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24281/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763257578623, "cdate": 1763257578623, "tmdate": 1763257634272, "mdate": 1763257634272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Robust Latent Neural Operator (RLNO), a variational autoencoder‚Äìlike latent neural operator framework. The authors introduce an encoder, termed the OPERATOR-RNN, which takes as input a short window of possibly irregularly sampled time series data and outputs a Gaussian posterior over a finite-dimensional latent vector  $ùëß_0$. A latent DeepONet is then used as the evolution map in the latent space, generating a trajectory that is decoded back to the observed state space. The training objective maximizes an ELBO loss over $ùëß_0$. The authors perform ablations  are consider where the encoder is substituted with baselines, and vary the encoder window and the latent vector size. The authors consider a series of experiments on simple PDEs and compare against baselines with favorable results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The encoder architecture is a reasonable engineering upgrade to ODE/RNN/Decay approaches as it targets irregular sampling specifically.\n- The ablations are useful as they show how choices affect the prediction capabilities of the model. Also, they consider noisy signals.\n- The methods seems to be more accurate than the competition in the chosen metric.\n- Moreover, the encoder architecture represents a reasonable engineering refinement over existing ODE-, RNN-, or Decay-based approaches, as it is specifically designed to handle irregularly sampled data. However, methods for tackling non-equidistant sampling have already been explored in the literature, including graph-based and recurrent‚Äìconvolutional hybrids such as the GNN-tCNN and LSTM-tCNN architectures, which also operate on irregularly spaced observations."}, "weaknesses": {"value": "- The main novelty of the paper is the encoder engineering improvement. Different latent flow approaches such that Learning Effective Dynamics (see ‚ÄúMultiscale simulations of complex systems by learning their effective dynamics‚Äù), Latent ODEs (Learning the intrinsic dynamics of spatio-temporal processes through Latent Dynamics Networks), and generative diffusion models (see Generative learning for forecasting the dynamics of high-dimensional complex systems) which the authors do not compare against. What the authors propose is in a way a subset of these methods.\n- DeepONet is only effective when the outputs of the neural operator lie in a linear span of trunk features, which is not the case for complex physical systems. I would suggest the author to consider the approach described in Non linear Manifold Decoders for Operator Learning.\n- The neural operator terminology is a bit loose and misused because the DeepONet, as used here is a map between finite dimensional vectors, not functions spaces.\n- I believe that the sign for the 2D PDE is flipped, but perhaps this is a wrong interpretation of the derivation."}, "questions": {"value": "- How is $p(s_i | g(z_i))$ defined?\n- Can you consider non-GRF inputs, such as piece-wise, to test robustness beyond GRFs?\n- How does your method compare to the baselines described above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qeUM1KRanG", "forum": "bVyg58mjhF", "replyto": "bVyg58mjhF", "signatures": ["ICLR.cc/2026/Conference/Submission24281/Reviewer_GZq4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24281/Reviewer_GZq4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843521426, "cdate": 1761843521426, "tmdate": 1762943029033, "mdate": 1762943029033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method for learning operators between function spaces focusing on robustness to noise and ability to adapt to sparsely sampled observations. The method is named RLNO (Robust Latent Neural Operator), is designed to overcome the  degradation of that standard neural operator methods (e.g. DeepONet, FNO) when inputs are irregularly sampled or corrupted by noise.\nTo that end, RLNO introduces a variational autoencoder (VAE) structure around the neural operator: (1) an RNN-based encoder captures temporal and dynamical patterns from sequential or irregularly spaced observations, (2) a neural operator in latent space models the mapping between functional inputs and outputs, and (3) a decoder reconstructs the target functions in the original space.\nBy using a low-dimensional latent space, RLNO can tackle high-dimensional dynamical systems. Experiments on several benchmark PDE and dynamical datasets show improved accuracy and stability under noise compared to DeepONet, FNO, and related baselines."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Aim: Paper tackles important practical problem, the robustness of NO methods to spareness of samples and noise of observations, and proposes a promising approach.\n- Novelty: While each component (RNN-based temporal encoding, VAE-style probabilistic latent representation, and NO mappings) have been individually studied, RLNO‚Äôs novelty lies in integrating these three elements.\n- Experiments: Baselines are appropriate and the performed empirical study shows the how RLNO overcomes the degradation of the competitors"}, "weaknesses": {"value": "First, unfortunately, I find that the paper is **not well written**. The presentation of the work is lacking on important aspects: \n- Overly strong claims:  Several claims need more nuance, and some are debatable in literature,  just to name two - line 053 _\"superior computational efficiency without sacrificing accuracy\"_, line 268 _\"thereby demonstrating the necessity and superiority of our framework design\"_.\n- Flow, intuition and technical complexity: Paper overly focuses to high-level presentation, while technical aspects are often introduce in an abrupt manner.  This makes the content hard to parse and key contributions hard to identify.  \n- Notation: For my personal taste, notations are not ideal. Further, there is inconsistencies in Eq (1) line 145 and Algorithms 2 and 3 lines 720 and 748 of the Appendix\n\nSecond, there is lack of theoretical guarantees.. claims are made citing other papers and generalising the reasoning, but not formally backed. The only try to make theoretical claim is in Theorem 1 in Appendix A.3. The claim is sloppy/not formally correct (e.g. confusing typos in lines 762, 768, domain of f is not consistent with Eq. (S6) ) and the proof is not provided but as many other thingd in the paper just briefly hinged.\n\nFinally, since the core contribution is methodology, I find that the empirical study of two PDE problems (one 1D and other  2D) is a bit underwhelming w.r.t. publication quality requirements."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M68fi9e9GA", "forum": "bVyg58mjhF", "replyto": "bVyg58mjhF", "signatures": ["ICLR.cc/2026/Conference/Submission24281/Reviewer_dJZr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24281/Reviewer_dJZr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902145799, "cdate": 1761902145799, "tmdate": 1762943028665, "mdate": 1762943028665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work builds upon the development of neural operators, which has been a highly cited and used method for learning maps between measurements to full state estimates. Or more broadly mapping functions to functions.\n\nThe key idea in this paper is to do the neural operator mapping in latent space.  That is the basic innovation of the paper, and the authors show that his is a much more way to learn the operator than in the original measurement space.  This is consistent across many emerging real-world examples:  work in the latent space instead of the measurement space in order to improve performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a very solid and useful contribution to the field of neural operators.   A clear and important in the next step of neural operators as exploiting the latent space is clearly what should be done.  The results back this up."}, "weaknesses": {"value": "Not many examples were presented, and some are not very convincing (equation 11 is linear PDE is it not?).  I think something like 2D Kolmogorov flow in the turbulent regime would be a much more convincing set of data than what they have.  So I found the examples not to the level of where they should be."}, "questions": {"value": "It seems the \"sparsity\" has not been well characterized in how this works?  Do the authors have a metric for this?  It is important to establish when the sampling will actually work or not.\n\nHow long can the roll out in latent space go?  Most latent space long-time roll outs eventually diverge or break.  Is there any guarantee about a stable long-term roll out?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pyj9eqo18x", "forum": "bVyg58mjhF", "replyto": "bVyg58mjhF", "signatures": ["ICLR.cc/2026/Conference/Submission24281/Reviewer_UyLw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24281/Reviewer_UyLw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930789433, "cdate": 1761930789433, "tmdate": 1762943028417, "mdate": 1762943028417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}