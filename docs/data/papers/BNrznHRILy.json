{"id": "BNrznHRILy", "number": 21528, "cdate": 1758318548367, "mdate": 1759896917499, "content": {"title": "IndicJR: A Benchmark for Jailbreak Robustness of Multilingual LLMs in South Asia", "abstract": "Safety alignment of large language models (LLMs) is often evaluated in English and under rigid refusal contracts, leaving vulnerabilities in multilingual and script-diverse contexts underexplored. We introduce $\\textbf{Indic Jailbreak Robustness (IJR)}$, the first judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (~2.09 billion speakers). IJR covers 42,636 prompts across two tracks: $JSON$ (contract-bound) and $Free$ (naturalistic). \n\nOur findings reveal three consistent patterns. First, contracts inflate conservatism without preventing jailbreaks: in \\textsc{JSON}, LLaMA and Sarvam exceed $0.92$ JSR despite high refusal rates, while in ${Free}$ all models reach $\\approx$1.0 JSR with refusals collapsing. Second, English$\\to$Indic transfer is seamless, both instruction and format wrappers succeed, with \\emph{format} often stronger, showing that high-resource adversaries compromise low-resource languages. Third, orthography shifts matter: romanized and mixed inputs typically \\emph{reduce} JSR under ${JSON}$, but correlations with romanization share and tokenization features ($\\rho\\approx0.28$â€“$0.32$) show systematic effects rather than noise. Human audits (E5) confirm detector reliability, and lite-to-full comparisons (E7) show conclusions hold under reduced evaluation. Taken together, IJR establishes a reproducible, multi-language stress test that uncovers vulnerabilities invisible to English-only, contract-only benchmarks, and highlights unique risks for South Asian users where code-switching, romanization, and cross-lingual prompts are pervasive.", "tldr": "Indic Jailbreak Robustness (IJR): contracts overestimate safety; Free shows near-universal jailbreaks and strong English to Indic transfer.", "keywords": ["Jailbreak Robustness", "low-resource languages", "Indic languages", "less-resourced languages", "resources for less-resourced languages"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7358d153be232eda86a926d9c13a2e02eaae3a92.pdf", "supplementary_material": "/attachment/5863d33e494c0bf607697c196949d45af8710c79.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces IndicJR, the first comprehensive jailbreak robustness benchmark for South Asian languages, evaluating adversarial safety across 12 Indic languages. The benchmark employs a novel dual-track methodology comparing contract-bound evaluation (requiring structured JSON refusal schemas) against naturalistic free-form responses, enabling systematic assessment without human judges or expensive judge models. The findings reveal three main patterns: contracts create a false sense of security by inflating conservatism without preventing jailbreaks; English-to-Indic adversarial transfer succeeds universally; and orthography shifts systematically affect robustness through tokenization effects."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This benchmark fills an important need for systematic adversarial safety evaluation in South Asian languages, which collectively represent over 2 billion speakers but remain severely underserved in AI safety research. Furthermore, the systematic evaluation across native script, romanized, and mixed orthographies is a valuable contribution that reflects real-world usage patterns in South Asia where code-switching and romanization are prevalent."}, "weaknesses": {"value": "The paper claims to provide \"mechanistic insights\" (Section 1, contributions) but only performs correlational analysis between surface-level features (romanization ratio, token length) and outcomes. True mechanistic interpretability would require analyzing residual streams, attention patterns, or using causal interventions (e.g., activation patching) to isolate specific circuits or behaviors responsible for jailbreak susceptibility. The correlation analysis in describes associations but provides no causal or *mechanistic understanding* of why romanization affects model behavior.\n\nThe effects of orthographic bias on model safety has been studied before [1] and the paper fails to cite the work. In addition, the central finding about the \"contract gap\" (JSON track showing false safety vs. FREE track revealing vulnerabilities) is primarily a methodological insight about evaluation protocols rather than a finding specific to South Asian languages or multilingual safety. This comparison would likely hold for English or any other language, making it unclear what we learn specifically about Indic language vulnerabilities.\n\nThe paper would be a lot stronger and tighter if it focused on language-specific phenomena such as: comparative vulnerability across the 12 languages, deeper analysis on the cross-lingual transfer patterns unique to Indic language families, or cultural/linguistic factors that affect safety alignment in South Asian contexts. \n\nMore information about the evaluation dataset should be provided. For instance, it is also unclear how many of these 45.2k prompts are culture-specific. Based on the domains chem, bio and synth (line 153), it seems that the topics are not specific to South Asian culture. Furthermore, since the dataset is curated from Wikipedia, can we know if they are translated from their English counterparts? Based on my current understanding, the evaluation benchmark is not really reflective of how South Asians interact with LLMs, and the pipeline in Figure 4 can be simply replaced with applying translation + orthography variant to existing safety eval such as StrongReject or Harmbench.\n\n\n[1] Ghanim, Mansour Al, et al. \"Jailbreaking LLMs with Arabic transliteration and Arabizi.\" EMNLP 2024."}, "questions": {"value": "- For cross-lingual transfer, does the representativeness of the language within pretraining (can be approximated by linguistic resource availability) correlate with the transfer?\n- Can you compare your findings against [1} on how romanization affects jailbreak success?\n- Does translating existing English safety training data into respective langauges and introducing them into safety training reduce jailbreak success rate? \n- Where are these \"adversarial instructions\" (line 140) taken from? Can you provide citations for them?\n\n[1] Ghanim, Mansour Al, et al. \"Jailbreaking LLMs with arabic transliteration and arabizi.\" EMNLP 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fHfDDDtHv3", "forum": "BNrznHRILy", "replyto": "BNrznHRILy", "signatures": ["ICLR.cc/2026/Conference/Submission21528/Reviewer_r72B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21528/Reviewer_r72B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878941400, "cdate": 1761878941400, "tmdate": 1762941821272, "mdate": 1762941821272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces IndicJR, a benchmark intended to evaluate the adversarial safety robustness of LLMs across 12 South Asian languages. The benchmark comprises ~45k prompts , and its core methodology is a judge-free evaluation protocol featuring two tracks: JSON (contract-bound) and FREE (unconstrained natural language responses). The dataset is structured into three subsets: Attacked-Benign (AB), Clean-Benign (CB), and Clean-Harmful (CH). The authors' primary claims are:\n1. Contract Gap: JSON contracts induce excessive conservatism on CB prompts while failing to prevent jailbreaks on AB prompts.\n2. Cross-lingual Transfer: English-to-Indic attack transfer is highly effective.\n3. Orthography Effects: Under the JSON track, using randomized or mixed scripts decreases the JSR, which the authors link to tokenization features."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper attempts to address a well-known gap in LLM safety evaluation: the lack of focus on non-English and particularly low-resource languages. The focus on 12 South Asian languages is a necessary step.\n2. The JSON vs. FREE track design is a potentially useful methodological contribution, as it allows for quantifying the impact of the \"contract\" itself on model behavior. The AB/CB/CH subset split is a systematic approach to differentiating between jailbreaks, over-refusal, and leakage.\n3. The authors evaluated 12 different LLMs, including API, open-weight, and domain-specific models , which lends some empirical weight to their claims."}, "weaknesses": {"value": "1. The CH set is based on only three highly technical domains: chem_synth, biohazard, and illicit_access. This completely ignores the primary safety risks endemic to the South Asian region.\n2. The finding of the FREE track entirely on the efficacy of a multilingual detector. Yet, the paper provides zero details on this detector's design, validation, or per-language performance.\n3. A counter-intuitive finding is that romanization decreases JSR (i.e., improves safety). However, this critical analysis is only performed on the AB set."}, "questions": {"value": "1. The authors should provide a rigorous justification for their comically narrow definition of the CH set. Why were the primary safety risks endemic to the South Asian region context ignored?\n2. The authors should provide the detailed methodology, implementation, and a comprehensive validation for the multilingual detector for each of the 12 languages.\n3. References should be formatted consistently. Are there too many bold texts and blank lines in the main text?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YD12TTVp61", "forum": "BNrznHRILy", "replyto": "BNrznHRILy", "signatures": ["ICLR.cc/2026/Conference/Submission21528/Reviewer_qpzZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21528/Reviewer_qpzZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901993958, "cdate": 1761901993958, "tmdate": 1762941820741, "mdate": 1762941820741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "IndicJR is a benchmark for jailbreaking large language models (LLMs) focused on South Asian languages. It is relatively large and made of hand-annotated data. It comes with an automatic evaluation protocol. Results on this benchmark suggest several takeaways, including the importance of studying orthographic variation in supporting diverse languages in LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. A new hand annotated dataset in languages that have less coverage across typical benchmarks is a valuable resource.\n\n2. The results provide more hard evidence for the existence of a coverage gap between safety training for the most high-resource languages and other languages in LLMs.\n\n3. The focus on orthographic variation is something that has not been significantly studied in the context of jailbreaking attacks to my knowledge."}, "weaknesses": {"value": "Many of the paper's details could be more clearly explained:\n- After reading the paper I do not understand what a \"contract-bound\" setting is. This doesn't seem to be defined. Section 4 only makes reference to a \"refusal schema.\" It seems to have something to do with constraining the output of the LLM but not of these terms are defined or explained.\n- Other terms describing the data are jargony and undefined, like \"cores,\" \"slugs,\" \"intent flavor,\" and \"canaries.\"\n\nOverall, I liked the basic direction of the paper but even as someone fairly knowledgeable about this area, I had trouble understanding exactly what was done. I would prefer to see the paper go through a round of revision before publishing."}, "questions": {"value": "- In Table 1, what does size refer to? Is it number of annotated prompts? I think so, based on the end of section 3. Do all of these datasets contain similar prompts in terms of length and topic?\n- Also in Table 1, what is \"orthographic stress?\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nniiKuwYcQ", "forum": "BNrznHRILy", "replyto": "BNrznHRILy", "signatures": ["ICLR.cc/2026/Conference/Submission21528/Reviewer_SXkk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21528/Reviewer_SXkk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132569796, "cdate": 1762132569796, "tmdate": 1762941820217, "mdate": 1762941820217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}