{"id": "KM7qycd8EB", "number": 23553, "cdate": 1758345330823, "mdate": 1759896808632, "content": {"title": "TOOLWEAVE: FINE-GRAINED AND CONTROLLABLE SYNTHETIC DATA GENERATION FOR MULTI-TURN TOOL CALLING WITH NON-FRONTIER LLMS", "abstract": "Multi-turn tool-calling is a crucial capability for LLM-based agents and is typically improved via supervised fine-tuning on synthetic data. Existing multi-turn tool-calling synthetic data pipelines often rely on proprietary frontier LLMs (e.g., GPT-4) or commercial APIs (e.g., RapidAPI), introducing restrictive licensing. In contrast, data generated directly from open LLMs suffers from low fidelity, poor\ndiversity, and weak adherence to multi-constraint instructions, resulting in producing lower-quality datasets than frontier models. To address these limitations, we propose ToolWeave, a modular and controllable pipeline that synthesises high-quality multi-turn tool-calling datasets using non-frontier, license-friendly LLMs. ToolWeave supports both API and dialogue synthesis. Our framework’s novelty\nis threefold: (1) it is fully synthetic; given only a domain name, it builds a domain context from Wikipedia and Wikidata to synthesize a Tool Graph of APIs. (2) In contrast to other pipelines’ single, failure-prone planning step, ToolWeave’s scaffolding process first generates a high-level goal from the Tool Graph, then decomposes it into a turn-level dialogue plan. This two-stage approach enables non-frontier LLMs to generate high-fidelity, grounded dialogues. (3) A final post-processing stage injects lexical diversity and robustness patterns (e.g., error recovery) to simulate real-world scenarios. To validate our framework, we generated a dataset of ~3.2k dialogues using the open-source gpt-oss-120b. Compared to baselines, ToolFlow and ToolDial, ToolWeave shows clear gains: on the BFCL benchmark, our data improves Llama-3.1-70B to 33.25% (vs. ToolFlow’s 21.00% & ToolDial’s 3.75%) and Phi-4 to 24.50% (vs. ToolFlow’s 8.88% & ToolDial’s\n2.0%). Our data also shows strong generalization, with peak gains of 37.6% on the API Bank benchmark.", "tldr": "ToolWeave is a fully synthetic data generation framework that uses open-source models to create both the APIs and the multi-turn dialogues needed to fine-tune smaller, license-friendly LLMs for complex tool-calling.", "keywords": ["Tool Calling", "Synthetic Data Generation", "Multi-Turn Dialogues", "Large Language Models (LLMs)", "License-Friendly Data"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bfc33e1c78b272e8c9dd8bebbfe027cbf8a22e19.pdf", "supplementary_material": "/attachment/f7cdbb9160a70d7725556c7ccaab48b169d1a9af.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents ToolWeave, a fine-grained and controllable data synthesis pipeline for multi-turn tool calling. Through detailed design and task decomposition, the authors demonstrate that non-frontier LLMs (e.g., gpt-oss-120b) can generate high-quality training data. The API candidates are fully synthesized from a single domain name, thereby alleviating potential licensing issues. Furthermore, different graph traversal methods applied to the generated tool graph, in combination with subgoal decomposition, enable more fine-grained generation.\n\nData analysis indicates that the synthesized data is of high quality, and experimental results show that models trained on this data achieve improved performance across multiple multi-turn benchmarks, validating the effectiveness of the proposed pipeline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Good motivation for proposing pipelines that are suitable for non-frontier LLMs.\n2. Subgraph sampling with different methods ensures the diversity and compatibility for tool candidates.\n3. Detailed analysis on the data statistics validates the data quality."}, "weaknesses": {"value": "1. While I appreciate the efforts made to enable non-frontier LLMs to generate multi-turn tool-calling data samples, I am not fully convinced that the proposed pipeline effectively advances this goal. For example, synthesizing a group of tools capable of supporting complex queries (i.e., multiple combinations of dependent tool usages) based solely on a single domain name would be challenging even for frontier LLMs. Moreover, both subgoal generation and agent role assignment depend heavily on the LLMs’ overall planning ability.\n\n2. The paper lacks an ablation study to reveal the contributions of each proposed component. This issue is closely related to the first weakness. I do not see any analysis or experiment that demonstrates how the proposed components reduce the reliance on LLM capability, thereby enabling non-frontier LLMs to generate high-quality data.\n\n3. Results on more challenging and widely adopted benchmarks, such as $\\tau$-bench and $\\tau^2$-bench, are expected.\n\n4. While the authors claim that being “fully synthesized” is an advantage, I disagree. More recent work (e.g., MCP-related benchmarks) emphasizes the importance of developing LLM capabilities in real environments. Relying on LLMs to simulate tools inevitably introduces errors or inconsistencies in the generated tool outputs, resulting in data that is at high risk of deviating from real-world scenarios."}, "questions": {"value": "1. How do you get the initial domain name for tool synthesis? If merely hand-crafted, how to scale up for generation?\n2. It seems that the quality of the generated data highly depends on the JSON plan generated by the planner. What is your concerns determining that putting so many details (even the role of each step is maintained) in the JSON plan?\n3. Have you tried your pipeline with frontier LLMs like GPT-5 or with more smaller LLMs like Qwen3-32B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wTD4SzIbII", "forum": "KM7qycd8EB", "replyto": "KM7qycd8EB", "signatures": ["ICLR.cc/2026/Conference/Submission23553/Reviewer_JXqN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23553/Reviewer_JXqN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761103706187, "cdate": 1761103706187, "tmdate": 1762942709583, "mdate": 1762942709583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on multi-turn tool-calling synthetic data generation pipelines, specifically targeting non-frontier LLMs. To this end, the authors propose ToolWeave, a controllable and modular framework that can synthesize high-quality multi-turn tool-calling datasets using license-friendly, non-proprietary models. ToolWeave consists of four key components: 1. synthesizing a  library of tools 2. sampling subgraphs from structural motifs 3. generating dialogue plans that decompose complex reasoning into small, executable steps, and 4. executing these plans, followed by post-processing to enhance diversity and robustness.   \n\nExperimental results show that LLMs fine-tuned on ToolWeave-generated data (using gpt-oss-120b) significantly outperform SOTA baselines (ToolFlow and ToolDial) on multi-turn tool-calling benchmarks such as BFCL-V3."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces the an open and modular framework for generating multi-turn, multi-tool dialogues \n- The use of Wikipedia and Wikidata for domain grounding is both practical and elegant\n- The system is methodically designed and evaluated across different tool-calling benchmarks, showing consistent improvements over competitive baselines."}, "weaknesses": {"value": "- Terminology clarity and exposition  \nWhile most parts of the paper are well-written, some key terminologies lack concise explanations when first introduced.\nFor example, in the abstract (line 023), the authors mention “synthesize a Tool Graph of APIs,” but it is unclear what constitutes the nodes and edges of this graph. A brief definition before each new term would improve readability and accessibility.\n\n- Motivation justification   \nThe paper’s core motivation—to focus on non-frontier LLMs—needs stronger justification.\nThe mainstream approach for improving function-calling capabilities is to fine-tune on high-quality SFT data, and commercial models (e.g., GPT-5, BFCL multi-turn achieves 57.63) can already perform this task well.\nThus, if frontier models can readily produce such data, why is it necessary to design a specialized workflow for weaker models?\nThe motivation behind this work may be a false need.\n\n- Evaluation ceiling and benchmark interpretation   \nThe reported results on BFCL-V3 may not fully substantiate the claimed significance.\nFor instance, the best ToolWeave-tuned model (Llama-3.1-70B) achieves ~33% accuracy, while commercial systems like GLM-4.5 and xLAM-2-70B-fc-r can reach 65.62 and 75.38, respectively on the same benchmark.\nThis suggests that the absolute performance ceiling of ToolWeave-generated data remains relatively low, which raises concerns about its practical impact despite the relative gains over baselines."}, "questions": {"value": "- Have the authors attempted using a frontier or commercial LLM (e.g., GPT-5, Claude 4) within the ToolWeave pipeline to generate data, and then fine-tuned a smaller model on it for comparison?\nThis would clarify whether ToolWeave’s design is essential.\n\n- Since ToolWeave explicitly relies on non-frontier LLMs, data quality is a critical issue.\nAlthough Section 4.1 presents an API quality and coverage analysis, there seems to be no explicit quality validation of the generated dialogues themselves.    \nFor example, did the authors conduct: Cross-validation by regenerating the same dialogue multiple times and keeping only consistent outputs?\nThese steps would substantially increase confidence in the dataset’s reliability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D2cY7xXokK", "forum": "KM7qycd8EB", "replyto": "KM7qycd8EB", "signatures": ["ICLR.cc/2026/Conference/Submission23553/Reviewer_Kjdb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23553/Reviewer_Kjdb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728825937, "cdate": 1761728825937, "tmdate": 1762942709289, "mdate": 1762942709289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ToolWeave, a fully open-source and fine-grained controllable framework for synthesizing multi-turn tool-calling datasets using non-frontier large language models. Unlike prior pipelines that rely on proprietary models such as GPT-4, ToolWeave builds everything from scratch with open knowledge sources (Wikipedia, Wikidata) and a modular five-stage process: tool graph synthesis, structured sampling, fine-grained plan generation, multi-agent dialogue execution, and post-processing. This design enables weaker open models to produce coherent, realistic, and robust tool-use dialogues."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Structured and controllable generation pipeline:\nThe framework employs a five-stage modular design—from tool graph synthesis to dialogue post-processing—allowing precise control over each step, making the generation process transparent, reproducible, and easy to debug.\n\nReliance on open-source models:\nToolWeave operates entirely with non-frontier, open-source LLMs and open knowledge bases, ensuring license-friendly data generation without dependence on proprietary systems.\n\nSignificant performance improvement and strong generalization:\nModels fine-tuned on ToolWeave data achieve substantial gains and robust generalization across multiple benchmarks, outperforming prior pipelines such as ToolFlow and ToolDial."}, "weaknesses": {"value": "1. The paper appears to lack a clear description of any human verification process. While the goal of developing a fully automated pipeline is understandable, incorporating at least light sampling or manual validation would strengthen reliability. For instance, in the initial stage where the model generates APIs, it is unclear how the authors ensure that these APIs are logically consistent and error-free, as issues such as inconsistent parameter types or missing logical dependencies may still occur.\n\n2. The synthesized APIs may not fully reflect the complexity of real-world APIs. Since they lack real execution, latency, and side effects, there could be a gap between the synthetic and practical tool-calling scenarios.\n\n3. In several benchmarks, ToolDial and ToolFlow appear to negatively affect model performance after SFT. It would be helpful if the authors could provide a finer-grained analysis to understand why this happens—perhaps through ablations or error case studies."}, "questions": {"value": "1. It remains somewhat unclear whether being “fully open-source and license-friendly” alone constitutes a substantial contribution. Since many existing tool-calling datasets can already be generated locally using open models within sandboxed environments, the paper should clarify more explicitly what distinguishes ToolWeave in this aspect."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kJxtgfD8p6", "forum": "KM7qycd8EB", "replyto": "KM7qycd8EB", "signatures": ["ICLR.cc/2026/Conference/Submission23553/Reviewer_KXxZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23553/Reviewer_KXxZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890346689, "cdate": 1761890346689, "tmdate": 1762942708937, "mdate": 1762942708937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ToolWeave, a modular and controllable synthetic data generation pipeline for multi-turn tool calling using non-frontier LLMs. The method constructs a domain-specific synthetic Tool Graph from open sources (Wikipedia and Wikidata), samples structured tool workflows based on common motifs (linear, fan-in/out, conditional), and generates fine-grained dialogue plans that are then instantiated through a multi-agent dialogue synthesizer. The final dialogues are post-processed to enhance robustness and diversity. Experiments on BFCL-V3, API Bank, CONFETTI, and ToolHop benchmarks show that data generated by ToolWeave can significantly improve tool-calling performance of open LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets an important and timely problem: generating license-friendly, high-quality multi-turn tool-calling data without relying on frontier models.\n2. The paper evaluates across multiple benchmarks and base models, demonstrating consistent improvements. Results are strong and well-analyzed.\n3. The authors promise to release code, tools, and data, which will be valuable to the community."}, "weaknesses": {"value": "1. While the system is well-engineered, many ideas (e.g., tool graph construction, dialogue synthesis via plans) have been explored in ToolFlow, Magnet, and APIgen-MT. The novelty lies primarily in the granularity and control, which should be emphasized more clearly in the introduction and discussion.\n\n2. Since the APIs are synthesized by LLMs, it is unclear how faithfully they represent real-world API semantics. And the bias in LLMs will be amplified in the continual training with the synthesized dataset.\n\n3. Since the tool responses are role-played by LLMs rather than generated by real API executions, the responses in multi-turn or multi-step dialogues are always idealized and deterministic. In realistic scenarios, however, API responses are often unexpected, containing errors, missing fields, or inconsistent outputs that require the model to perform error correction or recovery. This mismatch introduces a systematic bias in the synthesized dataset, making the resulting fine-tuned models less robust to noisy or imperfect tool outputs.\n\n4. The ablation study could better justify the necessity of each stage (graph sampler, fine-grained planner, post-processor).\n\n5. Although the text describes tool motifs in detail, the paper lacks a clear visualization of the generated Tool Graph."}, "questions": {"value": "Please refer to the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YPBY30GRL8", "forum": "KM7qycd8EB", "replyto": "KM7qycd8EB", "signatures": ["ICLR.cc/2026/Conference/Submission23553/Reviewer_WxXD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23553/Reviewer_WxXD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995650017, "cdate": 1761995650017, "tmdate": 1762942708746, "mdate": 1762942708746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}