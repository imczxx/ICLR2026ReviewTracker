{"id": "jxyEci13Dd", "number": 11, "cdate": 1756728079090, "mdate": 1763761044098, "content": {"title": "Long-Text-to-Image Generation via Compositional Prompt Decomposition", "abstract": "While modern text-to-image models excel at generating images from intricate prompts, they struggle to capture the key details when the prompts are expanded into descriptive paragraphs. This limitation stems from the prevalence of short captions in their training data. Existing methods attempt to address this by either fine-tuning the pre-trained models, which generalizes poorly to even longer inputs; or by projecting the oversize inputs into short-prompt domain and compromising fidelity. We propose a compositional approach that enables pre-trained models to handle long-prompt by breaking it down into manageable components. Specifically, we introduce a trainable PromptDecomposer module to decompose the long-prompt into a set of distinct sub-prompts. The pre-trained T2I model processes these sub-prompts in parallel, and their corresponding outputs are merged together using concept conjunction. Our compositional long-text-to-image model achieves performance comparable to those with specialized tuning. Meanwhile, our approach demonstrates superior generalization, outperforming other models by 7.4\\% on prompts over 500 tokens in the challenging DetailMaster benchmark.", "tldr": "We decompose long-prompts to allow pre-trained Text-to-Image models to handle long-prompts input, demonstrating superior generalization as prompt length increases.", "keywords": ["Compositionality; Text-to-Image Generation; Generative Model Generalization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e04937ccf22dd3554d17a7f397999125a113d4b.pdf", "supplementary_material": "/attachment/8d0b75d6bfa9ccefd81852db1fc8ec579a826281.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the limitation of modern text-to-image models in capturing fine-grained details from long text inputs. The authors propose a trainable PromptDecomposer module that decomposes lengthy text prompts into multiple semantically coherent sub-prompts. Experimental results demonstrate that the proposed method achieves strong performance on the challenging DetailMaster benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A key strength of this work lies in the latent-space prompt decomposition strategy. By decomposing prompts in the latent space, the method avoids the semantic fragmentation issues often observed when splitting raw text. This leads to more coherent sub-prompt representations and enables the model to balance local detail preservation with global semantic consistency, contributing to improved text-image alignment."}, "weaknesses": {"value": "1. Limited Adaptability to Different Backbones:\nAs acknowledged by the authors, the proposed method exhibits scalability issues when applied to larger diffusion backbones. Specifically, both the training and inference costs grow significantly with model size, which raises concerns about its practicality on modern large-scale models such as SD3, FLUX, SD3.5, or Qwen-Image. This limitation restricts the method’s usability in real-world or production-level scenarios.\n\n2. Insufficient Experimental Coverage:\nTo ensure fair and comprehensive evaluation, the method should also be tested on the Evaluation Dataset proposed in LongAlign [1]. Without such comparison, it is difficult to judge whether the performance gains are consistent across different long-text benchmarks.\n\n3. Lack of Verification on Short Prompts:\nThe paper focuses primarily on long-text generation, but does not evaluate whether the proposed approach compromises the model’s ability to handle short prompts. Experiments on standard short-prompt benchmarks such as GenEval [2] and T2I-CompBench++ [3] are necessary to demonstrate the generalization and robustness of the proposed method.\n\n4. Missing Inference Efficiency Comparison:\nA comparison of inference memory consumption and latency between this method and LongAlign [1] would help clarify the trade-offs between alignment improvement and computational overhead, providing a more complete understanding of its practical value.\n\n[1] Improving Long-Text Alignment for Text-to-Image Diffusion Models.\n\n[2] GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment.\n\n[3] T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-image Generation."}, "questions": {"value": "1. The training data used in this paper differs from that used in LongAlign [1], which may lead to unfair comparison and make it difficult to attribute performance improvements solely to the proposed method.\n\n2. The PromptDecomposer module appears to have limited contribution when considering the scaling behavior across different model backbones. How does the proposed framework adapt or generalize to models with varying capacities?\n\n3. In [2], a training-free method for processing long texts at the sentence-level can be added to the baseline for reference, which can make the experiment more comprehensive.\n\n[1] Improving Long-Text Alignment for Text-to-Image Diffusion Models.\n\n[2] Hybrid Layout Control for Diffusion Transformer: Fewer Annotations, Superior Aesthetics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KCKcOtU0aM", "forum": "jxyEci13Dd", "replyto": "jxyEci13Dd", "signatures": ["ICLR.cc/2026/Conference/Submission11/Reviewer_rbyh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11/Reviewer_rbyh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485142749, "cdate": 1761485142749, "tmdate": 1762915436192, "mdate": 1762915436192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Compositional approach for long text-to-image generation using learnable queries to decompose long-prompt representations into sub-prompts in representation space, processed in parallel through frozen pre-trained T2I models and merged via concept conjunction. \nAchieves 7.4% better generalization on 500+ token prompts on SD 1.5-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Interesting compositional framework grounded in energy-based models.\n\n2) Efficient for legacy models: trains only PromptDecomposer (~20hrs on 4 A100s), avoiding large fine-tuning costs.\n\n3) Strong generalization results. 7.4% improvement on 500+ tokens is substantial.\n\n4) Comprehensive evaluation on DetailMaster. With thorough ablation studies and multiple metrics.\n\n4) Clear presentation with good visualizations, honest limitation discussion."}, "weaknesses": {"value": "1. **Questionable practical motivation with outdated baselines**: Focuses on SD 1.5 (2022) while modern models (SD 3.5, Flux) with T5-XXL encoders likely handle long prompts well. \n> Authors must demonstrate that these SOTA models actually fail on long prompts to justify the compositional approach, and provide comprehensive DetailMaster benchmark comparisons showing where SD 1.5-based methods fall short relative to modern baselines. At least when I see Table 4 result, Vanilla SD 3.5 already shows pretty good result in numbers compared to PromptDecomposer+SD1.5+Tuned or all the other baselines. I cannot understand why we should use PromptDecomposer, if the use of SD 3.5 (or Flux -- I want this added as baseline comparsion too.) is already better in numbers.\n\n\n2. **Poor scalability to modern architectures**: SD 3.5 experiments (Table 4) show minimal improvements despite requiring 1.2B parameters for PromptDecomposer-SD3.5; moreover, Table 4 reveals SD 3.5 already achieves strong scores (CLIPScore 34.97, DenScore 22.37) suggesting the problem may not exist for modern models. No experiments with Flux/SDXL provided. The approach appears limited to weak text encoders (CLIP), and generalization across different text-encoder architectures (CLIP vs T5 vs T5-XXL) needs systematic demonstration.\n> Please add text-encoder comparison experiments for clear demonstration of your contribution.\n\n3. **Evaluation Metrics** : In Tab. 2, you reported HPSv3, but in Tab. 4, you reported HPSv2. Is it Typo? If it is typo, I see that PromptDecomposer or other baselines in Tab. 2 stays at around 6.7 (ELLA) to 13.26 (LongAlign), but using vanilla SD 3.5 shows 28.86. Pickscore and Denscore shows better numbers in PromptDecomposer or on some baselines, but difference is not as big compared to HPS difference.\n> I want more explanations on this typo and number differences. Also, please add SD 3.5 / Flux vanilla (w/o tuning or adding PromptDecomposer component) to the main table for clear comparison. Also for these SOTA models I think adding qualitative comparison might be helpful if the actual result of PromptDecomposer demonstrating the long-prompt is better than those SOTA models."}, "questions": {"value": "**Check weaknesses section above for details.**\n\nThe major weakness of this work is the **questionable motivation** and **lack of evaluation/demonstration on state-of-the-art models**. The paper focuses on SD 1.5 (2022) while modern models with stronger text encoders (SD 3.5, Flux with T5-XXL) likely already handle long prompts effectively, yet no comprehensive comparison is provided. Table 4 shows SD 3.5 already achieves strong performance, and the minimal improvement from PromptDecomposer-SD3.5 raises fundamental questions about whether this problem still exists for current models.\n\n**To increase my score, the authors should address:**\n1. Demonstrate that SOTA models (SD 3.5, Flux) actually fail on long prompts with DetailMaster benchmark results (or at least on qualitative results.)\n2. Include these models as baselines in main comparison tables\n3. Provide ablations showing improvements come from decomposition rather than just better text encoding (e.g., T5-XXL with SD 1.5 without decomposition)\n4. Add user studies comparing generation quality against modern models\n\nIf these additional experiments convincingly show the compositional approach provides value beyond simply using better text encoders, I will reconsider my evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RD4cu8Y2Mr", "forum": "jxyEci13Dd", "replyto": "jxyEci13Dd", "signatures": ["ICLR.cc/2026/Conference/Submission11/Reviewer_2frZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11/Reviewer_2frZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914960805, "cdate": 1761914960805, "tmdate": 1762915436045, "mdate": 1762915436045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of generating high-quality images from lengthy text prompts by proposing PromptDecomposer, a trainable module that decomposes long prompts into manageable sub-prompts processed in parallel by pre-trained T2I models, with outputs fused via concept conjunction. The method achieves competitive performance on DetailMaster benchmark and demonstrates superior generalization on prompts exceeding 500 tokens, improving performance by 7.4% over existing approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes compositional long-text-to-image generation and unsupervised long-prompt decomposition methods to enable models to better perceive lengthy text inputs.\n\n2. Experimental validation effectively demonstrates the effectiveness of the proposed approach."}, "weaknesses": {"value": "1. While decoupling long text representations is a reasonable idea, the proposed modules bear strong resemblance to existing architectures (e.g., Q-Former), lacking sufficient novelty. The approach is heavily data-driven without explicit representation loss guidance, which raises concerns about the generalization capability of learned parameters. Additionally, the acquisition of image captions is critical for training but not thoroughly discussed.\n\n2. In Table 1, SDXL-based models outperform the proposed method on Character Presence and Object metrics. Why were experiments not conducted on SDXL? SD-1.5 is outdated. As acknowledged in the limitations, transferring to SD3.5 does not yield significant improvements. Given the emergence of Flux, Qwen-Image, and similar models, exploring complex prompt generation on these newer architectures would be more valuable."}, "questions": {"value": "How should this method be adapted to state-of-the-art models like Qwen-Image (with Qwen2.5-VL as encoder) or MetaQuery-type architectures? What modifications are necessary for effective transfer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ma4KNaycKQ", "forum": "jxyEci13Dd", "replyto": "jxyEci13Dd", "signatures": ["ICLR.cc/2026/Conference/Submission11/Reviewer_ZpWd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11/Reviewer_ZpWd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958831081, "cdate": 1761958831081, "tmdate": 1762915435923, "mdate": 1762915435923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to enhance image generation on long, paragraph-length prompts. It proposes a compositional pipeline with a trainable PromptDecomposer that splits a long prompt into distinct sub-prompts. A pre-trained T2I model processes these sub-prompts in parallel, and outputs are merged via concept conjunction. The method shows good gain on >500-token prompts in the DetailMaster benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Good motivation and problem formulation: it’s a well-established research question that the state-of-the-art image generation models are usually trained on limited-length captions, which generates lower-quality images on long prompts. \n - Intuitive modules and good results: the proposed modules in PromptDecomposer are well motivated. The experiments show PromptDecomposer clearly outperformed baselines on long prompts."}, "weaknesses": {"value": "- Limited novelty: many modules in this paper can be found in references. For example, cross-attention, T5, CLIP are off-the-shelf modules. It’ll be great if the authors could explain more about what’s the unique contribution and novelty in this paper. \n - Risk of losing global coherence: when merging independently generated components,  global coherence (such as lighting, perspective, style) might be lost, or might be conflicting with other components (e.g. day vs night, mountain vs sea)."}, "questions": {"value": "- The authors ablated the number of learnable queries in the paper. But they are still fixed. I wonder if we should make the number of learnable queries adaptive or dependent on the prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VH5wcCpbbo", "forum": "jxyEci13Dd", "replyto": "jxyEci13Dd", "signatures": ["ICLR.cc/2026/Conference/Submission11/Reviewer_TWGK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11/Reviewer_TWGK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970330397, "cdate": 1761970330397, "tmdate": 1762915435700, "mdate": 1762915435700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We thank the reviewers for their constructive feedback and positive outlook. We are glad that the reviewers recognized the significance of the **problem formulation** (TWGK) and found our **energy-based compositional framework** to be interesting and effective (TWGK, ZpWd, 2frZ). In particular, we appreciate the acknowledgment of our **latent-space decomposition strategy** in avoiding semantic fragmentation (rbyh), as well as the recognition of our model's **training efficiency and comprehensive experimental validation** (ZpWd, 2frZ, rbyh).\n\nWe truly appreciate all reviewers and meta reviewer’s time and effort. **We have carefully read and addressed all your concerns, including:**\n1. Transferability to modern architectures (2frZ, ZpWd, rbyh);\n2. Insufficient evaluations (2frZ, ZpWd, rbyh);\n3. Practical value on modern T2I architectures (2frZ, rbyh); \n3. Limited novelty in module designs (TWGK, ZpWd);\n4. Risk of losing global coherence (TWGK);\n5. Possibility and necessity of an adaptive decomposition (TWGK).\n\n**All major modifications are highlighted in red in the paper.** We thank you again for your time and constructive insights."}}, "id": "S78Cg1YAGX", "forum": "jxyEci13Dd", "replyto": "jxyEci13Dd", "signatures": ["ICLR.cc/2026/Conference/Submission11/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission11/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763760538137, "cdate": 1763760538137, "tmdate": 1763760538137, "mdate": 1763760538137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}