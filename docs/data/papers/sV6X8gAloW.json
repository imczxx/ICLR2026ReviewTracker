{"id": "sV6X8gAloW", "number": 2560, "cdate": 1757145774069, "mdate": 1759898140812, "content": {"title": "SVRG and Beyond via Posterior Correction", "abstract": "Stochastic Variance Reduced Gradient (SVRG) and its variants aim to speed-up training by using gradient corrections, but have seen limited success in deep learning.\n   Here, we show surprising new foundational connections of SVRG to a recently proposed Bayesian method called posterior correction. Specifically, we show that SVRG is recovered as a special case of posterior correction over the isotropic-Gaussian family, while novel extensions are automatically obtained by using more flexible exponential families.\n   We derive two new SVRG variants by using Gaussian families: First, a Newton-like variant that employs novel Hessian corrections, and second, an Adam-like extension that improves (continual) pretraining and finetuning of Transformer language models. This is the first work to connect SVRG to Bayes and use it to boost variational training for deep networks.", "tldr": "Posterior correction generalizes SVRG and yields faster variational training for deep learning", "keywords": ["Variance Reduction", "Deep Learning", "Bayesian Methods", "Optimization"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/53024e410e77a26e7d56b9b40552ae558c18687d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper establishes a connection between SVRG (Johnson & Zhang, 2013) and posterior correction (PC)\n(Khan, 2025), showing that SVRG can be understood as a special case of PC assuming isotropic Gaussian distributions.\nPC is subsequently used to improve the IVON (Shen et al., 2024) optimizer across a wide range of experiments."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The mathematical connection between SVRG and PC is novel.\n- A strong and wide-ranging set of experiments together with a comprehensive set of ablations demonstrating the power of combining IVON with PC."}, "weaknesses": {"value": "- While the equivalence of SVRG as a special case of PC is interesting, the paper reads like a\nfollow-up to Khan (2025) that demonstrates how PC and IVON can be combined to markedly improve\nthe performance of the latter, a comparison that was missing in Khan (2025).\nThe SVRG connection feels like an afterthought that does not provide deeper insights,\ne.g., the extent to which convergence properties from SVRG still hold or can be extended to non-isotropic Gaussians, or the extent to which smoothness assumptions and variance reduction empirically hold.\n- The paper lacks an analysis of the added computational cost, both from SVRG and from the Hessian approximations.\n- All runs appear to be single runs, without any results on variance across training runs.\n- No code is provided."}, "questions": {"value": "- Q1: Does PC gain any theoretical properties from the derived SVRG connection?\n\n\n\n\n_____\n*Note: The low score is primarily due to the strong focus on SVRG without the paper gaining anything from this focus. If one were to ignore that and simply read it as an application paper combining IVON with PC and evaluating it extensively, the score would be higher.*"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Byoq7NDnXv", "forum": "sV6X8gAloW", "replyto": "sV6X8gAloW", "signatures": ["ICLR.cc/2026/Conference/Submission2560/Reviewer_CxSC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2560/Reviewer_CxSC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761039479440, "cdate": 1761039479440, "tmdate": 1762916282389, "mdate": 1762916282389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a connection between Stochastic Variance Reduced Gradient (SVRG) with Posterior Correction (PC). The authors show that SVRG emerges as a special case of posterior correction when using isotropic Gaussian distributions. This insight allows them to derive new SVRG variants by applying posterior correction to broader exponential-family distributions. Two extensions are proposed: 1. Newton-like variant (VON-PC) — incorporates stochastic variance reduction for both gradients and Hessians, improving stability and enabling second-order corrections. 2. Adam-like variant (IVON-PC/IVON-PCM) — adapts posterior correction over the IVON optimizer, showing strong performance in continual pre-training and fine-tuning of large models like GPT-2 and ViT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The original contribution by establishing a connection between SVRG and posterior correction. The interpretation reframes variance reduction as a form of knowledge transfer, a new perspective that unifies two previously separate research threads.\n2. The authors successfully extend this connection to derive two new SVRG variants: One is a Newton-like method incorporating stochastic variance reduction for both gradients and Hessians, introducing Hessian corrections rarely explored in prior work; another is an Adam-like method (IVON-PC/IVON-PCM) with diagonal covariance approximations."}, "weaknesses": {"value": "1. Compared with the well established optimization method e.g., AdamW, posterior correction does not yield clear improvements in deep learning tasks. \n2. Can the variance reduction method be applied to reinforcement learning? The authors can explore the experiments on either RLVR of LLMs post-training or some other traditional tasks in RL area. It can also compared with TRPO (Trust Region Policy Optimization)."}, "questions": {"value": "How about its computational overhead (e.g., Hessian estimation, mega-batch refresh) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HumEgmUkgT", "forum": "sV6X8gAloW", "replyto": "sV6X8gAloW", "signatures": ["ICLR.cc/2026/Conference/Submission2560/Reviewer_cZ3B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2560/Reviewer_cZ3B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888536368, "cdate": 1761888536368, "tmdate": 1762916282038, "mdate": 1762916282038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper establishes a connection between SVRG and posterior correction, enabling the generalization of variance reduction method to the IVON optimizer. At a high level, it aims to establish a relationship between non-Bayesian optimization methods and variational inference within a common theoretical framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The connection between SVRG and posterior correction is sound.\n- An advantage of the connection is that it generalizes the variance reduction method to higher-order optimizers, leading to a novel IVON-PC."}, "weaknesses": {"value": "- While the theoretical analysis provides a fresh perspective on SVRG, the resulting IVON-PC method does not appear to offer practical benefits.\n\n  - Figure 3 shows an initial improvement, but IVON-PC ultimately requires a similar number of gradient computations as SVRG to reach comparable final performance. Likewise, Figure 5 demonstrates that their final performance remains nearly identical.\n\n  - The gains reported in Table 5 are also insignificant, especially when considering the error bars.\n\n- Another limitation is that the established connection does not clarify why SVRG or the proposed IVON-PC fail to deliver stronger empirical results.\n\nThe above two points raise questions about whether we obtained any value through the established connection. Nonetheless, I lean toward acceptance, as the work introduces a novel viewpoint and may inspire deeper future investigations on SVRG."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bcZOWpYuEK", "forum": "sV6X8gAloW", "replyto": "sV6X8gAloW", "signatures": ["ICLR.cc/2026/Conference/Submission2560/Reviewer_XyZw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2560/Reviewer_XyZw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953700488, "cdate": 1761953700488, "tmdate": 1762916281459, "mdate": 1762916281459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}