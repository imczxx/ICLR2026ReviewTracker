{"id": "h21iXALHk6", "number": 6609, "cdate": 1757990496886, "mdate": 1762923747304, "content": {"title": "RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding", "abstract": "Medical image understanding plays a crucial role in enabling automated diagnosis and data-driven clinical decision support. However, its progress is impeded by two primary challenges: the limited availability of high-quality annotated medical data and an overreliance on global image features, which often miss subtle but clinically significant pathological regions. To address these issues, we introduce RegionMed-CLIP, a region-aware multimodal contrastive learning framework that explicitly incorporates localized pathological signals along with holistic semantic representations. The core of our method is an innovative region-of-interest (ROI) processor that adaptively integrates fine-grained regional features with the global context, supported by a progressive training strategy that enhances hierarchical multimodal alignment. To enable large-scale region-level representation learning, we construct MedRegion-500k, a comprehensive medical image-text corpus that features extensive regional annotations and multilevel clinical descriptions. Extensive experiments on image–text retrieval, zero-shot classification, and visual question answering tasks demonstrate that RegionMed-CLIP consistently exceeds state-of-the-art vision language models by a wide margin. Our results highlight the critical importance of region-aware contrastive pre-training and position RegionMed-CLIP as a robust foundation for advancing multimodal medical image understanding.", "tldr": "", "keywords": ["Region-Aware", "Multimodal", "Medical Image", "RegionMed-CLIP"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d776a9de43c4bfc0c0b9c693f13309dfd7a59b3b.pdf", "supplementary_material": "/attachment/848714c40018df42731a1507328860adde70257c.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a region-aware multimodal contrastive learning framework that integrates the learning of both global features of the image and local features of ROI in images. The authors construct a large-scale medical image-text dataset with extensive region-level annotations and multi-level clinical descriptions. Extensive experiments demonstrate that the proposed method significantly outperforms state-of-the-art vision-language models on image-text retrieval, zero-shot classification, and visual question answering tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1) The idea of constructing a large scale medical image pre-training dataset with fine-grained annotation on ROI is reasonable. \n2) The design of the proposed method that fuses local ROI features into global image feature in the pre-training is reasonable, because it provide some global context for understanding a local region."}, "weaknesses": {"value": "1. It seems to me that a subset of references in the paper appear fabricated, misattributed, or improperly formatted. Specifically, I can not trace some of these references back to their origin or they seem to not exist in the stated vanue. Some of the arXiv IDs in the citation link to unrelated papers. \n\n2. Important technical details are missing on the pre-trained framework of this paper. For example, this paper does not clearly explain how is the cross-attention in ROI processer is designed. What are the corresponding key, value and query of this cross-attention module? The multi-stage progressive training is also poorly explained, making it hard for others to understand exactly how is the proposed model trained. \n\n3. Important details are missing on how is the proposed dataset curated. There should be a step-by-step explaination on how to use external models to extract ROI from medical image and how the caption of each ROI is generated. \n\n4. There are inconsistencies in the sample numbers reported in Table 1. Specifically, the sample counts for several included datasets do not match the original dataset statistics, and there is no explanation provided for these discrepancies. It is important for the authors to clarify why certain samples from the original datasets were excluded, or why Table 1 lists more samples than the original sources. Additionally, the total sample number for the included datasets in Table 1 is approximately 100,000, which does not align with the claimed overall total of 500,000. I recommend the authors provide a detailed explanation to ensure transparency and reproducibility."}, "questions": {"value": "While the paper presents an interesting topic, the manuscript currently lacks clarity and omits several essential details necessary for full understanding. I strongly encourage the authors to carefully address the concerns regarding the references, as accurate and traceable citations are fundamental to scholarly work. Such oversights should be avoided in any peer-reviewed publication, especially in a top-tier venue like ICLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9WCh1rbgnp", "forum": "h21iXALHk6", "replyto": "h21iXALHk6", "signatures": ["ICLR.cc/2026/Conference/Submission6609/Reviewer_qkDq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6609/Reviewer_qkDq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810826125, "cdate": 1761810826125, "tmdate": 1762918932284, "mdate": 1762918932284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "YQwKpr7Vuf", "forum": "h21iXALHk6", "replyto": "h21iXALHk6", "signatures": ["ICLR.cc/2026/Conference/Submission6609/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6609/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762923746081, "cdate": 1762923746081, "tmdate": 1762923746081, "mdate": 1762923746081, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents RegionMed-CLIP, a model for understanding medical images by linking both whole images and important local regions with their text descriptions. The authors built a large dataset called MedRegion-500k, which contains around 500,000 medical image–text pairs from different imaging types. Each image includes both a full view and smaller region crops found automatically using Med-SAM and Grounding DINO. Text descriptions for these regions were written by a fine-tuned Qwen-2.5VL-72B model, with some human experts checking a small portion for accuracy. Each region also has a few “negative” captions, where words are slightly changed to make the description incorrect, helping the model learn to tell apart subtle differences.\n\nRegionMed-CLIP uses one branch to learn from full images and another for cropped regions. It combines the two with an attention module and trains in several steps, starting from global alignment and then adding region-level supervision. When tested on medical image tasks such as classification, question answering, and image–text matching, the model performs better than previous systems like BiomedCLIP and SigLIP.\n\nThe contributions are as follows:\n- The paper puts major emphasis on creating a region-aware dataset (image-text pairs) with automatically extracted ROIs and multi-level captions (summary, detailed report, region caption, and negatives). This dataset is what enables fine-grained learning and it’s positioned as the key reason RegionMed-CLIP outperforms other models, even though it’s smaller in scale than datasets like PMC-15M or BiomedCLIP.\n- The encoder design builds on standard CLIP ideas, ViT-B/16 for images and PubMedBERT for text, but adds an ROI processor that fuses global and local features using cross-attention."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces MedRegion-500k, a region-aware dataset (image-text pairs) with automatically extracted ROIs and multi-level captions (summary, detailed report, region caption, and negatives). This dataset is what enables fine-grained learning and it’s positioned as the key reason RegionMed-CLIP outperforms other models, even though it’s smaller in scale than datasets like PMC-15M or BiomedCLIP.\n\n- The paper extends the CLIP framework in a clear and logical way by combining whole-image and region features through an ROI processor. This design helps it detect small but clinically important details without overcomplicating the architecture.\n\n- Comprehensive evaluation across multiple tasks (image–text retrieval, classification, and VQA) demonstrates consistent gains over strong baselines like BiomedCLIP and SigLIP, suggesting robust generalization."}, "weaknesses": {"value": "- One noticeable weakness is that the paper doesn’t clearly explain how much human expertise actually went into validating the data. They mention that a “small subset” of the ROI crops and Qwen-generated captions were reviewed by medical experts, but they never say how many experts were involved, how many samples they checked, or what proportion of the 500k dataset was manually verified. As a result, it’s hard to judge the true reliability of the region annotations or captions; most of the dataset seems to rely on automated tools (Med-SAM, Grounding DINO, Qwen) with limited human quality control.\n\n- Comparisons with larger baselines (BiomedCLIP, PMC-CLIP, SigLIP-400M) are not entirely controlled. RegionMed-CLIP is trained on a smaller but much cleaner, domain-specific dataset, so improvements may largely reflect dataset curation and domain matching rather than architectural superiority. \n\n- The framework design is incremental rather than conceptually new; it reuses known components with modest adjustments."}, "questions": {"value": "Your reported gains over BiomedCLIP and PMC-CLIP are impressive, especially given that RegionMed-CLIP is trained on a much smaller dataset. However, since MedRegion-500k is highly curated and domain-matched to the evaluation benchmarks, how can we be confident that the performance improvement comes from the proposed ROI processor and region-aware architecture rather than from differences in data quality or domain alignment? Have you considered retraining a baseline (e.g., CLIP or BiomedCLIP) on the same MedRegion-500k dataset to isolate the architectural contribution?\n\nThe paper states that MedRegion-500k is assembled from multiple publicly available and ethically approved datasets such as Kaggle collections and MIMIC-CXR. Could you clarify what steps were taken to avoid duplication or overlap between pre-training and evaluation sets .  A detailed breakdown of data origins and quality-control procedures would help assess the reliability and fairness of your comparisons."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GPetCWegMB", "forum": "h21iXALHk6", "replyto": "h21iXALHk6", "signatures": ["ICLR.cc/2026/Conference/Submission6609/Reviewer_5KdU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6609/Reviewer_5KdU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854518589, "cdate": 1761854518589, "tmdate": 1762918931827, "mdate": 1762918931827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an architecture capable of fusing both global and regional information from images and text during training, and employs negative captioning to enhance model performance when trained on limited datasets. The authors attempt to validate the effectiveness of their architecture through a series of experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The architectural design builds upon ideas from Alpha-CLIP and UMG-CLIP in natural image domains—specifically, augmenting the standard CLIP framework with the ability to attend to local (region-level) information. To the best of my knowledge, this is the first such attempt in the medical imaging context.\n2. The authors curate a multimodal dataset encompassing various levels of textual annotations and lesion mask annotations. They also commit to open-sourcing a subset of this dataset in the future."}, "weaknesses": {"value": "### **Major Weaknesses**\n\n1. While the introduction briefly mentions how the dataset was annotated, neither the main experiments nor the appendix fully disclose which public datasets were integrated to construct *MedRegion-500k*. This is a serious oversight. For a dataset assembled from multiple existing sources, it is essential to list all constituent datasets. If the list is lengthy, it should at least appear in the appendix; otherwise, readers cannot properly assess the validity or reproducibility of the work.\n2. The method section raises numerous unresolved questions:\n    1. It appears that both the global image and ROI images are encoded using the same ViT. However, ROI images likely have variable resolutions. How does the model handle this? Is RoPE or another mechanism employed?\n    2. When a single image contains multiple ROIs, are these ROIs fed into the ViT separately or concatenated and processed together?\n    3. The role of the “Global processor” is never explained. What function does it serve?\n    4. Do lines 242–245 provide an explanation of the cross-attention mechanism introduced in lines 214–215?\n    5. In lines 244–245, does “global context” refer to $\\tilde{z}_{\\text {global }}$, and does “each ROI region” refer to $\\tilde{z}_{\\text {roi}}$? The notation and phrasing are so ambiguous that it is impossible to confidently link these terms across sections.\n    6. What are the query, key, and value inputs in the attention mechanism described in lines 242–245? What are the input and output shapes?\n    7. The output of this attention module is never assigned a symbol or variable name—how is it used in subsequent computations?\n    8. Line 261 mentions “all components”. What exactly does this include?\n    9. The loss functions described in lines 264–281—are they all applied simultaneously, or are different losses used in different stages of the claimed “progressive training” strategy? This is never clarified.\n3. While the paper cites Alpha-CLIP, it omits UMG-CLIP, despite the clear conceptual overlap. Both works are highly relevant, and the absence of UMG-CLIP in the literature review weakens the contextual framing.\n4. Given that Alpha-CLIP and UMG-CLIP already provide strong, well-established frameworks for region-aware multimodal alignment—Alpha-CLIP excels in zero-shot transfer across seven diverse downstream tasks, while UMG-CLIP demonstrates superior performance on dense prediction tasks like segmentation and detection—it is unclear why the authors chose to deviate from these architectures. The paper offers no justification for this design choice. Moreover, the evaluation is limited to classification and retrieval (with VQA effectively treated as a classification task), failing to demonstrate capabilities on dense tasks where region-level alignment should matter most.\n5. Alpha-CLIP and UMG-CLIP should serve as essential baselines, yet the paper does not include any comparison with them.\n6. The paper does not explain how the full architecture is deployed during downstream evaluation. Which components are active during inference? How are classification and retrieval actually performed?\n7. Equation (4) introduces multiple hyperparameters without any description of their roles or how they were selected.\n8. There is no information about the test datasets—specifically, whether ground-truth lesion locations are available. If not, does the model rely on external tools (e.g., detectors) to obtain ROI proposals during testing? This is critical for evaluating the practicality of the approach.\n\n---\n\n### **Minor Weaknesses**\n\n1. Notational inconsistencies: e.g., $h_{\\text {j}}$uses subscript j, but the first occurrence of $x_{\\text {roi}}$ lacks a subscript.\n2. Missing t-SNE visualizations of the learned embeddings.\n3. Figure 6 lacks comparative visualizations (e.g., against baselines), limiting its interpretability."}, "questions": {"value": "All concerns listed above must be addressed. I must emphasize that the writing quality is severely deficient, particularly in the methodology section. If the authors aspire for this work to meet the standards of a venue like ICLR, a substantial revision focused on clarity, rigor, and completeness of exposition is absolutely necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GowJQOWbKB", "forum": "h21iXALHk6", "replyto": "h21iXALHk6", "signatures": ["ICLR.cc/2026/Conference/Submission6609/Reviewer_aEi6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6609/Reviewer_aEi6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913818985, "cdate": 1761913818985, "tmdate": 1762918931500, "mdate": 1762918931500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses two challenges of medical image understanding:\n1. the lack of high-quality annotated datasets\n2. a method that produces predictions based on small critical details in an image in contrast to global image features.\n\nTo tackle the first challenge, the authors use human–AI collaborative annotation to construct a novel dataset called MedRegion-500k (only a subset of which will be publicly released). \n\nFor the second challenge, the authors propose RegionMed-CLIP, a region-aware multimodal contrastive learning framework that explicitly incorporates localized pathological signals along with holistic semantic representations."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Breadth of the dataset, with coverage of various medical imaging modalities and anatomical regions.\n2. The model seems to outperform all methods it was compared to."}, "weaknesses": {"value": "1. There is no evidence for the quality and reliability of the resulting annotations provided. \n2. The baseline models are fairly old (the latest model is from 2023). It would be advisable to compare the model's effectiveness in comparison to more recent studies both within this domain as well as for generalist VLM models.\n3. There is a significant number of inconsistencies in the references (a significant portion of references in the paper refers to non-existing works, including references to the models used as baseline).\n4. Negative labels are purely text based. Is there evidence that the model learns to differentiate similar-looking but different diseases from images?\n5. It is difficult to disentangle the contribution of the architecture from the dataset without having results of other architectures trained on the same dataset. No proof that the architecture did in fact lead to the demonstrated improvement in performance and not purely the dataset."}, "questions": {"value": "1. What was the size of the fine-tuning train and test datasets? What is the fine-tuned model performance on the test dataset? Were there any additional tests validating the quality of the resulting MedRegion-500k?\n2. Can the model process multiple regions? What is the distribution of the number of regions per image in the dataset? What is the average size of the region? What is the distribution of region sizes in the dataset? Does the model perform better or worse when the region is small or big compared to the baseline models?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hd3LwSE1yw", "forum": "h21iXALHk6", "replyto": "h21iXALHk6", "signatures": ["ICLR.cc/2026/Conference/Submission6609/Reviewer_itfz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6609/Reviewer_itfz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134034835, "cdate": 1762134034835, "tmdate": 1762918931021, "mdate": 1762918931021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}