{"id": "e8rF9RKVLI", "number": 24143, "cdate": 1758353282427, "mdate": 1759896779897, "content": {"title": "SVDF-20: A LARGE-SCALE MULTILINGUAL BENCHMARK FOR AI-GENERATED SINGING DETECTION", "abstract": "As generative models replicate human singing with uncanny precision, detection systems must operate reliably across all languages, not just English or Mandarin. Current detectors fail catastrophically on unfamiliar languages, a critical gap we address with SVDF-20, the first comprehensive multilingual singing voice deepfake detection benchmark. Our contributions are threefold: (1) We provide a quality-controlled dataset of 24,421 songs ($1,475.6$ hours) across 20 languages, introducing $87%$ novel linguistic content compared to existing resources—including all 10 major Indic languages previously absent from singing voice deepfake detection research. (2) We demonstrate through experiments on eight architectures that multilingual training is essential: models trained on limited languages degrade to $45%$ Equal Error Rate (EER) on diverse languages, while SVDF-20-trained models achieve a $31%$ relative improvement, maintaining robust detection across all linguistic contexts. (3) We establish evaluation protocols with singer-disjoint splits and codec robustness tests that reveal how linguistic diversity fundamentally changes what models learn, shifting from language-specific patterns to universal synthesis artifacts. These findings establish that SVDF-20 enables the development of deepfake detectors capable of safeguarding musical authenticity globally, not just in data-rich languages.\n\nData and Code: \\href{https://anonymous.4open.science/r/SVDF20-D328/}{https://anonymous.4open.science/r/SVDF20-D328/}", "tldr": "SVDF-20: First large-scale multilingual singing voice deepfake detection benchmark (20 languages, 772K+ clips). Training on SVDF-20 yields 30-40% better performance on unseen languages vs SingFake baselines.", "keywords": ["singing voice deepfake detection", "multilingual benchmark", "cross-lingual generalization", "audio forensics", "deepfake detection", "SVDD", "multilingual audio", "voice synthesis detection", "audio authenticity", "singing voice synthesis", "cross-domain evaluation", "audio deepfake benchmark", "multilingual audio forensics", "singing voice authentication", "audio spoofing detection", "machine learning", "deep learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7601ec72141b16a910b65bb2f5ea27ce1b1bcfca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SVDF-20, which is a new large-scale dataset and benchmark for singing‐voice deepfake detection covering 20 languages. The dataset is created with matched bona fide vs. AI‐generated vocals, obtained via a controlled YouTube-based pipeline and vocal separation. The authors conduct experiments with various SVDD architectures, and ablate reseach questions relevant to the generalizability of monolingual models to multi-lingual songs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper considers research questions regarding the generalizability of synthetic music detection models to unseen languages and provides a dataset and evaluation methodology for this. \n2. The multi-lingual music collection is generalized, providing a very good dataset for multi-lingual music generation and detection."}, "weaknesses": {"value": "1. One of the core motivation for this paper is the question: \"Can SVDD models trained on limited languages generalize to unseen ones?\". However, there is no dataset split that directly measures this. T04 split measures unseen distribution, not strictly unseen languages. Thus, later results don't directly answer the core motivation question.\n2. The deepfake samples come from heuristic YouTube queries. The paper lacks detail on how correctness of labels was ensured. If many samples come from a single synthesis method or user community, models might learn dataset-specific artifacts. The potential bias in what content appears on YouTube (genres, singers) is not addressed. Furthermore, there is no human evaluation of the dataset, which would have provided some validation for the dataset creation process. \n3. There is a discrepancy in the number of bonafide and deepfake songs for some languages. This could introduce bias in training and evaluation.\n4. The evaluation shows benefits over training on SingFake, but it would be informative to see performance comparison over other datasets. For example, the performance of a model trained on SONICS or on CTRSVDD. Furthermore, only EER on the dataset level is shown. But extending this to other metrics such as F1 scores would be great.\n5. The evaluation is done on vocal-only clips. This doesn't capture the nuances of a 'song', as music and relevant background information also provide essential information. So the background information can provide enough information for existing fake detection methods to detect the song is fake. This calls into question the necessity of multi-lingual training in this domain, without supporting evidence. Also, a model is used to extract the vocals which might add further artifacts."}, "questions": {"value": "1. Did the authors conduct evaluation of existing methods on a unseen languages test-set?\n2. Can the authors provide more details regarding their fake song generation setup? \n3. Did the authors conduct any human evaluation (both quantitative and qualitative) of the generated dataset, to ensure correctness?\n4. Did the authors conduct cross-dataset training and evaluation experiments such as training on SONICS and evaluation on their multilingual dataset?\n5. The paper shows that speech recognition and representation learning can generalize to unseen languages, but SVDD models don't. Can the authors provide any explanation for this?\n6. Can the authors provide evidence that multilingual understanding is required for fake song detection when music is included in the vocal-only clips?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "As real songs are used and fake songs generated, there can be implications about privacy and legal compliance."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IGb58FfBkQ", "forum": "e8rF9RKVLI", "replyto": "e8rF9RKVLI", "signatures": ["ICLR.cc/2026/Conference/Submission24143/Reviewer_hqhy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24143/Reviewer_hqhy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811423261, "cdate": 1761811423261, "tmdate": 1762942958665, "mdate": 1762942958665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SVDF-20, a multilingual singing-voice deepfake detection benchmark spanning 20 languages (10 Indic + 10 global), comprising 24421 songs (~1476 h) segmented into 388631 clips across codec variants. The work demonstrates that multilingual training yields 13.78 pp EER improvement on cross-domain evaluation versus monolingual baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is the first large-scale multilingual SVDD benchmark with substantive Indic representation; clear community need. Singer-disjoint splits, codec augmentation, and multi-tier evaluation protocol (T01–T04) demonstrate thoughtful construction from authors. Despite undertraining, the multilingual advantage appears across all eight architectures tested, suggesting a robust (if underestimated) effect."}, "weaknesses": {"value": "(1) 25 epochs cannot be defended for a dataset of this scale. The paper should either (a) extend training to convergence and revise claims accordingly, or (b) explicitly reframe as a \"dataset paper with baseline experiments\" rather than claiming definitive conclusions about architectural comparisons.\n\n(2) No per-language metrics to assess fairness or identify low-resource failure modes; No learning curves to validate that 25 epochs represents reasonable stopping; and No feature-space analysis (e.g., t-SNE by language, language-adversarial probing) to substantiate the invariance claims beyond aggregate EER. \n\n(3) Equations 4, 5 formalize the invariance hypothesis; but this hypothesis was never empirically measured. I recommend the authors to include quantitative divergence metrics (MMD, JS) on learned representations.\n\n(4) While metadata-only release is stated, multi-jurisdictional sourcing from platforms requires clearer documentation of takedown handling, consent mechanisms (especially for lesser-known singers), and re-download rights for researchers."}, "questions": {"value": "(1) Can you provide validation loss curves or per-epoch EER trajectories to demonstrate that 25 epochs approaches a plateau? Current absolute EERs (31–38%) suggest otherwise.\n\n(2) What are EERs for each language independently? Are low-resource Indic languages disproportionately harmed?\n\n(3) What happens if you train on only Indic-10 or global-10 subsets? Does the multilingual benefit scale linearly with language diversity?\n\n(4) Real-world SVDD can also operate on full songs with accompaniment (the \"mixture\" setting). How do these models perform on non-vocal-isolated audio?\n\n(5) Can you quantify Eq. 4–5 using representation divergence metrics across language pairs as previously mentioned?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "I request verification that the reconstruction pipeline includes some automated checks for content takedowns and that platform ToS interpretations hold across all 20 linguistic/jurisdictional contexts."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Odq0U7As8s", "forum": "e8rF9RKVLI", "replyto": "e8rF9RKVLI", "signatures": ["ICLR.cc/2026/Conference/Submission24143/Reviewer_8ujm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24143/Reviewer_8ujm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958354877, "cdate": 1761958354877, "tmdate": 1762942957674, "mdate": 1762942957674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduced a large-scale benchmark dataset for singing voice deepfake detection (SVDD). The authors proposed a dataset consisting of 20 languages (which includes 10 major Indic languages underexplored in SVDD research). They showed rigorous experiments across eight diverse architectures to show that the models trained on their dataset outperform on less diverse data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The primary strength of the paper is the diversity of the languages in their SVDD dataset, which is expected to allow generalizability across more languages. \n\nThe authors have conducted rigorous experiments to isolate the impact of the training data and utilized the Singfake dataset to compare the improved out-of-distribution generalizability to language."}, "weaknesses": {"value": "While the paper introduces a new benchmark, it did not manage to strongly establish the importance of a multi-lingual dataset of this level of diversity. The authors utilized multiple models to compare the difference in performance between Singfake and SVDF-20. But the difference in their number of samples (Singfake dataset has a total of 16k samples, whereas SVDF-20 has a total of 388k samples) reduces the impact of this finding, since the improved generalizability can also come from simply having more samples rather than having diversity of languages. A stronger claim would require a control experiment of comparing against a less linguistically diverse or even monolingual dataset with a comparable size to strongly prove that the advantages are actually from the diversity of language, not simply due to having more training data.\n\nInadequate information regarding the generation methods (Singing Voice Conversion, Singing Voice Synthesis, or even end-to-end song generation) used in this dataset limits the utility of this dataset for diagnosing where the model is failing."}, "questions": {"value": "1. Could the authors discuss how they can prove the observed performance gains are due to having diverse languages and not simply the higher data volume?\n2. Could the authors provide more details regarding the generation methods present in their dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jbwcufovZy", "forum": "e8rF9RKVLI", "replyto": "e8rF9RKVLI", "signatures": ["ICLR.cc/2026/Conference/Submission24143/Reviewer_UGdf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24143/Reviewer_UGdf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974657329, "cdate": 1761974657329, "tmdate": 1762942957316, "mdate": 1762942957316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SVDF-20, a large-scale, multilingual dataset for singing voice deepfake detection (SVDD). It includes 24,421 songs across 20 languages (10 Indic and 10 global), covering ~1,475 hours of audio. The authors position this as the most linguistically diverse SVDD benchmark to date, aiming to answer three questions: (1) how multilingual data affects SVDD performance, (2) whether multilingual training improves cross-lingual generalization, and (3) whether the dataset helps bridge domain gaps compared to prior English or East-Asian–focused datasets such as SingFake, CTRSVDD, and SONICS. They train and evaluate several open-source audio models (AST, WavLM, AASIST, etc.) on these setups and conclude that multilingual training improves robustness, though performance naturally degrades when moving across unseen languages."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Data effort:** The authors clearly spent effort in curating and cleaning multilingual singing data. \n\n- **Transparency in setup:** The paper reports training settings, architectures, and language groupings in a reproducible manner. It’s easy for others to replicate or extend the experiments."}, "weaknesses": {"value": "1. **Contribution is incremental.**  \n   The paper mainly extends prior multilingual datasets rather than introducing a novel task or method. *SingFake*, *CTRSVDD*, and *WildDeepfake-SVDD* already include multilingual material. This work’s novelty is scale, not concept.\n\n2. **Misalignment around SONICS.**  \n   The paper repeatedly frames *SONICS* as an “advancement of SVDD,” but SONICS is a **synthetic song detection** task, both vocals and accompaniment are generated. SVDD, on the other hand, deals with *fake vocals over real music*. The authors conflate the two tasks, which weakens the motivation and literature positioning.\n\n3. **No substantial modeling or algorithmic contribution.**  \n   All experiments rely on off-the-shelf backbones (AASIST, WavLM, AST) and standard training setups. There’s no new detection technique, loss function, or data processing beyond dataset expansion.\n\n4. **Research questions are shallow.**  \n   RQ1 and RQ2, impact of multilingual data and multilingual training are intuitive. Everyone expects monolingual models to degrade cross-lingually, and multilingual training to help. The results merely confirm common sense without offering mechanistic or analytical insight (e.g., language-specific phoneme transfer, prosody shifts, etc.)."}, "questions": {"value": "My concerns are mostly about the scope and motivation rather than implementation details. One suggestion I would like to give is fix the confusion of SVDD and SONICS paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "g2O2Zfslhc", "forum": "e8rF9RKVLI", "replyto": "e8rF9RKVLI", "signatures": ["ICLR.cc/2026/Conference/Submission24143/Reviewer_KE9u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24143/Reviewer_KE9u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044313543, "cdate": 1762044313543, "tmdate": 1762942956988, "mdate": 1762942956988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}