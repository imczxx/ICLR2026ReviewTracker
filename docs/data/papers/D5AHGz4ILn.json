{"id": "D5AHGz4ILn", "number": 11099, "cdate": 1758189174779, "mdate": 1763656989809, "content": {"title": "Lego-Edit: A General Image Editing Framework with Model-Level Bricks and MLLM Builder", "abstract": "Instruction-based image editing has garnered significant attention due to its direct interaction with users. However, real-world user instructions are immensely diverse, and existing methods often fail to generalize effectively to instructions outside their training domain, limiting their practical application. To address this, we propose Lego-Edit, which leverages the generalization capability of Multi-modal Large Language Model (MLLM) to organize a suite of model-level editing tools to tackle this challenge. Lego-Edit incorporates two key designs: (1) a model-level toolkit comprising diverse models efficiently trained on limited data and several image manipulation functions, enabling fine-grained composition of editing actions by the MLLM; and (2) a three-stage progressive reinforcement learning approach that uses feedback on unannotated, open-domain instructions to train the MLLM, equipping it with generalized reasoning capabilities for handling real-world instructions. Experiments demonstrate that Lego-Edit achieves state-of-the-art performance on GEdit-Bench and ImgBench. It exhibits robust reasoning capabilities for open-domain instructions and can utilize newly introduced editing tools without additional fine-tuning.", "tldr": "Our Lego-Edit leverages a Multimodal Large Language Model to organize a set of model-level editing tools and trains it via progressive reinforcement learning for generalized instruction-based image editing.", "keywords": ["Instruction-based Image Edit", "Multimodal Large Language Model", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/060a3bb19d88660a3da4304d4121ddcf5b368233.pdf", "supplementary_material": "/attachment/20fe1b860f5453443bec97888c96f543a41bb754.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Lego-Edit, a modular framework for instruction-based image editing. The system consists of a Builder (a multimodal large language model fine-tuned with reinforcement learning) that organizes a library of model-level tools, Bricks, such as segmentation, inpainting, and style-transfer modules. A three-stage progressive RL scheme, supervised fine-tuning, ground-truth-based RL, and GT-free RL guided by a critic mode, gradually improves the Builder’s reasoning and tool-composition ability. Experiments on GEdit-Bench and ImgEdit-Bench show performance gains over strong baselines (e.g., BAGEL, Step1X-Edit, UniWorld-V1). The system also demonstrates zero-shot generalization and the ability to integrate new tools without retraining."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a well-motivated modular architecture that addresses the limited flexibility of end-to-end image-editing models.\n- The three-stage reinforcement-learning pipeline is a strong design choice that contributes to improved reasoning and generalization.\n- Experimental results are extensive and include both quantitative benchmarks and visual demonstrations that convincingly illustrate the system’s strengths.\n- The implementation details and reproducibility statement are thorough and transparent.\n- The framework shows promising potential for extensibility and future multimodal applications."}, "weaknesses": {"value": "- The contribution is primarily engineering-focused. The paper combines known techniques, MLLM agents, RL fine-tuning, LoRA adapters, into a unified system rather than introducing a new theoretical or algorithmic idea. While the integration is impressive, it offers limited conceptual insight into why or how the components interact optimally.\n- The complexity of the architecture (Builder, Executor, Bricks, Critic) makes it difficult to interpret which design choices drive performance gains. More controlled ablations would clarify the marginal value of each component.\n- The critic model used in the final RL stage plays an important conceptual role but is evaluated mostly qualitatively; quantitative ablations or alternative critic formulations would strengthen the argument.\n- The writing and structure could be improved to highlight the core intuition behind the framework before diving into system specifics.\n- The evaluation scope is limited to existing benchmarks. Although results are strong, the paper would benefit from evidence of user studies or real-world use cases demonstrating the practicality of Lego-Edit in diverse conditions.\n- Some important limitations and failure modes are only briefly mentioned (e.g., potential tool conflicts, scalability bottlenecks) and deserve more explicit discussion.\n- Runtime analysis is incomplete. The paper reports latency only relative to BAGEL but does not compare against other strong baselines such as Step1X-Edit, OmniGen2, or FLUX.1 Kontext. A broader efficiency analysis would help position Lego-Edit’s computational trade-offs relative to state-of-the-art models."}, "questions": {"value": "1. How dependent is Lego-Edit’s performance on the chosen MLLM backbone (MiMo-VL-7B)? Could smaller or publicly available models achieve comparable coordination ability?\n2. Could the authors report quantitative results comparing Stage 2 (GT-based RL) and Stage 3 (critic-based RL) to show how much each stage improves reasoning or compositional success rate?\n3. How does the framework scale when the number of available tools increases or when workflows become more complex? Are there computational or latency trade-offs?\n4. What are the typical failure modes? For example, does the Builder sometimes misinterpret instructions, or do tool boundaries introduce visual artifacts?\n5. Since the paper is positioned in “applications to multiple modalities,” how feasible would it be to extend this framework to non-visual domains such as audio or video editing?\n6. Have the authors evaluated how easily new users can add tools or control the Builder’s behavior without retraining? Understanding the system’s real-world maintainability could increase its practical impact.\n7. Beyond the single latency result against BAGEL, could the authors provide runtime or computational cost comparisons with other leading systems such as Step1X-Edit, OmniGen2, or FLUX.1 Kontext? Such information would clarify Lego-Edit’s practical efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JWFjXVe6mZ", "forum": "D5AHGz4ILn", "replyto": "D5AHGz4ILn", "signatures": ["ICLR.cc/2026/Conference/Submission11099/Reviewer_ozbk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11099/Reviewer_ozbk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760603390256, "cdate": 1760603390256, "tmdate": 1762922275800, "mdate": 1762922275800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "## Additional Experimental Updates\n\nWe sincerely thank the reviewers for their suggestion to include more comprehensive performance comparisons. Considering the limited baseline performance of FLUX and to further demonstrate the rapid adaptability and iterative efficiency of our framework, we conducted additional experiments and updated our results accordingly.\n\nSpecifically, we retrained our tool LoRA using the open-source **Qwen-Image-Edit-2509** [1] model following the data and procedures described in our Implementation Details (Section 4.1). In addition, we trained a new LoRA tool for better foreground object segmentation. All tools adopt the mask-input design inspired by Lanpaint [2], and use 8 denoising steps. The overall runtime efficiency remains comparable to the FLUX-based tools. The Builder module remains unchanged.\n\nOur latest results surpass both **Qwen-Image-Edit** and **SeedEdit 4.0** [3], two recent state-of-the-art systems, on **GEditBench** and **ImageEditBench**. SeedEdit results were obtained through the closed-source *doubao-seedream-4.0* API.\n\n\n**GEditBench Results**\n\n| Model               | G_SC↑ | G_PQ↑ | G_O↑ |\n|---------------------|-------|-------|------|\n| Qwen-Image-Edit-2509| 8.15  | 7.86  | 7.54 |\n| SeedEdit-4.0        | 8.17  | 7.66  | 7.44 |\n| Ours FLUX Tools     | 6.45  | 7.45  | 6.88 |\n| **Ours Qwen Tools** | **8.42** | **7.90** | **7.84** |\n\n**ImageEditBench Results**\n\n| Model | Add | Adjust | Extract | Replace | Remove | Style | Action | Hybrid | Background | Overall↑ |\n|-------|-----|--------|---------|---------|--------|-------|--------|--------|------------|----------|\n| Qwen-Image-Edit-2509 | **4.32** | 4.36 | 4.04 | **4.64** | 4.52 | 4.84 | 4.71 | 3.39 | 4.37 | 4.35 |\n| SeedEdit-4.0 | 4.17 | 4.35 | 3.87 | 4.43 | 4.66 | 4.70 | 4.68 | 3.51 | **4.49** | 4.32 |\n| **Ours FLUX Tools** | 4.03 | 3.84 | 2.47 | 3.41 | 3.42 | 4.48 | 4.04 | 3.20 | 3.41 | 3.59 |\n| **Ours Qwen Tools** | 4.19 | **4.41** | **4.17** | 4.58 | **4.70** | **4.86** | **4.74** | **3.60** | 4.22 | **4.39** |\n\nOur approach achieves leading performance on all metrics of GEditBench, and achieves the highest overall score and best performance in six ImageEditBench sub-tasks.\n\nWe further evaluated facial similarity before and after editing, following the metric described in our supplementary materials (Section B.2). This metric assesses the preservation of non-edited facial regions. Results are shown below:\n\n**Facial Similarity Evaluation**\n\n| Methods | Background Change | Color Alter | Subject Add | Subject Remove | Overall↑ |\n|---------|-------------------|-------------|-------------|----------------|----------|\n| Qwen-Image-Edit-2509 | 0.82 | 0.84 | 0.85 | 0.79 | 0.83 |\n| SeedEdit-4.0 | 0.77 | 0.83 | 0.81 | 0.87 | 0.82 |\n| Ours FLUX Tools | 0.90 | **0.85** | 0.88 | 0.87 | 0.88 |\n| **Ours Qwen Tools** | **0.92** | **0.85** | **0.90** | **0.88** | **0.89** |\n\nAcross all tasks, our updated model shows superior preservation of non-edited regions.\n\nThese extended experiments further validate the efficiency and flexibility of our framework for integrating arbitrary tools and iteratively improving overall performance. They also support our claim that training LoRA modules for specific functionalities helps improve performance. Notably, training and integrating all the new tools required only one day on eight H20 GPUs, significantly less than the computational demands of training large end-to-end models.\n\nAdditionally, we observed pixel-shift artifacts in the open-source Qwen-Image model during our tests; interestingly, these artifacts were largely mitigated after LoRA training.\n\n## Discussion of Failure Cases\n\nWe thank the reviewers for this valuable suggestion. Following the feedback, we have added a detailed discussion of failure modes in **Section 5** of the revised paper and highlighted the corresponding subsection title in blue for clarity.\n\n\n1. LanPaint: Training-Free Diffusion Inpainting with Asymptotically Exact and Fast Conditional Sampling, arxiv\n2. Qwen-image technical report, arxiv\n3. Seedream 4.0: Toward next-generation multimodal image generation, arxiv"}}, "id": "a6EtS591M5", "forum": "D5AHGz4ILn", "replyto": "D5AHGz4ILn", "signatures": ["ICLR.cc/2026/Conference/Submission11099/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11099/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11099/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763656373008, "cdate": 1763656373008, "tmdate": 1763656373008, "mdate": 1763656373008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets on the problem that though have somewhat good editing performance, the current instruction-based image editing model always fail to generalize to unknown instructions. They propose a method called Lego-Edit, it finetunes a Multi-modal Large Language Model (MLLM) by reinforcement learning, enable it to coordinate model-level editing tools. In the MLLM part, it three-stage progressive reinforcement learning training strategy to make the MLLM more adapted to the current task. Extensive experiment results prove the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is in a good organization, the idea is easy to understand. \n2. The experiment section is good, with abundant results and clear explanation. So I think the results are convincing."}, "weaknesses": {"value": "1. Some typos, eg. Line 182, fine-graine. Please check the whole paper. \n2. If I do not mis-understand, since you incorporate more models, can you have some discussions about the train / inference efficiency of the proposed method?\n3. I am willing to see more discussions about the novelty of the proposed method, since dividing one editing task to multiple different tasks is not interesting enough I think. \n4. I also want the authors to have more discussions about the limitation and future work of the work."}, "questions": {"value": "See weaknesses. I think this paper is ok, but we can have more discussions during the rebuttal period. If my concerns are solved, I will be happy to raise the score. I hope the discussions can also bring about some new insights."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hm0LLVnoRl", "forum": "D5AHGz4ILn", "replyto": "D5AHGz4ILn", "signatures": ["ICLR.cc/2026/Conference/Submission11099/Reviewer_4Y2L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11099/Reviewer_4Y2L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761481807723, "cdate": 1761481807723, "tmdate": 1762922275273, "mdate": 1762922275273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for fine-tuning MLLMs to establish an automated workflow for instruction-based image editing tasks. By appropriately invoking pre-trained predictive models and editing models, this workflow achieves a variety of editing tasks and even exhibits a certain level of zero-shot capability for flexible editing requirements. Extensive visual results demonstrate the reliability of the method in terms of editing effectiveness, while quantitative results indicate its performance improvement over comparative methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper presents a valuable task formulation: how to orchestrate multiple sufficiently strong single-function perception models and editing models to achieve automated, instruction-compliant, and flexible image editing.\n2. The proposed method in the paper is ingenious yet intuitive. Its multi-stage training strategy, tailored for the task, along with the reward design, effectively addresses the challenge of imbalanced training data annotations.\n3. The results achieved by the proposed method are satisfactory, positioning it at the forefront of current community standards from both qualitative and quantitative perspectives.\n4. The authors' approach to solving the problem is concise and clear, making it easy to understand and facilitating follow-up research."}, "weaknesses": {"value": "1. Lacking a comparison with manual workflow construction would better highlight the efficiency and automation advantages of the proposed method.\n2 . Lacking a dedicated limitations section. A section of limitations would improve the academic rigor and transparency of the work.\n3. Authors need to provide a clearer explanation of the dataset construction for each training stage of the Builder to facilitate reader comprehension. If the authors used Qwen2.5-VL to generate ground truth data, why did they opt for the MLLM trained in the paper instead of directly calling the Qwen API in practical applications?"}, "questions": {"value": "1. The tools are not always accurate. How to handle this situation?\n2. How to avoid the error accumulation when using the tool chain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D05yLz0xOa", "forum": "D5AHGz4ILn", "replyto": "D5AHGz4ILn", "signatures": ["ICLR.cc/2026/Conference/Submission11099/Reviewer_MQQG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11099/Reviewer_MQQG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969292119, "cdate": 1761969292119, "tmdate": 1762922274695, "mdate": 1762922274695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a general instruction-based image editing framework addressing poor generalization of existing methods to out-of-training-domain instructions. It uses a reinforcement learning (RL)-fine-tuned Multimodal Large Language Model (MLLM) called Builder to organize model-level tools (Bricks).\n\nBricks include predictive models (e.g., RES for object segmentation, ADD-PRED for adding position prediction) and editing models (e.g., INPAINT for inpainting, STYLE for style transfer), each trained independently for flexibility and performance.\n\nIts three-stage progressive RL training: first, Supervised Fine-Tuning (SFT) builds basic capabilities; second, GT-based RL optimizes tool composition; third, GT-free RL uses an MLLM critic for feedback to enhance open-domain instruction handling.\n\nExperiments show LEGO-Edit achieves state-of-the-art on GEdit-Bench and ImgBench. It handles complex multi-step edits, adapts to new tools/feedback zero-shot, maintains non-edited region consistency, and supports Chinese instructions and text editing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a new direction to handle complex human-instructed image-editing tasks that uses existing image-editing models to construct an agent.\n2. It shows superior experimental results when compared with other SOTA image generation models."}, "weaknesses": {"value": "1. The description of training Builder is not sufficient, especially for the reinforcement learning section.  How to obtain an accurate reward when dealing with complex scenarios?\n2. How about the results when compared with the Qwen-image and Seedream 3.0/4.0 series, which are end-to-end image generation models? \n3. There seem to be no failure cases presented in this paper. How to deal with the situation where the agent determines the error workflow, which may obtain terrible results? And how about the success rate of the agent?"}, "questions": {"value": "1. What is your opinion on whether the image generation area will pursue a strong end-to-end model or develop an agent that uses specific models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jBGGKkBBhG", "forum": "D5AHGz4ILn", "replyto": "D5AHGz4ILn", "signatures": ["ICLR.cc/2026/Conference/Submission11099/Reviewer_AU9w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11099/Reviewer_AU9w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762083081814, "cdate": 1762083081814, "tmdate": 1762922274211, "mdate": 1762922274211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}