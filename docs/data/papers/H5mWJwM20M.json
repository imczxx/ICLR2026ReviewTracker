{"id": "H5mWJwM20M", "number": 23743, "cdate": 1758347842268, "mdate": 1763702541215, "content": {"title": "Towards a Unified Theory of Quantization and Sparsity", "abstract": "Quantization and sparsification are two model compression strategies that are traditionally treated as orthogonal in the literature. Building on recent work, we show that jointly considering these techniques can meaningfully affect compression performance. First, we extend prior tensor-level analyses and prove that for any $L_p$ norm, applying sparsification before quantization ($\\mathbf{S} \\to \\mathbf{Q}$) always yields lower errors than the reverse. However, we demonstrate that tensor-level analysis is insufficient to predict model performance, motivating the need for model-level evaluation. As such, we provide the first model-level analysis showing that $\\mathbf{S} \\to \\mathbf{Q}$ obtains better loss in certain settings when we choose quantization and sparsification algorithms independently. Yet, this preference does have its limits. When fully relaxing model assumptions, we find it difficult to prove the superiority of $\\mathbf{S} \\to \\mathbf{Q}$, casting doubt on the preference in the general case. To that end, we introduce Quantization-Aware Sparsification (QAS), a novel compression framework that sparsifies accounting for prior quantization, as a simple counterexample. Using this framework, we provide a simple counterexample in which $\\mathbf{Q} \\to \\mathbf{S}$ using QAS performs comparably to $\\mathbf{S} \\to \\mathbf{Q},$ illustrating that careful co-design between model compression steps can greatly influence performance.", "tldr": "", "keywords": ["model compression", "quantization", "sparsity"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98a2316e069339fa2c123e7c2a894984e24cbbc1.pdf", "supplementary_material": "/attachment/ed9e1f5d15d069488af947af3662fc412c6e5259.zip"}, "replies": [{"content": {"summary": {"value": "The work advances the theoretical understanding of combining sparsity and quantisation for compressing deep learning models. First, the authors generalise previous results on the advantage (w.r.t. parameter reconstruction error) of running sparsification-before-quantisation vs quantisation-before-sparsification. Then, the authors consider the model-level with a diagonal Hessian, equivalent to a $\\mathrm{diag}(H)$-weighted squared reconstruction error, showing that (under some conditions) an advantage for sparsification-first. However this is shown not to hold true in the more general case. A short empirical investigation follows, showing that sparsification-first can be better in practice, but that if the sparsification mask is taken from the original weights, then quantisation-first can also work as well."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work is clearly presented and makes a reasonably robust case for the main claims. Particular strengths:\n\n - Notation is clear and sufficiently well-explained\n - Helpful intuition is given for some proofs, e.g. in Section 4.1\n - Results for QAS are interesting and somewhat surprising to me\n - The idea in section 6.3 of using a short re-training phase to disambiguate weights that are quantised to the same level is interesting"}, "weaknesses": {"value": "**Main concern:** Although I appreciate the rigorous derivation, the results in Theorems 1, 2 and 4 seem trivial and unsurprising. T1 since (as explained in 4.1), weights that are quantised to the same level cannot be disambiguated, T2 is easily reduced to T1, as stated and the claim associated with T4 is weak, having dropped the constraints that made it possible to show T3. I found T3 the most interesting, but I don't completely follow the constraints (question below). Combined with the short and small-scale empirical investigation, I do not take the results as sufficiently informative to the community to recommend acceptance.\n\n**Specific concerns and questions:**\n\n1. The scale bound $\\delta \\leq \\epsilon$ isn't entirely clear to me - do we know if this is likely to be satisfied in practice, if scale is derived from a tensor-absmax?\n1. Is the definition of $\\epsilon_S$ in Theorem 3 (body) correct? I think as stated the following assumption would reduce to $\\epsilon_S = 0$, and seems inconsistent with that of the proof in L1134, which would expect $\\epsilon_S \\coloneqq \\Delta w_{Q \\rightarrow S} - \\Delta w_Q - \\Delta w_S$).\n1. Block (group) quantisation is common (and indeed seems to be used by default with AWQ in your supplementary scripts), while as I understand it, many of the theoretical results only apply to weights within a single block.\n1. Although it is acceptable to have a limited empirical example given the theoretical focus, I think there are some obvious gaps. Although the theoretical results concern (Q, S) = (Naive Max-Scaled, Magnitude) or (Naive Max-Scaled, OBD), the results do not include these settings, always using AWQ for quantisation and not including OBD for sparsity. It would also greatly help to see sparsity-only and quantisation-only results for comparison.\n1. In Table 1, despite the observation that for INT4 with 10% Magnitude pruning, the INT4 quantisation step already sets enough weights to zero to make sparsity a no-op, I can't understand why the more accurate INT8 format should under-perform INT4. (Anything that sparsity flushes to zero in the INT8 case would already have been flushed to zero in INT4.)\n1. The model scale, sparsity and quantisation settings considered in the experiments are far from the state of the art (e.g. Tseng et al., 2024, Liu et al., 2024, Frantar et al. 2023), making it hard to be confident that the theoretical results can be observed in practice.\n\n**Minor concerns:**\n\n - The \"key insight\" of L168, stating \"$|w_i - \\bar{Q}(\\bar{S}(w))_i| = 0$ if $w_i$ is pruned\" doesn't seem right, wouldn't it be $= |w_i|$ in this case?\n - While I accept Theorem 1, the implication stated in L185 concerning $L_{\\infty}$ seems potentially misleading - isn't equality very highly likely in this case (assuming tensor-wise quantisation scaling), since the same-quantisation-level collision would have to occur for the extreme value?\n - Body section 4.4 references Theorem 5 from the appendix.\n - In Tables 1 and 2, I most wish to compare Sparsity Method & Order for given Precision & Sparsity levels, which would be much easier if the major grouping was on Precision & Sparsity levels and minor grouping on Sparsity Method & Order.\n\n---\n\n_Tseng, A., Sun, Q., Hou, D. and De Sa, C.M., 2024. Qtip: Quantization with trellises and incoherence processing. Advances in Neural Information Processing Systems, 37, pp.59597-59620._\n\n_Liu Z, Zhao C, Fedorov I, Soran B, Choudhary D, Krishnamoorthi R, Chandra V, Tian Y, Blankevoort T. Spinquant: Llm quantization with learned rotations. arXiv preprint arXiv:2405.16406. 2024 May 26._\n\n_Frantar, E. and Alistarh, D., 2023, July. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International conference on machine learning (pp. 10323-10337). PMLR._"}, "questions": {"value": "I would appreciate any clarifications/corrections/counter-arguments, especially regarding my main concern and specific concerns above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OKMR0oLSh6", "forum": "H5mWJwM20M", "replyto": "H5mWJwM20M", "signatures": ["ICLR.cc/2026/Conference/Submission23743/Reviewer_9bPN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23743/Reviewer_9bPN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761251429659, "cdate": 1761251429659, "tmdate": 1762942788147, "mdate": 1762942788147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a stronger theoretical result that $S \\rightarrow Q$ (sparsity before quantization) is optimal over $Q \\rightarrow S$ on a tensor level, and the same result on the model level under some assumptions. To mitigate error in $Q \\rightarrow S$ setup, the authors propose using the unquantized weights for calculating sparsity mask, which they name Quantisation-Aware Sparsification (QAS). Then they demonstrate on OPT-125M model that $Q \\rightarrow S$ can perform on par with  $S \\rightarrow Q$ with QAS."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper tackles an open practical question of interaction between quantization and sparsity and highlights the necessity of codesigning the hybrid compression formats. \n2. This work proves on a tensor level for any $L_p$ norm that sparsity before quantization minimizes the loss."}, "weaknesses": {"value": "1. Limited novelty. The work closely follows Harma et al. (2025), in particular their claim on $S \\rightarrow Q$ being optimal over $Q \\rightarrow S$ on a tensor level and empirical validation on a model level. Harma et al. proves the statement of Theorem 1 for $p=1$, and the authors of this work only extend it to arbitrary $p \\geq 1$ without gaining new insight.\n2. Too strong assumptions: Theorems 2 and 3 assume Hessian to be an identity matrix, and a diagonal matrix, respectively. The condition $\\Delta w_{Q|S} = \\Delta w_Q$ is particularly restrictive, it basically means the sparsity only affects the weights that would be quantized to the same value as 0, $Q(S(x)) = Q(0) = Q(x)$. This practically renders sparsity unnecessary in this setup.\n3. The paper lacks contribution and insight. Proposed Quantisation-Aware Sparsification effectively makes element-wise quantization into quantization applied after sparsification. The paper offers no explanation on how would the training dynamics with QAS change for more complex quantization / sparsity schemes.\n4. Limited validation: the authors conduct experiments using a single model instance, and the results in Table 1 mainly reproduce Harma et al. (2025). Sparsity rates only include 10 and 25%."}, "questions": {"value": "1. What is the interplay between quantization and sparsity for other compression schemes, like SparseGPT, GPTQ, structured N:M sparsity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nh0ssYVSlj", "forum": "H5mWJwM20M", "replyto": "H5mWJwM20M", "signatures": ["ICLR.cc/2026/Conference/Submission23743/Reviewer_Sg9T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23743/Reviewer_Sg9T"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925925852, "cdate": 1761925925852, "tmdate": 1762942787942, "mdate": 1762942787942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyses the effects of quantization and sparsification of a model and the order in which they are applied. Through tensor-level and model-level analysis, they show that sparsification then quantization is more preferred than quantization then sparsification, especially when the methods for quantization and sparsification are chosen at random. They also propose a new Quantization Aware Sparsification through which they show that quantization, then sparsification, can perform better than the other way around. They show empirical evidence of their claims using the OPT 125M parameter model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. They show that S -> Q is better than Q -> S through tensor-level analysis\n2. They present the novel model-level analysis of the interaction between quantization and sparsification\n3. They propose a new Quantization Aware Sparsification method that prunes quantized models\n4. They validate the claims made through theoretical analysis with empirical evidence."}, "weaknesses": {"value": "1. The specific scenarios where Q -> S is strictly better than S -> Q are not specific theoretically; only the possibility is shown.\n2. No comparison to other quantization or sparsification methods.\n3. The choice of AWQ for quantization seems arbitrary, and no justification is provided\n4. No ablation study of how the proposed method scales with model sizes, sparsity levels, and bit-widths is shown"}, "questions": {"value": "Please see the weaknesses and \n1. It would be better to explicitly mention the metric (perplexity; lower is better) used to evaluate the models.\n2. In line 168, if $w_i$ is pruned, shouldn't the error be $|w_i|$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8hMGxeSfhX", "forum": "H5mWJwM20M", "replyto": "H5mWJwM20M", "signatures": ["ICLR.cc/2026/Conference/Submission23743/Reviewer_cNqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23743/Reviewer_cNqy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971850375, "cdate": 1761971850375, "tmdate": 1762942787685, "mdate": 1762942787685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}