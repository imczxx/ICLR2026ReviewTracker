{"id": "sEKJYLnTpA", "number": 2188, "cdate": 1757015773580, "mdate": 1759898164009, "content": {"title": "HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes", "abstract": "Language-specified mobile manipulation tasks in novel environments simultaneously face challenges interacting with a scene which is only partially observed, grounding semantic information from language instructions to the partially observed scene, and actively updating knowledge of the scene with new observations. To address these challenges, we propose, a hierarchical scene representation and associated search objective to perform language specified pick and place mobile manipulation tasks. We construct 2D maps containing the relevant semantic and occupancy information for navigation while simultaneously actively constructing 3D Gaussian representations of task-relevant objects. We fuse observations across this multi-layered representation while explicitly modeling the multi-view consistency of the detections of each object. In order to efficiently search for the target object, we formulate an objective function balancing exploration of unobserved or uncertain regions with exploitation of scene semantic information. We evaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and place benchmark in which perception is challenging due to large and complex scenes with comparatively small target objects. HELIOS achieves state-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also transfer to the real world without requiring additional data, as we illustrate by demonstrating it in a real world office environment on a Spot robot. We evaluate HELIOS on the OVMM benchmark in the Habitat simulator and achieve state-of-the-art results. As our approach is zero-shot, it can also transfer to the real world without requiring additional data to be captured, as we illustrate by demonstrating it in a real world office environment on a Spot robot.", "tldr": "", "keywords": ["active perception", "mobile manipulation", "3D Gaussian splatting"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bace589fa9d97627e6764956301841c6ac5d6eac.pdf", "supplementary_material": "/attachment/df3d7977ab548f690c9ab590fb6505a9b8460320.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents HELIOS, a novel method for performing language-specified mobile pick-and-place tasks in open-world environments. HELIOS introduces a hierarchical scene representation combining 2D semantic and occupancy maps with sparse 3D Gaussian object modeling. The method also formulates a global search objective that balances exploration and exploitation via information gain and semantic confidence. The approach is evaluated on the Open-Vocabulary Mobile Manipulation (OVMM) benchmark in the Habitat simulator and demonstrates state-of-the-art performance. Additionally, the method is shown to transfer to the real world zero-shot on a Boston Dynamics Spot robot."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Hierarchical representation: Combines 2D semantic maps and 3D Gaussian instance modeling, enabling robust semantic search and manipulation.\n\n- Well-formulated global search objective: Uses uncertainty-aware object scoring and information gain estimation to balance exploration vs. exploitation.\n\n- Comprehensive experiments: Includes comparisons against strong baselines (HomeRobot, MoManipVLA), ablation studies, limited vs. unlimited pick attempts, and real-world robot demonstrations.\n\n- Zero-shot transfer experiment: Modular design and zero-shot transferability make it a promising foundation for future real-world mobile manipulation systems.\n\nSOTA performance: HELIOS consistently outperforms baselines across all metrics, especially in success rate and robustness to failures."}, "weaknesses": {"value": "- Technical novelty: The primary contributions lie in system integration. Key components—BLIP-2, DETIC, 3D Gaussian Splatting, and frontier-based exploration—are all existing techniques. The global search objective is largely heuristic and lacks rigorous derivation or theoretical insight. The engineering contribution is appreciable; however, technical novelty can be improved.\n\n- The theory of the search strategy:\nThe proposed global search objective assumes optimistic information gain estimates. There is no theoretical justification or analysis of its performance bounds or optimality. From instinct, the method is reasonable, but a more strict analysis will help to improve the paper.\n\n- Generalization analysis can be more detailed\nWhile the method is claimed to be zero-shot, there is no systematic evaluation of generalization across scene types, instruction types, or camera viewpoints. The robustness to domain shift or novel object categories is unclear.\n- Inconsistent or unfair comparisons\nThe baselines (e.g., MoManipVLA, HomeRobot) may not be using the same level of semantic fusion or multi-view detection. Comparisons may thus favor HELIOS due to richer intermediate representations. More new or related baselines?"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PNmkSHQgXT", "forum": "sEKJYLnTpA", "replyto": "sEKJYLnTpA", "signatures": ["ICLR.cc/2026/Conference/Submission2188/Reviewer_UXNY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2188/Reviewer_UXNY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990145097, "cdate": 1761990145097, "tmdate": 1762916120817, "mdate": 1762916120817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HELIOS, a hierarchical exploration and interaction framework for language-specified mobile pick-and-place. HELIOS builds a 3 layer scene representation consisting of (i) a 2D occupancy map, (ii) layered 2D semantic value maps to guide frontier exploration and (iii) a sparse 3D Gaussian object map. Planning is done by a global search objective that trades off exploration and exploitation via an optimistic information-gain estimate. Results are presented on the OVMM and ObjectNav benchmark in Habitat."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The method achieves the state of the art results on the OVMM benchmark.\n* The authors also show the transfer of their method to the real world."}, "weaknesses": {"value": "1) The contributions of the paper are very unclear to me. Section 3.1.1 and 3.1.2 are taken from or heavily based on prior work. Most of Sec 3.1.3 is Preliminaries. From what I understand, the main contributions are the integration of the 2D and 3D scene representations, the Uncertainty-weighted object score and the global search objective. If I have gotten it correctly, I believe in its current form, the work falls below the ICLR acceptance bar.\n\n2) While 3DGS is sparse and instance-clustered, it's still very memory and compute intensive based on the details in section 6.2 in the appendix. \n\n3) The Spot demo is qualitative; no success rates, timing, or object set diversity are reported.\n\n4) I would not consider the results in section 4.2 to be the strength of the proposed method since it leads to 1.5% gain in Success but 2.0% drop in SPL compared to the baseline."}, "questions": {"value": "Minor suggestion: The paper’s readability could be improved if there were fewer images taking half width of the page."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "O5LWUIEPtU", "forum": "sEKJYLnTpA", "replyto": "sEKJYLnTpA", "signatures": ["ICLR.cc/2026/Conference/Submission2188/Reviewer_wuZW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2188/Reviewer_wuZW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064702900, "cdate": 1762064702900, "tmdate": 1762916119408, "mdate": 1762916119408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents HELIOS, a hierarchical exploration and scene representation framework for language-grounded mobile manipulation tasks in partially observed and open-world environments. The core challenge addressed is performing language-specified pick-and-place tasks when both the spatial environment and semantic cues are only partially observable. HELIOS combines: 2D layered maps, 3D Gaussian splatting, a hierarchical search objective, and an uncertainty-weighted object score into one framework. The system is evaluated on the OVMM benchmark and HM3D semantic navigation tasks, achieving state-of-the-art performance in open-vocabulary mobile manipulation and improved success rates when integrated as a stopping criterion for semantic object search."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The hierarchical integration of 2D and sparse 3D Gaussian maps for exploration and manipulation is novel and elegantly addresses multi-scale reasoning.  And the uncertainty-weighted object score provides a principled way to leverage semantic confidence and multi-view consistency in decision-making.  \n\n- The paper is technically solid and mathematically well-founded. The 3D Gaussian semantics formulation with Dirichlet updates, entropy-based information gain, and uncertainty weighting is rigorous. \n\n- Ablation studies (e.g., removing global search, using ground-truth semantics) are extensive and well-designed to isolate each contribution.  Clear experimental validation shows consistent improvement over strong baselines and across different pick-attempt limits.\n\n- This paper aims at addressing a pressing open problem in embodied AI: bridging language grounding, exploration, and manipulation under partial observability.  The zero-shot transfer to real hardware adds strong practical significance."}, "weaknesses": {"value": "- HELIOS mainly combines existing building blocks: frontier-based exploration, open-vocabulary object detection (BLIP-2, DETIC), and 3D Gaussian splatting with Bayesian updates. The integration is well-engineered but does not introduce fundamentally new algorithms.\n\n- The system involves many components (3DGS updates, frontier mapping, uncertainty weighting, search objectives, multi-threshold heuristics).  Although ablations exist, the paper lacks deeper sensitivity analyses on hyperparameters like $\\alpha_{cs}$, $\\tau_g$, $\\tau_{inc}$, and $\\alpha_d$, which control trade-offs between exploration and confidence.\n\n- Improvements in success rate (~2–3%) over MoManipVLA, while consistent, are relatively small considering the complexity. Performance is bottlenecked by upstream perception (object detection) and downstream grasping failures - limitations acknowledged but not experimentally analyzed.\n\n- The system is heavy: ~288 GPU-hours for 1199 episodes on 8 GPUs.  Real-time feasibility for on-board deployment is unclear, particularly for dense environments or more complex instructions."}, "questions": {"value": "I am not an expert in this domain, thus I have the following questions. If misunderstanding, please let me know.\n\nQ1: Could you elaborate on how information gain influences exploration behavior in practice? How sensitive is performance to its weighting relative to distance ($\\alpha_d$)?  \n\nQ2: Does the uncertainty-weighted score ever mislead the agent into revisiting false positives with low variance? Could you visualize failure cases or confidence evolution over time?  \n\nQ3: Have you considered learned policies (e.g., RL or imitation learning) to modulate the exploration–exploitation balance instead of fixed weights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OYWw2CPg39", "forum": "sEKJYLnTpA", "replyto": "sEKJYLnTpA", "signatures": ["ICLR.cc/2026/Conference/Submission2188/Reviewer_JyoA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2188/Reviewer_JyoA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762080942207, "cdate": 1762080942207, "tmdate": 1762916116127, "mdate": 1762916116127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper present HELIOS, which is a hierarchical framework for language-specified mobile manipulation. The motivation of HELIOS is to address how to ground high-level language instructions into low-level visual representations when the environment is partially observable to the robot. It introduces a two-level scene representation for effective object search that integrates:\n- a 2D semantic occupacy map\n- a 3D Gaussian-based object representation for modeling task-relevant entities\n\nThe proposed framework is tested on OVMM benchmark in Habitat and it achieves the state-of-the-art performance compared to the baselines. Moreover, authors also test their method's performance on a Spot Robot of Boston Dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem motivations: object search in partially observable environment is a very important problem in mobile manipulation\n- Solid representation structure: combining 2-D semantic and occupancy map and 3D Gaussian splatting seems very reasonable\n- Balanced exploration and exploitation object search: combining exploration and exploitation objective in search functions is reasonable\n- Real-robot evaluation: authors not only show the validness of their method on simulation, but also validate the effectiveness on real-robot: Spot"}, "weaknesses": {"value": "- Limited novelty and lacking comparisons with existing works: Although it makes sense to combine 2-D semantic and occupancy map with 3D Gaussian splatting representations, all modules, e.g. semantic map, 3D Guassian representations, frontier exploration, uncertainty weighting is all existing techniques. It remains unclear to me what the novelty of the authors besides the combination of existing modules.\n- Limited analysis on efficiency: authors claim the effectiveness of sparse 3DGS, however they lack the quatitative results comparisons of the FLOPS and parameters with baselines.\n- Significantly lacking baseline comparisons: lacks quantitative comparisons with strong baselines. I am not familiar with this field, but I believe the literature is not vacant, I trust other reviewers on judging this."}, "questions": {"value": "- How is the multi-view consistency constructed? Do you use probabilisty models?\n- Have you compare against any open-world manipulation methods?\n- How is language information grounded into the hierarchical representation?\n- Could you provide quantitative results on inference efficiency and memory usage compared to dense scene graphs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tpGdKrhmKh", "forum": "sEKJYLnTpA", "replyto": "sEKJYLnTpA", "signatures": ["ICLR.cc/2026/Conference/Submission2188/Reviewer_Yd22"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2188/Reviewer_Yd22"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762127426963, "cdate": 1762127426963, "tmdate": 1762916115842, "mdate": 1762916115842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors present a hierarchical scene representation with the goal of open-vocabulary mobile manipulation. The presented scene representation is composed of two tasks: the first being a 2D, birds eye view containing semantic, occupancy, and value information, and a 3DGS based volumetric map that keeps track of objects in the scene. The primary novelty in this work is the integration of this confidence aware 3DGS representation that prompts the agent to add more views of the target object or location if there is more uncertainity over the classification. The authors present evaluations of their method in the simulated OVMM benchmark, and compare it with baselines from HomeRobot and MoManipVLA. The authors also present the baselines, where they show that integrating the hierarchical structure and the confidence based methods help in each individual steps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* integrating a 3D layer over the 2D layers for search and object discovery makes the scene representation more robust to multi-view inconsistencies, which is one of the primary cases where object recognition and discovery fails in OVMM.\n* The uncertainty aware 3DGS representation gives a great way to both represent objects in the scene and also create a good way to resolve object detection noise.\n* Improving the scene representation over multiple pick and places gives a good way to learn an online, dynamic representation of the map, which is necessary for the real world."}, "weaknesses": {"value": "* The presented method works with a fixed number of pre-determined class, which belies the \"open vocabulary\" claim. While the authors claimed that we can pre-select a fixed number of classes, that does break the open vocabulary assumption in reality.\n* The real world experiments should be more front-and-center than they are right now. As it is, it doesn't seem to be a big part of the submission where this work would truly shine with them.\n* The paper does not give us enough detail about the local search process, and how it is performed with obstacles in the environment."}, "questions": {"value": "* Why are real world experiments done with monocular depth estimation? If the representations are 3DGS based, should that not be sufficient to figure out a depth representation that works together with it?\n* What are the experimental numbers from the real world trials?\n* What are the primary causes of failure right now? An analysis similar to OK-Robot's waterfall chart would be very helpful to understand the current limitation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OdEVQ48SbC", "forum": "sEKJYLnTpA", "replyto": "sEKJYLnTpA", "signatures": ["ICLR.cc/2026/Conference/Submission2188/Reviewer_gr26"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2188/Reviewer_gr26"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132626209, "cdate": 1762132626209, "tmdate": 1762916115699, "mdate": 1762916115699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents HELIOS, a hierarchical framework designed to address language-guided pick-and-place mobile manipulation tasks in open, partially observed scenes. It integrates 2D semantic/occupancy maps for efficient navigation and sparse 3D Gaussian representations for task-relevant object modeling, alongside a search objective that balances exploration of unobserved regions and exploitation of semantic cues. The work demonstrates performance on the OVMM benchmark (outperforming baselines like HomeRobot and MoManipVLA in limited/potential settings) and zero-shot transfer to a real-world Spot robot, with ablation studies verifying the utility of its hierarchical representation and uncertainty-weighted object scoring."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "One potential strength of the work is its **practical integration of multi-modal scene representation for end-to-end language-guided pick-and-place tasks**, which bridges 2D semantic/occupancy maps (for efficient navigation) and sparse 3D Gaussians (for precise object modeling). Unlike some prior methods that rely solely on 2D maps (e.g., HomeRobot) or dense 3D scene representations (e.g., early 3D Gaussian-based SLAM works), HELIOS tailors its layered structure to the specific needs of mobile manipulation—using 2D maps to prioritize exploration and 3D Gaussians to track task-relevant objects with multi-view consistency. This design, while building on existing components, aligns representation granularity with real-world manipulation demands (e.g., avoiding redundant full-scene 3D modeling) and enables direct zero-shot transfer to physical robots (Spot) without additional data, demonstrating a degree of engineering utility for practical deployment scenarios."}, "weaknesses": {"value": "1. **Weak Motivation Grounding**: The paper fails to clearly pinpoint unaddressed gaps in existing literature. It only lists generic challenges (partial observation, semantic grounding) but does not quantify or exemplify how state-of-the-art methods (e.g., HomeRobot, MoManipVLA) struggle with these issues—for instance, no data on prior methods’ false detection rates or inefficient exploration is provided. This makes the necessity of HELIOS unconvincing, as it does not solve a well-documented, unmet need.  \n\n2. **Lack of Methodological Novelty**: Core components are incremental tweaks of existing techniques rather than original innovations. The 2D semantic value map extends Yokoyama et al. (2023b) with minor multi-target support; the sparse 3D Gaussian object modeling adapts Lu et al. (2024)’s 3D Gaussian framework without new modeling logic; and the “exploration-exploitation balance” relies on classic information gain calculations, with no novel objective function design. There is no theoretical or algorithmic breakthrough to advance the field.  \n\n3. **Insufficient Experimental Validation**: While the paper claims state-of-the-art results on OVMM, key comparisons are incomplete. For example, real-world Spot robot experiments lack side-by-side performance metrics (e.g., task success time, grasp failure rate) with other zero-shot manipulation methods. Additionally, the “place skill” is acknowledged as a major failure source, but no analysis of why HELIOS cannot address this (or how it compares to methods with better placing) is provided, weakening the work’s practical credibility.  \n\n4. **Unconvincing Zero-Shot Transfer Claim**: The paper asserts zero-shot transfer to real-world environments but provides limited details to support this. There is no analysis of how HELIOS handles real-world noise (e.g., variable lighting, occlusions) that differs from simulation, nor is there evidence that its performance in the office setting is superior to or even on par with existing zero-shot manipulation systems. The transferability claim thus remains unsubstantiated."}, "questions": {"value": "1. The paper mentions that HELIOS uses sparse 3D Gaussians to model task-relevant objects instead of dense full-scene 3D representations. However, it does not clarify: how does the framework **dynamically define \"task-relevant objects\"** from ambiguous language instructions (e.g., instructions like \"fetch the small item on the shelf\")? Is there a risk of missing key objects due to over-sparsification, and how is this risk mitigated?  \n\n2. In the real-world Spot robot experiments, the paper claims \"zero-shot transfer\" but lacks comparative data with SOTA zero-shot manipulation methods (e.g., HomeRobot’s real-world variants). Could you provide **quantitative metrics** (e.g., task success rate, average execution time, grasp failure rate) that directly compare HELIOS with these methods in the same office environment?  \n\n3. The discussion notes that HELIOS occasionally fails at the \"place\" step due to improper gripper angle adjustment. Unlike the \"pick\" step (supported by multi-view 3D Gaussian verification), the paper does not explain: why is there no similar **3D geometric verification mechanism** for the \"place\" target (e.g., checking if the object aligns with the placement surface)? Is this a deliberate design choice, or a limitation to be addressed?  \n\n4. The 2D semantic value map in HELIOS extends Yokoyama et al. (2023b) to multi-target scenarios. However, it is unclear: when multiple targets overlap in the 2D map (e.g., two cups on a table viewed from above), how does the framework **resolve spatial ambiguity** to avoid guiding the robot to the wrong target? What modifications were made to the original single-target value map logic to handle this?  \n\n5. The paper states that HELIOS balances exploration and exploitation via \"information gain\" calculations. But in cluttered environments (e.g., a desk with many small objects), the \"information gain\" of unobserved regions may be low. How does the framework **prevent stagnation** (e.g., the robot repeatedly checking the same high-probability area without exploring new regions) in such cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MICbGcPEAH", "forum": "sEKJYLnTpA", "replyto": "sEKJYLnTpA", "signatures": ["ICLR.cc/2026/Conference/Submission2188/Reviewer_9iMV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2188/Reviewer_9iMV"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission2188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762166845884, "cdate": 1762166845884, "tmdate": 1762916115452, "mdate": 1762916115452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses open-vocabulary mobile manipulation in partially observable indoor environments. The authors propose HELIOS, a hierarchical exploration system that integrates navigation, perception, and manipulation through a three-layer scene representation. The first layer is a 2D occupancy map for navigation, the second is a semantic value map for prioritizing frontiers, and the third is a sparse, task-relevant 3D Gaussian representation that maintains Dirichlet statistics for language-grounded semantic consistency. The key contribution is a unified global objective function that balances exploration (visiting previously unseen frontiers) and exploitation (re-observing previously detected objects to reduce uncertainty), utilizing semantic scores, information gain, and distance penalties. The method is evaluated on the Habitat OVMM benchmark and demonstrates improved success rates in finding and manipulating objects compared to baseline systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper tackles a real gap in open-vocabulary mobile manipulation — the high failure rate in locating objects or placing them correctly due to unstable open-vocabulary perception. By explicitly modeling multi-view semantic consistency, the method addresses this critical bottleneck. \n2.\tThe hierarchical representation design is coherent: metric navigation, semantic reasoning, and object-level language understanding are neatly decoupled yet connected through a unified planning objective. This makes the system modular and interpretable. The use of information gain and uncertainty reduction in the unified objective provides a principled mechanism for switching between exploration and local observation, rather than relying on heuristic thresholds or stage-wise rules. \n3.\tThe experiments follow a standard benchmark (HomeRobot/OVMM) with clear stage metrics (FindObj, Pick, FindRec, Place, SR), demonstrating reproducibility and measurable improvements."}, "weaknesses": {"value": "1.\tThe claimed novelty overlaps with recent work from 2024–2025 on language-embedded 3D Gaussian representations and task-driven active exploration. In particular, ATLAS Navigator (2025) already integrates language-conditioned 3D Gaussians with information gain–based viewpoint selection for task-driven exploration. The current paper should more clearly delineate how its global objective and sparse Gaussian representation differ from ATLAS and similar approaches.\n2.\tRecent methods such as Splat-MOVER, GaussianGrasper, and POGS already apply object-level Gaussian representations for open-vocabulary manipulation. The present paper’s claim that it is the first to use online, task-relevant Gaussian maps for manipulation is therefore too strong. A quantitative or qualitative comparison to these systems would strengthen the paper’s contribution statement. \n3.\tThe proposed “frontier vs. re-observation” trade-off has been previously explored in hierarchical semantic navigation papers (e.g., “Uncertainty-Aware Embodied Exploration,” 2024). The paper should emphasize that its novelty lies in embedding both behaviors into a single scalar objective, rather than fixed scheduling, and provide ablation evidence to support this claim.\n4.\tWhile the results show performance gains, it remains unclear how much of the improvement is due to the 3D Gaussian consistency versus the global objective design. Further analysis isolating these effects would clarify the main source of success."}, "questions": {"value": "1.\tHow exactly does your unified global objective differ mathematically from ATLAS Navigator’s task-driven information-gain formulation? \n2.\tIn the Dirichlet update of object semantics, how do you avoid bias toward frequently observed (but incorrect) objects, e.g., normalizing by observation count or adding temporal decay?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z47W40LKmB", "forum": "sEKJYLnTpA", "replyto": "sEKJYLnTpA", "signatures": ["ICLR.cc/2026/Conference/Submission2188/Reviewer_5ipo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2188/Reviewer_5ipo"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission2188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762757282782, "cdate": 1762757282782, "tmdate": 1762916113942, "mdate": 1762916113942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}