{"id": "A0D9jUo8gm", "number": 11364, "cdate": 1758197491814, "mdate": 1763122359840, "content": {"title": "Leveraging Class Similarity for Enhanced Conformal Prediction", "abstract": "Conformal Prediction (CP) has emerged as a powerful statistical framework for high-stakes classification applications. Instead of predicting a single class, CP generates a prediction set, guaranteed to include the true label with a pre-specified probability. In setups where the classes can be partitioned into semantic groups, e.g., diseases that require similar treatment, users can benefit from prediction sets that are not only small on average, but also contain a small number of semantically different groups. This paper begins by addressing this problem and then extends far beyond it. First, given a class partition, we propose augmenting the CP score function with a term that penalizes predictions with \"out-of-group\" errors. We theoretically analyze this strategy and prove its advantages for group-related metrics. Surprisingly, we show mathematically that, for common class partitions, it can also reduce the average set size of any CP score function. Our analysis reveals the class similarity factors behind this improvement and motivates us to propose a model-specific variant, which does not require any human semantic partition and can further reduce the prediction set size. Finally, we present an extensive empirical study, encompassing prominent CP methods, multiple models, and several datasets, which demonstrates that our class-similarity-based approach consistently enhances CP methods, making it a widely applicable tool in the CP toolbox.", "tldr": "", "keywords": ["Conformal prediction", "classification", "class similarity"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/8cc7189ed9d88cef0fc7881f348669c433e60b72.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new framework for improving Conformal Prediction (CP) in classification tasks by incorporating class similarity information into the CP score function. Traditional CP guarantees marginal coverage but does not account for the semantic coherence of labels within the prediction set. Thus, the authors propose Model-Agnostic Class Similarity (MA-CS)  and Model-Specific Class Similarity (MS-CS)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper goals to ensure semantic coherence in CP prediction sets, which is a real gap in CP's applicability to high-stakes domains. \n\n2. MS-CS moves from human-defined partitions to model-driven class similarity is creative. \n\n3. The experiments include three datasets (CIFAR-100, Living-17, Mini-ImageNet) and three score functions (LAC, RAPS, SAPS)."}, "weaknesses": {"value": "1. For the MA-CS variant, performance may depend heavily on the quality of predefined groupings and similarity measurement functions. Besides, the \"quality\" of similariy also depends on the performance of the model.\n\nSimilarly, both methods rely on a regularization parameter $\\lambda$. How sensitive are the results to the choice of $\\lambda$? How does cosine similarity compare (empirically or theoretically) to other kernel-based similarities?\n\n2. The theoretical results assume *well-calibrated softmax outputs* and *consistent class groupings*. \nIt would be better to include experiments or analysis under unexpected cases, such as noisy labels, unreliable (predefined or learned) group partitions, or poor model performance.\n\nHow does performance degrade if the predefined groups are noisy or partially incorrect?\n\n3. Some theoretical sections (especially Theorem 4.5 and its discussion) are dense and could benefit from additional intuition or graphical illustrations.\n\n4. Could the framework generalize to other data formats, such as text or tabular? If yes, it is recommended to show more results.\nFurthermore, if it fits structured prediction or multi-label settings where class relationships are hierarchical or overlapping?\n\n5. The writing/presentation is hard to follow, making the manuscript \"incomplete\"."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "aPryrFZ2OE", "forum": "A0D9jUo8gm", "replyto": "A0D9jUo8gm", "signatures": ["ICLR.cc/2026/Conference/Submission11364/Reviewer_c9AR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11364/Reviewer_c9AR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761292874884, "cdate": 1761292874884, "tmdate": 1762922494028, "mdate": 1762922494028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely thank the reviewers for the time and effort they dedicated to evaluating our submission. After careful consideration, we have decided to withdraw the paper.\n\nWe feel that our practical and theoretical contributions were not communicated as clearly as we intended. For example, our work demonstrates the ability to improve both prediction-set sizes and the number of superclasses by tuning a single hyperparameter in just a few minutes using only a small portion of the data and our theory explains it. Additionally, we highlight the consistent improvement of the LAC algorithm, which, to the best of our knowledge, has not been reported previously.\n\nNevertheless, we greatly appreciate the reviewers’ feedback and will carefully consider it as we work to strengthen the clarity, quality, and impact of our work in a future submission."}}, "id": "9eo1NbGlmC", "forum": "A0D9jUo8gm", "replyto": "A0D9jUo8gm", "signatures": ["ICLR.cc/2026/Conference/Submission11364/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11364/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763122298219, "cdate": 1763122298219, "tmdate": 1763122298219, "mdate": 1763122298219, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes two new conformal score functions (more precisely, two ways of modifying any existing score function) that achieve two goals. The primary goal is to promote semantic similarity of labels in the resulting prediction set (e.g., classes that belong to the same group). The secondary goal is to reduce the average set size.  The proposed score functions are obtained by combining an existing score function with an additive penalty on the distance $d$ between the input $y$ and the model’s top-1 prediction $\\hat{y}$. The first score function sets $d(y, \\hat{y})$ to be the indicator for whether $y$ and $\\hat{y}$ are in the same group (where groups are pre-defined). The second score function sets $d(y, \\hat{y})$ to be the distance between the centroid of embedded examples of class $y$ and embedded examples of $\\hat{y}$, where the embedding is the output of the penultimate layer of the neural network (before the regression head)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is generally well written.\n* The idea of using the NN’s learned features to measure class similarity (as described in Sec 5) is nice."}, "weaknesses": {"value": "* Unclear practical relevance. There are two reasons for this: \n    * First, I am not convinced by the problem motivation (as described in the second paragraph of the introduction). Ambiguity can be useful for determining when further investigation or caution is needed. In the autonomous driving example, let’s consider a rare event that requires an uncommon response: the car is currently going at high speed, and a pedestrian has stepped out into the road in front of the car. In this case, we want the car to honk, so the pedestrian can get out of the way. However, using your sets, if the model’s top-1 prediction is not “pedestrian crossing”, then your method makes it so the prediction set will not contain “pedestrian crossing” in most cases. \n    * Second, the empirical results are a bit weak, in my reading. Standard with LAC is a strong baseline, and differences between the proposed methods and this baseline do not generally seem to be statistically significant. \n* No consideration of class-conditional coverage (see Question below)\n* The theory is not particularly surprising\n* Assumption 1 is not stated very clearly. It would be useful to write out the mathematical definition of the “statistical quantile.” Does this just mean that we replace the score threshold with the population quantile?"}, "questions": {"value": "* How do MA-CS and MS-CS impact class-conditional coverage? Can you compute the average class-conditional coverage gap of the baselines and your methods? \n* How does this paper relate to https://arxiv.org/pdf/2410.01767 (Cortes-Gomez et al., 2025)? The setting in that paper seems quite related. \n* On line 418 there is a comment about how one of the baselines and one of the proposed methods cannot be applied to Mini-ImageNet due to the lack of a superclass structure. Why can the WordNet hierarchy that exists for full ImageNet not be used here? Moreover, why are the experiments run on Mini-ImageNet instead of the full ImageNet? All of the experiments are run datasets with roughly 100 classes or less. Would it not work on the 1000 classes in full ImageNet? \n* The problem the paper solves is “how do we encourage prediction sets to contain labels from a single semantic group?” and not “how do we encourage prediction sets to contain labels from as few semantic groups as possible?” With strong classifiers (with, e.g., 80% top-1 accuracy, as is the case in the experiments), these problems are effectively equivalent. But in general, these problems are not the same. The score function $s_{\\lambda}(x,y) = s(x,y) + \\lambda d(y, \\hat y(x))$ (Eq. 2) does not capture the fact that if we want to solve the second question, once we decide to include a an out-group class, it is better to include classes from the same group as that class than a class from a not-yet-included class. \n    * First, a suggestion: It could be useful to highlight the difference between these two problems. Currently, the language in the abstract (“users can benefit from prediction sets … [that] contain a small number of semantically different groups”) led me to initially believe that the paper would address the second problem\n    * Second, a question: Do you have ideas about how your approach can be adapted to solve the second problem? \n* Sadinle et al, 2019 says Standard CP with the LAC score approximates the theoretically optimal smallest sets. How should readers interpret your Theorem 4.5 in the context of this existing optimal size result? \n\nSmall comment:\n* Line 167: “semantical” -> semantic"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ookO5uvgYI", "forum": "A0D9jUo8gm", "replyto": "A0D9jUo8gm", "signatures": ["ICLR.cc/2026/Conference/Submission11364/Reviewer_JwSg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11364/Reviewer_JwSg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761685214831, "cdate": 1761685214831, "tmdate": 1762922493644, "mdate": 1762922493644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper modifies the score function by adding a label similarity."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The empirical result is good."}, "weaknesses": {"value": "1. The contribution is very mild. Adding a similarity to the score function is clearly not enough for publication at ICLR. I don’t have much more to say. It is a clear rejection for me. \n\n2. If the similarity is helpful for prediction, it shouldn’t only appear in the conformal prediction set size; it should appear in other accuracy measurements as well. Even if the similarity is helpful for classification, this paper makes no contribution to conformal prediction. Adding a useful measure to the score function can clearly help improve conformal prediction results."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qcyBsVEopb", "forum": "A0D9jUo8gm", "replyto": "A0D9jUo8gm", "signatures": ["ICLR.cc/2026/Conference/Submission11364/Reviewer_xAqH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11364/Reviewer_xAqH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761693891531, "cdate": 1761693891531, "tmdate": 1762922493315, "mdate": 1762922493315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a regularization framework for Conformal Prediction (CP) that leverages class similarity to improve the efficiency and semantic similarity of prediction sets. In the model-agnostic version (MA-CS), a binary distance function penalizes classes belonging to different predefined groups, preserving exchangeability and theoretical coverage guarantees while reducing average set size. The model-specific variant (MS-CS) replaces the predefined grouping with a similarity measure derived from the cosine similarity of centered class mean feature embeddings of a NNs (no need for human-defined groups). Experiments on four image datasets and two ResNet models show that both variants achieve the target coverage (with minor variability) while producing smaller and more coherent prediction sets than regularization baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is clearly organized and effectively motivates the problem. It addresses the lack of semantic coherence in conformal prediction sets, enhancing their practical usefulness for downstream tasks such as healthcare or autonomous systems. \n\n* The proposed regularization approach is conceptually simple, easy to implement, and can be integrated with existing nonconformity scores and CP frameworks. \n\n* For the model-agnostic variant (MA-CS), the authors provide a formal analysis demonstrating that coverage is preserved and the average set size can be reduced. The extension to model specific class similarity sounds reasonable and well-motivated. \n\n* The experimental results are promising for both MA-CS and MS-CS, showing improvements in comparison with other regularization strategies across four image datasets and two network architectures."}, "weaknesses": {"value": "1. The model-specific variant (MS-CS), while well motivated, lacks a more formal analysis. It remains unclear what the cosine-based similarity matrix is truly capturing and how it aligns with meaningful semantic concepts/groupings. An examination of these learned relationships would provide valuable insight.\n\n2. The method’s performance depends on the regularization weight λ but the paper offers limited discussion on its robustness. Since λ is chosen based on splitting the calibration set (Paragraph in L408), it would be useful to analyze how these choices affects the variance of the estimated quantile and the resulting coverage. \n\n3. It is not discussed how the approach behaves under different target error rates (values of α other than 0.1). \n\n4. Although the empirical improvements are consistent, they can be modest in some settings, raising questions about when and why the method yields the largest benefits. Is there some general characteristic that we can identify here? Under which conditions do this approach make a difference?\n\n5. Experiments are restricted to image classification with resnet models; evaluating the approach on other modalities (text, tabular data) would better support claims of generality from an empirical point of view."}, "questions": {"value": "Please, see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KqzBwDLIxl", "forum": "A0D9jUo8gm", "replyto": "A0D9jUo8gm", "signatures": ["ICLR.cc/2026/Conference/Submission11364/Reviewer_JRi9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11364/Reviewer_JRi9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928325618, "cdate": 1761928325618, "tmdate": 1762922492891, "mdate": 1762922492891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}