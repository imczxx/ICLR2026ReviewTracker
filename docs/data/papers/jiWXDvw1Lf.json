{"id": "jiWXDvw1Lf", "number": 10170, "cdate": 1758162955901, "mdate": 1759897669624, "content": {"title": "Action Chunking and Data Augmentation Yield Exponential Improvements for Imitation Learning in Continuous Spaces", "abstract": "This paper presents a theoretical analysis of two of the most impactful interventions in modern learning from demonstration in robotics and continuous control: the practice of *action-chunking* (predicting and executing sequences of actions in open-loop) and the use of *data-augmentation* of expert demonstrations. Though recent results show that learning from demonstration, also known as imitation learning (IL), in continuous settings can suffer errors that scale *exponentially* with task horizon, we demonstrate that action chunking and data-augmentation can mitigate these effects in different regimes. Our results identify control-theoretic stability as the key mechanism underlying the benefits of these interventions. On the empirical side, we validate our predictions and the role of control-theoretic stability through careful experimentation on popular robot learning benchmarks. On the theoretical side, we demonstrate that the control-theoretic lens leads to tighter statistical guarantees on imitation learning error when these interventions are applied than previous techniques based on information-theoretic considerations alone.", "tldr": "Imitation learning in continuous spaces is more nuanced than over discrete spaces, and can suffer exponential-in-horizon errors. We distill core mechanisms from popular approaches to provably and demonstrably mitigate these errors.", "keywords": ["Imitation learning", "compounding errors", "distribution shift", "control theory", "dynamical systems", "robotics", "action chunking", "noise injection"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a6d299bec95d4cd3ef1a7ba065027f93dad7d87.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The main point of the work seems to be that predicting a suitable sequence - chunk -  of actions ( 2, 4, 8, ...) instead of a single action can be beneficial for imitation learning of certain tasks. The same finding holds for adding noise to training data. This are related to the compounding error problem of offline learning (small deviations in the real world accumulate). In online learning these problems are solved by using the current policy in sampling new data (DaGGER). \n\nI believe these findings are somewhat known in the scientific community. The authors claim to derive \"theoretical guarantees\" (lines 46-47), but their writing is so difficult to follow that I cannot understand what the authors mean by those. Moreover, Sections 5 and 6 do not anymore discuss about the theoretical guarantees and their consequences which makes it confusing what really are the main contributions of this work."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Action chunking is still a rather new methodology in imitation learning and therefore makes it an interesting topic to study and investigate its properties and reasons why it is sometimes beneficial."}, "weaknesses": {"value": "**Major weaknesses:**\n\n - This paper writing style is very complicated and therefore I cannot really follow what are the main research questions, contributions, and the main findings. The authors should simplify the text and clarify the contributions. In this form I cannot accept the work.\n\n**Moderate weaknesses:**\n\n - The main contribution is rather complicated to be easily understandable: \"We provide the first theoretical guarantees in continuous state-action IL for interventions that provably prevent compounding error without iterative expert feedback.\" This contribution leads to *two practices* that result two \"surprising\" takeaways. So contribution is limited to practices that result to surprising takeaways. Could this line of thinking be simplified to something easier to understand and follow?\n\n - Text is partially difficult to follow (perhaps too extensive use of AI language correction). For example, the second paragraph of Introduction: first sentence is 5 plus lines and the last sentence is more than 4 lines. Such monster sentences appear all over the manuscript.\n\n - Usage of figures is not good. For example, Figure 1 on the page 3 is not referred until page 9. Figures should flow with the text and not be randomly around the manuscript.\n\n - Main findings from the experimental part (page 9) are difficult to comprehend. For example, in Figure 2 it seems that prediction horizon and action chunk 16 result to the best result (should not come as surprise that the both are them same). By adding noise, some of the prediction horizons improve (e.g. 8), but on the other hand the best performing 16 performance degrades. So the effects of noise and chunking are not complementary always. [Btw. \"left\" and \"right\" in a figure with three images is confusing, why not (a), (b), and (c)? Moreover, marking 2³ instead of 8 is odd until the purpose really is to confuse the reader from the obvious]\n\n**Minor weaknesses:**\n\n - Citations should be updated. Some of them are listed as ArXiv papers despite that the paper have already been published in a conference or journal (e.g. Chi et al. 2023)\n\n - Use of colors to emphasize parts of the text is already a bit exhaustive. On page two there are normal text, bold text, blue text, and red text."}, "questions": {"value": "If the authors can restate their research question(s), main contributions, and findings in more concise manner"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qDr66x2h9U", "forum": "jiWXDvw1Lf", "replyto": "jiWXDvw1Lf", "signatures": ["ICLR.cc/2026/Conference/Submission10170/Reviewer_Y1gT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10170/Reviewer_Y1gT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571309368, "cdate": 1761571309368, "tmdate": 1762921537871, "mdate": 1762921537871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides theoretical justification for two major ways that people have done imitation learning in robotics: action chunking (where an agent chooses K > 1 actions in parallel to execute) and data augmentation (where one can learn a generative model to generate more useful synthetic expert data to imitate), particularly in a state-based continuous control setting with deterministic dynamics.\n\nTheir theory relies on the notion of EISS, in which perturbations to actions and next states have a bounded effect over future state distributions under the dynamics across time. In particular, there is a clear upper bound on the deviation between states far away in time, depending on the initial perturbation amount. This, combined with Lipschitzness of the policy class, yield upper bounds for compunding error that scale polynomially with respect to the Lipschitz coefficient of the policy and the EISS coefficient of the dynamics. Furthermore, their theory shows that one can achieve a strong compounding error bound with a chunked policy, where the compounding error coefficient is independent of the horizon of the problem, which is a strong result.\n\nIn the case where the dynamics are not open-loop EISS, then the authors propose noise injection, where one injects uniform noise to the actions selected by the learner at each step. In this particular case, the imitation learning procedure is to then imitate both over clean expert trajectories and \"noised\" expert trajectories."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I am not a theoretical expert nor a control theory expert, but it seems that the theory is decently sound, at least from the control theory perspective. This was corroborated by looking into Simchowitz et. al. (2025), the closest work to this.\n\nThe content is also quite novel, as no one (at least in robotics) has looking into an analysis of data augmentation and action chunking (aside from Simchowitz et al. (2025) partly), as it seemed to be more of a practical trick than a deeply analyzed method. Moreover, the authors analyze different components of learning that of Simchowitz et. al. (2025), notably the policy parameterization (chunking) and data collection (e.g. noisy expert), different from the learning algorithm perspective that the prior paper takes.\n\nThe empirical results were also quite compelling, and showed that the combination of both chunking horizon (e.g. Figure 2, middle & right) and noising of both actions (e.g. Figure 2, right) and trajectory data (e.g. Figure 3) yield improvements in imitation performance beyond that of standard BC."}, "weaknesses": {"value": "While I am not a control theory person, it was quite difficult for me to follow along with the theory in this work. For instance, I didn't know what the difference between \"closed-loop\" and \"open-loop\" was, particularly when it came to defining the EISS notion (e.g. Theorem A). As such, while I was able to semi-follow the theory on action chunking reasonably well, I still had some questions regarding definitions. The same went for the noising analysis, which I found difficult to read through. I think that presentation could be fixed there, and that would clear things up for anyone who reads the paper, theoretician or not.\n\nThere is a typo in Eqn. (2.2) -- need a T in the subscript on the LHS. Small nit."}, "questions": {"value": "See my thoughts in the Weaknesses section -- I feel like this is more of a misunderstanding than anything."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ni7DiMGD2R", "forum": "jiWXDvw1Lf", "replyto": "jiWXDvw1Lf", "signatures": ["ICLR.cc/2026/Conference/Submission10170/Reviewer_zW76"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10170/Reviewer_zW76"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959887653, "cdate": 1761959887653, "tmdate": 1762921537506, "mdate": 1762921537506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper gives a novel and rigorous analysis connecting two widely-used heuristics in continuous imitation learning, (1) action-chunking and (2) expert action noise-injection, to control-theoretic stability and compounding-error guarantees. \nUnder the notion of an open-loop exponential incremental input-to-state stability (EISS) assumption on the true dynamics, \nthe authors prove that sufficiently long action chunks yield horizon-free trajectory error bounds (Theorem 1). \nWhen the true dynamics are not open-loop EISS, they advocate Practice 2: collect a mixture $P_{\\pi^\\star,\\sigma_u,\\alpha}=\\alpha P_{\\pi^\\star}+(1-\\alpha)P_{\\pi^\\star,\\sigma_u}$ (noise injected at execution but clean action labels).\nThis paper also show that this mixture certifies first-order matching on the controllable subspace, producing a trajectory bound that removes the worst-case exponential blowup (Theorem 2). \nEmpirical validation on robomimic and MuJoCo tasks supports the theory and compares the offline noise-injection approach to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "**1. A novel control-theoretic perspective on imitation learning.**\n\nThe paper reframes two widely used practical heuristics (1) **action-chunking** and (2) **expert action noise-injection,** in control-theoretic perspectives, centered on exponential incremental input-to-state stability (EISS). \nBy reducing compounding error to a stability property of the true dynamics $f$ and the policy–environment closed loop, the work gives precise conditions under which each heuristic is principled. \nThis perspective clarifies when a practitioner should prefer chunking (longer chunks $\\ell$ when $f$ is open-loop EISS) versus noisy expert rollouts, and it interestingly connects IL practice to established stability concepts rather than treating the heuristics as ad-hoc.\n\n**2. Mutually aligned theoretical and experimental results**\n\nTheoretical predictions, (i) action-chunking yields horizon-free trajectory error under open-loop EISS, and (ii) noise-injection facilitates first-order matching in controllable subspaces when open-loop stability fails—are supported by empirical results on Robomimic and MuJoCo tasks."}, "weaknesses": {"value": "**1. Narrow applicability of action-chunking under EISS assumption**\n\nThe primary theoretical guarantee for action-chunking (Theorem 1 / Propositions 3.1, 3.2) is stated under the strong assumption that the **true dynamics $f$ are open-loop EISS**. \nThis crucial scope condition is only briefly noted in the text, but the paper lacks a clear characterization of the method’s behavior when that assumption is violated. \nTo be more useful to practitioners and convincing to readers, the manuscript should clarify the conditions where chunking is recommended (e.g., global vs. local EISS), show how the bound degrades when stability weakens, and provide either a weakened theoretical statement for local/probabilistic EISS or empirical sweeps that identify where chunking helps versus hurts. \n\n**2. Limited empirical scope and reproducibility concerns**\n\nThe experimental validation is limited to a small set of tasks, which makes it difficult to assess the generality of the claims. In addition, the authors have not released the code or full data-collection scripts, which prevents independent reproduction and detailed ablation.\nThe paper also omits results and practical guidelines for applying the two practices jointly; reporting combined-case experiments and decision rules would be valuable.\n\n**3. Overbroad use of the term “data augmentation”**\n\nThe use of the generic term **data augmentation** in the title and abstract overstates the scope of the empirical/theoretical contribution: the manuscript studies a specific augmentation (additive action noise executed during expert rollouts with *clean* action labels) rather than arbitrary augmentation families (state perturbations, visual augmentations, domain randomization, etc.). This wording risks misleading readers and reviewers. To avoid overclaiming, replace or qualify “data augmentation” in the title/abstract/introduction with a precise phrase (e.g., “expert action noise-injection”, etc.), and if broader augmentation types are intended, either provide evidence for them or explicitly mark such extensions as future work.\n\n**Minor comments** \n\n- The paper’s presentation is a quite dense and often assumes control-theory familiarity that may not be shared by the broader RL/IL audience (who is more common ICLR reader); this reduces accessibility and reproducibility. \nAdditionally, Figures 1,2,3, whose experiments are discussed on page 9, are currently shown much earlier (pages 5 and 7) and therefore disrupt the narrative; these figures should be relocated.\n\n- In Theorem A, $\\mathcal{P}^\\star_{\\mathrm{stab}}$ \nand $\\mathcal{P}^\\star_{\\mathrm{unst}}$ are defined in a circular way, which does not seem a proper definition."}, "questions": {"value": "Q1. Could the authors recommend a practical criterion (estimable stability indicator, threshold) to decide when to apply action-chunking?\n\nQ2. Could you justify the specific choice $\\alpha=0.5$ used in Proposition 4.4 and Theorem 2?\n\nQ3. Line 463 states “Practice 2 collects data in one shot, without ever observing learned policy rollouts.” By “one shot” do you mean a single interaction of many trajectories (not literally one trajectory)?\n\nQ4. Beyond the offline protocol, Practice 2 could plausibly be applied interactively (i.e., inject noise into expert actions when the expert is queried during training, similar to DAgger-style interaction). Do the authors see this interactive variant as theoretically or practically different from the offline mixture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CxJWBfrlyh", "forum": "jiWXDvw1Lf", "replyto": "jiWXDvw1Lf", "signatures": ["ICLR.cc/2026/Conference/Submission10170/Reviewer_Xggu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10170/Reviewer_Xggu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762080654467, "cdate": 1762080654467, "tmdate": 1762921537081, "mdate": 1762921537081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}