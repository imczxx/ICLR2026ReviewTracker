{"id": "zqOCacBD3P", "number": 19724, "cdate": 1758298869449, "mdate": 1759897023023, "content": {"title": "Prompt Curriculum Learning for Efficient LLM Post-Training", "abstract": "Reinforcement learning (RL) is widely used to post-train large language models for tasks such as mathematical reasoning and coding. However, the convergence of RL training remains sensitive to batching and prompt selection strategies. We investigate the factors that affect convergence, including batch size and prompt difficulty. Through large-scale experiments across multiple models and datasets, we show that there exists an optimal batch size that balances generation time and gradient quality, and that prompts of intermediate difficulty (where the model has roughly a 50\\% chance of success) are the most sample-efficient for model convergence. Motivated by these findings, we propose Prompt Curriculum Learning (PCL), a lightweight algorithm that selects intermediate-difficulty prompts using a learned value model. PCL avoids costly rollouts and efficiently guides training by focusing on the most informative samples. Empirically, PCL either achieves the highest performance or requires significantly less training time to reach comparable performance across a suite of benchmarks. Compared to using rollouts to filter, PCL is $12.1\\times$ and $16.9\\times$ faster on identifying intermediate-difficulty prompts when training on MATH and DeepScaleR respectively.", "tldr": "", "keywords": ["reinforcement learning", "large language models", "post-training", "curriculum learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec250867c41f033c8822043f94555ae7634dd8aa.pdf", "supplementary_material": "/attachment/5e21ba8fad38552a793d2cc1f15fa30596cb6662.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates how batching and prompt selection affect reinforcement learning post-training of large language models, identifying two key empirical findings: an optimal total batch size where rollout time scaling transitions from sublinear to linear, and the highest learning efficiency when prompts have intermediate difficulty (success probability ≈ 0.5). Building on these insights, the authors propose Prompt Curriculum Learning (PCL), which trains a lightweight value model to estimate per-prompt reward and selects prompts near a target difficulty without costly on-policy rollouts. Across several math reasoning benchmarks (MATH, Olympiad-Bench, AIME, etc.) and models (Qwen3, Llama3.2), PCL matches or surpasses other post-training baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper focuses on a practically important yet underexplored question, how batching and prompt difficulty affect the efficiency of RL post-training for large language models, and grounds its approach in two clear empirical observations.\n\n2. Through systematic experiments, the study identifies an optimal total batch size and shows that prompts of intermediate difficulty yield the most effective learning signal, offering useful insights for designing efficient RL fine-tuning pipelines.\n\n3. The method is validated across several models and math reasoning benchmarks with comprehensive ablations, demonstrating consistent performance trends and providing a transparent analysis of efficiency, stability, and cost trade-offs."}, "weaknesses": {"value": "1. The effectiveness of PCL may heavily rely on the accuracy of the value model. In more complex settings, a small or undertrained value model may fail to provide reliable estimates, leading to suboptimal prompt selection and degraded performance.\n\n2. Concentrating too much on p(x)≈0.5 (especially with large n, small m) lowers prompt diversity, which can reduce robustness and even degrade accuracy beyond some point.\n\n3. The experiments focus almost entirely on math reasoning tasks, leaving it unclear whether the findings on optimal batch size and prompt difficulty generalize to more diverse domains such as code generation or instruction following.\n\n4. If the average reward over the dataset is far from 0.5 (label imbalance), filtering near 0.5 may implicitly rebalance, which can be good or bad depending on goals; it’s another place PCL’s assumptions can break."}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rM7o4evm3E", "forum": "zqOCacBD3P", "replyto": "zqOCacBD3P", "signatures": ["ICLR.cc/2026/Conference/Submission19724/Reviewer_BGex"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19724/Reviewer_BGex"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015945973, "cdate": 1762015945973, "tmdate": 1762931563242, "mdate": 1762931563242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how prompt difficulty and batch configuration affect the efficiency of RL post-training for LLMs. The authors show empirically that there exists an optimal batch size that balances generation time and gradient quality, and that prompts of intermediate difficulty (where the model has roughly a 50% chance of success) are the most sample-efficient for model convergence. They propose Prompt Curriculum Learning (PCL), which trains a value model V(x) to estimate expected reward and select prompts closest to a target threshold $\\tau = 0.5$, enabling efficient on-policy filtering without expensive rollouts. PCL is shown to either exceed the performance of baseline methods or require less time to train for a similar performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Advances the efficiency of LLM post-training based on interesting empirical findings about the batch size and the intermediate difficulty of prompts, which are supported by broad ablations across models, datasets, and a significant spend of resources.\n* The value network to select prompts is lightweight and easy to adopt.\n* The experimental section is well presented and results show the method reaches highest accuracy, or reaches baseline accuracy significantly faster."}, "weaknesses": {"value": "* The method assumes that the dataset always contains a sufficient number of prompts with approximately 50% success probability. However, during early training, there may be very few intermediate-difficulty prompts, which could hinder learning and cause training to stagnate, as the model would not encounter harder examples. Similarly, toward the end of training, if the model improves rapidly, only a small subset of prompts may continue to be selected, potentially biasing the training distribution. The paper does not appear to analyze or address these edge cases, which could affect the robustness of the proposed curriculum.\n* Filtering may introduce distributional bias by over-representing specific mathematical subtypes while under-exposing the model to both foundational easy patterns and extremely difficult reasoning structures. This could lead to topic drift or a collapse toward narrower reasoning styles, as the curriculum disproportionately focuses on intermediate-difficulty prompts. The paper does not provide an analysis of how filtering affects topic coverage or reasoning diversity over training.\n* The lack of variance reporting weakens the paper's claims of robustness, as the absence of standard deviation metrics makes it difficult to assess the statistical significance of the reported improvements. Moreover, the paper does not present results across multiple random seeds for the final evaluations, leaving open the possibility that the observed gains may be sensitive to initialization or sampling noise."}, "questions": {"value": "1. What is the empirical distribution of prompt difficulty $\\pi(x)$ across the full dataset for the initial policy and throughout training? Without this, it is unclear whether intermediate-difficulty prompts are abundant, rare, or emergent as a function of policy improvement.\n2. If intermediate prompts are scarce early on, is learning bottlenecked?\n3. Did you evaluate selecting a band of difficulties (e.g., $\\tau \\pm \\epsilon$) such as 0.2–0.8 or 0.4–0.6, instead of a point target? I think this may alleviate the scarcity issue by increasing sample diversity while still maintaining meaningful gradient feedback."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tfj3w9Trwi", "forum": "zqOCacBD3P", "replyto": "zqOCacBD3P", "signatures": ["ICLR.cc/2026/Conference/Submission19724/Reviewer_SyDP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19724/Reviewer_SyDP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045851955, "cdate": 1762045851955, "tmdate": 1762931562643, "mdate": 1762931562643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates strategies for efficient post-training of large language models (LLMs) using reinforcement learning (RL). The authors first conduct a systematic study on the effects of batching and prompt difficulty on RL convergence. They identify two key findings: (1) an optimal batch size exists at the transition point from sublinear to linear growth in generation time, and (2) prompts of intermediate difficulty, where the model has approximately a 50% success rate, are the most sample-efficient for training. Based on these insights, the paper proposes Prompt Curriculum Learning (PCL), a lightweight algorithm that uses a value model to efficiently select intermediate-difficulty prompts. This avoids the high computational cost of rollout-based filtering. Experiments on mathematical reasoning benchmarks (MATH and DeepScaleR) show that PCL achieves competitive performance with significantly less training time compared to several baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. To my knowledge, this is the first work to systematically analyze the optimal batch size and the optimal decomposition into the number of prompts and generations per prompt.\n2. Using an additional value model for difficulty estimation before rollout is an interesting idea.\n3. The paper is well-written and easy to understand.\n4. The authors have conducted comprehensive empirical evaluation to support the findings on Math datasets."}, "weaknesses": {"value": "1. The main weakness I see is that in many cases, the performance gaps between PCL and GRPO is less than 1%, thus the significance of conducting active curriculum learning is somewhat limited in these settings. And the experimental evaluation is too narrow, as it relies solely on mathematical datasets. I suggest trying to highlight the effectiveness and test the generalization of PCL in other reasoning tasks, such as coding.\n2. Some related baselines are missing for discussion and comparison (e.g. MoPPS [1]). I mean this can be added to show iteration numbers as an efficiency metrics as the ablation to show connections (Unnecessary to implement for all experiments considering the time constraint in rebuttal). \n3. This paper lacks systematic theoretical analysis regarding why an additional value model is sufficient to predict prompt difficulty. (In Line 300, it says: \"Note that the value model $V$ in our algorithm is one step behind the policy $\\pi$, which is acceptable since each update is small with $\\pi_{t+1}\\approx\\pi_{t}$\".) However, I think this can be resolved by relating to some existing works or adding some existing theoretical findings. For example, this method belongs to a model predictive sampling strategy [2], which utilizes the optimization history to build up a risk-ware module. Note that using iteration $\\pi_{t}$'s generalization to approximate $\\pi_{t+1}$'s task difficulty evaluation outcome first occurs in MPTS [2], and this provides a rigorous predictive foundation. Adding reference and theoretical discussions on MPTS as this work's predictive foundation can well support the validation of the prediction results.\n4. \"we find that both training and inference of the value model incur negligible cost and can be\n  completed under 30 seconds for each step\", I think providing a detailed breakdown of time consumption for the whole training process (e.g., rollout, log_prob, policy model update, value model update, ...) would better support this claim.\n\n[1] Can prompt difficulty be online predicted for accelerating RL finetuning of reasoning models?\n\n[2] Model Predictive Task Sampling for Efficient and Robust Adaptation.\n\nOverall, I am quite positive on this work in terms of novelty and contributions. I'll update the score if the above weaknesses are addressed in the revised version."}, "questions": {"value": "1. The idea of using a value model for estimating sample difficulty is somewhat connected to LLM-as-judge methods; can you add some discussions in related work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QtzzJoCDiZ", "forum": "zqOCacBD3P", "replyto": "zqOCacBD3P", "signatures": ["ICLR.cc/2026/Conference/Submission19724/Reviewer_ZNyN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19724/Reviewer_ZNyN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762075214250, "cdate": 1762075214250, "tmdate": 1762931561776, "mdate": 1762931561776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically studies how batch size and prompt difficulty affect LLM post-training dynamics and produces two key findings. First, the optimal batch size occurs at the transition point between sub-linear and linear generation time scaling. Second, they empirically confirm that prompts of intermediate difficulty (where the model has a ~50% success rate) provide the highest gradient norms and lead to the best test accuracy, validating insights from prior work.\n\nBuilding on these findings, the paper proposes Prompt Curriculum Learning (PCL). PCL trains a value model alongside the main policy to estimate a prompt's difficulty (expected reward) with a single forward pass. At each step, it greedily samples prompts whose predicted value is closest to 0.5. This value model serves as a lightweight and on-policy substitute for computationally expensive methods like rollout-based filtering and avoids the off-policy staleness of dictionary-based approaches, proving to be ~12-17 times faster at identifying informative prompts.\n\nEmpirical results across various math reasoning benchmarks show that PCL either achieves state-of-the-art performance or reaches comparable results in significantly less training time."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper contains systematic and rigorous experimentation to empirically validate all its core claims on optimal batch size, prompt difficulty and the effectiveness of PCL. \n - PCL is a relatively lightweight algorithm that addresses the high computational cost of rollout-based filtering and the off-policy staleness of dictionary-based methods.\n - The value model trained has reasonable accuracy (same as 3 samples per prompt) and provides a clear enough signal on prompt difficulty and significantly reduces training time during the filtering stage. \n - The paper is well presented and easy to follow with a clear narrative."}, "weaknesses": {"value": "- A core assumption in the paper is that rewards are binary. There are many tasks like creative writing where partial rewards are the norm. It is not clear how intermediate difficulty can be mathematically determined in those cases. Similarly, it is unclear whether the value model would succeed in accurately estimating the value of a prompt in such such tasks. \n - For practical reasons, oftentimes rollouts and policy updates happen simultaneously. In such a setup the value model would be even further behind the policy and it is not clear if the accuracy of value prediction would be affected in any way. \n - Filtering exclusively for intermediate difficult could lead to the model regressing on easy tasks and never improving on the hard ones. An interesting experiment could be to do weighted sampling of the prompts during training instead of greedy sampling to allow some easy and hard prompts to also be included in training. \n - While more efficient than rollouts, PCL requires training and maintaining a second, often same-sized, value model, which doubles the memory footprint compared to single-model approaches."}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RiGYuePvMG", "forum": "zqOCacBD3P", "replyto": "zqOCacBD3P", "signatures": ["ICLR.cc/2026/Conference/Submission19724/Reviewer_MXSJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19724/Reviewer_MXSJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148132606, "cdate": 1762148132606, "tmdate": 1762931561146, "mdate": 1762931561146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper re-uses classical active batch selection method (phrased as curriculum learning) in the post-training of large language models. \n- Method: At each gradient update step, the method first samples a large pool of prompts, then chooses a subset from the pool as the training batch. The choice is based on signals from the value function. \n- Experiments: The authors conduct the experiments using MATH and DeepScaleR, with models ranging from 1.7B to 8B. As in Table 1, the performance gain is relatively neutral."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Presentation: The write-up is clear, and the descriptions of the experiments are detailed and easy-to-understand.\n- Experiments: The attempt in understanding \"optimal\" batch size is very interesting (although there should be many artifacts in experimental settings that affect the optimality other than the factors proposed by the authors. It would be nice to further scale. One such example is [[Goyal et al., 2018](https://arxiv.org/pdf/1706.02677)]).\n\nReferences:  \n[1] Goyal, Priya, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. \"Accurate, large minibatch sgd: Training imagenet in 1 hour.\" arXiv preprint arXiv:1706.02677 (2017)."}, "weaknesses": {"value": "- Lack of novelty: While the manuscript is well-written as a technical report, it offers limited methodological novelty or new insights to the field. The findings are well-known, and numerous prior works have explored similar directions. There appears to be little distinction between the proposed method and existing approaches, aside from the specific experimental setting (models/datasets). Consequently, this work seems better suited as a report or blog post, rather than a distinct contribution to academic research.\n- Missing citations and comparisons: Related works should be put in the main body of the paper, instead of in the appendix. Also, please properly cite prior works on active batch selection and curriculum rl, e.g., [[Mindermann et al., 2020](https://arxiv.org/pdf/2206.07137)], [[Ash et al., 2019](https://arxiv.org/pdf/1906.03671)], [[Parker-Holder et al, 2022](https://arxiv.org/abs/2203.01302)], which all to some extent show it is better to learn from examples of medium difficulty. The proposed method appears to be a special case of many existing works, like [[Ye et al., 2024](https://arxiv.org/pdf/2411.00062)] but using greedy selection, or [[Muldrew et al., 2024](https://arxiv.org/pdf/2402.08114)] but using reinforce instead of dpo, or [[Kawaguchi et al, 2020](https://arxiv.org/pdf/1907.04371)] but using value as the selection criteria. A proper comparison with these works is necessary to accurately situate the paper within the fundamental literature and clarify the specific methodological contribution. \n- Empirical performance: The empirical gains are marginal compared to existing baselines, in terms of both accuracy and computational efficiency. Further algorithmic exploration is needed to demonstrate significant performance improvements.  \n\nReferences:  \n[1] Kawaguchi, Kenji, and Haihao Lu. \"Ordered sgd: A new stochastic optimization framework for empirical risk minimization.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.  \n[2] Ye, Ziyu, Rishabh Agarwal, Tianqi Liu, Rishabh Joshi, Sarmishta Velury, Quoc V. Le, Qijun Tan, and Yuan Liu. \"Scalable Reinforcement Post-Training Beyond Static Human Prompts: Evolving Alignment via Asymmetric Self-Play.\" ICML, 2024.  \n[3] Mindermann, Sören, Jan M. Brauner, Muhammed T. Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt Höltgen et al. \"Prioritized training on points that are learnable, worth learning, and not yet learnt.\" In International Conference on Machine Learning, pp. 15630-15649. PMLR, 2022.  \n[4] Ash, Jordan T., Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. \"Deep batch active learning by diverse, uncertain gradient lower bounds.\" arXiv preprint arXiv:1906.03671 (2019).    \n[5] Parker-Holder, Jack, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, and Tim Rocktäschel. \"Evolving curricula with regret-based environment design.\" In International Conference on Machine Learning, pp. 17473-17498. PMLR, 2022."}, "questions": {"value": "I encourage the authors to consider more fundamental algorithmic improvements, instead of simply re-using old ideas as low-hanging fruit, which can be misleading. For example, can we do better than top-m subset selection? One related line of research is stochastic batch acquisition [[Kirsch et al., 2024](https://openreview.net/pdf/abd7cf88617c72ff4ec92264fb8a7919ef9ffee3.pdf)]. It would be helpful if the authors could provide a thorough discussion on the limitations of current work and include new experiments with better algorithms that bring something new to the academia. I am happy to raise my score if these concerns are sufficiently addressed.  \n\nReferences:     \n[1] Kirsch, Andreas, Sebastian Farquhar, Parmida Atighehchian, Andrew Jesson, Frederic Branchaud-Charron, and Yarin Gal. \"Stochastic batch acquisition: A simple baseline for deep active learning.\" arXiv preprint arXiv:2106.12059 (2021)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "21yBWzguV7", "forum": "zqOCacBD3P", "replyto": "zqOCacBD3P", "signatures": ["ICLR.cc/2026/Conference/Submission19724/Reviewer_tnns"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19724/Reviewer_tnns"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762892945935, "cdate": 1762892945935, "tmdate": 1762931560558, "mdate": 1762931560558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}