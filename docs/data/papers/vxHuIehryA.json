{"id": "vxHuIehryA", "number": 25086, "cdate": 1758363965515, "mdate": 1759896734797, "content": {"title": "Enriching Knowledge Distillation with Intra-Class Contrastive Learning", "abstract": "Since the advent of knowledge distillation, much research has focused on how the soft labels generated by the teacher model can be utilized effectively. Previous papers point out that the implicit knowledge within soft labels originates from the multi-view structure present in the data. Feature variations within samples of the same class allow the student model to generalize better by learning diverse representations. However, in existing distillation methods, teacher models predominantly adhere to ground-truth labels as targets, without considering the diverse representations within the same class. Therefore, we propose incorporating an intra-class contrastive loss during teacher training to enrich the intra-class information contained in soft labels. In practice, we find that intra-class loss causes instability in training and slows convergence. To mitigate these issues, margin loss is integrated into intra-class contrastive learning to improve the training stability and convergence speed. Simultaneously, we theoretically analyze the impact of this loss on the intra-class distances and inter-class distances. It has been proved that the intra-class contrastive loss can enrich the intra-class diversity. Experimental results demonstrate the effectiveness of the proposed method.", "tldr": "", "keywords": ["Knowledge distillation; soft labels; contrastive learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d7b4f24cec816ddd4776241d3009228d8497532.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces an intra-class contrastive objective to the retraining of teacher to enrich its intra-class knowledge to allow better knowledge distillation. A margin loss is incorporated to improve the convergence of teacher training. Experimental results on different classification datasets, models, and settings demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method is well-motivated, simple, and clearly presented. Empirical evidence suggests that the proposed method yields promising results on different classification datasets. Furthermore, it is compatible with the classic relational method of RKD, which adds to its versatility."}, "weaknesses": {"value": "1.Recent strong KD baselines are missing for introduction and comparison. For example, Tables 2 and 3 only present outdated KD methods, and recent strong baselines such as FCFD [1], LSKD [2], CRLD [3], and SDD [4], are absent for comparison.\n\n2.The proposed method involves retraining the teacher, which makes the comparison to other traditional non-retraining methods seemingly unfair. Instead, the authors should compare their method to other baselines that also involve teacher retraining [5, 6].\n\n3.Since the concepts of using contrastive/margin loss for better representation learning and the exploitation of intra-class relation are well-established, and have also been explored in the knowledge distillation literature, the technical contribution of the paper is limited.\n\n4.Some descriptions in the manuscript seem inaccurate or incorrect. For example, on lines 44-45, the method of Tian et al. (2019) does not involve retraining the teacher model and should therefore not be given as an example here.\n\n5.The method is only evaluated on image classification task. To prove its practical value, the authors should consider conducting experiments on other tasks such as object detection (like in DKD and ReviewKD), or cross-architecture (like in [7, 8]), and show that its effectiveness persists.\n\n[1] Liu et al. Function-Consistent Feature Distillation. ICLR 2023.\n\n[2] Sun et al. Logit Standardization in Knowledge Distillation. CVPR 2024.\n\n[3] Zhang et al. Cross-View Consistency Regularisation for Knowledge Distillation. ACM MM 2024.\n\n[4] Wei et al. Scale Decoupled Distillation. CVPR 2024.\n\n[5] Dong et al. Toward Student-Oriented Teacher Network Training for Knowledge Distillation. ICLR 2024.\n\n[6] Hamidi et al. How to Train the Teacher Model for Effective Knowledge Distillation. ECCV 2024.\n\n[7] Hao et al. One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation. NeurIPS 2023.\n\n[8] Zhang et al. Cross-Architecture Distillation Made Simple with Redundancy Suppression. ICCV 2025."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4WpRdcRlgF", "forum": "vxHuIehryA", "replyto": "vxHuIehryA", "signatures": ["ICLR.cc/2026/Conference/Submission25086/Reviewer_LB8r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25086/Reviewer_LB8r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615369413, "cdate": 1761615369413, "tmdate": 1762943320191, "mdate": 1762943320191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Margin-Based Intra-Class Contrastive Distillation. During teacher training, besides the cross-entropy loss, an intra-class contrastive loss is added to mildly scatter same-class samples in feature space, enriching the intra-class variance encoded in soft labels. A learnable margin threshold is designed so that the contrastive loss is computed only on high-confidence samples, mitigating gradient conflicts and slow convergence; a pipeline cache is further adopted to cut GPU memory usage. Theoretical analysis shows that the proposed loss monotonically increases intra-class spread while preserving inter-class separation. The student model is trained with standard KD using the enhanced soft labels, requires no extra architecture, and can be combined orthogonally with existing distillation methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper takes a novel perspective by starting from the idea of generating richer soft labels from the teacher model, rather than following the common line of improving how the student fits the teacher.\n2. The paper introduces a combination of intra-class contrastive learning and learnable margins to enhance intra-class diversity. Theoretical derivations are rigorous, providing a strong theoretical foundation for the proposed method."}, "weaknesses": {"value": "1. In knowledge distillation, the teacher model is typically very large. The proposed method substantially increases the training cost of the teacher model, and given the relatively marginal performance gains, the overall benefit may not justify the additional computational expense.\n2. The comparison methods used are rather outdated, making the experimental results less convincing.\n3. It is recommended to adjust the citation format in the main text, as the current style negatively affects readability."}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wmnboh72zN", "forum": "vxHuIehryA", "replyto": "vxHuIehryA", "signatures": ["ICLR.cc/2026/Conference/Submission25086/Reviewer_FwFU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25086/Reviewer_FwFU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705216296, "cdate": 1761705216296, "tmdate": 1762943319826, "mdate": 1762943319826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel approach to knowledge distillation (KD), focusing on enhancing the intra-class information present in soft labels. The authors propose incorporating an intra-class contrastive loss during the teacher model training phase, which helps preserve the nuanced variations within each class. This method enriches the soft labels, leading to better performance in distillation tasks. The authors also address training challenges, such as mode collapse and slow convergence, by integrating margin loss to stabilize training. Experimental results demonstrate the effectiveness of this method, with improvements in accuracy across various image classification datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The proposed intra-class contrastive loss is a novel contribution to the field of knowledge distillation. The integration of margin loss to stabilize training is also a unique aspect of this work.\n\nQuality: The methodology is well-developed, and the experimental results provide solid evidence supporting the proposed approach. The paper demonstrates clear improvements over existing distillation methods.\n\nClarity: The overall structure of the paper is clear, with well-organized sections and effective use of figures. Some sections, particularly the theoretical analysis, could be improved for clarity.\n\nSignificance: The paper’s findings are significant for the advancement of knowledge distillation techniques, offering practical improvements that could lead to better performance in real-world applications."}, "weaknesses": {"value": "The theoretical analysis could benefit from further clarity, particularly in explaining the implications of the results for practical applications.\n\nThe paper primarily focuses on standard image classification tasks, which limits its generalizability. Further exploration of how the method applies to other domains would strengthen its impact.\n\nThe paper could provide more extensive ablation studies, particularly to explore the impact of different components of the loss function."}, "questions": {"value": "1 Intra-Class Contrastive Loss: How does the balance between intra-class contrastive loss and cross-entropy loss evolve during training, particularly in the early stages? Are there specific stages where the intra-class loss has a more pronounced impact on model performance?\n\n2 Effect of Margin Loss: Could you elaborate on how the margin loss affects model performance at different margin thresholds? Specifically, what impact does the margin have on convergence speed and stability, especially for imbalanced datasets?\n\n3 Memory Efficiency: How does the pipeline-based caching mechanism compare to traditional contrastive learning methods with larger batch sizes in terms of memory usage and training speed? Could you provide empirical evidence of its effectiveness?\n\n4 Ablation Study: In your ablation study, how do the results change when only intra-class contrastive loss is used versus when both intra-class loss and margin loss are combined? What insights can you provide from these comparisons regarding training stability?\n\n5 Theoretical Results: The theoretical analysis suggests a relationship between intra-class and inter-class distances. How do these theoretical insights directly translate to real-world performance improvements, particularly in terms of model generalization and robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dlCthhtx4N", "forum": "vxHuIehryA", "replyto": "vxHuIehryA", "signatures": ["ICLR.cc/2026/Conference/Submission25086/Reviewer_LXTg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25086/Reviewer_LXTg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828406332, "cdate": 1761828406332, "tmdate": 1762943319572, "mdate": 1762943319572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel approach to knowledge distillation (KD) by incorporating an intra-class contrastive loss during the teacher model's training to enhance the intra-class variations. The method is augmented with a margin-based sample selection and a pipeline caching mechanism to address stability and memory issues, with a theoretical analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The topic of intra-class learning is important for KD.\n- The paper is easy to follow with a clear logic.\n- The proposed method seems to be effective, with a theoretical support."}, "weaknesses": {"value": "- The idea of intra-class learning is not new in KD [1-2], especially intra-class contrastive learning [3-4].\n- The proposed method requires the teacher model to be trained with intra-class contrastive learning loss, which is very rigor and may increase the difficulty of training. This limits its practicability.\n- Adding the intra-class learning loss may be helpful to the dataset with high intra-class variations, while it may be harmful to the dataset with low variations.\n- The loss (**inter-class contrastive loss** + intra-class contrastive loss) used in the theoretical analysis is not consistent with the proposed one (**cross-entropy loss** + intra-class contrastive loss).\n- Many recent works are not cited and analyzed.\n\n[1] A Good Teacher Adapts Their Knowledge for Distillation. ICCV 2025\n[2] I2CKD: Intra- and Inter-Class Knowledge Distillation for Semantic Segmentation. Neurocomputing 2025\n[3] CKD: Contrastive Knowledge Distillation From a Sample-Wise Perspective. TIP 2025\n[4] Pay Attention to Your Positive Pairs: Positive Pair Aware Contrastive Knowledge Distillation. ACMMM2022\n[5] DA-KD: Difficulty-Aware Knowledge Distillation for Efficient Large Language Models. ICML2025\n[6] ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via α-β-Divergence. ICML2025"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3wicShqPYp", "forum": "vxHuIehryA", "replyto": "vxHuIehryA", "signatures": ["ICLR.cc/2026/Conference/Submission25086/Reviewer_TppX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25086/Reviewer_TppX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012595793, "cdate": 1762012595793, "tmdate": 1762943319308, "mdate": 1762943319308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}