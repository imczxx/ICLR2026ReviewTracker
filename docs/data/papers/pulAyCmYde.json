{"id": "pulAyCmYde", "number": 15666, "cdate": 1758253695005, "mdate": 1759897290177, "content": {"title": "Semantic-Anchored, Class Variance-Optimized Clustering for Robust Semi-Supervised Few-Shot Learning", "abstract": "Few-shot learning has been extensively explored to address problems where the amount of labeled samples is very limited for some classes. In the semi-supervised few-shot learning setting, substantial quantities of unlabeled samples are available. Such unlabeled samples are generally cheaper to obtain and can be used to improve the few-shot learning performance of the model. Some of the recent methods for this setting rely on clustering to generate pseudo-labels for the unlabeled samples. Since the effectiveness of clustering heavily influences the labeling of the unlabeled samples, it can significantly affect the few-shot learning performance. In this paper, we focus on improving the representation learned by the model in order to improve the clustering and, consequently, the model performance. We propose an approach for semi-supervised few-shot learning that performs a class-variance optimized clustering coupled with a cluster separation tuner in order to improve the effectiveness of clustering the labeled and unlabeled samples in this setting. It also optimizes the clustering-based pseudo-labeling process using a restricted pseudo-labeling approach and performs semantic information injection in order to improve the semi-supervised few-shot learning performance of the model. We experimentally demonstrate that our proposed approach significantly outperforms recent state-of-the-art methods on the benchmark datasets. To further establish its robustness, we conduct extensive experiments under challenging conditions, showing that the model generalizes well to domain shifts and achieves new state-of-the-art performance in open-set settings with distractor classes, highlighting its effectiveness for real-world applications.", "tldr": "", "keywords": ["few-shot learning", "semi-supervised few-shot learning", "image classification"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6bced7f4601a5e48e9c3194f3b5320dce704b65.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a approach for semi-supervised few-shot learning. The core idea is to improve the quality of learned representations and the effectiveness of pseudo-labeling for unlabeled data. The method introduces a class-variance optimized clustering process and a cluster separation tuner. Furthermore, a semantic injection network is trained to align visual prototypes with semantic features. The authors claim state-of-the-art performance across benchmark datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The method introduces variance optimized clustering and cluster separation tuner to directly optimize the structure of the embedding space by enforcing cluster compactness and separation.\n- The use of a semantic injection network to refine visual prototypes is a well-motivated attempt to correct prototypes.\n- Good results are shown empirically."}, "weaknesses": {"value": "- Most of the components in the proposed system are combinations of existing techniques, making the overall contribution incremental.\n- The writing can be improved, for example, there are many formatting issues for the citations. \n- The proposed method involves too many components, making it unpractical for real-world applications."}, "questions": {"value": "- What is new in the paper in constrast to techniques already in literature. \n- address W2, W3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I04KAIJhQ4", "forum": "pulAyCmYde", "replyto": "pulAyCmYde", "signatures": ["ICLR.cc/2026/Conference/Submission15666/Reviewer_f7Mc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15666/Reviewer_f7Mc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712922563, "cdate": 1761712922563, "tmdate": 1762925922230, "mdate": 1762925922230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel approach for semi-supervised few-shot learning (SSFSL) that integrates a class variance optimized clustering (CVOC) with the cluster separation tuner (CST) and label propagation (LP). The method aims to improve pseudo-labeling quality and feature representation by explicitly optimizing intra-class compactness and inter-class separation while incorporating semantic information from class descriptions. Extensive experiments on standard benchmarks (miniImageNet, tieredImageNet, CUB) demonstrate state-of-the-art performance, with additional strong results in cross-domain and distractor-class settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The integration of CVOC with CST provides a principled way to refine cluster assignments and prototype positions, going beyond standard clustering methods. The semantic injection network is a creative way to leverage language-vision alignment for more robust prototypes.\n2. The paper includes extensive experiments across multiple datasets. Ablation studies effectively validate the contribution of each component."}, "weaknesses": {"value": "1. The semantic injection module relies on CLIP and GPT-4o-mini for generating class descriptions. The performance sensitivity to the choice of these models (e.g., using a smaller or less capable language model) is not thoroughly explored. A discussion or experiment on this dependency would strengthen the paper.\n2. The CVOC distance blends a reconstruction-based distance with episode-level intra- and inter-class Euclidean terms and scalar weights $w_{intra}$/$w_{inter}$ (Eq.4). While the empirical gains are shown, the manuscript lacks a clear theoretical/intuition-driven explanation for why this particular additive form is appropriate in settings where class manifolds and prototypes may have very different geometries (e.g., non-linear manifolds where Euclidean prototype distance is misleading).\n3. CST is inspired by the firefly heuristic and uses a brightness score to move weaker prototypes toward stronger ones. This is an attractive idea but it is presented as a pragmatic heuristic without analysis of stability, convergence guarantees, or failure modes. The authors state they use one CST iteration for efficiency and specific constants, but do not convincingly show when CST can hurt or why one iteration is enough across diverse datasets. Please add examples where CST moves prototypes incorrectly.\n4. The method keeps the lowest-entropy k% of unlabeled samples. Entropy alone can be an unreliable confidence proxy in overconfident networks or imbalanced classes. The paper shows a figure but does not compare RPL against alternative selection rules (e.g., margin-based confidence, calibrated probabilities, or ICI credibility scores)."}, "questions": {"value": "1. For CVOC you blend drec with $w_{intra}$·$L_{intra}$ − $w_{inter}$·$L_{inter}$ (Eq.4). How were $w_{intra}$ and $w_{inter}$ selected for each dataset and episode? Are they fixed global scalars or adapted per-episode?\n2. Why choose CLIP text encoder over alternatives? The paper shows CLIP vs BERT comparison (Figure 11). Were other multimodal or larger/smaller CLIP variants tried? Does semantic anchoring still help if CLIP is weaker (smaller text encoder)?\n3. Restricted pseudo-labeling: entropy threshold keeps k% lowest-entropy samples. Did you compare entropy-selection to margin-based selection or ICI-like credibility ranking? If not, please add a small comparison; if yes, please report results and include the chosen selection rule justification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NLN6lA70TA", "forum": "pulAyCmYde", "replyto": "pulAyCmYde", "signatures": ["ICLR.cc/2026/Conference/Submission15666/Reviewer_iyBx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15666/Reviewer_iyBx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804356543, "cdate": 1761804356543, "tmdate": 1762925921867, "mdate": 1762925921867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel approach for semi-supervised few-shot learning (SSFSL) designed to improve clustering-based pseudo-labeling by learning more discriminative feature representations. The authors introduce three key contributions. \n\n1. **Class-Variance Optimized Clustering (CVOC)** uses a composite distance metric to generate cleaner pseudo-labels by enforcing both intra-class compactness and inter-class separation. \n2. A lightweight **Cluster Separation Tuner (CST)** further refines class prototypes to maximize their separation. \n3. A **Semantic Injection Network** anchors visual prototypes to textual class descriptions, leveraging language priors to resolve visual ambiguities, which is especially useful when labeled data is scarce.\n\nExperiments on the miniImageNet, tieredImageNet, and CUB benchmarks demonstrate that this combined approach significantly outperforms existing state-of-the-art methods. The paper also validates the model's robustness in challenging cross-domain and distractive open-set scenarios, highlighting its effectiveness for practical applications."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Improved Pseudo-Label Quality**: The Class-Variance Optimized Clustering (CVOC) method directly addresses the challenge of noisy pseudo-labels in SSFSL. By employing a composite distance metric that optimizes both intra-class compactness and inter-class separation, the paper ensures the generation of cleaner and more reliable pseudo-labels, which is crucial for effective semi-supervised learning.\n2. **Enhanced Class Discriminability**: The Cluster Separation Tuner (CST) acts as a refinement mechanism for class prototypes. This lightweight optimization procedure actively maximizes the separation between different class centroids, leading to more distinct and discriminative feature representations, which in turn boosts the model's ability to differentiate between classes.\n3. **Robustness to Visual Ambiguity and Limited Data**: The Semantic Injection Network provides a powerful way to leverage external knowledge. By grounding visual prototypes with semantic embeddings derived from textual descriptions, the model can overcome issues arising from visual ambiguities or insufficient labeled data, making it more robust and effective in challenging real-world scenarios."}, "weaknesses": {"value": "# **Major Weaknesses：**\n## **1. Limited and Potentially Outdated Task Formulation:**\nThe paper's entire experimental validation is confined to the task of standard image classification. In an era where the field is rapidly advancing towards more complex, fine-grained understanding tasks (e.g., few-shot object detection, semantic segmentation, or part localization), focusing solely on image-level classification feels narrow and regressive. The proposed contributions, which operate on global image embeddings, may not be applicable to these more demanding, localized tasks. This severely limits the broader impact and generalizability of the work, making it feel more like an incremental improvement on a legacy problem rather than a significant step forward for the community.\n ## **2. Use of Outdated Architectures and Baselines:** \nThe experiments are conducted exclusively on older network architectures like ResNet-12 and WRN-28-10. The lack of evaluation on modern, prevalent backbones such as Vision Transformers (ViTs) or contemporary CNNs (e.g., ConvNeXt) is a major flaw. The inductive biases of these newer architectures are fundamentally different, and a method optimized for ResNet's feature space may not provide any benefit on a ViT. Furthermore, many of the baseline methods cited for comparison are several years old most of which are before 2023. The latest iPLC method claimed by the author in 2025 is actually an extended journal version of experiments from the 2021 ICCV method$^{[1]}$.\n\n# **Minor Weaknesses:**\n## **1. Poor Quality of Figures:** \nThe text within several figures is too small to be easily legible, particularly in the schematic diagrams like Figure 1 and Figure 2. At the same time, the layout of content in some images is unreasonable; for example, in the first image, there is a large blank area on the left side, while the right side is too crowded. In addition, some text placement is inappropriate, for example, in Figure 3 the numbers overlap with the chart border, and in Figure 7 the labels for the two methods on the right are not centered.\n## **2. Typos:**\nLine 317: \"...$w_{fs}$ the overall few-shot weight...\" – a verb is missing here. It should likely read \"...$w_{fs}$ is the overall few-shot weight...\". \nLine 696: \"1-shot epsiodes/tasks\" – \"episodes\" is misspelled.\n\nLine 1133: \"WordNet glosses for each classes are automatically retrieved...\" - \"classes\" should be \"class\".\n\n# **Referrence:**\n[1] Lazarou, Michalis, Tania Stathaki, and Yannis Avrithis. \"Iterative label cleaning for transductive and semi-supervised few-shot learning.\" Proceedings of the ieee/cvf international conference on computer vision. 2021."}, "questions": {"value": "**1. Performance on Modern Architectures:** The experiments are conducted on ResNet and WRN backbones. Given the prevalence of Vision Transformers (ViTs) and modern CNNs, could the authors provide experiments on these more current architectures? It would be crucial to understand if the performance gains from the proposed clustering and semantic injection modules hold on models with different inductive biases, which would significantly strengthen the claims of generalizability.\n\n**2. Applicability to Fine-Grained Few-Shot Tasks:** The paper focuses exclusively on image-level classification. Could authors elaborate on how the method might be adapted for more fine-grained few-shot tasks, such as few-shot object detection or segmentation? Specifically, how would the concepts of class prototypes, CVOC, and semantic injection apply when the goal is to localize objects (i.e., predict bounding boxes) rather than just classifying global features?\n\n**3. Performance of the Full Method in Cross-Domain Settings:** In Appendix A.1.2, the authors state that the semantic anchor module was disabled during the cross-domain evaluation to isolate the performance of the core visual framework. This is an insightful ablation, but it leaves the performance of the full method in this challenging scenario unknown. Could authors provide results for the cross-domain experiments with the semantic anchor module enabled? This would help verify whether the complete proposed system can generalize across domains, or if the semantic anchors are sensitive to domain shift.\n\n**4. Justification for the Four-Stage Agentic Pipeline:**  The paper details a four-stage agentic chain for generating class descriptions. While the process is well-described, its complexity raises questions about its necessity for this task. Could authors provide a quantitative comparison between this four-stage pipeline and a simpler baseline, such as using a single, direct prompt to the LLM to generate the description? Demonstrating a significant downstream accuracy improvement would be necessary to justify the added complexity of the proposed pipeline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LZUoapPj82", "forum": "pulAyCmYde", "replyto": "pulAyCmYde", "signatures": ["ICLR.cc/2026/Conference/Submission15666/Reviewer_o6qo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15666/Reviewer_o6qo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807202376, "cdate": 1761807202376, "tmdate": 1762925921309, "mdate": 1762925921309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the Semantic-Anchored, Class Variance-Optimized Clustering (SA-CVOC) method is proposed for semi-supervised few-shot learning (SSFSL). It generates more reliable pseudo-labels by optimizing inter- and intra-class variance, thereby producing improved clusters. It combines a Class Variance-Optimized Clustering (CVOC) module with a Cluster Separation Tuner (CST) to refine class prototypes through regularization. It also introduces a Semantic Injection Network (SIN) that leverages CLIP-based text embeddings in parallel with image features to inject class-level semantic information via reconstruction. This semantic anchoring is designed to enhance the model’s ability to represent and separate classes under limited supervision. SA-CVOC also employs restricted pseudo-labeling to mitigate noise and stabilize adaptation. Experiments on miniImageNet, tieredImageNet, and CUB datasets show consistent improvements and strong robustness over Cluster-FSL and other SOTA SSFSL methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Overall, the paper is clearly written, well organized, and easy to follow. It integrates semantic information into SSFSL through the SIN and refining class prototypes with intra-/inter-class variance control. \n+ The paper focuses on an important challenge (improved clustering to address PL noise) that is SSFSL. It introducing semantic anchoring, leading to substantial accuracy gains in both standard and cross-domain settings. Overall, it provides a meaningful step toward combining semantic priors and clustering for robust FSL.\n+ The detailed experimental validation compares SA-CVOC against several baselines and datasets. Good results on  robustness and generalization are reported."}, "weaknesses": {"value": "- Limited novelty and related works: The paper main focus is to improve clustering using well known technique of optimizing inter- and intra-class variance. The proposed method is not positioned with respect to existing works. There was no explanation for the limitations of SOTA works, and how clustering is one of them. The related literature on SSFSL does not go beyond 2022. Is there no more recent works?\n- Different components of the paper are provided as facts without justification: Section 4.1:  modules and networks; the rotation part in Section 4.2 is not explained. Not clear why the author suggest using it; the reconstruction part in Eq.4. is not explained and left for supplementary material; refining prototypes with cluster separation tuner (CST) is not justified; Why predict labels in Eq. 8 is done using reconstruction residuals; using class-text embedding and joining them with image embedding.\n- Limited discussion and interpretation of experimental results in Section 5.  This paper should also contain an experimental analysis of time and memory complexity.    The SIN adds complexity and introduces reliance on pre-trained CLIP text embeddings, which may limit applicability."}, "questions": {"value": "Can you please:\n- add publication year/venue of methods in result tables;\n- compare with more the literature about SSFSL;\n- situate your method in the SOTA literature and explain how it can address limitations of previous works;\n- improve the discussion on experimental results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eLbZr29sCH", "forum": "pulAyCmYde", "replyto": "pulAyCmYde", "signatures": ["ICLR.cc/2026/Conference/Submission15666/Reviewer_zLBo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15666/Reviewer_zLBo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959834128, "cdate": 1761959834128, "tmdate": 1762925920731, "mdate": 1762925920731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}