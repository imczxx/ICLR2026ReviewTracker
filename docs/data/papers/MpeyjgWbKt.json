{"id": "MpeyjgWbKt", "number": 11477, "cdate": 1758200003513, "mdate": 1763760166848, "content": {"title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "abstract": "Traditional Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router’s decisions align well with the experts’ capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling loss (ERC loss), a lightweight auxiliary loss that couples expert capabilities and the router’s decisions. We treat each row of the router matrix as a cluster center for the tokens assigned to a particular expert. From these centers, we create proxy tokens by applying a perturbation with noise. Using these proxy tokens, the ERC loss forces the router and experts to satisfy two constraints: (1) each expert exhibits higher activation for its corresponding proxy token than for any other proxy token, and (2) each proxy token elicits stronger activation in its designated expert than in any other expert. This optimization leads to two key effects: each row of the router matrix is an accurate representation of its expert’s capabilities, while each expert develops expertise that closely match the tokens routed to it. Our experiments involve pre-training multiple 3B-parameter MoE-LLMs on trillions of tokens in total, providing detailed evidence of the ERC loss’s effectiveness. Additionally, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing many valuable insights into MoEs.", "tldr": "", "keywords": ["Mixture-of-Experts", "Large language models", "Auxiliary loss", "Expert-router coupling", "Expert specialization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f286af2159144f76a35f18068c3f6ab08073f27a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes expert-router coupling loss, a lightweight auxiliary loss that couples expert capabilities (activation norm) and the router’s decisions. The authors claim that this loss encourages each expert and each proxy token to match with each other, improving performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The ERC loss is computationally cheap.\n2. The experiments show MoE gets considerable gain from this ERC loss.\n3. Much analysis and ablation are provided."}, "weaknesses": {"value": "1. You might need to compare with Router Orthogonalization Loss in https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf, since your loss is somewhat similar to ||(RW_g)TRWg - I||_F, if you assume W_g^TW_g\\approx I, it is similar to ||R^TR - I||_F.\n2. It seems that this ERC loss can be optimized to 0 when RMS(R) -> 0 or RMS(W_g) -> 0, so will it only serve like weight decay?"}, "questions": {"value": "1. Are both R and W_g optimized by ERC loss (rather than R only)?\n2. Can you directly calculate the expectation of ERC loss under \\delta and optimize the expectation directly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2N1iNmGFAR", "forum": "MpeyjgWbKt", "replyto": "MpeyjgWbKt", "signatures": ["ICLR.cc/2026/Conference/Submission11477/Reviewer_WsdJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11477/Reviewer_WsdJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889837526, "cdate": 1761889837526, "tmdate": 1762922582735, "mdate": 1762922582735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, to mitigate performance degradation caused by the representation mismatch between the router and experts in MoE architectures, each row of the router matrix is treated as a representative vector for the vectors processed by an expert. A constraint is introduced in the form of a loss function, ensuring that when values in the vicinity of this representative are input to the corresponding expert, the activation is maximized. This constraint helps ensure that the expert selected by the router based on the actual input is the one most efficiently activated by that input. Experimental results show that the proposed method consistently outperforms the standard MoE setup, with few exceptions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Many recent MoE papers focus on improving routing. Methods exist to resolve the mismatch between routing and experts, such as adding an auxiliary loss to teach desired properties in expert specialization, or modifying the model architecture, like AoE (which is also adopted as a baseline in this paper). The proposed method belongs to the former category; it achieves its goals regarding expert specialization by simply adding a simple constraint, without modifying the conventional model architecture at all. The proposed method can leverage existing software assets as-is, for example, by being integrated into existing training toolkits. The proposed method is an extremely lightweight loss function, and it is believed to have no or very small practical impact on training speed when introduced into the MoE training process. Personally, I am impressed that the authors conceived of this method, and I would like to try it in our own training framework.\n\nIt is noteworthy that the proposed method enables the router and experts to have an explicit geometric correspondence, succeeding in achieving a similar effect to AoE without requiring significant architectural modifications. It may also facilitate the visual analysis of the model's internals."}, "weaknesses": {"value": "The experiments only validate the method on a single, very small-scale model instance. It has not been demonstrated whether the method is effective across the wide variety of MoE architectures. Since the experiments involve expensive pre-training, it is understandable that validating on various settings must be forgone due to cost, but it is true that the information provided feels somewhat insufficient.\n\nThe method includes randomness, which may be a source of training instability, although as shown in 4.4 (2), the fact that this randomness contributes to generalization appears to be valid.\n\nA new hyperparameter, $\\alpha$, which is difficult to tune intuitively, is introduced. Given the current lack of experimentation across a wide range of model instances, applying the settings used in the paper directly to other experiments is considered to carry a certain amount of risk."}, "questions": {"value": "Regarding the randomness: could a method be devised to make the training behavior more theoretically consistent and predictable? For example, could a derivative algorithm be considered, such as marginalizing $R[i] \\odot \\delta_i$ over $\\delta_i$, or handling the noise as a distribution (without sampling)?\n\nThe proposed method can be seen as a form of contrastive learning between the router and expert features, and therefore it seems relatively natural to consider leveraging techniques from contrastive learning. Are there any thoughts on this at this time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O1XcUq668o", "forum": "MpeyjgWbKt", "replyto": "MpeyjgWbKt", "signatures": ["ICLR.cc/2026/Conference/Submission11477/Reviewer_2vnc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11477/Reviewer_2vnc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045287555, "cdate": 1762045287555, "tmdate": 1762922582332, "mdate": 1762922582332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new auxiliary loss for MoE training which better couples the experts and the routers and promotes expert specialization. The idea is to think of the router rows as cluster centers and to generate cluster data from the centers via random perturbations. The data from a given cluster should induce higher activation norm in the corresponding expert relative to the other experts and to data from other clusters. This is enforced by a soft hinge penalty. Adding the auxiliary loss adds modest overhead during training and does not affect inference. The authors find that the penalty improves downstream metrics over a baseline vanilla MoE and is comparable to the more expensive AoE method."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The method is simple, intuitive, and clearly presented. The experiments and evaluations are thorough and the auxiliary loss does appear to improve performance."}, "weaknesses": {"value": "The activation metric is not scale invariant, the auxiliary loss can be decreased in a non-meaningful manner simply by scaling up $W_g^i$.\n\nThe auxiliary appears to make the gradient dense across experts since activations norms for each token are computed across experts."}, "questions": {"value": "In addition to clarification of the weaknesses, I have the following questions:\n\nIs it possible that $\\alpha > 1$ can perform even better? At what $\\alpha$ will you recover vanilla MoE?\n\nWhat about using post SwiGLU activations or $W_o$?\n\nDo the norms $\\lVert R[i] \\rVert$ stay comparable across $i$? If this is not true, then there seems to be a mismatch between Euclidean distance based clustering and inner-product based routing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1VzPaO0JWS", "forum": "MpeyjgWbKt", "replyto": "MpeyjgWbKt", "signatures": ["ICLR.cc/2026/Conference/Submission11477/Reviewer_n6Ei"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11477/Reviewer_n6Ei"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762130516985, "cdate": 1762130516985, "tmdate": 1762922581905, "mdate": 1762922581905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}