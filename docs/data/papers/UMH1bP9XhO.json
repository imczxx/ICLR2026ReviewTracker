{"id": "UMH1bP9XhO", "number": 8585, "cdate": 1758091739006, "mdate": 1759897774949, "content": {"title": "Scaled Gradient Mean Subtraction: A Lightweight Method for Amplifying Underutilized Gradient Directions", "abstract": "We propose Scaled Gradient Mean Subtraction (SGMS), a lightweight method that improves neural network training by amplifying underutilized gradient directions. In mini-batch training, gradients from individual samples are expected to point in diverse directions, but in practice they are often highly correlated, spanning only a few dominant directions. Consequently, weight updates are confined to a low-rank subspace, leaving many directions underutilized. SGMS addresses this imbalance by subtracting a scaled mean from the gradient. With common ReLU-like activations (e.g., ReLU, GeLU, SiLU), this simple operation weakens the most dominant direction, allowing complementary directions to play a greater role in optimization. Unlike approaches that rely on costly covariance statistics or matrix decompositions, SGMS achieves a similar rebalancing of gradient directions with a single mean-subtraction step, adding negligible overhead and requiring no architectural changes. Formally, SGMS generalizes Gradient Centralization (GC) as a special case, but—by partially rather than fully suppressing the mean direction—it retains valuable gradient components that GC eliminates. Experiments on CIFAR and ImageNet across multiple architectures show consistent accuracy improvements, validating SGMS as a practical and flexible framework for rebalancing gradient directions.", "tldr": "We propose Scaled Gradient Mean Subtraction (SGMS), a lightweight optimization method that modifies gradients to amplify underutilized directions in the weight parameter space, improving training efficiency in DNNs without extra memory cost.", "keywords": ["Gradient Modification", "Training Efficiency", "Deep Learning Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0e631a344b4a4f4b7cc190bba41cae149a0e25e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a method termed Scaled Gradient Mean Subtraction (SGMS), which aims to enhance neural network training by mitigating the influence of dominant gradient directions. The core idea involves scaling the mean of the gradient before its subtraction. The authors provide an observation that in layers with non-negative activations (e.g., ReLU), this dominant direction frequently aligns with the uniform vector, thus framing SGMS as an efficient approximation for suppressing the dominant singular value. A significant advantage of this approach is its computational efficiency, as it avoids the expensive matrix decompositions required by preconditioners like AdaBK. The empirical evaluation covers CNNs and Vision Transformers on CIFAR and ImageNet benchmarks. The reported results consistently demonstrate accuracy improvements over standard training, supporting the efficacy of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The motivation is clear, the authors take an insight into subspace concentration of gradient updates, and  analyze ‘why weight updates naturally concentrate in a low-rank subspace’ and provide evidences of how SGMS approximate the decomposition process, even though I have concerns on its novelty.  The writing is clear and the paper is easy to follow."}, "weaknesses": {"value": "1. **The novelty of SGMS is limited and a bunch of related work are missed**. Firstly, it is highly consistent with Gradient Centering (GC).  Without demonstrating a notable improvement over GC, the introduction of an extra hyper-parameter ($\\beta$) may even reduce its practical utility. Secondly,  the idea of centering can theoretically improve the conditioning (i.e., suppressing the dominant singular value) can at least track back to the paper [1] in 1998, and following work [2,3]. This paper miss the related work along this track. \n\n   Besides, the field's focus has shifted towards covariance-based pre-conditioners because they decouple gradient directions, whereas centering operations primarily make the gradients well-distributed and only approximately utilizes gradients better. In this way, the formulation of this method is more like the recently work in gradient orthogonalization[4], or projected based gradient updates for  LLM training, e..g, the Galore[5]. I think this paper should provide a detailed comparison to these methods. \n\n   \n\n2. **Concerns on the experiments :** \n\n   (1) The empirical validation is limited, covering only image classification tasks (CIFAR, ImageNet-1k) with a limited set of models (ResNet, DeiT-Small). Broader tasks and architectures are needed to demonstrate generality.\n\n   (2) The authors claim GC is a \"special case\" of SGMS, yet the ImageNet-1k experiments lack a critical comparison between Vanilla, GC, and SGMS. This makes it difficult to assess the actual performance gap and the value of the proposed scaling factor.\n\n   (3) The authors suggest that $\\beta$ should be chosen statistically based on singular values (Line 286-290). However, in practice, it is treated as a fixed hyperparameter without a learnable or strategic selection process. Furthermore, the evidence for a \"good\" $\\beta$ is insufficient and is only provided on small-scale experiments (ResNet on CIFAR10), not on the more challenging settings like ImageNet-1k or DeiT.\n\n3. **Clarity of Assumption:** The authors' central claim that GC/SGMS balances gradient distribution seems to rely on an unstated assumption: \"The greater the activation value after a nonnegative activation function, the greater the gradient, and the more the distribution.\" Making this assumption explicit is crucial for the reader's understanding.\n\n4. **Justification of Claims:** The observation in Line 143-144 that SGMS \"amplifies underutilized directions\" is interesting but not fully substantiated. While it may be ignored by LoRA, it does not prove that these directions are useful rather than being noise. Further analysis is required to validate their utility.\n\n5. **Typo:** Table 9 is missing the conventional bold and underline formatting, which should be corrected.\n\n   \n\n   **Ref:**\n\n   [1] Accelerated Gradient Descent by Factor-Centering Decomposition, 1998\n\n   [2] Deep Boltzmann Machines and the Centering Trick,  2012\n\n   [3] Mean-normalized stochastic gradient for large-scale deep learning. ICASSP, 2014\n\n   [4] SWAN: SGD with Normalization and Whitening Enables Stateless LLM Training, ICML, 2025.\n\n   [5] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection, ICML, 2024"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YpNysucgUD", "forum": "UMH1bP9XhO", "replyto": "UMH1bP9XhO", "signatures": ["ICLR.cc/2026/Conference/Submission8585/Reviewer_d3aU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8585/Reviewer_d3aU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901592486, "cdate": 1761901592486, "tmdate": 1762920433698, "mdate": 1762920433698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Scaled Gradient Mean Subtraction (SGMS), a simple method to improve neural network training by rebalancing gradient directions to use underutilized directions. The authors observe that mini-batch gradients often lie in a low-rank subspace dominated by a few directions, limiting optimization efficiency. SGMS subtracts a scaled mean from layer gradients to partially suppress the dominant gradient direction (unlike full Gradient Centralization (GC)), thereby amplifying underutilized directions and potentially improving generalization. Experiments on Cifar and ImageNet show consistent small accuracy gain for SGMS."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper structures an argument connecting gradient subspace degeneracy to underutilization and introduces SGMS as a low-cost alternative to heavy preconditioning methods.\n2.\tResults on multiple datasets (CIFAR, ImageNet) and architectures (ResNets, DeiT), show that SGMS improves performance."}, "weaknesses": {"value": "1.\tThe authors present SGMS as a generalization of GC, using partial rather than full suppression of dominant gradient directions. However, this modification is conceptually minor and lacks deeper analytical justification for why or when the partial suppression (β < 1) works better, the method is indeed a parametric relaxation of GC with some interesting thought.\n2.\tThe paper lacks a theoretical guarantee that SGMS stabilizes training. Some parts of the theoretical discussion is based on intuition and observation, for example, the authors approximate the dominant singular vector with the uniform vector e, empirically true for BatchNorm–ReLU networks but may not generally apply to architectures lacking normalization or using zero-centered activations. \n3.\tThe performance gains are limited (mostly between 0.2-0.8 %), which is expected given that β=0.9 is very close to β=1 in GC. There is now evidence or theoretical analysis explaining why higher β values perform better. On the other hand, given that AdaBK achieves higher accuracy, SGMS’s contribution appear limited. \n4.\tLack of long training, or larger transformer architectures (e.g., ViT-B) undermines the claimed robustness and generality. Moreover, all experiments are conducted from scratch, with no investigation of cross-task or transfer-learning scenarios such as fine-tuning pretrained backbones, which limits understanding of SGMS behavior in practical pipelines. \n5.\tThe ablation study lacks analysis of optimizer dynamics (e.g., with momentum, weight decay, or learning rate scaling), as well as evaluations under large-batch training."}, "questions": {"value": "1. Can you theoretically justify how mean subtraction (Eq. 12) affects convergence or stability?\n\n2. What is the rationale behind choosing β = 0.9, and could smaller β values be beneficial in cases with more correlated gradients? Is there any theoretical guidance for selecting β instead of purely empirical tuning?\n\t\n3. Have you verified whether the assumption v_1≈e holds for networks without normalization (e.g., LSTM-based architectures)?\n\t\n4. Can you examine SGMS robustness across optimizer dynamics?\n\t\n5. Does SGMS help in under-parameterized or small-data regimes, where gradient diversity is naturally lower?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xh5zTVleq5", "forum": "UMH1bP9XhO", "replyto": "UMH1bP9XhO", "signatures": ["ICLR.cc/2026/Conference/Submission8585/Reviewer_G3oa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8585/Reviewer_G3oa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970221713, "cdate": 1761970221713, "tmdate": 1762920433281, "mdate": 1762920433281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their thorough and constructive feedback.\n\nWe are currently preparing detailed responses to each comment, including additional analysis and experiments where feasible.\nWe will provide point-by-point replies shortly."}}, "id": "GPF6OgPhCW", "forum": "UMH1bP9XhO", "replyto": "UMH1bP9XhO", "signatures": ["ICLR.cc/2026/Conference/Submission8585/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8585/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8585/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763123491213, "cdate": 1763123491213, "tmdate": 1763123491213, "mdate": 1763123491213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Scaled Gradient Mean Subtraction (SGMS), \na simple gradient regularization method that subtracts a fraction \n$\\beta$ of each column’s mean from layer gradients.\n\nIt generalizes Gradient Centralization ($\\beta=1$) and approximates \na lightweight form of spectral flattening. SGMS improves optimization stability \nand test accuracy on Resnets (CIFAR, ImageNet) with minimal computational cost, \nshowing modest but consistent gains. The experimental section also includes results on transformer models, \nwhere improvements are smaller but generally positive."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- $S_1$: SGMS is a one-line modification applicable to any optimizer and architecture, requiring negligible additional computation or memory. Appendix F details how to extend the intuitive SGMS to convolutions and attention layers and Appendix G details the theoretical overhead of the method. They both complement well the main text.\n\n- $S_2$: The paper provides a clear theoretical motivation based on a linear-algebraic view of gradient formation ($\\Delta W = X^\\top \\Delta Y$), connecting SGMS to both Gradient Centralization and spectral preconditioning. I would have liked a maybe more detailed paragraph on preconditionners in the related works section as structured second-order and preconditioning methods such as\nK-FAC, Shampoo, or SOAP, are closely related to this line of work (together with AdaBK). However, given the page limitation, this is understandable.\n\n- $S_3$: Empirical results on CIFAR and ImageNet ResNets show modest but consistent accuracy improvements with minimal cost. SGMS outperforms Gradient Centralization and approaches AdaBK performance at a fraction of its compute and memory.\n\n- $S_4$: The method is empirically validated to be orthogonal to optimizer choice, showing compatibility and consistent benefits with SGD, Adam, and AdamW.\n\n- $S_5$: The paper is transparent and thorough, reporting all hyperparameters, efficiency metrics, and activation-type dependencies, with well-designed ablations and diagnostics. Furthermore, it is enjoyable to read as the methodology section progressively walks us through the authors' intuition.\n\n- $S_6$: Overall, this paper offers a clean conceptual unification between mean-subtraction methods and curvature-based preconditioners through the tunable scaling factor $\\beta$, bridging Gradient Centralization and full whitening."}, "weaknesses": {"value": "- $W_1$: The theoretical justification relies on the heuristic approximation $v_1 \\approx e$ (alignment between the leading singular vector of $X$ and the uniform direction). This assumption is plausible but not formally justified; a perturbation bound or regularity assumption on $X^\\top X$ would strengthen the argument.\n\n- $W_2$: Transformer evaluation is limited to a shortened 100-epoch DeiT-Small schedule, which does not fully test the method under standard long-training regimes. Claims of generality beyond ResNets therefore remain preliminary. Furthermore, I would really like to see if this simple (yet efficient!) modification of the gradient transfers to other tasks than vision. \nExperiments on NLP as (i) pretraining a small GPT-2 like transformer on OpenWebText (or similar dataset) and/or (ii) fine-tuning of a RoBERTa-like transformer on canonical tasks such as GLUE would be a strong plus to the paper and I would be happy to raise my score if the authors presented them before the end of the discussion phase.\n\n- $W_3$: The Gradient Centralization baseline might be under-tuned (fixed at $\\beta=1$ without rescaling), although canonical. Exploring stronger GC variants could narrow the observed gap and yield a fairer comparison.\n\n- $W_4$: SGMS is most effective with nonnegative activations (ReLU/SiLU); effects are smaller or neutral with zero-centered activations such as \\texttt{tanh} or under short schedules, suggesting some activation dependence. As GeLU is prominent in nowadays NLP, it could be interesting to study if SGMS still compares favorably to vanilla training regime."}, "questions": {"value": "- $Q_1$: Could the authors formalize the heuristic $v_1 \\approx e$ assumption by providing a perturbation bound or a mild structural condition (e.g., approximate column homogeneity of $X^\\top X$) under which the replacement $v_1 v_1^\\top \\to ee^\\top$ is theoretically justified? I know this might be very technical to get to but as the justification in the paper is for now only empirical, I am worried this might not translate to other application fields (NLP, speech, etc.)\n\n- $Q_2$: Have the authors considered extending SGMS to non-vision domains such as NLP? In particular, would the method improve optimization or stability when (i) pretraining a small GPT-2–like transformer on OpenWebText or (ii) fine-tuning a RoBERTa-style model on GLUE?\n\n- $Q_3$: How sensitive is SGMS to the scaling factor $\\beta$ in large-scale or long-training regimes? Do the authors observe consistent optimal values (e.g., $\\beta \\approx 0.9$), or does it vary across architectures and optimizers? Could the $\\beta$ be tuned **during** training to avoid instabilities and to help convergence? Although the following may introduce additional cost and go somewhat beyond the paper’s scope, but I would be interested to know whether $\\beta$ could in principle be learned automatically?\n\n- $Q_4$: For activations like GeLU or other zero-centered functions, do the authors expect theoretical or empirical limitations for SGMS? Would a modified mean-subtraction scheme (e.g., per-feature normalization) alleviate this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6NSu6xQTU7", "forum": "UMH1bP9XhO", "replyto": "UMH1bP9XhO", "signatures": ["ICLR.cc/2026/Conference/Submission8585/Reviewer_4W9V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8585/Reviewer_4W9V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987175606, "cdate": 1761987175606, "tmdate": 1762920432741, "mdate": 1762920432741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to center weight gradients during the update of gradient-descent trained neural networks (NN).\n\nThey motivate the update as an approximation of the update obtained by a rank-one correction on the original euclidean update, claimed to approximate the leading eigendirection of X^TX, which is empirically checked as the correlation between a numerically computed leading eigenvector (if I undestood correclty).\n\nThe update is benchmarked on ResNets and Transformers, showcasing some accuracy improvement over non-centered variants for a minimal overhead."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear and accessible presentation, making the concepts easy to understand.\n- Low additional implementation and computational overhead, with a versatile approach that can be integrated into various optimization algorithms such as Adam or momentum-based gradients."}, "weaknesses": {"value": "1. The explanations in sections 3.2 and 3.3 are somewhat unclear. If I understand correctly, the ideal goal would be to perform the operation in Equation 6, which is subsequently approximated using Equation 9. How is the inverse-square root computed in Equation 6? This step does not seem to directly lead to the formulation in Equation 9, and clarification on this point would be helpful.\n2. The technique of gradient centering has been known since the early days of neural network training. I have included some references below as examples. However, it also appears that such methods are not commonly implemented in standard neural network libraries like PyTorch, which might explain their limited popularity compared to heuristics like Adam. What novel insights or contributions does your paper provide in relation to these papers?\n3. Based on my understanding of neural network optimization, the effects described are likely related to the interaction between bias vector updates and weight vector updates, through second-order effects or information geometry if motivating using the Natural Gradient. However, the paper does not discuss bias vectors at all. I would expect the impact of the proposed method to diminish in networks with small or negligible biases, and addressing this aspect could strengthen the analysis.\n\n- Le Cun, Y., Kanter, I., & Solla, S. A. (1991). Eigenvalues of covariance matrices: Application to neural-network learning. Physical review letters, 66(18), 2396.9\n- Schraudolph, N. N. (2002). Centering neural network gradient factors. In Neural Networks: Tricks of the Trade (pp. 207-226). Berlin, Heidelberg: Springer Berlin Heidelberg.\n- Ollivier, Y. (2015). Riemannian metrics for neural networks I: feedforward networks. Information and Inference: A Journal of the IMA, 4(2), 108-153."}, "questions": {"value": "4. Your method is motivated as an optimization technique, yet the results shown primarily report improvements in validation accuracy, if I am correct? Could you clarify this distinction? Do you also get improvement in training loss ?\n5. How is the vector v_1 computed in Figure 1, which serves as baseline ?\n6. What is the reasoning for considering the concentration of the update in a low-dimensional subspace as potentially undesirable, apart from claimed empirical improvement in validation accuracy ?\n7. Lines 245–251 are somewhat imprecise, using phrases like \"... often ...\", \"... some degree of similarity ...\", and \"... tends to align ...\".\n\nFinally, a minor comment that I don't expect to be addressed :)\n- I disagree with the fact that performing power-iterations on X^TX would be too costly in practice (l243), especially given the fact that the spectrum of X^TX is highly imbalanced (this would require only a few, if not a single interation), and also since you can initiate the power iteration using your vector e. So this might be a lead for future improvement as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tC3nE6v4f7", "forum": "UMH1bP9XhO", "replyto": "UMH1bP9XhO", "signatures": ["ICLR.cc/2026/Conference/Submission8585/Reviewer_51Nx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8585/Reviewer_51Nx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762188717052, "cdate": 1762188717052, "tmdate": 1762920432323, "mdate": 1762920432323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}