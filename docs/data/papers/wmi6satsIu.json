{"id": "wmi6satsIu", "number": 13993, "cdate": 1758226602945, "mdate": 1763562247799, "content": {"title": "DGPO: Mitigating Likelihood Displacement with Bidirectional KL Divergence Gap", "abstract": "The current margin-based model alignment method, represented by Direct Preference Optimization (DPO), aims to expand the margin between chosen and rejected responses. However, some works point out the log-probability of chosen response always decreases, thus affecting the likelihood of its generation. This likelihood displacement caused by gradient entanglement is a failure mode for preference optimization and has not been fully resolved. In this paper, we focus on forward and reverse Kullback-Leibler (KL) divergence on the probability distribution of preference pairs to form Divergence Gap Preference Optimization (DGPO). We prove DGPO can promote the increase of the chosen log-probability. Besides, DGPO also maintains a lightweight and automatic manner in real-world alignment. The downstream experimental results demonstrate that DGPO maintains competitive performance across various mainstream benchmarks without the reference model and additional key hyperparameters.", "tldr": "", "keywords": ["Alignment", "Likelihood Displacement", "Gradient Entanglement"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/810638ef270a8f959b50e8b7fc2821da9cfd9626.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a loss function named DGPO to mitigate the phenomenon of decreasing likelihood of positive samples observed in many preference alignment methods. The paper presents the empirical results with enough clarity, but the quality of the rest of the paper is poor (detailed in the weakness section). Although I appreciate the effort, I do not think the paper meets the acceptance bar of ICLR at its current state."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents the experimental settings and results clearly."}, "weaknesses": {"value": "1. Low readability. The paper contains too many typos, grammatical errors, and notation ambiguities. The most serious one is the notation ambiguity in the proposed loss function (Equation 10). It is unclear what this loss function really means. The context seems to suggest that $\\pi_w$ and $\\pi_l$ represent $\\pi(y_w)$ and $\\pi(y_l)$. If so, then using $D_{KL}$ is an abuse of notation because $\\pi_w$ and $\\pi_l$ are not distributions but two likelihoods.\n2. Questionable motivation. The paper situates itself as a method to alleviate the \"problem\" of the decreasing likelihood of positive samples. However, whether it is really a problem is still debatable in the first place. Literature arguing against this (such as [1]) is unfairly ignored by the paper. \n3. Lack of meaningful baselines. Even if we assume that the suggested problem is indeed a problem, there are already a plethora of methods available, e.g., [2] can be a strong and meaningful baseline. However, the paper only compares the proposed method with relatively weak baselines that are not intended as cures to the decreasing likelihood of positive samples.\n\n------\n[1] Rafailov et al, 2024, From r to Q∗: Your Language Model is Secretly a Q-Function\n\n[2] Chen et al., 2024, Noise Contrastive Alignment of Language Models with Explicit Rewards"}, "questions": {"value": "I do not have meaningful questions to ask besides the serious problems as I have listed in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gtv9Sby929", "forum": "wmi6satsIu", "replyto": "wmi6satsIu", "signatures": ["ICLR.cc/2026/Conference/Submission13993/Reviewer_8rtp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13993/Reviewer_8rtp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555985395, "cdate": 1761555985395, "tmdate": 1762924489851, "mdate": 1762924489851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "# Common Concerns Addressed\n\n## 1.Baselines(Reviewer nUXw W1&W2.1&W2.2, Reviewer ohV4 W4&W5, Reviewer 8rtp W3)\nThank reviewers for highlighting some works, and we clarify that our baselines selection is strictly grounded in DGPO’s core focus: **mitigating likelihood displacement caused by gradient entanglement in margin-based preference alignment, a theoretical framework we detail in Section 2.** We intentionally choose these methods as baselines because they all fit the standard margin-based alignment paradigm (Table 2) and process gradient entanglement condition in different ways. This ensures fair, in-paradigm comparisons: DGPO not only addresses gradient entanglement more reasonably but also retains lightweight design while matching their downstream performance.\nThese related works does not fit the \"margin-based alignment + corresponding gradient entanglement condition mitigation\" scope of our work. Their design paradigms and targeted mechanisms differ fundamentally from DGPO’s. So comparing them would not reflect DGPO’s meaning in solving the specific failure mode we focus on. **DGPO achieves a practical balance—dealing with gradient entanglement condition in a more reasonable way to mitigate likelihood displacement, simplifying deployment, and retaining competitive effectiveness rather than pursuing SOTA performance**.\n\n## 2.Derivation(Reviewer nUXw W3, Reviewer x6EF W2, Reviewer 8rtp W1)\n**We apologize for the over-simplification of some notations and derivation processes rather than factual flaws, leading to a misunderstanding.** $\\pi_w$ and $\\pi_l$ are actually $\\pi_\\theta(y_{w} |x)$ and $\\pi_\\theta(y_{l} |x)$ (the chosen and rejected response probability).\n\nNext, we demonstrate the core derivation of DGPO:\n\nThe targets of these two strategies in DGPO are the token probability distributions for each corresponding position in the chosen and rejected responses. Hence, for Eq.8 and Eq.9 at the $i$-th token:\n\n\\begin{equation}\n\\theta_{w,i} := \\max\\left(D_{KL}\\left(\\pi_\\theta(\\cdot | x, y_w^{<i}) \\parallel \\pi_\\theta(\\cdot | x, y_l^{<i})\\right)\\right)\n\\end{equation}\n\\begin{equation}\n\\theta_{l,i} := \\min\\left(D_{KL}\\left(\\pi_\\theta(\\cdot | x, y_l^{<i}) \\parallel \\pi_\\theta(\\cdot | x, y_w^{<i})\\right)\\right)\n\\end{equation}\n\nwhere $\\pi_\\theta(\\cdot | x, y_w^{< i})$ and $\\pi_\\theta(\\cdot |x, y^{< i}_{l})$ are the probability distributions of the chosen and rejected responses at $i$-th token. Then we integrate two strategies to form sequence-level KL divergence control:\n\n\n\\begin{equation}\n\\begin{aligned}\nL_{DGPO_1} &= -\\\\text{log} \\\\sigma \\left( \\\\beta D_{SeqKL} \\left(x,y,\\\\pi_w \\parallel \\\\pi_l \\right) - \\\\beta D_{SeqKL} \\left( x,y,\\\\pi_l \\parallel \\\\pi_w \\right) \\right ) \\\\\\\\\n&= -\\\\text{log} \\\\sigma \\left( \\\\beta \\\\sum_{i=1}^{n} D_{KL}( \\\\pi_\\\\theta(\\\\cdot | x, y_w^{<i}) \\parallel \\\\pi_\\\\theta(\\\\cdot | x, y_l^{<i})) - \\\\beta \\\\sum_{i=1}^{n} D_{KL}( \\\\pi_\\\\theta(\\\\cdot | x, y_l^{<i}) \\parallel \\\\pi_\\\\theta(\\\\cdot | x, y_w^{<i})) \\right ) \\\\\\\\\n&= -\\\\text{log} \\\\sigma \\left( \\\\beta \\\\sum_{i=1}^{n} \\\\left( \\\\text{log} \\\\frac{\\\\pi_\\\\theta(y_{w,i} | x, y_w^{<i})}{\\\\pi_\\\\theta(y_{l,i} | x, y_l^{<i})} - \\\\text{log} \\\\frac{\\\\pi_\\\\theta(y_{l,i} | x, y_l^{<i})}{\\\\pi_\\\\theta(y_{w,i} | x, y_w^{<i})} \\\\right) \\right ) \\\\\\\\\n&= -\\\\text{log} \\\\sigma \\left( 2\\\\beta \\\\sum_{i=1}^{n} \\\\left( \\\\text{log} \\\\pi_\\\\theta(y_{w,i} | x, y_w^{<i}) - \\\\text{log} \\\\pi_\\\\theta(y_{l,i} | x, y_l^{<i}) \\\\right) \\right ) \\\\\\\\\n&\\Leftrightarrow -\\\\text{log} \\\\sigma \\left( \\\\beta ( \\\\text{log} \\\\pi_\\\\theta(y_w | x) - \\\\text{log} \\\\pi_\\\\theta(y_l | x) ) \\right) \\\\text{(We put 2 into $\\\\beta$)}\n\\end{aligned}\n\\end{equation}\n\nHere You can clearly see this standard margin-based form. **It is worth noting that DGPO needs to perform stable policy updates implicitly given the absence of the reference model and additional hyperparameters. We further apply an adaptive weight $(\\pi_w+\\pi_l)$ in Eq.11**:\n\n1)When the model has a high probability of both chosen and rejected responses for a certain prompt, it indicates that as $(\\pi_w+\\pi_l)$ increases, the model will increase its intensity to expand the margin.\n\n2)When the model has a low probability of both chosen and rejected responses a certain prompt, it indicates that the model's chosen and rejected response probability estimates for this prompt are unreliable, and may have deviated from the initial reasonable distribution. At this time, $(\\pi_w+\\pi_l)$ becomes smaller, and the model will weakly optimize this prompt to avoid further distribution shift.\n\n**We supplement all the above content in the revised version.**\n\n## 3.Typos(Reviewer nUXw Q2, Reviewer x6EF Q2, Reviewer 8rtp Q1)\nThank all the reviewers for providing valuable feedback on grammar errors and other issues. DGPO is elegant and we make unified modifications seriously in the revised version for better readability."}}, "id": "CYhhDTiwLL", "forum": "wmi6satsIu", "replyto": "wmi6satsIu", "signatures": ["ICLR.cc/2026/Conference/Submission13993/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13993/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13993/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763302497527, "cdate": 1763302497527, "tmdate": 1763302497527, "mdate": 1763302497527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new measure for preference optimization that weights \\log\\pi_w and \\log\\pi_l by the sum (\\pi_w+\\pi_l)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Gradient entanglement in DPO-like measures is an important problem."}, "weaknesses": {"value": "Innovation:\n\nAll of the derivations in Section 2.1 have been presented more clearly in other papers; they do not need to be presented with this much detail in this paper.\n\nCorrectness:\n\nSections 3.1-3.2 use an incorrect definition of KL divergence.  KL divergence is defined to be the average log ratio between two different probability measures, averaged over all possible token sequences using weights given by the numerator measure.  Eq. (11) shows that you have misinterpreted D(pi_w||pi_l) to be the scalar \\pi_w\\log(\\pi_w/\\pi_l) --- KL divergence is defined to be the average of that quantity over all possible token sequences, not the scalar quantity computed based on a single scalar \\pi_w.\n\nThe reasons given for maximizing Eq. (8) but minimizing Eq. (9) are post-hoc and incoherent.  In fact, the KL divergence from \\pi_w to \\pi_l is undefined, because both \\pi_w and \\pi_l are scalars, not distributions.\n\nIf h_w is a function of two arguments, then its scalar derivative h_w' is no longer well-defined; you must instead explicitly write dh_w/d\\log\\pi_w and dh_w/d\\log\\pi_l.  By making that substitution it is possible to recompute Eqs. (2) and (3) as attempted in Eqs. (12) and (13), but this recomputation is irrelevant, because the linearization in Eq. (4) and (5) is no longer true, so the conditions given in Eqs. (6) and (7) are no longer true."}, "questions": {"value": "Presentation:\n\nThe derivations from Eq. (1) to Eq. (7) are interesting and have been covered in other papers, but they do not make obvious that Delta-log-piw < 0.  One way to make that obvious from these equations is to show that \\Lambda'<0; there may be other interesting cases.\n\nMinor presentation issues:\n\np. 2 both log-probability decrease -> the two log-probabilities decrease\n\nhigh correlation between positive and negative feedback -> high correlation between winning and losing examples\n\nSome works (Yuan et al., 2024a; Razin et al., 2024) have considered to be the reason why -> Some works (Yuan et al., 2024a; Razin et al., 2024) have considered the reason why\n\nEq. (1) is missing a close-paren.  Eq. (1) will be more readable if the symbol for loss is in calligraphic font, and if the symbol for log is in roman rather than italic font.\n\nThe presentation will be easier to follow if Eqs. (2) and (3) are moved after Eqs. (4) and (5) (as explanations of the terms d_w and d_l) rather than before.  Immediately after Eqs. (2) and (3) are presented for the first time, you should specify that prime denotes scalar derivative - this notation is common but not universal.  Immediately after Eqs. (4) and (5) you should specify that eta is the step size."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PrUJGB0hpF", "forum": "wmi6satsIu", "replyto": "wmi6satsIu", "signatures": ["ICLR.cc/2026/Conference/Submission13993/Reviewer_x6EF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13993/Reviewer_x6EF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863592018, "cdate": 1761863592018, "tmdate": 1762924489181, "mdate": 1762924489181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a common failure in preference-based alignment called likelihood displacement, where training with DPO-like objectives lowers the likelihood of both the preferred and rejected responses when the two are very similar, and traces it to gradient entanglement in high-similarity preference data. To address this, it proposes DGPO (Divergence Gap Preference Optimization), a reference-free objective that compares the preferred and rejected responses using a bidirectional KL-divergence gap, effectively reweighting the gradients so the chosen response is more likely to increase in probability even when the pair is similar."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper targets a concrete and currently observed problem in preference-based alignment (likelihood displacement) and clearly links it to gradient entanglement in high-similarity preference data.\n\n* The core idea, using a bidirectional KL-divergence gap to reweight the chosen and rejected responses, is a principled and minimally invasive way to make it more likely that the chosen response’s probability actually increases."}, "weaknesses": {"value": "* Could the authors also evaluate on Arena Hard benchmark, which is an extension of MTBench?\n\n* In Figure2, could the authors also plot the DPO's chosen and reject probability to compare? It would also be nice to see the dynamics of the margin between the chosen and reject across the training. It is hard to see whether the method is effectively mitigating the decrease of chosen probability compared to DPO or other variants like SimPO (although its in Table 3, a visualization would be better).\n\n* As an extension to previous comment, could we Table 3 only shows comparision of chosen. How about the rejected sentence?\n\n* The performance gain seems very minimal compared to DPO and other variants. \n\n* There are a few papers tackling the decrease of chosen probability during direct alignment. For example [1], [2]. The paper lacks comparison with such methods. \n\n[1] https://arxiv.org/abs/2506.12725\n\n[2] https://arxiv.org/abs/2405.16436"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rzi44HBNXe", "forum": "wmi6satsIu", "replyto": "wmi6satsIu", "signatures": ["ICLR.cc/2026/Conference/Submission13993/Reviewer_ohV4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13993/Reviewer_ohV4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985545109, "cdate": 1761985545109, "tmdate": 1762924488760, "mdate": 1762924488760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Divergence Gap Preference Optimization (DGPO) to address the well-known issue of probability decrease in DPO, which is also termed \"likelihood displacement\". \n\nThe authors introduce a new reference-free objective function that effectively weights the standard DPO log-ratio margin by the sum of the chosen and rejected probabilities. This reduces the gradient signal when both probabilities are already low, preventing further displacement. \n\nTheoretical motivation is provided via a simple \"Bidirectional KL Divergence\" theory and experiments are conducted on standard benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Timely Research Direction**\n\nThe focus on mitigating likelihood displacement via objective function reformulation is a valuable direction. I myself was recently considering a variant of this method and was glad to see the authors' exploration in this area, which produces a nice loss function addressing this.\n\n**Practical Efficiency**\n\nThe proposed DGPO method is reference-free, reporting a ~13% GPU memory saving and 25% speedup over DPO. Though it must be said that in practice, the likelihoods can be computed offline (or asynchronously offline) and hence the memory savings of  not storing both \\pi_theta and \\pi_ref may be moot. Also, aren't such gains common to all reference free methods (like Simpo)? \n\n**Addressing a Real Problem**\n\nTargeting likelihood displacement is well-motivated. The intuition behind weighting the gradient by the sum of probabilities $(\\pi_w + \\pi_l)$ is a theoretically simple way to decrease weighting on the RL optimization when the model has already drifted too far from its initial high-likelihood region for a given prompt.\n\n**Multi-turn Robustness**\n\nDGPO appears to maintain performance better than DPO on subsequent turns in multi-turn benchmarks like MT-Bench (Figure 4), particularly in Reasoning and STEM tasks. This suggests the objective may be better for the model's general purpose capabilities than standard contrastive losses."}, "weaknesses": {"value": "**1. Inconsistent gains against older baselines**\nThe empirical advantage over existing reference-free baselines (SimPO) is not clear-cut. In Appendix Figure 9 (Llama-3-8B), DGPO slightly beats SimPO on Length-Controlled Win Rate but loses to SimPO on raw Win Rate (43.3% vs 44.4%). Given that SimPO is a baseline from early 2024, a new method for ICLR 2026 should ideally demonstrate decisive improvements over it.\n\n**2.1 Missing Literature on Reference Free DPO baselines**\nLikelihood displacement, reference free DPO, and the related issue of model bias post DPO training have been active areas of research recently. The paper omits some recent works that also address these issues, sometimes with superior empirical results. For example, RefA [1] explicitly tackles length bias (a symptom of displacement) via reference-free token-level regularization. Game-theoretic approaches [3] and multi-preference optimized objectives [2] also mitigate these standard DPO failure modes, sometimes achieving higher win-rates.\n\n**2.2 Outdated Performance Ceiling**\nWhile DGPO improves over vanilla DPO, its absolute performance (~43% WR on Llama-3-8B) is significantly below the current state-of-the-art. The aforementioned recent methods [1, 2, 3] have achieved win rates between 50% and 60% on AlpacaEval 2 using similar base models and training data.\n\n**Suggestion:** I recommend placing DGPO in the context of these more recent, and relevant baselines. \n\n**3. Theoretical Derivation**\nThe connection between the Bidirectional KL theory and the final loss function needs more fleshing out, perhaps a rewrite to help me see it. Currently, it seems somewhat heuristic and can at best be termed a motivation rather than directly derived. The jump from minimizing forward/reverse KL to specifically weighting the margin by the scalar sum $(\\pi_w + \\pi_l)$ relies on simplifying assumptions that may not fully hold for complex sequence distributions. Can the authors consider a derivation in the style of the one taken up in the DPO paper. This would greatly strengthen their work.\n\n---\n\nShould these concerns be addressed, I would certainly consider raising my score.\n\n\n**References**\n[1] Gupta, T., et al. (2025). REFA: Reference Free Alignment with Fine-Grained Length Control. COLM 2025.\n[2] Gupta, T., et al. (2025). AMPO: Active Multi Preference Optimization for Self-play Preference Selection. ICML 2025.\n[3] Tang, X., et al. (2025). Game-Theoretic Regularized Self-Play Alignment of Large Language Models. arXiv preprint arXiv:2503.00030."}, "questions": {"value": "**SimPO Comparison:** In your Llama-3-8B results (Figure 9), DGPO achieves a lower raw win rate than SimPO (43.3% vs 44.4%). Why does your method underperform the simpler SimPO baseline? Could the issue be related to noise on the benchmark, or lack of hyperparameter tuning?\n\n\n### Minor Suggestion:\n\nPlease consider a language rewrite/polish. The paper could be polished to an even greater degree for readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8Rrs7JaDMI", "forum": "wmi6satsIu", "replyto": "wmi6satsIu", "signatures": ["ICLR.cc/2026/Conference/Submission13993/Reviewer_nUXw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13993/Reviewer_nUXw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762036960465, "cdate": 1762036960465, "tmdate": 1762924488326, "mdate": 1762924488326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}