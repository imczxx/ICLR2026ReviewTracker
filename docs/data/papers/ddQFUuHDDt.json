{"id": "ddQFUuHDDt", "number": 22240, "cdate": 1758328198872, "mdate": 1759896878332, "content": {"title": "When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework", "abstract": "We investigate the challenge of applying Large Language Models (LLMs) to long texts. We propose a theoretical framework that distinguishes the failure modes of long context tasks into three categories: cross-chunk dependence (task noise), confusion that grows with context size (model noise), and the imperfect integration of partial results (aggregator noise). Under this view, we analyze when it is effective to use multi-agent chunking, i.e., dividing a length sequence into smaller chunks and aggregating the processed results of each chunk. Our experiments on tasks such as retrieval, question answering, and summarization confirm both the theoretical analysis and the conditions that favor multi-agent chunking. By exploring superlinear model noise growth with input length, we also explain why, for large inputs, a weaker model configured with chunk-based processing can surpass a more advanced model like GPT4o applied in a single shot. Overall, we present a principled understanding framework and our results highlight a direct pathway to handling long contexts in LLMs with carefully managed chunking and aggregator strategies.", "tldr": "", "keywords": ["Long Context", "Multi-agent", "LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0dd93e26421dde61d656457d4cb008f6347dac6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a theoretical framework that decomposes the errors in long-context LLM processing into three types of noise: task noise, model noise, and aggregator noise. The empirical results demonstrate that, when task noise is controllable, chunking processing can effectively suppress the superlinear growth of model noise. Ultimately, this framework elucidates why, when dealing with ultra-long contexts, a weaker model employing a divide-and-conquer strategy can outperform the single-pass processing performance of a stronger model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper presents a noise decomposition framework, proposing model noise, task noise, and aggregator noise.\n2. The experimental design of the paper is ingenious, with its objective not being to blindly pursue the sota performance but rather to provide validation for the theoretical framework.\n3. The paper devises a well-developed system to validate this theoretical framework, which comprises a planner, a worker agent, and a manager agent."}, "weaknesses": {"value": "1. Although Appendix N attempts to alleviate concerns about latency by assuming parallelism, this is often impractical in real-world deployments, which undermines its guidance value in engineering practice.\n2. The experiments solely rely on basic methods such as BM25 and embeddings, and then draw the conclusion that the performance of RAG is unsatisfactory. This may underestimate the capabilities of current RAG systems. For instance, semantic chunking or LLM-based chunking could be employed to optimize chunking, and superior semantic retrieval methods could be utilized."}, "questions": {"value": "1. Can a detailed argumentation be provided regarding why the multiplicative operator is chosen as the foundation of the theoretical framework, for example, in Section 3.2?\n2. In Section 4, it is mentioned that the planner has the capability of iterative optimization. Can evidence be provided to demonstrate that this iterative optimization indeed outperforms the prompts generated by the zero-shot planner on unseen test sets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "67b3vq97Aw", "forum": "ddQFUuHDDt", "replyto": "ddQFUuHDDt", "signatures": ["ICLR.cc/2026/Conference/Submission22240/Reviewer_njn8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22240/Reviewer_njn8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752546683, "cdate": 1761752546683, "tmdate": 1762942130349, "mdate": 1762942130349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to explore the applicable boundaries of the divide-and-conquer strategy in long-context tasks. The core of this framework attributes the failure modes in long-text processing to the interaction among three key error sources: task noise, model noise, and aggregator noise. Through modeling and empirical analysis, the study provides insights into when chunking strategies should be adopted."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper not only defines aggregator noise but also demonstrates in practice how to reduce this type of noise by introducing a planner.\n2. The noise framework proposed in the paper possesses diagnostic capabilities. By analyzing the relative dominance of the three types of noise, it divides long-context tasks into three distinct regions. This aids in determining whether a specific task is more suitable for the divide-and-conquer approach.\n3. The paper offers a possible explanation for the phenomenon where weaker models combined with the divide-and-conquer method can outperform top-tier single-pass models in ultra-long contexts."}, "weaknesses": {"value": "1. It is commendable that the paper attempts to provide a scientific explanation from a theoretical perspective. However, when constructing its core theoretical framework (Sections 3.1 and 3.2), the paper lacks rigor in the use of mathematical operators. There is no explanation as to why multiplication and addition can be directly performed in the output space. For instance, what does the product of the results of two functions imply? This is puzzling and weakens the persuasiveness of the entire theoretical framework.\n2. The validity of the whole framework relies on the super-linear growth hypothesis. Nevertheless, the paper fails to derive this growth pattern from the attention mechanism or information flow bottleneck of the Transformer. Using the method of elimination is difficult to provide a scientific explanation for this phenomenon.\n3. The planner proposed in the paper also has similar role assignment and task planning modules in other multi-agent systems, lacking innovation. The theoretical contribution merely serves as a qualitative metaphor rather than a verifiable mathematical model."}, "questions": {"value": "1. Is this mathematical representation strictly valid, or is it merely a formal analogy? If it is an analogy, to what extent does the results derived from such a non-rigorous mathematical form still retain its theoretical validity?\n2. In Sections 5.3 and 5.4 of the paper, integrating and differentiating discrete variables is extremely puzzling, and it weakens the validation strength of the experimental part for the theoretical framework. This part of the content also requires detailed clarification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iC9eNjZQgc", "forum": "ddQFUuHDDt", "replyto": "ddQFUuHDDt", "signatures": ["ICLR.cc/2026/Conference/Submission22240/Reviewer_TKHf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22240/Reviewer_TKHf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817981667, "cdate": 1761817981667, "tmdate": 1762942129845, "mdate": 1762942129845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a theoretical framework that decomposes the error of long context solutions, including one-shot single agent and divide-and-conquer multi-agent, into three parts: task, model, and aggregator errors. Task demonstrates the errors of cross-chunk dependency, model indicates errors with growing task length, and aggregator indicates errors during aggregating results of subagents. Then, the authors propose a simple multi-agent framework and evaluate the real data. Both theory and experiments showed that chunking can outperform single-shot usage in many tasks where superlinear noise growth appears with context length. However, if cross-chunk synergy is too large or the aggregator prompt is ineffective, chunk-based approaches may fail."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Long context tasks are important as many real-world questions are based on long context.\n2. The theoretical analysis gives unique evidence to show that chunking is better than single-agent. This conclusion is non-trivial and might change the understanding of users of Long-LLMs. \n3. Experiments further echo the theory and give a more fine-grained analysis of the problem."}, "weaknesses": {"value": "I think some settings are over-simplified. For instance, there are a lot of different structures in multi-agent systems, for instance, there are chains, trees, and graphs. However, the theoretical analysis and experiment directly assume the agents (chunks) are independent, where the failure of one agent will not infect its sublings, which is commonly seen in chains. Next, the work in a multi-agent system is different from one-shot agent as they have different tasks to finish (summary vs. generating an answer). Thus, their performance curve might not be the same. Third, it is also assumed that the answers are evenly distributed in the question. However, in many datasets, such as QA, the answer is only in one or two chunks. I suggest that the authors carefully define the problem setting and show how strong the assumption is."}, "questions": {"value": "I am curious if the theory can be applied to the Agentic workflow system as well. This can futher enhance the conclusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BBoVLVFJE2", "forum": "ddQFUuHDDt", "replyto": "ddQFUuHDDt", "signatures": ["ICLR.cc/2026/Conference/Submission22240/Reviewer_vy5A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22240/Reviewer_vy5A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983480443, "cdate": 1761983480443, "tmdate": 1762942129457, "mdate": 1762942129457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a divide-and-conquer (D&C) framework for long-context LLM tasks that decomposes error into three sources: **task noise** (cross-chunk dependencies), **model noise** (degradation as input length grows), and **aggregator noise** (errors when fusing partial outputs). The central claim is that when input length is large, model noise grows **superlinearly**, so chunking with a well-designed aggregator can outperform single-shot inference—even with weaker worker models. The authors provide a planner–workers–manager implementation, experiments on retrieval/QA/summarization and others, and a cheap sampling method to estimate near-optimal chunk size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Unifying perspective.** The three-noise decomposition offers a clear lens for deciding when to chunk, how to chunk, and how to aggregate. \n* **Actionable empirical takeaways.** The paper documents superlinear performance decay with length and shows cases where chunking beats single-shot, provided aggregator noise is controlled. \n* **Concrete implementation.** The planner/manager/worker design and prompt scaffolding make the approach reproducible in spirit and show how aggregator strength materially affects outcomes.  \n* **Efficient chunk-size selection.** A low-budget sampler often identifies the same chunk size as exhaustive grid search at 128K tokens on QA-IB and summarization."}, "weaknesses": {"value": "* **Superlinearity is argued largely by phenomenon, not direct estimation.** The paper builds the thesis from diagnostic curves and counter-hypotheses, but does not directly fit an exponent for error growth vs. length with uncertainty quantification. Stronger statistical evidence would help. \n* **Metric/assumption clarity.** Parts of the theoretical setup and noise interactions would benefit from clearer units/assumptions and closer alignment with common additive error analyses (currently the exposition is somewhat abstract). \n* **Baselines and fairness.** The headline claim (“weak D&C > strong single-shot”) should be compared under **matched token/compute/latency budgets**, with significance tests. Aggregator comparisons risk “straw-man” effects if manual prompts are weak; stronger retrieval- or structure-aware aggregators should be included. (The appendix lists a compute/latency section, but main-text results under unified budgets are limited.)"}, "questions": {"value": "1. Can you re-run the “weak D&C beats strong single-shot” comparisons under **matched token/compute/wall-clock** budgets and report statistical significance? This is crucial for the main claim. \n2. Can you **directly estimate** the superlinearity exponent (e.g., error ∝ L^α with CI) across models/tasks rather than primarily relying on diagnostic exclusion? \n3. How do results change with **stronger aggregators** (retrieval-augmented fusion, entity/event graph alignment, structured parsers) and with carefully **matched prompt lengths/few-shot examples** across baselines? \n4. The appendix motivates **equal-length, non-overlapping** splits to control variables. Have you evaluated robustness to boundary placement, small overlaps, or adaptive routing at similar budgets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V5HztT3lwP", "forum": "ddQFUuHDDt", "replyto": "ddQFUuHDDt", "signatures": ["ICLR.cc/2026/Conference/Submission22240/Reviewer_YK8x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22240/Reviewer_YK8x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762436707650, "cdate": 1762436707650, "tmdate": 1762942129214, "mdate": 1762942129214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}