{"id": "3d0FRYx0D0", "number": 22231, "cdate": 1758328061023, "mdate": 1760126160145, "content": {"title": "CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics", "abstract": "We introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark. CMPhysBench is composed of more than 520 graduate-level meticulously curated questions covering both representative subfields and foundational theoretical frameworks of condensed matter physics, such as magnetism, superconductivity, strongly correlated systems, etc. To ensure a deep understanding of the problem-solving process,we focus exclusively on calculation problems, requiring LLMs to independently generate comprehensive solutions. Meanwhile, leveraging tree-based representations of expressions, we introduce the Scalable Expression Edit Distance (SEED) score, which provides fine-grained (non-binary) partial credit and yields a more accurate assessment of similarity between prediction and ground-truth. Our results show that even the best models, Grok-4, reach only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a significant capability gap, especially for this practical and frontier domain relative to traditional physics.", "tldr": "", "keywords": ["LLM Benchmark", "Condensed Matter Physics", "LLM Evaluation", "AI for Physics"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07e63cec19614b70b0e0bfb5670a0180d1e24437.pdf", "supplementary_material": "/attachment/7454ba38bf90b23ca876f4c77f54f001a24045f8.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces CMPhysBench, a benchmark designed to evaluate LLMs for Condensed Matter Physics (CMP) problems. The benchmark consists of 520 graduate-level calculation problems, spanning subfields such as magnetism, superconductivity, semiconductors, and strongly correlated systems. Compared to prior benchmarks, CMPhysBench emphasizes symbolic reasoning. A key contribution is the SEED metric, which extends existing expression-based similarity measures to support multiple answer types through syntax tree representations. The authors evaluate 18 proprietary and open-source LLMs, finding that even top models like Grok-4 achieve only 28% accuracy, suggesting a significant gap in domain-specific reasoning for CMP."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The dataset is a good contribution, as it is manually curated by domain experts and spans diverse CMP subfields.\n\n- SEED provides a novel measure of partial correctness, addressing a gap in symbolic assessment.\n\n- The evaluation across 18 models is comprehensive, offering generalizability and comparative insight.\n\n- Evaluation with human alignment validation and the error analysis with detailed categorization of reasoning failures helps to verify the findings."}, "weaknesses": {"value": "- Although the error analysis is informative, the reasoning behind LLM failures remains speculative rather than causal.\n\n- Maybe it's because CMP is a very specific area beyond my research scope. I find the dot examples in the figures very confusing. If a bit more background can be provided to explain the meaning of those symbols, it will help non-experts to better understand the paper."}, "questions": {"value": "- Could the SEED metric be adapted to multimodal problems involving diagrams or figures in physics textbooks?\n\n- Would you expect domain-specific fine-tuned LLMs (e.g., physics-trained models) to exhibit qualitatively different failure modes than general LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2sC539xB5D", "forum": "3d0FRYx0D0", "replyto": "3d0FRYx0D0", "signatures": ["ICLR.cc/2026/Conference/Submission22231/Reviewer_TCqD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22231/Reviewer_TCqD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575022738, "cdate": 1761575022738, "tmdate": 1762942127540, "mdate": 1762942127540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Graduate level condensed matter physics benchmark manually curated in high detail. The authors main contributions are (1) mix of LLM and expert question generation (2) presentation of a new metric SEED which helps to reward both partial correctness as well as (intended) to help with symbolic equivalence matching of answers, correlating well with human preference, (3) evaluation of several SoTA models on CMPhysBench showing that models currently struggle with condensed matter physics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1, Scope and subject: Graduate level benchmark based on standard graduate textbooks, requiring complex step-by-step solutions across diverse answer types. Specifically, CMYPhysBench is a non MC/QA benchmark, so much more difficult.\n\n2. Diversity and coverage: Clear balance between categories, and clear explanation and validation of the source material from which benchmark is derived from. The authors also perform strong analysis on failure modes, which are possibly actionable and of interest to members of the AI for science community trying to understand LM capabilities on scientific domains (and when to (or not) use models for specific scientific tasks).\n\n3. SOTA LLMs moderately struggle on this benchmark making it of key interest to the community and interesting for marking LLM progress over time."}, "weaknesses": {"value": "1. Relevance of SEED as an benchmark evaluation metric versus actual accuracy.\n\nThe goal seems to reward partial correctness (which is understandable from an RL or intermediate reward feedback perspective), however in practice: does SEED actually properly weight when LMs make minor incorrect reasoning steps (or does it only purely give partial credit when LMs fail to decode a final correct answer)? Some more discussion on this would be helpful.\n\nRelated, from model thinking trajectories, how well does SEED correlate with minor incorrect steps in reasoning: I’m worried that SEED partial correctness of answer may not correlate with minor reasoning incorrectness, which may defeat some of the benefit of this partial correctness.\n\nRelated, is it true that partial misses in edit distance should be only partially penalized just at an AST level?\n\n2. Human preference seems to be binary (0 or 1): are there studies on how accurate human labelling is here? Were questions + ground truth answers verified by multiple raters? As such would the human grading here be close to an “accuracy” or a “preference” type of statistic?"}, "questions": {"value": "Figure 6 very difficult to read even while zoomed in (Model text and also the colors)\n\nFigure 4: I’m unclear on how SEED is actually computed here, where does the 60 in Model Response 1 come from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WaypmEnGKZ", "forum": "3d0FRYx0D0", "replyto": "3d0FRYx0D0", "signatures": ["ICLR.cc/2026/Conference/Submission22231/Reviewer_K4Jm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22231/Reviewer_K4Jm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890129401, "cdate": 1761890129401, "tmdate": 1762942127224, "mdate": 1762942127224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CMPhysBench, a new benchmark to test LLMs on graduate-level Condensed Matter Physics. It's made of 520+ hard calculation problems from textbooks. They also created a new scoring metric called SEED that gives partial credit for complex math answers (like equations or tuples) using tree-based analysis. Their tests on 18 LLMs show that even the best models, like Grok 4, perform poorly (36 SEED score, 28.9% accuracy), showing a big gap in this specific domain."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper's main strength is tackling a new, hard domain: graduate-level condensed matter physics. Most benchmarks are easier, so this is a needed step up. The SEED metric is also a big plus; it's a smart way to give partial credit on complex math answers instead of just right/wrong. This metric seems useful for other science benchmarks too. The testing of 18 models is thorough, and the error analysis in Figure 6 gives a good breakdown of why models fail, with \"Concept and Model Misuse\" being the biggest problem."}, "weaknesses": {"value": "The main weakness I see is in the error analysis. The authors used GPT-4o to categorize all the model mistakes. While this is fast, it's not clear how accurate GPT-4o is at this task. It would be better if they had human experts check a sample of these to confirm the error breakdown. Also, the SEED score focuses on the final boxed answer. The prompt asks for step-by-step solutions, but it's not clear if the steps themselves are evaluated. A model could get the right answer with the wrong steps."}, "questions": {"value": "For the error analysis, how do you know GPT-4o's categorizations are correct? Did you have any humans double-check its work? Does your evaluation look at the reasoning steps, or just the final answer in the box? It seems possible for a model to get the right answer by luck or by making mistakes that cancel out."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cj56oo5WDr", "forum": "3d0FRYx0D0", "replyto": "3d0FRYx0D0", "signatures": ["ICLR.cc/2026/Conference/Submission22231/Reviewer_nemX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22231/Reviewer_nemX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977881250, "cdate": 1761977881250, "tmdate": 1762942126945, "mdate": 1762942126945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces CMPhysBench, a novel benchmark that successfully establishes a rigorous standard for evaluating LLMs on advanced scientific reasoning tasks in physics. The adoption of a fine-grained evaluation protocol is a major contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. High Problem Difficulty\n\nFocuses exclusively on graduate-level material, comprising more than 520 meticulously curated questions that require LLMs to generate complete, step-by-step solutions for complex calculation problems. This moves beyond the limitations of high school or undergraduate benchmarks, demanding advanced mathematical rigor and conceptual understanding.\n\n2. Expert-Aligned Metric\n\nThe proposed Scalable Expression Edit Distance (SEED) score provides highly accurate, fine-grained, non-binary partial credit for mathematical responses. SEED exhibits the highest correlation (ρ = 0.90) with human expert ratings, demonstrating superior alignment in evaluating complex symbolic reasoning compared to prior metrics.\n\n3. Extensive Model Analysis\n\nThe paper conducts a comprehensive empirical study evaluating 18 proprietary and open-source LLMs. This extensive analysis identifies a significant capability gap, with the best models achieving only a 36 average SEED score and 28% accuracy, providing quantitative illumination of specific failure modes across the LLM ecosystem."}, "weaknesses": {"value": "1. It would be better if the authors could discuss how the issues of LLM in this domain identified in the analysis could be mitigated in the future research. Currently, the analyses only show LLM can make multiple types of error and it is still unclear how to improve LLM to avoid such errors. Proposing potential solutions for the identified errors could further improve the contribution of the paper."}, "questions": {"value": "1. please add additional analyses or discussions towards future research directions or the contribution of this paper towards the future improvements of LLM in this domain."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fW33GKOJ9s", "forum": "3d0FRYx0D0", "replyto": "3d0FRYx0D0", "signatures": ["ICLR.cc/2026/Conference/Submission22231/Reviewer_aQN2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22231/Reviewer_aQN2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996694487, "cdate": 1761996694487, "tmdate": 1762942126653, "mdate": 1762942126653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}