{"id": "5eTpRIULtb", "number": 24663, "cdate": 1758359099139, "mdate": 1759896756164, "content": {"title": "Flow2GAN: Hybrid Flow Matching and GAN with Multi-Resolution Network for One-/Two-step High-Fidelity Audio Generation", "abstract": "Existing dominant methods for audio generation include Generative Adversarial Networks (GANs) and diffusion-based methods like Flow Matching. GANs suffer from slow convergence and potential mode collapse during training, while diffusion methods require multi-step inference that introduces considerable computational overhead. In this work, we introduce Flow2GAN, a two-stage framework that combines Flow Matching training for learning generative capabilities with GAN fine-tuning for efficient one-/two-step inference. Specifically, given audio's unique properties, we first improve Flow Matching for audio modeling through: 1) reformulating the objective as endpoint estimation, avoiding velocity estimation difficulties when involving empty regions; 2) applying spectral energy-based loss scaling to emphasize perceptually salient quieter regions. Building on these Flow Matching adaptations, we demonstrate that a further stage of lightweight GAN fine-tuning enables us to obtain one-step and two-step generators that produce high-quality audio. In addition, we develop a multi-branch network architecture that processes Fourier coefficients at different time-frequency resolutions, which improves the modeling capabilities compared to prior single-resolution designs. Experimental results indicate that our Flow2GAN delivers high-fidelity audio generation from Mel-spectrograms or discrete audio tokens, outperforming existing state-of-the-art GAN-based and Flow Matching-based methods in both quality and efficiency. Online demo samples are available at \\url{https://flow2gan.github.io}.", "tldr": "", "keywords": ["Flow2GAN", "audio generation", "Flow Matching", "GAN", "multi-resolution"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee11e93a9f75f61df8dba7ae0fccfe9471c062d5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Flow2GAN, a framework that combines flow matching and GAN for efficient one- and two-step neural vocoder generation. It employs a robust flow matching training strategy based on end-point estimation and spectral energy–adaptive loss scaling to achieve high-fidelity waveform synthesis."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[CFM Training Optimization for Robust Waveform Generation]\n\nThis paper replaces vector field estimation with end-point estimation to achieve more robust waveform generation. In addition, the loss is scaled by spectral energy. Table 3 demonstrates the effectiveness of the proposed optimization method for high-fidelity waveform synthesis.\n\nHowever, the analysis in Figure 2 is limited to the spectral domain. While end-point estimation combined with spectral energy–based scaling can improve training efficiency in the early stages and enhance spectral-domain performance, it would be beneficial to further evaluate the model using waveform-level metrics. I understand that standardized waveform-level metrics are lacking, but I recommend including MOS evaluations in Tables 1 and 2 to provide a more comprehensive assessment of perceptual quality."}, "weaknesses": {"value": "While I like the concept of this paper, my primary concern lies in its lack of novelty.\n\n[End-Point Estimation]\n\nPrevious works such as PeriodWave and RFWave have already demonstrated the effectiveness of conditional flow matching for high-fidelity waveform generation. The proposed end-point estimation seems to be a relatively simple modification. Moreover, several recent models, including sCM, have adopted similar end-point estimation schemes for few-step generation.\n\n[Spectral Energy Scaling]\n\nMany works already adopt an energy-based scaling. PriorGrad uses a data-dependent prior based on energy, and they scale the loss with this energy. RFWave also adjusts the loss according to the target's standard deviation. \n\nPlease add further discussion with previous scaling methods. Also, I have a concern about that it only reflect the spectral domain information by removing other information. Did you encounter any issues related to this limitation?\n\n[GAN Fine-tuning]\n\nHonestly, I can not find any difference between your approach and PeriodWave-Turbo which has already shown the efficiency of GAN post-training for CFM-based waveform generation. The paper seems to overclaim novelty in Section 4.2 regarding this aspect."}, "questions": {"value": "[Q1. Multi-Resolution Network]\n\nAre the inverted outputs from different iSTFT branches simply added together or averaged?\n\n[Q2. Further Steps with GAN Fine-Tuning]\n\nHave you experimented with training four-step or more generators after GAN fine-tuning? It would be interesting to see how performance scales with additional sampling steps.\n\n[Q3. EnCodec Audio Token Experiments]\n\nCould you include PeriodWave-Turbo in Table 2? PeriodWave-Turbo released checkpoints for the same EnCodec-based experiments on their GitHub, so a direct comparison would strengthen your results.\n\n[Q4. TTS Experiments]\n\nPlease consider adding TTS results comparing various neural vocoders. Since some vocoders might overfit to ground-truth Mel-spectrograms, conducting two-stage TTS experiments (The generated Mel from TTS Models to waveform) would enhance the quality of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zGjHJuOd8i", "forum": "5eTpRIULtb", "replyto": "5eTpRIULtb", "signatures": ["ICLR.cc/2026/Conference/Submission24663/Reviewer_X1ZZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24663/Reviewer_X1ZZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760865895158, "cdate": 1760865895158, "tmdate": 1762943155838, "mdate": 1762943155838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Flow2GAN introduces a two-stage framework combining Flow Matching (FM) pre-training and GAN fine-tuning to balance audio generation quality and inference speed. Key innovations include endpoint-prediction-based FM (to handle silent regions), spectral energy-adaptive loss scaling (for perceptual alignment), and a multi-resolution ConvNeXt network. Experiments show competitive performance against SOTA models, though critical gaps exist in comparisons to key competitors and hybrid FM+GAN baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Hybrid Paradigm: Effectively merges FM’s stable training with GAN’s detail refinement, resolving FM’s slow inference and GAN’s mode collapse issues.\nAudio-Specific FM Improvements: Endpoint prediction and energy-adaptive loss significantly boost FM’s performance, validating their utility for audio synthesis.\nStrong Experimental Results: Outperforms Vocos, RFWave, and WaveFM on most metrics (Table1/2) for Mel-spectrogram and Encodec token conditioning.\nMulti-Resolution Network: Enhances frequency modeling compared to single-resolution designs (Table5), improving perceptual audio quality."}, "weaknesses": {"value": "Training Complexity: Two-stage training (FM pre-training + GAN fine-tuning) increases the barrier to reproduction and deployment—users need to manage separate pipelines and hyperparameters for each stage.\n\nIncomplete and Unconvincing Comparison Landscape:\na. Lack of coverage of GAN-enhanced Flow Matching models: The paper claims novelty in its hybrid FM+GAN framework, but many existing models leverage GANs to accelerate Flow Matching. A detailed comparison to these models—including their design choices, performance, and efficiency—is missing, which obscures Flow2GAN’s unique contributions and relative standing.\nb. Performance does not exceed BigVGAN: Even accounting for dataset size differences (BigVGAN uses a larger dataset), the paper’s reported metrics (e.g., PESQ, audio quality scores) do not demonstrate that Flow2GAN outperforms BigVGAN. The claim of being \"state-of-the-art\" is thus unsubstantiated against this key competitor.\nc. Significant speed gap with Vocos: The paper emphasizes inference efficiency but fails to address that its speed is much slower than Vocos (a leading efficient audio generation model). This gap undermines the practical utility of Flow2GAN for real-world applications where low latency is critical."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SH462MR3hP", "forum": "5eTpRIULtb", "replyto": "5eTpRIULtb", "signatures": ["ICLR.cc/2026/Conference/Submission24663/Reviewer_Vxds"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24663/Reviewer_Vxds"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905254136, "cdate": 1761905254136, "tmdate": 1762943155488, "mdate": 1762943155488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Flow2GAN, a two-stage hybrid framework for high-fidelity neural vocoding that aims to combine the training stability of Flow Matching with the detail-refinement capabilities of GANs. The authors first propose improvements to the Flow Matching stage tailored for audio, including reformulating the objective to direct endpoint prediction and introducing a spectral energy-adaptive loss scaling to focus on perceptually important regions. Subsequently, a pre-trained model is used to initialize one- and two-step generators, which are then fine-tuned using adversarial training. The architecture is a multi-resolution network that processes spectral coefficients at different time-frequency scales. Experiments on both mel-spectrogram and audio token conditioning show that Flow2GAN achieves state-of-the-art results in terms of both audio quality and inference efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's core concept of a two-stage training paradigm is well-motivated and presents a clever solution to a known trade-off in generative modeling. The approach logically leverages Flow Matching for robust, global structure learning and then uses a fast GAN fine-tuning stage for refining high-frequency details, which is an effective strategy. The proposed modifications to the Flow Matching objective appear sound; the shift to endpoint prediction is an intuitive way to handle silent regions in audio, and the ablation studies convincingly demonstrate its benefits. The experimental results are another clear strength. The model achieves impressive scores across multiple metrics and tasks, outperforming several strong baselines while offering significantly faster inference than multi-step diffusion models."}, "weaknesses": {"value": "Despite the strong results, the paper has several weaknesses in its positioning and methodological clarity that should be addressed. First, the proposed \"spectral energy-adaptive loss scaling\" is conceptually very similar to the \"energy balanced loss\" used in prior work like RFWave, yet the paper fails to discuss, or compare against it. This omission makes it difficult to assess the novelty of this specific contribution. Second, the reformulation of the prediction target from velocity to endpoint is a significant change to the underlying probability flow ODE. The paper presents a modified sampling equation but does not provide a clear derivation or justification for it, leaving a gap in the methodological explanation.\nFurthermore, a key concern with any GAN-based method is the risk of mode collapse. The paper claims that its two-stage approach mitigates this risk, but this assertion is made without any supporting evidence, either quantitative (e.g., diversity metrics) or qualitative. Finally, while the paper emphasizes its fast one/two-step inference, it critically lacks comparisons to other state-of-the-art fast sampling methods for flow and diffusion models, such as consistency models or recent shortcut models, which are the most relevant competitors for few-step generative performance."}, "questions": {"value": "To help clarify the contributions and rigor of the paper, I would appreciate the authors' response to the following:\n1. Could you please elaborate on the relationship between your proposed spectral energy-adaptive loss scaling and the energy balanced loss from RFWave? How does your approach differ, and what are its specific advantages that lead to the observed performance gains?\n2. When you reformulate the training objective to endpoint prediction (Equation 4), the underlying ODE changes. Could you provide a more thorough derivation or explanation for the new sampling process?\n3. You claim that the Flow Matching pre-training mitigates the risk of GAN mode collapse. This is a significant claim. Could you provide any empirical evidence to support it, for instance, by analyzing the output diversity of Flow2GAN compared to a pure GAN baseline trained for a similar duration?\n4. The main appeal of the model is high-quality generation in one or two steps. Why were there no comparisons against other prominent few-step or single-step generative model sampling strategies, such as consistency models or recent \"shortcut models\" (e.g., as proposed by Frans et al., 2024), which are designed to solve the exact same problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tsX8HF90Zl", "forum": "5eTpRIULtb", "replyto": "5eTpRIULtb", "signatures": ["ICLR.cc/2026/Conference/Submission24663/Reviewer_AhmV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24663/Reviewer_AhmV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920773979, "cdate": 1761920773979, "tmdate": 1762943154171, "mdate": 1762943154171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Flow2GAN is a two-stage framework for high-fidelity audio generation that integrates Flow Matching and GAN. It first uses improved Flow Matching to learn robust generative capabilities, which reformulated as endpoint estimation to avoid velocity prediction issues in empty audio regions and enhanced with spectral energy-adaptive loss scaling to emphasize perceptually important quiet areas. Then, lightweight GAN fine-tuning refines details, enabling efficient one- or two-step inference. Equipped with a multi-resolution network processing Fourier coefficients at different time-frequency resolutions, it outperforms state-of-the-art GAN and Flow Matching-based methods in both quality and efficiency under Mel-spectrogram and Encodec audio token conditioning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The two-stage design effectively combines the stable training of Flow Matching and the efficient fine-grained generation of GAN, addressing the slow convergence/mode collapse of GANs and high computational cost of diffusion methods.\n2. For audio’s unique properties, the authors propose endpoint estimation and spectral energy-adaptive loss scaling to improve Flow Matching, significantly enhancing generation quality in silent regions and perceptual consistency.\n3. The multi-resolution network structure outperforms single-resolution designs in modeling audio complexity, providing a powerful backbone for generative learning."}, "weaknesses": {"value": "1. Compared to BigVGAN-v2 trained on a larger dataset, it still has a slight gap in some metrics, suggesting limitations in generalization to larger-scale data.\n2. The one-step model’s performance at low bandwidth (1.5 kbps) is inferior to its two-step version and some competitors, leaving room for improvement in low-bandwidth audio generation."}, "questions": {"value": "1. What is the majoy difference between the proposed method and PeriodWave-Turbo? Is it just the improved Flow Matching model?\n2. Can this improved Flow Matching strategy be applied to text to speech/audio tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dcM1zTtr4H", "forum": "5eTpRIULtb", "replyto": "5eTpRIULtb", "signatures": ["ICLR.cc/2026/Conference/Submission24663/Reviewer_KvbH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24663/Reviewer_KvbH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991665778, "cdate": 1761991665778, "tmdate": 1762943153607, "mdate": 1762943153607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}