{"id": "OlidGx8oKr", "number": 21017, "cdate": 1758312857040, "mdate": 1759896946874, "content": {"title": "Incentive-Aligned LLM Summaries", "abstract": "Large language models (LLMs) are increasingly used in modern search and answer systems to synthesize multiple, sometimes conflicting, texts into a single response, yet current pipelines offer weak incentives for sources to be accurate and are vulnerable to adversarial content. We introduce Truthful Text Summarization (TTS), an incentive-aligned framework that improves factual robustness without ground-truth labels. TTS (i) decomposes a draft synthesis into atomic claims, (ii) elicits each source’s stance on every claim, (iii) scores sources with an adapted multi-task peer-prediction mechanism that rewards informative agreement, and (iv) filters unreliable sources before re-summarizing. We establish formal guarantees that align a source’s incentives with informative honesty, making truthful reporting the utility-maximizing strategy. Experiments show that TTS improves factual accuracy and robustness while preserving fluency, aligning exposure with informative corroboration and disincentivizing manipulation.", "tldr": "We propose an incentive-aligned, claim-level pipeline for LLM search summaries: score sources via multi-task peer prediction, filter low-scorers, re-summarize—rewarding truthful reporting and discouraging manipulation, without ground-truth labels.", "keywords": ["LLM summarization", "incentive alignment", "truthfulness", "retrieval-augmented generation (RAG)", "peer prediction"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f600a6b1e6160ae473c8d8ea1281bec8d752dd06.pdf", "supplementary_material": "/attachment/d3ca1fb440b10c923bc5e25394040f3d9a9fbf8e.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Truthful Text Summarization (TTS), a multi-document text summarization framework that promotes peer-agreement among the sources on the facts in the summary and scores each source's stance on each atomic claim from the summary. In this way, TTS achieves summary faithfulness in noisy and adversarial conditions (where some documents are manipulative or noisy).\n\nThe authors provide a substantial mathematical framework of peer prediction applied to summarization under incentives.\nThey conduct experiments on two datasets, NaturalQuestions and ClashEval (300-document subsets) which they synthetically augment to contain faithful and unfaithful sources for each query. They use Gemini-2.5-flash as the LLM backbone and compare TTS to 3 baseline methods: Initial Summary (multi-doc summary), Majority Prompt (an LLM summary that only includes majority claims), and Majority Claims (majority claims are used to obtain a re-summary). In automatic evaluation, TTS strongly outperforms all the baselines in terms of Precision, Recall, F1 Score and Accuracy (calculated via an LLM-as-a-Judge)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* a novel technique is introduced that frames summarization with incentive as peer prediction\n* a substantial theoretical framework is presented for informative corroboration among multiple sources\n* the technique outperforms a series of baselines on NaturalQuestions and ClashEval synthetically adapted for the task"}, "weaknesses": {"value": "* evaluation datasets, although originally containing real-world data, heavily rely on data synthesis - both truthful sources (paraphrases) and untruthful ones are synthesized using an LLM. For evaluating a technique that claims to be robust against strategic misinformation / manipulation attacks, this does not seem to be enough\n* evaluation would also be much more complete with human judgements which could expose what rather brittle P, R, F1 & LLMaaJ (a bit less so) cannot capture. ROUGE / BLEU for fluency evaluation are questionable"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lVWPVsVvx1", "forum": "OlidGx8oKr", "replyto": "OlidGx8oKr", "signatures": ["ICLR.cc/2026/Conference/Submission21017/Reviewer_eWFW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21017/Reviewer_eWFW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727304491, "cdate": 1761727304491, "tmdate": 1762999997109, "mdate": 1762999997109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TTS, a mechanism-design-based framework that incentivizes LLMs to prefer truthful over adversarial or uninformative sources during multi-source summarization. The core idea is to apply LOO peer-prediction scoring mechanism that assigns reliability scores to individual sources and rewards summaries consistent with high-reliability evidence. By reframing summarization as an incentive alignment problem rather than a post-hoc filtering task, the paper aims to structurally encourage honesty in LLM-generated summaries."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Reframes summarization robustness as an incentive-alignment problem, introducing a peer-prediction mechanism rarely explored in LLM summarization.\n- Demonstrates measurable suppression of uninformative and adversarial sources, yielding large factual-accuracy improvements.\n- Provides empirical evidence that truthful reporting tends to emerge as the dominant strategy, as dishonest stances consistently reduce expected rewards."}, "weaknesses": {"value": "- Although these assumptions (A1–A3) are standard in peer-prediction theory, they seem overly restrictive and arguably unnecessary for LLM summarization. Real-world sources are correlated through shared training data and retrieval biases, making the independence assumptions theoretically elegant but empirically weak.\n- TTS requires each source to generate, cross-evaluate, and aggregate stances in a leave-one-out manner, leading to a quadratic number of model calls. The paper provides no runtime or scalability analysis, raising doubts about its practical feasibility for large-scale use.\n- The evaluation relies mainly on internal ablations rather than strong external baselines. While TTS is positioned as conceptually distinct from retrieval-robust RAG systems, both aim to improve factual reliability under noisy evidence. Direct or qualitative comparisons with recent methods would better clarify TTS’s relative robustness and contribution."}, "questions": {"value": "- What happens when all sources are unreliable? Does the system abstain or hallucinate?\n- Does the incentive-based scoring scale to multi-hop or long-context summarization?\n- Can TTS be integrated into the retrieval stage to detect adversarial documents earlier and promote a richer set of high-quality, reliable sources from the start?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y4KbmI7MdA", "forum": "OlidGx8oKr", "replyto": "OlidGx8oKr", "signatures": ["ICLR.cc/2026/Conference/Submission21017/Reviewer_hbFR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21017/Reviewer_hbFR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875213679, "cdate": 1761875213679, "tmdate": 1762999997604, "mdate": 1762999997604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the important challenge of building LLM-based RAG summarization systems that are incentive-robust. They propose TTS, a mechanism for decomposing summaries into claims and using hold-one-out evaluate of each source to determine whether it scores highly enough under a peer-consistency-based reliability score to remain in the retrieved context.\nThe authors build a sophisticated theoretical framework modeling source claims, player strategic incentives, and the mechanisms for whether a claim is reported in order to show that the approach incentivizes truth-telling from all sources. \nThe approach is evaluated over 300 QA pairs from two datasets (NQ and ClashEval) for which the authors construct synthetic scenarios in which sources have mixed truthfulness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The proposed algorithm addresses an important, understudied problem of incentive alignment and is of interested to researchers working on creating adversary-robust RAG approaches.\n\nS2. The authors provide substantial valuable theoretical backing to show that the proposed approach disincentivizes false reporting. The extent of the analysis is comendable. \n\nS3. Experiments model multiple interesting scenarios, including coordinated and uncoordinated adversarial behavior. In these scenarios, the results seem convincing."}, "weaknesses": {"value": "W1. The experiments include only 300 datapoints for 2 datasets; this is substantially less than a typical robust evaluation. The paper could benefit from larger datasets and at least one more, noticeably different source/domain for which RAG models are typically used. The scenarios constructed from these datasets are also synthetically generated and not necessarily reflective of real world RAG QA. The Sources (reliable, deceptive, and adversarial) are all LLM generated and may have artifacts that are easier to detect (i.e. less subtle) than real ones.\n\nW2. The approach is substantially slower and more computationally intensive than typical RAG -- it requires generating stance classifications for every (claims, document) in the cross-product of claims/documents in all retrieved sources except a claims's original source. (The claims also need to be extracted, but this can be pre-computed in a RAG index). The paper could benefit from a real-world case study. \n\nW3. The writing can often be dense and inaccessible. While the framework is interesting, it reads as aggressively over-mathematized. The authors could benefit from providing simple, prosaic, direct summaries of components of the theoretical framework before diving into some of the equations, some of which might be better appendixed. Some of the wording is also generally difficult to digest; the paragraph that conveys some of the contributions P80-86, is jargon-heavy and quite difficult to understand even after multiple passes by a native speaker with a PhD in the field. e.g. these lines are hard to read: L.83-85 \"Signals are embedded in prose: [...] formalize implementability and an equivalence to the standard signal–report model.\""}, "questions": {"value": "Q1. Does source selection that disincentivizes a source from having a unique claim end up hurting downstream QA because it decreases diversity in the retrieved context?\n\nQ2. The theoretical analysis makes the modeling assumption that all claims are exchangeable, but is this ever true in practice? Some kinds of sources are likely avoid claims about controversial/uncertain topics and are more likely to report claims that are newsworthy. Therefore it seems that $Pr(Q_{ik} = 1 \\mid T_i)$ cannot be boiled down to a constant, it is dependent on a claim's type (e.g. \"controversial\", \"speculative\", etc) that is not exchangeable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iu9YUfYvZ4", "forum": "OlidGx8oKr", "replyto": "OlidGx8oKr", "signatures": ["ICLR.cc/2026/Conference/Submission21017/Reviewer_fpEF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21017/Reviewer_fpEF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948093509, "cdate": 1761948093509, "tmdate": 1762999997463, "mdate": 1762999997463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}