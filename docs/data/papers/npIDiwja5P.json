{"id": "npIDiwja5P", "number": 8052, "cdate": 1758055457760, "mdate": 1759897811549, "content": {"title": "SciDA: Scientific Dynamic Assessor of LLMs", "abstract": "Advancement in Large Language Models (LLMs)  reasoning capabilities enables them to solve scientific problems with enhanced efficacy. Thereby, a high-quality benchmark for comprehensive and appropriate assessment holds significance, while existing ones either confront the risk of data contamination or lack involved disciplines. To be specific, due to the data source overlap of LLMs training and static benchmark, the keys or number pattern of answers inadvertently memorized (i.e. data contamination), leading to systematic overestimation of their reasoning capabilities, especially numerical reasoning. \n\nWe propose **SciDA**, a multidisciplinary benchmark that consists exclusively of over 1k Olympic-level numerical computation problems, allowing randomized numerical initializations for each inference round to avoid reliance on fixed numerical patterns. We conduct a series of experiments with both closed-source and open-source top-performing LLMs, and it is observed that the performance of LLMs drop significantly under random numerical initialization. Thus, we provide truthful and unbiased assessments of the numerical reasoning capabilities of LLMs.", "tldr": "", "keywords": ["Large Language Models", "Multidisciplinary Benchmark", "Data Contamination", "Randomized Initialization", "Dynamic Assessor"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b85fa33beb6b1da5b040a4c9e7b6fe68e9ca6dc1.pdf", "supplementary_material": "/attachment/dc45a788bde5a9faed018ef0db42aa521a3a6735.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SciDA, a dynamic, multidisciplinary benchmark designed to assess the numerical reasoning abilities of large language models (LLMs) while mitigating data contamination. SciDA consists of over 1,000 expert-curated, Olympic-level numerical computation problems covering mathematics, physics, chemistry, and biology, with dynamic variable initialization to systematically eliminate answer memorization and pattern-based shortcuts. The authors evaluate a wide suite of contemporary LLMs in both fixed and randomized settings and find significant performance drops under randomization, suggesting overestimation of existing models’ reasoning abilities on static benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Dynamic Contamination-Proof Design: SciDA leverages randomized parameter initialization, which directly tackles data contamination and memorization issues commonly plaguing static benchmarks.\n2. Multidisciplinary Coverage: By including computation-heavy problems from mathematics, physics, chemistry, and biology, SciDA offers broader evaluative reach than most prior benchmarks limited to single domains.\n3. Robust Experimental Evaluation: The authors present comprehensive experiments involving 14 LLMs, reporting per-model results in both fixed and random settings.\n4. Insightful Error Analysis: The paper provides a nuanced breakdown of error types and links performance degradation to the lack of true numerical/generalization skills."}, "weaknesses": {"value": "1. Limited Discussion of Theoretical Underpinnings of Randomization Strategy: While Equations (3.1)–(3.5) define the sampling and evaluation framework, the theoretical justification for why uniform randomization over the prescribed intervals is sufficient to prevent contamination (vs. adversarial or structured randomization), or how initialization ranges are determined for each problem, remains underexplored.\n2. Potential Annotation and Validation Quality Risks: The annotation/validation process relies on teams of medalists and students, but the degree of independent double-checking, systematic error checking, or inter-annotator agreement is not quantified.\n3. Insufficient Mathematical Formalization of Correctness/Tolerances: While the paper specifies that model predictions are deemed correct if \"within a prescribed tolerance,\" the exact metrics are not made explicit, nor are the thresholds justified for each domain/problem type."}, "questions": {"value": "1. Unclear Definition and Justification of Correctness Tolerances Across Domains: How are correctness tolerances defined and justified across the different scientific domains? For example, do relative/absolute error thresholds vary by subject (e.g., chemistry vs. mathematics), and what is the rationale for those choices?\n2. Absence of Quantitative Uncertainty and Error Bar Reporting: Are there quantitative uncertainty/error bars (e.g., variance across runs) that could be reported for the benchmark’s headline metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XCd6aDEPtc", "forum": "npIDiwja5P", "replyto": "npIDiwja5P", "signatures": ["ICLR.cc/2026/Conference/Submission8052/Reviewer_tnzN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8052/Reviewer_tnzN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761288873900, "cdate": 1761288873900, "tmdate": 1762920044367, "mdate": 1762920044367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SciDA, a new dynamic benchmark with over 1,000 scientific problems designed to address performance overestimation in LLMs caused by data contamination in static benchmarks. Its core feature is the random initialization of numerical parameters for each problem, preventing models from relying on memorization. The authors' key finding is that this randomization causes a significant drop in accuracy for all tested LLMs, revealing a vulnerability in their reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem this paper addresses is a well-recognized and critical issue in the field of LLM evaluation.\n2. The dynamic \"functionalization\" and random initialization of problems is a sound and necessary methodology for testing true generalization over memorized patterns.\n3. The rigorous, expert-led data collection and annotation pipeline ensures a high-quality, difficult set of problems that yield valuable insights into model capabilities."}, "weaknesses": {"value": "1. The dataset's scale (1,000 problems) is limited when distributed across four distinct scientific disciplines and multiple difficulty levels, which may affect the statistical reliability of results in specific sub-domains.\n\n2. The paper's main conclusion is ambiguous. The performance drop is attributed to a failure in \"genuine problem-solving\" , yet the paper's own error analysis (Figure 4) shows \"Calculation Errors\" are far more common than \"Logical Errors\" in most subjects . This suggests models might be generalizing the logic correctly but failing at the computation. If the logic is correct, do the models just need a calculator or code interpreter tool to solve the problems?\n\n3. Key methodological details are missing, specifically how the \"scientifically valid ranges\" for parameter randomization were defined and validated."}, "questions": {"value": "1.  Could the authors please provide a more detailed statistical breakdown of the 1000 problems in an appendix? Specifically, a table showing the exact distribution across the four disciplines and three difficulty levels (as referenced in Figure 6a 35) would be very helpful.\n\n2.  I suggest the authors refine their claim of being the \"first\" dynamic benchmark. Please clarify the distinction from KORgym and Math-perturb more explicitly in the Related Work section and adjust the contribution statement to be more precise.\n\n3. Please elaborate on the implications of the findings in Figure 3,4. If models are primarily making \"Calculation Errors,\" does this not imply that their logical generalization is (at least partially) successful, but their internal arithmetic/symbolic manipulation capabilities are brittle? This distinction is crucial for understanding what \"reasoning\" means for LLMs and where the true bottleneck lies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U5sYv0hoG7", "forum": "npIDiwja5P", "replyto": "npIDiwja5P", "signatures": ["ICLR.cc/2026/Conference/Submission8052/Reviewer_AZaQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8052/Reviewer_AZaQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482265873, "cdate": 1761482265873, "tmdate": 1762920043920, "mdate": 1762920043920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SciDA (Scientific Dynamic Assessor of LLMs), a multidisciplinary dynamic benchmarking framework designed to comprehensively and authentically evaluate the scientific reasoning capabilities of large language models (LLMs). SciDA comprises over 1,000 olympiad-level numerical computation problems, each of which can be randomly initialized with different numerical values at every inference attempt, thereby preventing models from relying on fixed numerical patterns. Experiments conducted on SciDA with multiple top-tier open-source and closed-source LLMs reveal a significant performance drop under randomly initialized numerical values, demonstrating that SciDA provides a realistic and unbiased assessment of numerical reasoning abilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "SciDA effectively prevents models from relying on fixed numerical patterns by randomly initializing the variables in each problem. Moreover, SciDA spans multiple disciplines, including mathematics, physics, chemistry, and biology, and all its problems are drawn from Olympiad-level competitions, ensuring high quality and complexity. This comprehensive evaluation approach provides researchers with a more realistic and holistic assessment tool for evaluating the scientific reasoning capabilities of large language models (LLMs)."}, "weaknesses": {"value": "- The core approach of SciDA, randomly initializing variables within problems, effectively mitigates model reliance on fixed numerical patterns. While this strategy reduces the risk of data contamination to some extent, it remains relatively simplistic and lacks deeper analysis or evaluation of the model’s actual reasoning process.\n- Although SciDA’s dynamic initialization strategy addresses data contamination to a degree, similar techniques have already been employed in other domains, such as dynamic benchmarking. Consequently, SciDA does not represent a significant innovation in methodology.\n- The approach may also lack flexibility: the range and manner of random initialization are fixed and not adaptively tuned based on problem difficulty or model capability. This rigidity could result in some problems becoming either too easy or excessively hard after randomization, thereby failing to accurately reflect the model’s true reasoning capacity."}, "questions": {"value": "1. In the experiments involving random initialization, did the authors consider potential differences in how sensitive various models are to such randomization? For instance, some models might be more robust and adaptable to randomized inputs, while others could be heavily reliant on fixed numerical patterns.\n\n2. Within the dynamic initialization strategy, did the authors explore the possibility of adaptively adjusting the range and distribution of variables based on problem difficulty and model capability? For example, for particularly challenging problems, could narrowing the variable range help moderate difficulty and thereby yield a more precise assessment of a model’s reasoning ability?\n\n3. In the error analysis section, did the authors consider performing a finer-grained categorization of model errors? Beyond logical and computational errors, could additional error types—such as misinterpretation (understanding errors) or formulation/representation errors (expression errors)—be introduced to provide deeper insights into model failure modes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VIZ3lpOE1M", "forum": "npIDiwja5P", "replyto": "npIDiwja5P", "signatures": ["ICLR.cc/2026/Conference/Submission8052/Reviewer_qfU1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8052/Reviewer_qfU1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570699026, "cdate": 1761570699026, "tmdate": 1762920043225, "mdate": 1762920043225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SciDA, a multidisciplinary benchmark designed to assess the scientific reasoning ability of LLMs under dynamic randomized conditions. The authors identify a critical issue about data contamination in the current benchmarks for evaluating LLMs. To address this, SciDA collects over 1K Olympiad-level scientific computation problems covering math, physics, chemistry, and biology, each parameterized with random variable ranges to eliminate memorization effects. The benchmarking approach involves expert curation and dynamic random initialization. Experiments on 14 mainstream LLMs reveal that accuracy drops by up to 60% under randomization, exposing an overestimation of current LLMs' reasoning ability. The paper concludes that SciDA offers a contamination-free approach for evaluating genuine reasoning performance, with plans for future expansion into more disciplines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The data curation pipeline with Olympiad-level problems and expert annotation ensures quality and complexity.\n\n- The empirical findings of systematic performance drop show the effect of randomized conditions and reveal concerns of data contamination.\n\n- Code is provided for reproducibility."}, "weaknesses": {"value": "- Some statements are a bit overclaimed.\n\t- The claim of being \"contamination-proof\" is not fully substantiated. This is actually a very hard problem to completely solve.\n\t- In the abstract, there is \"we provide truthful and unbiased assessments of the numerical reasoning capabilities of LLMs\", but there are actually no guarantees.\n\n- Writing can be refined for conciseness and professional tone."}, "questions": {"value": "- How to ensure that the dynamic randomization does not inadvertently generate unsolvable problems beyond the predefined ranges?\n\n- How to verify the \"contamination-proof\" claim? How to verify empirically that SciDA problems are unseen in major training corpora like Common Crawl?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rStT5xZ6JG", "forum": "npIDiwja5P", "replyto": "npIDiwja5P", "signatures": ["ICLR.cc/2026/Conference/Submission8052/Reviewer_GamU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8052/Reviewer_GamU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618534076, "cdate": 1761618534076, "tmdate": 1762920042724, "mdate": 1762920042724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}