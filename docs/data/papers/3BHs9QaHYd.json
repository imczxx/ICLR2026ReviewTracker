{"id": "3BHs9QaHYd", "number": 18716, "cdate": 1758290371650, "mdate": 1759897085281, "content": {"title": "Coding Triangle: How Does Large Language Model Understand Code?", "abstract": "Large language models (LLMs) have made remarkable progress in code generation. However, the evaluation of their actual programming capabilities remains largely limited to solving standard coding problems, and a full understanding is under exploration. We propose Coding Triangle, a systematic approach for evaluating LLMs across three core dimensions: code editing, code implementation, and test case generation. Through comprehensive experiments on competitive programming benchmarks, we evaluate the model performance across these dimensions and uncover both self-consistency and self-inconsistency within the model's own cognition. Self-consistency often results in solutions that lack the diversity and robustness seen in human programmers, leading to a significant distribution shift between model cognition and human submissions. Our analysis of interactions between different dimensions reveals that self-inconsistency also exists, which may enable self-reflection and self-improvement and provide a potential direction for developing more powerful coding models.", "tldr": "", "keywords": ["LLM", "Code Generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0321ba5c34b1897f6d1e643795fc6b57723a78eb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses an important topic in the discussion of understanding how LLMs understand code. It introduces the framework of a **Coding Triangle** that covers 3 different ways models address and understand coding problems. While each individual component is not novel, the framework is a novel way of combining the various components. U"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Framework of the Coding Triangle is novel and original\n- The writing is clear and easy to follow for the majority of the paper and the results are sensible as well.\n- In particular the insights on self-consistency are insightful.\n- The idea and framework could be very impactful"}, "weaknesses": {"value": "- There are not enough models evaluated. Most notably, the paper doesn't cover core models like GPT, Sonnet, Gemini, etc.\n- Evaluating on AtCoder seems relatively limited. Are there any other benchmarks where you can do this analysis also?\n- While the paper brings in the idea of a coding triangle, the paper doesn't thoroughly address how each component connects to one another. The main connection discussed by the paper is how editorials affect code generation, but the reverse or how cases affect code or how cases affect editorials are not discussed. The paper should try to cover all combinations to be thorough.\n- An obvious extension would be seeing if the model could revise its solution based on its test cases and if that improves performance. I might have missed this, but don't see it in the paper.\n- Minor, but needs more detail on what AtCoder is, the problem count, why A-F is different, etc."}, "questions": {"value": "Last two points in weaknesses are my questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DqcqFweK0L", "forum": "3BHs9QaHYd", "replyto": "3BHs9QaHYd", "signatures": ["ICLR.cc/2026/Conference/Submission18716/Reviewer_XSWv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18716/Reviewer_XSWv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857457444, "cdate": 1761857457444, "tmdate": 1762928423429, "mdate": 1762928423429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes \"Coding Triangle,\" a three-dimensional framework for evaluating LLMs' programming capabilities through editorial analysis, code implementation, and test case generation. The authors conduct comprehensive experiments on competitive programming problems, revealing interesting phenomena of self-consistency and self-inconsistency in model behaviors. The work addresses an important gap in current code evaluation benchmarks and provides valuable insights into model cognition."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The three-dimensional evaluation framework is innovative and addresses limitations of existing benchmarks\n\nThe analysis of self-consistency and self-inconsistency reveals important characteristics of model cognition\n\nThe discovery that model mixtures enhance diversity and robustness is practically valuable\n\nComprehensive experiments across multiple model types and problem difficulties"}, "weaknesses": {"value": "The evaluation is limited to competitive programming problems; generalization to real-world coding scenarios needs verification\n\nThe \"self-consistency\" and \"self-inconsistency\" concepts could be more precisely defined and quantified\n\nLimited analysis of why reasoning models still exhibit self-inconsistency despite extended reasoning capabilities\n\nNo discussion about the computational cost of implementing the full Coding Triangle evaluation"}, "questions": {"value": "1. How would your framework perform on more practical coding tasks like software maintenance or code review, beyond algorithmic problems?\n\n2. Have you considered applying your findings to improve model training, perhaps by explicitly addressing the identified self-consistency issues?\n\n3. The hidden state in recurrent models maintains some memory - could similar mechanisms help LLMs maintain consistency across coding tasks?\n\n4. What's the computational overhead of implementing all three dimensions of your evaluation framework compared to traditional benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lhob1cbURr", "forum": "3BHs9QaHYd", "replyto": "3BHs9QaHYd", "signatures": ["ICLR.cc/2026/Conference/Submission18716/Reviewer_F4GB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18716/Reviewer_F4GB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922564114, "cdate": 1761922564114, "tmdate": 1762928422642, "mdate": 1762928422642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper evaluates three dimensions of code generation: (1) Code, (2) Editorial, and (3) Case. It proposes no new methods or datasets. All experiments use Qwen series models and Deepseek V3. The paper reports several interesting observations: (1) LLMs show self-consistency between editorial and code dimensions, (2) code generation capabilities are similar across different LLMs but distinct from human submissions, (3) human submissions are more diverse than LLMs on code and cases, (4) human editorials are more helpful than LLM-generated editorials for code generation, (5) LLMs can recognize their own mistakes, (6) LLMs can generate more comprehensive test cases than they can solve."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "No obvious grammar flaw in the paper."}, "weaknesses": {"value": "1. Figure 1, the teaser is difficult to follow, I can’t understand the relationship between green, blue, and orange arrows and blocks. And which dimensions are self-consistent or not self consistent cannot easily tell from the figure. \n2. The evaluation models: QWQ, Qwen coder and Qwen instruct are basically from the same company, my concern is I think their would be some similarity in pretrain data, a more diverse model to be used would make the observations in the paper seems more reasonable.\n3. As an evaluation paper, there is no qualitative analysis in the paper, only outcome based analysis make this paper shallow in the depth of analysis. \n4. Again without in-depth analysis and explaination, I cannot understand why human editorial is better than human generated editorials on code and cases generation. \n5. In the section: From Code to Cases: Can LLM generate more comprehensive cases with the code? How to make sure the generated cases is inherently correct? How complex the cases are compared to the original seed cases. \n6. According to the authors, the LLMs can find their own mistakes, I am confused in what extent they can find mistakes, in what extent they can’t? And why models can find mistakes but cannot solve the problem directly? would test-time scaling help? would self-refinement such as ReAct help?\n7. To what extent the model cannot find its own mistakes? is there any qualitative anlysis on that?\n8. All experiments are down via Pass@1, would the conclusions in the paper be different if use test-time scaling to evaluate?\n9. The experiment setting is unclear, for example the temperature of LLMs to decode, thus the similarity experiment in Figure 3 is not that accurate from my perspective, temperature could affect the diversity by a lot. And I am not sure why human generated code is more diverse than LLMs? is it because the code are submitted by different people, and the author use an outcome based analysis, I think is not enough to say they are more diverse. \n\nFrom my humble perspective, the paper is not polished enough, lots of experiment settings and qualitative analysis are missing. The evaluation method is not surprising or make sense enough. I think the paper need a major revision, thus I would rate a maximally score of 2 to the paper."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KrAvxPUlQ5", "forum": "3BHs9QaHYd", "replyto": "3BHs9QaHYd", "signatures": ["ICLR.cc/2026/Conference/Submission18716/Reviewer_8kHe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18716/Reviewer_8kHe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984831447, "cdate": 1761984831447, "tmdate": 1762928422020, "mdate": 1762928422020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the underlying code understanding capabilities of Large Language Models (LLMs), arguing that current benchmarks focused on functional correctness are insufficient. The authors propose a new, three-dimensional evaluation framework called the \"Coding Triangle,\" which assesses models across: (1) Editorial (natural language problem analysis), (2) Code (algorithm implementation), and (3) Cases (test case generation).\n\nThrough experiments on AtCoder problems, the authors analyze the \"self-consistency\" and \"self-inconsistency\" of LLM cognition. The primary contributions are:\n\nThe identification of high \"self-consistency\" in LLMs, which leads to a lack of solution diversity and a significant \"distribution shift\" when compared to human-generated solutions .\n\nThe finding that this self-consistency limits exploration (e.g., models' self-generated editorials do not improve their coding performance , and their code easily passes their own flawed test cases ).\n\nThe observation that \"self-inconsistency\" also exists (e.g., models can sometimes identify their own failed solutions ) and that \"model mixture\" can improve robustness by increasing diversity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Framework: The primary strength is the proposal of the \"Coding Triangle\" (Editorial, Code, Cases). This is a novel, intuitive, and significant contribution. It provides a multi-dimensional, interpretable framework that moves beyond simple functional correctness to probe an LLM's analytical and validation capabilities.\n\n\n\n* Insight: The paper clearly identifies and provides evidence for \"self-consistency\" and \"distribution shift\". The finding that LLM-generated solutions are highly similar (high cosine similarity) and lack the diversity of human solutions is an important one for the community.\n\n\n\n* Connecting Self-Consistency to Exploration: This work provides an explanation for the success of exploration-based sampling techniques. By showing that models are often trapped in their own narrow \"cognition\" , the paper highlights that breaking this self-consistency (e.g., via \"model mixture\" ) is a good way to improve robustness.\n\n\n\n* Holistic Analysis: The analysis of the interactions between the triangle's vertices (the \"edges\" ) is a high-quality contribution."}, "weaknesses": {"value": "* Methodological Opacity (Critical Weakness): As detailed in the \"Soundness\" section, the paper is missing the most crucial experimental details. The authors analyze solution diversity and self-consistency without specifying the decoding parameters (temperature, top-p, etc.) or the number of samples (k) used for the diversity analysis in Figure 3. These parameters are not minor details; they are the central variables that control the exploration and diversity the paper claims to measure. This omission makes the core experimental results non-reproducible and unverifiable.\n\n\n\n* Limited Exploration of Diversity Enhancement: The paper correctly identifies \"model mixture\" as a way to enhance diversity. However, this is a very expensive (inference-time) and somewhat obvious solution. The paper would be much stronger if it explored other, single-model methods for increasing exploration as a counterpoint. For example, how do the diversity metrics (Fig 3) and robustness scores change if one simply increases the sampling temperature for a single model? Does that achieve the same gains as the model mixture? The discussion feels incomplete by only proposing model mixture as the solution to the self-consistency problem.\n\n\n\n* Ambiguous \"Cases\" Metric: The metric for the \"Cases\" dimension, S_{case}, is convoluted and its reliability is unclear. It is defined by the ability to identify \"incorrect human submissions\" (\\mathcal{H}_{i}^{wrong}). How large is it? How was it curated? A model might get a high S_{case} score by generating a few simple cases that catch common errors, which does not fully capture the \"depth of understanding in terms of validation criteria\" that the authors claim to measure."}, "questions": {"value": "Most of the questions are mentioned in the weakness part. Additional one: \n\nThe paper proposes \"model mixture\" to increase diversity. Have the authors experimented with single-model techniques for increasing exploration, such as increasing the sampling temperature or using nucleus sampling with a high p? How do these simpler methods compare to the expensive \"model mixture\" approach in terms of increasing robustness and the \"Unique Set Size\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oItS4P8RFm", "forum": "3BHs9QaHYd", "replyto": "3BHs9QaHYd", "signatures": ["ICLR.cc/2026/Conference/Submission18716/Reviewer_J9TQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18716/Reviewer_J9TQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988023850, "cdate": 1761988023850, "tmdate": 1762928421433, "mdate": 1762928421433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}