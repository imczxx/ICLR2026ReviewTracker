{"id": "xvGyyh6MG7", "number": 11702, "cdate": 1758203203801, "mdate": 1759897559919, "content": {"title": "Revisiting Long-context Modeling from Context Denoising Perspective", "abstract": "Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-world applications. The success of LCMs can be attributed to their ability to locate implicit critical information within the context for further prediction. However, recent research reveals that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens, that can mislead model attention. In this paper, we conduct a fine-grained analysis of the context noise and propose an effective metric, the Integrated Gradient (IG) score, to detect and quantify the noise information within the context. Our findings reveal that even simple mitigation of detected context noise can substantially boost the model's attention on critical tokens and benefit subsequent predictions. Building on this insight, we propose Context Denoising Training (CDT), a straightforward yet effective training strategy that improves attention on critical tokens while reinforcing their influence on model predictions. Extensive experiments across four tasks, under both context window scaling and long-context alignment settings, demonstrate the superiority of CDT. Notably, when trained with CDT, an open-source 8B model can achieve performance (50.92) comparable to GPT-4o (51.00).", "tldr": "This paper introduces CDT (Context Denoising Training), a simple yet effective strategy that improves long-context models by reducing the impact of noisy context and enhancing focus on critical information.", "keywords": ["Language Modeling", "Long-context Understanding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9cc8e56667ed73bfdf780726822861043a885f86.pdf", "supplementary_material": "/attachment/e52483a07ad16aa9b02f388762e2ef389d00e1f2.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Context Denoising Training (CDT), a straightforward yet effective training strategy that improves attention on critical tokens while reinforcing their influence on model predictions. The authors first propose the Fact Retrieval (FR) score and Integrated Gradient (IG) score to identify and quantify context noise, demonstrating that existing long-context methods fail to distinguish critical from irrelevant information, degrading performance. They show that simple mitigation of detected context noise can substantially boost the model’s attention on critical tokens and benefit subsequent predictions.\n\nBuilding on this insight, CDT subtracts the corresponding gradients to manipulate the irrelevant token embeddings, thus suppressing the context noise. This allows the model to focus on critical tokens and strengthens the causal link between them and the final output. Across LongBench, RULER, BABILong, LongPPL, and other benchmarks, CDT consistently boosts long-context performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper conducts extensive experiments across multiple settings and long-context benchmarks, thoroughly demonstrating the effectiveness of CDT over multiple baselines.\n2. The introduction of the FR score, IG score, and identifier $\\mathcal{I}(x_i)$ provides valuable insights into how long-context LLMs comprehend long contexts.\n3. Beyond presenting an effective long-context training paradigm, the authors offer in-depth discussions and visualized analyses that solidify the credibility of their approach."}, "weaknesses": {"value": "See Questions."}, "questions": {"value": "1. The paper introduces the FR score, IG score, and an identifier. However, only the identifier is actually used in CDT. The FR and IG scores serve only as diagnostic tools because they require prior knowledge of the four token types in the context. I would appreciate it if the authors could clarify these two distinct contributions more clearly.\n2. It is unclear why low-frequency words should be singled out. No prior work is cited to justify this taxonomy. A simpler partition into supporting, interference, and irrelevant tokens appears sufficient, and the proposed identifier itself only decides whether a token is irrelevant.\n3. I find it confusing that authors compare CE, LongCE, and CDT with YaRN. YaRN is not a training paradigm. It is merely an interpolation method like NTK. Likewise, comparing CDT at inference time with FlexPrefill and XAttention is also confusing. These techniques aim at enhancing efficiency, not performance.\n4. As a work focused on long-context training, CDT lacks citations to early efforts in this line[1-3]. In addition, the strategy of identifying critical tokens to improve training or inference should be clearly differentiated from similar techniques originally developed in short-input or long-output scenarios[4-5].\n5. The authors note in the appendix that the improvement brought by our method on complex reasoning tasks is not as significant as that on other tasks. Since CDT also identifies critical tokens to enhance reasoning, could the high-entropy token approach[5] be combined to further boost performance?\n6. Typo: irrelevant instead of irrevelant in Figure 3a\n\n[1] Effective Long-Context Scaling of Foundation Models https://arxiv.org/abs/2309.16039\n\n[2] Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models https://arxiv.org/abs/2405.17915\n\n[3] LongWanjuan: Towards Systematic Measurement for Long Text Quality https://arxiv.org/abs/2402.13583\n\n[4] RHO-1: Not All Tokens Are What You Need https://arxiv.org/abs/2404.07965\n\n[5] Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning https://arxiv.org/abs/2506.01939"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aQwn0DieGA", "forum": "xvGyyh6MG7", "replyto": "xvGyyh6MG7", "signatures": ["ICLR.cc/2026/Conference/Submission11702/Reviewer_Rdcm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11702/Reviewer_Rdcm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760799979719, "cdate": 1760799979719, "tmdate": 1762922749725, "mdate": 1762922749725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key weakness in long-context models (LCMs): their susceptibility to contextual noise—irrelevant tokens that distract the model from critical information. The authors propose a novel training strategy called Context Denoising Training (CDT), which improves a model’s ability to focus on salient tokens and strengthen their influence on predictions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The Integrated Gradient (IG) score, which more accurately identifies important tokens compared to attention-based methods.\n\n2. Manual noise suppression experiments: Showing that reducing noise in input embeddings boosts attention on critical tokens by ~10×.\n\n3. The CDT training strategy: A lightweight, online method that detects and suppresses noisy tokens during training, improving model focus without heavy computational overhead.\n\n4.Extensive evaluation: Across 4 task types (real-world, synthetic, language modeling, reasoning) and multiple models, CDT consistently outperforms baselines and even enables an 8B model to nearly match GPT-4o on LongBench-E."}, "weaknesses": {"value": "1. Is there any relevant literature or experimental evidence supporting the statement in line 759 that \"performance gains typically exhibit diminishing returns with increased token budgets\"?\n\n2. In lines 768–769, it is stated that \"LongCE achieves a 13-point gain per 1B tokens versus ProLong’s 0.3-point gain per 1B tokens.\" How was the 13-point gain calculated?\n\n3. Why are the results for NExtLong-512K-Instruct and ProLong-512K-Instruct shown in Table 1, but not included in Table 2?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C1PG3ZTnch", "forum": "xvGyyh6MG7", "replyto": "xvGyyh6MG7", "signatures": ["ICLR.cc/2026/Conference/Submission11702/Reviewer_L6Fs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11702/Reviewer_L6Fs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824951821, "cdate": 1761824951821, "tmdate": 1762922748925, "mdate": 1762922748925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper address the issue of Long-context language models (LCMs) struggle with contextual noise—when irrelevant information in lengthy contexts overwhelms critical tokens, leading to some issues. The paper propose some measurement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Tackles a crucial challenge in long-context language models: contextual noise overwhelming critical information\n* High practical relevance for real-world applications (RAG, document QA, long-context reasoning)\n* Problem is timely given the trend toward longer context windows in LLMs"}, "weaknesses": {"value": "1. The paper proposes an indirect approach: compute IG scores externally, identify critical tokens, perturb embeddings, then train. However, they provide no compelling explanation for why this is superior to simply training the model to learn token importance directly through standard optimization.\n\n2. The preliminary study (Section 3) relies entirely on synthetic tasks with manually injected noise, which provides no evidence that the observed attention patterns occur in real-world scenarios.\n\n3. The proposed method pre-selects critical tokens for training sequences, which seems to circumvent the model's own ability to learn token importance organically during training. Since intuitive learning suggests independent discovery of salient tokens, the authors should clarify why this indirect training approach outperforms direct training."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CSRMESYnvy", "forum": "xvGyyh6MG7", "replyto": "xvGyyh6MG7", "signatures": ["ICLR.cc/2026/Conference/Submission11702/Reviewer_gbM2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11702/Reviewer_gbM2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954808626, "cdate": 1761954808626, "tmdate": 1762922748418, "mdate": 1762922748418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}