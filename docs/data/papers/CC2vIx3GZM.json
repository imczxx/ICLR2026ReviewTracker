{"id": "CC2vIx3GZM", "number": 14220, "cdate": 1758230441162, "mdate": 1759897382941, "content": {"title": "Towards Coordinate- and Dimension-Agnostic Machine Learning for Partial Differential Equations", "abstract": "Machine learning methods for data-driven identification of partial differential equations (PDEs) are typically defined for a fixed number of spatial dimensions and a particular choice of coordinates in which the data have been collected. This dependence prevents the learned equation from generalizing to other spaces. In this work, we reformulate the problem in terms of coordinate- and dimension-independent representations, paving the way toward what we might call “spatially liberated\" PDE learning. In this work, we propose an approach to learning differential equations by expressing them in a way that is independent of the coordinate system and even the underlying manifold where the equation is defined. This allows us to learn a differential equation in low-dimensional spaces and generalize to higher-dimensional spaces with different geometric properties. We provide extensive numerical experiments that demonstrate that our approach allows for seamless transitions across various geometric contexts. We show that the dynamics learned in one space can be used to make accurate predictions in other spaces with different dimensions, coordinate systems, boundary conditions, and curvatures.", "tldr": "", "keywords": ["Geometric Deep Learning", "PDE Identification", "Any-dimensional Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35cce11239dd33a90ca672cd339c652811d86fc3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a methodology for data-driven discovery of PDE models that is agnostic to the spatial domain, coordinate system, spatial dimension, and more. The approach is based on computing analytically known local features from full field observation data. Neural networks are then trained to map these features to the right hand side dynamics of the PDE. A suite of numerical experiments in 1 to 3 dimensions and various geometries are presented that support the potential of the approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is very original. It nicely combines ideas from different areas of mathematics to produce an implementable numerical framework. The idea of spatially liberated scientific machine learning is highly significant, even if the PDEs are known because it could accelerate training data generation (train in 1D, test in higher dimensions). The writing is of adequate quality and mostly clear. \nThe authors' methodology is well thought out and some limitations are addressed. For example, I like how in lines 203--208 the well-posedness of the PDEs (e.g., in terms of the required boundary conditions for Helmholtz) are mentioned when discussing generalization to higher dimensions. However, I find the notation hard to parse at times, and the authors do not emphasize the locality of the neural networks and the data."}, "weaknesses": {"value": "While the paper does succeed in showing that 1D training data can provide information about solutions in higher dimensions (under appropriate assumptions on the PDEs), beyond this the numerical results as presented in the main text are not super convincing. \nThe results are all qualitative and show ``eyeball norm'' only. No actual function space norms of the error are computed or reported until the appendix (the vector PDE Navier-Stokes example has a large 12\\% relative error, which is not reported in the main text). Even then, it would be informative to understand what is lost by transferring 1D solutions to higher dimension or coordinates. For example: how much does the relative error change? At what rate? The paper would be strengthened if error tables or convergence rate plots are presented, instead of showing eyeball norm images of fixed problem instances. If the method is not very accurate, could it still be useful? Say, as a preconditioner? Or could one dynamically correct the learned model with data assimilation?\n\nThere is not much discussion of the limitations of the proposed framework. For instance, due to the locality required, the approach will not be able to handle integro-differential equations such as the kinetic Boltzmann equation (which has a nonlocal collision operator). One way to get around this is to use a neural operator instead of a fully connected NN in item 2 line 299 page 6. This would allow going from pointwise local maps (Nemytskii operators) to fully nonlocal operators. It is also important for the authors to describe the failure modes of the approach. For example, is it sensitive to model error in the dynamics?"}, "questions": {"value": "Questions:\n1. The invariant features in Eqn (1) are selected from prior knowledge and ``hard-coded''. Would there be scenarios where $ B $ should itself be learned from the training data?\n1. Although any-dimensional machine learning ideas motivate the work, I did not see mention of any-dimensional architectures being used by the authors (e.g., DeepSets, PointNet). Is there an opportunity to blend what's done in the paper and these methods?\n1. The particular form of the data is not discussed in detail; it is just vaguely referred to as field data. Although the appendix considers noisy data, what about sparse measurements in space and time instead of a full field grid?\n1. The amount of data is also not discussed often. Even in Appendix N, the amount of training data generated is not reported. Relating to Weaknesses, it would be very interesting to understand the data requirements of the approach. For example, consider training on $ N $ 1D data fields versus training directly on $ M $ 2D data fields/trajectories. Always compute test error in 2D. How is $ N $ related to $ M $? This is a very interesting sample complexity question.\n\nTypos:\n1. In Eqn 3 page 5, what is $ v_i $? And is $ f_iv_i $ supposed to be interpreted as pointwise multiplication? Eqn 3 is very hard to understand.\n1. line 303 p 6 typo: spatial not spacial\n1. Typo line 696 p. 13 $ \\Delta x $ not $ dx $ in numerator\n1. In line 230, why is $ \\langle v, \\nabla\\cdot v\\rangle  $ vector-valued?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WckMTznx9R", "forum": "CC2vIx3GZM", "replyto": "CC2vIx3GZM", "signatures": ["ICLR.cc/2026/Conference/Submission14220/Reviewer_tW7v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14220/Reviewer_tW7v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761513912640, "cdate": 1761513912640, "tmdate": 1762924677720, "mdate": 1762924677720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors reformulate the problem in terms of coordinate- and dimension-independent representations, paving the way to spatially liberated PDE learning. This allows us to learn a differential equation in low-dimensional spaces and generalize to higher-dimensional spaces with different geometric properties. The proposed approach is summarized in Figure 1. Numerical experiments demonstrate that the proposed approach allows for seamless transitions across various geometric contexts."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 4}, "strengths": {"value": "* (Technical contribution) The paper shows that the dynamics learned in one space can be used to make accurate predictions in other spaces with different dimensions, coordinate systems, boundary conditions, and curvatures.\n* (Technical contribution & Novelty) The paper push previous ideas further and enables PDE learning not only in a coordinate-independent way but also independently of the data domain, dimension, and geometry: a novel contribution."}, "weaknesses": {"value": "* The proposed approach can be applied only to invariant systems under local orthogonal transformations and diffeomorphic translations.\n  * How can we remove this resstriction?\n* The paper, Section 2 in particular, is hard to follow, requiring an extensive revision.\n  * How can we compute the Laplacians and inner products of the gradients (lines 166-168) from the \"given data\" (line 150)? \n    * I assume the \"given data\" are physically observable data and are the fields \\\\Psi_j. Is this correct? \n    * Do the authors use symbolic programming?\n    * Also, an elaboration on the footnote 3 is appreciated.\n  * (Line 188-189) What does it mean by \"independent\" and \"dependent\"? \n  * (Figure 1) What are the inputs to the neural network? Are they static numbers (scalar or vector values), symbols, or functions of x at t (fields)? If they are functions of x, how can we input functions to a neural network (e.g., is space discretized?)? Or using a symbolic neural network?\n  * The explanation in Section 2 is not self-contained, and some contents in Section 3 (e.g., lines 368-373) should be merged.\n\n## Minor comments\n* Use the term \"PDE\" or \"partial differential equation\" consistently."}, "questions": {"value": "See above.\n\n\n## Review summary\n\nThe motivation and goal are clear, and the contribution (PDE learning not only in a coordinate-independent way but also independently of the data domain, dimension, and geometry) appears noteworthy. However, a significant revision is required to clarify the explanation of the proposed architecture."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ww7KfZZrYa", "forum": "CC2vIx3GZM", "replyto": "CC2vIx3GZM", "signatures": ["ICLR.cc/2026/Conference/Submission14220/Reviewer_YLLF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14220/Reviewer_YLLF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634382037, "cdate": 1761634382037, "tmdate": 1762924677295, "mdate": 1762924677295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a coordinate free and a dimension free way to learn PDEs: instead of learning in a fixed grid/coordinate system, it learns the dynamics from an intrinsic feature library built only from the metric, gradients, Laplacians, and gradient inner products, that are well defined on any Riemannian manifold and in any spatial dimension. Models trained in simple 1D settings transfer to 2D/3D, curved surfaces, and different boundary conditions. The authors build the invariant feature set $B$, learn F(B) from data, and then integrate with a coordinate free integrator, or for implicit PDEs learn a low  dimensional manifold in $B$ space and enforce it via a PINN when solving on a new domain.\nThe authors show that models trained on 1D data accurately reproduce dynamics in: (i) other 1D coordinate systems, (ii) 2D/3D Euclidean domains (spiral/scroll waves for FitzHugh–Nagumo/Barkley), (iii) curved manifolds (sphere, in geographic/stereographic coords), and (iv) non-trivial topology (annulus), with quantitative error reports."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(i) Principled invariance: the paper builds a coordinate and dimension free feature library, so the learned law depends only on intrinsic geometric operators, and not on the chosen coordinates. A coordinate free integrator lets a model trained in simple 1D settings transfer directly to new coordinates, higher dimensions, and curved manifolds by recomputing the same invariants with the target metric, (ii) Cross domain demos: the authors show successful transfer 1D to 2D/3D, across coordinate systems, and onto curved/topologically nontrivial domains, providing concrete evidence that the invariants generalize, (iii) The work covers both evolution and implicit PDEs, hence broadening the applicability."}, "weaknesses": {"value": "(i) Restricted operator class (up to second-order spatial derivatives): the invariant library B is built only from fields, Laplacians, and gradient inner products, (ii) The learned manifold in B space does not encode which boundary/initial data make the problem well posed, which must be supplied separately when moving to new domains, (iii) Limited stability analysis: long horizon stability or higher order integrators are not explored, (iv) The 2D Navier–Stokes example assumes that the pressure field is known and the continuity equation is given, the rollout is short (10 Euler steps), so the full difficulty of coupled scalar vector systems and pressure recovery is not fully addressed."}, "questions": {"value": "Suggestions: (i) Ablations that isolate what really matters:  remove each invariant family (\\Delta\\Psi, gradient inner products) in turn, vary feature normalization, train in 1D and compare to training directly in target 2D/3D, in order to attribute gains to the intrinsic library, rather than incidental choices, (ii) Robustness study: quantify sensitivity to metric/discretization errors by perturbing g, adding mesh noise, varying stencil order, and report transfer degradation, (iii) Augment the learned B manifold with BC tags, Dirichlet/Neumann/mixed, or a boundary operator penalty so the constraint carries boundary information, (iv) Upgrade numerics and analyze long horizon stability, (v) For incompressible NS, add a coordinate free pressure recovery by Poisson solve, enforce \\nabla\\!\\cdot v=0 via projection each step, and report transfer when pressure is not given."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pwOfb0o6BY", "forum": "CC2vIx3GZM", "replyto": "CC2vIx3GZM", "signatures": ["ICLR.cc/2026/Conference/Submission14220/Reviewer_BMSv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14220/Reviewer_BMSv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718387286, "cdate": 1761718387286, "tmdate": 1762924676907, "mdate": 1762924676907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for coordinate- and dimension-agnostic learning of partial differential equations (PDEs). The main idea is to represent PDE operators using a set of coordinate-free invariant features derived from classical invariant theory and Riemannian geometry. A neural network is then trained to learn the time-evolution law from these invariant quantities, enabling the model to generalize across different coordinate systems, geometries, and spatial dimensions.\n\nThe approach aims to achieve “spatially liberated” PDE learning, where a model trained on data from one domain (for example, a one-dimensional Cartesian grid) can be applied to others (such as curved or higher-dimensional manifolds) without retraining. The method is demonstrated on several benchmark systems, including FitzHugh–Nagumo, Barkley, Patlak–Keller–Segel, Helmholtz, and Navier–Stokes equations.\n\nThe paper claims three main contributions:\n(i) a coordinate- and dimension-free feature representation for PDEs,\n(ii) a neural framework for learning invariant operators, and\n(iii) empirical demonstrations of geometric and dimensional transfer of the learned dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality:\nThe paper introduces a geometric formulation of PDE learning that enforces coordinate and dimension invariance. While the mathematical tools are drawn from established invariant theory, the combination with neural PDE modeling provides a moderately original framing rather than a fundamentally new methodology.\n\nQuality:\nThe experiments are competently executed on standard reaction–diffusion systems. The results qualitatively demonstrate that a model trained in one coordinate system can reproduce dynamics in another, though the evaluation remains limited to simple toy settings.\n\nClarity:\nThe manuscript is clearly written and well-organized. The figures are helpful, and the derivations are presented with sufficient detail for replication. However, the exposition occasionally overstates generality and interpretability relative to what is actually demonstrated.\n\nSignificance:\nThe proposed framework may stimulate further exploration of coordinate-invariant representations in scientific ML. Nonetheless, its current scope—restricted to low-order, autonomous PDEs with constant coefficients—significantly limits its impact for practical PDE modeling or neural operator research."}, "weaknesses": {"value": "Limited generality of the framework.\nDespite claiming coordinate- and dimension-agnostic PDE learning, the method is applicable only to PDEs involving first-order time derivatives and up to second-order spatial derivatives. This excludes many physically and scientifically important systems such as the Cahn–Hilliard, Kuramoto–Sivashinsky, Swift–Hohenberg, and wave equations. The paper briefly mentions that higher-order operators could be included by extending the invariant basis, but this is not implemented or analyzed. A practical next step would be to demonstrate how the feature basis can systematically scale to higher derivative orders or nonlocal operators.\n\nRestriction to autonomous, constant-coefficient PDEs.\nThe method implicitly assumes that coefficients are spatially uniform and that the PDE operator does not depend explicitly on time. This limits applicability to homogeneous systems. Extending the invariant basis to include spatially varying fields (e.g., variable diffusivity or forcing terms) or explicit time dependence would make the framework more relevant to real-world physical systems.\n\nOverstated interpretability.\nThe paper repeatedly claims “physical interpretability,” but the neural network mapping from invariant features to time derivatives remains a black box. Using physically meaningful inputs does not make the learned operator interpretable in a mechanistic sense. To move toward genuine interpretability, the authors could integrate symbolic regression (e.g., SINDy, DeepMoD) or sparsity-promoting methods to recover explicit PDE forms from the learned operator.\n\nWeak empirical validation.\nAll experiments involve simple synthetic systems (FitzHugh–Nagumo, Barkley, Patlak–Keller–Segel, Helmholtz) trained on one-dimensional data and evaluated on idealized higher-dimensional domains. There is no quantitative comparison with strong baselines especially recent development in symbolic machine learning for PDE discovery that already demonstrate high efficiency. Including quantitative benchmarks (e.g., error metrics, stability over time integration) and ablation studies on the invariant basis would substantiate the claimed generalization.\n\nInflated claims of dimension and geometry generalization.\nAlthough the method generalizes from 1D to 2D or 3D manifolds, these extensions are limited to cases with identical underlying dynamics and simple coordinate transformations. The framework does not demonstrate robustness to complex topologies, irregular meshes, or noisy real data. A more realistic validation on non-ideal geometries (e.g., curved or anisotropic manifolds) would strengthen the generalization claim."}, "questions": {"value": "Extension beyond second-order derivatives.\nThe current invariant basis is limited to first-order time and second-order spatial derivatives. Could the authors clarify whether their invariant construction extends in a systematic way to higher-order PDEs (e.g., fourth-order Cahn–Hilliard, Kuramoto–Sivashinsky)? Are there computational or theoretical barriers that prevent this extension?\n\nHandling variable coefficients and inhomogeneities.\nThe proposed framework appears to assume constant coefficients and homogeneous domains. How would the method incorporate spatially varying coefficients (e.g., position-dependent diffusivity) or explicit time dependence? Would introducing these as additional scalar fields preserve coordinate invariance?\n\nScope of generalization.\nThe paper claims dimension- and geometry-agnostic learning, but the examples mostly involve smooth coordinate transformations (1D → 2D Cartesian or spherical). Can the authors clarify whether the framework can handle more complex manifolds (e.g., irregular meshes, topologically nontrivial domains, anisotropic metrics)?\n\nComparison with neural operator baselines.\nHave the authors compared the proposed method with established symbolic machine learning approaches like deep symbolic regression, PySR, Finite Expression Method? \n\nInterpretability claim.\nThe paper emphasizes physical interpretability, but the learned mapping F(B) is still a neural network with no transparent internal structure. Can the authors provide evidence that the model yields interpretable or analyzable representations (e.g., through symbolic regression, sparsity, or feature saliency)? If not, would they consider revising the claim to “physically consistent” rather than “interpretable”?\n\nQuantitative evaluation and error metrics.\nThe experiments are primarily qualitative. Could the authors report quantitative metrics (e.g., relative L2 or energy norm errors) for predictions across domains and compare these to direct numerical simulations or neural operator baselines?\n\nEffect of noise and data sparsity.\nReal-world data are often sparse or noisy. The paper briefly mentions smoothing but does not explore robustness. How does the invariant representation behave under noise, interpolation errors, or partial observability of fields?\n\nComputational cost and scalability.\nThe invariant construction and coordinate-free integration may add overhead compared to standard PDE solvers or neural operators. Can the authors discuss computational complexity and memory scaling when applied to high-resolution or multi-field systems?\n\nAutonomous vs. non-autonomous systems.\nThe current formulation focuses on autonomous dynamics. Could the authors confirm whether the approach supports non-autonomous PDEs (e.g., external forcing or time-varying boundary conditions) and, if so, how time-dependent features would be integrated?\n\nClarification of the claimed “dimension-agnostic” property.\nThe notion of “dimension-agnostic” could be interpreted in multiple ways. Is the learned operator truly dimension-independent (i.e., reusable across dimensions without retraining), or is it empirically adjusted for each target dimension through retraining or fine-tuning? Clearer experimental evidence would help substantiate this claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DDPcbeLPBG", "forum": "CC2vIx3GZM", "replyto": "CC2vIx3GZM", "signatures": ["ICLR.cc/2026/Conference/Submission14220/Reviewer_wQZM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14220/Reviewer_wQZM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960335970, "cdate": 1761960335970, "tmdate": 1762924676538, "mdate": 1762924676538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}