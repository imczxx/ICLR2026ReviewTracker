{"id": "C9LAf2tlKj", "number": 3814, "cdate": 1757533301087, "mdate": 1759898068527, "content": {"title": "On Structured State-Space Duality", "abstract": "Structured {S}tate-{S}pace {D}uality (SSD)  [Dao \\& Gu, ICML 2024] is an equivalence between a simple Structured {S}tate-{S}pace {M}odel (SSM) and a masked attention mechanism. \nIn particular, a state-space model with a scalar-times-identity state matrix is equivalent to a masked self-attention with a $1$-semiseparable causal mask.\nConsequently, the same sequence transformation (model) has two algorithmic realizations: \na linear-time $O(T)$ recurrence or as a quadratic-time $O(T^2)$ attention.\nIn this work, we formalize and generalize this duality:\n(i) \nwe extend SSD from the scalar‑identity case to general diagonal SSMs (diagonal state matrices); \n(ii) we show that these diagonal SSMs match the scalar case's training complexity lower bounds while supporting richer dynamics;\n(iii) we establish a necessary and sufficient condition under which an SSM is equivalent to $1$-semiseparable masked attention; \nand\n(iv) we provide a negative result that such duality is impossible to extend to standard softmax attention due to rank explosion.\nTogether, these results strengthen the theoretical bridge between recurrent SSMs and Transformers, and widen the design space for expressive yet efficient models.", "tldr": "", "keywords": ["mamba", "mamba2", "structured state-space model", "transformer", "attention"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e621e06fb18d95f37c0ac1fe2296282e85e315da.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates and generalizes the \"Structured State-Space Duality\" (SSD) , a established equivalence between Structured State-Space Models (SSMs) and masked attention mechanisms. The authors build upon the work of Dao & Gu (2024), which demonstrated this duality for SSMs with simple scalar-times-identity state matrices. The core contributions are fourfold: (i) formalizing the duality for general diagonal SSMs, showing they are equivalent to a sum of N 1-semiseperable masked attentions; (ii) demonstrating that these diagonal SSMs can be trained with the same optimal $O(TN)$ complexity as the scalar case; (iii) establishing a necessary and sufficient condition under which a general SSM has a 1-semiseperable masked attention dual, relating it to the number of \"new columns\" in the induced kernel matrix; and (iv) providing negative results showing the duality cannot extend to standard softmax attention or general low-dimensional SSMs due to rank constraints. The work strengthens the theoretical bridge between recurrent and attention-based sequence models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The extension from scalar-identity to general diagonal SSMs is a non-trivial generalization that substantially widens the design space for sequence models. The introduction of the \"new columns\" concept to characterize the duality is a novel and insightful theoretical tool.\n\n* The paper provides a complete picture by not only proving the positive results but also clearly drawing the boundaries of the theory with negative results.\n\n* The technical execution is of good quality. The proofs are well-written and comprehensive. The paper's structure is logical, and the formal definitions and propositions are precise. \n\n* This work provides a crucial theoretical underpinning for two classes of widely used sequence models. By formally connecting more expressive SSMs to structured attention, it enables future work to explore a architectural advancements."}, "weaknesses": {"value": "* My biggest concern is by far the empirical validation. While the paper's contributions are theoretical, even a small-scale synthetic experiment would greatly elevate the value of the work. \n\n* The other primary weakness is the highly abstract and algebraic presentation, which may limit its immediate impact and understanding outside a core theoretical audience. \n\n* The paper repeatedly and (most likely) correctly claims that diagonal SSMs support \"richer dynamics\" and \"$N$ independent state modes.\" However, this claim is never illustrated and is not immediately obvious. A small, toy example of a sequence-processing task that a diagonal SSM can solve but a scalar-identity SSM cannot would make the practical motivation for the generalization much more concrete."}, "questions": {"value": "* Could the authors provide more intuition for Definition 4.5 (\"new column\")? What is the sequential, state-space interpretation of a \"new column\" arising in the kernel matrix M? Does it relate to the model's memory or its ability to create new, linearly independent contexts?\n\n* Looking forward, what immediate and concrete directions do you see this work enabling for the architectural design of new sequence models? \n\n* The practical success of models like Mamba is due not just to their linear-time complexity, but also to their hardware-aware design, which allows for fast, fused GPU kernels. Your paper proves that general diagonal SSMs match the optimal $O(NTd)$ training complexity of the scalar case. Does this theoretical efficiency necessarily translate to similar practical speed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2z0qvGCmTv", "forum": "C9LAf2tlKj", "replyto": "C9LAf2tlKj", "signatures": ["ICLR.cc/2026/Conference/Submission3814/Reviewer_AtZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3814/Reviewer_AtZV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619130552, "cdate": 1761619130552, "tmdate": 1762917048075, "mdate": 1762917048075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the extensions of the results presented in [1] to more general diagonal SSMs (i.e., SMMs with a diagonal state matrix). In particular, they extend their results from the scalar identity case (i.e., the state matrix is of the from $\\alpha \\mathbb{I} $ to general diagonal SSMs, investigating their training time complexity in the process. Thereby they also show training complexity preservation.  Additionally, they present the necessary and sufficient conditions for a general diagonal  SSM to have a 1-semiseparable masked attention, building upon the number of new columns of a matrix M representing the input–output relation of the considered dynamical system. Finally, the authors highlight two limitations of the SSD framework: the impossibility of extending it to softmax attention due to the corresponding induced kernel's rank explosion, and the fact that general SSMs do not have a state-space dual, even for small state dimensions.\n\n[1]: Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through\nstructured state space duality. arXiv preprint arXiv:2405.21060, 2024."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well structured, with its individual contributions clearly stated at the beginning and addressed one by one in the subsequent sections. \n\n- The increment to the work in [1] is clearly stated. \n\n- The detailed background section makes the paper largely self-contained and easier to follow. \n\n- The provided figures present helpful visualizations of $ b^{n} $ and $ c^{n} $."}, "weaknesses": {"value": "- While the general structure of the paper is sound, its technical derivations show several inconsistencies, listed in the following\n\n- The introduced dynamical system is not consistent with their definition of $ M_{t,s} $ in equation (3.3). Setting $ h_{0} = 0 $ the input-output relation at $ t = 1 $ should be $ y_{1} = c_1^{\\top}b_1x_{1} $; however, $ M_{1,1} $ is defined as $ c_1^{\\top}A^{1}A^{2}b_1x_{1} $. This could also be a matter of (not properly introduced) notation, but it should be addressed to restore consistency. \n\n- This inconsistency is then also present in (3.4) and many subsequent equations. \n\n- For their special case derivations with state dimension equal to 1, the input–output relation additionally does not remain consistent with the roles of b and c and I think diag(p) and diag(q) should be interchanged for the equation on line 205.\n\n- The matrix M is simultaneously used for the input–output relation as well as for the masking matrix of a masked attention, which can lead to confusion and should be avoided.\n\n- To the best of my understanding, the definition of $ M^{n} $ would not work with the current system definition, and the multiplication order of $b^{n}$ and $c^{n}$ should be interhchanged. If I'm not mistaken, this would then also hold for the equation on line 294 and the definitions of $b^{'n}$ and $c^{'n}$ should be adjusted accordingly, such that $b^{'n} = b^{n}/(A_{n,n}^{1}....)$ and $c^{'n} = c^{n}(A_{n,n}^{1}....)$.\n\n\nFurthermore, I have a few minor comments. \n\n- In Part 1 of the proof of Proposition 4.6, it should be stated that M is assumed to have N + 1 new columns. \n\n- There is a small typo on line 407."}, "questions": {"value": "- How do your findings, especially regarding the rank blow up of the induced kernel of softmax attention relate to the findings using the dynamical systems framework in [2]. \n\n- Where follows your provided Lemma 4.7 from? I think only part of it can be deducted from Proposition 4.6. \n\n- In the appendix line 615/616, should the $A^{j_1}$ in the definition of $S^{2}[i,:]$ be a $A^{i_1}$?\n\n[2]: Sieber, Jerome, et al. \"Understanding the differences in foundation models: Attention, state space models, and recurrent neural networks.\" Advances in Neural Information Processing Systems 37 (2024)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2HX7WNyA53", "forum": "C9LAf2tlKj", "replyto": "C9LAf2tlKj", "signatures": ["ICLR.cc/2026/Conference/Submission3814/Reviewer_RkCy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3814/Reviewer_RkCy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905202206, "cdate": 1761905202206, "tmdate": 1762917047779, "mdate": 1762917047779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The paper extends Structured State-Space Duality (SSD) from scalar-identity state matrices to general diagonal state matrices.\n2. It proves that diagonal SSMs retain the same asymptotic compute and memory complexity as the scalar case.\n3. The authors formalize and prove the conjecture from [1] that an SSM of state dimension (N) corresponds to an (N)-semiseparable matrix.\n4. They further provide a necessary and sufficient condition for when a general SSM admits a 1-SS masked-attention dual, namely that the induced sequence matrix is block-diagonal with at most (N) new columns per block.\n\n---\n\n[1]: *Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality*, Tri Dao and Albert Gu, 2024"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well written and is an easy read.\n2. The paper does a good job presenting and abstracting the most important results from previous works"}, "weaknesses": {"value": "1. **The extension of SSD to diagonal SSMs is mathematically sound but system inefficient.**\n\n   The diagonal extension rewrites the mixer entry $(C_i^\\top A_i\\cdots A_{j+1}B_j)$ as a sum over state coordinates. This results in a per-state coordinate outer product gated by 1-SS decay mask. In contrast SSD, by the virtue of the fact that $A_i$ is diagonal, features a GPU friendly matmul version: $(C B^\\top) \\odot L$. The latter uses tensor cores which are 20-30x faster than non-tensor core operations used by the former formulation. Furthermore, as noted in their paper, the reason SSD made the choice to keep $A_i$ as a scalar was to make use of this tensor core acceleration.\n\n2. **While the Big-O FLOPs and memory do not change, they differ by a huge constant factor vis-a-vis SSD**.\n\n     While the authors are correct that theoretically diagnonal SSMs have the same Big-O FLOP count---multiplying with a matrix with diagonal matrix or with scalar have same FLOPs---and are equally parallelizable, the issue is that, as state above, in practice the proposed method very slow as it does not use tensor cores.\n\n     Also the memory requirement for both the methods is $O(TNd)$, where $T$: sequence length, $N$: state size, $d$: model dimension, using SSD formulation reduces the memory requirement by (constant) factor of chunk size (generally 128). This is why SSD, as opposed to Mamba-1 which also used a diagonal state transition matrix, avoided materializing (B,L,D,N) intermediates states in HBM and got away with materializing chunks in the much faster SRAM.\n\n3. **Novelty of N-SS and N-SSS equivalence.**\n\n   The claim in the introduction of proving the N-SS = N-SSS \"conjecture” in the introduction is misleading. Authors also admit in a remark later in the paper that this results has been well established in the literature. Positioning it as a conjecture resolved here overstates the contribution which is qualified later in the paper to be a more approachable proof for an ML audience.\n\n\n4. **Theorem 4.8 is interesting but it needs more work to make it implementable**\n\n   The column-growth/diagonal-block condition for 1-SS masked-attention duals is novel as far as I know. However, the paper stops short of (i) an algorithm to transform the general SSM formulation into the tensor-core accelerated SSD-like dual, and (ii) empirical evidence that such a formulation is empirically better than SSD and is comparable in speed."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bxESiDb0y1", "forum": "C9LAf2tlKj", "replyto": "C9LAf2tlKj", "signatures": ["ICLR.cc/2026/Conference/Submission3814/Reviewer_f3V3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3814/Reviewer_f3V3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936223790, "cdate": 1761936223790, "tmdate": 1762917047552, "mdate": 1762917047552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}