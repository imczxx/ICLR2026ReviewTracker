{"id": "PUm9OaFNvf", "number": 809, "cdate": 1756819301614, "mdate": 1759898240808, "content": {"title": "Rethinking Traffic Representation: Pre-training Model with Flowlets for Traffic Classification", "abstract": "Network traffic classification with pre-training has achieved promising results, yet existing methods fail to represent cross-packet context, protocol-aware structure, and flow-level behaviors in traffic. To address these challenges, this paper rethinks traffic representation and proposes Flowlet-based pre-training for network analysis. First, we introduce Flowlet and Field Tokenization that segments traffic into semantically coherent units. Second, we design a Protocol Stack Alignment Embedding Layer that explicitly encodes multi-layer protocol semantics. Third, we develop two pre-training tasks motivated by Flowlet to enhance both intra-packet field understanding and inter-flow behavioral learning. Experimental results show that FlowletFormer significantly outperforms existing methods in classification accuracy, few-shot learning and traffic representation. Moreover, by integrating domain-specific network knowledge, FlowletFormer shows better comprehension of the principles of network transmission (e.g., stateful connections of TCP), providing a more robust and trustworthy framework for traffic analysis.", "tldr": "FlowletFormer is a pretraining model for network traffic analysis that improves classification by segmenting traffic into semantically meaningful units, capturing multi-layer protocol semantics, and enhancing inter-packet learning.", "keywords": ["Traffic Classification", "Pre-training", "BERT", "FlowletFormer"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f78556883f79154ec872559ecee84c95ab019b4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "One of the many papers tackling traffic analysis with a BERT-like (with very tiny engineering changes) approach. The paper follows the same line an present very minor incremental detail, with no actual methodological contribution.\n\nSeveral of these paper and the dataset they use have been found to be flawed and debunked in several rank A* conferences -- starting from ACM CCS'2022 and with an up-to-date debuning in ACM SIGCOMM'2025\n\nThis ignorance can be either be done on purpose (which is bordeline  unhetical... how can they cite a paper from CCS'2018 and ignore *that* paper from CCS'2022? Same for citing SIGCOMM'2019 and ignoring *that* one in  SIGCOMM'2025?) or in full unawararennss of such work (which is equally worring and disqualify the authors work altogether), but the result does not change.\n\n\nAdditionally, the engineering is lightweight, the evaluation is biased and flawed: the paper would not be accepted at one of the above rank A* conference of network/security field.\n\nFinally, there is no methodological contribution, there is no statistical relevance: as such, the paper does not find its place in ICLR'26 either\n\n\n\n[CCS 2022] Jacobs, Arthur S., et al. \"Ai/ml for network security: The emperor has no clothes.\" Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security. 2022.\n\n\n[USENIX Sec 2022] Arp, Daniel, et al. \"Dos and don'ts of machine learning in computer security.\" 31st USENIX Security Symposium (USENIX Security 22). 2022.\n\n\n[SIGCOMM 2025]\nZhao, Yuqi, et al. \"The Sweet Danger of Sugar: Debunking Representation Learning for Encrypted Traffic Classification.\" Proceedings of the ACM SIGCOMM 2025 Conference. 2025."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper has more flaws than strenghts -- but if I have to find one, then I would say the paper is clearly written (although I don't agree with the  styling m"}, "weaknesses": {"value": "all weaknesses are detailed next in the **Question** section \n\n- Incremental in nature\n- No learning methodological contribution\n- Flowlet Lightweight engineering contribution\n- Tokenizer Lightweight engineering contribution\n- Possibly biased, weak evaluation, which translate into fundamentally \n- Lack of statistical relevance\n- lack of critical results analysis \n- lack of comparison baselines\n- lack of relevant technical details"}, "questions": {"value": "This paper has several flaws that prevent it from publishing\n\n\n\n## Incremental in nature\n\n add itself to the pile of paper in the comparison table\n\n## No learning methodological contribution\n\n BERT-based with ``automatic'' definition of flowlet based on interarrival time (IAT) and some loose considerations about tokenizer (flawed in my opinion) or loss (too narrow, specific and not a contribution per se)\n\n## Flowlet Lightweight engineering contribution\n\ndefining a flowlet by using an IAT filter (Alg in B2 p 16) is very lightweight. it is a bonus not having to define the number of packets in a burst a priori as generally done, but it does not qualify as a scientific/technical contribution (doublly so given the evaluation flaws)\n\n##  Tokenizer Lightweight engineering contribution\n\n the paper states that related work overlooks the nature of the protocol segmentation. it pompously says to use morphenes to have undividied semnantic units -- but the true fact is that their tokenizer is not doing that at all. several fields in IP, TCP and any protocols are binary flags, which are independent yet packed together in the same byte for transfer, whose smallest units would be a bit and not a hex-unit (packing 4 such flags). So adding sub 16 sub encodings to a 65k vocabulary does not ring as a fundamental contribution unless you would be able to show instances (not rand% accuracy) where this does a semantic contribution -- yet even in Appx D. FL does joint Flowlet and tokenization\n\n## Possibly biased, weak evaluation, which translate into fundamentally flawed study.\n\nStarting from ACM CCS'2022 and with an up-to-date debuning in ACM SIGCOMM'2025, researchers have shown limits of these approaches attempting at learning from encrypted traffic payload. These studies are widely known, \nand additionally suggest (along with USENIX Sec'22)  best practices that this work do not follow. At the end, the gap with the proposal and the simple baselines from ACM CCS'2022 and ACM SIGCOMM'2025 suggest this work is trying to shoot a mosquito with a cannon\n \n\n\n## Lack of statistical relevance\n\n all tables dumps 4 decimal values, no statistical relevance whatsoever -- no mean/ci, no repetittions, no statistical tests,  no paired tests, no critical distance plots.\n\n## lack of critical results analysis \n\nsome of the results show 1.0000 (Tab5)-- there is no critical analysis of the results whatsovere, likely some shortcut as those already shown in [CCS22] for the ISCX VPN-nonVPN and CIC-IDS-2017  datasets -- a 5 nodes DecisionTree achieves in excess of 99% accuracy for those tasks due to shortcut learning, which was debunked 5 years ago already\n\nwhereas shortcut learning are  mentioned in appendix C.2, it is not enough (randomized IP addresses and ports, and removed aboslute timestaps)\n\n## lack of comparison baselines\n\nbenchmnark for simple (eg given SEQNO sequence, have a specialized ML for \nthat taks, and show the gap) as done in CCS, SIGCOMM\n\n## lack of relevant technical details  \n\nfine tuning -- is it end-to-end (=destroying pre-training value)\nor layers are frozen -- check SIGCOMM'25 why this is relevant"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fSlPCZoYUK", "forum": "PUm9OaFNvf", "replyto": "PUm9OaFNvf", "signatures": ["ICLR.cc/2026/Conference/Submission809/Reviewer_LKds"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission809/Reviewer_LKds"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821769212, "cdate": 1761821769212, "tmdate": 1762915609113, "mdate": 1762915609113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FlowletFormer, a Flowlet-based pre-training framework for network traffic analysis that captures cross-packet context, protocol semantics, and flow-level behavior. It employs Flowlet and Field Tokenization, a Protocol Stack Alignment Embedding Layer, and two Flowlet-inspired pre-training tasks to enhance semantic and behavioral understanding. Experiments demonstrate that FlowletFormer achieves superior accuracy, few-shot adaptability, and robustness by incorporating domain-specific network knowledge."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces Flowlet and  Field Tokenization as efficient traffic representation.\n- Protocol Stack Alignment-Based Embedding Layer is proposed to explicitly encode the hierarchical semantics of network protocols, enabling the model to distinguish fields across protocol boundaries and better capture protocol-specific behaviors.\n- Two pre-training tasks are proposed. Extensive experiments are performed on comprehensive downstream tasks under various settings, demonstrating the effectiveness of the proposed method.\n- The paper is overall well written."}, "weaknesses": {"value": "- Besides the superior performance, the technical novelty follows the general BERT pretraining pipeline with similar pretraining tasks.\n- From the development of the community, besides the pretraining code, the model weights are suggested to be open-sourced."}, "questions": {"value": "- In Table 2, Flowlet Former underperforms TrafficFormer on USTC-TFC only. Please explain the reason."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PCnIDvnklU", "forum": "PUm9OaFNvf", "replyto": "PUm9OaFNvf", "signatures": ["ICLR.cc/2026/Conference/Submission809/Reviewer_FjKy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission809/Reviewer_FjKy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881188026, "cdate": 1761881188026, "tmdate": 1762915608981, "mdate": 1762915608981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of network traffic analysis. To effectively capture cross-packet context, protocol-aware structures, and flow-level behaviors, the authors propose a Flowlet-based pre-training framework. The framework consists of three major components: Flowlet and Field Tokenization, a Protocol Stack Alignment Embedding Layer, and two pre-training tasks designed to enhance both intra-packet understanding and inter-flow learning.\nExperimental results demonstrate the effectiveness and robustness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The overall paper is well-written.\n\nThis work investigates a BERT-based pre-training model, called FlowletFormer, for network traffic analysis. The proposed framework incorporates three key strategies: (1) Flowlet segmentation, (2) a Protocol Stack Alignment-based Embedding Layer, and (3) two pre-training tasks—Masked Field Modeling (MFM) and Flowlet Prediction Task (FPT).\nThe effectiveness of the method is evaluated on eight public datasets, demonstrating the superiority and robustness of the proposed approach."}, "weaknesses": {"value": "The three components together constitute FlowletFormer. Among them, which element plays the most critical role, and how are these components interrelated?\n\nIn the FlowletFormer framework, how are the Flowlet Prediction Task (FPT) and the Masked Field Modeling (MFM) task distinguished in the flowchart?\n\nRegarding the downstream tasks, the seven tasks mentioned appear to be from previous benchmarks. Have any new tasks been introduced or conducted in this work?\n\nIt seems somewhat unusual that the performance of TrafficFormer (2025) is lower than that of YaTC (2023) and ET-BERT (2022) in most cases—what factors might explain this discrepancy?\n\nFinally, in the ablation study, the pre-training (PT) stage shows the greatest influence on performance across the four evaluation metrics compared to other components. Could you clarify how the pre-training data were sourced from the three repositories?"}, "questions": {"value": "see above weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6NKxLcaKef", "forum": "PUm9OaFNvf", "replyto": "PUm9OaFNvf", "signatures": ["ICLR.cc/2026/Conference/Submission809/Reviewer_WPqN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission809/Reviewer_WPqN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762698605246, "cdate": 1762698605246, "tmdate": 1762915608858, "mdate": 1762915608858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}