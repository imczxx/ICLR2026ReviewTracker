{"id": "w9yhzYhzz4", "number": 8749, "cdate": 1758096954611, "mdate": 1762939994544, "content": {"title": "Alleviating Suboptimality of Flow Maps with Improved Self-Distillation Guidance", "abstract": "Consistency-based approaches have been proposed for fast generative modeling, achieving competitive results compared to diffusion and flow matching models. However, these methods often rely on heuristics to mitigate training instability, which in turn limits reproducibility and scalability. To address this limitation, we propose the generalized flow map framework, unifying recent consistency-based methods under a common perspective. Within this framework, we investigate the suboptimality of existing approaches and identify two key factors for reproducibility: time-condition relaxation and marginal velocity guidance. To incorporate these, we leverage self-distillation to guide consistency models along the marginal velocity. We further propose improved Self-Distillation (iSD) by exploring the design space of flow maps, thereby reducing reliance on heuristics. Our formulation naturally extends to classifier-free guidance, achieving four-step generation with an FID of 11.06 on ImageNet $256\\times256$. iSD shows qualitatively comparable results to prior few-step generative models, providing a theoretical and empirical foundation for reproducible consistency training.", "tldr": "", "keywords": ["Consistency Model", "Flow Map Model", "Generative Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9049cb848e6df868c49a1957f5897e884eb20c74.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper analyzes the weaknesses and instabilities of consistency and flow-map models, proposing an improved self-distillation objective to simplify the training procedure and improving stability. The method is validated on common image generation benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides an analysis of why consistency training can be unstable, and proposes a grounded method to overcome such an instability."}, "weaknesses": {"value": "I think the paper can benefit from a more clear writing. I found it hard to understand the contribution of the proposed improvements from the experimental section. \nIn particular, the actual contributions of the paper are reported in section 4.2. From my understanding, the main contribution is in the reformulation of the target for the consistency/flow-map loss, where the authors use the use the instantaneous velocity learned by the online model in the target, rather than the conditional velocity. I think an ablation on how this new target works compared to the one from eq.13 could help understanding the relevance of the contribution, keeping the remaining settings unchanged.\n \nThe Adaptive Weighting is not novel, and is the one used in MeanFlow and similar to pseudo-huber loss when p=0.5, so should not be mentioned as a contribution.\n\nThe use of the relaxed objective is not new and is already used in Inductive Moment Matching [1], MeanFLow, and other flow-map-based methods. The fact that using the Flow Matching Loss improves stability is also not new, and use in for example in FACM [2], even though the method is relatively recent and could be considered as concurrent work, but should definitely be added to the references.\n\nThe ablation part is unclear to me (see questions), and the experimental results on Imagenet are not competitive with state of the art.\n\nThe results on CIFAR-10 are actually relatively close to SOTA, which is generally < 3 FID. The improvement is substantial compared to [3], but a more systematic analysis using the same settings as in [3] and highlighting what contributes to the improvement could help appreciating the significance of the contribution.\n\n## References\n[1] Zhou, Linqi, Stefano Ermon, and Jiaming Song. \"Inductive Moment Matching.\"\n\n[2] Peng, Yansong, et al. \"Flow-anchored consistency models.\"\n\n[3] Boffi, Nicholas M., Michael S. Albergo, and Eric Vanden-Eijnden. \"How to build a consistency model: Learning flow maps via self-distillation.\""}, "questions": {"value": "- 1) In Table 3, the experiments start with a baseline. What is the baseline?\n- 2) The second and third entry with CFM and CT relaxation are respectively the addition of L_CFM and the s-conditioning, correct?\n- 3) What is the exactly the addition of self-distillation? Does it refer to Eq 14 or to the addition of CFG with weight w=1.5?\n- 4) Why would simply switching from the linear interpolation to the TrigFlow kernel bring such a big boost in performance?\n- 5) Are your CIFAR-10 result obtained with CFG? It's not entirely clear to me what the final configuration is.\n- 6) Are the authors using a pretrained model or are the weights initialized at random?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JFzvo5fQjg", "forum": "w9yhzYhzz4", "replyto": "w9yhzYhzz4", "signatures": ["ICLR.cc/2026/Conference/Submission8749/Reviewer_g7CR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8749/Reviewer_g7CR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533589359, "cdate": 1761533589359, "tmdate": 1762920537915, "mdate": 1762920537915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thank you for your reviews. Based on the comments, we have decided to withdraw our submission. We truly appreciate the constructive suggestions, and we will incorporate them to improve the work for future publication."}}, "id": "mIib93DXY8", "forum": "w9yhzYhzz4", "replyto": "w9yhzYhzz4", "signatures": ["ICLR.cc/2026/Conference/Submission8749/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8749/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762936962657, "cdate": 1762936962657, "tmdate": 1762936962657, "mdate": 1762936962657, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Improved Self-Distillation Guidance (iSD), a new framework for training Consistency Models (CMs) and Flow Matching (FM) models that aims to mitigate the suboptimality and instability issues inherent in existing methods. The authors first unify CMs and FMs under a generalized flow map formulation, identifying that conventional training objectives optimize conditional rather than marginal velocity fields—leading to unstable or irreproducible training. Building on this insight, they propose iSD, which relaxes the time constraint \n$s=0 \\rightarrow s < t$ and introduces self-distillation guided by marginal velocities. This modification improves convergence stability and reproducibility without requiring pretraining or heuristic regularization. Empirically, iSD achieves competitive or superior results on CIFAR-10 and ImageNet-256 with few-step sampling, outperforming prior self-distillation and shortcut-based methods in both FID and stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a conceptually clear and theoretically grounded reformulation of consistency training through the generalized flow map framework, offering a rigorous explanation of why existing self-distillation methods underperform.\n2. The iSD objective is a simple yet elegant modification that improves both the stability and reproducibility of CM training while reducing dependence on heuristic regularizers.\n3. Empirical results are comprehensive and reproducible, showing substantial gains in both FID and standard deviation across runs—addressing a long-standing criticism of CM reproducibility.\n4. The introduction of Pre-CFG guidance as an integrated training mechanism is a practically relevant addition that aligns well with modern diffusion model practices."}, "weaknesses": {"value": "1. While the theoretical motivation is solid, the derivation and intuition behind marginal vs. conditional velocity mismatch could be explained more intuitively; the connection to variance reduction or stochastic gradient alignment is only briefly mentioned.\n2. The computational trade-offs of iSD—especially the approximate JVP computation—are not quantified in FLOPs or runtime, leaving uncertainty about scalability to higher-resolution datasets.\n3. Evaluation is primarily limited to ImageNet-256 and CIFAR-10; demonstrating results on higher-resolution tasks (e.g., text-to-image or unconditional generation) would strengthen the paper’s generality claims.\n4. Although the method improves reproducibility, ablation studies on the impact of each design choice (e.g., time relaxation vs. marginal guidance) are limited.\n5. The paper’s theoretical component stops short of a formal convergence guarantee; although the empirical stability results are convincing, a deeper analysis of training dynamics (e.g., curvature of loss landscape) would enhance credibility."}, "questions": {"value": "1. Could the authors provide more intuition on how marginal velocity alignment alleviates suboptimality in practice? Is this equivalent to a bias correction in gradient estimation?\n2. How sensitive is iSD to the choice of the relaxation schedule for $s<t$? Would adaptive or learned schedules further improve convergence?\n3. What is the computational overhead of the approximate JVP calculation, and does it affect inference latency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gVwKETmyiM", "forum": "w9yhzYhzz4", "replyto": "w9yhzYhzz4", "signatures": ["ICLR.cc/2026/Conference/Submission8749/Reviewer_5saY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8749/Reviewer_5saY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891284762, "cdate": 1761891284762, "tmdate": 1762920537493, "mdate": 1762920537493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a generalized flow map framework that unifies recent consistency-based generative models and identifies two root causes of their suboptimality and instability: (i) lack of marginal velocity guidance and (ii) fixed time conditioning $(s = 0)$. To address these, the authors introduce improved Self-Distillation (iSD), which combines self-distilled marginal velocity guidance with relaxed time conditioning $(s < t)$, JVP approximation, and classifier-free guidance (CFG). The method trains from scratch without heuristics and achieves competitive 4-step FID of 11.06 on ImageNet 256×256 with high reproducibility. The work provides theoretical analysis of convergence gaps and empirical validation across design choices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Generalized flow map (Eq. 6) cleanly subsumes prior methods (Tab. 1); Propositions 3.3–3.4 rigorously show suboptimality/instability of direct and consistency training.\n2. The method builds on DiT and avoids distillation from external teachers. Comparisons to UCGM, MeanFlow, Shortcut Model, and iCT are fair and comprehensive."}, "weaknesses": {"value": "1. All main results are on class-conditional ImageNet 256x256. There is no demonstration on unconditional generation, higher resolutions (512x512), or other modalities (e.g., text-to-image). This limits claims of generality.\n2. While stable, iSD requires ~800K iterations and joint optimization of $L_{CFM} + L_{SD}$, which is slower than one-stage consistency training (e.g., UCGM). No wall-clock time or GPU-hour comparison is provided.\n3. iSD-U (Pre-CFG) is empirically best but relaxes $s = t$, violating the marginal flow map convergence proof. The paper acknowledges this but does not quantify the practical impact of this compromise."}, "questions": {"value": "Refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "66K87ppxdD", "forum": "w9yhzYhzz4", "replyto": "w9yhzYhzz4", "signatures": ["ICLR.cc/2026/Conference/Submission8749/Reviewer_Momx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8749/Reviewer_Momx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971950934, "cdate": 1761971950934, "tmdate": 1762920537081, "mdate": 1762920537081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}