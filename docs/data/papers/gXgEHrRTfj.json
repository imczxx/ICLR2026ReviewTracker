{"id": "gXgEHrRTfj", "number": 14972, "cdate": 1758246370373, "mdate": 1763765564188, "content": {"title": "Euclid: Lessons for Geometric Low-level Visual Perception in Multimodal LLMs", "abstract": "Multimodal large language models (MLLMs) have advanced rapidly in recent years, yet they continue to struggle with *low-level visual perception*—particularly, in accurately identifying and describing geometric relationships within images. In this paper, we first diagnose this shortcoming by introducing a dedicated benchmark, *Geoperception*, which focuses exclusively on evaluating geometric low-level perceptual capabilities that serve as essential prerequisites for higher-level visual reasoning. We then present a comprehensive empirical study that investigates strategies for improving model performance in this setting, making use of synthetic geometry data. Our findings highlight the benefits of certain model architectures and training techniques, including training with a data curriculum, the use of CNN-based visual encoders and the effect of LLM size and finetuning visual encoders. Of note, we find that adopting a data curriculum enables models to learn challenging geometric concepts that they fail to acquire from scratch. Finally, we explore how well we can endow multiple geometric low-level visual perception capabilities into one generalist MLLM. We demonstrate that training on such data with carefully chosen composition can significantly enhance a model's geometric visual perception ability *without compromising its general multimodal capabilities*, shedding light on the development of future generalist MLLMs that can excel simultaneously across multiple challenging domains.", "tldr": "We study low-level geometric visual perception in multimodal LLMs by (1) releasing a benchmark (Geoperception), (2) conducting an empirical study on the MLLM design space, and (3) training a model (Euclid) with strong geometric perception abilities.", "keywords": ["Multimodal LLMs", "Geometric Perception", "Low-level Visual Perception"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4955b8650b6a7bd2de5cd4901fc62b0e504a3ddc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper analyzes the capability of MLLMs to identify and describe geometric relationships among geometric shapes in images. The authors introduce a new benchmark, Geoperception, and conduct experiments to investigate the low-level visual perception capability of MLLMs. The paper provides detailed analyses and identifies the benefits of curriculum learning and CNN-based visual encoders. Finally, the authors show that MLLMs trained on their dataset, Euclid-200k, achieve improved performance on out-of-distribution multimodal benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* This paper focuses on the low-level visual perception of MLLMs, which is an important research topic for improving their performance and reliability.\n\n* The analysis in Section 4 is well controlled and provides valuable insights for the future development of MLLMs. This paper presents useful suggestions for enhancing the low-level geometric perception of MLLMs, including the effectiveness of curriculum learning and CNN-based visual encoders. It also demonstrates that using larger LLMs or fine-tuning visual encoders has only limited influence."}, "weaknesses": {"value": "I consider that this paper presents very useful observations, as mentioned in the Strengths section, but there are some concerns regarding the details of the dataset and experiments.\n\n* **Rationale behind the task selection**. Among aspects of low-level geometric perception, this paper particularly focuses on the capability of MLLMs to accurately identify and describe geometric relationships within images. However, there is no clear explanation of why the paper focuses only on this aspect, rather than on other capabilities such as understanding individual geometric shapes.\n\n* **Diversity of images in the dataset**. The introduced dataset is derived from Geometry-3K, which includes geometric shapes in a uniform format with the same color, line thickness, and text font and size. This level of uniformity makes it less suitable as an evaluation benchmark. Even within geometric shapes, there can be some variation in style and format, and it would be better for an evaluation dataset to include more diversity.\n\n* **Missing citation**. Kamoi et al. (COLM 2025) [1] introduce a similar dataset. Their dataset is also designed to evaluate low-level geometric perception and includes tasks similar to AngleClassification and LineComparison. This paper should compare Geoperception with their dataset.\n\n[1] Kamoi et al. (COLM 2025). VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information. COLM 2025. https://openreview.net/forum?id=PYHwlyu2fa\n\n---\n\nThe following additional experiments would strengthen the paper, but I do not prefer to request too many additional experiments during the rebuttal phase. The authors may prioritize experiments requested by other reviewers. If feasible, I would particularly like to see additional results for \"Training: Model diversity.\n\n* **Evaluation: Model diversity**. [1] reported that Gemini-2.5-Pro shows a substantial improvement in geometric perception compared to previous models. The paper would be strengthened if the authors also included results for this model. If there are constraints in budget, it would also be helpful to provide results on some challenging tasks such as POL.\n\n* **Training: Model diversity**. I consider the analysis in Sections 4 and 5.2 to be a major contribution of this paper. However, the paper only evaluates models with the Qwen-2.5 family, which is not sufficient to support their claim. Evaluation with at least one more model family would largely strengthen the paper.\n\n* **Training: Model scale**. As mentioned above, the experiment in Section 5.2 is conducted only with Qwen-2.5-VL-3B, which is a relatively small model. Experiments on larger models, even at the 8B scale, would strengthen the claim. *I understand the constraints in computational resources, so this point did not affect my final score.*"}, "questions": {"value": "* I would appreciate responses to the points I listed in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TxRTxgkkzN", "forum": "gXgEHrRTfj", "replyto": "gXgEHrRTfj", "signatures": ["ICLR.cc/2026/Conference/Submission14972/Reviewer_AqGS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14972/Reviewer_AqGS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761337796915, "cdate": 1761337796915, "tmdate": 1762925305970, "mdate": 1762925305970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank the reviewers for valuable and constructive comments. We will address shared concerns of the reviewers below.\n\n1.  Concerns that the dataset is too synthetic (pYoq), lacks diversity (AqGS), or that the selected tasks are arbitrary/limited (1TVt, AqGS)\n\nFor Geoperception, rather than relying on synthetic data, all images in Geoperception originate from real-world, textbook-style geometry datasets that feature diverse geometric shapes, background colors, and annotation styles, including human-drawn annotations. Moreover, all geometric relations are manually annotated and reorganized into large-scale question–answer pairs to support systematic and robust evaluation.\n\nFor empirical study, following the reviewer’s suggestion, we adopt Geoperception as the out-of-domain evaluation throughout training and repeat our experiments on LLM size and visual encoder, the result is included in Figure 10 and Figure 11 in the updated version of the paper. Our observations remain consistent with the conclusions drawn from the main experiment. Notably, although most visual encoders achieve near-perfect in-domain performance in Figure 4, we find that a lower final training loss is associated with better OOD generalization for CNN-based visual encoders, as demonstrated in Figure 11.\n\nIn addition, we also repeat the experiment comparing ViT and ConvNeXt but under the joint-training setting across all seven tasks with 43 unique geometry arrangements, reporting both the training loss and the testing accuracy. The results, presented in Figure 12 of the updated version of the paper, show that ConvNeXt converges faster and attains a higher final testing accuracy than ViT. These observations also remain consistent with the conclusions drawn from our main experiments.\n\n\n2. Limited novelty in scaling dataset size, and performance gains are restricted to in-domain evaluations.\n\nFollowing the reviewers’ suggestions, we have additionally evaluated Euclid models on We-Math and GeoQA datasets, both of which are general mathematical reasoning benchmarks that are OOD. Euclid models demonstrate marked improvements when trained on a mixture of our dataset and general multimodal reasoning datasets. This observation suggests that low-level perception can be a bottleneck for mathematical reasoning, and our dataset directly addresses this challenge. \n\n| Model               | Geoperception | MMBench | MMStar | MathVista | Math-Vision | We-Math | GeoQA |\n|---------------------|---------------|---------|--------|-----------|-------------|---------|-------|\n| Random Baseline     | 16.37         | 2.90    | 24.60  | 17.90     | 5.86        | 21.59   | 25.00 |\n| Qwen-2.5-VL-3B       | 26.89         | 77.84   | 47.87  | **52.08** | 15.92       | 35.34   | 51.48 |\n| Qwen-General-Only    | 30.00         | 81.01   | 47.73  | 51.66     | **16.79**   | 38.33   | 53.43 |\n| Euclid               | **78.70**     | 74.87   | 43.20  | 50.23     | 13.87       | 31.90   | 38.26 |\n| Euclid-Star             | 78.53         | **81.52** | **48.13** | 51.38 | **16.79** | **42.13** | **61.66** |"}}, "id": "mkr4uLN834", "forum": "gXgEHrRTfj", "replyto": "gXgEHrRTfj", "signatures": ["ICLR.cc/2026/Conference/Submission14972/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14972/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14972/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763766044230, "cdate": 1763766044230, "tmdate": 1763766044230, "mdate": 1763766044230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets a well-know gap in MLLMs: low level geometric visual perception. The authors introduce Geoperception, a 2D geometry benchmark derived from real textbooks with seven task types, and show that leading open/closed models underperform humans. They also build a controlled synthetic geometry data engine to study training/architectural choices and report several lessons. Finally, they train Euclid (Qwen-2.5-VL-3B) on a 200k synthetic instruction dataset and report competitive gains on LLVP without obvious degradation on general tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. By isolating LLVP in 2D geometry and using a synthetic engine, the study can attribute effects to data/architecture rather than confounds from high-level semantics.\n2. Geoperception spans seven tasks and evaluates fifteen major MLLMs with a transparent scoring rule, plus a human baseline.\n3. Curriculum helps; CNN encoders excel at LLVP; and tuning the vision encoder is often unnecessary—practical guidance for building LLVP-sensitive MLLMs."}, "weaknesses": {"value": "1. It’s almost a community consensus that multimodal models underperform on a specific task mainly because they lack task-specific data; this paper’s core contribution is essentially to reiterate that point. In my view, the novelty is insufficient.\n2. As for the other findings—e.g., that CNN-based visual encoders are more suitable than ViT—this is not surprising. I suspect the result is largely due to insufficient data; with proper data scaling, the conclusion may not hold.\n3. That MLLMs struggle with LLVP is already broadly observed; the novelty mainly lies in a geometry-focused, cleaner testbed and a systematized study, rather than a new principle or paradigm."}, "questions": {"value": "How do your conclusions change under explicit domain randomization on the rendering/annotation pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BawzmIqTqo", "forum": "gXgEHrRTfj", "replyto": "gXgEHrRTfj", "signatures": ["ICLR.cc/2026/Conference/Submission14972/Reviewer_sdwW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14972/Reviewer_sdwW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761671431435, "cdate": 1761671431435, "tmdate": 1762925305578, "mdate": 1762925305578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Geoperception, a benchmark revealing that current multimodal large language models struggle with geometric low-level visual perception, such as identifying collinearity, angle types, or line lengths. To address this, the authors create a synthetic 2D geometry dataset engine and conduct controlled experiments, uncovering four key lessons. Additionally, they train Euclid, a 3B-parameter model on a 200k synthetic dataset, resulting in improved performance on Geoperception while retaining general capabilities."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated problem: The paper clearly articulates a real and consequential gap in MLLM capabilities: LLVP, which impacts applications like mathematical reasoning, robotics, and medical imaging.\n2. Actionable insights: The four lessons provide concrete guidance for improving LLVP in MLLMs, some of which challenge previous assumptions."}, "weaknesses": {"value": "The major problem of this paper is its poor soundness. As claimed by the authors, contribution of this work is threefold: the proposal of Geoperception, the conclusion of four lessons for LLVP, and training the Euclid model. However, none of these contributions are sufficiently solid.\n1. In the Geoperception benchmark, the proposed task types may not be able to cover geometric perception capabilities. For example, the authors mention task types PointLiesOnLine and PointLiesOnCircle, which are designed for (non-numeric) geometric shape understanding. However, there may be many other task types belonging to this category, including but not limited to: determining the type of a specific shape (triangle, rectangle, circle, etc.), identifying the geometric relationship between two shapes (parallel, similar, tangent, etc.), or estimating the size and position of geometric shapes.\n2. The four lessons are rather arbitrary with irresponsible overclaim. First of all, all ablation studies only include two subtasks. It is extreme overclaim that such narrow taskset could represent LLVP. Besides, each lesson seems to be established in unreliable experiment settings:\n  - Lesson 1: the dataset sizes of different settings are not equal. While for each difficulty level the dataset size used for training is 32k, the mixed configuration uses 96k training data in total. Therefore, it is unclear whether performance gains should be contributed to the inclusion of samples from different difficulty levels or simply the larger dataset size.\n  - Lesson 2: Most models converge to nearly 100% accuracy on the two subtasks, and it is insufficient to judge the capability of models solely by their convergence speed.\n  - Lesson 3: Similarly, ViT-g and ViT-L share extremely close terminating performance with convolution-based visual encoders.\n  - Lesson 4: The experiment only includes convolution-based visual encoders, while the popular ViT-based ones are uncovered.\n3. In sec. 5, the cliam \"endow multiple geometric low-level visual perception capabilities into a generalist MLLM\" is also unestablished.\n  - The improvements in Table 3 are trivial. Geoperception improvements are trivial because Geoperception and Euclid share identical (or at least very similar) data distributions. It is expected that training on in-domain data improves test performance. For the general benchmarks, compared with Qwen-General-Only, Euclid* exhibits only neglegible improvements (<1%). Fluctuations brought by different training seeds might even achieve greater difference. \n  - The scope of the incorporated benchmark is limited. It is helpful to add datasets such as GeoQA, Math-Vista or We-Math."}, "questions": {"value": "1. In line 275-276, what's the meaning of \"a mixture of **three** geometric shapes\"? Does it mean that there are only three fixed geometric arrangements for all the images, along with Figure 2? If so, the overclaim problem for the lessons will be even more severe since the data diversity is also limited in addition to limited task type; if not, what are the specific criteria for splitting easy, middle and hard sets in Sec. 4.1?\n2. Why not include Gemini-2.5-pro in Table 1 while the authors have already used it in dataset construction in Sec. 5.1? As the generally strongest MLLM, it will be helpful to report its performance on geometric perception."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lgMuuLdOC1", "forum": "gXgEHrRTfj", "replyto": "gXgEHrRTfj", "signatures": ["ICLR.cc/2026/Conference/Submission14972/Reviewer_1TVt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14972/Reviewer_1TVt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966951748, "cdate": 1761966951748, "tmdate": 1762925304584, "mdate": 1762925304584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors explore the issue of low-level visual perception (LLVP) of VLMs. They start by building a dataset with QA pairs for geometry understanding and demonstrate the significant shortcomings of VLMs. Then, they try to propose some basic ideas to test how to improve the model on these datasets, including Lesson 1: Exposure to simpler visual perception data aids in learning difficult LLVP. Lesson 2: Scaling LLM size yields limited gains in LLVP learning. Lesson 3: CNN-based visual encoders enable more efficient learning of LLVP tasks. Lesson 4: Tuning the vision encoder does not offer a strong advantage in learning LLVP. The authors further extend the dataset and train a model; results show that the proposed model achieves high performance on the LLVP dataset and can improve the performance on real-world datasets as well."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. LLVP is a very fundamental problem of VLMs, and without this capability, the VLM's performance on complex tasks can not be guaranteed.\n2. The paper is very well written and easy to follow.\n3. The lessons are useful for the future development of the VLMs.\n4. The proposed model is effective and can beat the performance of strong models on math reasoning datasets."}, "weaknesses": {"value": "1. Dataset validation. Geoperception is mostly synthetic, so the performance of the dataset largely reflects the performance on certain tasks and rules. It is not very convincing to say LLVP is bad for VLMs, given that the test results are based on a synthetic dataset. Thus, some human-annotated or drawn samples are helpful.\n\n2. Dataset choices in experiments. All experiments use LC and PLOL, narrowing the generalizability of the lessons. It might be better to run them on all datasets or change to some other dataset combinations for other lessons.\n\n3. Lacks related work and novelty. For instance, VisonlyQA [1] demonstrates a very similar conclusion and is not cited. Considering VisonlyQA, the novelty of this paper should be adjusted to a lower level.\n\n[1] Kamoi, Ryo, et al. \"Visonlyqa: Large vision language models still struggle with visual perception of geometric information.\" COLM 2025"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IEnfXaljAT", "forum": "gXgEHrRTfj", "replyto": "gXgEHrRTfj", "signatures": ["ICLR.cc/2026/Conference/Submission14972/Reviewer_pYoq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14972/Reviewer_pYoq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984968101, "cdate": 1761984968101, "tmdate": 1762925303913, "mdate": 1762925303913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}