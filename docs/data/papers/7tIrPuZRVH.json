{"id": "7tIrPuZRVH", "number": 19556, "cdate": 1758297247731, "mdate": 1759897032795, "content": {"title": "VARSHAP: A Variance-Based Solution to the Global Dependency Problem in Shapley Feature Attribution", "abstract": "Feature attribution methods based on Shapley values, such as the popular\nSHAP framework, are built on strong axiomatic foundations but suffer\nfrom a critical, previously underappreciated flaw: global dependence. As\nrecent impossibility theorems demonstrate, this vulnerability is not\nmerely an estimation issue but a fundamental one. The feature\nattributions for a local instance can be arbitrarily manipulated by\nmodifying the model's behavior in regions of the feature space far from\nthat instance, rendering the resulting Shapley values semantically\nunstable and potentially misleading.\n\nThis paper introduces VARSHAP, a novel feature attribution method that\ndirectly solves this problem. We argue that the source of the flaw is\nthe characteristic function used in the Shapley game — the model's\noutput itself. VARSHAP redefines this game by using the reduction of\nlocal prediction variance as the characteristic function. By doing so,\nour method is, by construction, independent of the model's global\nbehavior and provides a truly local explanation. VARSHAP retains the\ndesirable axiomatic properties of the Shapley framework while ensuring\nthat the resulting attributions are robust and faithful to the model's\nlocal decision landscape. Experiments on synthetic and real-world\ndatasets confirm our theoretical claims, showing that VARSHAP provides\nstable explanations under global data shifts where standard methods fail\nand demonstrates superior performance, particularly in robustness and\ncomplexity metrics.", "tldr": "", "keywords": ["explainability", "Feature Attribution", "Explainable AI (XAI)", "Shapley Values", "Local Interpretability", "Model-Agnostic Methods"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b3097b64ef633a741a5a4fc58de7467786aaaca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents VarSHAP as a novel value function for Shapley-based feature attribution. VarSHAP reformulates the classical value function looking at the expected model output (i.e. class-wise logits for classification, model output for regression) as a variance of the model output. The paper shows that this new value function addresses some known issues with feature attribution scores. \n\nThe following references are used in the remainder of the review:\n\n## References for Review\n- Fumagalli et al. 2025: https://proceedings.mlr.press/v258/fumagalli25a.html\n- Sobol, 2001: https://www.sciencedirect.com/science/article/abs/pii/S0378475400002706\n- Bordt, 2024: https://proceedings.mlr.press/v206/bordt23a/bordt23a.pdf"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Significance:** The paper studies an important question, in that attribution methods if applied poorly may lead to inconsistent or wrong interpretations. Therein, the paper addresses a good research topic and proposes an interesting solution. Simply reformulating the value function and thus introducing a quick fix to a problem allows the traditional black box estimation methods (KernelSHAP or Shapley interaction methods) to still be applicable.\n- **Synthetic Evaluations:** I like the use of synthetic evaluations for analyzing the core concepts of the work.\n- **Well Written:** The paper is generally well written."}, "weaknesses": {"value": "- **Framing and Motivation:** In my opinion, it is generally well known that the feature distribution plays a crucial role in interpreting feature-based explanations such as feature attribution. The recent AISTATS paper (Fumagalli et al, 2025) unifies this issue by linking cooperative game theory with functional ANOVA decomposition. The resulting framework makes it quite clear that the more information is modeled in the removal mechanism (conditional vs. baseline) the more influence the distribution has on the resulting explanations (e.g. attributions). This is generally not new and in my opinion not a drawback but an important positive side-effect of these methods. In your experiments (Section 3.1) you are **changing** the model function to be explained. While, yes at $x=(0,0)$ both models predict 0 output since you designed them to be identical at this point. But around this single point the models do not behave the same because of the introduction of an interaction between $X_1$ and $X_2$. This interaction is also influencing the attribution of the $X_2$ feature since the Shapley value is influenced by all interactions in the model (this is also the second dimension in the framework in Fumagalli et al, 2025). If you were to compute all Shapley interactions (for example after Bordt (2023)) then the attribution of $X_2$ will become zero again since it does not influence the model individually and there are no correlations between the $X_1$ and $X_2$. Hence, my point is that this _problem_ is not really a problem but a correct behavior of the attribution methods. I do not think that this is per-se a drawback for looking into different value functions (like it is done here) but I do think that this discussion should be **stronger substantiated** and compared to with more important **related methods** addressing this issue already (Sobol indices, Shapley interactions). \n- **No Source code Available**: The submission contains **no** code. This is quite a big problem for proper reviewing. I wanted to check the computational efficiency of the variance estimation (which the authors write is easy, but do not show), when I noticed this.\n- **Limited Evaluation:** While I wholeheartedly agree that for a contribution like this a proper synthetic evaluation is absolutely necessary (which the paper includes), but the synthetic examples (with three datasets) basically makes up the whole experimental evaluation. Section 3.3 paints a relatively unclear picture of the empirical implications of the proposed value function change. VarSHAP seems to be on par with the traditional value function. It is unclear how easy it is how to translate the paper to different data modalities and/or implement it in traditional data science pipelines. Experiments on real world data would greatly improve this.\n- **Missing Ablations:** The paper does not analyze the influence of traditional parameters such as dataset size, feature size, data modalities to VarSHAP. This makes it quite challenging to gauge the practical implications of the method."}, "questions": {"value": "- **Q1:** How does your value function compare against Definition 5 (page 6) of Fumgalli et al. (2025)? How does the new value function relate to the well known Sobol indices?\n\n- **Q2:** What are computational limits of your method? In lines 286-293 you describe the issue but you do not show any analysis in your empirical evaluation (particularly in regards to Section 3.3). Is this a bottleneck?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "80pfjNme77", "forum": "7tIrPuZRVH", "replyto": "7tIrPuZRVH", "signatures": ["ICLR.cc/2026/Conference/Submission19556/Reviewer_QD3h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19556/Reviewer_QD3h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571008403, "cdate": 1761571008403, "tmdate": 1762931438254, "mdate": 1762931438254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new feature attribution method, named VARSHAP,  that aims to eliminate the global dependence problem that affects the approximations of KernelSHAP. Instead of using the model’s direct output as the characteristic function, the proposed method defines it as the reduction in prediction variance.\nThe authors claim that VARSHAP maintains the axiomatic properties of Shapley values. The empirical evaluation shows that VARSHAP can provide robust attributions."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-The paper proposes a novel idea that approximates the Shapley values, but for a different objective compared to the traditional Shapley value explainers.\n\n-The paper is generally clear and easy to follow. \n\n-The evaluation includes case studies designed to measure certain properties of the proposed explanation method, as well as a benchmark evaluation."}, "weaknesses": {"value": "-The authors claim, in line 66, that the problem is in the Shapley value concept itself, yet the proposed solution uses the same Shapley value concept. I find this statement quite strong and potentially incorrect. I agree that the solution proposed by KernelSHAP can be inaccurate and can be improved, and that is not an issue of an implementation, but that does not extend to the Shapley value concept.\n\n-The proposed approach uses marginal expectations to marginalize features out of coalitions, but does not discuss the baseline removal approach [1].\n\n-One of the desired properties of Shapley value methods is the local accuracy, i.e., the solution matches the prediction of the underlying model, which makes their interpretation intuitive. On the other hand, VARSHAP proposes values that sum to \"the negative of the initial total variance under full local perturbation\", which I think is unintuitive and makes it difficult to explain the predictions, especially when the user is not an expert in machine learning or a statistician. \n\n-The proposed method violates the consistency property of Shapley values with respect to the original prediction, i.e., the Shapley value increases or stays the same if a player’s contribution grows or stays the same. In other words, if a feature $\\beta$ negatively affects the prediction and a feature $\\gamma$ positively affects it, but both ($\\beta$ and $\\gamma$) have similar effects on the prediction variance, both will be assigned the same importance, which I think makes the interpretation of the outcome more challenging. This property is promoted under the \"sign independence\", which I cannot understand why it can be considered a desired property.\n\n-KernelSHAP, which can be no better than random guessing (according to the paper), is outperforming the proposed VARSHAP with respect to fidelity. Additionally, VARSHAP never outperformed the competitors with respect to faithfulness in the LATEC benchmarking. \n\n-I doubt that the proposed approach is showing superior performance to KernelSHAP, as claimed in the conclusions. Additionally, VARSHAP has worse computational complexity than KernelSHAP.\n\n-The method is not compared to the unbiased KernelSHAP [2], which, given a sufficiently large number of samples, converges to the true Shapley values. I also think it addresses the global dependence that has been put as the central problem of this paper.\n\n\n[1]-Sundararajan, M. and Najmi, A. The many Shapley values for model explanation. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 9269–9278. PMLR, 13–18 Jul 2020.\n\n[2]-Covert, I. and Lee, S.-I. Improving kernelshap: Practical Shapley value estimation using linear regression. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130, pp. 3457–3465, April 2021."}, "questions": {"value": "1- Why can the sign independence be considered a desired property?\n\n2- How to explain the outcome of VARSHAP to a user who is not an expert in machine learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TmNMDHttrj", "forum": "7tIrPuZRVH", "replyto": "7tIrPuZRVH", "signatures": ["ICLR.cc/2026/Conference/Submission19556/Reviewer_k82t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19556/Reviewer_k82t"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840752444, "cdate": 1761840752444, "tmdate": 1762931437740, "mdate": 1762931437740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript identifies a feature of inherent in Shapley values, namely the global dependence identified by Bilodeau et al. (2024): as Shapley's value is a global estimator, it takes into account 'value added' across the data distribution.  Thus, 'local' explanations (e.g. feature importance for the third observation) can be influenced by altering 'irrelevant' relationships in the data.\n\nFor example (q.v. $\\S 3$), consider Dataset 1, comprising of patient types A and B.  Now consider Dataset 2, identical to Dataset 1, but adding a new patient type, C.  Shapley's value for a patient in group A changes from data sets 1 to 2, although the data-generating process has not changed.\n\nThe manuscript introduces VARSHAP which, instead of performing a weighted sum of 'value added', performs a weighted sum of variances.  Thus, whereas Shapley's value measures the change in a model's prediction resulting from knowing a feature's actual value (rather than a baseline value), VARSHAP measures the change in the variance of the model's predictions.\n\nAs the variances are derived from perturbations of the Gaussian, they are thin-tailed, so effects fade quickly with distance - attenuating the global dependence identified in Bilodeau et al.\n\nIn addition to the example above, the paper presents:\n1. ($\\S 3.2$) an example in which LIME (a linear, local approximation method) is fooled by a non-linear feature, while SHAP and VARSHAP correctly identify its irrelevance.\n1. ($\\S 3.3$) LATEC benchmark results comparing faithfulness, robustness and complexity scores of VARSHAP, SHAP and LIME on a range of models and datasets.  The authors conclude, ``For faithfulness, KernelShap ... often ranks top [while] VARSHAP variants excel in robustness ... [and] complexity metrics''."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Generally, I think that there are strong arguments for interpreting large ML models - and, thus, that there is an ongoing need for good research in this area.\n\nFurther, the authors extend Bilodeau et al.'s argument from marginal SHAP attributions to conditional SHAP attributions.\n\nFinally, the example in $\\S 3.1$ does raise concerns about how SHAP and LIME have been applied in that environment."}, "weaknesses": {"value": "The feature attribution literature has been strongly rooted in Shapley's value and variants.  Most contributions to the literature identify perceived mathematical weaknesses of e.g. the Shapley value, and propose a mathematical variant to resolve it.\n\nA small literature, though, returns to the motivating question: does the proposed measure help us explain or interpret the ML model?  This paper is detached from that question.  As such, in my view, the motivation is immediately weakened: yes, this is a tweak on the Shapley value that one can perform and, yes, it scores well in some metrics - but leaves open the question of whether it does anything to help the metrics implicitly motivating the literature: do developers or users better understand?\n\nOn the mathematics alone, the perturbations assume independence of features (p.4).  I would regard this as a secondary concern: any novel paper leaves open research questions.\n\nIndependently of this, I found the exposition less crisp than I would have liked:\n1. There is a lengthy verbal introduction that relies on assertions, rather than carefully defining terms or providing intuitions.  This left me unclear about what was being asserted, why it was a problem, and so on.  \n\n1. When seeing a modification of Shapley, I would like to know early on which Shapley axioms are being replaced, and with what.\n\n1. Some of the assertions seem misleading.  For example, the description of SHAP opening $\\S 2.1$ is vague enough to seem to better describe a partial derivative than Shapley's value.\n\n1. Proposition 1 uses terms like \"aims to satisfy\" and \"fundamentally\".  Functions don't have purpose: they satisfy or don't; I don't know what it means to \"fundamentally\" measure.  As the proposition's conclusion is that the function \"must take the following general form\", one would expect - instead - a phrasing like: \"Suppose that f satisfies axioms A, B, C and D.  Then f has the form...\"  Further, A, B, C and D should be defined before they are used - rather than a page later.  (Similarly, on p.6, the word \"precisely\" tends to be added to sentences.)\n\n1. Section $\\S 3.3$ *could* be the strongest argument for VARSHAP, showing more general performance, rather than just in special cases.  For this to help the authors' argument, though, it needs to be properly set up rather than raced through: *explain* faithfulness, robustness and complexity metrics in a sentence or two, to convince the reader that this mean something from an interpretability point of view."}, "questions": {"value": "For the example given in $\\S 3.1$, how was set membership encoded?  If $\\\\{A, B, C \\\\}$ is a feature set, then the example seems to be looking for an interaction between set membership and the $X$ variables - something that the base Shapley value cannot disentangle."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qE7oIrhtG8", "forum": "7tIrPuZRVH", "replyto": "7tIrPuZRVH", "signatures": ["ICLR.cc/2026/Conference/Submission19556/Reviewer_s5JJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19556/Reviewer_s5JJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884707490, "cdate": 1761884707490, "tmdate": 1762931437318, "mdate": 1762931437318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper revisits the Shapley feature attribution framework and proposes VARSHAP, which replaces the traditional expected-value characteristic function with a variance-based one.\nThe authors show that under three axioms (zero property, sign independence, additivity), the unique valid transformation is $d(x) = x^2$, leading to a variance-based expected marginal contribution. The method thus measures each feature’s contribution to the variance of the model output rather than its raw expectation.\nThey claim this resolves the “global dependency” flaw of SHAP, where correlated features distort importance scores. Empirical results on synthetic datasets and benchmark interpretability tasks demonstrate that VARSHAP produces stable and faithful attributions, particularly when strong dependencies exist between features."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear and formally correct derivations.\n\nTheoretical framing aligns Shapley, variance decomposition, and interpretability in a unified narrative.\n\nEmpirical results are consistent and easy to reproduce.\n\nAddresses an important practical flaw (feature correlation) with simple mathematical machinery."}, "weaknesses": {"value": "Limited novelty: Strong overlap with Sobol sensitivity analysis; the variance-based approach is not a new concept.\n\nUnder-citation: Lacks acknowledgment of prior equivalence results (e.g., Da Veiga, S. (2021)) showing Sobol and Shapley variance connections.\n\nNo empirical stress test: Only toy regressions; no nonlinear or high-dimensional domains.\n\nOverclaiming impact: The method does not “solve” dependency. It merely changes the objective from mean to variance.\n\nNo practical pipeline: Absent complexity or estimator analysis for real SHAP implementations.\n\nUnclear interpretability gain: It is debatable whether variance-based attributions are easier or harder to interpret in applied settings.\n\nReferences:\nDa Veiga, S. (2021). Kernel-based ANOVA decomposition and Shapley effects--Application to global sensitivity analysis. arXiv preprint arXiv:2101.05487."}, "questions": {"value": "How exactly does VARSHAP differ mathematically from Sobol total-effect indices $S_{T_i}$?\n\nCan you prove that VARSHAP satisfies the same orthogonality decomposition as functional ANOVA?\n\nHow would VARSHAP behave if the model output has negligible variance but large mean shifts?\n\nWould replacing variance with another second-moment measure (e.g., covariance of residuals) produce similar properties?\n\nCan you quantify computational complexity compared to KernelSHAP and Sobol estimation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed. Purely theoretical and synthetic experiments."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aU0CwwPi13", "forum": "7tIrPuZRVH", "replyto": "7tIrPuZRVH", "signatures": ["ICLR.cc/2026/Conference/Submission19556/Reviewer_X9ZB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19556/Reviewer_X9ZB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987981073, "cdate": 1761987981073, "tmdate": 1762931436960, "mdate": 1762931436960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comments"}, "comment": {"value": "**Sign independence property of VARSHAP**\n\nIn standard feature attribution (like SHAP), a positive value is almost universally interpreted by users as: \"If I increase the value of this feature, the model's prediction will increase.\" This is the counterfactual interpretation. However, this interpretation is frequently mathematically wrong. Because SHAP calculates an average contribution relative to a global baseline, a feature can have a positive SHAP value simply because the current feature value is higher than the dataset average, even if the model's gradient is negative at that specific point.\nImagine a \"u-shaped\" model. You are on the downward slope. Your feature value is high, and the prediction is higher than the average prediction. SHAP gives a positive value (because prediction > average). However, locally, increasing the feature further would lower the prediction. \nBy adopting Sign Independence (squaring the differences), VARSHAP explicitly removes the directional component. It provides a value representing the magnitude of influence or sensitivity. It tells the user: \"This feature is critical to the prediction because it significantly constrains the model's output range.\" This prevents the user from making false counterfactual assumptions and encourages them to look at specific tools (like Partial Dependence Plots) to determine directionality safely.\n\n**Explanation of VARSHAP attributions for non-experts**\n\nA simple analogy that can be used to explain VARSHAP feature attributions to non-experts is the \"fog of uncertainty\" analogy. Imagine the AI model is trying to make a prediction, but it is standing in a thick fog. It doesn't know the value of any of the patient's medical test results yet. Because it is 'blind' to the specific patient, its prediction is very uncertain, it could be anything from very low to very high. Now, we reveal one test result: blood pressure. Suddenly, the fog clears a little bit. The model is now less uncertain; the range of possible predictions shrinks. Next, we reveal the age of the patient. The fog clears significantly more, and the prediction narrows down further. VARSHAP measures exactly how much 'fog' each feature clears away. If a feature has a high VARSHAP score, it means knowing that feature drastically reduced the model's uncertainty. It was a crucial piece of information. If a feature has a zero VARSHAP score, it means knowing it didn't help clear any fog. So, VARSHAP tells us which features did the heavy lifting in narrowing down the possibilities to reach the final decision."}}, "id": "cQqYSDua1R", "forum": "7tIrPuZRVH", "replyto": "7tIrPuZRVH", "signatures": ["ICLR.cc/2026/Conference/Submission19556/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19556/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19556/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763643151493, "cdate": 1763643151493, "tmdate": 1763643151493, "mdate": 1763643151493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comments"}, "comment": {"value": "**Impossibility theorem and KernelSHAP being “as good as random”**\n\nSome reviewers confused the Impossibility Theorem (Bilodeau et al. 2024)  with the estimation noise inherent in methods like KernelSHAP (which uses random sampling). Reviewers thought we claimed KernelSHAP is \"random\" because it uses Monte Carlo sampling and might be inaccurate if n is small. In reality, the Impossibility Theorem applies to the theoretical, exact Shapley value itself, not just its approximation.The theorem states that for any method that satisfies the axioms of Completeness and Linearity (which SHAP does), the resulting attribution depends so heavily on the marginal distribution of the background data that it effectively disconnects from the model's local behavior. Bilodeau shows that you can construct two models that behave in opposite ways locally (e.g., one increases with feature X, one decreases) but produces the exact same SHAP value for feature X because of how the model behaves in distant regions of the data. So, if one uses the SHAP value to decide how to change a feature, one is flipping a coin. The SHAP value mathematically cannot distinguish between the increasing model and the decreasing model. This is why we say standard SHAP is \"no better than random guessing\" for these tasks, not because the calculation is noisy, but because the definition of the value includes irrelevant global information that obscures the local truth. VARSHAP fixes this by removing the global dependence entirely.\n\n**Local and global explanation vs dependence**\n\nThere is significant confusion regarding the terms \"local\" and \"global\" in feature attribution. We clarify the definitions used in our paper. In particular, the confusion is caused by the fact that the terms “local” and “global” are used to characterize both explanations and dependence. A standard definition of a “local explanation” is that it explains a single prediction (e.g., why did this applicant get rejected?). A \"global explanation\" explains the model generally (e.g., which features are most important on average?). Both SHAP and VARSHAP produce \"local explanations\" in this sense, i.e., they both produce explanations for a single data instance. The main difference between SHAP (and many of its variants) and VARSHAP lies in the dependency. SHAP employs “global dependency” which means that to explain a single instance, SHAP simulates \"missing\" features by sampling from the entire global dataset (the background distribution). This means the explanation for Patient A is mathematically influenced by Patient Z who is completely different. This is the flaw. VARSHAP defines the \"missing\" features by sampling only from a tight distribution centered around the instance itself using the Gaussian perturbation. As a result, VARSHAP explains the single instance using only the mathematics of the immediate neighborhood."}}, "id": "SiHPrc2kH1", "forum": "7tIrPuZRVH", "replyto": "7tIrPuZRVH", "signatures": ["ICLR.cc/2026/Conference/Submission19556/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19556/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19556/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763643174321, "cdate": 1763643174321, "tmdate": 1763643174321, "mdate": 1763643174321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}