{"id": "svxlmwLGEc", "number": 15317, "cdate": 1758250215549, "mdate": 1759897313854, "content": {"title": "ECA: Efficient Continual Alignment for Open-Ended Image-to-Text Generation", "abstract": "Incremental Learning (IL) for Open-ended Image-to-Text Generation (OpenITG) enables models to continuously generate accurate, contextually relevant text for new images while preserving previously acquired knowledge. Unlike prior studies, this paper addresses a more practical scenario in which the predominant category of visual data shifts over time as environments evolve. In this context, we introduce a new notion of continual alignment, which incrementally adapts the alignment module within pre-trained VLMs to preserve high-quality cross-modal representations. Based on this idea, we propose **E**fficient **C**ontinual **A**lignment (ECA), a novel exemplar-free IL approach for OpenITG. The key challenge is enabling the model to acquire new, task-specific features while minimizing interference with the established alignment without accessing raw data from previous tasks. To address this, ECA employs three core mechanisms: a **M**ixture **o**f **Q**uery (MoQ) module that adapts task-specific query tokens, a **F**ish**e**r **D**ynamic **Ex**pansion (FeDEx) that dynamically expands model structure based on a Fisher Information Matrix (FIM)-based metric, and an embedding dictionary with **D**ictionary **R**eplay (DR) to retain past knowledge. To evaluate ECA's performance, we construct four new IL OpenITG benchmarks that better reflect real-world scenarios. Experimental results demonstrate that ECA significantly mitigates catastrophic forgetting and improves IL performance compared to baseline methods. Benchmarks are available at <https://anonymous.4open.science/r/ECA-ToS-Benchmarks-FB17>.", "tldr": "", "keywords": ["Incremental Leanrning", "Vision-Language Model", "Image-to-Text Generation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a4894696d7d8aacd3764043ad8236b9c30c5af5c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of Incremental Learning (IL) for Open-ended Image-to-Text Generation (OpenITG), such as VQA and captioning. \n\nExperiments on the four new benchmarks show that ECA significantly outperforms strong baselines (including regularization and prompt-based IL methods) and achieves performance very close to the joint-training upper bound, demonstrating its ability to mitigate catastrophic forgetting while remaining parameter-efficient."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The \"main topic\" shift scenario, which incorporates semantic overlap, is a significant and more realistic formulation for IL benchmarks compared to standard disjoint-category setups. The four new ToS benchmarks are a strong contribution.\n2. The FIM-based metric for deciding when to expand with a new parallel adapter is the paper's strongest technical novelty. It is theoretically motivated (Theorem 1) and empirically validated (Fig. 4), providing a non-heuristic way to balance positive transfer (reusing adapters) and mitigating interference (adding new adapters).\n3. The combination of MoQ and DR (Dictionary Replay) is a highly effective *exemplar-free* approach. The DR mechanism, using a learned sparse dictionary of embeddings, is a clever way to perform rehearsal without storing raw data, addressing privacy and storage concerns."}, "weaknesses": {"value": "1. The final ECA method combines three distinct modules (MoQ, FeDEx, DR), each with its own logic and (minor) hyper-parameters (e.g., $\\lambda$ for $\\mathcal{L}_{DR}$, dictionary size $m$). This is inherently more complex than a simpler baseline like a single PA or a prompt-based method.\n2. The paper does not explicitly quantify the computational cost of the FeDEx module. Calculating the FIM-based metric $S(\\omega_t)$ requires additional gradient and FIM diagonal computations at the end of each task to decide whether to expand. While likely manageable (as it's not per-batch), a brief analysis of this overhead would be beneficial.\n3. The method is instantiated on BLIP-2's Q-Former. While the *principles* (adapting the alignment module) are general, it's not immediately clear how MoQ or FeDEx would be applied to different VLM architectures, such as projector-based models (e.g., LLaVA), which lack a Q-Former. A brief discussion on this potential for generalization would improve the paper."}, "questions": {"value": "1. Could you please quantify the computational overhead of the FeDEx module? Specifically, what is the cost of calculating the FIM-based metric $S(\\omega_t)$ at the end of each task (relative to the task's training time)?\n2. The Dictionary Replay (DR) module relies on a dictionary of a fixed size $m=5 \\times d_v$. How does this fixed-size dictionary scale as the number of tasks $T$ grows very large? Do you foresee this becoming a bottleneck, and would a dynamic-sized dictionary (e.g., adding new atoms per task) be beneficial?\n3. Could you elaborate on how the core ideas of ECA, particularly FeDEx and MoQ, could be adapted to other popular VLM architectures that do not use a Q-Former, such as those with simple MLP projectors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nnReGNBzJj", "forum": "svxlmwLGEc", "replyto": "svxlmwLGEc", "signatures": ["ICLR.cc/2026/Conference/Submission15317/Reviewer_phKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15317/Reviewer_phKC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659912098, "cdate": 1761659912098, "tmdate": 1762925615295, "mdate": 1762925615295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes Efficient Continual Alignment (ECA), an exemplar-free incremental learning framework for open-ended image-to-text generation (OpenITG) that adapts pre-trained vision-language models to evolving visual domains. ECA introduces continual alignment, ensuring cross-modal consistency while learning new tasks without accessing prior data. It achieves this through three key components: a Mixture of Query (MoQ) module for task-specific query adaptation, a Fisher Dynamic Expansion (FeDEx) mechanism that expands model capacity using FIM-based metrics, and a Dictionary Replay (DR) strategy to preserve past knowledge. Together, these techniques effectively mitigate catastrophic forgetting and enhance continual generation performance in dynamic visual environments."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper presents the proposed setting and methodology through clear introductions and illustrations, along with detailed definitions.\n2. This paper proposes a novel IL Benchmarks for OpenITG framework that addresses the issue of semantic overlap in image categories or background scenes across different tasks."}, "weaknesses": {"value": "1. Given the continual changes in visual semantic themes, why only fine-tune the alignment module? In real-world scenarios, can this solution still perform well when encountering scenes or categories that the visual extractor has never seen before?\n2. This paper employs BLIP-2 for experimentation. Has consideration been given to validating the method's effectiveness on more novel models? In particular, consider other forms of multimodal alignment such as Linear Projector/MLP.\n3. How effective is the Fisher metrics screening in Fisher Dynamic Expansion? The lack of experimental demonstration shows how much unnecessary expansion Fisher metrics reduce during dynamic expansion. Also, the increased inference costs resulting from dynamic expansion have not been taken into account."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pEPFD4tbGD", "forum": "svxlmwLGEc", "replyto": "svxlmwLGEc", "signatures": ["ICLR.cc/2026/Conference/Submission15317/Reviewer_ZE8G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15317/Reviewer_ZE8G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801702196, "cdate": 1761801702196, "tmdate": 1762925614567, "mdate": 1762925614567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Efficient Continual Alignment (ECA), a novel exemplar-free incremental learning approach for open-ended image-to-text generation that addresses the practical scenario where visual data categories shift over time. ECA enables vision-language models to continuously adapt to new images while preserving previously learned knowledge through three core mechanisms: a Mixture of Query module for task-specific adaptation, Fisher Dynamic Expansion for strategic model growth based on Fisher Information Matrix metrics, and Dictionary Replay using an embedding dictionary to maintain past knowledge without storing raw historical data. The authors construct four new benchmarks reflecting real-world conditions and demonstrate that ECA significantly reduces catastrophic forgetting and outperforms baseline methods in incremental learning scenarios where the alignment module must continuously evolve without access to previous task data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper addresses a more realistic scenario where visual data distributions shift over time as environments evolve, unlike previous static assumptions. \n2. This paper introduces an exemplar-free approach that preserves knowledge without storing raw historical data, making it more practical and privacy-preserving. \n3. This paper proposes a novel continual alignment framework with dynamic model expansion that efficiently adapts to new tasks while minimizing interference with established cross-modal representations."}, "weaknesses": {"value": "1. Based on Figure 2, the proposed FeDEx and Q-Former appear to be the same component, which contradicts the paper's claimed contributions.\n2. The manuscript does not include a limitations section. The reviewer requests that the authors provide a comprehensive discussion of the method's limitations in their rebuttal response.\n3. The experimental comparisons are limited to earlier image captioning approaches and lack benchmarking against recent large-scale vision-language models (e.g., Qwen-VL and LLaVA)"}, "questions": {"value": "Please refer to above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "hyXMgnFsOX", "forum": "svxlmwLGEc", "replyto": "svxlmwLGEc", "signatures": ["ICLR.cc/2026/Conference/Submission15317/Reviewer_DSk8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15317/Reviewer_DSk8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824433577, "cdate": 1761824433577, "tmdate": 1762925613664, "mdate": 1762925613664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Efficient Continual Alignment (ECA), an exemplar-free incremental learning (IL) framework for open-ended image-to-text generation (OpenITG), which addresses catastrophic forgetting by adapting only the alignment module of pre-trained vision-language models (VLMs) (e.g., BLIP-2’s Q-Former) while freezing visual encoders and large language models (LLMs). ECA integrates three key components (Mixture of Query, Fisher Dynamic Expansion, Dictionary Replay) to preserve cross-modal alignment, and the authors construct four realistic IL benchmarks (ToS-COCO Caption, ToS-VQAv2, etc.) split by image main topics, with experiments showing ECA outperforms SOTA exemplar-free baselines in average performance, forward/backward transfer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A key strength is the introduction of \"continual alignment\" as a novel concept for multi-modal IL, marking the first work to explicitly target preserving the cross-modal alignment of VLM alignment modules in exemplar-free OpenITG—filling a gap in existing works that either rely on raw exemplars or ignore alignment stability.\n\nECA’s component design is highly motivated and parameter-efficient: Fisher Dynamic Expansion uses FIM-based metrics to avoid unnecessary adapter expansion, and Dictionary Replay replaces raw exemplars with a sparse embedding dictionary (solving privacy/memory issues), while the self-constructed benchmarks (capturing real-world semantic overlap) ensure rigorous evaluation."}, "weaknesses": {"value": "1. It is recommended that the authors conduct a comparative analysis of the computational complexity and memory costs between the proposed ECA method and the compared baselines.\n\n2. Only MoE-LoRA is included as a multi-modal IL baseline. Recent works in 2025 tailored for incremental vision-language task are omitted, making it difficult to fully assess ECA’s standing in the multi-modal IL landscape.\n\n3. It is suggested that the authors discuss existing MLLM-based continual learning methods [1] [2] that have covered VQA and image captioning tasks, and explicitly elaborate on the significance of the proposed ECA method in comparison to these works within this paper.\n\n[1] MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark\n\n[2] Continual LLaVA: Continual Instruction Tuning in Large Vision-Language Models"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r3kIPI0qqJ", "forum": "svxlmwLGEc", "replyto": "svxlmwLGEc", "signatures": ["ICLR.cc/2026/Conference/Submission15317/Reviewer_sepo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15317/Reviewer_sepo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938085478, "cdate": 1761938085478, "tmdate": 1762925613311, "mdate": 1762925613311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}