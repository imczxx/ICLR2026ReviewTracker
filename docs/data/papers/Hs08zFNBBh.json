{"id": "Hs08zFNBBh", "number": 18133, "cdate": 1758284219030, "mdate": 1759897130036, "content": {"title": "Fairness by Design: Efficient Fair Ensembles for Low-Data Classification", "abstract": "We address the problem of fair classification in settings where data is scarce and unbalanced across demographic groups. Such low-data regimes are common in domains like medical imaging and hate speech. Our proposed method mitigates these biases by training efficient ensembles of fair classifiers on different data partitions. Aggregating predictions across ensemble members, each trained to satisfy fairness constraints, yields more consistent outcomes and stronger fairness-accuracy trade-offs than existing methods across multiple challenging medical imaging datasets, as well as on hate speech detection.\n\nTo support these findings, we provide theoretical guarantees: we prove when our fair ensembles improve performance and how much data is needed to observe these gains with statistical significance. These results extend the literature by explaining why and under what conditions ensembles improve algorithmic fairness in high-stakes applications.", "tldr": "We introduce fair deep ensembles that work well with extremely low-data groups with theoretical guarantees.", "keywords": ["Algorithmic fairness", "Group fairness", "Minimum recall", "Equal opportunity", "Ensemble learning", "Low-data learning", "Medical imaging", "Theoretical guarantees"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3724665b001276bac54b6cad93713f36f082327c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The submission dicusses the problem of guaranteing the fairness for ensemble classifiers, if individual classifiers are known to be fair. Specifically, it extends a result from (Theisen'23) about the accuracy of ensembles to notions of fairness that can be expressed as the accuracy on a subset of the data (or a restricted data distribution), such as controlling the false-negative rate (=recall) by measuring the accuracy on the set of negative examples (=recall). The main results is that fairness of the ensemble is at least as high as that of the participating classifiers as long as these are independent and have at least 50% recall (analogously to the 50% individual accuracy criterion of (Theisen'23). The submission also states how many samples are needed to ensure this reliably from finite data under approximate Gaussianity. \nWhile the contribution is mostly conceptual, the manuscript also reports experiments, indicating that the promised fairness properties hold on real-world datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ the topic of ensemble fairness is relevant and interesting.\n+ the described analysis appears correct and provides useful answers in some setting.\n+ the viewpoint of treating recall as accuracy on a subset might be useful for other problems as well.\n+ details supplemental material and high quality source code is provided."}, "weaknesses": {"value": "The manuscript has a number of shortcomings, mostly in terms of presentation,\nand a limited scientific contributions.\n\n* Framing\n\nThe manuscript argues that ensembles are relevant in the small-data regime, but it does not provide any evidence for this. The title states \"fairness by design\", but the contribution really not about any design, but the fairness is simply enforced by increasing each classifier's recall threshold. The title also claims \"efficient\" ensembles, but efficiency is not explicitly studied or modelled, it only emerges from the use of a deep architecture with pretrained shared backbone (which is independent of any fairness aspect or theoretical guarantees). \n\n* Clarity of presentation\n\nI found the presentation to be unclear in some parts. \nThe work crucially relies on (Theisen'23), from which is adopts the definition of a *compenent* ensemble and the guarantee that ensembling cannot make the accuracy worse. However, the way the prior work is presented makes it hard to understand the prior work. For example, $C_{\\rho}$ is taken from Assumption 1 in (Theissen'23). However, this quantity depends on the threshold $t$. In the submission, $t$ is not mentioned or quantified, leaving the definition incomplete. I had to consult (Theissen'23) to see that the condition should hold for all possible $t\\in[0,0.5]$. The main result (Theorem 1) of (Theissen'23) is that ensembling never makes accuracy worse. The submission consistently phrases this fact as that it *improves* performance (I would suggest to avoid 'performance' anyway, as it can be mistaken for a property of runtime, not quality). \nThe definition of *restricted ensemble error* (line 186) includes an expectation over the data distribution, while the prior definition of error rate was a per-datapoint quality. I assume this is a mistake, as otherwise it would be a deterministic quantity and the following probability statements (line 176) would not make sense. \nA central observation in the submission is \"Minimum recall corresponds to competence on g+, minimum sensitivity to competence on g−, overall accuracy to competence on the full dataset\" (line 192).\nThis seems wrong, or at least it is unclear to me. The *restricted ensemble error* should correspond to 1 minus recall, as it specifies the probability of a false negative (positive label, negative decision). \n*Competence* is defined as the difference between the probability of two error rate intervals, I do not see the relation to recall here. Again, the threshold $t$ is not discussed here. \n\nI also found the presentation to be rather imbalanced. The manuscript, e.g., presents a proof sketch of a special case of a Jury theorem (line 264ff), which could be skipped or moved to the appendix, but other parts of too brief to allow, e.g., the exact data split procedure for training the ensembles is missing, which is important to understand e.g. if the assumption of independence between ensemble members is fulfilled. \nThe description how fairness is enforced (line 123ff) is quite brief. It appears to use not the ground truth protected attribute but its learned prediction as a source, which is a clear source of bias that could invalidate the guarantees. It seems that the procedure can only enforce overall per-classifier recall, but definition of competence requires a positivity conditions between all intervals $t, 1/2)$ and $[1/2, 1−t]$. \n\n\n* Scientific contribution\n\nI find the scientific contribution quite limited. Apart from questions about correctness (see above),\nthe main observation is that (Theissen'23)'s results can be transferred from overall accuracy to accuracy of suitable subsets, which recovers, e.g., recall. The subsequent mathematical analysis is rather straight-forward, including the derivations of a necessary sample size (5) based on approximate normality. A further shortcoming is that the results require independent errors between ensemble members (which are not fulfilled for standard bootstrap/cross-validation samples in practice), and that the condition of at least 50% recall emerges, which limits the tasks where this result is applicable. \n\n* Experimental evaluation\n\nThe experiments appear valid (with the caveat that they are not reproducible from the manuscript alone, one has read the code). It is not clear, though, how exactly they relate to the main scientific contribution. The statement of the manuscript is that fairness is at least preserved from ensemble members to the majority vote classifier. This is a new form of analysis, but not a new *method* that could be shown to be superior to other methods. Comparing the resulting fairness to other fairness methods or ensembles on non-fair classifiers is good to see, but tangential to the main statement. \nFurthermore, one important baseline is missing: given that for the base classifiers the fairness is forced by adjusting decision thresholds on a validation set, what stops the user from doing so for the resulting ensemble classifier? The latter would be required baseline. In the studied setting, is it better at all to train an ensemble of fair classifiers, or would it be enough to train a standard ensemble and make it fair afterwards?"}, "questions": {"value": "* please address my concerns about the theoretical description and the mapping to the experimental setup.\n* please discuss my question about the baseline of making the majority-vote classifier fair post-hoc (you do not have to run such an experiment, as I do not believe that author responses should contain new experiments).\n\nIn case my low scores were based on a misunderstanding of your contribution, I will be happy to adjust them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ww9CYERDYf", "forum": "Hs08zFNBBh", "replyto": "Hs08zFNBBh", "signatures": ["ICLR.cc/2026/Conference/Submission18133/Reviewer_p8PR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18133/Reviewer_p8PR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761143115218, "cdate": 1761143115218, "tmdate": 1762927893158, "mdate": 1762927893158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of algorithmic fairness in low-data and imbalanced regimes, which are common in domains such as medical imaging and hate speech detection. The authors propose **FAIRENSEMBLE**, an ensemble framework of fair classifiers that shares a pretrained backbone while enforcing fairness constraints at the member level. During validation, each ensemble member optimizes a weighted combination of accuracy and fairness (minimum recall or equal opportunity), and final predictions are aggregated by majority voting.  \nTheoretically, the paper adapts _ensemble competence_ theory (Theisen et al., 2023) to group fairness, showing that (i) enforcing minimum recall above 0.5 guarantees ensemble competence for positive samples, and (ii) fairness constraints such as error parity remain bounded across groups. Empirically, FAIRENSEMBLE outperforms baseline fairness methods (OxonFair, FairRet, ERM) on three medical imaging datasets (HAM10000, Fitzpatrick17K, FairVLMED) and one hate speech dataset, achieving better fairness–accuracy trade-offs measured by the proposed **FairAUC** metric."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Addresses a **high-impact but underexplored problem**: ensuring fairness under small and unbalanced datasets, a realistic setting for medical AI.\n+ Presents a **clear and coherent methodology**, integrating fairness constraints into ensemble training with shared backbones to improve data efficiency.\n+ Provides **formal theoretical analysis** connecting ensemble competence and fairness guarantees, grounding the method in provable conditions.\n+ Introduces **FairAUC**, a simple yet effective metric to evaluate the fairness–accuracy Pareto frontier.\n+ Demonstrates consistent empirical improvements across both imaging and NLP tasks, with notable gains in fairness metrics over strong baselines such as OxonFair."}, "weaknesses": {"value": "+ **Limited theoretical originality**: the core proofs and guarantees rely heavily on prior results from Theisen et al. (2023) and Condorcet Jury Theorem. The paper adapts these to group fairness rather than providing novel derivations.\n+ **Fairness validation lacks per-group evidence**: experiments report global metrics (FairAUC, DEO) but do not show _which_ disadvantaged groups actually improve (e.g., recall on minority skin tones). The fairness claim is therefore not fully substantiated.\n+ **Fairness metrics and theoretical assumptions are not perfectly aligned:**  \nthe theoretical guarantee relies on each ensemble member (or fold) achieving a minimum recall greater than 0.5, but the experiments do not verify whether this condition actually holds for all folds. As a result, it remains unclear whether the theoretical fairness guarantees are truly satisfied in practice."}, "questions": {"value": "1. Can the authors provide **per-group confusion matrices or recall statistics** to verify that the observed improvements indeed arise from better recall of disadvantaged groups, rather than overall averaging effects?\n2.  The theoretical guarantee relies on minimum recall ≥ 0.5. How often does this condition hold in practice across datasets?  \n3. The proposed FairAUC metric is an interesting way to quantify fairness–accuracy trade-offs. Could the authors clarify how well it correlates with standard fairness metrics (e.g., Equal Opportunity, Equalized Odds) and whether it can consistently reflect group-level improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ojY7ZImEaQ", "forum": "Hs08zFNBBh", "replyto": "Hs08zFNBBh", "signatures": ["ICLR.cc/2026/Conference/Submission18133/Reviewer_SM5J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18133/Reviewer_SM5J"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447104171, "cdate": 1761447104171, "tmdate": 1762927892828, "mdate": 1762927892828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an efficient deep-ensemble framework that enforces group-fairness constraints at the member level and argues these constraints are preserved at the ensemble level. The method shares a frozen ImageNet-pretrained backbone across multiple classifier heads, where each member is trained on a stratified fold and equipped with auxiliary 'protected-attribute' heads. At validation time, the method tunes head weights to enforce either minimum recall or equal opportunity, while maximizing accuracy. Finally, the predictions are aggregated by majority vote at inference. \n\nTheoretical results show that (i) if each member meets a minimum rate and the group is competent, the ensemble also meets that minimum rate; (ii) for error-parity metrics, approximate parity among members leads to bounded disparity in the ensemble; and (iii) under independent errors, ensuring recall > 0.5 for each member guarantees ensemble-level recall > 0.5. Empirically, the method improves fairness-accuracy balances on CV tasks and hate-speech detection, measured with FairAUC."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Focus on low-data fairness where parity metrics can level down performance; the choice of minimum recall as a primary fairness target for safety-critical medical tasks is appropriate and clearly argued. \n\n* A shared (frozen) EfficientNetV2 backbone with masked multi-heads makes M-member ensembling cheap; inference is near-single-model cost while retaining diversity from fold-specific heads. Clear training/validation partitioning and majority voting. \n\n* Extends ensemble competence to group-restricted subsets and links guarantees to minimum-rate constraints; provides (approximate) error-parity bounds and a validation/evaluation sample-size formula intended to guide split sizes.\n\n* Authors provide anonymized code, deterministic seeds, and explicit compute details."}, "weaknesses": {"value": "- *Presentation*. As a note, the paper names their method FAIRENSEMBLE. In my opinion, this causes confussion with the literature, since one of the already identified close works is Ko et al \"Fair-Ensemble: When Fairnes...\", which they (Ko et al) use in the title and in the section 3 to name the effect of some ensembles improving the workst group accuracy in certain tasks. I suggest renaming the method to avoid confusion.\n\n\n- *Contribution and Related Work*. Authors claim that related work does not do theoretical analysis on the fairness of ensembles (l.43, l.156, l.287). However this is not totally true. For instance Grgić-Hlača et al focus on theoretical considerations on ensemble fairness, specifically about conditions inder when ensembles are guaranteed to be fair or not. In additon, Sweighofer et al. also provide theoretical insights on the potential causes of the fairness issues in ensembles, and propose a bayesian perspective on the average predictive diversity.\n\n- *Related work*. The sentence in the intro \"Existing fairness interventions often fail in these low-data settings\" (l.37) must be supported by citations or evidence. The current citation Zong et al does not support this claim.\n  \n- *Related work*. The paper (l.97) currently states that ensembles sometimes help fairness and sometimes don't, but it doesn't disentangle when and why. Please be precise about: (i) ensemble heterogeneity (heterogeneous vs. homogeneous members), (ii) task modalities (tabular vs. vision vs. language), and (iii) fairness metrics (e.g., minimum per-group accuracy/recall, disparate impact, equalized odds/opportunity). A structured and more extended related-work paragraph or even table separating these axes would let readers see under which conditions prior work finds gains or not.\n\n- *Related Work*. I also miss discussion with more works in the fairness in deep ensembles. Please I recommend the authors to look in the Sweighofer et al., and Claucich et al. papers, which are very related to the topic of this paper. Examples of relevant missing literature are Gohar et al. (ensembles of shallow models), Grgic-Hlaca et al. (theoretical considerations on ensemble fairness) or Bhaskaruni et al. (Fair AdaBoost).\n\n- *Contribution*. Several guarantees rely on independence correlation of member errors. With a shared frozen backbone, heads may be substantially correlateds. The paper cites disagreement/DER conceptually, but I could not find per-group empirical DER/competence measurements on validation and test (especially on the positive groups $g^+$). Adding these would strengthen the claims.\n\n- *Contribution*. The main contributions of the authors build upon known ensemble-competence results (Theisen et al. 2023, Condorcet variants) to subgroup-restricted sets and fairness metrics. This is valuable, but the mathematical steps feel incremental. Clarifying what is genuinely new (beyond restatement with restricted distributions) would help to highlight the contributions.\n\n- *Contributions and Evaluation*. Although authors propose their theoretical framework for EOp and minimum-rate constraints, the main focus of their empirical evaluation focuses on FairAUC, which limits the interpretability of their contributions. EOp and min recall are just mentioned in Fig 3 center and right, but no discussion about the theoretical insights and empirical findings have been done on min recall and EOP. Including discussion about this connection would strengthen both the empirical and the theoretical contribution of the paper.\n\n- *Evaluation*. To enable clearer comparisons with the already identified close work (e.g., Schweighofer et al.; Ko et al.; Claucich et al.), include standard public benchmarks commonly used in this literature for computer vision areCelebA, LFWA, CheXpert, FairFace. However, the paper would be benefited from including tabular benchmarks as well (COMPAS, Adult, German Credit, ACSIncome...). This also helps stress-test across modalities and metrics (parity and minimum-rate).\n\n- *Evaluation*. Given how closely the paper builds on Schweighofer et al., Ko et al., and Claucich et al., these must be baselines. Schweighofer et al. propose an ensemble threshold post-processing for fair decisions, leveraging better calibration for group-aware threshold adjustment. Ko et al. analyze initialization and batch order as primary sources of diversity in deep ensembles. Finally, Claucich et al. show that equal rebalancing can be harmful and that over-representing the minority when their task is harder can mitigate harms, which is of critical importance for author's focus on scenarios where data is scarce and unbalanced across groups. These are relatively simple, pre or post-processing techniques. Without these, it's hard to attribute the authors contributions's gains vs. known ensemble/thresholding/calibration/diversity/sampling effects. In addition, there are more relevant baselines in the literature that should be included (e.g., Bhaskaruni et al.), missing due to the lack of depth in the literature review.\n\n- *Evaluation*. I'd like to see ablations on the backbone. For instance, frozen vs. fine-tuned backbone, and without protected-attribute heads (how much does the surgery vs. ensembling contribute?) would help isolate which component drives the gains.\n\n- *Presentation*. Add consistent venue/URL/DOI across all references, especially the closest prior work. \n  - Add conference (or arxiv) in all references (especially in the ones that the paper closely relies on (!!)), for instance: \n    * !! Schweighofer et al. is in ICML 2025 https://proceedings.mlr.press/v267/schweighofer25a.html\n    * !! Claucich et al. is in FAccT 2025 https://dl.acm.org/doi/10.1145/3715275.3732200\n    * !! Ko et al. is in arXiv only, but add it https://arxiv.org/abs/2303.00586\n    * Chen et al. is in arXiv only, but add it https://arxiv.org/abs/2007.00649\n    * Jiménez-Sánchez et al. is in FAccT 2025 https://dl.acm.org/doi/10.1145/3715275.3732035\n  - Remove or footnote non-academic sources (e.g., blogposts) to avoid confusion (e.g. Biewald).\n\n---\n*Refs*\n* Gohar et al. Towards understanding fairness and its composition in ensemble machine learning\n* Grgić-Hlača et al. On fairness, diversity and randomness in algorithmic decision making\n* Bhaskaruni et al. Improving prediction fairness via model ensemble."}, "questions": {"value": "In addition to the weaknesses mentioned above, I have some minor questions for the authors that I would like clarified.\n\n* You state the approach 'applies equally to other group fairness metrics.' Could you demonstrate at least one additional metric (e.g., minimum precision or FNR parity) to corroborate the theory claim\n\n* How exactly are head weights tuned on validation to meet fairness constraints? Is this a constrained optimization, a grid search...? What are the runtime/feasibility behaviors when constraints are tight? \n\n* Did you try lightly fine-tuning the backbone to improve diversity vs. overfitting? Any change in DER, fairness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HM09e6LPPE", "forum": "Hs08zFNBBh", "replyto": "Hs08zFNBBh", "signatures": ["ICLR.cc/2026/Conference/Submission18133/Reviewer_n89Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18133/Reviewer_n89Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649718197, "cdate": 1761649718197, "tmdate": 1762927892369, "mdate": 1762927892369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the authors for their comments.\n\nTo correct a common misapprehension. This work is not a theory paper.\n\nWe went into this project purely concerned with the question \"How can fairness be enforced in practice for low data and high-dimensional imaging?\"\n\nWe went into the project with 3 goals:\n1. Unlike existing approaches, the method should be usable with low data -- the performance cost of holding back enough data to reliably estimate and correct error rates, often degrades the generalisation so much that it's not worth doing.\n2. The method should not be prohibitively slow to run, and should have similar training and evaluation times as a standard classifier.\n3. We should have some idea of the theoretical guarantees of the approach, and when we can expect it to work.\n\nAs such, the experiments address point 1. We compare with a wide range of methods (including a strong ERM baseline) and show we outperform all of them.\n2. follows directly from the design of our architecture. This is a straightforward contribution but has significant practical implications.\n3. follows from the application of concordant and ensemble theorems to subsets of the data. We are not looking to make a substantial contribution to concordant theory, but to show why our method is guaranteed to work."}}, "id": "nm0YQ5pRXZ", "forum": "Hs08zFNBBh", "replyto": "Hs08zFNBBh", "signatures": ["ICLR.cc/2026/Conference/Submission18133/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18133/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18133/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763564075328, "cdate": 1763564075328, "tmdate": 1763564075328, "mdate": 1763564075328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}