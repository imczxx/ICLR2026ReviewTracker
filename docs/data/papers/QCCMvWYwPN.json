{"id": "QCCMvWYwPN", "number": 18816, "cdate": 1758291070199, "mdate": 1759897079793, "content": {"title": "M$^3$Ret: A Mixed Multimodal Image Dataset and Benchmark for Personalized Multi-Retinal Disease Detection", "abstract": "In ophthalmic clinical practice, various imaging examinations, such as retinal fundus photography and OCT imaging, provide ophthalmologists with non-invasive methods to assess the condition of the retina and highlighting the importance of multimodal data. The imaging examinations are individually tailored according to each patient’s clinical condition, resulting in diverse modality combinations. However, existing multimodal ophthalmic imaging datasets only collected one combination of multimodal data for single disease detection. Correspondingly, previous multimodal models were designed to learn from a fixed combination of modalities, overlooking the personalized nature of clinical examinations and the variability in modality combinations. As a result, the models often fail to generalize well to real-world clinical applications. To bridge the gap, this paper proposes (1) $\\mathbf{\\mathsf{M^3Ret}}$, a $\\textbf{M}$ixed $\\textbf{M}$ultimodal ophthalmic imaging dataset for personalized $\\textbf{M}$ulti-$\\textbf{Ret}$inal disease detection, which consists of scanning laser ophthalmoscopy (SLO) images and optical coherence tomography (OCT) images and includes various modality combinations, and (2) $\\mathbf{\\mathsf{PersonNet}}$, a new baseline model for personalized multimodal multi-retinal disease detection, which can handles samples with various modality combinations during both training and inference phase, (3) benchmark results of our $\\mathsf{PersonNet}$ and 13 existing multimodal learning methods, which demonstrate the superiority of the proposed $\\mathsf{PersonNet}$ and highlight the significant room for improvement before clinical application can be achieved.", "tldr": "this paper proposes multimodal ophthalmic imaging dataset with multiple modality combinations and benchmark for personalized multi-retinal disease detection", "keywords": ["multimudal learning", "personalized multi-retinal disease detection", "multimodal ophthalmic imaging dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00f88e08019d591bd8d5142c462a86809f62d37c.pdf", "supplementary_material": ""}, "replies": [{"content": {"title": {"value": "Reply to Reviewer bWtN"}, "comment": {"value": "We thank reviewers for their timely feedback and valuable comments. Below is our point-by-point response.\n\n**Weakness1 & 2:** M$^3$Ret only includes three diseases and was collected from a single hospital, reducing its applicability to broader clinical scenarios and limiting generalization to other clinical settings.\n\n**Reply:** Thanks for pointing this out. Compared to existing publicly available multimodal ophthalmic datasets, which typically focus on a single specific retinal disease, M³Ret covers three retinal diseases and two modalities by two different imaging devices, providing a more comprehensive resource for research. In the future, we plan to collaborate with ophthalmologists from other countries to collect additional data encompassing a wider range of retinal diseases and more diverse ethnicities. Nevertheless, we still think it is important and timely to release the data collected from real clinical workflows and benchmarks to foster research in this field.\n\n**Weakness3:**  A portion of labels are derived from treatment records (not direct diagnoses) or marked as “unclear”, which may introduce noise into training.\n\n**Reply:**  Thank you for your valuable comment. In the health domain, labels must be highly reliable. To ensure this, our dataset was collected from the ophthalmic outpatient department of the hospital, and over 90% of the samples have clear diagnostic decisions recorded in the electronic medical record system, complemented by treatment records when the diagnoses were positive. These diagnostic decisions were made by experienced ophthalmologists based on a combination of clinical examinations, including visual field tests, intraocular pressure measurements, slit-lamp examinations, SLO imaging, OCT imaging, and others.\nFor samples without clear diagnostic labels, if two experienced ophthalmologists could confidently determine the disease labels based on detailed treatment and medication records, their labels were assigned accordingly. Otherwise, the disease labels were marked as “unclear.” As a result, only approximately 5.2% of the samples have unclear labels for partial disease classes, demonstrating that the labeling process was conducted carefully to ensure high reliability in our dataset.\nFurthermore, for samples with an “unclear” label for one disease but “clear” labels for other diseases, we still use the “clear” labels in model training. Specifically, the loss is calculated only over disease classes with clear labels, while disease classes with “unclear” labels are excluded from loss computation. Therefore, the presence of “unclear” labels does not introduce noise into model training.\n\n**Question1:**  PersonNet’s fusion module uses modality combination-aware fully connected layers. How many such layers are there (one per combination?), and how do you avoid overfitting given the varying sample sizes across combinations (e.g., Tri-modal only has 118 samples)?\n\n**Reply:**  Thank you very much for these questions. In clinical practice, UWF-SLO images, macular OCT, and their paired combinations are commonly used to diagnose macular edema (ME), diabetic retinopathy (DR), and glaucoma. UWF-SLO images, disc OCT images, and their paired combinations are primarily used for diagnosing glaucoma because glaucoma results in degradation of optic nerves which gathering in optic disc while ME and DR do not. This is the reason why only a very limited number of patients undergo all three examinations (1.4%) or both macular and disc OCT scans (0.7%) as shown in Table 2. Accordingly, we reorganise the data to two sub-datasets: $\\mathcal{D}_A$ and $\\mathcal{D}_B$. $\\mathcal{D}_A$ consists of three combinations of UWF-SLO image and macular OCT images and is used for the detection of ME, DR and glaucoma and $\\mathcal{D}_B$ consists of three combinations of UWF-SLO images and disc OCT images and is used for the detection of glaucoma. The summary can be found in Table 3.  Since each sub-dataset only contains three combinations, we naturally set the number of MLP in Figure 5 to three. For samples in Tri-modal, the UWF-SLO and macular OCT are included in $\\mathcal{D}_A$ while UWF-SLO and disc OCT are included in  $\\mathcal{D}_B$.\n\n**Question 2:** Have you tested PersonNet’s performance on edge devices (e.g., mobile GPUs) to evaluate its deployment potential in primary care settings?\n\n**Reply:** Thank you very much for the question. We have not tested our model’s performance on edge devices. Probably in the health domain, the better option for deploying the disease classification model is a client–server architecture rather than edge computing architecture."}}, "id": "MHBcTc4tYv", "forum": "QCCMvWYwPN", "replyto": "QCCMvWYwPN", "signatures": ["ICLR.cc/2026/Conference/Submission18816/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18816/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18816/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762970120046, "cdate": 1762970120046, "tmdate": 1762971108391, "mdate": 1762971108391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the gap between existing multimodal ophthalmic datasets/models and real-world clinical needs, where personalized examinations lead to diverse modality combinations and coexisting retinal diseases. Although this article still has some weaknesses in terms of the extensiveness and quality of the dataset, it has made significant progress compared to past work. In my view, this paper is marginally above the acceptance bar of ICLR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unlike existing datasets (e.g., FairVision, MMC-AMD) that only support single-disease detection or fixed modality combinations, M³Ret includes 7 mixed modality combinations (unimodal, bimodal, trimodal) and multi-label disease labels, fully reflecting personalized clinical examinations.\n\n2. With 8,558 eye samples, it is one of the largest multimodal ophthalmic datasets. Its disease prevalence rates are consistent with real-world statistics (e.g., close to Teo et al.’s 4.07% ME and 6.17% DR), avoiding bias from overrepresented diseases (e.g., FairVision’s 48.7% glaucoma). \n\n3. Comprehensive experiments cover 13 baseline methods across complete and incomplete multimodal learning and explore subgroup analysis (age/gender fairness), and single-vs-multimodal comparisons, revealing limitations of existing SOTA and providing insights into model behavior."}, "weaknesses": {"value": "1. M³Ret only includes three diseases (DR, ME, glaucoma), excluding other common retinal diseases (e.g., age-related macular degeneration (AMD)), reducing its applicability to broader clinical scenarios.\n\n2. M³Ret is collected from a single hospital, lacking diversity in patient demographics (e.g., ethnicity, regional medical practices) and imaging devices (only Optos Panoramic 200 for UWF-SLO, CIRRUS HD-OCT 500 for OCT), limiting generalization to other clinical settings.\n\n3. A portion of labels are derived from treatment records (not direct diagnoses) or marked as “unclear”, which may introduce noise into training."}, "questions": {"value": "1. PersonNet’s fusion module uses modality combination-aware fully connected layers. How many such layers are there (one per combination?), and how do you avoid overfitting given the varying sample sizes across combinations (e.g., Tri-modal only has 118 samples)?\n\n2. The paper suggests future work report computational metrics (e.g., FLOPS, FPS). Have you tested PersonNet’s performance on edge devices (e.g., mobile GPUs) to evaluate its deployment potential in primary care settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9eFNoebsqy", "forum": "QCCMvWYwPN", "replyto": "QCCMvWYwPN", "signatures": ["ICLR.cc/2026/Conference/Submission18816/Reviewer_bWtN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18816/Reviewer_bWtN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728561293, "cdate": 1761728561293, "tmdate": 1762999998130, "mdate": 1762999998130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Could dot find reviewers' comments"}, "comment": {"value": "Dear PC, SAE and AE,\n\nWe could not find the reviewers' comments under the author' console page. Could you please help to set the reviewers' comments to be visible for us? Thank you very much!"}}, "id": "NqFnl94Y4P", "forum": "QCCMvWYwPN", "replyto": "QCCMvWYwPN", "signatures": ["ICLR.cc/2026/Conference/Submission18816/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18816/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18816/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762970746514, "cdate": 1762970746514, "tmdate": 1762970746514, "mdate": 1762970746514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces M3Ret, a mixed-modality ophthalmic dataset with multiple modality combinations. \n\nIt also proposes PersonNet, a baseline for personalized multi-disease detection with a memory-bank completion and fusion strategy. \n\nTens of multimodal methods are benchmarked on this dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The problem of multiple retinal disease and AI diagnosis is timely.\n\n+ The dataset scale is attractive.\n\n+ Overall the paper is easy-to-follow and clearly-presented."}, "weaknesses": {"value": "- This paper claims “first” to support diverse combinations for eye disease, which is not true. Some prior works are listed as follows.\n\n[1] Harvard Glaucoma Detection and Progression: A Multimodal Multitask Dataset and Generalization-Reinforced Semi-Supervised Learning. [https://arxiv.org/abs/2308.13411]\n\n[2] FairVision: Equitable Deep Learning for Eye Disease\nScreening via Fair Identity Scaling [https://github.com/Harvard-Ophthalmology-AI-Lab/FairVision]\n\n- The center and vendor type in this work is not clearly detailed, and may be rather limited. Instead, cross-site testing and generalization to other vendors are needed, to enrich the diversity of the benchmark.\n\n- For the evaluation metrics, it may be more rationale to consider precision, recall, F1-score and their class-wise metric.\n\n- Besides, please do more analysis and show the baseline outcomes on the per-class per-disease performance.\n\n- The technique novelties of the proposed method are limited. Specifically, memory-bank completion with class-wise prototypes is a straightforward instantiation of missing-modality completion.\n\n- Since this paper considers the imcomplete modality settings, some stronger generative baselines should also be compared.\n\n- The experiments and validation seem insufficient. For example, the sensitivity to prototype bank size/update. Besides, how does the class imbalance problem impact the performance?"}, "questions": {"value": "Please refer to the weakness section, and address these concerns point-by-point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ogHGJtYua5", "forum": "QCCMvWYwPN", "replyto": "QCCMvWYwPN", "signatures": ["ICLR.cc/2026/Conference/Submission18816/Reviewer_Zydr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18816/Reviewer_Zydr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757369680, "cdate": 1761757369680, "tmdate": 1762999996388, "mdate": 1762999996388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenges of incomplete modality combinations and multi-disease detection in ophthalmic multimodal image analysis by proposing the $M^3Ret$ dataset and the $PersonNet$ method. $M^3Ret$ comprises retinal images with diverse modality combinations and supports the detection of three retinal diseases. $PersonNet$ handles varying modality combinations through missing modality completion and personalized fusion strategies. The study conducts a benchmark evaluation of the proposed method alongside existing multimodal learning approaches on the $M^3Ret$ dataset and provides an analysis of the results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The $M^3Ret$ dataset holds certain advantages in scale and encompasses diverse imaging modality combinations, which better reflects real-world clinical heterogeneity compared to existing datasets that only include complete modality pairs.\n\n2. This paper identifies a limitation in current ophthalmic multimodal research, specifically, its reliance on fixed, complete modality combinations. And introduces the problem of personalized detection, which is more aligned with practical clinical scenarios."}, "weaknesses": {"value": "1. All evaluations of the proposed PersonNet method were conducted solely on the $M^3Ret$ dataset, with no testing performed on external datasets. This limitation fails to demonstrate the method's generalization capability and broader effectiveness.\n\n2. The core components of PersonNet, the class-wise prototype-based missing modality completion and the SKNet-inspired feature fusion—represent relatively straightforward and conventional approaches within the existing literature on incomplete multimodal learning. The paper does not sufficiently justify the significant innovation of the proposed method compared to existing techniques.\n\n3. Critical ablation studies are reported only on the validation set, lacking final verification on the test set. This omission undermines the reliability of the conclusions drawn."}, "questions": {"value": "As indicated in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2tz9jr6zmJ", "forum": "QCCMvWYwPN", "replyto": "QCCMvWYwPN", "signatures": ["ICLR.cc/2026/Conference/Submission18816/Reviewer_nhTT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18816/Reviewer_nhTT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825653685, "cdate": 1761825653685, "tmdate": 1762999996154, "mdate": 1762999996154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces M$^3$Ret, a mixed multimodal ophthalmic dataset of 8,558 eyes / 5,235 patients covering UWF-SLO, macular OCT (128 B-scans), disc OCT (200 B-scans) with seven real-world modality combinations. It defines two benchmark suites (DA: multi-disease ME/DR/Glaucoma; DB: glaucoma) with stratified 6:2:2 patient splits, and provides PersonNet as a baseline supporting incomplete-modality inputs. The benchmark reports comprehensive performance metrics. Authors have release code and a data link and discuss ethics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**S1.** This paper is well-organized and easy to follow.\n\n**S2.** Seven observed modality combinations reflect routine workflows (uni-/bi-/tri-modal sampling), which is rare in this space.\n\n**S3.** The paper reports age/sex distributions and disease prevalence close to epidemiology, supporting external validity. In addition, the provided dataset and code are both complete."}, "weaknesses": {"value": "**W1.** Most labels come from EMR diagnoses; when those are missing, experts infer them from treatment records. Some cases are still marked ‘unclear,’ and there’s no inter-rater protocol, adjudication process, or label-noise audit.\n\n**W2.** DA and DB are reorganizations of the same hospital cohort with stratified 6:2:2 splits, but it is not explicit whether splits are patient-disjoint within and across DA/DB, nor whether tri-modal cases can leak information between tasks.\n\n**W3.** Benchmark scope is misses clinically critical metrics including ROC-AUC, calibration (ECE/Brier), or subgroup performance (e.g., age/sex strata) and analyses.\n\n**W4.** Missing details on OCT resampling/padding and on whether duplicate scans are kept or discarded."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pHcMalhuRW", "forum": "QCCMvWYwPN", "replyto": "QCCMvWYwPN", "signatures": ["ICLR.cc/2026/Conference/Submission18816/Reviewer_iVNK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18816/Reviewer_iVNK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911706627, "cdate": 1761911706627, "tmdate": 1762999996268, "mdate": 1762999996268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}