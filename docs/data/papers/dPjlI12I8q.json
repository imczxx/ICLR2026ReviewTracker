{"id": "dPjlI12I8q", "number": 12829, "cdate": 1758210621508, "mdate": 1759897482409, "content": {"title": "Is Random Attention Sufficient for Sequence Modeling?", "abstract": "The transformer architecture is central to the success of modern Large Language Models (LLMs), in part due to its surprising ability to perform a wide range of tasks -- including mathematical reasoning, memorization, and retrieval -- using only gradient-based learning on next-token prediction. While the core component of a transformer is the self-attention mechanism, we question how much, and which aspects, of the performance gains can be attributed to it. To this end, we compare standard transformers to variants in which either the attention weights or the MLP layers are frozen at initialization. Surprisingly, we find that attention with frozen key and query weights is not only able to form induction heads, but can also perform competitively on language modeling. We formalize this by proving a new expressivity result for transformer models with frozen attention weights. To further isolate the contribution of attention, we design MixiT -- the Mixing Transformer -- an architecture variant with entirely random attention scores, with provably stable signal propagation that overcomes prior depth-wise scaling challenges in random transformers. We use the successes and failures of our spectrum of models to pinpoint the role each main transformer component plays. Our results suggest that the transformer architecture has a built-in inductive bias towards in-context reasoning, as it can form specialized circuits even without learnable attention weights.", "tldr": "The transformer architecture has a built-in inductive bias towards forming specialized circuits.", "keywords": ["Transformer architecture", "expressiveness", "interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f86e82da6edd1f544e8ddaadea54f76647736879.pdf", "supplementary_material": "/attachment/e805a320bc00277ee1e756eb1fcc840ab7ff8791.zip"}, "replies": [{"content": {"summary": {"value": "This article investigates the model performance under fixed random QK, random attention score, and random MLP, and proves that some tasks do not require learnable QK, and some tasks do not even require learnable attention score."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The result is very interesting. The author conducted sufficient experiments to verify the impact of different degrees of weakening on sequence modeling. Even tasks such as induction heads can still be learned well when fixing Wq and Wk. \n\n2. The author proposed a randomized attention score method and theoretically demonstrated its stability. In addition, the author also proved the universal approximation under random matrices."}, "weaknesses": {"value": "1. \nThe presentation of MixiT is confusing.  \n\nFrom the experimental results, it can be seen that the performance of MixiT is not very good, especially in tasks related to language modeling. And only frozen-QK is well. So the \"random attention\" in your title is refers to frozen-QK only? I think if we could more clearly distinguish between random attention weight (frozen-QK) and random attention matrix (MixiT) in this article, it would be more conducive to reading.  Besides, the appearance of Theorem 2.1 in the seciton is also very abrupt, It should perhaps be placed in the section 5 where it will be used."}, "questions": {"value": "1. \nThe boundary between what tasks Mixit can and cannot do well is very empirical. Do you have any further elaboration on thisï¼ŸOn the other hand, I find that Mixit seems to be not able to do very well on long context tasks, such as induction heads and language modeling, Is that so? \n\n2. \nThe results of Frozen-mlp are missing in Table 2 and Table 3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6WWUG6NgKG", "forum": "dPjlI12I8q", "replyto": "dPjlI12I8q", "signatures": ["ICLR.cc/2026/Conference/Submission12829/Reviewer_Mct9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12829/Reviewer_Mct9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12829/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760518148114, "cdate": 1760518148114, "tmdate": 1762923632820, "mdate": 1762923632820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper\n- finds that Frozen-QK with random attention weights can perform competitively with the standard transformer on language modeling tasks. It's expressiveness is also enough for a wide class of sequence-level functions.\n- proposes MixiT, proves its training stability. It achieves performance comparable to fully trained tf and Frozen-QK expect induction heads tasks and language modeling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper did abundant experiments and derived theory to support the papers claim, which makes the paper sound."}, "weaknesses": {"value": "W1: Your main claim is that the performance separation between input-dependent and input-independent attention is largely driven by the latter's inability to form induction heads. However, there are some papers [1], [2] explaining the mechanistic of induction heads theoretically which I think is missing from your paper. In their papers, they all have data-dependent attention for the second layer and it must contribute to the proof. What is the contribution of your paper to the (theoretical) understanding of the mechanics?\n\nW2: You proved Frozen-QK has enough expressivity, but it does not answer the question how can Frozen-QK form the induction head. The former doesn't guarantee the latter because the latter question is about optimization.\n\n[1] Nichani, E., Damian, A., & Lee, J. D. (2024). How transformers learn causal structure with gradient descent. arXiv preprint arXiv:2402.14735.\n\n[2] Chen, S., Sheen, H., Wang, T., & Yang, Z. (2024). Unveiling induction heads: Provable training dynamics and feature learning in transformers. Advances in Neural Information Processing Systems, 37, 66479-66567."}, "questions": {"value": "Q1: I didn't understand your third contribution and didn't find the part of the paper it correlated to. \n\nAlso see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kMR9schv0G", "forum": "dPjlI12I8q", "replyto": "dPjlI12I8q", "signatures": ["ICLR.cc/2026/Conference/Submission12829/Reviewer_Cdgz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12829/Reviewer_Cdgz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12829/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856410252, "cdate": 1761856410252, "tmdate": 1762923631924, "mdate": 1762923631924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work studies what aspects of transformer performance rely on the self-attention mechanism by comparing the transformer to variants with frozen attention weights or MLP layers. They find that the transformer with frozen QK layers can achieve competitive performance on the induction head and language modeling tasks. The authors also propose the Mixing Transformer (MixiT) which uses random attention and overcomes prior depthwise scaling challenges of random transformers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The writing and methodology are clear and easy to follow.\n- The experiments cover a diverse set of tasks that highlight which components are most important.\n- The finding that the Frozen-QK model can form induction-head-like behavior and achieve strong performance on retrieval and k-hop tasks is particularly interesting."}, "weaknesses": {"value": "- The discussion of MLPs being important for knowledge storage mostly confirms prior findings and does not extend existing insights.\n- The motivation and practical usefulness of the MixiT architecture are not entirely clear. The paper emphasizes that the MixiT architecture provides stable signal propagation as compared to a random transformer, but still underperforms on the induction heads task and on language modeling.\n- For the language modeling experiments the sequence length is 256 which is quite short. Do results hold for longer sequence length?"}, "questions": {"value": "- How sensitive are the Frozen-QK and MixiT models to the specific random initialization used?\n- How do the authors see MixiT being used beyond this study -- as a diagnostic model or are there some implications for architecture design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SQiUFGPOpk", "forum": "dPjlI12I8q", "replyto": "dPjlI12I8q", "signatures": ["ICLR.cc/2026/Conference/Submission12829/Reviewer_5cHZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12829/Reviewer_5cHZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12829/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871746658, "cdate": 1761871746658, "tmdate": 1762923631558, "mdate": 1762923631558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}