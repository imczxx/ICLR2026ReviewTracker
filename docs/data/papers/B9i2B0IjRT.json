{"id": "B9i2B0IjRT", "number": 20676, "cdate": 1758308901871, "mdate": 1759896964562, "content": {"title": "From small to large language models: How much confidence can we have?", "abstract": "In this paper, we provide novel insights into information-based, consistency-based and self-verbalized uncertainty quantification (UQ) for multi-label text classification across a range of recent language models on a new unsaturated benchmark of medical device adverse event reports with interdependent labels. We compare more than twenty encoder- and decoder-only language models across three paradigms: discriminative fine-tuning, generative fine-tuning, and few-shot in-context prompting (instruction-tuned and reasoning variants, local and API-accessible). UQ is performed using token-information measures, consistency under stochastic generation, and self-verbalized confidence, with utility assessed via selective prediction. We provide practical guidance on model selection, when fine-tuning is preferable to prompting, and which UQ signals are most effective for routing and human-in-the-loop triage. Our results reveal trade-offs across model types. Discriminatively fine-tuned decoders achieve the strongest head–tail accuracy while still offering solid uncertainty quantification (UQ). In contrast, generative fine-tuning provides the most reliable UQ overall. Reasoning models improve performance on extreme-tail labels but yield weak UQ. Finally, self-verbalized confidence proves unreliable as an indicator of model certainty.", "tldr": "We propose a new multi-label dataset and compare uncertainty quantification for models trained via discriminative or generative fine-tuning and for prompting-based approaches.", "keywords": ["Large Language Models", "Uncertainty Quantification", "Multi-label classification"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43575402be2d5df0ec30725d802b9dda6ff5b7ca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work quantifies three types of uncertainty (into information-based, consistency-based and self-verbalized) of 20+ encoder- and decoder-only language models in three context: discriminative fine-tuning, generative fine-tuning, and few-shot in-context prompting. Experimental results were analyzed to provide practical guidance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Use multiple uncertainty quantification methods\n* Evaluate 20+ models."}, "weaknesses": {"value": "* How to ensure there is no pre-training exposure? The proposed dataset was compiled from information publicly available at  https://open.fda.gov, which may have been used to pretrain LLMs\n* Lines 112-113: Softmax is used. Are those labels mutually exclusive?\n* This work claims that it \"systematically study multiple UQ approaches ... including information-, consistency- and self-verbalized based uncertainty\". Please discuss why chose them and How they overlap and differ. Note: Harsha Tanneru et al. (2024) proposed \"verbalized uncertainty\" and reported that \"verbalized uncertainty is not a reliable estimate of explanation confidence\"."}, "questions": {"value": "* Line 275: Are U_info and U_cons independent? What does their multiplication mean semantically?\n* The \"self-verbalized uncertainty\", explained in lines 294-295, may not be the uncertainty of the prediction value produced by model.  How does an LLM generate a \"confidence score\"? Do the \"confidence scores\" produced by all LLMs mean the same thing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5U4JN74qjq", "forum": "B9i2B0IjRT", "replyto": "B9i2B0IjRT", "signatures": ["ICLR.cc/2026/Conference/Submission20676/Reviewer_DefB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20676/Reviewer_DefB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761690249656, "cdate": 1761690249656, "tmdate": 1762934061458, "mdate": 1762934061458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines uncertainty quantification for multi-label text classification using over 20 recent language models in three usage setups: discriminative fine-tuning, generative fine-tuning, and few-shot prompting. It introduces a large dataset of FDA medical device reports labeled with hierarchical codes. The study compares information-based, consistency-based, and self-verbalized confidence methods. It finds that generative models provide the most reliable uncertainty estimates, while self-verbalized confidence performs poorly."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a large labeled dataset of FDA medical device reports, which might not overlap with the existing dataset, allowing for a fair evaluation.\n2. The experiments are done over three different settings and over 20 models."}, "weaknesses": {"value": "1. The writing of this paper lacks some important details. For example, there is no information about which U_{info} is used in the Table1. Besides, for consistency-based methods and reasoning models, whether CoT is considered in the consistency calculation is also not mentioned. \n2. The evaluation is only based on the text classification task, which constrains the conclusion a lot since many methods are designed for open-end generations like the consistency used in this paper.  I also doubt some choices of U_{info} used in the paper. For example, Perplexity and Avg(\\pi) should be exactly the same for ranking ability since exp is a monotonous function and will not influence the ranking. And the evaluation metric PRR only considers the rank ability.\n3. Lack of some important methods in UQ. For example, p(true) [1] should be considered as a self-verbalized uncertainty and it performs much better than asking models to output a confidence. Besides, I am also wondering how semantic entropy [2], which combines both consistency and information methods, can be categorized in this paper’s framework. Besides, it seems less meaningful to use U_{cons} in the current setting because U_{cons} is based on the semantic similarity, that is why the original U_{cons} paper shows that using a natural language inference model could get a better result. However, the current output from models (A0102) does not contain any semantic information.\n4. The evaluation of ‘calibrated’ UQ seems not accurate in the paper. Normally, calibration of UQ refers to expected calibration error (ECE) [3] instead of the ranking ability of UQ.\n5. The findings from the experiments lack of in-depth explanations. For example, the author claims that ‘generative fine-tuning yields the strongest UQ, followed by discriminative fine-tuning’, which is a good start to a series of important investigations. However, the authors do not provide analysis on why generative fine-tuning can yield the strongest UQ. Which difference between generative and discriminative fine-tuning causes the different performances in UQ? Why is generative fine-tuning better? Is it because of the training prompt? The additional classification layer or something else? Then why does Llama-3.2-3B-Base and Llama-3.2-8B-Base have a better PRR for discriminative fine-tuning instead of generative fine-tuning?  Then for reasoning, what cause the reasoning models to perform worse in UQ?\n\n\n[1] Kadavath, Saurav, et al. \"Language models (mostly) know what they know.\" arXiv preprint arXiv:2207.05221 (2022).\n[2] Kuhn, Lorenz, Yarin Gal, and Sebastian Farquhar. \"Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation.\" The Eleventh International Conference on Learning Representations.\n[3] Guo, Chuan, et al. \"On calibration of modern neural networks.\" International conference on machine learning. PMLR, 2017."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4P8ZhUQ9T6", "forum": "B9i2B0IjRT", "replyto": "B9i2B0IjRT", "signatures": ["ICLR.cc/2026/Conference/Submission20676/Reviewer_kUyz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20676/Reviewer_kUyz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927802620, "cdate": 1761927802620, "tmdate": 1762934061094, "mdate": 1762934061094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper evaluates different LLMs on a self-curated biomedical dataset to aim to understand when to select an appropriate approach for a given model. The evaluation is extensive, using accuracy and UQ metrics, with particular attention paid to the latter. The evaluation is also done across models after discriminative finetuning, generative finetuning, and reasoning prompting. The paper finds that generative finetuning provides the most reliable UQ, smaller encoder models achieve competitive performance after discriminative finetuning, and thinking models have weaker UQ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The strengths of this paper are the scope of experiments and clarity. The paper cross-compares three areas. Each area is important and the research community would be interested in learning how it influences the LLM: 1) different ways of strengthening a model (namely, two finetuning methods and one in-context/prompting method), 2) model types and sizes; 3) their UQ capabilities. The writing is pretty clear as well.\n\nThe paper also curates a dataset that would help the LLM community to study and improve LLMs."}, "weaknesses": {"value": "A weakness of the paper is a lack of central claim or central finding. While experiments cross-compare different aspects of language models, the results are hard to interpret. The paper summarizes several patterns from the results (such as those I mentioned in 'Summary'), but I cannot find a deep analysis of these patterns. \n\nThe current experiments are a good start, but I would suggest that the paper finds one or two of these result patterns and does a closer study. Just as an example, the paper finds that reasoning models offer weaker UQ. It is possible to look into why they are weaker in UQ and use additional experiments to test these claims. Then, the phenomenon of reasoning models offering weaker UQ will be more convincing.\n\nAnother aspect I would like to know more about is why generative finetuning has the best UQ, and in particular better than discriminative UQ. Intuitively generative finetuning is harder for UQ because there is semantic uncertainty vs token uncertainty: there are different words that can achieve the same meaning (e.g. \"I'm happy to\" vs \"I'm glad to\"). More analysis on the nature of these two types of finetuning would help. Currently, the picture is still unclear to me."}, "questions": {"value": "Additional suggestions:\n\n* Abstract - \"We provide practical guidance on model selection, when fine-tuning is preferable to prompting, and which UQ signals are most effective for routing and human-in-the-loop triage\" - grammatical error, maybe needs a 'such as' before 'when'\n* Introduction - Needs a paragraph break in the long first paragraph\n* Additional citation suggestion in UQ: Gruver, et al. (2023) - Large Language Models Are Zero-Shot Time Series Forecasters"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X9hNQ7BYZw", "forum": "B9i2B0IjRT", "replyto": "B9i2B0IjRT", "signatures": ["ICLR.cc/2026/Conference/Submission20676/Reviewer_km3S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20676/Reviewer_km3S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762718368044, "cdate": 1762718368044, "tmdate": 1762934060459, "mdate": 1762934060459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines uncertainty quantification (UQ) for multi-label text classification across various language models to provide guidance for model selection. The authors propose a new unsaturated benchmark (to avoid training data leakage) of medical device adverse event reports with a hierarchical and multi-label structure. The authors fine-tune or prompt over twenty language models ranging from small encoder-only to large decoder-only models, examining different learning paradigms: discriminative fine-tuning, generative fine-tuning, and few-shot prompting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**The following are the strengths of the paper:**\n1. This paper compares different uncertainty quantification (UQ) measures for multi-label text classification over 20 language models.\n\n2. The authors create a new dataset of medical device adverse event reports with a hierarchical and multi-label structure."}, "weaknesses": {"value": "**The following are the weaknesses of the paper:**\n1. The primary contribution of this paper is introducing a new dataset designed for uncertainty quantification (UQ) of different language models.\n\n2. The paper does not propose any novel problem formulation, method, or new UQ metric.\n\n3. As the evaluation depends solely on a single dataset, the findings and insights presented in the paper may be biased or limited in their generalizability."}, "questions": {"value": "I have a few more questions/comments:"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not find any ethical concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MJuTvF7utt", "forum": "B9i2B0IjRT", "replyto": "B9i2B0IjRT", "signatures": ["ICLR.cc/2026/Conference/Submission20676/Reviewer_Havv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20676/Reviewer_Havv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20676/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762943571661, "cdate": 1762943571661, "tmdate": 1762943614573, "mdate": 1762943614573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}