{"id": "BAsE1o52LZ", "number": 16845, "cdate": 1758269344479, "mdate": 1763738980905, "content": {"title": "The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models", "abstract": "Existing large audio-language models perceive the world as \"mono\"—a single stream of audio that ignores the critical spatial dimension (\"where\") required for universal acoustic scene analysis. To bridge this gap, we first introduce a hierarchical framework for Auditory Scene Analysis (ASA). Guided by this framework, we introduce a system that enables models like Qwen2-Audio to understand and reason about the complex acoustic world. Our framework achieves this through three core contributions: First, we build a large-scale, synthesized binaural audio dataset to provide the rich spatial cues. Second, we design a hybrid feature projector, which leverages parallel semantic and spatial encoders to extract decoupled representations. These distinct streams are integrated via a dense fusion mechanism, ensuring the model receives a holistic view of the acoustic scene. Finally, we employ a progressive training curriculum, advancing from supervised fine-tuning (SFT) to reinforcement learning via Group Relative Policy Optimization (GRPO), to explicitly evolve the model's capabilities towards reasoning. On our comprehensive benchmark, the model demonstrates comparatively strong capability for spatial understanding. By enabling this spatial perception, our work provides a clear pathway for leveraging the powerful reasoning abilities of large models towards holistic acoustic scene analysis, advancing from \"mono\" semantic recognition to spatial intelligence.", "tldr": "Spatial Understanding in Large Audio-Language Models", "keywords": ["Spatial Audio", "Large Audio-Language Models", "Acoustic Scene Analysis", "Mixture-of-Experts", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/927931f9f0c169b2c4bddf6f1face71aac001a16.pdf", "supplementary_material": "/attachment/47c1f9f362088184612c170205c94d175e381a83.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces \"The World is Not Mono\" (TWNM), a comprehensive framework for instilling spatial audio understanding in large audio-language models (LALMs). TWNM combines a synthetic large-scale binaural audio dataset, a Mixture-of-Experts (MoE) architecture that decouples semantic and spatial processing, and a multi-stage training curriculum culminating in reinforcement learning with Group Relative Policy Optimization (GRPO). The authors benchmark their system on an auto-generated, multi-task evaluation suite that probes perception, integration, and reasoning skills."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an often-overlooked limitation in LALMs—spatially aware auditory reasoning. The framing underscores the need for models capable of holistic scene analysis rather than mono-dimensional semantic understanding.\n2. The use of decoupled semantic and spatial encoders combined via a conditional MoE is interesting.\n3. Paper is well written and easy to follow."}, "weaknesses": {"value": "1. This work lacks a proper baseline and only compares results from different training stages of its own model.\n\n2. It would be better if the analysis also included sim-to-real performance.\n\n3. The use of multiple encoders cannot guarantee complete disentanglement of audio information, and to some extent, it may even lead to desynchronization between different information streams. As discussed in the paper, the system may correctly identify sound sources A and B but confuse their directions or distances or others. The performance of spatial-relationship reasoning at 34.02% and attribute binding integration at 37.07% further prove this. Therefore, is such a disentangled multi-encoder approach combined with MoE truly an appropriate method?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LEDO5HMklc", "forum": "BAsE1o52LZ", "replyto": "BAsE1o52LZ", "signatures": ["ICLR.cc/2026/Conference/Submission16845/Reviewer_fUwJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16845/Reviewer_fUwJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761544571643, "cdate": 1761544571643, "tmdate": 1762926865542, "mdate": 1762926865542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response: Theoretical Framework, Architecture Updates, and Sim-to-Real Justification"}, "comment": {"value": "We sincerely thank the reviewers for their constructive feedback. We are encouraged that the reviewers recognize the **originality** of tackling spatial reasoning in LALMs (Reviewer cEND, UYn1) and the **significance** of bridging the \"spatial deafness\" gap (Reviewer cEND, fUwJ).\n\nIn response to the reviewers' common concerns regarding **problem formulation**, **architectural validity**, **baselines**, and **Sim-to-Real evaluation**, we have performed a substantial revision. The key updates are summarized below:\n\n**1. Formalized Theoretical Framework: Auditory Scene Analysis (ASA)**\n* **Response to Reviewer UYn1:** We acknowledge that the initial formulation was too general. In the revision (Section 3.1), we have introduced a formal framework inspired by **Auditory Scene Analysis (ASA)**. We explicitly define three cognitive layers:\n    * **$\\mathcal{L}_1$ Static Identification:** Atomic perception of \"what\" (semantic) and \"where\" (spatial).\n    * **$\\mathcal{L}_2$ Relational Integration:** Solving the \"binding problem\" to associate attributes (e.g., linking a sound to a location).\n    * **$\\mathcal{L}_3$ Cognitive Reasoning:** High-level inference (causality, counterfactuals) based on the scene graph.\n\n**2. Architectural Refinement: From Sparse MoE to Hybrid Dense Fusion**\n* **Response to Reviewer fUwJ & UYn1:** Reviewer fUwJ raised a valid concern that multi-encoder decoupling might lead to \"desynchronization\" or binding errors. To address this, we upgraded the architecture from a sparse-gating MoE to a **Hybrid Feature Projector with Dense Fusion** (Section 4.2).\n* Instead of dynamically selecting experts (which risks information loss), our improved architecture processes semantic and spatial streams through specialized parallel pathways and then employs a **dense fusion mechanism**. This ensures that the LLM receives a simultaneous, holistic view of all attributes at every step, significantly improving the **Attribute Binding ($\\mathcal{L}_2$)** performance compared to the sparse routing approach.\n\n**3. New Baseline Comparison: TWNM vs. BAT**\n* **Response to All Reviewers:** We have added a comprehensive comparison with **BAT**, a representative SOTA spatial audio model (Section 6.2).\n* **Key Finding:** While BAT performs comparably on basic perception, it fails catastrophically on **$\\mathcal{L}_3$ Reasoning** tasks, achieving only **36.40%** accuracy (worse than random chance on binary verification). In contrast, our proposed model achieves **79.60%**, proving that generic audio encoders are insufficient for spatial intelligence.\n\n**4. Clarification on Sim-to-Real Evaluation**\n* **Response to Reviewer cEND & fUwJ:** We acknowledge the suggestion to test on real-world datasets (e.g., STARSS23). However, we respectfully clarify why a direct Sim-to-Real evaluation is currently infeasible for high-fidelity binaural reasoning:\n    * **Data Format Mismatch:** Most public real-world spatial datasets record audio in **First-Order Ambisonics (FOA)**.\n    * **Theoretical Limitation:** Converting FOA to binaural audio results in severe information loss. Acoustically, to avoid spatial aliasing and perfectly reconstruct the sound field for a human head up to 20 kHz, a spherical harmonic order of $N \\approx kr \\approx 32$ is theoretically required.\n    * **Perceptual Consequence:** Our internal tests show that FOA-to-Binaural rendering suffers from severe **\"in-head localization\"** and spatial blur, making it impossible to evaluate fine-grained directional reasoning.\n    * **Future Work:** We view support for native Ambisonics (FOA/HOA) input as a promising future direction to bridge this gap, rather than evaluating on degraded binaural conversions.\n\nReproducibility: As stated in our paper, we have uploaded the **anonymized core code** (including model architecture definitions and training scripts) in the supplementary material to facilitate reproducibility checks."}}, "id": "5W2zkYZWBZ", "forum": "BAsE1o52LZ", "replyto": "BAsE1o52LZ", "signatures": ["ICLR.cc/2026/Conference/Submission16845/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16845/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16845/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763739595320, "cdate": 1763739595320, "tmdate": 1763740105708, "mdate": 1763740105708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates equipping Large Audio Language Models (LALM) with spatial perception besides semantic understanding. To accomplish this, an MoE framework with four-stage learning is trained sequentially: (1) Learn the stem spatial audio representation with a spatial encoder, decoupled from the semantic representation; (2) Learn the MoE experts and router to process the concatenated semantic and spatial embeddings; (3) (SFT1.0) Align the MoE weights with LLM (finetuned with LoRA), with additional router supervision; and (4) (SFT2.0) Remove router supervision to further align MoE with LLM in end-to-end training. Additionally, GRPO is employed to further optimize the LLM under these spatial tasks. Moreover, this work proposes a new dataset and benchmark for LALM’s spatial audio reasoning by synthesizing binaural audio from single-channel audio data and querying LLM for question-answer pairs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "-\tOriginality: This paper accurately identifies the research gap in LALM’s spatial understanding capabilities. This is a nontrivial problem as humans have strong spatial cognition in hearing.\n\n-\tQuality:  The engineering efforts of this work are solid and well-planned. The four-stage training strategy appears justified with ablations, especially by the contrast of performance between SFT1.0 and SFT2.0.\n\n-\tClarity: The proposed pipeline is illustrated clearly in a sequential manner.\n\n-\tSignificance: This work is among the first to address the spatial audio understanding problem for LALMs."}, "weaknesses": {"value": "-\tThe presentation of problem formulation is general and unclear. The authors define spatial audio understanding by examples in introduction, but do not categorize this understanding from a broad concept into specific tasks and define each. One can only grasp the outline of these tasks until the experiments section where dataset is introduced. Paragraph 2 in introduction is especially confusing because of this over-abstraction. Please see question 1 below for a request of clarification.\n\n-\tWhile it’s understandable that the proposed problem is relatively novel and lacks baseline models, sufficient experiments are still needed to demonstrate the proposed solution’s legitimacy. Instead of the complex training involved, one could prompt a spatial audio model for spatial localization, and another LALM for semantic understanding of the scene. Combining these predictions and prompting them to another LLM would resolve the “spatially deaf” limitation of current LALMs. Further experiments need to be conducted on these alternative baselines for comparison, delineating the necessity of the proposed method.\n\n-\tIt’s curious why the proposed training pipeline requires this much complexity. From an engineering perspective, employing MoE is sound for its ability to dedicate parameter groups to semi-explicit subtasks. However, theoretically how much more gain can be achieved with MoE than a unified encoder-LLM mapper is under-studied here. It’s hard to justify this sheer amount of engineering tricks without seeing the performance tradeoff. The originality and novelty of this work thereby are heavily affected by the lack of this ablation."}, "questions": {"value": "-\tWhat are the major tasks in spatial audio understanding? Why is each dependent on binaural cues instead of single-channel semantics?\n\n-\tHow much performance gain can be attributed to the fusing module? The model could be separately picking up semantic cues from the binaural channels to accomplish certain tasks.\n\n-\tWhy MoE is needed to address this task? Could a unified encoder-LLM mapper achieve similar/better performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yDCKKrtUGD", "forum": "BAsE1o52LZ", "replyto": "BAsE1o52LZ", "signatures": ["ICLR.cc/2026/Conference/Submission16845/Reviewer_UYn1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16845/Reviewer_UYn1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835126288, "cdate": 1761835126288, "tmdate": 1762926865042, "mdate": 1762926865042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework called “The World is Not Mono (TWNM)”, which aims to make Large Audio-Language Models (LALMs) spatially aware. Most existing models only process mono audio, this paper tackles that by giving models the ability to reason about spatial cues like direction, distance, and room acoustics. The authors build a synthetic binaural dataset using physically simulated environments with BRIR and HRTF filters to generate spatially accurate audio scenes. They further propose a task-aware Mixture-of-Experts (MoE) architecture that separates semantic and spatial processing, with specialized experts for handling aspects like direction, distance, reverberation, and source count. The training follows a progressive curriculum, beginning with supervised fine-tuning and culminating in reinforcement learning through GRPO to enhance spatial reasoning. The paper presents a new spatial reasoning benchmark comprising 1,000 multiple-choice questions designed to test perception, integration, and reasoning capabilities. Overall, the model demonstrates strong improvements across all task types, achieving an overall accuracy of 61%, with particularly notable gains in complex reasoning tasks after GRPO training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a fundamental gap in LALMs — spatial reasoning, which few have tackled.\n- The staged curriculum (SFT → SFT 2.0 → GRPO) helps stabilize training."}, "weaknesses": {"value": "- Only one LALM i.e. Qwen2-Audio is tested. \n- Limited experimental comparison with other spatial LALMs.\n- Missing human evaluation."}, "questions": {"value": "- Can this be generalized to other models like SALMONN, AudioGPT, or Whisper-based LALMs?\n- Can spatial LALMs like BAT, etc. be compared?\n- Have you tested on real spatial datasets (e.g., STARSS23 or L3DAS23)? If not, how do you expect it to handle real-world acoustics that deviate from simulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8n1XSaUPcu", "forum": "BAsE1o52LZ", "replyto": "BAsE1o52LZ", "signatures": ["ICLR.cc/2026/Conference/Submission16845/Reviewer_cEND"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16845/Reviewer_cEND"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053121334, "cdate": 1762053121334, "tmdate": 1762926864055, "mdate": 1762926864055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}