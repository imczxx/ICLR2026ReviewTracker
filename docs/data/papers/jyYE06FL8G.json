{"id": "jyYE06FL8G", "number": 4658, "cdate": 1757737264607, "mdate": 1763522604795, "content": {"title": "EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models", "abstract": "Advancements in Large Language Models (LLMs) drive interest in scientific applications, necessitating specialized benchmarks such as Earth science. Existing benchmarks either present a general science focus devoid of Earth science specificity or cover isolated subdomains, lacking holistic evaluation. Furthermore, current benchmarks typically neglect the assessment of LLMs' capabilities in open-ended scientific exploration. In this paper, we present a comprehensive and professional benchmark for the Earth sciences, designed to evaluate the capabilities of LLMs in scientific exploration within this domain, spanning from fundamental to advanced levels. Leveraging a corpus of 100,000 research papers, we first construct two Question Answering (QA) datasets: Earth-Iron, which offers extensive question coverage for broad assessment, and Earth-Silver, which features a higher level of difficulty to evaluate professional depth. These datasets encompass five Earth spheres, 114 disciplines, and 11 task categories, assessing foundational knowledge crucial for scientific exploration. Most notably, we introduce Earth-Gold with new metrics, a dataset comprising open-ended multi-turn dialogues specifically designed to evaluate the depth and diversity of LLMs in scientific exploration, including methodology induction, limitation analysis, and concept proposal. Extensive experiments reveal limitations in 11 leading LLMs across different domains and tasks, highlighting considerable room for improvement in their scientific exploration capabilities.", "tldr": "", "keywords": ["large language model", "science", "earth", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d73fd05eb3429c274c10d1c78e858083dbb972ee.pdf", "supplementary_material": "/attachment/d6875f6df312af08f441717648c882737cd97c1a.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new set of benchmarks that deal with Earth Science, a topic that is missed by current LLM benchmarks. The contribution is two QA datasets, an easier Earth-Bronze and a more challenging Earth-Silver, with an additional scientific exploration Earth-Gold dataset. All in all, over 100,000 papers were considered for the benchmark construction. The paper additionally proposes a new metric to be used to judge LLM performance on scientific exploration, which is used on Earth-Gold. Lastly, 11 LLMs are evaluated on the 3 proposed benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Extending LLMs to topics unexplored by current benchmarks, like Earth Science is important\n- The benchmarks are comprehensive, taking into account a pool of more than 100,000 papers. The end result features a large set of 114 disciplines, which is relatively large compared to other similar benchmarks\n- The paper is clear and easy to understand"}, "weaknesses": {"value": "- LLMs are used in both benchmark construction and in evaluation (retention, win rate). Given that LLMs have been shown to be biased in various ways [1], using them to deconstruct papers into parts and then come up with questions, and then judge proposed answers seems prone to noise, biases and errors. Even with expert curation, the sheer volume of the base set of papers raises concerns with the regards about the amount of errors that can be caught. \n- The nature of the tasks means they're hard to define well. For example, the example tasks given in Table 2's research section are open ended and I could see human experts giving varied and even contradictory answers. \n- Looking at Earth-Iron/Silver: these feature more well defined answers, but on the other hand, are close to being saturated. Apart from fill in the blank, the best models get 60/70/80% on the other categories. This is approaching saturation, which raises questions about the benchmark’s continued relevance and value to the community.\n\nMinor things:\n- Figure 2 should probably be a table\n- In line 238 in the caption of Table 2, $P_{hj}$ appears twice\n- I think that [2] is very relevant and should be included in the discussion or in Table 1, or both. \n\n[1] Ye, Jiayi, et al. \"Justice or prejudice? quantifying biases in llm-as-a-judge.\" arXiv preprint arXiv:2410.02736 (2024).\n\n[2] Skarlinski, Michael D., et al. \"Language agents achieve superhuman synthesis of scientific knowledge.\" arXiv preprint arXiv:2409.13740 (2024)."}, "questions": {"value": "1. How are biases and noise from using LLMs for both question generation and evaluation and metrics controlled? Is it possible that even that the data cleaning step, which uses LLMs, also exhibits biased/erroneous behavior?\n2. In open ended tasks, what happens when more than one answer is reasonable? Are there quantitative metrics that show this not to be an issue? \n3. Given the near-saturated results on Earth-Bronze/Silver, why are these benchmarks still worth further exploration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MoAnbLQqIJ", "forum": "jyYE06FL8G", "replyto": "jyYE06FL8G", "signatures": ["ICLR.cc/2026/Conference/Submission4658/Reviewer_XoSt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4658/Reviewer_XoSt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760830948741, "cdate": 1760830948741, "tmdate": 1762917496925, "mdate": 1762917496925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents EarthSE, a QA dataset benchmarking the earth science exploration capability of LLMs. These questions are sourced from 100k Earth Science papers, and are constructed into three subsets (Iron, Silver, and Gold). The authors decomposed each paper and then used GPT-4 to generate dialogues, followed by using human expert validation to ensure the dialogue quality. On several LLMs, the authors found that CoT guidance enhances the performances on challenging questions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This is the first science QA dataset for earth science that is derived from this magnitude of academic papers.  \n- This paper presents a principled approach to construct benchmark datasets from earth science publications."}, "weaknesses": {"value": "- Using one metric (SES) to assess advanced capabilities of scientific exploration (e.g., methodology induction, limitation analysis, and concept proposal) seems a bit too on reductionist side to me. Looking at the two components of the SES metric (retention rate and diversity), I became less convinced about the utility of this metric to the intended capabilities to evaluate.  \n- How are the Iron and the Silver subsets divided? The boundary seems a bit arbitrary to me.  \n- Similarly, the decision to make Iron & Silver in QA formats while making Gold in dialogue format also appears very arbitrary to me."}, "questions": {"value": "- How is the paper decomposed into the structured components? Is it also using GPT-4?  \n- The experts scored the dialogue, but did they score a subset of the QA questions in the benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "876Wm7w6Yo", "forum": "jyYE06FL8G", "replyto": "jyYE06FL8G", "signatures": ["ICLR.cc/2026/Conference/Submission4658/Reviewer_zDSD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4658/Reviewer_zDSD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589299437, "cdate": 1761589299437, "tmdate": 1762917496558, "mdate": 1762917496558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "EarthSE introduces a benchmark for evaluating LLMs’ Earth science exploration capabilities. It includes three datasets: (1) Earth-Iron, (2) Earth-Silver, and (3) Earth-Gold. These datasets target different levels of knowledge, from broad assessment (Earth-Iron) to higher difficulty requiring professional understanding (Earth-Silver), and finally open-ended scientific exploration through dialogue (Earth-Gold). The authors then analyze these capabilities in leading LLMs and find that they perform reasonably well on Earth-Iron but struggle with Earth-Silver and show low retention and diversity on Earth-Gold tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "EarthSE introduces a benchmark with various difficulty levels and broad coverage across subfields (114 disciplines and 11 tasks), as well as multi-turn, open-ended dialogues for scientific exploration and discovery. To build the QA datasets, the authors leverage around 100,000 research papers and categorize them by journal impact, citation count, and topical focus. They prioritize high-quality sources and exploit paper structure aligned with the scientific discovery process, followed by extensive data cleaning and expert validation. They also introduce additional metrics for assessing Earth-Gold, evaluate multiple LLMs, and report interesting, important insights."}, "weaknesses": {"value": "(1) Earth-Gold uses a fixed two-turn format. Real scientific exploration often needs longer iterative chains. It would be interesting to see the results with more turns.\n\n(2) The inference-time “initial CoT steps” taken from question construction seem to boost FIB accuracy, but may leak answer-related cues a model wouldn’t get at test time. It would be interesting to see the self-generated CoT results as well."}, "questions": {"value": "(1) In lines 455-460, it is mentioned that the initial CoT steps from the question construction step are provided during inference to see whether that helps with performance on more challenging question types. However, it is not clear to me why you didn’t instead ask the model to generate CoT reasoning while answering the question. Providing the CoT steps from the question construction step might leak knowledge about the answer that it shouldn’t, and that the model wouldn’t normally have access to."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Uf69B7Ccqr", "forum": "jyYE06FL8G", "replyto": "jyYE06FL8G", "signatures": ["ICLR.cc/2026/Conference/Submission4658/Reviewer_aJ8Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4658/Reviewer_aJ8Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052727272, "cdate": 1762052727272, "tmdate": 1762917495090, "mdate": 1762917495090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a suite of 3 new LLM benchmarks, two QA benchmarks around Earth science. \nThere are two QA style datasets and a multi-turn conversation benchmark. \n\nThe benchmark is based on 100k papers. Papers are categorized for field/sub-field.\nThe authors select subsets of size 100k, 10k, 1k by impact/popularity.\n\n\nThe QA pairs are constructed using LLMs. Each question/answer pair is built around a task category. The tasks include understanding but also reasoning and research tasks, such as experimental design and code generation.\nFor each paper tasks are selected to match the abstract.\nToo simple examples are filtered.\nExperts reviewed the remaining questions, keeping \n\n\nFor the multi-turn dialog they model papers as a transition from( existing methods and their limitations) to (novel method and its constraints). They extract these from the top 1k most cited papers. \nThen they auto-generate two turn dialogues of summary / proposal with the respective limitations.\nAgain, the examples are reviewed by experts.\n\nTo evaluate the multi-turn task, the paper proposes a novel metric based on ranking several samples from an LLM against the gold example along with a diversity metric.\n\nA slightly dated set of models is used in experiments showing significant differences between models, particularly for the multi-step task.\n\nThere are some issues with this paper that I am hopeful can be fixed."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Addressed a variety of scientific tasks, particularly beyond reasoning/extraction\n\nLarge corpus of papers used.\n\nPaper is well written and has very little fluff.\n\nHeadroom for evaluation\n\nExpert verification of at least parts of the data"}, "weaknesses": {"value": "GPT-4o was used in several steps of the generation process which might lead to bias. It doesn't look like a problem from the results but this should at least be explicitly acknowledged.\n\nThere is little detail on \"human experts determined their retention based on the question's value\". Can you tell a bit more about this process?\n\nEvaluation of the Free Response QA tasks could be extended beyond embedding similarity, e.g. by using a judge model or using rubrics.\n\nIt is unclear if diversity should be evaluated by temperature sampling. Without calibration, that parameter might do different things with different models, as evidenced by results in Table 5\n\nEvaluation of some more recent models with thinking enabled"}, "questions": {"value": "Is there an expert review of only the questions or also of the answers? How were the experts recruited / compensated? What is the acceptance / rejection rate? What is the agreement for these decisions?\n\n\nSmaller issues / improvements\n\nPlease check references for the models, e.g.\n* Not sure Islam & Moushi is the right reference for GPT-4o\n* Gemini is cited as \"Team et al\" ?\n\nThe model names need to be more explicit, e.g. Gemini 2.0 Flash or Claude 3.7 Sonnet\n\nIn Figure 3, \"a task that best suits your construction…\" did you want to say qualification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eKpJsq6CXb", "forum": "jyYE06FL8G", "replyto": "jyYE06FL8G", "signatures": ["ICLR.cc/2026/Conference/Submission4658/Reviewer_h6Tz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4658/Reviewer_h6Tz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169798696, "cdate": 1762169798696, "tmdate": 1762917494650, "mdate": 1762917494650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}