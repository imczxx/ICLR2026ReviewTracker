{"id": "AryG8SZxUa", "number": 6918, "cdate": 1758001797829, "mdate": 1763549007572, "content": {"title": "HumanoidVerse: A Versatile Humanoid for Vision-Language Guided Multi-Object Rearrangement", "abstract": "We introduce HumanoidVerse, a novel framework for vision-language guided humanoid control that enables a single physically simulated robot to perform long-horizon, multi-object rearrangement tasks across diverse scenes. Unlike prior methods that operate in fixed settings with single-object interactions, our approach supports consecutive manipulation of multiple objects, guided only by natural language instructions and egocentric camera RGB observations. HumanoidVerse is trained via a multi-stage curriculum using a dual-teacher distillation pipeline, enabling fluid transitions between sub-tasks without requiring environment resets. To support this, we construct a large-scale dataset comprising 350 multi-object tasks spanning four room layouts. Extensive experiments in the Isaac Gym simulator demonstrate that our method significantly outperforms prior state-of-the-art in both task success rate and spatial precision, and generalizes well to unseen environments and instructions. Our work represents a key step toward robust, general-purpose humanoid agents capable of executing complex, sequential tasks under real-world sensory constraints.", "tldr": "", "keywords": ["Humanoid", "vision-language", "object-rearrangement", "robot", "long-horizon"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f0e5b71f7c7e3730e233a03fc24ec8afeffe2ca.pdf", "supplementary_material": "/attachment/acf00a94d1e0c720c906c3f2b012a621df6bb1ef.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces HumanoidVerse, a framework for a simulated humanoid robot to perform \"long-horizon, multi-object rearrangement tasks.\" The core contribution is a 4-stage curriculum that distills two separate teacher policies into a single VLA model: one for distill during the initial rearrangement + releasing and the other for the second object rearrangement. First, two expert \"teacher\" policies are trained using reinforcement learning with privileged state information (e.g., object poses). The key idea is that the second teacher (Stage 3) is specifically trained to handle the diverse, non-standard starting poses left after the first teacher completes its task, whereas the first teacher (Stage 2) is trained to let go and step back after the first object rearrangement. Both teachers are then distilled using DAgger into a single student Vision-Language-Action (VLA) model that operates from only egocentric RGB video and natural language instructions. The authors demonstrate how their framework can effectively tackle sequential humanoid object rearrangement tasks on Isaac Gym."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel Problem Formulation**: The paper's main strength is tackling continuous, sequential, multi-object manipulation, which is often overlooked in existing works on VLA models for humanoid loco-manipulation. This is a clear and important step beyond the single-object, fixed-start tasks prevalent in prior work (e.g., the HumanVLA baseline).\n\n- **Effective Curriculum**: The 4-stage pipeline is a well-engineered solution. The ablation study (Table 3) clearly validates the authors' design, showing that explicitly training for the transition (Stage 2 for stepping back) and the second task's varied starts (Stage 3 for handling diverse initial configurations) are critical for effective training.\n\n- **New Benchmark for Humanoid Sequential Object Rearrangement**: The creation of a benchmark dataset with 350 sequential two-object tasks is a useful contribution for future research in this area."}, "weaknesses": {"value": "- **Limited Algorithmic Novelty and Scalability**: The paper's contribution is a highly specific training curriculum, not a new or generalizable algorithm. This curriculum is only demonstrated for a two-step ($N=2$) task, and all task involves sequentially rearranging two different objects. The paper does not provide a clear path for scaling this \"dual-teacher\" framework to $N>2$ tasks. This seems to imply a non-scalable $N$-teacher pipeline, which contradicts the \"long-horizon\" claim.\n\n- **Weak Baseline Comparison**: The primary baseline is HumanVLA, a model designed for single-object tasks. As shown in Table 2, it achieves a 0.000% success rate on the sequential task. This is expected and only confirms the new task is harder; it does not validate that the proposed dual-teacher curriculum is a better method for sequential tasks than other plausible baselines (e.g., end-to-end RL and a hierarchical RL, or subsequently their distilled policies).\n\n- **Simulation-Only**: All results are in simulation. The paper does not discuss the significant sim-to-real gap for a complex policy that must simultaneously handle locomotion, manipulation, and perception from RGB data.\n\n- **No Failure Modes**: It would be nice to provide analyses on failures modes of both HumanVLA and HumanoidVerse to justify some design choices or mention potential rooms for improvements."}, "questions": {"value": "- **Scalability**: How do you propose to scale this framework beyond $N=2$? Does your \"dual-teacher\" approach not become an \"N-teacher\" model, and if so, do you consider that a scalable solution for \"long-horizon\" tasks?\n\n- **Baselines**: Why were more relevant baselines for sequential tasks, such as a monolithic end-to-end teacher or a standard hierarchical RL approach, not included in the comparison?\n\n- **Failure Modes**: What are the common failure modes in simulated tasks? Were there any failures around the transitions between the first and second tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ICEIuKUM3e", "forum": "AryG8SZxUa", "replyto": "AryG8SZxUa", "signatures": ["ICLR.cc/2026/Conference/Submission6918/Reviewer_MbxK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6918/Reviewer_MbxK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889359630, "cdate": 1761889359630, "tmdate": 1762919155670, "mdate": 1762919155670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "HumanoidVerse presents a method that trains VLA models to control simulated humanoid robots to perform multi-object rearrangement. It consists of a multi-stage training process to rearrange the first object, step away from the first object, and rearrange the second object. The authors distill this into a VLA using DAgger, such that it operates using only image and language input (no privileged information) at test-time."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The success rate on multi-object sequences increases dramatically over the HumanVLA model the authors continue training from.\n- The method appropriately drops privileged conditioning, representing a plausible input-output setup for real-world deployment.\n- Distilling privileged teachers into a VLA with general world knowledge is an approach that can introduce additional robustness and sim data to VLAs and is not limited to teleoperated data collection.\n- The paper is clearly written and easy to follow."}, "weaknesses": {"value": "- The biggest drawback of the paper is the limited scope of the task, which conflicts with the central claim of the method being \"a key step toward robust, general-purpose humanoid agents.\" The reinforced rewards are very hand-designed with rule-based triggers and remedies to prevent observed, undesirable behavior. This is directionally opposite of scalable learning.\n- For the above reason, the improvements are likely isolated to the multi-stage object rearrangement setting.\n- There isn't discussion about inference latency, which is a major drawback of using VLA for locomotion tasks and somewhat hidden by the sim-only deployment."}, "questions": {"value": "- How is the transfer to other simulators and environments? It's interesting to know how much generalization the VLM pretraining affords versus just overfitting to the IsaacGym environments.\n- Are there recovery behaviors if the robot falls during a rollout?\n- Does the VLA exhibit strong text adherence and vision adherence individually? Does it just pay attention to one of the conditioning signals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JAhXrCAut7", "forum": "AryG8SZxUa", "replyto": "AryG8SZxUa", "signatures": ["ICLR.cc/2026/Conference/Submission6918/Reviewer_YEVG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6918/Reviewer_YEVG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951130837, "cdate": 1761951130837, "tmdate": 1762919155299, "mdate": 1762919155299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their insightful and constructive feedback. We are encouraged that the reviewers recognize the novelty and significance of our work, HumanoidVerse, in enabling a humanoid robot to perform continuous, sequential multi-object rearrangement tasks—a problem largely overlooked in prior work focused on single-object or fixed-start scenarios. Key strengths highlighted by the reviewers include:  \n\n- **Novel problem formulation:** Our framework addresses the critical limitation of existing humanoid VLA models by enabling reliable two-object sequential rearrangement without environment resets, with explicit handling of release, retreat, and diverse object start configurations.  \n- **Effective multi-stage curriculum:** The four-stage teacher–student training pipeline, including dual-teacher distillation, is carefully designed to tackle long-horizon, sequential manipulation. Ablation studies validate that each stage contributes significantly to performance, especially on second-object manipulation.  \n- **New benchmark dataset:** We introduce a diverse dataset of 350 tasks and scenes across four room types, covering 79 objects (25 movable), providing a valuable resource for future research in humanoid sequential rearrangement and embodied learning.  \n- **Strong empirical performance:** Experiments demonstrate significant improvements over the baseline, with higher task success rates and more accurate object placement, particularly on challenging second-object tasks.  \n- **Generalizable and robust student VLA:** By distilling privileged teacher policies into a student model that uses only egocentric RGB images and natural language instructions, we achieve good performance while dropping privileged conditioning, paving the way for real-world applicability.  \n\nIn response to reviewer comments, we have made several clarifications and updates in the paper:  \n\n- Added quantitative comparisons between end-to-end RL, single-teacher, and dual-teacher methods (Section 4.2, Table 2), highlighting the effectiveness of the dual-teacher design. \n- Included additional ablation studies showing the critical role of vision, language, and proprioception (Section 4.4, Table 5). \n- Added detailed descriptions of robot action and proprioceptive state representations in Appendix A.3, Table 9, 10, 11, 12, 13.       \n\nThese updates aim to address reviewer concerns while reinforcing the robustness, scalability, and originality of our approach, demonstrating its potential as a general-purpose framework for humanoid multi-object manipulation."}}, "id": "8izfnO0zuq", "forum": "AryG8SZxUa", "replyto": "AryG8SZxUa", "signatures": ["ICLR.cc/2026/Conference/Submission6918/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6918/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6918/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763549522788, "cdate": 1763549522788, "tmdate": 1763549522788, "mdate": 1763549522788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HumanoidVerse, a framework for training a simulated humanoid robot to perform two-object rearrangement tasks from egocentric vision and natural-language instructions. The method uses a four-stage teacher–student pipeline: first, reinforcement learning to learn grasp, move and place skills for single object rearrangement; second, a release and step-back stage guided by Teacher 1 to ensure stable placement and clear workspace; third, training to rearrange the second object from canonical initial states with Teacher 2; and fourth, a dual-teacher DAgger distillation stage in which the student switches between teachers according to the task phase. The authors also introduce a dataset with 350 task configurations across four room layouts and conduct extensive evaluation in Isaac Sim. Results show improved success accuracy and lower object-to-goal placement error compared to HumanVLA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written, clearly motivated, and explains the proposed approach in an intuitive and accessible manner.\n2. Introduces a novel curriculum for multi-object rearrangement with a single humanoid, consisting of three structured teacher training stages followed by dual-teacher student distillation, enabling reliable sequential two-object rearrangement.\n3. Presents a new dataset with 350 configurations across four room layouts, providing a valuable benchmark for future work in humanoid rearrangement and embodied learning.\n4. Demonstrates strong empirical improvements over HumanVLA, with higher task success accuracy and more precise object placement in goal locations.\n5. Includes ablation studies that highlight the contribution of each component in the training pipeline and justify the curriculum design to an extent."}, "weaknesses": {"value": "1. The pipeline appears tailored specifically for two-object rearrangement. While the curriculum learning approach for staged learning of the teachers for single-object rearrangement, release-and-step-back, and second-object rearrangement is novel, it is unclear how easily this structure generalizes to other long horizon settings. The ablations demonstrate that each stage contributes, but do not clarify why a more skill-based modular approach (e.g., pick, place & step-back, navigate) would not work or scale. Moreover, the switching strategy between teachers is hand-crafted, and there is no sensitivity analysis or investigation of potential failure cases for this designed approach.\n\n2. Limited scalability is demonstrated. Although the title suggests multi-object rearrangement, experiments only cover a two-object case. There is no discussion or demonstration of scaling to more objects (e.g., 3, 5, 10) or to more complex multi-room environments, so the scalability of the approach is unclear.\n\n3. Limited separation between training and evaluation configurations. The 350 two-object setups are split into 700 single-object cases for training the teachers, meaning the student essentially trains in the same configurations where the teachers are supervised. The only major difference between teacher and student seems to be privileged information, and evaluations are conducted on the same scenarios rather than unseen configurations, which weakens claims on generalization.\n\n4. Limited Dataset Analysis: The dataset is not analyzed in terms of scene diversity or complexity. It is unclear how many other objects or receptacles are present in each scene, how cluttered or constrained the workspace is, or how the spatial arrangements vary across configurations. There is no quantitave breakdown of object categories, receptacle types, distractor objects, or proximity constraints that may affect rearrangement difficulty. Without understanding scene distribution and difficulty profiles, it is difficult to assess how challenging or diverse the benchmark is, and whether it meaningfully stresses humanoid rearrangement capabilities.\n\n5. Reproducibility is incomplete. Code and dataset are not provided, and only high-level hyperparameters (e.g., number of epochs and reward structure) are disclosed. Details such as optimization settings, network architectures, dataset generation scripts, rollout procedures, and training infrastructure are omitted, making reproduction challenging.\n\n6. Generalization is limited and under-studied. The method is not tested beyond the training distribution on IsaacSim, with no evaluation on new layouts, unseen objects or different simulators. There is also no discussion of the possibility or challenges of real-world deployment.\n\n7. Evaluation metrics are narrow. In addition to success and placement distance, metrics such as collision frequency, disturbance to already-placed objects, interaction safety, and motion smoothness would offer a more complete view of humanoid performance in rearrangement settings.\n\n8. Minor: Variance, confidence intervals, or multiple-seed results are not reported, making it difficult to assess statistical robustness."}, "questions": {"value": "1. How do you envision extending this approach beyond two-object rearrangement? Do you expect the current curriculum stages to scale to three or more objects? If not, what modifications would be required?\n\n2. Have you considered benchmarking against a modular skill-based pipeline (e.g., separate pick, place & step-back and navigate policies)? Such a baseline would help clarify whether curriculum learning provides advantages over compositional skills.\n\n3. Current approach uses two teachers, did you consider using a single teacher? If yes, what are the failure cases with single teacher and how dual teacher approach is better?\n\n4. Since the student is evaluated on the same underlying scenarios used for teacher training, how do you ensure that the student does not overfit to the training layouts and object placements? Are there results on held-out task configurations or unseen object and room layouts?\n\n5. Can you provide statistics or analysis of dataset diversity: number and type of distractor objects, receptacle categories, clutter level, and spatial variation? How varied are object pairings and placements overall?\n\n6. Do you foresee challenges in scaling the dataset or the approach to more objects or multi-room scenes? Are there preliminary experiments or insights on scaling behavior?\n\n7. Have you explored domain randomization, perception noise, or other techniques to prepare for real-world deployment? What are the main challenges anticipated for sim-to-real transfer?\n\n8. When do you plan to release the code, dataset and model checkpoints? Could you also share additional implementation details such as optimizer settings, architectures, and training infrastructure?\n\n9. Did you measure safety or interaction-related metrics such as collision frequency, object disturbance, or motion smoothness? If not, would you consider including such evaluations?\n\n10. Could you please report multiple seeds, variance, or confidence intervals to strengthen statistical reliability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NA67zi02Y8", "forum": "AryG8SZxUa", "replyto": "AryG8SZxUa", "signatures": ["ICLR.cc/2026/Conference/Submission6918/Reviewer_Q2fY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6918/Reviewer_Q2fY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963673611, "cdate": 1761963673611, "tmdate": 1762919154810, "mdate": 1762919154810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HumanoidVerse, a framework for vision-language guided humanoid control that enables physically simulated robots to perform multi-object rearrangement tasks. The approach uses a multi-stage curriculum learning pipeline with dual-teacher distillation, where teacher policies trained with privileged state information are distilled into a student VLA model that inputs egocentric RGB images and natural language instructions. A dataset of 350 tasks across four room layouts is constructed, each involving sequential manipulation of two objects. Experiments demonstrate significant improvements over the HumanVLA baseline, particularly on second-object manipulation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses the limitation in existing humanoid manipulation work of unable to perform sequential, multi-object tasks without environment resets.\n- The multi-stage curriculum learning pipeline is well-motivated for the multi-stage rearrangement task, with stage 2 specifically addressing object release and retreat behaviors, and stage 3 handling diverse initial configurations. The ablation study validates the importance of each component.\n- The results show significant gains over HumanVLA, particularly on Success 2 metrics, demonstrating the effectiveness of the proposed multi-stage training approach."}, "weaknesses": {"value": "- Despite claiming to address \"multi-object rearrangement\", the system only demonstrate two object rearrangement. No evaluation is provided on how the current distilled model would perform on 3+ object scenarios.\n- There lacks analysis of when and why the teacher and student policies fail. What are some common failure modes?\n- Figure 5 in appendix shows many egocentric views are heavily occluded, especially when holding large objects. How does the policy determine where to place objects when visual information is limited? Is the model potentially overfitting to proprioceptive signals rather than learning robust vision-based reasoning?\n- There lacks discussion of how this could transfer to real humanoid robots. What are the main bottlenecks? (sim2real gap, stable whole-body control, tracking accuracy etc.)"}, "questions": {"value": "- Can the approach directly extend to 3+ objects, or does it require training additional teacher models for each subsequent object?\n- What is the representation of robot actions and proprioceptive state (dimensionality, joint/cartesian, coordinate frames, absolute/relative)?\n- In Supplementary Figure 5, many views are occluded. Can you provide ablation studies showing performance with/without proprioceptive information to clarify the role of vision vs. proprioception?\n- How sensitive is the dual-teacher switching mechanism (Algorithm 3) to the hand-tuned thresholds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1wdbkd69yN", "forum": "AryG8SZxUa", "replyto": "AryG8SZxUa", "signatures": ["ICLR.cc/2026/Conference/Submission6918/Reviewer_HCH9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6918/Reviewer_HCH9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144257002, "cdate": 1762144257002, "tmdate": 1762919154272, "mdate": 1762919154272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}