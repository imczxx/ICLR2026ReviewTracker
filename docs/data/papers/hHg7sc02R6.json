{"id": "hHg7sc02R6", "number": 10879, "cdate": 1758183998760, "mdate": 1759897622892, "content": {"title": "Rethinking Transformer Inputs for Time-Series via Neural Temporal Embedding", "abstract": "Transformer-based models, originally introduced in the field of natural language processing (NLP), have recently demonstrated strong performance in time-series forecasting. Due to the order-agnostic nature of the attention mechanism, these models have relied on positional encoding (PE) to capture temporal information. However, recent studies have reported that simple linear models can outperform complex Transformer architectures, and other works have also shown that modifying the Transformer input design can improve performance.\nMotivated by this issue, we propose Neural Temporal Embedding (NTE), an embedding mechanism that effectively internalizes temporal dependencies without relying on either value embedding or positional encoding. NTE leverages simple neural modules such as Conv1D and LSTM to independently process each variable’s time-series and directly learn temporal patterns. As a result, it removes the need for linear projection for value embedding and positional encoding in the input stage, thereby enabling the model to simultaneously achieve architectural flexibility and competitive performance.\nExperimental results on standard benchmarks including ETT, ECL, and Weather show that the proposed NTE-based models match or outperform state-of-the-art Transformer variants, particularly maintaining stable accuracy in long-horizon forecasting. These empirical findings show that Transformer-based models for time-series forecasting can achieve performance improvements through simple input enhancements without complex architectural modifications, thereby suggesting new possibilities for simpler and more generalizable input architectures.", "tldr": "We propose Neural Temporal Embedding (NTE), a simple input mechanism that removes value embedding and positional encoding, showing that Transformers for time series can improve performance through simple input enhancements.", "keywords": ["Time Series Forecasting", "Positional Encoding Elimination", "Neural Temporal Embedding(NTE)"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b25201943c301965a61b1e777c527558e182647c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Neural Temporal Embedding (NTE), an embedding mechanism that effectively internalizes temporal dependencies without relying on either value embedding or positional encoding. The authors claim that a learnable NTE layer (using FC, Conv1D, LSTM, etc.) can process each variable’s time series and directly learn temporal patterns. Experimental results show that NTE-based models match or outperform state-of-the-art Transformer variants, particularly maintaining stable accuracy in long-horizon forecasting."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The motivation of the paper is valuable, which rethinks the input stack of time-series Transformers and presents NTE as a unified, learnable temporal layer that can replace value embedding and explicit positional information.\n2. The paper includes ablations over PE variants, various NTE module types (FC, LSTM, Conv1D, Dilated, Bi-DilatedConv1D), bidirectional dilated Conv1D structures, and analyses of representation similarity (CKA) and entropy."}, "weaknesses": {"value": "1. While the motivation for the proposed method is well-founded, the experimental results reveal notable shortcomings. Specifically, Table 1 shows that introducing NTE leads to significant performance degradation for the Vanilla Transformer on certain datasets, such as ETTh1 and ECL. This raises concerns about the robustness of NTE when combined with standard Transformer architectures and suggests that its benefits may be limited to specific backbone designs. \n\n2. The paper does not provide sufficient theoretical grounding to explain why using modules like Conv1D or LSTM within NTE leads to better results compared to the original value embedding. While the empirical results support the effectiveness of these modules, a theoretical analysis of how these architectures capture temporal dependencies more effectively would strengthen the contribution and improve the general interpretability of NTE design.\n\n3. The ablation studies, while extensive, could be further expanded to explore the impact of kernel size in Conv1D-based NTE modules. The paper primarily reports results with fixed kernel sizes (e.g., 3 or 5). However, it is unclear whether these choices are optimal for capturing temporal dependencies in time series data, which often vary significantly in terms of patterns, seasonality, and granularity."}, "questions": {"value": "1. Could the authors clarify what \"Future-Dilated\" means in Figure 3 and how the future embedding is constructed?\n2. RoPE is a commonly used positional embedding method in Transformer-based models, particularly for tasks involving sequential data. However, it is not included in the experiments for comparison. Could the authors provide insights into how NTE compares to RoPE in terms of performance and effectiveness for time series forecasting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vsjyJCe5Rg", "forum": "hHg7sc02R6", "replyto": "hHg7sc02R6", "signatures": ["ICLR.cc/2026/Conference/Submission10879/Reviewer_LL6R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10879/Reviewer_LL6R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570129369, "cdate": 1761570129369, "tmdate": 1762922090862, "mdate": 1762922090862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Author Response to Reviewer Comments"}, "comment": {"value": "We sincerely thank all reviewers for the thoughtful and constructive feedback. We fully acknowledge many of the concerns raised, and would first like to clarify an important point that our original presentation may have caused ambiguity.\n\nOur proposed NTE is not intended to claim that a “simple yet more powerful module” can replace existing input designs. Rather, the core motivation of this work is that, despite the growing number of time-series Transformer models that achieve strong performance without explicit positional encodings, the standard Transformer input pipeline of “Linear Embedding + Positional Encoding” is still widely adopted as a default assumption.\n\nOur goal is therefore not to argue for the superiority of any particular neural component, but to encourage a reconsideration of the entire design paradigm of input embedding in time-series Transformers.\n\nWe recognize that our manuscript may have unintentionally suggested that “a simple NTE is a better alternative,” and we appreciate the reviewers’ comments for highlighting this ambiguity. Our experimental observations, however, reflect a more nuanced picture.\n\n- Simple NTE variatns (FC, LSTM) perform comparably to LPE, while providing clear efficiency advantages in parameters and FLOPs.\n\n- More structured NTE variants (Conv1D, Dilated Conv, etc.) show improvements on certain datasets or backbones, but the gains are not universally consistent.\n\n- In some settings, removing the linear embedding (LE) even improves performance.\n\nThese results are not meant to elevate a single architecture, but rather to demonstrate that the input representation stage—often considered as fixed—actually admits a much broader design space, and that alternative choices can meaningfully affect model behavior.\n\nWe also acknowledge that the efficiency comparison with LPE was not sufficiently emphasized in the main text, as only a subset of datasets was reported in the appendix. We will revise the manuscript to more clearly highlight that simple NTE can reduce parameter count and FLOPs by over 50% compared to LPE, without sacrificing accuracy.\n\nFinally, we are actively conducting additional experiments requested by the reviewers (e.g., RoPE comparisons, Conv kernel sweeps, expanded efficiency evaluations), and we will share the results during the discussion period.\n\nOnce again, we greatly appreciate the reviewers’ insights and hope that this work contributes to further discussion on the often-overlooked design space of input representations for time-series Transformers."}}, "id": "6KyrG3pMsa", "forum": "hHg7sc02R6", "replyto": "hHg7sc02R6", "signatures": ["ICLR.cc/2026/Conference/Submission10879/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10879/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10879/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763712505860, "cdate": 1763712505860, "tmdate": 1763712505860, "mdate": 1763712505860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel technique for the input embedding / transformation for time series foundation models (TSFM). In particular, they propose to combine the position embedding with the input embedding using different neural networks, which is a timely and interesting research area."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem that the authors work on is timely and a critical problem for any TSFM. So far, the default mode has been to simply embed the inputs either directly with a linear layer, or after applying a patching technique. The authors unify these two aspects with their proposed Neural Temporal Embedding, which is a simple neural network. In my view, this would be a novel aspect."}, "weaknesses": {"value": "Despite the novelty, the idea and the paper has several critical flaws:\n\nFirst, it is unclear until almost the very end of the paper on page 7, what the NTE is really doing. Up until this point the authors only mention that the NTE can be a 1D convolution, a LSTM, a fully connected network and several others, but they don't provide any concrete examples. Then, even though the authors provide this simple description of the two Conv1D layers, it is still unclear what the precise architecture of the NTE in Table 1 is. Is it the two conv layers, or is it something else? Only from the text, the reader can infer that the results in Table 1 stems from the two conv 1D layers. However, the authors state that \"the sequential bias introduced by NTE is insufficient to compensate for the order-agnostic nature of the standard Transformer\". However, they do not elaborate whether any other structure of the NTE would improve that. There is some ablation study in Table 2 that, which the reader can appreciate, but then this table is also confusing. Firstly, because the standard in the literature is to use the learnable PE (which should also be the reference point in Table 1), and then with this more realistic comparison point, none of the NTE architecture really seem to make a significant difference. Finally, since the paper puts so much emphasis on the NTE architecture as a novelty, detailed investigations of it are absent. For example what is the associated computational cost with the different variants, what are the features that those architectures provide? Are there specific cases in which to use one architecture of the NTE over the other?\n\nOverall, the study spends a lot of time explaining and reiterating on the basics for the NTE, but doesn't dive in to the essence of it."}, "questions": {"value": "See the text above, there are several open questions which should be addressed:\\\nWhy to choose the NTE over the Learnable PE if performance is not better?\\\nWhat is the associated computational cost with the different variants?\\\nWhat about patching techniques paired with the NTE?\\\nWouldn't a recurrent network break the parallelism and thus limit the efficiency of an attention backbone?\\\nWhat are the features that those architectures provide?\\\nAre there specific cases in which to use one architecture of the NTE over the other?\nEtc.\\"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "83dgxpoV0X", "forum": "hHg7sc02R6", "replyto": "hHg7sc02R6", "signatures": ["ICLR.cc/2026/Conference/Submission10879/Reviewer_ZWHb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10879/Reviewer_ZWHb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906637884, "cdate": 1761906637884, "tmdate": 1762922090201, "mdate": 1762922090201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits a fundamental but often overlooked design aspect of time-series Transformers — the input embedding stage. The authors propose **Neural Temporal Embedding (NTE)**, a simple yet effective alternative to conventional **value embedding + positional encoding** pipelines. NTE employs lightweight neural modules such as **Conv1D** and **LSTM** to process each variable’s time series individually and encode temporal dependencies directly, without relying on positional encodings.  The key claim is that much of the Transformer’s inefficiency in time-series forecasting stems not from the attention mechanism itself, but from **suboptimal input representation**.  Experiments on standard benchmarks (ETT, ECL, Weather) demonstrate that NTE-based Transformers achieve comparable or better performance than specialized architectures, such as Autoformer, PatchTST, and iTransformer, particularly on long-horizon forecasting tasks.  The contribution is conceptually simple yet cleanly executed, offering an interesting perspective that suggests **input design** improvements can yield non-trivial gains without requiring architectural overhauls."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear motivation and conceptual simplicity.**  \n   The paper makes a strong case that input embedding deserves more attention. The removal of positional encoding is a bold but well-motivated design choice.\n\n2. **Empirical clarity.**  \n   The experimental setup is well organized, with fair comparisons to established baselines. The results convincingly show that input modifications alone can lead to performance gains.\n\n3. **Strong writing and accessibility.**  \n   The narrative is concise and approachable — the authors explain their ideas clearly without unnecessary jargon.\n\n4. **Relevance to the ICLR community.**  \n   The study fits the current trend of revisiting Transformer assumptions for efficiency and simplicity. It may inspire further work on lightweight input layers.\n\n5. **Practical insights.**  \n   The findings suggest that some of the architectural “complexity arms race” in time-series forecasting might be avoidable, which is refreshing."}, "weaknesses": {"value": "1. **Limited novelty at the algorithmic level.**  \n   NTE combines well-known neural components (Conv1D and LSTM) in a new configuration. While the empirical insight is valuable, the conceptual innovation is modest.\n\n2. **Insufficient theoretical explanation.**  \n   The paper would benefit from a deeper discussion of *why* NTE works — e.g., whether the learned temporal encoding approximates sinusoidal patterns or adapts to variable frequencies.\n\n3. **Lack of broader baselines.**  \n   The study compares mainly against mainstream Transformer variants. Including recent input-focused or embedding-free models (e.g., TSMixer, FreTS) would help strengthen the claim of generality.\n\n4. **Ablation analysis could go further.**  \n   It would be useful to isolate the impact of each NTE component (Conv1D vs LSTM) and analyze whether NTE benefits small-data or irregularly sampled settings differently.\n\n5. **Unclear scalability implications.**  \n   Since NTE introduces extra pre-processing per variable, a brief discussion of runtime or memory overhead would make the work more complete."}, "questions": {"value": "1. How sensitive is the model to the choice of neural encoder (e.g., Conv1D vs GRU)?  \n2. Does NTE preserve translation invariance in temporal shifts, or does the neural encoder introduce biases?  \n3. Could the authors test whether NTE generalizes to irregular or non-uniform sampling rates?  \n4. Are there cases where positional encodings outperform NTE (e.g., highly periodic signals)?  \n5. How does the per-variable processing scale when the number of dimensions exceeds 100?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iZzWK5Dwk7", "forum": "hHg7sc02R6", "replyto": "hHg7sc02R6", "signatures": ["ICLR.cc/2026/Conference/Submission10879/Reviewer_G8MA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10879/Reviewer_G8MA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981564103, "cdate": 1761981564103, "tmdate": 1762922089775, "mdate": 1762922089775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a mechanism, namely Neural Temporal Embedding (NTE), to replace the Positional Encoding (PE) in transformer-based models. Neural networks like FC, LSTM, and Conv1D are used to build the NTE module."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed mechanism achieves improvement when applied to backbones like iTransformer and PatchTST, especially on ETTh1 and ECL.\n- The proposed NTE, as a plug-and-play module, can easily work with different backbones without changing the downstream structure.\n- Multiple experiments are conducted to verify the effectiveness and efficiency of the proposed mechanism."}, "weaknesses": {"value": "- Structure issues:\n    - Although many experiments are conducted, only a few of the results are displayed in the main body of the paper.\n    - In Sec.4, the paragraph `Bi-directional Dilated Convolutional Embedding' seems to have little relevance to the experiment. (Should it be in Sec.3 or Appendix?)\n- Motivation issues:\n    - Although the NTE is claims to simplify the input, the results in Table 6, 7, 13, and 14, indicate that the NTE may add to the computation burden.\n- Experiment issues:\n    - The NTE shows poor performance on Exchange, raising concerns that NTE may not be competent for long-horizon forecasting.\n    - It is recommended to conduct multiple experiments and calculate the standard deviation to verify the stability.\n- Others:\n    - As mentioned in the paper, the NTE can only be conducted on time-series forecasting tasks."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5bFA4gHqtX", "forum": "hHg7sc02R6", "replyto": "hHg7sc02R6", "signatures": ["ICLR.cc/2026/Conference/Submission10879/Reviewer_6i5G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10879/Reviewer_6i5G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762236124593, "cdate": 1762236124593, "tmdate": 1762922089378, "mdate": 1762922089378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}