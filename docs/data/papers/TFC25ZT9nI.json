{"id": "TFC25ZT9nI", "number": 19232, "cdate": 1758294626115, "mdate": 1759897051145, "content": {"title": "LiteGuard: Efficient Task-Agnostic Model Fingerprinting with Enhanced Generalization", "abstract": "Task-agnostic model fingerprinting has recently gained increasing attention due to its ability to provide a universal framework applicable across diverse model architectures and tasks. The current state-of-the-art method, MetaV, ensures generalization by jointly training a set of fingerprints and a neural-network-based global verifier using two large and diverse model sets: one composed of pirated models (i.e., the protected model and its variants) and the other comprising independently-trained models. However, publicly available models are scarce in many real-world domains, and constructing such model sets requires intensive training efforts and massive computational resources, posing a significant barrier to practical deployment. Reducing the number of models can alleviate the overhead, but increases the risk of overfitting, a problem further exacerbated by MetaV's entangled design, in which all fingerprints and the global verifier are jointly trained. This overfitting issue leads to compromised generalization capability to verify unseen models.\n\nIn this paper, we propose LiteGuard, an efficient task-agnostic fingerprinting framework that attains enhanced generalization while significantly lowering computational cost. Specifically, LiteGuard introduces two key innovations: (i) a checkpoint-based model set augmentation strategy that enriches model diversity by leveraging intermediate model snapshots captured during the training of each pirated and independently-trained model—thereby alleviating the need to train a large number of pirated and independently-trained models, and (ii) a local verifier architecture that pairs each fingerprint with a lightweight local verifier, thereby reducing parameter entanglement and mitigating overfitting. Extensive experiments across five representative tasks show that LiteGuard consistently outperforms MetaV in both generalization performance and computational efficiency.", "tldr": "", "keywords": ["neural network fingerprinting", "ownership verification"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b70050f4a7fa315deba5cd7922fafb688aaaf02a.pdf", "supplementary_material": "/attachment/ff284862ab815d1dcad19be1ac39bc23518a13cf.zip"}, "replies": [{"content": {"summary": {"value": "Liteguard targets the fingerprinting of DNNs (i.e., not LLMs) and places a focus on lightweight execution.\n\nThe idea of aggregating fingerprints and paired verifiers (the second component of the approach) is interesting, as verifiers are generally a single large model trained on all fingerprints."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* interesting verifier aggregating individual classifiers"}, "weaknesses": {"value": "* no guarantees whatsoever\n* no proper related work\n* missing discussions (open garden models, execution timings)\n\nLiteguard is not backed by any formal guarantee, the whole performance assessment is experiment-based. And as the experimental section relies on modest scenarios in practice (cifar 100...), it questions the performance to be expected in modern applications. In particular, it is clear that performance depends on the models present and compared in both sets; experiments only go up to 5 or 30 models and variants per set, which is a tiny number today (see e.g., \"Fingerprinting Classiﬁers with Benign Inputs\",2023 where 1046 variants are discriminated, for larger DNNs), facing the proliferation of architectures, proposals, and potential variants. This precludes a clear forecast of practical performances and generalization to other included models.\n\nLacks consideration of unseen models: this is not very clear in the paper, but it seems that all experiments consider a \"closed garden\" setup, where only models at hand are considered (ie models that have been part of the training for discrimination). In practice, and as in multiple papers in the SoA, the question is what happens to accuracy where there is an unseen model in the blackbox under scrutiny. What are the generalization capabilities of Liteguard in open garden/sets scenarios? This must be discussed.\n\nAs one of the major motivations of the approach is a lower computational cost than MetaV for instance, and provided that both are compared in the experiments, I find it unfortunate not to have a plot or table measuring the gain (faster timings). To judge, we are left with a brief discussion on parameter complexity, that is interesting, but insufficient to support the claim in my opinion.\n\nA proper related work section is missing, some papers are only briefly mentioned in the introduction. For instance, multiple papers on fingerprinting DNNs from recent conferences (e.g., AAAI 2025) are missing from the references.\n\nIn summary, I think this is an interesting lead, but that the experimental part in its current form is not enough to convey evidence and support the claims made at the beginning of the paper. The positioning w.r.t. the proliferation of prior work is also a problem."}, "questions": {"value": "* how is is Litegard performing in an open garden setup?\n\n* what about execution timings w.r.t. MetaV?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6XQIFZ6QyJ", "forum": "TFC25ZT9nI", "replyto": "TFC25ZT9nI", "signatures": ["ICLR.cc/2026/Conference/Submission19232/Reviewer_GCH5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19232/Reviewer_GCH5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835293492, "cdate": 1761835293492, "tmdate": 1762931214385, "mdate": 1762931214385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LiteGuard, an efficient and task-agnostic model fingerprinting framework that enhances the generalization and computational efficiency of ownership verification compared to the state-of-the-art method MetaV. The authors identify that MetaV’s reliance on large, diverse training model sets and its globally entangled fingerprint–verifier architecture cause severe overfitting and high training cost. To address these issues, LiteGuard introduces two key innovations: (1) a checkpoint-based model set augmentation strategy that leverages intermediate training snapshots to enrich model diversity without additional training effort, and (2) a local verifier architecture that pairs each fingerprint with a lightweight, independent verifier to reduce parameter entanglement. Extensive experiments across five representative tasks (covering CNNs, MLPs, AEs, GNNs, and RNNs) demonstrate that LiteGuard achieves significantly higher AUC scores, superior robustness against six ownership-obfuscation techniques, and comparable verification accuracy, highlighting its practicality for efficient model IP protection."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Elegant and Practical Design Innovations.** The checkpoint-based augmentation cleverly utilizes existing training artifacts to boost model diversity. The local verifier design directly reduces overfitting and decouples parameter dependencies. Both are low-cost, widely applicable strategies that can be generalized beyond fingerprinting. \n\n2. **Strong Problem Motivation and Practical Relevance.** This paper addresses a real deployment bottleneck in model IP protection—MetaV’s dependence on large training model sets—and proposes a solution that is conceptually simple yet effective.\n\n3. **Strong Empirical Results and Robustness.** LiteGuard shows large AUC gains over MetaV. The robustness experiments under six obfuscation types are particularly convincing.\n\n4. **Excellent Writing and Clarity.** This paper is well-structured and clearly written."}, "weaknesses": {"value": "1. **Limited Theoretical Analysis.** The paper’s central claim—improved generalization via reduced entanglement—is only empirically validated. There is no formal generalization or bias-variance analysis to support the intuition.\n\n2. **Verifier Design.** The local verifier is a single linear layer; its capacity to handle high-dimensional nonlinear outputs is questionable. Runtime or query-efficiency trade-offs are not discussed."}, "questions": {"value": "1. Could the authors provide a theoretical justification (even high-level) for why local verifiers improve generalization?\n\n2. Would a small shared backbone with partially independent verifiers achieve better efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper’s focus is on ownership verification and IP protection, which supports ethical model usage and accountability. No privacy-violating or harmful applications are implied."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zMjbM61ZPJ", "forum": "TFC25ZT9nI", "replyto": "TFC25ZT9nI", "signatures": ["ICLR.cc/2026/Conference/Submission19232/Reviewer_CS8j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19232/Reviewer_CS8j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926060293, "cdate": 1761926060293, "tmdate": 1762931214004, "mdate": 1762931214004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed LITEGUARD, a Task-agnostic model fingerprinting model fingerprinting framework with enhanced generalization. It features two key innovations, (1) using intermediate model check-points from training to enrich the model set, and (2) using a local verifier architecture that all fingerprints are optimized independently. Empirical results are provided to show that the proposed method achieves a better performance as well as a lower computation cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well structured and easy to follow.\n\n2. The empirical results are strong and the experiment design is fair. It shows a large margin of improvement given by the proposed method compared to the baselines.\n\n3. Very rich experimental results. It considered both scenarios with and without obfuscation methods. \n\n4. Ablation studies are also well designed and convincing."}, "weaknesses": {"value": "1. The novelty of this paper is relatively weak. Independently optimizing different fingerprints is also used in other previous works. Although these works are not considered task-agnostic, it would be better to clarify how this work differs from them.\n\n2, Font size in Fig. 4 is too small to read."}, "questions": {"value": "1. In the experiments, is the number of models (including different model snapshots) the same between the proposed method and baselines? \n\n2. In the complexity discussion in section 4 \"LiteGuard: Decoupled Architecture\", shouldn't it have a factor N in the formula?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yThTmP0P9B", "forum": "TFC25ZT9nI", "replyto": "TFC25ZT9nI", "signatures": ["ICLR.cc/2026/Conference/Submission19232/Reviewer_588g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19232/Reviewer_588g"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981992578, "cdate": 1761981992578, "tmdate": 1762931213464, "mdate": 1762931213464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LiteGuard, an efficient and task-agnostic framework for model fingerprinting designed to enhance generalization and reduce computational overhead in ownership verification of deep neural networks. Building upon the limitations of prior work such as MetaV—which relies on extensive model collections and joint training of entangled fingerprints and verifiers—LiteGuard contributes two core innovations: a checkpoint-based model set augmentation strategy that increases model diversity by reusing intermediate training checkpoints, and a local verifier architecture that pairs each fingerprint with an independent, lightweight verifier to mitigate overfitting. Extensive experiments across five representative tasks—spanning classification, regression, generation, and graph modeling—demonstrate that LiteGuard achieves superior generalization and computational efficiency, while maintaining robustness against a wide range of ownership obfuscation attacks. This paper offers a well-motivated and technically grounded contribution that effectively addresses a genuine limitation of existing task-agnostic fingerprinting methods. While it represents a solid and practically meaningful advance, further analytical depth would strengthen its impact."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-identified practical bottleneck (generalization vs. efficiency trade-off).\n\nThe authors clearly articulate that the core limitation of existing task-agnostic fingerprinting, especially MetaV, lies in its dependence on large and diverse model collections to achieve generalization. This observation grounds the work in a genuine real-world constraint: publicly available models are scarce, and constructing them is resource-intensive. The authors convincingly motivate the need for a lightweight yet generalizable alternative, establishing a coherent link between practical constraints and methodological innovation. This framing situates LiteGuard as a realistic and deployable solution to a widely recognized challenge in model IP protection.\n\n2. Checkpoint-based model set augmentation strategy.\n\nThe checkpoint-based augmentation mechanism is one of the paper’s most compelling contributions. By systematically incorporating intermediate model snapshots captured during training, LiteGuard effectively transforms existing computational by-products into valuable training diversity (Sec. 3.2; L162–L215). The checkpoint selection scheme—starting from a mid-training epoch and uniformly sampling at fixed intervals—balances representational variety and redundancy. The ablation results in Figure 4 empirically confirm that this augmentation consistently improves AUCs across tasks, even when applied to MetaV, illustrating its general applicability. Conceptually, this design elegantly aligns with the paper’s goal of efficiency without sacrificing robustness, and it represents a practically impactful contribution to fingerprint generation under constrained resources.\n\n3. Modular local verifier architecture with analytical justification.\n\nLiteGuard’s second key innovation is its decoupled verifier structure, which addresses the overfitting and scalability issues inherent in MetaV’s global verifier design. Each fingerprint is paired with an independent, lightweight verifier trained only on its own outputs (Sec. 3.3; L213–L217), dramatically reducing parameter entanglement. The formal complexity analysis in Sec. 4 shows that this approach lowers parameter scaling from O(N(F+O⋅H)) to O(F+O), thereby bounding overfitting risk. This theoretical rationale is reinforced experimentally: Figure 5 shows that substituting LiteGuard’s verifier into MetaV significantly boosts performance, while removing it leads to a marked decline. The synergy of analytical reasoning and empirical validation gives this architectural refinement both conceptual soundness and demonstrable impact.\n\n4. Comprehensive and multi-domain empirical validation.\n\nThe experimental design is broad and methodologically rigorous. LiteGuard is evaluated across five tasks—classification, regression, graph prediction, tabular data generation, and time-series modeling—each involving distinct architectures and datasets (Sec. 5; Table 2). The results reveal consistent AUC improvements over both task-specific and task-agnostic baselines, with gains ranging from 9% to 35%. Moreover, Figure 3 demonstrates that LiteGuard achieves comparable performance to MetaV while using up to 80 % fewer trained models, confirming its superior efficiency. Table 3 further shows that LiteGuard maintains robustness under diverse ownership obfuscation techniques, outperforming MetaV across nearly all categories. The breadth and consistency of these results give the work strong empirical credibility."}, "weaknesses": {"value": "1. Limited theoretical understanding of generalization improvement.\n\nSection 4 provides a parameter-count argument suggesting that reduced entanglement mitigates overfitting, but it stops short of a quantitative or theoretical treatment. There is no analysis of how checkpoint diversity or modularity affects the variance or bias of fingerprint representations, nor are there experiments correlating parameter scale with test AUC. As a result, the claim of “enhanced generalization” remains empirically supported but theoretically underdeveloped, limiting the scientific depth of the contribution.\n\n2. Lack of theoretical justification for why checkpoint augmentation and local verifiers work.\n\nThe paper convincingly shows that both the checkpoint-based augmentation and the local verifier design improve performance, but it is unclear whether checkpoints truly capture orthogonal decision behaviors or merely introduce correlated noise, and whether independent verifiers learn complementary discriminative subspaces or simply reduce parameter coupling. \n\n3. Incomplete analysis of computational efficiency and broader implications.\n\nWhile the efficiency advantage is repeatedly emphasized, the paper evaluates it solely through AUC comparisons at different model counts (Fig. 3) rather than concrete computational metrics such as training time, GPU hours, or memory usage. This omission makes it difficult to quantify the real-world resource savings claimed in the abstract. Moreover, the conclusion section does not discuss potential security or ethical implications—such as false positives in ownership verification or adversarial adaptation to the local verifier design—leaving the practical boundaries of LiteGuard’s applicability somewhat underexplored."}, "questions": {"value": "1. Could the authors provide quantitative evidence of computational efficiency?\n\n2. Would it be possible to characterize the relationship between checkpoint diversity, parameter decoupling, and generalization empirically or theoretically?\n\n3. Could the authors complement Tables 2 and 3 with some statistical significance tests or confidence intervals to ensure the reported improvements are not due to stochastic variation?\n\n4. Would a comparative summary table outlining architectural dependencies, training assumptions, and verifier coupling help clarify the theoretical boundary of LiteGuard’s contributions relative to existing methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8NeFI2o63F", "forum": "TFC25ZT9nI", "replyto": "TFC25ZT9nI", "signatures": ["ICLR.cc/2026/Conference/Submission19232/Reviewer_QEjw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19232/Reviewer_QEjw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129940576, "cdate": 1762129940576, "tmdate": 1762931213040, "mdate": 1762931213040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}