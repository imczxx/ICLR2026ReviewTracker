{"id": "wec4qy2XIF", "number": 2026, "cdate": 1756977601918, "mdate": 1759898173383, "content": {"title": "Explainable LLM Unlearning through Reasoning", "abstract": "LLM unlearning is essential for mitigating safety, copyright, and privacy concerns in pre-trained Large Language Models (LLMs). Compared to preference alignment, it offers a more explicit way by removing undesirable knowledge characterized by specific unlearning datasets. \nIn previous works, Gradient Ascent (GA) and its variants have shown promise for implementing unlearning, yet their untargeted nature results in unintended degradation of general capabilities, incomplete removal of knowledge, and the generation of incoherent responses, among many others. We argue that these issues stem from the absence of explicit guidance on what and how models should unlearn.\nTo fill this gap, we introduce a novel unlearning target, *reasoning-based unlearning target*, which satisfies both the specified unlearning scope and the specified post-unlearning response. Building on this, we propose *Targeted Reasoning Unlearning* (TRU), which leverages reasoning-based unlearning target as guidance. We employ the target using a cross-entropy supervised loss combined with a GA-based loss, enabling the model to learn reasoning ability for precise knowledge removal while preserving unrelated abilities.\nWe evaluate TRU against strong baselines across multiple benchmarks and LLM backbones, and find that it achieves more reliable unlearning while preserving general capabilities. Moreover, TRU exhibits superior robustness under diverse attack scenarios, stemming from the reasoning ability learned through reasoning-based targets. Overall, our study establishes reasoning-augmented unlearning as a practical paradigm for reliable and explainable LLM unlearning.", "tldr": "", "keywords": ["LLM Unlearning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c7a7254794801c2eec25eb5911ea2cb8b71ba304.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "**From the methodological perspective,** this paper proposes Targeted Reasoning Unlearning (TRU), a framework that introduces reasoning-based unlearning targets to guide LLMs during the unlearning process.\nEach unlearning sample x_u is paired with a reasoning trace r_u (why not answer the question) and a refusal response y_u, automatically generated by API.\nTRU combines a cross-entropy target loss (to teach reasoning-based refusals) with a GA-based loss (to enforce true knowledge erasure), enabling precise and explainable unlearning behavior.\n\n**From the experimental perspective,** the authors evaluate TRU on multiple benchmarks such as WMDP, MUSE and TOFU , showing consistent improvements over GA-based baselines.\nTRU achieves stronger unlearning effectiveness, better retention of general capabilities, and more coherent outputs, demonstrating that incorporating reasoning signals leads to more reliable and interpretable unlearning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**1. Originality:** The paper introduces a well-motivated concept of in-scope unlearning. This represents a meaningful step beyond existing GA-based methods that treat unlearning as purely data-point suppression.\n\n**2. Quality and Clarity:** The paper clearly formulates the response problem in current unlearning methods—where models tend to produce incoherent or nonsensical outputs after forgetting—and effectively mitigates it through the reasoning-based unlearning target. \n\n**3. Empirical Validation:** The authors conduct comprehensive main experiments across multiple datasets and model architectures, demonstrating consistent improvements in both unlearning effectiveness and retention performance."}, "weaknesses": {"value": "**1. Limited evidence on the “in-scope” capability.**\n\nIt seems that the reasoning trace in this paper merely serves as a supervision signal for how to refuse the question. L_{\\text{target}} functions like an SFT objective, where the model only produces the corresponding responses r and y when the x in the triplet appears. Why can this approach ensure that when an example with the same scope appears, the model will also refuse? I don’t see the potential of this method in solving the in-scope problem. Regarding this concern, I think the example given in Box 2 of Section 3 is quite interesting. I would like to see how the unlearned model obtained by TRU behaves when facing the Spanish example. Similarly, for evaluating the performance of the unlearned model, I didn’t see how the existing metrics Rel, Rej, and Help are related to the in-scope issue.\n\n**2. Baseline selection and comparison fairness.**\n\nThe comparison omits refusal-based or alignment-style baselines (e.g., models trained on refusal corpora or safety-aligned datasets). Since the paper argues that manually constructing refusals is costly (in the paper's Line 94 \"For specified response, manually constructing coherent refusals is prohibitively costly, since unlearning tasks often involve large datasets and require consistent behavioral patterns across diverse queries.\").  But for past refusal methods, they just need a refusal corpus, which is easy to collect. For TRU, it needs to use the API to generate refusal per data, which seems much costly than collecting a general refusal corpus.\n\n**3. Fairness of evaluation using LLM.**\n\nThe paper uses LLM-as-a-judge for evaluating unlearning quality, but it is unclear whether this evaluation is reliable or unbiased. Especially as the prompt template shows in Figure 11, such a long request, not sure whether LLM could handle such a complex task and whether LLM as a judge is fair.\n\n**4. Insufficient utility and retention analysis.**\n\nAnother important thing is the general utility of the model after unlearning, not the performance on the unlearning benchmark's retain performance, but the general ability, for example, question answering ability on MMLU, or math performance on GSM8K."}, "questions": {"value": "The in-scope concept proposed in this paper is interesting and potentially impactful. However, as discussed in the Weaknesses section, I still have several questions and doubts regarding how the current method effectively addresses this problem. If the authors can clarify these concerns and provide stronger evidence or analysis in the rebuttal, I would be glad to reconsider and raise my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cuOTNJg6Dz", "forum": "wec4qy2XIF", "replyto": "wec4qy2XIF", "signatures": ["ICLR.cc/2026/Conference/Submission2026/Reviewer_TzSC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2026/Reviewer_TzSC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797176280, "cdate": 1761797176280, "tmdate": 1762915994073, "mdate": 1762915994073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Targeted Reasoning Unlearning (TRU), a new framework for controllable and explainable LLM unlearning.\nTraditional gradient ascent (GA)–based methods effectively remove undesired content but often cause degraded fluency, incoherent refusals, and loss of unrelated abilities. TRU addresses these issues by introducing reasoning-based unlearning targets, which specify both the unlearning scope and the post-unlearning response.\nThe proposed method combines:\n1. a cross-entropy loss for reasoning-based supervision, and\n2. a GA-based loss for knowledge erasure.\nExperiments on three benchmarks—WMDP, MUSE, and TOFU—show that TRU outperforms baselines (GradDiff, NPO, WGA, RMU, etc.) in both unlearning quality (UQ) and retention quality (RQ). Ablation and robustness analyses further show that reasoning traces are key to maintaining scope control and logical refusals under attacks (cross-lingual, jailbreak, relearning).\nOverall, TRU introduces a promising direction for explainable and controlled unlearning."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality – Proposes reasoning-guided unlearning, a novel perspective that goes beyond optimization tricks.\n\nTechnical soundness – The combination of supervised reasoning loss with GA-based unlearning is simple yet effective.\n\nComprehensive experiments – Evaluated on three benchmarks and multiple backbones (Llama-2/3, Zephyr).\n\nExplainability and robustness – Demonstrates interpretable refusals, cross-lingual generalization, and resilience to jailbreak/relearning.\n\nStrong ablation and analysis – Clear evidence that reasoning traces and dual losses are crucial."}, "weaknesses": {"value": "Limited human evaluation – The reliance on LLM-as-a-Judge may inherit biases; some human validation would strengthen the claims.\n\nComputational cost – The reasoning-target generation via Deepseek and extra supervision might be expensive for large-scale deployment.\n\nGenerality – While TRU is effective for safety/copyright removal, it’s unclear how well it generalizes to factual correction or bias unlearning.\n\nLimited interpretability metrics – The paper claims explainability, but lacks quantitative metrics for “reasoning clarity” or “explanation coherence.”\n\nMinor presentation issues – Some figures and mathematical definitions could be simplified."}, "questions": {"value": "How sensitive is TRU to the quality of the reasoning traces generated by Deepseek? Have you tried weaker reasoning models?\n\nCould TRU be extended to continual unlearning or online updates, where undesired knowledge evolves over time?\n\nHave you compared LLM-as-a-Judge with human-rated unlearning quality to calibrate the metric reliability?\n\nCan the reasoning target generation be self-improving, e.g., via iterative self-refinement?\n\nHow does TRU interact with alignment methods (e.g., DPO/RLHF)? Could it serve as a post-alignment correction stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fA7xrZay8B", "forum": "wec4qy2XIF", "replyto": "wec4qy2XIF", "signatures": ["ICLR.cc/2026/Conference/Submission2026/Reviewer_hNqG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2026/Reviewer_hNqG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929814544, "cdate": 1761929814544, "tmdate": 1762915993888, "mdate": 1762915993888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper demonstrates that standard LLM unlearning methods fails to unlearn within scope data (for example, unlearning information in English but not doing so in Spanish). Based on that, the paper defines in-scope unlearning as unlearning all rephrasings and variations of a particular data. In addition to that, they demonstrate that methods do not have any response control for in-scope data i.e. they reply with nonsensical outputs.\n\nTo mitigate this, the authors propose to first create an unlearning target by generating a reasoning trace followed by a refusal response using Deepseek-reasoner API. \\\nThen they introduce an unlearning loss which minimizes the cross-entropy loss on the above generated reasoning trace followed by the refusal.\nThe rationale here is that due to the reasoning trace, the unlearned model can understand in-scope data better.\\\nUnlearning is performed using this loss along with the gradient difference loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and easy to read.\\\nThe problem raised in the paper about in-scope unlearning failure is of significant importance for LLM unlearning.\\\nFinally, they perform comprehensive experiments with different unlearning benchmarks and wide range of unlearning methods."}, "weaknesses": {"value": "Please address the following major concerns I have: \n-  Kindly describe how you converted responses from a reasoning model (which should contain \\<think\\> tokens) to non-reasoning based models like Zephyr-7B-beta. Do you ignore the think tokens and concatenate reasoning trace with the refusal ? \n- If so, do you have any intuition as to why the reasoning is important ? As we see in the ablation study in Table 2, there seems to be a tradeoff between UQ and RQ with and without reasoning. Thus it seems that TRU does not effectively address the issue. We can definitely improve retention if we decrease unlearning. I am not sure if this can be attributed specifically to TRU. \n- I am skeptical of the result of RMU having 0 UQ in Table 1.  This is because RMU is known to preserve utility [1]. Can you please explain this discrepancy ? \n- The above could be a result of the introduced metric of evaluation. I strongly suggest the authors to introduce the standard metrics used in [1] for increasing reliability . Additionally, for generated responses, the author may also use ES score [2]\n- In Figure 3, kindly include other baselines. Without that, it is hard to understand how good the robustness is. \\\nSince the paper focuses on mitigating the failures of other methods for in-scope data, it is important to demonstrate that clearly.\n\n\nI believe that the problem scope of the paper is important and the idea is well-formed.\\\nHowever, the above issues reduce the reliability of the results.\\\nHence, I am unable to provide a high score due to my above concerns.\\\nI am willing to raise my score significantly if the above discrepancies are addressed.\n\n\n[1] Li, Nathaniel, et al. \"The wmdp benchmark: Measuring and reducing malicious use with unlearning.\" ICML 2024\\\n[2] Yuan, Xiaojian, et al. \"A closer look at machine unlearning for large language models.\" ICLR 2025"}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RWjt0JgI4M", "forum": "wec4qy2XIF", "replyto": "wec4qy2XIF", "signatures": ["ICLR.cc/2026/Conference/Submission2026/Reviewer_NNq9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2026/Reviewer_NNq9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011834668, "cdate": 1762011834668, "tmdate": 1762915993637, "mdate": 1762915993637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies loss-of-control failure modes in prior LLM unlearning (scope underspecification and incoherent post-unlearning responses) and proposes Targeted Reasoning Unlearning (TRU). TRU uses reasoning-based unlearning targets: triplets (input, reasoning trace, refusal) generated by a reasoning LLM (Deepseek API). The target model is trained with a supervised cross-entropy loss on these targets plus a GA (gradient-ascent) unlearning loss to remove memorized knowledge. The authors evaluate TRU on three benchmarks (WMDP, MUSE, TOFU), show ablations and robustness tests (cross-lingual, jailbreak, relearning), propose an LLM-as-a-Judge (LaaJ) evaluation framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper articulates two concrete failure modes (scope and response control) and motivates why prior GA-style methods fail. The case studies are persuasive. \n\n2) Combining supervised training on reasoning+refusal traces with a GA loss is conceptually straightforward yet addresses both criteria (scope + response). The objective and algorithm are easy to implement.\n\n3) The paper ablates the GA component, the target loss, and the reasoning traces to show the role of each piece (Table 2 - Table 4).\n\n4) The authors evaluate on multiple standard unlearning benchmarks (WMDP, MUSE, TOFU), include multiple baselines (GA, GradDiff, NPO, RMU, PO, etc.), present ablations, and test robustness to cross-lingual/jailbreak/relearning attacks. Results show large improvements in Unlearning Quality while keeping reasonable retention."}, "weaknesses": {"value": "1) TRU relies on reasoning traces and refusal responses generated by a reasoning LLM (Deepseek) to define what to learn to refuse. This raises important questions:\n\n1-a) If the same or a closely related large reasoning model produced the targets and is used in evaluation (via LaaJ or shared model families), the method may partly be learning the style/behavior of that external model rather than an independent notion of refusal. The paper needs to discuss whether (and how) target-generation LLMs are disjoint from models used for evaluation and from the unlearned model families (risk of circularity / proxy-overfitting).\n\n1-b) The ethical and safety implications of reintroducing underlying harmful knowledge inside the reasoning trace generator must be discussed: the generator must expose the underlying knowledge to describe why the query is in scope and how is leakage prevented in target generation? The paper should clarify prompt engineering, filtering, or redaction steps used when generating RU targets.\n\n2) TRU mixes supervised target loss and GA unlearning with a balancing hyperparameter α. The paper gives α=0.1 default, but more insight is required. How sensitive are results to α? Is there a principled way to set it? Provide a sensitivity sweep or curve in appendix. Explain potential failure modes when GA is too strong (collapse) or too weak (incomplete erasure).\n\n3) Some ablation numbers appear surprising. For instance (Table 2 / Table 4), the row w/o Reasoning sometimes reports higher UQ (e.g., 8.99 in Table 2) than the full TRU (7.01) on WMDP-Bio, yet the text argues that excluding reasoning harms generalization and RQ. The authors must explain why w/o Reasoning yields higher UQ in some configs (is it due to trivial refusals that get high Rejection but low Helpfulness / low RQ?). Provide breakdowns per UQ sub-metric and example outputs to make this transparent."}, "questions": {"value": "1) Although TRU reports modest drops in RQ in many settings, some retained quality numbers (e.g., Specificity / Logic) are still low in certain datasets. The paper should better characterize which capabilities degrade (language fluency vs factual knowledge vs reasoning). Provide targeted evaluations (e.g., MMLU slices) or examples showing preserved vs degraded behaviors.\n\n2) The authors themselves show an evaluation instability phenomenon where simple reordering of the answer changes metrics drastically. This weakens confidence in LaaJ and task comparisons. The paper must provide quantitative analysis showing LaaJ’s robustness (e.g., inter-annotator agreement analog using different judge LLMs / prompts) and show that reported improvements are robust to judge choice / answer reordering. Report any checks the authors ran to ensure LaaJ does not favor TRU due to shared inductive biases with the Deepseek generator.\n\n3) Clarify target generation pipeline: the Deepseek prompt template is shown, but more specifics (filtering, temperature, number of examples per target, deterministic seed) should be shown in the appendix.\n\n4) It would be interesting to see the comparison of results with a recent paper \"Agents are all you need for LLM Unlearning\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lOGBhgNltD", "forum": "wec4qy2XIF", "replyto": "wec4qy2XIF", "signatures": ["ICLR.cc/2026/Conference/Submission2026/Reviewer_bpmL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2026/Reviewer_bpmL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762077913496, "cdate": 1762077913496, "tmdate": 1762915993413, "mdate": 1762915993413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}