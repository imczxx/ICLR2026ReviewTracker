{"id": "dPuAYNTNQ4", "number": 7119, "cdate": 1758008486643, "mdate": 1759897871811, "content": {"title": "Cognitive Loop: Reversible Hierarchical Markov Chain for Bidirectional Self-Verifying Reasoning", "abstract": "Multi-step Chain-of-Thought (CoT) has significantly enhanced the mathematical reasoning capabilities of large language models by leveraging clear reasoning steps and task-specific logical structures. However, with the widespread adoption of Long CoT, the number of reasoning steps often exceeds the system's manageable limits. To address this, existing approaches attempt to reduce redundancy in KV Cache by introducing Markov chain-like reasoning structures, thereby improving inference efficiency. Nonetheless, such Markov chain-based reasoning methods introduce two critical issues: Lack of memory and Limited backward reasoning capability. To address these limitations, we propose a novel Chain-of-Thought framework based on Reversible Hierarchical Markov Chains, termed Cognitive Loop of Thought (CLoT), and a backward reasoning dataset CLoT-Instruct. In CLoT, the original problem is decomposed into sub-problems with hierarchical dependencies and modeled as a hierarchical Markov chain based on the number of dependencies. Humans typically revisit and verify their reasoning steps after reaching a conclusion to avoid errors. Inspired by this cognitive behavior, we introduce a similar backward verification mechanism at each layer. Moreover, when all higher-level (multi-dependency) sub-problems are verified as correct, we prune the remaining lower-level (fewer-dependency) sub-problems. CLoT effectively mitigates error propagation along the reasoning path and enhances the robustness of the entire reasoning process. We conduct experiments on four mathematical reasoning benchmarks, demonstrating the effectiveness of CLoT. Notably, on the AddSub dataset, when applied to the GPT-4o-mini model, CLoT achieves an accuracy of 99.0\\% , outperforming traditional CoT and CoT-SC by 4.1\\% and 2.9\\%. Our code is publicly available at: https://anonymous.4open.science/r/CLoT-7EBD.", "tldr": "We propose a novel Chain-of-Thought framework based on Reversible Hierarchical Markov Chains, termed Cognitive Loop of Thought (CLOT), and a backward reasoning dataset CLOT-Instruct.", "keywords": ["Large language models", "Chain of thought", "Markov chain"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b4b7760e7e561d38dd61b3cbcd3f3d2343cbd59.pdf", "supplementary_material": "/attachment/ad808dcb2c1c0c3ce35fbf76cf7ee52151db0e84.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce Cognitive Loop of Thought (CLoT), where the authors build on chain of thought reasoning work using hierarchical markov chain through the decomposition of a problem into sub-problems. At every step, a backward verification mechanism is used to avoid errors throughout the thinking process, mimicking human thinking."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Proposed framework outperforms several COT baselines proposed in the literature. \n- The framework is evaluated on 6 different reasoning benchmarks, covering three types of reasoning tasks, with consistent results across most of them. \n- Efficiency analysis shows that CLoT consumes less tokens than several other baselines, while still achieving high performance."}, "weaknesses": {"value": "- Has this been tested on reasoning-tuned LLMs? It seems that the experimental set-up only looks at GPT-4. Although results are good, this does not necessarily mean that this works on other LLMs. \n- It seems that CLoT is most effective on mathematical reasoning. Why is this case? More discussion needs to be included on this point."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RzlJCsPK7c", "forum": "dPuAYNTNQ4", "replyto": "dPuAYNTNQ4", "signatures": ["ICLR.cc/2026/Conference/Submission7119/Reviewer_18ot"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7119/Reviewer_18ot"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755686702, "cdate": 1761755686702, "tmdate": 1762919288589, "mdate": 1762919288589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new reasoning framework for LLMs.\nIt is designed to improve the accuracy and reliability of multi-step reasoning by mimicking the human cognitive process of verifying one's own work.\nThey propose to verify both forward and backward and design a hierarchical pruning to reduce token cost.\nThe authors validate CLoT on six mathematical and commonsense reasoning benchmarks, demonstrating better performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core concept mimics human reasoning behavior and the verification is a novel method for self-correction.\n2. The paper considers the trade-off between efficiency and effectiveness and CLoT creates a balance between them.\n3. The empirical results prove the effectiveness of the concept."}, "weaknesses": {"value": "1. The backward verification process is easy for mathematical problems, but for more complex tasks, the backward question could be vague and hard to define.\n2. Although the authors propose a instruct dataset, the effectiveness of training on such dataset is not reported."}, "questions": {"value": "1. How does the CLoT-Instruct dataset contribute to the training of the LLM?\n2. The reversible hierarchical Markov chain relies on decomposing the problem into sub-problems with hierarchical dependencies. How are these hierarchies and sub-problems initially generated? Is this decomposition an automatic process performed by the LLM, and if so, how sensitive is the final accuracy of CLoT to the quality of this initial decomposition step?\n3. How does the process differ from deductive reasoning paradigm [1][2], apart from the backward verification?\n4. Typos:\n    - Line 171: fforming\n\n[1] Ling, Zhan, et al. \"Deductive verification of chain-of-thought reasoning.\" Advances in Neural Information Processing Systems 36 (2023): 36407-36433.\n\n[2] Zhu, Tinghui, et al. \"Deductive beam search: Decoding deducible rationale for chain-of-thought reasoning.\" arXiv preprint arXiv:2401.17686 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sd0py0Pm2E", "forum": "dPuAYNTNQ4", "replyto": "dPuAYNTNQ4", "signatures": ["ICLR.cc/2026/Conference/Submission7119/Reviewer_93F3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7119/Reviewer_93F3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864937583, "cdate": 1761864937583, "tmdate": 1762919287510, "mdate": 1762919287510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel reasoning framework Cognitive Loop of Thought (CLoT) for large language models (LLMs) that mimics human cognitive verification. Unlike conventional Chain-of-Thought (CoT) methods that rely on forward-only reasoning, CLoT introduces a Reversible Hierarchical Markov Chain (RHMC) that alternates between forward reasoning and backward verification. Each problem is decomposed into hierarchical sub-problems; the model validates each reasoning step by reversing the logic—treating conclusions as known and re-deriving premises. A pruning strategy further skips verification of lower layers once higher-level consistency is confirmed, cutting inference cost by 41.8%. The authors also construct CLoT-Instruct, a dataset that teaches backward verification. Experiments on six benchmarks (AddSub, GSM8K, SVAMP, MATH, AQuA, CommonsenseQA) show consistent accuracy gains over CoT, CoT-SC, and other baselines, achieving 99.0% on AddSub with GPT-4o-mini and 90.5% average accuracy on GPT-4"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a cognitively inspired, reversible reasoning paradigm, and provides rigorous mathematical formulation and efficient hierarchical pruning.\n2. It demonstrates strong empirical improvements with reduced token usage."}, "weaknesses": {"value": "1. The validation is only limited to reasoning benchmarks, and the experiment section lacks generalization tests on non-mathematical or other real-world tasks.\n2. Does the backward verification assumes deterministic reversibility? Hierarchical Pruning assumes that if high-level reasoning passes backward verification, then all lower-level steps are also correct. Does upper-level reasoning happen to appear logically consistent while masking subtle arithmetic or semantic errors in lower layers?\n3. The pruning decision depends on a calibrated threshold $\\tau$ for backward consistency. If $\\tau$ is too low, false positives occur (invalid reasoning passes verification); if too high, pruning rarely triggers, negating efficiency gains. Any robust and adaptive method for tuning $\\tau$?\n4. The overall novelty is incremental given some previous work, e.g., Atom of thoughts for Markov llm test-time scaling, and the work is missing some important references, e.g., Markov chain of thought for efficient mathematical reasoning."}, "questions": {"value": "Refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nl2eYB3nAR", "forum": "dPuAYNTNQ4", "replyto": "dPuAYNTNQ4", "signatures": ["ICLR.cc/2026/Conference/Submission7119/Reviewer_r8AS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7119/Reviewer_r8AS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956962842, "cdate": 1761956962842, "tmdate": 1762919286640, "mdate": 1762919286640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CLoT (Cognitive Loop of Thought), which is a reasoning method where the model generates forward chain-of-thought and also checks if each step can be logically reversed. If a step cannot be reversed to recover the previous information, the model identifies it as incorrect and revises only that part instead of restarting everything (a form of self-correction). The reasoning is organized from high-level plans to detailed steps, so verification begins at the top and only goes deeper if needed, making the process efficient. This approach improves accuracy and uses fewer tokens compared to standard CoT or self-consistency methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The work is well inspired and well motivated\n2. It is good to see that the authors have not only chased after final performance, but rather showed that this improvement comes with same number of tokens\n3. The dataset released would be useful to the community \n4. The idea, from what I can tell is a novel contribution\n\nI do have some critiques, Please see weaknesses"}, "weaknesses": {"value": "1. The approach seems math specific ( or at least specific to deductive reasoning). While this is true, the title and abstract does not mention this explicitly. And this is a concern for me. \n2. I can understand the use of LLMs to polish a paper for grammar and typos (which the authors also declared in appendix), but I feel In the process of polishing with LLMs, this has gotten unnecessarily verbose. \n3. Further It is unclear to me why so much mathematical equations are used in the main body of the paper, while the concept can probably be explained in much simpler words ? For example I see equation 4 defined a   L_rhmc, but I cant see any algorithm directly optimizing it, or even using it as metric. \n4. It is hard to understand the method in detail unless I see the prompts, so I would request the authors to kindly share them with the reviewers, and later add to the main paper."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BxBhnP6JPc", "forum": "dPuAYNTNQ4", "replyto": "dPuAYNTNQ4", "signatures": ["ICLR.cc/2026/Conference/Submission7119/Reviewer_zxbC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7119/Reviewer_zxbC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173141299, "cdate": 1762173141299, "tmdate": 1762919286361, "mdate": 1762919286361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}