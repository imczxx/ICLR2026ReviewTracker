{"id": "Dhd7nHdAaf", "number": 9086, "cdate": 1758110085528, "mdate": 1759897744487, "content": {"title": "Theoretical Guarantees for Iterative Alignment of Self-Rewarding Language Models", "abstract": "Self-Rewarding Language Models (SRLMs) achieve notable success in iteratively improving alignment without external feedback. Yet, despite their striking empirical progress, the core mechanisms driving their capabilities remain unelucidated, leaving a critical gap in theoretical understanding. This paper provides the first rigorous theoretical guarantees for SRLMs. We first establish a lower bound that characterizes the fundamental limits of a single update step, revealing a critical dependence on the quality of the initial model. We then derive finite-sample error bounds for the full iterative paradigm, showing that performance improves at a rate of $\\mathcal{O}\\left(1/\\sqrt{n}\\right)$ with sample size $n$. Crucially, our analysis reveals that the dependence on the initial model decays exponentially with the number of iterations $T$. This provides a formal explanation for *why* iterative self-rewarding succeeds: it robustly overcomes the limitations of a poor initialization. Finally, we instantiate our theoretical framework for the linear softmax model class, yielding tailored guarantees that connect our high-level insights to practical model architectures.", "tldr": "This paper establishes rigorous convergence guarantees and explains why iterative self-rewarding works by proving finite-sample rates with exponential decay of initialization effects.", "keywords": ["Learning theory", "Self-Rewarding", "Language Models", "Convergence Guarantees", "Finite-Sample Analysis"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/81f4d31a06be388b413bea745de8f6a03345357a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides the first rigorous theoretical analysis of Self-Rewarding Language Models (SRLMs), a paradigm that has recently shown strong empirical success. The authors aim to explain why iterative self-alignment works without external feedback. They introduce a \"policy condition number\" to quantify a model's suitability for self-alignment and establish a lower bound showing that a single update step can easily fail if the initial model is poorly conditioned. The main contribution is a finite-sample guarantee for the full iterative process. The analysis reveals that the dependence on the (potentially poor) initial model quality decays exponentially with the number of iterations. This provides a clear theoretical explanation for the success of SRLMs: the process first self-stabilizes before entering a standard statistical learning phase. The framework is then instantiated for linear softmax models, connecting the general theory to parametric model classes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This is a timely and important paper that addresses a critical gap in our understanding. While SRLMs are being used and developed rapidly, the theoretical principles behind their stability and success have been missing. To my knowledge, this is the first work to provide a formal theoretical foundation for this class of methods.\n- The central idea of the paper is very powerful. The introduction of the \"policy condition number\" (`κt`) and the demonstration that the iterative update acts as a contraction mapping on this value is an elegant and insightful explanation. The resulting two-stage view of the process—first, an internal self-correction phase where the influence of a poor initialization vanishes, followed by an efficient statistical learning phase—is compelling and aligns with empirical intuitions.\n- The paper is technically sound and comprehensive. It correctly identifies the core challenges by first establishing a lower bound on the failure of single-step updates (Theorem 1), which motivates the need for iteration. The main upper bound (Theorem 3) is the key result, and its tightness in the single-step case is a strong sign of a robust analysis. The detailed proofs in the appendix appear thorough.\n- The authors make a good effort to connect their theoretical findings to practical scenarios. The analysis of the number of iterations required (Corollary 4) provides a practical insight, suggesting that even very poorly conditioned models can be aligned without an excessive number of steps. Furthermore, the extension to linear softmax models and the discussion of effective dimensionality (Corollary 6) make the results more relevant to the overparameterized models used in practice."}, "weaknesses": {"value": "- The theoretical model, while insightful, relies on a few key simplifying assumptions.\n    -  The self-reward function is defined specifically as the model's log-probability. While this is a natural starting point, many practical SRLM approaches use the model in a more complex, \"LLM-as-a-Judge\" capacity to generate scores or critiques. The applicability of the theory to these more complex reward schemes is unclear.\n   -  The realizability assumption (Assumption 1) is common in theoretical analyses but can be a strong requirement in practice.\n- The extension to linear softmax models is a necessary and welcome step to move from finite model classes to parametric ones. However, this is still a considerable abstraction from the complex, non-linear dynamics of Transformer architectures. While this is a limitation of most current theoretical work in deep learning, it's worth acknowledging the potential gap.\n- The paper is mathematically dense. While this is expected for a strong theory paper, the main text could benefit from building a bit more intuition around some of the key constants. For example, what would a low vs. high \"margin constant\" (γ) or \"minimum confidence\" (c) mean in the context of a practical LLM's output distribution?"}, "questions": {"value": "1.  The choice of log π(y|x) as the reward is crucial to the analysis. Could you comment on the challenges of extending this framework to other self-reward functions? For instance, if the reward is a scalar score produced by the LLM in a separate forward pass (i.e., LLM-as-a-Judge), does the core mechanism of the policy condition number contracting still hold?\n2.  The policy condition number κt is a fantastic theoretical tool. Does this quantity have a practical analogue that could be estimated or tracked during training? It seems like it could serve as a powerful diagnostic to monitor whether the model is in the \"stabilization\" or \"learning\" phase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "u7lqR0eLeN", "forum": "Dhd7nHdAaf", "replyto": "Dhd7nHdAaf", "signatures": ["ICLR.cc/2026/Conference/Submission9086/Reviewer_u3cM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9086/Reviewer_u3cM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761033920570, "cdate": 1761033920570, "tmdate": 1762920791783, "mdate": 1762920791783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Summary.** The paper gives the first finite-sample analysis for *iterative* self-rewarding LMs (SRLMs), where the policy both generates data and provides the reward via its own log-likelihood. It shows (i) any **single** self-rewarding step can fail when the initial policy is too diffuse, and (ii) under a DPO-style iterative update, the “policy condition number” contracts so that the dependence on initialization decays exponentially, leading eventually to an $(O(\\\\frac{\\\\log(n|\\\\Pi|/\\\\rho)}{\\\\sqrt{n}}))$-type rate."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Many theoretical results."}, "weaknesses": {"value": "**Weaknesses / comments.**\n\n1. **Scope / limitation not explicit.** The analysis is done for the *simplest* self-reward: $(r\\_t(x,y)=\\\\log \\\\pi\\_t(y\\\\mid x)).$ This is much weaker than current SRLM practice (meta-judges, rubric-based scoring, multi-turn evaluation). The paper should clearly state that the guarantees do **not** cover those richer judges and explain which parts of the proof rely on “reward \\= current log-probability.” (E.g. the recursion for ($\\\\kappa\\_t$) and the DPO square-loss instantiation both use this form.)  \n     \n2. **No empirical illustration.** Given how interpretible the policy condition number (\\\\kappa\\_t) is, the paper misses an easy check: plot (\\\\kappa\\_t) (or a Monte-Carlo estimate of it) over iterations and show it actually contracts as predicted by the bound $(\\\\kappa\\_T \\\\le U \\+ q^T(\\\\kappa\\_0 \\- U)).$ Even a toy experiment would make Theorem 3 and the “two-stage” story in Remark 4 more convincing.  \n     \n3. **Relation to pseudo-labelling / logged-data works is underdeveloped.** The paper’s setting is very close to self-training / pseudo-labelling: the model supplies its own supervision and then trains on it. Please add a related-work paragraph comparing to (i) classical pseudo-labelling in semi-supervised learning, and (ii) pseudo-reward / off-policy batch learning from logged data such as *Aminian et al., 2022, “Semi-supervised Batch Learning From Logged Data” (arXiv:2209.07148)*, which also studies learning from regularization perspective.   \n\n4. **Remark 1 is too casual about “ignoring the second log term.”** The lower bound in Theorem 1 is $\\[ \\\\Pr\\[\\\\text{fail}\\] \\\\gtrsim \\\\Bigg(\\\\frac{\\\\kappa\\_0 \\\\log|\\\\Pi|}{n ,\\\\log\\\\big(\\\\frac{n\\\\kappa\\_0}{\\\\log|\\\\Pi|}\\\\big)}\\\\Bigg)^{1/2} \\] $and Remark 1 says that “ignoring the second-order logarithmic term” gives $(\\\\big(\\\\frac{\\\\kappa\\_0 \\\\log|\\\\Pi|}{n}\\\\big)^{1/2}).$ Please clarify it.  \n     \n5. **Eq. (7) and Remark 3 need a cleaner asymptotic reading.** Eq. (7) has the form $\\[ \\\\Pr\\[\\\\text{fail}\\] ;\\\\lesssim; \\\\frac{1}{\\\\sqrt{n}}, \\\\frac{\\\\log(n|\\\\Pi|/\\\\rho)}{\\\\gamma \\\\delta} \\\\Bigg( \\\\frac{1}{\\\\sqrt{c}}\\\\frac{\\\\sqrt{\\\\kappa\\_0}}{(1+\\\\sqrt{nc})^{(T-1)/2}} \\\\Bigg), \\]$ so it is natural to summarize it as “up to logs, $(O(\\\\log n / \\\\sqrt{n})).$” please clarify it.\n\n   \n6. **Practical relevance.** Right now the story is: “if we choose $(\\\\beta \\\\lesssim n^{-1/2})$ and we can sample (n) pairs per round from $(\\\\pi\\_t)$, then we get a contraction.” It would help to add 2–3 sentences on how realistic this is for large LMs (cost of self-sampling, judge cost, etc.)."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8NKRL2T3o3", "forum": "Dhd7nHdAaf", "replyto": "Dhd7nHdAaf", "signatures": ["ICLR.cc/2026/Conference/Submission9086/Reviewer_uGyn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9086/Reviewer_uGyn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813433474, "cdate": 1761813433474, "tmdate": 1762920791390, "mdate": 1762920791390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper establishes the first rigorous theory for Self-Rewarding Language Models (SRLMs), where a model iteratively improves itself without external feedback. The authors introduce the policy condition number ($\\kappa_t$) to quantify self-alignment stability and prove that single-step self-rewarding updates can fail under ill-conditioned initialization, while iterative updates achieve $\\mathcal{O}(1/\\sqrt{n})$ improvement with exponentially diminishing dependence on $\\kappa_0$. This explains SRLMs’ empirical success as a two-phase process—self-correction followed by efficient learning—and extends to parameterized models, showing scalability through the concept of effective dimension."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is the first to provide rigorous theoretical guarantees for the iterative self-rewarding (SRLM) paradigm. SRLMs represent an empirically successful yet theoretically opaque (“black-box”) area, and this work fills a major gap. \n\n2. The paper clearly identifies and formalizes the core question of why iteration works, introducing the policy condition number $\\kappa_t$ as a key metric and modeling the iterative process as a contraction mapping on $\\kappa_t$ (Remark 4). This offers a insightful and compelling mathematical explanation for the stability of SRLMs."}, "weaknesses": {"value": "1. Although this is a theoretical paper, it provides no experiments at all, neither real-world LLM results nor even simple simulations verifying its theoretical predictions. For instance, visualizing the exponential decay of $\\kappa_t$ in a toy linear Softmax setting would substantially strengthen the paper’s credibility and overall impact.\n\n2. The theoretical analysis relies heavily on several assumptions that may not hold in practice. The realizability assumption requires the optimal model $\\pi^*_\\beta$ to lie within the model class $\\Pi$, which is problematic in SRLMs since the self-reward objective $r_t = \\log \\pi_t$ makes $\\pi_t$ itself a degenerate fixed point.Tthe definitions of the minimum confidence $c > 0$ and minimum margin $\\gamma > 0$ are unrealistic for large-scale LLMs as most token probabilities in a vast vocabulary are near zero, and response quality differences are often negligible ($\\gamma \\approx 0$)."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nUmnWq5NcX", "forum": "Dhd7nHdAaf", "replyto": "Dhd7nHdAaf", "signatures": ["ICLR.cc/2026/Conference/Submission9086/Reviewer_12gE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9086/Reviewer_12gE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832252268, "cdate": 1761832252268, "tmdate": 1762920790916, "mdate": 1762920790916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper theoretically tries to understand the iterative self-rewarding procedure in LLMs. It first shows that optimizing a DPO-style objective with one step can lead to a model with a high probability of failure (depending on the base model). It then shows that iterative self-rewarding alignment can exponentially (in reward steps $T$) suppress the effect of a bad initial model. It finally applies the results to a linear softmax model as an example."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper works on a very important and hot problem of understanding the iterative self-rewarding in LLMs. The paper is very well written and easy to follow, and the authors have spent time discussing their results thoroughly to make them digestible for the reader. I feel the results are very clean and show the effect of iterative self-rewarding alignment, which is the main goal of the work."}, "weaknesses": {"value": "**Dependence on $\\gamma$**: The finite-sample guarantee of iterative procedure (Theorem 3) depends on the margin $\\gamma$ being strictly positive. However, $\\gamma$ is defined as the infimum over all prompts and all iterations $T$. What prevents a 'margin collapse' ($\\gamma \\to 0$) during training—for instance, on a prompt where two responses are equally optimal? If $\\gamma = 0$, doesn't the bound become vacuous and the guarantee of stability disappear? I feel like this is a strong assumption, one could easily have a bad initial point leading to a margin collapse. \n\nFollowing up on this, does the self-rewarding process itself, $r_t = \\log \\pi_t(y|x)$, have any mechanism to prevent this margin collapse, or could it, in fact, cause it by optimizing two similar responses to have the same high probability? How robust is the theory to this hard-margin assumption?\n\n**Tightness of Thm. 3**: Line 304 states that the bound in Thm. 3 is tight in its dependence on key parameters. For $T=1$ it is, as discussed. But for $T>1$, we don’t know, right? And it's not as if $T$ is not a key parameter of the problem. How can we say that the iterative decay is tight?\n\n**On exponential stability and role of objective**: The paper's proof of exponential stability (Remark 5) fundamentally relies on deriving a recurrence where $\\kappa_t$depends sub-linearly on $\\kappa_{t-1}$ (i.e., as $\\sqrt{\\kappa_{t-1}}$). This dependency is what creates the contraction mapping. However, the proof also explicitly relies on the $\\mathcal{O}(1/\\sqrt{n})$ statistical error rate, which is a well-known property of the DPO/ERM loss (Eq. 3).\n\nMy question is: Is this stability-inducing $\\sqrt{\\kappa_{t-1}}$ relationship a fundamental property of the self-reward mechanism ($r_t = \\log \\pi_t$) itself, or is it an emergent property of its combination with a stable, low-variance objective like DPO? If, for example, the DPO objective were replaced with a different update rule that has a higher-variance or slower-converging statistical error, would the entire proof of a sub-linear recurrence collapse? This would seem to imply the paper's guarantees are not about self-rewarding in general, but only about the specific combination of self-reward and DPO.\n\n\n**On the role of model complexity**: All results in the paper (Thm. 1, Thm. 3) are reminiscent of classical statistical learning theory, where model complexity (here $|\\Pi|$) is a penalty that makes the error bound worse. Moving from classical to overparameterized models, people developed new results to explain benign overfitting. In their current shape, I don’t see the empirical reality of “scaling laws” for alignment reflected here, where larger models are better at alignment. In the linear softmax case, the authors introduced spectral decay in the feature covariance to deal with the curse of a larger model, which is one data model under which people explain benign overfitting in linear regression. I feel that connection is missing here and would be useful to the reader. Also, I am interested to hear the authors’ comments on how to translate their original results to reflect the role of a larger model."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iVXdhLkFpx", "forum": "Dhd7nHdAaf", "replyto": "Dhd7nHdAaf", "signatures": ["ICLR.cc/2026/Conference/Submission9086/Reviewer_kTQ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9086/Reviewer_kTQ9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762856234670, "cdate": 1762856234670, "tmdate": 1762920790487, "mdate": 1762920790487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}