{"id": "TTfqiRjoCa", "number": 4369, "cdate": 1757668041616, "mdate": 1759898036763, "content": {"title": "Piecewise Polynomial Regression of Tame Functions via Integer Programming", "abstract": "Tame functions are a class of nonsmooth, nonconvex functions that appear in a wide range of applications: in training deep neural networks with all common activations, as value functions of mixed-integer programs, or as wave functions of small molecules. We consider approximating tame functions with piecewise polynomial functions. We present a theoretical bound on the approximation quality of a tame function by a piecewise polynomial function. We also present mixed-integer programming formulations of piecewise polynomial regression and demonstrate promising computational results.", "tldr": "We approximate nonsmooth nonconvex, yet tame, functions with piecewise polynomial functions.", "keywords": ["tame functions", "o-minimality", "piecewise polynomial functions", "regression"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0e52c58ab98a3db80e740baf623071c8588ed74e.pdf", "supplementary_material": "/attachment/15dba81388ab97b2417e951eabd44398f28bc53b.zip"}, "replies": [{"content": {"summary": {"value": "The paper titled “PIECEWISE POLYNOMIAL REGRESSION OF TAME FUNCTIONS VIA INTEGER PROGRAMMING” presents a method for piecewise polynomial regression of tame functions, which is  a class of non-smooth, non-convex functions. This is done by first showing bounds on the decomposition of a non-convex function into piecewise polynomials and then formulating the regression problem as a Mixed-Integer Program. The core contributions include the first theoretical bound on the approximation error for generic tame functions, showing that the error decreases as the polynomial degree and the number of partition pieces (controlled by tree depth) increase. The practical side of this approach is demonstrated by showing effective approximation of functions like a mixed-activation neural network."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper claims to show the first theoretical bound on the approximation error of generic nonsmooth and nonconvex tame functions by piecewise polynomial functions. \n\nOverall it gives a way to compute the best piecewise polynomial function, where each piece is a polyhedron defined by a number of affine inequalities, and the polynomial on each piece has a given degree."}, "weaknesses": {"value": "While the paper’s experiments approximates  a small neural network example, the MIP approach's scalability is limited to low-dimensional functions in terms of proving global optimality. For typical high-dimensional models, the full optimization process will be prohibitively slow, as it is well known that MIPs takes hours to solve. This suggests the method is not practically viable for finding globally optimal solutions for large, modern neural networks.\n\n\nThe paper's primary theoretical contribution is a general approximation bound for tame functions and its core practical solution is a Mixed-Integer Programming (MIP) formulation. Given the focus on o-minimal structures,and  stratification theorems, the paper is arguably a better fit for an optimization or theoretical mathematics conference than a major DL conference like ICLR, where the scalability of solving the underlying optimization problem is a crucial metric for practical relevance.\n\nWhat is not answered explicitly is what value the approximation adds from a practical point of view.\nIt hints at several potential applications but any solid experimental results backing it up is lacking.\nThis makes the paper and its proposed approach, more of a proof-of-concept for tame functions rather than a practical tool for the deep learning community"}, "questions": {"value": "na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w0Ael0jiKR", "forum": "TTfqiRjoCa", "replyto": "TTfqiRjoCa", "signatures": ["ICLR.cc/2026/Conference/Submission4369/Reviewer_A4b4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4369/Reviewer_A4b4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879088436, "cdate": 1761879088436, "tmdate": 1762917319882, "mdate": 1762917319882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies approximation of tame functions using piecewise polynomial models. It formulates an MIP approach that learns an affine-hyperplane tree to partition the input space and fits a polynomial on each leaf, implemented via monomial lifting and standard linearizations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main innovations are the tidy, two-term error characterization for general tame functions and a practically solvable MIP formulation that operationalizes piecewise polynomial regression beyond axis-aligned trees."}, "weaknesses": {"value": "However, I have the following concerns:\n\n1. The problem framing is tailored to be theory-friendly. The tame assumption is difficult to verify and can be fragile under noise or distribution shift, so it is unclear whether the structural guarantees meaningfully carry over to practical implementations. Besides, the proposed approach relies on monomial lifting and discrete tree decisions, which causes rapid growth in variable counts and branching complexity. The experiments remain small-scale and do not convincingly demonstrate that the method remains tractable as dimensionality or dataset size increases. Finally, the work offers limited guidance on stopping criteria, numerical stability, or deployment playbooks.\n\n2. For medium-to-high dimensional regression or large datasets, the training and maintenance overhead of a MIP-based piecewise polynomial model will generally be prohibitive. Tasks with highly intricate non-smooth boundaries will force deeper trees and many leaves, further increasing the combinatorial burden. In strongly real-time applications that must update models continually, mainstream methods such as gradient-boosted decision trees, random forests, neural networks, kernel methods, and spline/FEM surrogates tend to reach useful accuracy much faster and with lower engineering cost. \n\n3. As I know, there is no automated, provably stable strategy for setting and tightening big-M constants or for adapting numeric scales, which are essential to avoid weak relaxations and solver pathologies. The model complexity is not adapted to choose polynomial degree or leaf capacity based on error indicators, so accuracy improvements inevitably come with combinatorial blow-ups. \n\n4. When evaluated on the axes of error versus training time versus memory, the proposed approach is unlikely to be superior to GBDT/LightGBM, modern neural regressors, or spline/FEM surrogates at moderate or large scales. The method’s edge is interpretability and constraintability, since leafwise polynomials can be audited and global constraints such as monotonicity or safety can be encoded, but the paper does not quantify this advantage against strong baselines that already support monotonic constraints, fairness regularizers, or distillation for interpretability."}, "questions": {"value": "1. How to diagnose whether the tame structure is a reasonable working assumption on noisy, shifting datasets？\n\n2. What are the empirical and analytical scaling laws linking dimension, data size, polynomial degree, and leaf count to time, memory, and solver gap？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kuPtL73a4K", "forum": "TTfqiRjoCa", "replyto": "TTfqiRjoCa", "signatures": ["ICLR.cc/2026/Conference/Submission4369/Reviewer_W6es"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4369/Reviewer_W6es"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894734520, "cdate": 1761894734520, "tmdate": 1762917319181, "mdate": 1762917319181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a theory for the uniform approximation of tame functions by piecewise-polynomial functions. The proof makes use of the stratification of tame functions into smooth parts, combining the usual polynomial approximation theory of smooth functions with recently-developed techniques for estimating/controlling the singular set. The construction yields an algorithm for this approximation via mixed integer programming, which the authors demonstrate numerically on several simple test functions (norm, cone function, small neural network)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is very clearly written, and the proof of the approximation theorem is easy to follow. I know it can sometimes make the math less clear when the theory has to be summarized in the main paper and proved in appendices, but I think this work does well at communicating the main technical ideas.\n\n2. It is definitely valuable to understand the complexity of piecewise-polynomial approximation for broad classes of functions. I am unfamiliar with definable structures and such, but the piecewise-smooth structure of tame functions makes it a very natural class to analyze using this paper's argument (bootstrapping the smooth case using control on the strata). It is also sufficiently general to be interesting, as many functions of approximation interest are tame."}, "weaknesses": {"value": "1. One thing that could be more clear is which elements of this paper are novel. (Every review of every paper in every ML conference says this so I will try to be specific). In my understanding, the approximation result combines classical polynomial approximation of smooth functions (Jackson bounds) with the results from [Boissonat et al, 2023] which control the singular set. The numerical approach uses standard polyhedral optimization machinery (decision trees, encoding as a MIP, etc.). These are definitely strong elements and the authors combine them naturally, but I do feel like the \"related work/preliminaries\" is a bit lacking in these departments. The authors emphasize that the stratification of tame functions is a pioneering classical result, but the other tools are not given the same exposition. I would like to know more about the fields/problems where they come from.\n\n2. Having the constants in the approximation theorem depend on f is a scary thing... Of course your result is still saying something useful, but I think one thing about Jackson's bound (such as Theorem 1.7 of https://www.math.umd.edu/~petersd/666/amsc666notes02.pdf) that is crucial in application is that the exact dependence of the constant on the function f is known (it's just the ||\\cdot||_{C^k} norm). I am not sure if the exact dependence of the constants on f and dimension can be identified (would be very helpful if so!), but I think it would be nice if the authors could say a bit about the behavior they expect. The main questions I have are (1) which types of singularities are more efficient to approximate than others and (2) how does the \"cost\" of singularities scale with dimension.\n\n3. I think the experimental section is a bit underwhelming. The MIP approach does not seem efficient enough to scale to mid-size problems, and computational restrictions made it so that only very small regressions could be attempted. I know one of the reasons for excitement in piecewise-smooth approximation is due to modern neural net practices, but it's hard to gain insight from these experiments about larger-scale approximation questions. I am not sure how the authors want to position it, but it is tricky; the approximation result is strong and an optimization algorithm pairs nicely, but it seems improvements are needed on the optimization side."}, "questions": {"value": "1. Naturally, I have the question corresponding to Weakness (1) above. Since I am not familiar with tame stuff, I can contextualize my question with an analogous situation in geometric measure theory (GMT). In this field, the notion of \"rectifiable sets\" was crafted to represent \"sets which are covered (almost everywhere) by a stratification into smooth pieces\". I think any reasonable person would try to apply the smooth polynomial approximation case (Jackson bounds) on the smooth parts and glue them together across the singular parts, and so the name of the game becomes understanding the structure of the singular set well enough to conclude nontrivial things. Sometimes there is a smaller class of geometric objects, such as minimal surfaces, for which we can say more about their singular sets, which naturally would yield similar results about \"how to glue smooth results together when given a patchwork of smooth results\". Many proofs in nonsmooth geometry have to address this at some point.\n\nSorry for the rant, but I share the above anecdote to say that there is a whole continuum of different results about \"piecewise polynomials approximate class X with rate Y\" which may be proved through this high-level argument. The tools are out there, and sometimes the results  don't get written up and proven. In my eyes, the value of such a result comes from an interesting/general function class X or a rate Y which is somehow informative. In the minimal surface setting, people took the time to write hundreds of pages about it because the class of minimal surfaces is interesting enough. In short, I would like to know more about how approximation of tame functions is thought of/used more widely, as well as if there is anything interesting about the error rate you get. Is the dependence on \\ell and n surprising/optimal, does it agree with or improve on someone else's rate for a related problem? Upon reading the paper now, it is unclear how the choice of X=tame functions and the rate ell^{-1/n} fit with expectations or comparable results. \n\n\n2. What does the rate \\ell^{-1/n} suggest? I know Boris Hanin showed that for e.g. random ReLU nets (piecewise linear), the typical number of regions is much smaller than expected (poly(dim) instead of exp(dim)); where do you think your bound incurs any pessimism, and how does this approximation rate relate to the curse of dimensionality?\n\n3. Can you share a bit more about Assumption 1? It appears to be some nondegeneracy/transversality thing, and I guess I am wondering if it is a merely technical assumption or an important element that somehow makes the singular set covering more efficient and allows you to get the rate you did. As I mentioned earlier, while reading this my main question was \"what extra things are they using other than being piecewise-smooth?\". It would definitely help me out if the authors could detail a bit more how the assumptions of tameness and Assumption 1 constrain the singularities in a way that makes the approximation construction more efficient (or, if it doesn't make it more efficient, why they are needed at all)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k6bYpVYW8l", "forum": "TTfqiRjoCa", "replyto": "TTfqiRjoCa", "signatures": ["ICLR.cc/2026/Conference/Submission4369/Reviewer_6Gn6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4369/Reviewer_6Gn6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954840862, "cdate": 1761954840862, "tmdate": 1762917318998, "mdate": 1762917318998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}