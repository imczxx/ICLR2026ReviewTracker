{"id": "yiMlVBAoQi", "number": 24228, "cdate": 1758354402172, "mdate": 1759896775712, "content": {"title": "Efficient Quantization of Mixture-of-Experts with Theoretical Generalization Guarantees", "abstract": "Sparse Mixture-of-Experts (MoE) allows scaling of language and vision models efficiently by activating only a small subset of experts per input. While this reduces computation, the large number of parameters still incurs substantial memory overhead during inference. Post-training quantization has been explored to address this issue. Because uniform quantization suffers from significant accuracy loss at low bit-widths, mixed-precision methods have been recently explored; however, they often require substantial computation for bit-width allocation and overlook the varying sensitivity of model performance to the quantization of different experts. We propose a theoretically grounded expert-wise mixed-precision strategy that assigns bit-width to each expert primarily based on their *change in router’s* $l_2$ *norm* during training. Experts with smaller changes are shown to capture less frequent but critical features, and model performance is more sensitive to the quantization of these experts, thus requiring higher precision. Furthermore, to avoid allocating experts to lower precision that inject high quantization noise, experts with large *maximum intra-neuron variance* are also allocated higher precision. Experiments on large-scale MoE models, including Switch Transformer and Mixtral, show that our method achieves higher accuracy than existing approaches, while also reducing inference cost and incurring only negligible overhead for bit-width assignment.", "tldr": "We propose a theoretically provable method for efficient quantization of large Mixture-of-Experts models.", "keywords": ["Mixture-of-Experts", "Quantization", "Theoretical Generalization Guarantees"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0bc8526c0ac88ac9be97245f5c838d9800891236.pdf", "supplementary_material": "/attachment/3af80b546f4616dfcf9c97670a0b753b7b28b570.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel quantization method for Mixture-of-Experts (MoE) models that dynamically selects the precision for each expert based on changes in the $l_2$ norm of its weights. Theoretical analysis is provided for a toy MoE architecture, and numerical experiments demonstrate that the proposed approach outperforms heuristic methods in low-bit quantization scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Theoretical results are provided, and the method design is well aligned with the theoretical justification.  \n2. Numerical results demonstrate that the proposed method achieves performance gains over baseline approaches, delivering comparable or even superior performance to heuristic methods and techniques that rely on calibration sets."}, "weaknesses": {"value": "1. The theoretical analysis is based on a highly simplified MoE architecture focused on binary classification tasks. The gap between the theory and empirical results is not sufficiently addressed. In particular:  \n   (1) Can the insights from Theorem 4.4 be used to *adaptively* determine quantization precision—rather than relying on predefined bit levels?  \n   (2) Do activations in practical MoE models align with the conclusions of Theorem 4.3? If not, what are the key discrepancies, and how do they impact the method’s effectiveness?\n\n2. Several important experimental details are missing:  \n   (1) In Figure 2, the “activation weights” baseline—which specific method  does it correspond to?  \n   (2) How are the hyperparameters (e.g., $\\zeta$) selected? Is their choice task- or model-dependent, and how sensitive is the final performance to variations in $\\zeta$?"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7stLWKAiZj", "forum": "yiMlVBAoQi", "replyto": "yiMlVBAoQi", "signatures": ["ICLR.cc/2026/Conference/Submission24228/Reviewer_CgYX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24228/Reviewer_CgYX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566275356, "cdate": 1761566275356, "tmdate": 1762943006404, "mdate": 1762943006404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an expert-wise mixed-precision quantization method for MoE models. It allocates higher bit-widths to experts that are more sensitive to quantization, identified through the router’s L2 norm change and neuron variance. The paper provides theoretical support showing that experts learning rare but important features require higher precision. Experiments show that the method achieves well performance than existing quantization methods with lower average bit-width and minimal overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper proposes a novel and efficient expert-wise mixed-precision quantization strategy based on router norm change.\n2.\tThe paper provides theoretical analysis explaining why experts learning rare features are more sensitive to quantization errors.\n3.\tAchieves well performance and inference efficiency under the same average bit budget."}, "weaknesses": {"value": "1.\tWhen only pretrained router norms are used without fine-tuning, if the pretraining corpus and downstream data distributions differ, can the norm-based ranking still reliably identify “rare but important” experts?\n2.\tCan the claim that experts learning rare tokens exhibit weaker activations and smaller router norm changes be directly verified through visualization or statistical analysis on real LLM corpora?\n3.\tWould lightly retraining the low-bit experts further reduce the average bit-width or improve model robustness?"}, "questions": {"value": "see the questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2V7D7wtMJn", "forum": "yiMlVBAoQi", "replyto": "yiMlVBAoQi", "signatures": ["ICLR.cc/2026/Conference/Submission24228/Reviewer_LANt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24228/Reviewer_LANt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964830826, "cdate": 1761964830826, "tmdate": 1762943006196, "mdate": 1762943006196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel expert-wise mixed-precision quantization strategy for Mixture-of-Experts (MoE) models, aiming to reduce their substantial memory footprint without significant performance degradation. The core contribution is a theoretically-grounded, two-stage heuristic for assigning bit-widths to experts. First, experts are ranked based on the change in their router's L2 norm during training $\\Delta^T= || w^T||_2 - || w^0||_2$, the paper's theory suggests that experts with a smaller norm change are more sensitive to quantization and thus require higher precision. Second, this ranking is adjusted by promoting experts with high maximum intra-neuron variance to higher ranks to mitigate quantization noise. The authors provide a theoretical analysis for a simplified two-layer MoE model to justify their primary metric. They validate their approach empirically on large-scale models, including Switch Transformer and Mixtral (8x7B and 8x22B), demonstrating superior performance compared to several baselines, including uniform quantization and prior expert-wise methods. A key advantage highlighted is the negligible computational overhead of this bit-assignment method compared to more expensive SOTA approaches like PMQ."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method in this paper for bit-width assignment is computationally trivial, requiring a simple sort based on router norms. This stands in stark contrast to computationally expensive SOTA methods like PMQ, which require extensive calibration. This is a major practical advantage.\n\nThe method demonstrates strong performance on challenging benchmarks with large-scale Mixtral models, often outperforming existing expert-wise and non-expert-wise quantization methods."}, "weaknesses": {"value": "1. For the key experiments on pre-trained models, the method abandons its primary metric (change in norm) for a surrogate (final norm). This switch is poorly justified and severs the link to the paper's own theoretical analysis. \n\n2. The `MaxVar` reordering step is not theoretically motivated and feels like an engineered solution to patch deficiencies in the primary metric. The lack of an ablation study makes it impossible to disentangle its effect, obscuring the true source of the performance gains."}, "questions": {"value": "Roughly same as weakness:\n\n- The use of the final router L2 norm as a surrogate for the change in norm is the most critical unsupported step in the paper. Can you provide any empirical evidence, for instance on a smaller model that can be fine-tuned, that these two metrics produce a similar expert ranking?\n- The proposed theory posits that experts with smaller router norm changes learn \"less frequent but critical features.\" Can the authors show what types of tokens or inputs are processed by the high-precision vs. low-precision experts in a real-world task, does this align with your \"critical but infrequent\" hypothesis?\n\n- Please provide an ablation study that evaluates the performance of your method under three conditions: (i) using only the router norm ordering, (ii) using only the MaxVar ordering, and (iii) the proposed combination. This will help understand where the gains are coming from."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3LbLdYgWeq", "forum": "yiMlVBAoQi", "replyto": "yiMlVBAoQi", "signatures": ["ICLR.cc/2026/Conference/Submission24228/Reviewer_8VPJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24228/Reviewer_8VPJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979104156, "cdate": 1761979104156, "tmdate": 1762943005772, "mdate": 1762943005772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}