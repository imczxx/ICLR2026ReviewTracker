{"id": "ioGQhr1lhZ", "number": 23595, "cdate": 1758346072127, "mdate": 1759896805727, "content": {"title": "Model-aware Counterfactual Data based Contrastive Decoding for Video-LLM", "abstract": "Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. \nWe propose Model-aware Counterfactual Data based Contrastive Decoding (GeWu), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM‚Äôs own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, and Perception-test show that GeWu consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including InternVL3, Qwen2.5-VL and Qwen2-VL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.", "tldr": "", "keywords": ["Video-language models", "Hallucination mitigation", "Contrastive decoding", "Object-level data augmentation", "Counterfactual inputs"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60f57d1958fb5534daacac1e146dac6e9bba42e8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "GeWu reduces Video-LLM hallucination by optimizing model-aware spatial/temporal masks to form counterfactual views and contrasting logits between original and masked videos at inference‚Äîno retraining, improved metrics across benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "It introduces a model-guided way to construct counterfactual data rather than using random perturbations.\nMask optimization via the model‚Äôs own gradients is intuitive and principled.\nIt reduces hallucination without retraining, making it appealing for large Video-LLMs."}, "weaknesses": {"value": "The pipeline assumes reliable detection/tracking; FP/FN or ID-switches can steer mask optimization toward irrelevant regions, especially for small/occluded/co-occurring objects. Suggestion: report robustness by varying detector strength, adding occlusion/motion blur, and including a GT-oracle upper bound.\nMaximizing next-token loss by ascending on mask strengths can increase loss for non-semantic reasons (e.g., edge artifacts) rather than by truly removing evidence. Suggestion: add TV/LPIPS or smoothness constraints and show qualitative mask visualizations to confirm human-interpretable evidence removal.\nAlthough inference-only, total cost includes mask-optimization iterations plus one extra contrastive forward per step."}, "questions": {"value": "Beyond the reported VLLMs, how well does GeWu transfer to other video-LLMs/backbones?\nHow are ùõº and ùõΩ chosen at test time?\nWhy prefer pixelwise max over confidence-normalized blending? In dense scenes, does union suppress secondary evidence needed by the query?\nCan you show side-by-side visualizations where optimized masks clearly remove query-relevant evidence (both successes and failures)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7N8urbCtzv", "forum": "ioGQhr1lhZ", "replyto": "ioGQhr1lhZ", "signatures": ["ICLR.cc/2026/Conference/Submission23595/Reviewer_fAuZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23595/Reviewer_fAuZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657291891, "cdate": 1761657291891, "tmdate": 1762942728230, "mdate": 1762942728230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets hallucination in Video-LLMs and proposes GeWu‚Äîa model-aware counterfactual data based contrastive decoding strategy. Instead of using random perturbations as the contrastive view, GeWu mines the Video-LLM‚Äôs own loss/gradient feedback to localize query-relevant objects and frames, softly masks those regions to build a targeted counterfactual view, and then performs contrastive decoding between the original and counterfactual inputs. This enforces evidence-grounded token selection at inference without additional training. Evaluated on EventHallusion, MVBench, and Perception-test, GeWu consistently reduces hallucinations while maintaining or improving task accuracy across multiple backbones (InternVL3, Qwen2/2.5-VL), with pronounced gains for small/occluded/co-occurring objects. The method also exposes practical guidance for tuning the decoding coefficients (Œ± for contrast strength, Œ≤ for plausibility filtering) and shows broadly stable behavior."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.Proposes a model-aware counterfactual view: uses the Video-LLM‚Äôs own gradients to mine object- and frame-level evidence and couples this with contrastive decoding. This training-free, evidence-driven suppression is more targeted than the random/heuristic views used in prior CD/VCD variants.\n2.A plug-and-play inference-time strategy: backbones remain frozen; only a learnable mask and one counterfactual branch are added. This yields near-zero training cost and easy portability across different Video-LLMs.\n3.Consistently reduces hallucination on EventHallusion, MVBench, and Perception-test across multiple backbones, with especially strong gains on small, occluded, and co-occurring objects; also provides practical Œ±/Œ≤ tuning guidance and robustness analyses."}, "weaknesses": {"value": "1.The experimental support is not commensurate with the claims: head-to-head comparisons against strong CD-style baselines (VCD/SID/DAMRO, etc.) are not matched for compute/tuning, and the evaluation suite omits widely used faithfulness metrics and human judgments.\n2.The method introduces a second branch and gradient-based mask optimization at inference, yet wall-clock latency, extra FLOPs, memory, and length scaling (to long videos) are missing. This blocks assessment of deployability.\n3.The pipeline depends on detector/tracker seeds for object masks‚Äîthe very regimes claimed as strengths (small/occluded/co-occurring) are where detectors are most fragile. Robustness to detector noise is not quantified."}, "questions": {"value": "1.What is the end-to-end overhead (ms/query, extra FLOPs, peak GPU memory) relative to VCD/SID/DAMRO under matched decoding budgets?Please add a quality‚Äìlatency Pareto by sweeping Œ±/Œ≤ and mask-optimization steps, and a length-scaling study (e.g., 16‚Üí256 frames) with throughput trade-offs.\n2.Can you provide head-to-head comparisons where all CD-style baselines are re-tuned under the same protocol and validation budget (including Œ±/Œ≤-like sweeps)?Please include broader faithfulness evaluations (video-adapted CHAIR/POPE, a small human study) and report statistical significance (e.g., paired bootstrap/McNemar with CIs).\n3.How sensitive is performance to detector/tracker errors (missed/jittered boxes, ID-switches) and to the choice of detector?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zQ8BZz6cGa", "forum": "ioGQhr1lhZ", "replyto": "ioGQhr1lhZ", "signatures": ["ICLR.cc/2026/Conference/Submission23595/Reviewer_Kanx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23595/Reviewer_Kanx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727513973, "cdate": 1761727513973, "tmdate": 1762942727483, "mdate": 1762942727483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GeWu, a model-aware counterfactual data‚Äìbased contrastive decoding method for video-llms. The idea is to reduce hallucinations by generating counterfactual inputs guided by the model‚Äôs own gradients. Instead of using random noise like in previous contrastive decoding-based approach. Specifically, GeWu detects object regions with an additional detector (yolov11), optimizes soft masks based on the model‚Äôs feedback, and performs decoding using the original and masked videos as contrastive views. Experiments on 3 benchmarks show consistent gains over prior methods including Qwen2-VL and InternVL3."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important problem (hallucination in video-llms) and proposes a clear, structured inference-time framework.\n\n- The method is conceptually intuitive: using model feedback to find key evidence regions is a reasonable idea.\n\n- Experimental coverage is good, including several models and benchmarks, and the presentation is clear overall."}, "weaknesses": {"value": "- The reviwer believes one of the ciritical missing point is on the computational analysis. The proposed method adds an extra optimization loop during decoding, requiring gradient computation and object detection for every frame. This seems provoke significant computation overhead at inference time. Without the analysis on actual runtime, it is hard to verify the real effectivness when the decoding phase becomes much heavier. (+Most modern lmms avoid optimization process at decoding phase exactly because of this computational issue. The additional computation often outweighs the benefit. In this case, optimizing masks per frame using OD models seems particularly expensive, especially for long videos. The reviewer thinks this optimization appears to be a main limitation rather than a strength.)\n\n\n- The claim that ‚Äúexisting methods rely on random perturbations‚Äù seems outdated and oversimplified. A broader related work section or more accurate positioning is needed. A lot of CD-based decoding approach adopated various counterpart signal for self-reflection, self-refinement etc,.\n\n\n- While the counterfactual construction seems clear, the decoding part is almost identical to prior CD. The overall improvement may come mainly from stronger object-level masking rather than a fundamentally new decoding strategy. The contribution therefore feels incremental."}, "questions": {"value": "- How heavy is the optimization process in practice? How does it scale with video length and number of objects? \n\n- The tuning of alpha and beta is standard in CD literature. The analysis in Sec 5.4 only confirms known behaviors (large alpha penalizes more, small beta filters more). There is no new finding or deeper explanation of why these parameters interact with the proposed counterfactual mechanism. So what can potential readers really learn from this analysis?\n\n- For the mask optimization, how many steps are needed? What happens when the video is longer or contains many objects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "reVGUxYwXV", "forum": "ioGQhr1lhZ", "replyto": "ioGQhr1lhZ", "signatures": ["ICLR.cc/2026/Conference/Submission23595/Reviewer_iYsV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23595/Reviewer_iYsV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812053443, "cdate": 1761812053443, "tmdate": 1762942726632, "mdate": 1762942726632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Model-aware Counterfactual Data based Contrastive Decoding, a training-free inference method designed to mitigate hallucination in Video-LLMs. The core idea is to improve Contrastive Decoding by replacing generic random perturbations with model-aware counterfactual data. This data is constructed by leveraging the Video-LLM's internal feedback, specifically, by performing gradient ascent on a soft mask applied to the video. The optimization goal is to maximize the query reconstruction loss, thus generating an adversarial perturbation that selectively erases the critical visual cues necessary for the correct prediction. Experiments on diverse benchmarks and various Video-LLMs demonstrate the effectiveness of the proposed method in consistently reducing hallucination while maintaining or improving accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The primary motivation for generating counterfactual data is clear. By utilizing a gradient-ascent approach to learn adversarial perturbations, the method can directionally erase the critical information required by the Video-LLM. This results in a superior contrastive view compared to those generated via simple random noise, directly addressing a key limitation of prior CD methods.\n\n2. The overall framework, including counterfactual data augmentation, generation, and contrastive decoding, is thoughtfully adapted from image-based methods to the temporal and spatial complexities of video and Video-LLMs. The integration of object detection/tracking, soft mask generation, and joint object-level/frame-level masking effectively controls the visual cues across the spatio-temporal domain.\n\n3. The method is evaluated across multiple modern Video-LLMs and three distinct, challenging benchmarks, providing strong evidence of its effectiveness in mitigating hallucination."}, "weaknesses": {"value": "Major Concerns\n\n1. The method, while \"training-free,\" involves a gradient optimization step (gradient ascent) for every counterfactual sample during inference. This process, which requires backpropagation through the visual encoder and potentially the full LLM, introduces a significant computational overhead compared to standard decoding or simple random perturbations. It would be better to provide an explicitly quantification of this latency cost and a stronger justification for why this overhead is acceptable for real-time video processing. The \"training-free\" claim must be balanced against the increased inference time.\n\n2. The optimization seeks to find an optimal mask $r^*$ that maximizes the loss for a specific query $q$ and video $V$. It is a local optimization problem. Authors need to discuss or investigate the robustness of this optimization: Are the resultant counterfactuals truly the best at isolating the hallucination source, or could the optimization converge to spurious local maxima?"}, "questions": {"value": "1. The optimization maximizes the loss to construct the counterfactual. Is it possible for a strong perturbation (high loss) to destroy too much information, leading to an overly weak contrastive view that is unhelpful for decoding? Did the authors experiment with alternative loss functions or constraints that target minimal information removal to achieve a threshold loss (making the perturbation just strong enough)?\n\n2. The paper focuses on comparing GeWu to VCD and SID. Could GeWu's model-aware counterfactual data generation module be integrated with other advanced decoding strategies (e.g., self-consistency methods) to achieve further performance gains? A brief discussion on this potential synergy would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zfGDiH5UQK", "forum": "ioGQhr1lhZ", "replyto": "ioGQhr1lhZ", "signatures": ["ICLR.cc/2026/Conference/Submission23595/Reviewer_Jp5V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23595/Reviewer_Jp5V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989594393, "cdate": 1761989594393, "tmdate": 1762942726375, "mdate": 1762942726375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}