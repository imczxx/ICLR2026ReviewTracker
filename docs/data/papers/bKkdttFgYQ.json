{"id": "bKkdttFgYQ", "number": 13729, "cdate": 1758221677815, "mdate": 1759897416954, "content": {"title": "SEEING THROUGH LANGUAGE: HOW TEXT REVEALS OBJECT AND STATE BIAS IN VLMS", "abstract": "Vision-Language models (VLMs) have demonstrated strong performance across a variety of multimodal benchmarks though not without internal biases. Little is known about how VLMs balance sensitivity to object identity versus object state. In this work, we systematically investigate object-state bias in VLMs by evaluating a broad set of models spanning diverse architectures and sizes. To enable controlled analysis, we introduce the Benchmark for Biases in Objects and States (BBiOS) dataset containing objects in both their original and transformed states. Across a variety of experiments, we examine model performance on recognizing objects, states, and their interactions. Our results reveal a consistent object bias, where models reliably recognize object categories but struggle to accurately capture states. Furthermore, attempts to steer models toward greater state sensitivity through prompting or injecting oracle information yield only marginal improvements. These findings highlight a fundamental limitation in current VLMs, suggesting that different training strategies or architectural innovations are required to reduce object-state bias in multimodal reasoning.", "tldr": "", "keywords": ["Computer Vision", "Vision Language models", "Object State Bias", "dataset and benchmark", "model bias"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88a8a07a58648933af31fad9a0789a820f9b0b1f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents an empirical study of VLM object and state recognition capabilities. To this extend, the authors introduce a novel benchmark (BBiOS) for the measurement of the introduced \"object-state bias\".  The benchmark dataset contains images of different vegetables and fruits at various states of processing (like raw, peeled, sliced, fried ...). The benchmark task is then to predict the object categories and their states. The experimental on this dataset evaluation shows that current (open) VLM models are better in predicting object categories than their state."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well written and easy to follow. The experiments use a large number of open VLMs"}, "weaknesses": {"value": "The reviewer strongly disagrees with the way the authors use the term \"bias\". While the investigation of all sorts of biases is an important ongoing topic, its inflationary usage as buzzword does not provide real insights. The conducted experiments show that VLMs are better in predicting categories than states (for a very specific dataset). It totally unclear how this represents a systematic model bias. While the formal definition of biases might be a bit weak, it always includes a clear relationship (dependency) between variables. The famous shape-texture bias for example, shows a trade-off between texture and shape information used by CNNs. A racial bias leads to model preferences towards people of a certain skin color (at expense of others). In the presented case, it remains unclear how the ability of models to predict object categories would limit the state predictions.\n\nTaking away the bias claim, the paper simply shows that VLMs are better in predicting fruit types than fruit states - this is not especially surprising since the state prediction task is much more complicated (given the variance of states in images)."}, "questions": {"value": "* What is your definition of BIAS ?\n* the paper only shows results for open VLM models. How do state of the art commercial models like GPT-5 behave?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 0}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XpWP0QhjoX", "forum": "bKkdttFgYQ", "replyto": "bKkdttFgYQ", "signatures": ["ICLR.cc/2026/Conference/Submission13729/Reviewer_x27o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13729/Reviewer_x27o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555409812, "cdate": 1761555409812, "tmdate": 1762924271667, "mdate": 1762924271667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission studies the object-state bias in vision-language models (both dual-encoder and autoregressive). The results are based on the collected and annotated BBiOS dataset, containing 16 kitchen objects with 4-6 states each (including one raw state) from a total of 8 states. A study on 23 different models then evaluates the object-state bias in multi-task and forced-choice settings and reveals that 1) models struggle at state recognition as compared to objects; 2) are biased toward objects; and 3) are mostly steerable in the state direction and sometimes not at all."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Introduces new “Bias in Object State (BBiOS)” dataset based on kitchen ingredients (derived from VidOSC) consisting of 16 objects and 4-6 states each (including one raw state) from a total of 8 states to evaluate object and state bias claiming a balanced coverage across object-state pairs\n- Bias investigation is systematically conducted in multi-task and forced choice settings\n- Evaluation of 23 diverse VLMs\n- Prompt-based steering is evaluated and shown to less effective than shown in previous studies on other cues"}, "weaknesses": {"value": "- Previous bias studies have been based around the methodology that all cues are easily individually recognizable. Yet, models seem to struggle to detect state (as also shown by Newman et al. (2024))–this may introduce a confounder, i.e., models may not be biased–they simply do not recognize the correct state (and then potentially hallucinate the state or fall back to object-only responses). \n- Prompts for “steering” are not tuned; in general no prompt seems to be tuned which may bias the study. For example, the terms \"state\" and \"object\" may be simple poorly aligned in VL spaces. It would be good to ablate a few other prompts to show that the findings are not limited by the choice of prompt.\n- The prompt template for LLMs is not properly described (Sec. 3.3.2 is lacking details). A few examples would be helpful.\n- Also, it is not described how LLM responses are sampled. Using non-greedy sampling may have introduced significant error and mandates a statistical analysis of error between sampled responses in parallel.\n- If LLMs are poor at state recognition (Newman, 2024) then they should also introduce an error in the data curation pipeline (L192ff)\n- Non-uniform distribution of object/state and  the paper is missing a heatmap two show the joint frequency of object-state pairs (or alternatively numbers/ratios in Table 1)\n- The models are poorly documented. Please precisely name the checkpoint (e.g, there are multiple SigLIP variants, Qwen – 10B is probably Qwen-Image-10B etc.), weight source, and inference resolution (transformations) \n\nMinor:\n- Figures are not vector graphics and blurry\n- LLaVa -> LLaVA (L99)\n- Unnecessary parentheses around citep in intro and elsewhere\n- Inconsistent reference capitalization (Fig/fig, e.g, L213; sometime abbreviated or not)"}, "questions": {"value": "- Does in-context learning or chain-of-thought change any of the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iMsrCQa6P0", "forum": "bKkdttFgYQ", "replyto": "bKkdttFgYQ", "signatures": ["ICLR.cc/2026/Conference/Submission13729/Reviewer_qmb8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13729/Reviewer_qmb8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869828406, "cdate": 1761869828406, "tmdate": 1762924271306, "mdate": 1762924271306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates object–state bias in Vision–Language Models (VLMs), where models tend to recognize object identity (e.g., \"apple\") more accurately than object state (e.g., \"sliced\", \"peeled\"). The authors introduce the Benchmark for Biases in Objects and States (BBiOS), a curated image dataset of 16 kitchen objects across eight states, collected semi-automatically from VidOSC using LLaMA-3.1 and CLIP for frame selection. The study designs two experimental paradigms—Multi-Task (object/state prediction with or without conditioning) and Forced-Choice (object vs. state classification)—and evaluates 23 diverse VLMs, including CLIP, BLIP, Qwen2-VL, InternVL, and LLaVA families. Results show consistent bias: models achieve high accuracy for object identity but struggle with state recognition, even under oracle conditioning or steering prompts. The paper concludes that object bias is structural and arises from training data and representation design rather than prompting limitations ."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A well-constructed dataset focusing on controlled object–state variations in realistic visual contexts.\n2. Comprehensive evaluation across 23 models spanning diverse architectures and parameter scales.\n3. Consistent empirical evidence showing strong object bias and limited steerability through text prompts or oracle information.\n4. Thoughtful discussion linking dataset composition, linguistic priors, and multimodal representations."}, "weaknesses": {"value": "1. The contribution is primarily diagnostic. The paper identifies object bias but does not analyze its causal origin in vision encoders, textual priors, or training objectives.\n2. The dataset size is small for large-scale model evaluation, making the generalization of conclusions uncertain.\n3. The evaluation lacks human baselines or psychometric reliability checks to contextualize bias severity.\n4. The analysis does not disentangle dataset bias (frequency imbalance) from representation bias (model internal weighting).\n5. The paper reuses known conceptual framing (object vs. state) without deeper theoretical grounding—similar trends have been observed in ChangeIt-Frames and recent multimodal perception studies [1–3].\n\n[1] Newman et al., “Do Pre-Trained Vision-Language Models Encode Object States?” arXiv 2024. \\\n[2] Y. Fu et al., “BLINK: Multimodal Large Language Models Can See but Not Perceive,” ECCV 2024. \\\n[3] Kawaharazuka et al., “Continuous Object State Recognition for Cooking Robots,” IEEE RAL 2024."}, "questions": {"value": "1. How does the bias magnitude correlate with visual encoder scale or architecture type (ViT-L vs. Swin)?\n2. Do object–state biases persist if the textual prompt explicitly disambiguates the state (e.g., “a peeled apple on a plate”)?\n3. Could linear probing or concept subspace analysis (e.g., CAV [4]) help localize the state-sensitive directions in the representation?\n4. How balanced is the BBiOS dataset in terms of background and lighting? Could visual confounds explain part of the state gap?\n5. Is there evidence that models trained on state-rich datasets (e.g., Something-Something V2) show reduced object dominance?\n\n[4] B. Kim et al., “Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV),” ICML 2018."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oJGMfFN7MK", "forum": "bKkdttFgYQ", "replyto": "bKkdttFgYQ", "signatures": ["ICLR.cc/2026/Conference/Submission13729/Reviewer_UCUV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13729/Reviewer_UCUV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942076463, "cdate": 1761942076463, "tmdate": 1762924270904, "mdate": 1762924270904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to analyze the sensitivity of vision-language models (VLMs) to object identity and state. To achieve this, the paper introduces a dataset containing objects in both their original and transformed states. Across a variety of experiments, the paper reveals that VLMs can reliably recognize object categories but struggle to accurately infer states. Moreover, steering models toward greater sensitivity via prompting or injecting oracle information yields marginal improvements. These findings highlight a fundamental limitation in current VLMs. Different training strategies or architectures may be needed to reduce the object-state bias."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a new dataset for analyzing object biases in VLMs.\n- Experiments cover a wide range of VLMs with different architectures and different scales, revealing that the object bias is prevalent and exists across almost all models.\n- The paper explores mitigation strategies via prompting and reveals that prompt engineering does not fundamentally solve the object bias in VLMs."}, "weaknesses": {"value": "- The proposed dataset is relatively small and restricted to a narrow domain, primarily focusing on kitchen-related objects and activities. This setup limits the dataset’s representativeness and generalizability to broader real-world scenarios.\n\n- The paper does not provide substantial new insights for explaining or addressing bias in VLMs. The discussion attributing object bias to the models’ latent state representations and their training data remains rather general and high-level, without offering concrete empirical evidence."}, "questions": {"value": "Line 193: How do you use a large language model, which does not support images, to do image analysis and select frames?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bTEtzlKOX0", "forum": "bKkdttFgYQ", "replyto": "bKkdttFgYQ", "signatures": ["ICLR.cc/2026/Conference/Submission13729/Reviewer_vgMx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13729/Reviewer_vgMx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136489303, "cdate": 1762136489303, "tmdate": 1762924270407, "mdate": 1762924270407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}