{"id": "QBvxXzHdZx", "number": 8524, "cdate": 1758089375621, "mdate": 1759897778579, "content": {"title": "ATLAS: Alibaba Dataset and Benchmark for Learning-Augmented Scheduling", "abstract": "Learning-augmented scheduling uses ML predictions to improve decision-making under uncertainty. Many algorithms in this class have been proposed with better theoretical guarantees than the classic methods. Translating these theoretical results into practice, however, requires an understanding of real workloads. Such an understanding is hard to develop because existing production traces either lack the ground-truth processing times or are not publicly available, while synthetic benchmarks fail to represent real-world complexity. We fill this gap by introducing *Alibaba Trace for Learning-Augmented Scheduling (ATLAS)*, a research-ready dataset derived from Alibaba's Platform of Artificial Intelligence (PAI) cluster trace—a production system that processes hundreds of thousands of ML jobs per day. The ATLAS dataset has been cleaned and features engineered to represent the inputs and constraints of non-clairvoyant scheduling, including user tags, resource requests (CPU/GPU/memory), and job structures with ground-truth processing times. We develop a prediction benchmark reporting prediction error metrics, along with feature importance analysis, and introduce a novel multiple-stage ML model. We also provide a scheduling benchmark for minimizing the total completion time, max-stretch, and makespan. ATLAS is a reproducible foundation for researchers to study learning-augmented scheduling on real workloads, available at https://anonymous.4open.science/r/non-clairvoyant-with-predictions-7BF8/.", "tldr": "", "keywords": ["Scheduling with predictions", "Dataset and benchmark", "Machine learning", "Learning augmented scheduling", "Non-clairvoyant scheduling"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/395fd67f85cf060c5751a31bf6b54b1255033d78.pdf", "supplementary_material": "/attachment/ff4410932894f5de6b2058dc14b193c32c5860f7.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a new real-world dataset for evaluating algorithms with predictions. In particular, the paper addresses non-clairvoyant scheduling, where job sizes are unknown to a scheduler. Instead, a scheduler has access to predictions about the job, for example, the job's processing times. Non-clairvoyant scheduling belongs to one of the most studied online problems within the area of algorithms with predictions. However, many results are only of theoretical nature, and it is unclear whether these algorithm bring a benefit in practice.\n\nThe paper proposes a dataset based on real world data of Alibaba's datacenter and preprocesses that for evaluating such algorithms. There are two main components:\n\n1. The main component is the dataset itself. It is very diverse and comes with different labels.\n2. The second component is a ready-to-use benchmark that already implements baselines such as simple learning-augmented algorithms and optimum solutions. It also comes with benchmark prediction models that can be used for generating e.g. job size predictions. Considered objectives are total completion time, maximum stretch and makespan\n\nThe paper gives characterizations of the dataset, a detailed explanation of the benchmark suite, and an evaluation of multiple learning-augmented and classic algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- To the best of my knowledge, this is the first attempt to introduce a benchmark for learning-augmented scheduling algorithms, which seems to be overdue. In the past, there often has been critique that learning-augmented algorithms are evaluated on synthetic data. Thus, I think this benchmark suite is a good contribution for the field and creates a bridge to practical applications.\n- The benchmark considers different objectives to capture a large variety of problems.\n- The dataset and benchmark are well-structured: a proper feature engineering pipeline, empirical-Bayes shrinkage, and conformal calibration.\n- The authors show attention to reproducibility."}, "weaknesses": {"value": "- The introduction is slightly dense, it assumes that readers are familiar with learning-augmented algorithms; also the statements about the different algorithm are a bit confusing in my opinion. I would like to see a revised introduction in the camera-ready version.\n- One could argue that the contribution of a dataset is not sufficient for a ML theory conference. However, since benchmarks are rather novel for learning-augmented algorithms, I would not value this too much.\n\nI would be happy to see the paper accepted, but would not fight for it."}, "questions": {"value": "- L25 mutli-stage?\n- L33: whitespace missing\n- L35: \"non-clairvoyant scheduling has a larger total completion time\" what do you mean here?\n- L105 \"ALTAS\" -> \"ATLAS\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OJUJYU6I8L", "forum": "QBvxXzHdZx", "replyto": "QBvxXzHdZx", "signatures": ["ICLR.cc/2026/Conference/Submission8524/Reviewer_shox"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8524/Reviewer_shox"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760760190988, "cdate": 1760760190988, "tmdate": 1762920386966, "mdate": 1762920386966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ATLAS, a large-scale dataset of over 730,000 jobs from Alibaba's PAI production cluster, which addresses the lack of public scheduling datasets containing ground-truth processing times. The dataset is specifically designed for non-clairvoyant scheduling research by strictly using only features available at submission time, and the authors build the LASched benchmark on top of it. LASched provides implementations and evaluations for both job size prediction tasks (using baselines like CQR and Meta-stacking) and scheduling tasks (using algorithms like SPJF and LPPT), serving as a reproducible foundation for comparing algorithms on real-world workloads."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper convincingly argues that existing datasets for scheduling research are insufficient, as they either lack ground-truth processing times or are not publicly available. ATLAS directly fills this gap with a large-scale, public trace from a real-world production system.\n\n* The dataset provides complete ground-truth job sizes, a critical feature missing from other traces. It is rigorously prepared as a non-clairvoyant dataset, strictly excluding post-execution metrics to prevent information leakage and ensure it is research-ready.\n\n* The authors provide LASched, a full benchmark built on ATLAS that defines both prediction and scheduling tasks. This includes implementations and evaluations for multiple prediction models and scheduling algorithms , establishing a strong baseline for future work."}, "weaknesses": {"value": "* Maybe I'm wrong but there seems to be a discrepancy between the complexity of the data source and the simplicity of the scheduling benchmark. The ATLAS dataset is sourced from a large scale, heterogeneous production environment with over 1,800 machines and complex constraints like gang scheduling. However, the LASched benchmark evaluates key metrics, such as total completion time and max-stretch, using a simplified single machine setting. This simplification might obscure the real world impact of prediction errors, which could be amplified in a complex, multi machine environment with heterogeneous resources and stricter constraints.\n\n* The dataset's two month time span limits its ability to capture long term temporal dynamics. While the trace includes over 730,000 jobs and exhibits clear patterns, a two month window is likely insufficient to model longer cycles, such as quarterly workload peaks or concept drift caused by major software framework updates. The paper does propose models to address drift, but the dataset itself may not provide enough data for researchers to fully validate the robustness of scheduling algorithms against these important long term workload evolutions."}, "questions": {"value": "* Since some metrics are measured on a single machine, how do you envision the benchmark evolving to incorporate real-world complexities, and what new challenges might arise when evaluating learning-augmented algorithms in that more realistic setting?\n\n* Do you have plans to release an extended version of ATLAS, perhaps covering a full year, or how would you recommend researchers use this dataset to develop models that are robust to such long-term changes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hSfpc0lZ26", "forum": "QBvxXzHdZx", "replyto": "QBvxXzHdZx", "signatures": ["ICLR.cc/2026/Conference/Submission8524/Reviewer_tfJj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8524/Reviewer_tfJj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761150943927, "cdate": 1761150943927, "tmdate": 1762920386537, "mdate": 1762920386537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper publishes a “plug-n-play” data set for training job size predictors in the context of non-clairvoyant scheduling, a practically-relevant problem that has been extensively studied in the literature on learning-augmented algorithms.  The authors create ATLAS, a data set of real workloads distilled from Alibaba’s PAI trace.  Furthermore, they provide a benchmark environment called LASched that provides a common environment for evaluating learning-augmented schedulers, and give example prediction methods that perform well in this environment.  The paper is released alongside an anonymized repo that contains the data set, simulator code, and code for the example ML models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The underlying problem (non-clairvoyant scheduling with job size predictions) is highly relevant in practice.   Providing a benchmark tool (LASched) alongside the data set will be appreciated by the community working on this problem as a way to speed up evaluation of new techniques and establish a common environment in which proposed scheduling algorithms can be directly compared. \n\nIn developing their own models for predicting job sizes, the authors additionally develop a feature engineering schema that is “ready to go” for other researchers developing new model architectures for the job size prediction task — they also show that these engineered features are effective for the ATLAS data set by proposing and evaluating their own predictors that perform well."}, "weaknesses": {"value": "The Alibaba PAI data set that provides all of the underlying data for ATLAS has been publicly available since 2022 — ATLAS is simply a sensible cleaned and processed version of this data set.  This weakness is a mild one since the paper also provides a new benchmark and simulator environment that is of equal value to the community working on this problem.\n\nWhile not necessarily expected from a paper in the datasets track, the example models trained using the data set are only evaluated with a fixed set of features and parameters — it would be good to see, for example, an ablation study of including/excluding certain features."}, "questions": {"value": "Can you speak more to the decision to provide the predictor with information about the user and group?  I would typically expect that including these features would bias the model towards overfitting to the data in a way that may not be generalizable (e.g., to a different scheduling context with similar workloads but completely different users).  How many users/groups exist in the underlying trace, and are these literally tied to individual human beings, or are they tied to frameworks such as TensorFlow?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aEQ3KPRuLO", "forum": "QBvxXzHdZx", "replyto": "QBvxXzHdZx", "signatures": ["ICLR.cc/2026/Conference/Submission8524/Reviewer_ufpH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8524/Reviewer_ufpH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761413974046, "cdate": 1761413974046, "tmdate": 1762920385609, "mdate": 1762920385609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Alibaba Trace for Learning-Augmented Scheduling (ATLAS), a research-ready dataset derived from Alibaba’s Platform of Artificial Intelligence (PAI) cluster trace—a production system that processes hundreds of thousands of ML jobs per day. The ATLAS dataset has been cleaned and features engineered to represent the inputs and constraints of non-clairvoyant scheduling, including user tags, resource requests (CPU/GPU/memory), and job structures with ground-truth processing times. This paper develops a prediction benchmark reporting prediction error metrics, along with feature importance analysis, and introduces a novel multiple-stage ML model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a real-world dataset and a standardized benchmark, which are valuable for studying learning-augmented scheduling in practical environments."}, "weaknesses": {"value": "1.\tPlease clarify whether each job exclusively occupies its allocated resources or whether multiple jobs can coexist on the same resource group (e.g., node or GPU). This is important for understanding the dataset’s applicability to scheduling policies that handle interference and resource sharing.\n\n2.\tThe trace seems to be collected under a specific scheduling policy. Please clarify what scheduler was used during data collection, and discuss whether this underlying policy could bias the dataset or affect the quality of downstream learned schedulers trained on this trace.\n\n3.\tThe paper should elaborate on how preemptive scheduling is represented in the dataset. Are both preemptive and non-preemptive execution cases included? If so, how is preemption recorded and labeled?"}, "questions": {"value": "1.\tPlease clarify whether each job exclusively occupies its allocated resources or whether multiple jobs can coexist on the same resource group (e.g., node or GPU). \n\n2.\tPlease clarify what scheduler was used during data collection, and discuss whether this underlying policy could bias the dataset or affect the quality of downstream learned schedulers trained on this trace.\n\n3.\tAre both preemptive and non-preemptive execution cases included? If so, how is preemption recorded and labeled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "08jUcE8WrC", "forum": "QBvxXzHdZx", "replyto": "QBvxXzHdZx", "signatures": ["ICLR.cc/2026/Conference/Submission8524/Reviewer_aucU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8524/Reviewer_aucU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834142442, "cdate": 1761834142442, "tmdate": 1762920384630, "mdate": 1762920384630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ATLAS, a dataset derived from Alibaba's artificial intelligence platform cluster trace that processes many thousands of machine learning jobs per day. The dataset contains about 730K jobs with actual processing times in addition to many other metrics along with LASched, a benchmark for learning augmented scheduling. The work specifically addresses the gap between theoretical advances in learning augmented scheduling with the evaluation of those algorithms on real workloads. Specifically, they develop a procedure to reproducibly and comprehensively evaluate scheduling algorithms with clear train test splits. across three commonly used scheduling metrics including total completion time, max stretch and makespan."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The scale of the dataset (730K) jobs and the vast metadata about the details of each job and resource it runs on make it a valuable contribution to the practical evaluation of learning augmented scheduling algorithms.\n- Moreover, to the best of my knowledge, they seem to be the first to enforce leakage safe construction, that is only information that an actual scheduler would have, for the evaluation of their algorithms which is critically important for rigorous evaluation.\n- Multiple scheduling objectives that are commonly used in the literature are evaluated on.\n- Provide an easy to use simulator to evaluate various learning augmented scheduling algorithms."}, "weaknesses": {"value": "- Excessive mathematical formalism and notation that is not used much after introduction. On page 4 and 5 for example, a fair bit of mathematical notation is introduced that would not look out of place in a scheduling paper with theoretical results. However, the notation is only used sparingly in pages 4, 5 and Algorithm 1 and there are no proofs in the paper that would benefit from such formal notation. If the notation was simplified the readability of the paper would significantly improve.\n- The authors introduce a really powerful scheduling benchmark and evaluate on various algorithms but do not really discuss their performance in detail and why certain algorithms perform better than others.\n- The authors do not really comprehensively evaluate the connection between prediction of the job duration and the actual scheduling algorithm used."}, "questions": {"value": "- A major suggestion I have for the authors would be to simplify and/or reduce the amount of mathematical notation used here. While I understand that a major audience is the learning augmented scheduling researchers who tend to be more theoretical, in my personal opinion the notation here reduces accessibility to broader audiences (those implementing these systems in industry for instance) that this paper could impact.\n- Another suggestion would be to really explain and elaborate why certain algorithms and job duration methods seem to perform better than others. There seems to be a disconnect in the writing between actually predicting the job duration versus the performance of the algorithms (competitive ratios). I could imagine that small changes in prediction could have large algorithmic impacts or even vice versa. Clearly characterizing this with a more in depth analysis connecting these two aspects would seriously improve the impact of the paper both for algorithm developers and those actually implementing schedulers in industry.\n- Maybe I missed this but I don't think I see results showing that you used the various job duration prediction methods on all of the scheduling algorithms listed. Adding these results, even if it is only to the appendix would really help out both algorithm developers and those in industry.\n- Given the scale and value of the dataset, I think it is reasonable for the authors to clearly explain what the holes are with current methods and suggest avenues/algorithmic directions for improvement. This seems low effort but would positively impact the community and improve uptake of this work.\n\nIf my concerns are addressed, I'm very inclined to increase my score. I think this paper adds substantial value to the community and if some of my questions are answered then it could have greater impact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EZHws0CIv1", "forum": "QBvxXzHdZx", "replyto": "QBvxXzHdZx", "signatures": ["ICLR.cc/2026/Conference/Submission8524/Reviewer_Lijb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8524/Reviewer_Lijb"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853645483, "cdate": 1761853645483, "tmdate": 1762920384222, "mdate": 1762920384222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}