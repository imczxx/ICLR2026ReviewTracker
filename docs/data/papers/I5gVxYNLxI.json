{"id": "I5gVxYNLxI", "number": 11053, "cdate": 1758188248581, "mdate": 1759897612044, "content": {"title": "Constructing coherent spatial memory in LLM agents through graph rectification", "abstract": "Given a map description through global traversal navigation instructions (e.g., visiting each room sequentially with action signals such as north, west, etc.), an LLM can often infer the implicit spatial layout of the environment and answer user queries by providing a shortest path from a start to a destination (for instance, navigating from the lobby to a meeting room via the hall and elevator). However, such context-dependent querying becomes incapable as the environment grows much longer, motivating the need for incremental map construction that builds a complete topological graph from stepwise observations. We propose a framework for LLM-driven construction and map repair, designed to detect, localize, and correct structural inconsistencies in incrementally constructed navigation graphs. Central to our method is the Version Control, which records the full history of graph edits and their source observations, enabling fine-grained rollback, conflict tracing, and repair evaluation. We further introduce an Edge Impact Score to prioritize minimal-cost repairs based on structural reachability, path usage, and conflict propagation. To properly evaluate our approach, we create a refined version of the MANGO benchmark dataset by systematically removing non-topological actions and inherent structural conflicts, providing a cleaner testbed for LLM-driven construction and map repair. Our approach significantly improves map correctness and robustness, especially in scenarios with entangled or chained inconsistencies. Our results highlight the importance of introspective, history-aware repair mechanisms for maintaining coherent spatial memory in LLM agents.", "tldr": "LLM-MapRepair is a framework that detects and fixes structural errors in navigation graphs built by LLMs.", "keywords": ["Large Language Model", "Spatial Reasoning", "LLM Reasoning", "multi agent"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cae761c2ac41a977d2031601f8d0632edde25b85.pdf", "supplementary_material": "/attachment/a1dc55fd05d617b8e0463a4d68b9afd095bb31f5.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a novel framework, LLM-MapRepair, for enabling large language model (LLM) agents to build and maintain coherent spatial memory during textual navigation tasks.\nUnlike prior works that rely solely on LLM contextual reasoning, the proposed approach incrementally constructs a topological graph of the environment and introduces mechanisms for error detection, localization, and repair."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Unlike prior LLM-based navigation works that depend purely on context-window reasoning, this study formalizes a persistent, structured memory mechanism that records graph evolution and enables rollback and repair.\n2. Current LLMs cannot sustain consistent spatial understanding over long reasoning horizons due to context limits and forgetting.\nEmulating human-like incremental mapping and introducing self-repair mechanisms are convincingly argued as necessary steps for robust embodied reasoning."}, "weaknesses": {"value": "1. Evaluation is restricted to text-based static environments; lacks validation in dynamic or multimodal contexts.\n2. The link to embodied or real-world robotic tasks could be emphasized more clearly.\n3. The evaluation domain is narrow, so the generality of the proposed approach for other tasks (e.g., visual navigation, embodied dialogue) remains uncertain.\n4. The lack of definition or quantitative analysis of map scale. The paper never specifies how many nodes or edges each constructed navigation graph typically contains, nor does it provide any complexity analysis or scaling study.\nAs a result, it remains unclear how large a map the proposed framework can effectively handle before memory usage, repair cycles, or LLM context length become bottlenecks."}, "questions": {"value": "See Weaknesses*"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IBotXq6v7C", "forum": "I5gVxYNLxI", "replyto": "I5gVxYNLxI", "signatures": ["ICLR.cc/2026/Conference/Submission11053/Reviewer_eVg9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11053/Reviewer_eVg9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761454119816, "cdate": 1761454119816, "tmdate": 1762922229075, "mdate": 1762922229075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a framework for repairing navigation graphs constructed by LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The authors have a really great idea via-a-vis their framework which in addition to their untended use for repair, can also be used to diagnose problems in LLM navigation."}, "weaknesses": {"value": "*Figure 2 needs more explanation\n* the authors propose a framework and then run an experiment that is confusing and not clear what they hope to accomplish. I recommend that in future work, they focus on showing why LLMs perform best/works across their 3 sections and also do some sensitivity analyses on their Edge Impact Score, particularly in light of complexity.\n*They also mention they want to enable LLMs to handle complex long texts, but I would focus on this after they have a more stable approach."}, "questions": {"value": "*Why would someone need to use your framework?\n*how could the results be used for future improvements?\n*What are the limitations of your framework?\n*How sensitive is your approach/scores to prompt variability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "z4KzT3B3WW", "forum": "I5gVxYNLxI", "replyto": "I5gVxYNLxI", "signatures": ["ICLR.cc/2026/Conference/Submission11053/Reviewer_NmKs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11053/Reviewer_NmKs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781902871, "cdate": 1761781902871, "tmdate": 1762922228683, "mdate": 1762922228683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes LLM-MapRepair, a framework that enables large language models to build and maintain consistent spatial maps from textual navigation instructions. Instead of relying solely on context reasoning (which leads to memory limits and inconsistencies) the method incrementally constructs a topological graph and repairs it through three integrated stages: conflict detection, error localization, and version-controlled repair. A central contribution is the introduction of a Version Control mechanism that rcords every graph edit for rollback and causal tracing, and an Edge Impact Score that prioritizes low-risk repairs based on reachability and path usage. Evaluated on a refined version of the MANGO benchmark, the approach significantly improves map correctness and robustness, especially for long-horizon navigation with accumulated inconsistencies.The experiments highlight the importance of history-aware reasoning for maintaining coherent spatial memory in LLM agents"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper:\n- Clearly identifies and motivates the problem of inconsistent spatial memory in LLMs\n- Proposes a novel and inventive solution: a modular andd interpretable framework (LLM-MapRepair) for detecting and fixing map inconsistencies\n- Introduces Version Control for persistent, history-aware reasoning, enabling rollback and causal tracing\n- Defines an Edge Impact Score to prioritize low-risk, high-impact repairs using a principled heuristic\n- Provides strong experimental validation with clear ablations showing complementary effects of each module\n- Cleans and refines the MANGO benchmark, improving evaluation quality for future work\n- Some of the figures make the approach easier to follow and conceptually well-motivated"}, "weaknesses": {"value": "- The paper is hard to read in its current form. Some examples below: \n   - The paper uses not-so-common terms without defining them, e.g., “local spatial cognition,” “cognitive biases,” “structural integrity,” and “contextual pressure.”\n   - In the introduction, the solution appears before the setup: what exactly are nodes and edges? I was only able to figure it out after reading the methods section.\n- Problem formulation and evaluation criteria are not specified clearly.\n- The motivation for using LLMs (as opposed to simpler map builders) is not provided.\n- Evaluated on a single dataset, and method appears tailored to it.\n- No baseline comparisons beyond GPT-4o, making it difficult to assess the method’s relative performance.\n- The paper lacks a deeper analysis explaining the method’s behavior, limitations, or challenges in solving the problem."}, "questions": {"value": "- Why do we need an LLM for this mapping problem at all? What capabilities are essential here?\n- The paper motivates with \"human-like cognition\", but what evidence suggests human spatial memory is graph-based?\n- Fig. 1 mentions forgetting in the LLM context; why would information in-context be \"forgotten\"?\n- If graph errors stem from LLM reasoning, how does version control fix them? Maybe shortening the reasoning context? Please motivate more clearly.\n- L158: How does 1k observation steps exceed context? What's the average tokens per observation?\n- The conflict taxonomy is confusing; aren't directional conflicts fundamentally topological?\n- \"Delayed/entangled/silent\" conflicts are introduced, but how is it tied to the solution later?\n- Why is \"usage\" a factor in resolving edge conflicts if all conflicts will be resolved anyway? How is it computed? \n- Why assume the true error lies between detected conflicts and their LCA?\n- The paper mentions Conflict Revelation Gain, but replaces it with a score function defined earlier. Why is that a good approximation?\n- How are each individual component of the score computed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DO22uUnyqI", "forum": "I5gVxYNLxI", "replyto": "I5gVxYNLxI", "signatures": ["ICLR.cc/2026/Conference/Submission11053/Reviewer_Wcjx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11053/Reviewer_Wcjx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762019728401, "cdate": 1762019728401, "tmdate": 1762922228290, "mdate": 1762922228290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}