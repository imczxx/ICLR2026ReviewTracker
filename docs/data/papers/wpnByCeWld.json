{"id": "wpnByCeWld", "number": 19654, "cdate": 1758298006039, "mdate": 1763712835447, "content": {"title": "Machine Learning Methods for Wind Profile Recovery in the Atmospheric Boundary Layer", "abstract": "Reconstructing atmospheric boundary layer wind profiles is crucial for weather prediction and wind energy applications. We study the task of predicting vertical profiles of the zonal (east–west) and meridional (north–south) wind components from the Integrated Global Radiosonde Archive (IGRA), given geostrophic wind and auxiliary predictors such as season, time of day, temperature difference, and pressure. We propose several machine learning architectures for this task, including CatBoost, TabM, and FT-Transformer, against classical baselines such as the power-law profile and Monin–Obukhov similarity theory. On the IGRA dataset, modern ML models achieve reconstruction errors of about 1.5 m/s for both wind components, which is superior than analytical models.", "tldr": "", "keywords": ["Wind Prediction", "Climate", "Tabular Data", "Neural Operator"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a8c352e8f2feb45a19d96ce1d89254ab1289206a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a systematic empirical study on reconstructing atmospheric boundary layer (ABL) wind profiles from radiosonde data using modern machine learning methods.  The authors utilize the Integrated Global Radiosonde Archive (IGRA) dataset and compare three representative models (FT-Transformer, CatBoost, and TabM) against traditional analytical parameterizations such as the Monin–Obukhov similarity theory and power-law profiles.  The machine learning models achieve substantially lower RMSE than physical baselines, showing strong predictive performance across multiple stations and meteorological regimes.  The study also examines model interpretability and generalization across different geographical regions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Comprehensive experimental design. The study covers a large dataset (>150,000 soundings) with rigorous preprocessing, ensuring statistical robustness and physical consistency.\n\n+ Clear empirical contributions. Provides a thorough benchmarking of three different ML paradigms (transformer-based, gradient boosting, and tabular network) for wind profile reconstruction.\n\n+ Strong performance. All ML models outperform classical empirical parameterizations with consistent improvement in RMSE, correlation, and bias metrics."}, "weaknesses": {"value": "+ Limited novelty in methodology. The paper mainly applies existing machine learning architectures to a new domain.  There is no substantial methodological innovation beyond model comparison.\n+ Physical interpretability analysis. The “physical explanation” section primarily relies on residual statistics and feature importance plots.\nIt demonstrates post-hoc consistency but does not reveal why or how the models capture stability-dependent patterns. More rigorous interpretability would strengthen the scientific credibility.\n+ Feature selection rationale unclear. While the feature list includes temperature, humidity, and stability parameters, it is unclear how these were chosen or whether adding/excluding variables affects performance. A sensitivity analysis on feature subsets would help clarify the physical relevance of each predictor."}, "questions": {"value": "+ Could you clarify how training and testing splits were made? Were samples separated by time or by station to avoid spatial–temporal leakage in IGRA data? If random sampling was used, have you verified performance under stricter non-overlapping splits?\n+ How were the input features chosen? Did you test whether removing or adding certain variables (e.g., humidity, potential temperature gradient) significantly changes performance or alters the learned physical relationships?\n+ FT-Transformer, CatBoost, and TabM show different performance levels. Have you analyzed why certain models perform better? For example, does the transformer capture vertical dependencies that boosting models cannot?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BSKtCTcowO", "forum": "wpnByCeWld", "replyto": "wpnByCeWld", "signatures": ["ICLR.cc/2026/Conference/Submission19654/Reviewer_Srkr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19654/Reviewer_Srkr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551396315, "cdate": 1761551396315, "tmdate": 1762931506114, "mdate": 1762931506114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "NUiV0eniXw", "forum": "wpnByCeWld", "replyto": "wpnByCeWld", "signatures": ["ICLR.cc/2026/Conference/Submission19654/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19654/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763712834545, "cdate": 1763712834545, "tmdate": 1763712834545, "mdate": 1763712834545, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper compares three data-driven approaches, namely CatBoost, FT-Transformer, and TabM, with two non-learning-based baselines for the task of wind profile reconstruction in the boundary layer. It is shown that the ML methods perform better and their performance is the best when train and test area coincide."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The methods used and compared are sound choices and work well together with the selected features."}, "weaknesses": {"value": "The contribution is relatively low. Although this is interesting, I don't think ICLR is the right community for this. There aren't any new insights, as the results are exactly as expected. Additionally, the choice of methods isn't really explained. I wouldn't consider the transformers necessarily standard for this setup, and I am missing a linear regression (or some other simple data-driven) model as a baseline. I'm sure applied climate scientists might be interested, though."}, "questions": {"value": "What is the key takeaway from your results for people trying to develop better methods or improve upon foundation models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SLwpyOAKZG", "forum": "wpnByCeWld", "replyto": "wpnByCeWld", "signatures": ["ICLR.cc/2026/Conference/Submission19654/Reviewer_an3o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19654/Reviewer_an3o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837624785, "cdate": 1761837624785, "tmdate": 1762931505546, "mdate": 1762931505546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper compares the performance of modern machine learning models—FT-Transformer, TabM, and CatBoost—with classical analytical parameterizations such as the Power-Law Profile and Monin-Obukhov Similarity Theory (MOST) in the task of reconstructing vertical profiles of horizontal wind in the atmospheric boundary layer (ABL). The study is based on the Integrated Global Radiosonde Archive (IGRA) dataset and evaluates the impact of different input feature configurations and training strategies on model accuracy and geographical generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive and Well-Chosen Baselines: The paper clearly compares three state-of-the-art tabular machine learning architectures (FT-Transformer, TabM, CatBoost) against two well-established meteorological analytical models. The selection of baselines is convincing and appropriate for the task.\n\n2. Clear Physical Context: The paper clearly distinguishes between modeling wind profiles in the boundary layer versus the free troposphere and provides a physically sound explanation for the observed error distribution (larger errors near the surface than at higher altitudes)."}, "weaknesses": {"value": "1. Application-Oriented Contribution: Although the results are compelling, the paper does not introduce algorithmic innovations and falls within the scope of applied research. As such, the depth of data and experimental analysis becomes critical in supporting its impact.\n\n2. Lack of Detailed Physical Mechanism and Case Studies: While the aggregate statistical results (e.g., mean RMSE) demonstrate the superiority of ML methods, they do not sufficiently explain why these methods perform better under specific meteorological conditions. It is recommended to include detailed case studies under different meteorological regimes—such as stable boundary layers, unstable boundary layers, or nocturnal low-level jets—to provide deeper physical insight.\n\n3. Limitations in Global Experimental Analysis: While the global model and cross-location validation demonstrate generalization capability, using only maximum RMSE as a generalization metric is somewhat limited. A more detailed assessment could be achieved by analyzing the performance of ML models across different climate zones (e.g., tropical, temperate, polar) or surface roughness types (e.g., ocean, urban, flat terrain), which would better evaluate their physical consistency."}, "questions": {"value": "1. Data Availability: The \"Reproducibility Statement\" mentions that data are sourced from the NOAA NCEI IGRA archive with a provided access link. Could the authors confirm whether they plan to release the preprocessed and filtered version of the dataset—specifically the final subset used for training and testing (e.g., the 158,097 radiosonde samples) along with all derived diagnostic features—upon paper acceptance?\n\n2. Physical Consistency Analysis: Have the authors conducted further physical consistency checks on the wind profiles predicted by the ML models? For instance, could they provide a comparison of wind shear predicted by ML models versus classical models (e.g., Power-Law) or observations under different stability conditions, to demonstrate that the ML predictions are not only accurate but also physically plausible in their trends?\n\n3. Computational Efficiency: Given the significant differences in training and inference costs among CatBoost, FT-Transformer, and TabM could the authors provide a brief latency comparison (inference time) to assess the practicality of these models in real-time numerical forecasting or wind energy applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aEODtkdNPM", "forum": "wpnByCeWld", "replyto": "wpnByCeWld", "signatures": ["ICLR.cc/2026/Conference/Submission19654/Reviewer_oD8c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19654/Reviewer_oD8c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998468497, "cdate": 1761998468497, "tmdate": 1762931505047, "mdate": 1762931505047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors trained three models (CatBoost, TabM, and FT-Transformer) to make predictions of vertical wind profiles based on the Integrated Global Radiosonde Archive. They achieved a high reconstruction accuracy, outperforming common analytic approximations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper tackles a classically analytic task with a data-driven approach and reports improved accuracy over a simple analytical approximation. The three main conclusions (lower reconstruction error, the importance of a well-chosen predictor set, and the ability of ML models to generalize) are intuitively sound and consistent with the reported experiments. The work underscores that incorporating relevant physical predictors materially impacts performance, aligning the results with domain intuition. Overall, the empirical findings appear coherent with the setup and offer a practical update to an existing prediction mechanism in meteorology."}, "weaknesses": {"value": "- The relevance to the ICLR community is unclear. Using machine learning to predict physical phenomena is well-established; the results and takeaways do not seem surprising.\n- The analytical baseline (power-law model) appears arbitrary. A stronger, more comparable physical baseline (e.g., the Åkerblom–Ekman model using radiosonde data) seems more appropriate; as stated, the improvement over the chosen baseline is trivial by the paper’s own framing.\n- The paper does not explain why the selected ML models are the right fit for this use case, nor does it connect the choices to prior studies in similar settings.\n- The experimental setup section is brief and omits rationale for key parameter choices. Important details (feature selection strategy, training protocol, hyperparameters, seeds, splits) are insufficiently documented to support reproducibility or to interpret generalization claims.\nPredictor set and ablations: While the importance of the predictor set is claimed, the paper lacks systematic ablations that would quantify each predictor’s contribution and clarify trade-offs.\n- The purpose and relevance of several figures (notably Figs. 1-3) to the core experiment are vague; they read as illustrative rather than evidentiary.\n- The results hint at potentially extracting an eddy-viscosity coefficient profile from radiosonde data (which is stated to be useful yet not reliably known), but the paper does not pursue or analyze this direction.\n- Heavy use of domain-specific jargon and abbreviations, limited contextual linking between sections, and inconsistent formatting of references/symbols hamper readability and assessment."}, "questions": {"value": "- Why was the power-law model selected as the analytical baseline? How would results compare against a stronger physical baseline such as the Åkerblom–Ekman model that also uses radiosonde data?\n\n- What motivated the specific ML models used? Were they chosen due to prior success in similar meteorological tasks, computational constraints, or theoretical alignment with the problem?\nPredictor importance: How was the predictor set determined? Can you provide ablations (or SHAP/feature permutation analyses) quantifying each predictor’s contribution?\n\n- What exact train/validation/test splits were used (e.g., across time, sites, or atmospheric regimes)? Do models trained in one context transfer to others, and what is the out-of-distribution behavior?\n\n- Please specify preprocessing, hyperparameters, optimization settings, random seeds, and the total number of runs. Are results averaged over multiple seeds with confidence intervals or statistical tests?\n\n- What specific hypotheses or claims do these figures support? Could they be streamlined or moved to the appendix if they are primarily illustrative?\n\n- Given the findings, can the eddy-viscosity coefficient profile be estimated from radiosonde data? If not, what limits this and what additional data or assumptions would be required?\n\n- Will code, data splits, and scripts to reproduce the tables and figures be released?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OtOeZXQuLY", "forum": "wpnByCeWld", "replyto": "wpnByCeWld", "signatures": ["ICLR.cc/2026/Conference/Submission19654/Reviewer_DLwT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19654/Reviewer_DLwT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008424080, "cdate": 1762008424080, "tmdate": 1762931504575, "mdate": 1762931504575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}