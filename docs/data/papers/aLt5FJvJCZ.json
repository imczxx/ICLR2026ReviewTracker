{"id": "aLt5FJvJCZ", "number": 13908, "cdate": 1758224829215, "mdate": 1759897404421, "content": {"title": "A Bi-metric Framework for Efficient Nearest Neighbor Search", "abstract": "We propose a new ``bi-metric'' framework for designing nearest neighbor data structures. Our framework assumes two dissimilarity functions: a ground-truth metric that is accurate but expensive to compute, and a proxy metric that is cheaper but less accurate. In both theory and practice, we show how to construct data structures using only the proxy metric such that the query procedure achieves the accuracy of the expensive metric, while only using a limited number of calls to both metrics. Our theoretical results instantiate this framework for two popular nearest neighbor search algorithms: DiskANN and Cover Tree. In both cases we show that, as long as the proxy metric used to construct the data structure approximates the ground-truth metric up to a bounded factor, our data structure achieves arbitrarily good approximation guarantees with respect to the ground-truth metric. On the empirical side, we apply the framework to the text retrieval problem with two dissimilarity functions evaluated by ML models with vastly different computational costs. We observe that for almost all the large data sets in the BEIR benchmark, our approach achieves a considerably better accuracy-efficiency tradeoff than the alternatives, such as retrieve-then-rerank.", "tldr": "", "keywords": ["Vector Similarity Search", "Information Retrieval", "Bi-metric"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72d1e77fdd7a6be8cbbbf783eb6ac20d128789a2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The problem the paper considers an approximate nearest neighbor search under an expensive-to-compute dissimilarity measure $D$ with the assumption that there exists a cheaper dissimilarity measure $d$ that approximates it. The task is to find the approximate nearest neighbors (w.r.t. $D$) for the query point $q \\in \\mathbb{R}^d$ from the data set $P \\subset \\mathbb{R}^d$ within a budget $\\mathcal{Q}$ of evaluations of the expensive distance metric $D$ (the number of evaluations of $d$ is unlimited). In the experiments of the paper,  $D$ is instantiated by the Euclidean distance between the embeddings computed by large and expensive-to-compute bidirectional encoder, and $d$ by the Euclidean distance between the embeddings computed by a smaller model. A traditional approach to this task is \"retrieve-then-rerank\", where $\\mathcal{Q}$ neighbors of $q$ under $d$ are first retrieved, and then these $\\mathcal{Q}$ points are reranked by evaluating $D$ between them and the query point.\n\nThe article proposes an approach where a graph-based index is built in the offline phase using a proxy metric $d$. Then in the query phase, $\\mathcal{Q} / 2$ nearest neighbors are retrieved using $d$. Then, these $\\mathcal{Q} / 2$ are used to initialize greedy search in a graph using $D$. The search terminates when $\\mathcal{Q}$ distances are evaluated.\n\nThe experimental results of the article demonstrate that the proposed approach (when combined with graph indexes DiskANN and NSG) enables reaching a higher recall with a same budget $\\mathcal{Q}$ than retrieve-then-rerank approach. The article also provides theoretical guarantees for their approach when combined with DiskANN and cover tree indexes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "(S1) The experiments of the article demonstrate that the proposed approach outperforms retrieve-then-rerank under the assumptions of the cost model of the article. \n\n(S2) Theoretical guarantees are provide for the approach (when combined with DiskANN and cover tree indexes)."}, "weaknesses": {"value": "(W1) Methodological novelty of the article is very limited. As stated in the summary, the proposed \"framework\" is: 1) Build a graph index using a proxy metric $d$ in the offline phase 2) In the query phase, first find the $\\mathcal{Q}/2$ neighbors of the query point under $d$. 3) Initialize greedy search from these points using $D$, and continue greedy search until $\\mathcal{Q}$ distances are evaluated. On a related note, \"Bi-metric framework\" on page 4 (lines 180-194) is not a framework, but a problem definition.\n\n(W2) The cost model used to motivate the approach and used in the experiments is not realistic in real-world applications (or, more exactly is realistic only on a very niche scenario mentioned by authors where the expensive model used to evaluate $D$ may change very often). Specifically, the authors assume that the embeddings of the expensive model (used to evaluate $D$) are not precomputed, but are always computed on demand when performing queries. If these embeddings were precomputed, queries would be much faster compared to the experimental scenarios presented in the paper, making the experiments irrelevant. The authors motivate their assumption that the embeddings of the large model are not precomputed by stating that they are expensive to compute. However, if the index is used to answer any number of queries that is realistic in an industrial application, almost all of the embeddings of the points of the data set $P$ have to computed anyway when the queries are answered. Thus, they could be as well precomputed (or cached from the previous queries)."}, "questions": {"value": "I do not have any particular questions for the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ufwZZ2VmNj", "forum": "aLt5FJvJCZ", "replyto": "aLt5FJvJCZ", "signatures": ["ICLR.cc/2026/Conference/Submission13908/Reviewer_GeBN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13908/Reviewer_GeBN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760605043819, "cdate": 1760605043819, "tmdate": 1762924416342, "mdate": 1762924416342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider a scenario for approximate nearest neighbor search where two metrics are involved: a cheap metric that is used to construct the ANN index, and an expensive metric with respect to which the nearest neighbors are found. A common instantiation of this scenario is to first retrieve a larger candidate set of nearest neighbors using the cheap metric and then rerank those points using the expensive distance metric.\n\nThe authors prove that under certain assumptions, the DiskANN and Cover Tree algorithms can be used to build a data structure using the cheap metric such that they can be used to query for the nearest neighbors using the expensive metric with only a sublinear number of expensive metric evaluations. A practical application of the proposed method is simple: first construct a graph index using the cheap metric. In the query phase, given a query budget $Q$ of calls to the expensive metric, first retrieve $Q/2$ nodes in the graph using the cheap distance metric, and then use those nodes as seed points for performing $Q$ distance computations using the expensive metric. The authors perform experiments where the cheap metric is a small embedding model, and the expensive metric is a large embedding model or an LLM listwise reranker."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The article addresses an interesting and, to the best of my understanding, novel topic. The proposed method is theoretically motivated and in practice, graph-based algorithms (which are demonstrated using DiskANN and NSG) are straightforward to modify for the proposed bi-metric framework. The experiments show that the method can yield improved results in certain cases. The paper is well written and easy to read."}, "weaknesses": {"value": "While I like the idea conceptually and the theory involved in the framework is nice, I think the experiments are not convincing enough to indicate that the framework or method would have practical impact.\n\nIn particular, as the expensive distance metric $D$, the authors consider only two choices, a 7 billion parameter embedding model and an LLM listwise reranker. The large embedding model is a somewhat unrealistic choice: the whole point of using an embedding model is that you need to compute the embedding only for the query during retrieval. Calling a 7B embedding model hundreds of times during retrieval is infeasible (I understand the indexing time can also be infeasible but it is a secondary concern).\n\nSimilarly, when using an LLM as a reranker you would typically only rerank a maximum of a few dozen queries due to the speed and costs involved. Moreover, for the listwise reranker the method only works better on half the datasets, and the experiments use a proprietary model.\n\nA natural choice would be of course to instead consider a cross-encoder as $D$. There are many available open-source cross-encoder models that would be quite suitable at 100M-1B parameters, and it is unclear why the authors do not consider these at all. If the authors also want a listwise approach, there are suitable rerankers available for that as well, e.g. the recent jina-reranker-v3 [1].\n\nFinally, of course an advantage of the simple retrieve-then-rerank approach is that it is easy to combine the retrieve phase with e.g. sparse search or filtering, and only rerank those results.\n\n[1] F. Wang, Y. Li, H. Xiao. jina-reranker-v3: Last but Not Late Interaction for Listwise Document Reranking. arXiv:2509.25085. 2025."}, "questions": {"value": "- Could the authors clarify reasons or propose a hypothesis for why the \"single metric\" method performs so poorly?\n\n- Have you tried how your method works with other graph methods such as HNSW?\n\n- Is source code available for reproducability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ymMQRlSoak", "forum": "aLt5FJvJCZ", "replyto": "aLt5FJvJCZ", "signatures": ["ICLR.cc/2026/Conference/Submission13908/Reviewer_aQro"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13908/Reviewer_aQro"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912901521, "cdate": 1761912901521, "tmdate": 1762924415662, "mdate": 1762924415662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposes a data structure design framework for nearest neighbor search: constructing an index using cheap but less accurate proxy metrics and performing only a few expensive and more accurate metric calls during the query phase. This approach achieves retrieval accuracy close to that of expensive metrics, while keeping the overall cost acceptable. In the preprocessing phase, the index is built using only the proxy metrics, while during the query phase, expensive metrics are used to perform greedy or hierarchical searches along the graph. The author shows that this separation of construction and querying approach still reports reliable nearest neighbor results under expensive metrics.\nKey Contributions:\n1. Proposed dual-metric framework: The index is built using approximate metrics, while the query phase combines real metrics to balance computational efficiency and accuracy.\n2. Theoretical proof: For two mainstream algorithms—DiskANN and Cover Tree. The author proves that as long as the deviation between the approximate and real metrics is bounded, a theoretical guarantee of approximation close to the real metric can be obtained.\n3. Experimental validation: The framework is validated in text retrieval tasks (BEIR dataset), where it outperforms the traditional \"retrieve-then-rerank\" method in accuracy-efficiency trade-offs on almost all large-scale datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The balance between accuracy and efficiency in NN problem is interesting. The motivation is good. \n2. The author proposes a general dual-metric retrieval framework. In this framework, the author constructs the index with cheap approximate metrics and performs only a few calls to expensive real metrics during the query phase, while maintaining results close to those of the real metrics.\n3. The paper provides a systematic theoretical analysis, showing that building an index using only approximate metrics and using real metrics during the query phase can still yield high-quality results. It also presents comparisons and counterexamples, explaining why methods like LSH are not suitable for this framework."}, "weaknesses": {"value": "1. The method, both theoretically and experimentally, relies on the assumption that the difference between the cheap and real metrics is bounded, but the samples and domains are relatively narrow (text retrieval).\n2. To ensure recall during the cheap-metric phase, the authors set a very long query length for graph search, which may favor the proposed method. A systematic study of hyperparameter sensitivity and ablation (e.g., varying query lengths, graph construction strategies) is recommended.\n3. When using LLMs for list-wise reranking, the authors acknowledge that occasional ranking errors cause fluctuations in the tail of the curves. Currently, only average trends are shown.\n4. The experiments are costly. For example, reranking experiments costs thousands of dollars, making it difficult for other researchers to reproduce the results.\n5. The structure needs optimization to make it more easy to follow. Some mistakes are made. e.g., The period is forgotten (First paragraph in Introduction section). The comma is missed (The second to last paragraph on Page 2). The proxy metric d and the data dimensionality d are reused."}, "questions": {"value": "1. The paper assumes a bounded distortion relationship between the cheap metric d and the expensive metric D. In real-world applications, if the ratio D/d deviates significantly (e.g., C>2 or is unstable), how would the algorithm's performance change?\n2. The paper mentions that to ensure sufficient recall in the first phase, a very large query length (e.g., L=30000) is used. Has the author tested more conventional parameter settings? How sensitive is the method's performance to these parameter changes? It would be helpful for the author to provide hyperparameter sensitivity or ablation experiments to assess the method's stability and ensure fair comparison.\n3. The paper primarily measures efficiency in terms of the number of expensive metric calls, but does not report end-to-end runtime or other efficiency metrics. Could the author provide the overall time cost, including both index construction and the query phase?\n4. What will the framework perform in image retrieval or cross-modal retrieval tasks (two typical NN search applications)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WJdE46GzRz", "forum": "aLt5FJvJCZ", "replyto": "aLt5FJvJCZ", "signatures": ["ICLR.cc/2026/Conference/Submission13908/Reviewer_hixp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13908/Reviewer_hixp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762075486922, "cdate": 1762075486922, "tmdate": 1762924415047, "mdate": 1762924415047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a **bi-metric framework** for efficient nearest-neighbor search that combines a cheap, approximate metric with an expensive, accurate one. The method builds an index using only the proxy distance and performs search using both distances. \n\n Theoretical analysis provides guarantees for **DiskANN** and **CoverTree**, demonstrating that under a bounded distortion between metrics, the resulting data structures can approximate the results of the expensive metric. \n\nEmpirically, the authors validate the approach on large-scale **BEIR** retrieval datasets, pairing lightweight embedding models such as bge-micro-v2 with stronger ones like SFR-Embedding-Mistral or even LLM-based comparators such as Gemini-2.0-Flash. Across benchmarks, the bi-metric method achieves the same accuracy as traditional re-ranking while requiring **3–4× fewer evaluations** of the expensive model. This efficiency gain translates into non-trivial reductions in time and cost, and the framework remains effective even when the second “metric” is a **non-metric** listwise reranker. \n\nOverall, the paper offers a **simple, general, and well-theorized alternative** to retrieve-and-rerank pipelines. It highlights that guiding search with a cheap proxy distance---then carrying a \"tail\" search using the expensive metric---can improve scalability with only modest decrease in quality compared to running an (often impractically expensive) search using solely the expensive metric."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A simple yet effective idea of carrying out a tail search: applying a computationally expensive distance only after obtaining a seed set of candidates through a cheap search with a proxy distance. \n\n2. The approach is generic and applicable to multiple retrieval algorithms! \n\n3. For two algorithms (graph-based retrieval and CoverTrees) it even comes with theoretical guarantees! \n\n4. What is interesting the second “metric”, in practice, actually does not have to be the metric (although the theory will not hold). \n\n5. Solid experimental results \n\n6. The paper is generally well written although some improvements in clarity can be made (in particular naming of baselines is not very informative)"}, "weaknesses": {"value": "1. Presentation can be improved (see some detailed notes). \n\n \n\n2. Compared to retrieve-and-rerank, gains are somewhat limited in the case of human-labels and NDCG@10. Although in L396 authors notice that gains are more substantial when measured through Recall@10. However, they do it in passing and it may be a lost opportunity to “sell” their approach properly. The issue with benchmarks like BEIR is that human labels are sparse and biased toward lexical search methods (since many annotations were generated by running lexical retrieval and labeling the top-K results). As a result, these benchmarks are likely not very sensitive to recall improvements from vector (semantic) search: once a method retrieves most of the human-labeled relevant documents, its score stops improving: even if it continues to find additional relevant items that were never labeled \n3. An actual search engine can use more than one re-ranking stage. For example, you can retrieve top-1000 using a very cheap embedding model, rescore them using an embedding model with a medium cost and finally apply your expensive model to top-100 results. That said, I would consider this only as an optional baseline and one can always argue that intermediate rankers add extra complexity. \n\n**Some editing recommendations** \n \n\nLL113-115 Here, we use the fact that graph-based algorithms for nearest neighbor search do not require the values D(p, q) per se, but only use comparisons between D(q, p) and D(q, s). -> During retrieval the model keeps a priority-queue of M closest neighbors. When new candidates come in, they need to be merged with existing candidates. This can be sort of inefficient. In any case, it’s good to refer here the reader here to Algorithm 4 in the Appendix. It would have saved me some time. \n\nL134-138  A good “classic” reference here is Matveeva, Irina, et al. \"High accuracy retrieval with multiple nested ranker.\" Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. 2006. \n\n\nL159 It might be worth mentioning (though **definitely not required**) that before Morozov and Babenko, Boytsov explored indexing and retrieval with different similarity measures in the context of graph algorithms and VP-trees. In one case, a cheap inner product was used to construct the k-NN graph, while retrieval relied on a more expensive similarity. This earlier work did not include theoretical guarantees and showed improvements over filter-and-rerank on one dataset out of three.. \n \nBoytsov, Leonid. Efficient and accurate non-metric k-NN search with applications to text matching (**Section 3**). Ph. D. Thesis. Carnegie Mellon University, 2018. \n\nL239 For a metric d, ∆d is the aspect ratio of the input set: the ratio between the diameter and closest pair -> This needs to be a definition. Moreover, it needs to be rewritten for clarity. You probably mean the ratio between the diameter (what is this exactly, distance between two farthest points) and the distance between closest pairs. \n\nL240 Note that both Definition 2.1 and 2.2 are only theoretical analysis: Definitions are not theoretical analysis. \n \nL241 Our experimental results verify the advantage of our bi-metric framework without any assumptions; see Section 4. -> It’s not clear what you mean by “advantages without assumptions”. \n\nL327 We help this method and ignore the large number of D distance calls in the indexing phase and only count towards the quota in the search phase. -> Please rewrite, this is a very awkward way to state that you use the expensive distance during the indexing phase as well. However, during retrieval you apparently still use your two-step algorithm. \n\nL370-372 This is rather confusing because you compute recall as k-NN recall, i.e., use true k-nearest neighbors as relevant entries. However, for NDCG@10 you use human-labeled ground truth entries. This needs to be clarified. It is possible to compute recall with respect to human labels as well! \n\nBi-metric baseline is an extremely confusing name: please, use retrieve-and-re-rank instead \n\nSingle-metric is somewhat confusing as well because it doesn’t reflect the fact that the retrieval is budgeted. Maybe something like single-expensive-metric-with-query-budget (or some compressed version of it will be more natural). \n \nMatveeva, Irina, et al. \"High accuracy retrieval with multiple nested ranker.\" Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. 2006."}, "questions": {"value": "Algorithm 1 and 4 are attributed to DiskANN: What is DiskANN specifics here? I think this is a standard 1-greedy retrieval algorithm that is known literally for dozens of years. Is the difference in the indexing algorithm? It would be nice to separate the indexing and retrieval algorithms."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m8UJrshH3q", "forum": "aLt5FJvJCZ", "replyto": "aLt5FJvJCZ", "signatures": ["ICLR.cc/2026/Conference/Submission13908/Reviewer_SDf3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13908/Reviewer_SDf3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144473481, "cdate": 1762144473481, "tmdate": 1762924414222, "mdate": 1762924414222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a **bi-metric framework** for efficient nearest-neighbor search that combines a cheap, approximate metric with an expensive, accurate one. The method builds an index using only the proxy distance and performs search using both distances. \n\n Theoretical analysis provides guarantees for **DiskANN** and **CoverTree**, demonstrating that under a bounded distortion between metrics, the resulting data structures can approximate the results of the expensive metric. \n\nEmpirically, the authors validate the approach on large-scale **BEIR** retrieval datasets, pairing lightweight embedding models such as bge-micro-v2 with stronger ones like SFR-Embedding-Mistral or even LLM-based comparators such as Gemini-2.0-Flash. Across benchmarks, the bi-metric method achieves the same accuracy as traditional re-ranking while requiring **3–4× fewer evaluations** of the expensive model. This efficiency gain translates into non-trivial reductions in time and cost, and the framework remains effective even when the second “metric” is a **non-metric** listwise reranker. \n\nOverall, the paper offers a **simple, general, and well-theorized alternative** to retrieve-and-rerank pipelines. It highlights that guiding search with a cheap proxy distance---then carrying a \"tail\" search using the expensive metric---can improve scalability with only modest decrease in quality compared to running an (often impractically expensive) search using solely the expensive metric."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A simple yet effective idea of carrying out a tail search: applying a computationally expensive distance only after obtaining a seed set of candidates through a cheap search with a proxy distance. \n\n2. The approach is generic and applicable to multiple retrieval algorithms! \n\n3. For two algorithms (graph-based retrieval and CoverTrees) it even comes with theoretical guarantees! \n\n4. What is interesting the second “metric”, in practice, actually does not have to be the metric (although the theory will not hold). \n\n5. Solid experimental results \n\n6. The paper is generally well written although some improvements in clarity can be made (in particular naming of baselines is not very informative)"}, "weaknesses": {"value": "1. Presentation can be improved (see some detailed notes). \n\n \n\n2. Compared to retrieve-and-rerank, gains are somewhat limited in the case of human-labels and NDCG@10. Although in L396 authors notice that gains are more substantial when measured through Recall@10. However, they do it in passing and it may be a lost opportunity to “sell” their approach properly. The issue with benchmarks like BEIR is that human labels are sparse and biased toward lexical search methods (since many annotations were generated by running lexical retrieval and labeling the top-K results). As a result, these benchmarks are likely not very sensitive to recall improvements from vector (semantic) search: once a method retrieves most of the human-labeled relevant documents, its score stops improving: even if it continues to find additional relevant items that were never labeled \n3. An actual search engine can use more than one re-ranking stage. For example, you can retrieve top-1000 using a very cheap embedding model, rescore them using an embedding model with a medium cost and finally apply your expensive model to top-100 results. That said, I would consider this only as an optional baseline and one can always argue that intermediate rankers add extra complexity. \n\n**Some editing recommendations** \n \n\nLL113-115 Here, we use the fact that graph-based algorithms for nearest neighbor search do not require the values D(p, q) per se, but only use comparisons between D(q, p) and D(q, s). -> During retrieval the model keeps a priority-queue of M closest neighbors. When new candidates come in, they need to be merged with existing candidates. This can be sort of inefficient. In any case, it’s good to refer here the reader here to Algorithm 4 in the Appendix. It would have saved me some time. \n\nL134-138  A good “classic” reference here is Matveeva, Irina, et al. \"High accuracy retrieval with multiple nested ranker.\" Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. 2006. \n\n\nL239 For a metric d, ∆d is the aspect ratio of the input set: the ratio between the diameter and closest pair -> This needs to be a definition. Moreover, it needs to be rewritten for clarity. You probably mean the ratio between the diameter (what is this exactly, distance between two farthest points) and the distance between closest pairs. \n\nL240 Note that both Definition 2.1 and 2.2 are only theoretical analysis: Definitions are not theoretical analysis. \n \nL241 Our experimental results verify the advantage of our bi-metric framework without any assumptions; see Section 4. -> It’s not clear what you mean by “advantages without assumptions”. \n\nL327 We help this method and ignore the large number of D distance calls in the indexing phase and only count towards the quota in the search phase. -> Please rewrite, this is a very awkward way to state that you use the expensive distance during the indexing phase as well. However, during retrieval you apparently still use your two-step algorithm. \n\nL370-372 This is rather confusing because you compute recall as k-NN recall, i.e., use true k-nearest neighbors as relevant entries. However, for NDCG@10 you use human-labeled ground truth entries. This needs to be clarified. It is possible to compute recall with respect to human labels as well! \n\nBi-metric baseline is an extremely confusing name: please, use retrieve-and-re-rank instead \n\nSingle-metric is somewhat confusing as well because it doesn’t reflect the fact that the retrieval is budgeted. Maybe something like single-expensive-metric-with-query-budget (or some compressed version of it will be more natural). \n \nMatveeva, Irina, et al. \"High accuracy retrieval with multiple nested ranker.\" Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. 2006."}, "questions": {"value": "Algorithm 1 and 4 are attributed to DiskANN: What is DiskANN specifics here? I think this is a standard 1-greedy retrieval algorithm that is known literally for dozens of years. Is the difference in the indexing algorithm? It would be nice to separate the indexing and retrieval algorithms."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m8UJrshH3q", "forum": "aLt5FJvJCZ", "replyto": "aLt5FJvJCZ", "signatures": ["ICLR.cc/2026/Conference/Submission13908/Reviewer_SDf3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13908/Reviewer_SDf3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144473481, "cdate": 1762144473481, "tmdate": 1763696294298, "mdate": 1763696294298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a \"new\" bi-metric framework for approximate nearest neighbor (ANN) search that operates over two dissimilarity functions: \n(1) an expensive but accurate metric $D$ and \n(2) a cheap yet approximate proxy metric $d$. \nThe key idea is to build the ANN data structure using only the proxy metric to ensure efficiency, while maintaining the accuracy of the true metric $D$ during query time through a sublinear number of calls to both metrics.\n\nThe authors instantiate this framework on two well-known graph-based algorithms, DiskANN and Cover Tree, and provide theoretical guarantees showing that if the proxy metric approximates $D$ within a bounded factor $C$, the structure achieves arbitrarily tight $(1 + \\varepsilon)$ approximation under $D$.\n\nExtensive experiments on six large-scale BEIR benchmark datasets demonstrate that the proposed method reduces expensive model evaluations by up to $4\\times$ compared to standard retrieve-then-rerank pipelines, while matching or surpassing their accuracy."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Introduces a semi novel bi-metric (refer to questions for why semi) framework that combines a cheap proxy metric with an expensive ground-truth metric, supported by solid theoretical guarantees. \n- Provides formal proofs of sublinear query complexity and $(1+\\varepsilon)$ approximation, improving upon standard retrieve-then-rerank approaches. \n- Extends theoretical analysis to DiskANN and Cover Tree, and clearly delineates the limitations for LSH-based methods. \n- Addresses a practical efficiency–accuracy tradeoff in large-scale retrieval, modeling computational and monetary costs realistically. \n- Validates the framework empirically on six BEIR datasets, showing up to $4\\times$ reduction in expensive model calls."}, "weaknesses": {"value": "- The authors briefly acknowledge that the proposed bi-metric framework relies heavily on the proxy metric $d$ being a reasonably good approximation of the ground-truth metric $D$. \nIn the conclusion, they state:\n``Our framework requires that the (cheap) proxy metric $d$ provides a ‘reasonable’ approximation to the (expensive) ground-truth metric $D$... performance may degrade if the proxy metric is a poor approximation of the ground truth.'' \n\n- They further note that this limitation is inherent when relying on a proxy, since in the extreme case where $d$ provides no useful information about $D$, the framework cannot yield accurate results.\n\n- Empirically, the paper supports this assumption by reporting that the ratio $C = D/d$ is tightly concentrated around 1 (e.g., $0.6 \\le C \\le 1.5$ for HotpotQA), indicating strong alignment between the proxy and ground-truth metrics. However, this analysis only covers well-aligned embedding models (e.g., bge-micro-v2 vs. SFR-Embedding-Mistral) and does not explore failure cases.\n\n- The paper does not provide:\n1.  A systematic study of when or why the proxy metric fails (e.g., under domain shift or weak semantic correlation).\n2. Insight into experiments with poor or random proxies.\n3. Theoretical sensitivity or robustness bounds for large distortion constants $C$."}, "questions": {"value": "1. The main theorem assumes $d$ closely approximates $D$. How realistic is this in practice? Please provide a real-world example where this assumption might fail. \n\n2. The thoery focuses on DiskANN and Cover Tree, exploiting their graph and net structures. Could similar guarantees be extended to other graph-based methods such as HNSW or NSG, or are there fundamental barriers preventing this?\n\n3. The analysis depends on bounded doubling dimension $\\lambda_d$ for space and query complexity bounds. How sensitive are these guarantees when this assumption fails, as often occurs in embeddings used for text or vision? \n\n4. The comparison between the proposed method and the bi-metric baseline is based on a fixed $D$-call budget $Q$. How were the choices of $Q/2$ and $Q$ determined, and could this parameterization influence the observed efficiency gap?\n\n5. All experiments are conducted on BEIR text datasets with embedding and LLM-based metrics. How would the framework perform on other modalities (e.g., image or multimodal retrieval) or with weaker proxies where the distortion constant $C$ is large?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hxIAT8QFSz", "forum": "aLt5FJvJCZ", "replyto": "aLt5FJvJCZ", "signatures": ["ICLR.cc/2026/Conference/Submission13908/Reviewer_JPva"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13908/Reviewer_JPva"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762336183820, "cdate": 1762336183820, "tmdate": 1762924413755, "mdate": 1762924413755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}