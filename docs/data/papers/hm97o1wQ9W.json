{"id": "hm97o1wQ9W", "number": 4744, "cdate": 1757756467848, "mdate": 1763117049082, "content": {"title": "InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization", "abstract": "We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views.\nExisting methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.", "tldr": "We enable consistent sparse-view editing by replacing the neural field in an SDS process with a multi-view diffusion model and adapting the key SDS steps accordingly.", "keywords": ["Image Editing", "Multi-view", "3D", "Distillation", "Diffusion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/eb138f6cbfe682444f05c3f08dbfa20c35880ae1.pdf", "supplementary_material": "/attachment/8df0696e7b8a418fdee732cdc1da0281c8932c56.pdf"}, "replies": [{"content": {"summary": {"value": "The paper presents a method for sparse-view editing, focusing on the challenging case of realistic scenes rather than synthetic objects (e.g., those from Objaverse). In such sparse-view settings, it is not feasible to properly train a NeRF or a 3DGS, which are commonly used as consolidators in multi-view image editing.\nThe proposed method leverages an instruction-based image editing model (InstructPix2Pix) and performs SDS with it. However, instead of optimizing a 3D representation as commonly done in previous works, the approach optimizes a multi-view student model. Specifically, the student model (SEVA) is trained to take M input views and generate N > M views.\nDuring the editing process, the input views are denoised iteratively. At each denoising step, k optimization iterations are performed using the SDS loss, allowing the edits to be applied gradually and coherently across views.\nThe method is evaluated against other approaches for multi-view image editing."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The setting of sparse multi-view image editing for realistic scenes is underexplored, and as demonstrated in the paper, previous methods struggle in this scenario. The paper presents a method that successfully addresses this challenging setting.\n- The proposed approach is novel and well-designed. As noted by the authors, it also has the potential to be applied to other related tasks. The idea of using an SDS loss to optimize a diffusion model is intriguing.\n- The results look good, and the inclusion of a large gallery of qualitative examples makes the findings more convincing.\n- While the method is somewhat complex, the presentation is clear, and the accompanying figures effectively aid understanding.\n\nOverall, the paper is interesting and well-executed, and I enjoyed reading it."}, "weaknesses": {"value": "My main concern is the evaluation of consistency. Although the qualitative results appear consistent, it is difficult to assess consistency based on only four frames. I believe that the CLIP consistency metric does not accurately capture the 3D consistency of the results. For example, I would expect the outputs produced by the student-only configuration to be much more 3D-consistent than those of the teacher-only configuration, since the student is explicitly trained to generate consistent views. However, the CLIP consistency reported for student-only is significantly lower. While I understand why this occurs given how the metric is defined, it highlights that this metric does not measure the desired aspect of consistency.\nEvaluating consistency in this setting is inherently difficult, but I have a few suggestions. One promising direction is to demonstrate 3D reconstruction quality: if the reconstructed 3D object from the generated views is reasonable, it would indicate a degree of cross-view consistency. While reconstruction from only four views is challenging, existing works such as LRM and its follow-ups have shown that this is possible in object-centric settings. Applying the proposed method to such data and using LRM (or similar approaches) for reconstruction could provide stronger evidence of consistency. Another option is to use SEVA itself to reconstruct the scene. Even if these evaluations are qualitative, they would substantially strengthen the paper's consistency claims.\n\nAdditional more minor concerns:\n- Running time. \n- The method is quite complex, and it may be difficult to reproduce the results without access to the authors' implementation.\n- I believe a few simple yet informative baselines were not considered:\n    - Applying SDEdit-style editing with the SEVA model over the provided views.\n    - Reconstructing the scene from the views using SEVA, followed by editing with IN2N or other baseline editing methods.\n\nI do not expect the authors to run all these additional experiments during the rebuttal. However, it would be helpful if they could clarify why these baselines were not included or explain why they are expected to perform poorly in this setting. Such a discussion would strengthen the experimental section and provide a clearer justification for the chosen comparisons."}, "questions": {"value": "What is your hypothesis for why the depth control does not work well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ix7PBuHAni", "forum": "hm97o1wQ9W", "replyto": "hm97o1wQ9W", "signatures": ["ICLR.cc/2026/Conference/Submission4744/Reviewer_i1vc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4744/Reviewer_i1vc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760908009990, "cdate": 1760908009990, "tmdate": 1762917551330, "mdate": 1762917551330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "iMiuLAJzou", "forum": "hm97o1wQ9W", "replyto": "hm97o1wQ9W", "signatures": ["ICLR.cc/2026/Conference/Submission4744/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4744/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763117048310, "cdate": 1763117048310, "tmdate": 1763117048310, "mdate": 1763117048310, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses multi-view image editing from sparse input views, focusing on modifying a scene based on textual instructions while maintaining consistency across all viewpoints. It proposes InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pre-trained multi-view diffusion model, leveraging its data-driven 3D prior. The approach incorporates specific adaptations within the Score Distillation Sampling (SDS) framework, such as incremental student updates, a specialized teacher noise scheduler, and Random Cross-View Attention (RCVAttn). Experiments involved comparisons against various baselines. Evaluation was conducted on scenes from datasets including I-N2N, Tanks and Temples, CO3D, and Mip-NeRF 360, reporting results using CLIP Similarity, CLIP Directional Similarity, and CLIP Directional Consistency metrics, alongside qualitative demonstrations."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper correctly identifies the limitations arising from the need for dense image data in 3D editing tasks and proposes a method that leverages two diffusion models without relying on a 3D model. This approach reflects an interesting attempt to reduce dependency on explicit 3D supervision."}, "weaknesses": {"value": "1. **Missing figure indices and captions throughout the paper.**\nMany figures are presented without indices or captions, particularly on pages 4, 5, and 8. In addition, the graph on page 9 also lacks both an index and a caption. These omissions significantly hinder the reader from following and understanding the paper.\n\n2. **Naive use of SDS leads to degraded editing quality.**\nThe paper applies SDS in a straightforward manner, which is known to produce low-quality and overly saturated results in 3D generation and editing tasks. Prior studies [1,2,3,4] have already demonstrated that naively using SDS can converge to suboptimal results.\n\n3. **Insufficient explanation and analysis of the student–teacher alignment.**\nThe paper merely cites a few related works when describing “representation mapping” while interpolating latent spaces to align two diffusion models trained on different resolutions, offering little to no original explanation of how the mapping is implemented or justified.\nFurthermore, there is no detailed analysis or theoretical reasoning on how this interpolation ensures proper alignment, raising concerns about whether simple resizing alone can enable the simultaneous use of two diffusion models without introducing inconsistencies.\n\n4. **Finetuning the diffusion model with SDS under sparse-view settings is a questionable design choice.**\nNumerous existing 3D models [5,6,7] are specifically designed to handle sparse-view settings efficiently, achieving high performance with significantly lower training costs than finetuning a diffusion model as a student network. Using SDS-based finetuning for this scenario seems unnecessary and computationally expensive. The proposed method requires an H200 GPU for about 40 minutes to perform 3D editing under the sparse-view setting, which represents a considerably high computational cost and is impractical.\n\n5. **Unfair comparison between dense-input models in sparse-view settings.**\nComparing methods that require dense input images (Gs2Gs, T2VZ, DGE) with those designed for sparse-view settings is inappropriate. Stable Virtual Camera already demonstrates strong performance in sparse-view scenarios. To properly validate the proposed method, comparisons should be made against baseline models that are robust under few-shot or sparse-view conditions.\n\n6. **Free-view rendering videos are not provided.**\nTo properly assess the quantitative results of the 3D editing task, free-view rendering videos should be included. Without them, it is difficult to evaluate how well the method performs under novel-view synthesis.\n\n7. **Lack of geometric consistency evaluation.**\nSince the editing is performed across multiple views, an evaluation of how well the geometric consistency is preserved after editing is necessary. The paper does not provide any quantitative or qualitative analysis on this aspect.\n\n---\n\n[1] Wang,et al. \"Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation.\"\n\n[2] Liang, et al. \"Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching.\" \n\n[3] Park, et al. \"Ed-nerf: Efficient text-guided editing of 3d scene with latent space nerf.\"\n\n[4] Koo, et al. \"Posterior distillation sampling.\" \n\n[5] Kim, et al. \"Infonerf: Ray entropy minimization for few-shot neural volume rendering.\"\n\n[6] Deng, et al. \"Depth-supervised nerf: Fewer views and faster training for free.\"\n\n[7] Zhu, et al. \"Fsgs: Real-time few-shot view synthesis using gaussian splatting.\""}, "questions": {"value": "1.  What is the geometric distribution or relative poses of the sparse input views used in the \"unordered, sparse-view settings\" with $N=4$ or $N=8$ inputs?\n2.  Can the method's performance be demonstrated with even fewer input views, such as $N=2$ or $N=3$, to substantiate further the claim of robustness with \"extremely sparse inputs\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zDwDsiLKvg", "forum": "hm97o1wQ9W", "replyto": "hm97o1wQ9W", "signatures": ["ICLR.cc/2026/Conference/Submission4744/Reviewer_RvBk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4744/Reviewer_RvBk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826454557, "cdate": 1761826454557, "tmdate": 1762917550615, "mdate": 1762917550615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new task of multi-view image editing from sparse input views. The main challenge differentiating it from previous works lies in the sparsity of inputs and large viewpoint variations, which make consistent editing difficult. To address this, the authors combine the strengths of two paradigms by distilling edits from a 2D editor (teacher) into a multi-view model (student) to enforce 3D consistency. The overall concept is interesting and the paper presents some promising visual examples. However, the work lacks several important baseline comparisons, and many of the claims in the paper are not sufficiently validated through quantitative or ablation studies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is generally easy to follow. \n- The proposed idea of bridging existing models through a teacher–student distillation framework is conceptually appealing and has potential for broader applicability."}, "weaknesses": {"value": "1. **Motivation for Sparse-View Editing is Weak**: The motivation for addressing sparse-view editing is not sufficiently convincing. The authors assume that users often possess only a limited number of input views to edit, but this assumption is not empirically supported. It remains unclear whether sparse multi-view editing scenarios are common in real-world applications.\n\n2. **Weak Multi-View Consistency**: The model exhibits noticeable inconsistencies across views e.g., the ear shape difference in Figure 7. A small number of dense frames could be used to validate whether the method maintains coherence across frames by visualizing them as videos to see flickering. Current qualitative examples are insufficient to demonstrate cross-view consistency.\n\n3. **Incomplete Baseline Comparisons**: The set of baselines is limited and selective. The paper should include video-editing baselines after interpolating sparse frames (e.g., using start-end frame interpolation or traditional frame interpolation) followed by SOTA video editing models. Diffusion-based editing baselines such as SDEdit variants initialized from original or independently edited views should also be included, applying SDEdit to multi-view diffusion models for consistent sampling. Reference-driven editing methods that propagate edits from a reference frame - by editing one anchor frame and transferring it to others -should also be considered. Finally, consistent multi-image editing methods (e.g., CSD), which operate without relying on neural fields, should be added for fair comparison (Collaborative Score Distillation for Consistent Visual Editing, Kim et al., NeurIPS 2023).\n\n4. **Unsupported Claims and Weak Evidence**: Several claims lack sufficient evidence. The statement that the method achieves semantic consistency but struggles under large viewpoint changes is not empirically validated. The claim that 3DGS overfits under sparse regimes is also weakly supported - Figure 2 shows under-edited rather than blurry results, contradicting the text. Per-frame edited visualizations (in edited datasets) are necessary to verify the claimed overfitting behavior. Similarly, the claimed extension to conditional generation (line 455) is unconvincing, as the presented results are qualitatively poor.\n\n5. **Presentation Issues**: Several figures lack clear numbering and are treated as inset figures.\n\n6. **Weak experimetnal results**: Notable visual artifacts include the blurry suit in Figure 8, the inconsistent ear shape in Figure 7, and oversaturated colors reminiscent of SDS artifacts."}, "questions": {"value": "1. **Camera pose assumptions**: It is ambiguous whether the poses are inherited from the dense original sources or re-estimated from the sparse subset. The authors should clarify how poses are obtained, whether pose quality differs between dense and sparse setups.\n\n2. **Per-Scene Hyperparameter Tuning**: It is unclear whether all baselines were tuned under comparable conditions. The authors should explicitly state the tuning protocol for each baseline to ensure experimental fairness and reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3hcY2UHO59", "forum": "hm97o1wQ9W", "replyto": "hm97o1wQ9W", "signatures": ["ICLR.cc/2026/Conference/Submission4744/Reviewer_fNmk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4744/Reviewer_fNmk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873538942, "cdate": 1761873538942, "tmdate": 1762917550221, "mdate": 1762917550221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents I-Mix2Mix, a framework for distilling a 2D monocular image editor into multi-view diffusion models.  \nGiven multi-view images, one view is edited by a 2D editing model.  \nA multi-view diffusion model serves as the student model. The outputs of the student are converted into the editing model’s latent space, perturbed with noise, and then predicted through the editing model. The student model is updated using the SDS loss.  \nComparisons with other 3D editing models and ablation studies on different design choices are reported."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of distilling a 2D monocular image editing model into multi-view diffusion models using SDS loss is interesting and novel.  \n- The experiments are extensive, and the ablation study is thorough.  \n- The paper is clearly written and well presented."}, "weaknesses": {"value": "- For 3D editing models, providing a continuous multi-view video visualization is important, as showing only selected frames in the paper is insufficient.  \n- It would be better to include an efficiency comparison (similar to Table 1 in DGE). Also, during real 3D editing applications, is an extra step needed to convert the multi-view diffusion model into a 3D representation?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RGqCo3RZyj", "forum": "hm97o1wQ9W", "replyto": "hm97o1wQ9W", "signatures": ["ICLR.cc/2026/Conference/Submission4744/Reviewer_mVMX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4744/Reviewer_mVMX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046426658, "cdate": 1762046426658, "tmdate": 1762917549872, "mdate": 1762917549872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We appreciate the reviewers’ time and feedback. While we are pleased that several reviewers recognized the novelty, clarity, and strong results of our work, we were disappointed by the **wide disparity in ratings**. In particular, several low scores appear to stem from **misunderstandings of the task definition (sparse multi-view editing)**, as well as from **factually incorrect or tangential statements**, rather than from issues with the technical content or experimental validation. We have addressed these points carefully and directly in the individual responses.\n\nTo further strengthen our empirical validation, we have now **added a human survey**, comparing our method to the strongest baseline (DGE). This evaluation confirms that our approach yields **substantially higher cross-view consistency**, reinforcing our main claims. See our response to Reviewer #2 for details.\n\nFinally, as requested, we also experimented with a **naive SDEdit-style combination** of the teacher and student models. This variant produced severe artifacts, thereby **validating the necessity of our distillation-based formulation**, which transfers the teacher’s editing ability to the multi-view student in a principled and stable way.\n\nOverall, we believe the paper presents a **clear, technically sound, and well-validated contribution** to the underexplored problem of sparse multi-view instruction-based editing, and we hope the rebuttal clarifies these points."}}, "id": "MRfhruuAn0", "forum": "hm97o1wQ9W", "replyto": "hm97o1wQ9W", "signatures": ["ICLR.cc/2026/Conference/Submission4744/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4744/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission4744/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763103510014, "cdate": 1763103510014, "tmdate": 1763103510014, "mdate": 1763103510014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}