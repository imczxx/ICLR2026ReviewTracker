{"id": "bRdMxfams1", "number": 22700, "cdate": 1758334555250, "mdate": 1759896851622, "content": {"title": "Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises", "abstract": "Existing decentralized stochastic optimization methods  assume the lower-level loss function is strongly convex and the stochastic gradient noise has finite variance. These strong assumptions typically are not satisfied in real-world machine learning models. To address these limitations, we develop a novel decentralized stochastic bilevel optimization algorithm for the nonconvex bilevel optimization problem under heavy-tailed noises. Specifically, we develop a normalized stochastic variance-reduced bilevel gradient descent algorithm, which does not rely on any clipping operation. Moreover, we establish its convergence rate by innovatively bounding interdependent gradient sequences under heavy-tailed noises for nonconvex decentralized bilevel optimization problems. As far as we know, this is the first decentralized bilevel optimization algorithm with rigorous theoretical guarantees under heavy-tailed noises. The extensive experimental results confirm the effectiveness of our algorithm in handling heavy-tailed noises.", "tldr": "", "keywords": ["Decentralized bilevel optimization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f7ac97f5a83e978083fdb9e66a5fcb3736dcee84.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new first-order decentralized algorithm for stochastic bilevel optimization in practical machine learning settings where the lower-level problem is non-convex and the stochastic noise is heavy-tailed. The proposed approach employs gradient normalization combined with variance reduction to ensure robustness and efficiency without computing second-order information. The authors provide the convergence guarantee for decentralized bilevel optimization under heavy-tailed noise, including novel bounds on interacting gradient sequences and consensus errors across workers. Experiments show the performance of their method in handling heavy-tailed noise."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper formalizes and addresses a challenging problem at the intersection of decentralized learning, non-convex bilevel optimization, and heavy-tailed noise. It provides the first-known theoretical convergence guarantee for this setting.\n- The paper is well-structured and well-written."}, "weaknesses": {"value": "- The hyperparameter settings in Theorem 4.1 (e.g., learning rates and momentum coefficients) depend on the total number of iterations $T$ and several problem-dependent constants (e.g., $\\sigma$, $\\kappa$, $\\lambda$). In practice, these quantities are typically unknown or difficult to estimate. For example, $T$ is often not predetermined in real-world training settings (e.g., early stopping or budget-dependent runs). Could the authors clarify how practitioners should choose these parameters in practical applications? Without such guidance, the practical relevance of the convergence guarantee in Theorem 4.1 is unclear.\n\n- In the experiments, the proposed method was not compared against any baseline, as the authors argue that “no existing approaches directly address nonconvex bilevel optimization” in their setting. However, the absence of baseline comparisons makes it difficult to assess the practical benefits of the proposed approach. It would be important to compare against SOTA methods used for the same tasks (e.g., hyperparameter optimization). Such comparisons are necessary to show that the method offers advantages beyond its theoretical guarantees."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZbwXV9ECMe", "forum": "bRdMxfams1", "replyto": "bRdMxfams1", "signatures": ["ICLR.cc/2026/Conference/Submission22700/Reviewer_2zGP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22700/Reviewer_2zGP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942349868, "cdate": 1761942349868, "tmdate": 1762942343502, "mdate": 1762942343502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses decentralized bilevel optimization with nonconvex objectives and heavy-tailed gradient noise. It proposes \\textbf{D-NSVRGDA} (Decentralized Normalized Stochastic Variance-Reduced Gradient Descent–Ascent), which replaces gradient clipping with normalization to stabilize updates. The method reformulates the bilevel problem into a penalized minimax structure and establishes the first convergence guarantee under heavy-tailed noise using only first-order information. Theoretical analysis provides convergence bounds under $s$–moment heavy-tailed noise $(1 < s \\le 2)$, and experiments validate the effectiveness of the proposed algorithm in this setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. First decentralized bilevel algorithm that formally addresses heavy-tailed gradient noise without relying on clipping.\n\n2. Employs normalized gradients to ensure stability and efficiency, while avoiding second-order computations.\n\n3. Provides rigorous convergence analysis for interdependent gradient sequences under heavy-tailed noise.\n\n4. Utilizes only first-order information, ensuring computational feasibility."}, "weaknesses": {"value": "1. The strong smoothness and Polyak-Łojasiewicz (PL) assumptions may not hold in practical deep learning models.\n\n2. The empirical evaluation appears limited. Specifically, the experiments presented in the paper primarily focus on synthetic datasets. It is recommended that the authors include additional experiments on real-world scenarios to better demonstrate the practical effectiveness of the proposed algorithm."}, "questions": {"value": "1. Can this method be extended to asynchronous or partially-participatory decentralized settings?\n\n2. Does gradient normalization hinder convergence when the gradients are already small?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nPAMNHHdRZ", "forum": "bRdMxfams1", "replyto": "bRdMxfams1", "signatures": ["ICLR.cc/2026/Conference/Submission22700/Reviewer_ArKm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22700/Reviewer_ArKm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968005883, "cdate": 1761968005883, "tmdate": 1762942343248, "mdate": 1762942343248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel decentralized stochastic bilevel optimization algorithm (D-NSVRGDA) tailored for nonconvex problems under heavy-tailed noise. The method leverages normalized variance-reduced gradients and avoids gradient clipping, addressing a significant gap in the literature. Theoretical convergence rates are established, and experiments on synthetic and real-world datasets validate the algorithm’s effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Novelty: This is the first work to address decentralized nonconvex bilevel optimization under heavy-tailed noise without relying on gradient clipping. The use of normalized variance-reduced gradients is innovative and well-motivated.\n\nTheoretical Rigor: The convergence analysis is non-trivial, especially given the challenges of interdependent gradients and consensus errors under heavy-tailed noise. The proofs are detailed and appear sound.\n\nPractical Relevance: The experiments convincingly demonstrate the advantages of the proposed method over clipping-based baselines, particularly in settings with varying levels of heavy-tailed noise.\n\nClarity and Reproducibility: The paper is well-structured, and the authors have provided a proof sketch, detailed appendix, and reproducibility statement."}, "weaknesses": {"value": "1. **Limited Novelty**  \n   The core technique—gradient normalization—has been extensively studied for single-level and minimax problems (e.g., Sun et al., 2024; Hubler et al., 2024; Liu & Zhou, 2025). The extension to bilevel settings, while non-trivial, appears incremental and does not constitute a significant conceptual advance.\n\n2. **Technical Contributions Are Overstated**  \n   The analysis largely follows established proof frameworks for normalized SGD and bilevel optimization. The interdependent gradient and consensus error challenges, while valid, are addressed using standard bounding techniques without introducing new analytical tools.\n\n3. **Narrow Experimental Validation**  \n   - Experiments are conducted only on synthetic and small-scale real-world datasets\n   - No comparison with state-of-the-art decentralized or heavy-tailed methods beyond simple ablated baselines\n   - Missing results on larger-scale or more complex bilevel problems (e.g., large neural nets, reinforcement learning)\n\n4. **Weak Baseline Comparison**  \n   The proposed method is only compared against its non-normalized and clipped variants. There is no comparison with recent decentralized bilevel methods (even if adapted) or other heavy-tailed optimization approaches, making it difficult to assess true performance gains."}, "questions": {"value": "1. How does the performance of D-NSVRGDA compare to recent decentralized bilevel methods (e.g., Wang et al., 2024) when adapted to handle heavy-tailed noise?\n\n2. Could the normalization approach be combined with other variance-reduction techniques (e.g., SAGA) for better performance?\n\n3. The analysis relies heavily on the PL condition. How would the algorithm perform if this condition is not satisfied, as in many deep learning applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LvFn8stSGC", "forum": "bRdMxfams1", "replyto": "bRdMxfams1", "signatures": ["ICLR.cc/2026/Conference/Submission22700/Reviewer_SuDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22700/Reviewer_SuDF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968227757, "cdate": 1761968227757, "tmdate": 1762942343021, "mdate": 1762942343021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies decentralized stochastic bilevel optimization where upper objectives are nonconvex and the lower level satisfies a PL condition in the inner variable. Gradients are corrupted by heavy-tailed noise with only finite $s$-th moment, $s \\in (1, 2]$. The bilevel problem is reformulated as a first-order penalized minimax objective. The authors propose D-NSVRGDA, a decentralized normalized stochastic variance-reduced gradient descent–ascent method with gradient tracking over a fixed communication graph. Key design elements are STORM-style variance reduction and unit-norm (normalized) steps instead of clipping to handle heavy tails. The theory yields sublinear convergence in the expected gradient mapping with explicit dependence on the number of workers, the heavy-tail index s, and the spectral gap of the graph. Simulations and a small MNIST bilevel regression showcase robustness to heavy-tailed perturbations and near-linear speedup in the number of nodes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear formulation of decentralized bilevel learning under heavy-tailed noise with only finite $s$-th moments. Algorithmic novelty in combining normalization and variance reduction with decentralized gradient tracking in a bilevel setting. Convergence guarantees that expose dependence on tail index $s$, spectral gap, and worker count K, including linear speedup terms. Proof techniques extend heavy-tail analysis to decentralized bilevel problems without gradient clipping."}, "weaknesses": {"value": "1. Communication model is synchronized with fixed topology. No theory for asynchronous or time-varying graphs, which limits practical relevance. \n2. Empirical section is narrow. Primarily synthetic tasks and a small MNIST bilevel regression; no large-scale or real federated scenarios. \n3. Constants in the rates depend strongly on condition number, smoothness bundle, and inverse spectral gap. Practical efficiency may degrade on ill-conditioned problems or sparse networks. \n4. Limited baseline coverage. Comparisons omit stronger decentralized bilevel or heavy-tail robust methods that use clipping or adaptive normalization; ablations on normalization vs clipping are light. \n5. Limited guidance on tuning penalty and stepsizes under different $s$ and network conditions."}, "questions": {"value": "1. How should the penalty parameter in the minimax reformulation be selected or adapted in practice to balance bias and stability under heavy-tailed noise? \n2. Do you have theory that compares normalized steps to clipping under finite $s$-moment noise, for example bias of stationary points or high-probability bounds?\n3. Can you provide explicit bounds on estimator bias and variance under mixing by the graph matrix in the STORM-style recursion?\n4. Are there matching lower bounds showing that dependence on the spectral gap of the communication matrix is unavoidable in decentralized bilevel settings?\n5. Which parts of the proof fail for asynchronous updates or time-varying topologies, and what additional assumptions would restore convergence (e.g., bounded delay, $B$-connected graphs)?\n6. Can you report or prove sharp transitions at $s = 2$ (finite variance) and as $s \\to 1$, and whether normalization remains stable without vanishing step sizes?\n7. Provide the exact exponents of the condition number and smoothness constants in the final bound. Are there preconditioning strategies compatible with your analysis?\n8. How is the approximation error in solving the inner penalized subproblem controlled in the decentralized setting, and how does it propagate to the outer stationarity measure?\n9. Can you add experiments against decentralized methods with robust gradient aggregation (median, trimmed mean) and clipped extragradient to isolate the benefit of normalization plus variance reduction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bv6DIjEmva", "forum": "bRdMxfams1", "replyto": "bRdMxfams1", "signatures": ["ICLR.cc/2026/Conference/Submission22700/Reviewer_s9N3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22700/Reviewer_s9N3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134131734, "cdate": 1762134131734, "tmdate": 1762942342738, "mdate": 1762942342738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}