{"id": "0GaCfBRFnf", "number": 17956, "cdate": 1758282395015, "mdate": 1759897142928, "content": {"title": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models", "abstract": "Recent advances in self-refinement have demonstrated significant potential for improving the outputs of large language models (LLMs) through iterative refinement. However, most existing self-refinement methods rely on a reactive process with a fixed number of iterations, making it difficult to determine the optimal timing and content of refinement based on the evolving generation context. Inspired by the way humans dynamically refine their thoughts during execution, we propose ProActive Self-Refinement (PASR), a novel method that enables LLMs to refine their outputs during the generation process. Unlike methods that regenerate entire responses, PASR proactively decides whether, when, and how to refine based on the model’s internal state and evolving context. We conduct extensive experiments on a diverse set of 10 tasks to evaluate the effectiveness of PASR. Experimental results show that PASR significantly enhances problem-solving performance. In particular, on Qwen3-8B, PASR reduces average token consumption by 41.6% compared to standard generation, while also achieving an 8.2% improvement in accuracy. Our code and all baselines used in the paper are available in the GitHub.", "tldr": "", "keywords": ["Large language models", "Self-refine"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bc704b7bedab5c7f854cc8447788460efbeba2e4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ProActive Self-Refinement (PASR) as a novel method for enabling Large Language Models (LLMs) to refine their outputs during the generation process, rather than as a post-hoc step. \n\nThe authors formalize this as a MDP and use RL to train models to decide whether, when, and how to refine their reasoning trace. \n\nKey contributions include:\n* A formulation of proactive self-refinement and trained the model with on-policy RL with GRPO.\n* The design of a fine-grained, comparison-based reward function that encourages meaningful refinements while penalizing unnecessary or harmful ones.\n* Extensive experiments on 10 diverse tasks using Qwen models, demonstrating that PASR improves accuracy and problem-solving performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* The core idea of shifting self-refinement from a reactive, post-hoc process to a proactive, in-process one is a novel and important conceptual leap.\n* The choice of formulating proactive refinement as a Markov Decision Process and using RL is appropriate for the task of learning a sequential decision-making policy.\n* The hybrid reward design, which combines rule-based format rewards with model-based accuracy and refinement rewards, is thoughtful and comprehensive.\n* The evaluation is extensive, spanning 10 different datasets that cover math, reasoning, knowledge, and generation."}, "weaknesses": {"value": "* Confusing claim about the token consumption: \n  * In page 1, Lines 023-024: the author mentioned \"PASR reduces average token consumption by 41.6%\"\n  * In Page 8, LIne 384 - 386, the author mentioned \"with only an 8.4% increase in token consumption compared to standard generation.\"\n  * It would be good to have a clear claim and explanation about the token consumption. So far, the claim of reduce by 41.6% is a claim not well supported.\n\n* The analysis in the Fig. 6 reveals that the model predominantly learns \"Task Alignment\" and \"Information Complement\" behaviors, with \"Error Correction\" being the least frequent type. The authors attribute this to the nature of the training data, but it may point to a limitation in the model's ability to identify and fix deep logical or factual fallacies. \"Text Alignment\" is a fragile section that there is concern that the approach is simply learning to hack the reward model by overfitting to its stylistic preference."}, "questions": {"value": "* Question about the token consumption. How does the 41.6% calculated. What's the setup to achieve this.\n\n* The performance is good but not outstanding, especially when compared with self-refine (row 2 in Table 1). In what types of scenarios does the additional complexity of RL training provides additional advantage. \n\n* It will be good to have more analysis on the pattern of timing of refinement: The model learns when to trigger a refinement. Does the model tend to refine early in its reasoning process to correct its trajectory, or does it make more refinements later once it has generated more context? Understanding these patterns could provide valuable insights into the model's learned metacognitive abilities.\n\n* nit: Page5, Ln 238, duplicated \"Consequently\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4hn8Hu1hzV", "forum": "0GaCfBRFnf", "replyto": "0GaCfBRFnf", "signatures": ["ICLR.cc/2026/Conference/Submission17956/Reviewer_zxHQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17956/Reviewer_zxHQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895540429, "cdate": 1761895540429, "tmdate": 1762927755127, "mdate": 1762927755127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ProActive Self-Refinement (PASR), a reinforcement learning framework that enables large language models to refine their outputs during generation instead of after completion. PASR formulates the process as a Markov Decision Process, allowing the model to decide whether, when, and how to refine based on context. Using a GRPO method and a multi-part reward (format, accuracy, and refinement), PASR improves both efficiency and reasoning quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of moving from reactive post-hoc correction to proactive in-process self-refinement is novel.\n2. MDP formulation and GRPO-based RL training are clearly presented. \n3. Effective reward scheme design: combines structure, correctness, and refinement quality to balance precision and efficiency.\n4. Strong results are shown across diverse tasks and baselines (Self-Refine, PTR, SCoRe, RISE)."}, "weaknesses": {"value": "1. The accuracy and refinement rewards depend on another LLM for scoring, which introduces potential bias and circular evaluation concerns (especially if the same family of models is used for training and evaluation).\n2. Only schematic examples (e.g., the logic puzzle in Figure 1) are shown. More real examples illustrating PASR’s step-by-step refinement and error correction would improve interpretability.\n3. RL overhead compared to SFT methods is not analyzed.\n4. The paper focuses mainly on reward variants without isolating other factors (e.g., tag format, PPO choice)."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EzW14sbScN", "forum": "0GaCfBRFnf", "replyto": "0GaCfBRFnf", "signatures": ["ICLR.cc/2026/Conference/Submission17956/Reviewer_n7rz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17956/Reviewer_n7rz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968282450, "cdate": 1761968282450, "tmdate": 1762927754700, "mdate": 1762927754700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current language models often benefit from self-refinement, where they are reflect upon their reasoning traces to come up with a more accurate final answer. The authors argue that this refinement should not come after each explicit reasoning chain, but instead be dynamic and inserted into the reasoning steps, as humans do. This paper proposes PASR, a method that enables LLMs to refine their reasoning during their generation process. Specifically, PASR requires employing reinforcement learning (via GRPO) with rollouts which are encouraged, via a reward function, to include specific tags which correspond with thinking and refinement. Assessed on Qwen models across a variety of datasets, PASR improves over base models and often is the strongest method, even compared to a large number of baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* A large number of datasets covering diverse reasoning capabilities are explored\n* The authors evaluate against a large set of baselines, making their results much stronger and contextualized\n* The method is written relatively cleanly and explained well. In particular, the reward design section is particularly informative, as it details and justifies each component of the multi-dimensional reward well. The intuition is clear and helpful here.\n* The proposed method consistently outperforms base models across different tasks. Some of the gains on datasets like MATH are significant.\n* The authors perform extra analysis seeing when refinement happens and if it helps with the reasoning, which is helpful. Section 3.4.2 is also well-written.\n* The proposed method generalizes well to tasks that it wasn't explicitly trained on, a great sign and pointing to robustness."}, "weaknesses": {"value": "* Only Qwen models of similar sizes are tested in this work. It would be important to show that the findings here generalize to other models, especially in light of recent discussion on Qwen's SFT making it perform much different than other base models at inference time. It would also be nice to have a scaling comparison - how do these results change on larger/smaller models of the same architecture? It's unclear if rollouts can be obtained in a zero-shot way with other reasoning models while following the formatting guidelines (as the 0 step for RL)\n* The main issue is that this method requires training a model, while other comparable strategies like Self-Refine emerged as simple, training-free ways to boost performance on outputs. It is unclear whether the performance gains across tasks is worth the high-cost of 1) training a large language model and 2) performing an RL pipeline which requires time to cycle through data collection and scoring."}, "questions": {"value": "Considering the RL training and tagging pipeline, is PASR actually cost-efficient compared to zero-shot or post-hoc self-refinement methods? Some rough compute-hours / GPU specs would help"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fJvPRFbEEv", "forum": "0GaCfBRFnf", "replyto": "0GaCfBRFnf", "signatures": ["ICLR.cc/2026/Conference/Submission17956/Reviewer_3Zjb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17956/Reviewer_3Zjb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982919190, "cdate": 1761982919190, "tmdate": 1762927754297, "mdate": 1762927754297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ProActive Self-Refinement (PASR), a method that allows large language models to refine their outputs dynamically during generation rather than through fixed, reactive iterations. PASR enables the model to decide whether, when, and how to refine based on its internal state and evolving context, instead of regenerating entire responses"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents strong results using PASR in comparison to other self-refinement methods. The paper compares PASR with 8 other self-refinement baselines.\n2. PASR is token efficient. It does effective refinement using less number of tokens compared to many baselines.\n3. The paper is well written and provides a clear explanation of all the rewards used. It also presents comprehensive ablation study of PASR."}, "weaknesses": {"value": "1. The paper could also show ablations on different rewards used to understand which ones are most effective and have the most impact on downstream accuracies."}, "questions": {"value": "1. It would be good to mention what data is PASR trained on?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ykGVimnysm", "forum": "0GaCfBRFnf", "replyto": "0GaCfBRFnf", "signatures": ["ICLR.cc/2026/Conference/Submission17956/Reviewer_RVyM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17956/Reviewer_RVyM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131448927, "cdate": 1762131448927, "tmdate": 1762927753887, "mdate": 1762927753887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}