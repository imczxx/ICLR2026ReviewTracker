{"id": "RYrFUkraWM", "number": 21562, "cdate": 1758319028893, "mdate": 1763695458696, "content": {"title": "Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning", "abstract": "Combining existing pre-trained expert LLMs is a promising avenue for scalably tackling large-scale and diverse tasks. However, selecting experts at the task level is often too coarse-grained, as heterogeneous tasks may require different expertise for each instance. To enable adaptive instance-level mixing of pre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and gradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained approach to selection by emphasizing skills, i.e., specialized subcategories such as algebra in mathematics. We propose a skill-based recruiting strategy that dynamically selects the most relevant set of expert LLMs for diverse reasoning tasks based on their strengths. Each selected expert then generates its own reasoning, resulting in k outputs from k experts, which are then synthesized into a final high-quality response by an aggregator, chosen based on its ability to integrate diverse outputs. We show that instance-level expert selection improves performance by a large margin but – when implemented naively – can introduce a high computational overhead due to the need for constant model loading and offloading. To address this, we implement a batch inference strategy that groups instances based on their assigned experts, ensuring each model will only be loaded once. This allows us to integrate 16 models on a single GPU with a time cost comparable to prior multi-agent baselines using 4 GPUs. Through extensive evaluations on diverse benchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we show that Symbolic-MoE outperforms prior multi-agent approaches, with an absolute average improvement of 8.15% over the best baseline. Moreover, Symbolic-MoE generalizes well to unseen tasks and removes the need for expensive multi-round discussions, outperforming discussion baselines with less computation.", "tldr": "", "keywords": ["LLM Reasoning", "Multi-agent Reasoning", "Mixture-of-Expert"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e4d5e3b2b4faa5805a3f18000c64ef41427959c6.pdf", "supplementary_material": "/attachment/d36460602adba30d60498386569b92084126452f.zip"}, "replies": [{"content": {"summary": {"value": "At its core, this paper presents a training‑free, language‑space mixture of experts that routes across off‑the‑shelf LLMs on a per‑instance basis and then fuses their outputs with a task‑specific aggregator. Routing is driven by compact “skill profiles” built from a small validation set: a lightweight keyword tagger assigns skills to each question, and each candidate model accrues +1/−1 per skill based on correctness. At test time, a model’s score is the product of its local suitability—the sum of its scores on the skills relevant to the query—and its global competence—the normalized score of its overall profile. A temperature‑0.5 softmax then defines a distribution from which k experts are sampled. The aggregator is selected per task via a synthetic probe that presents one correct and two incorrect chains and asks each model to choose or synthesize a final answer. For practicality, instances assigned to the same expert are batched, very low‑frequency experts (<5%) are pruned, and execution proceeds sequentially on a single GPU or in parallel across GPUs.\n\nEmpirically, the approach outperforms strong single‑model inference‑time scaling (Self‑Consistency), single‑model multi‑agent debate variants, and multi‑model MoA/Reconcile on MMLU‑Pro, AIME, GPQA, and MedMCQA, with clear efficiency gains from single‑round aggregation and batching. It also transfers reasonably well to unseen math benchmarks (MATH‑Hard, OmniMATH). The overall takeaway is pragmatic: with careful yet simple routing and aggregation, one can reliably extract gains from pools of 7–9B‑parameter models—without retraining and without the overhead of multi‑round discussions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem framing and a pragmatic recipe. The paper tackles a real pain point—how to combine heterogeneous, pretrained LLMs without retraining—and offers a pipeline that is simple enough to reproduce: keyword tagging → per‑model skill profiling → instance‑level routing → task‑level aggregator selection → batched execution.\n- Solid, consistent empirical improvements. Gains over SC, debate/MoA‑style multi‑agent setups, and even some larger/proprietary models are consistent, not cherry‑picked. The ablations (fixed Top‑k vs. adaptive, random experts, aggregator variants, keyword LLM choice, trimming) all point in the same direction and help build trust.\n- Efficiency matters—and is addressed head‑on. The batching plus expert‑frequency trimming is an important engineering contribution that makes the idea viable on a single GPU. The paper also analyzes runtime and token costs and explains why single‑round aggregation can be faster despite longer outputs from R1‑style models.\n- Generalization beyond the dev domain. Reusing AIME‑derived profiles on MATH‑Hard/OmniMATH and still seeing wins suggests the profiles capture something meaningful and not just overfit to the dev split."}, "weaknesses": {"value": "- Novelty is mostly compositional. Instance‑level expert selection, LLM‑as‑judge aggregators, and topic/skill‑based routing all have precedents (SMoE‑style routing; MoA/debate judges; semantic/keyword‑guided selection). The paper’s main contribution is a coherent, training‑free system that makes these parts work well together at scale. That’s valuable, but the conceptual leap is modest.\n- Heuristic skill profiling with limited statistical treatment. The +1/−1 scoring per skill is intuitive but brittle: no confidence modeling, no treatment of class‑imbalance or noisy keyword assignment, and little analysis of how profile reliability scales with dev set size. AIME’s tiny test split (30 items) exacerbates variance concerns. I’d like to see CIs, bootstrap stability of profiles, and “dev‑size vs. performance” curves.\n- Synthetic aggregator task representativeness. Choosing the task‑level aggregator via a “1 correct, 2 incorrect CoTs” proxy could misalign with real test‑time distributions (e.g., varying numbers of partially correct/contradictory CoTs, different error modes). How stable is aggregator ranking under different positive/negative ratios, error taxonomies, or difficulty strata? A correlation study to real aggregation outcomes would help.\n- Limited sensitivity and router baselines. Key knobs (k, softmax temperature, frequency threshold, embedding model/thresholds) deserve systematic frontiers (accuracy vs. latency vs. token cost). Also missing are stronger routing baselines: (i) purely semantic retrieval routing (no keywords), (ii) a tiny learned router (linear/MLP/contextual bandit) trained on the same dev set, and (iii) hybrid retrieval+keyword routing. These would sharpen the case for the symbolic approach.\n- Cost reporting could be more operational. The runtime and token metrics are useful, but deployment‑minded readers will also want end‑to‑end numbers under load: model load/unload amortization, memory footprint, throughput under QPS=10/50/100, and degradation curves as expert pool grows.\n- Potential failure modes under domain shift. The method relies on keyword tagging and SBERT matching; both can drift under distribution shift or adversarial phrasing. A qualitative error analysis (wrong keywords → wrong experts) and a quick robustness check with alternative embedders (E5/SimCSE) would round out the story."}, "questions": {"value": "- Novelty and positioning\n  - Please delineate what is genuinely new versus what is adapted from SMoE-style routing, MoA/LLM-as-judge aggregation, and topic/skill-based model selection. A side-by-side table (components, training need, cost, scalability, performance impact) would help clarify the incremental contribution.\n  - Which component(s) are necessary for the gains (ablation removing each: keywording, SBERT alignment, global competency term, frequency trimming, batching)? Which pieces are swappable without notable loss (e.g., different embedding models or scoring rules)?\n\n- Statistical robustness of skill profiling\n  - The +1/−1 per-skill scoring is intuitive but potentially brittle. Please report: (i) profile stability vs. dev set size (e.g., 50/100/200/350), (ii) confidence intervals or bootstrap variance for per-skill scores and per-model totals, and (iii) class/skill imbalance effects.\n  - Your keyword consolidation (keeping only skills with frequency >1 across the dev set) may prune rare-but-critical skills. What fraction of skills get filtered, and what is the downstream effect on routing and accuracy across datasets?\n  - Do a “winner-take-most” analysis: show the skill-by-model score matrix sparsity, entropy/Gini of score mass across models, and how this correlates with routing diversity.\n\n- Keywording and semantic alignment\n  - Provide sensitivity to the embedding backbone (SBERT vs E5 vs SimCSE) and the matching threshold. Report the effect on routing decisions and final accuracy.\n  - Give 5–10 concrete failure cases where keyword drift/synonyms/domain terms led to misrouting: question → extracted skills → matched skills → selected experts vs. post-hoc best expert, with a short diagnosis.\n  - What happens if you remove keywords entirely and route by pure semantic retrieval from the question embedding to per-model “profile embeddings”? Please include this baseline.\n\n- Router formulation and hyperparameters\n  - Why multiply local suitability and global competency rather than add or learn a weight? Please include an ablation (multiply vs add vs learned scalar/MLP).\n  - Provide systematic frontiers (not single points) for k (e.g., 2/3/5/7), softmax temperature (0.3/0.5/0.7), frequency-trimming threshold (1%/5%/10%), and embedding threshold—show accuracy/latency/token trade-offs.\n  - Deterministic Top-k vs sampling: how do stability, diversity, and accuracy compare, especially under limited compute?\n\n- Representativeness of the synthetic aggregator task\n  - Your (1 correct, 2 incorrect CoTs) proxy may not match real test-time mixtures. Vary the positive/negative ratio, error taxonomies (arithmetic slips vs reasoning fallacies vs vague text), and difficulty strata; report Spearman rank correlation of aggregator performance between the proxy and real aggregation.\n  - Quantify Pearson/Spearman correlations between proxy scores and actual task outcomes per aggregator across datasets. How well do aggregator rankings transfer across tasks?\n  - Does revealing model identities/“confidence” to the aggregator bias selection? Compare blinded vs non-blinded aggregator prompts.\n\n- Stronger routing baselines\n  - Include (i) purely semantic retrieval routing, (ii) a tiny learned router (logistic/MLP/contextual bandit; <1M params; trained on the same dev set), and (iii) a hybrid keyword+semantic router. Report accuracy and cost under matched budgets.\n  - If a small learned router cannot beat symbolic routing, analyze why (data insufficiency, label noise in skills, mismatch between proxy and true utility).\n\n- Efficiency and operational reporting\n  - Break down end-to-end latency into model load/unload, weight I/O, generation, aggregation, and batching overheads. Include curves under realistic load (QPS=10/50/100) and show throughput/latency/memory.\n  - Show scaling with expert-pool size (8/16/32): memory footprint, cache hit rates, routing entropy, and where returns diminish.\n  - Justify the 5% trimming threshold or make it task-adaptive. Provide sensitivity and an auto-tuning heuristic.\n\n- Fair comparisons and aggregation baselines\n  - Normalize cost with “tokens × unit price” and “wall time × energy” where possible, since SC vs Symbolic-MoE produce different output lengths (R1 models are verbose).\n  - Add non-LLM aggregators (majority vote, weighted voting by historical accuracy, evidence-consistency scoring) at similar cost. How close can they get to your task-level aggregator?\n  - Control for max output length (truncate or cap across models) to test whether long R1 outputs disproportionately benefit the aggregator.\n\n- Robustness and distribution shift\n  - Evaluate adversarial/noisy keywording (typos, synonym swaps, distractors). Quantify routing and accuracy drop; propose simple mitigations (lexical normalization, multi-candidate fusion, subword matching).\n  - Expand the profile-transfer study: cross-year, cross-subdomain (e.g., MMLU-Pro subject subsets), and cross-style prompts. Provide degradation laws and failure taxonomies.\n\n- Data leakage and fairness\n  - Assess potential train–test contamination for open-source models (e.g., AIME years, MATH subsets). If filtering or year splits were used, document them; otherwise, estimate leakage risk and its impact on transfer experiments.\n  - Analyze expert concentration by subject (e.g., medicine, law). Are some categories monopolized by a few models, leading to systematic misrouting elsewhere?\n\n- Failure cases and interpretability\n  - Provide 10 representative failure examples annotated with: extracted skills → matched skills → selected experts vs. best-in-hindsight experts → aggregator decision → root cause (keyword bias, profile noise, aggregation error, context truncation).\n  - Consider adding an “evidence consistency/conflict report” in the aggregator output to make decisions auditable and facilitate debugging.\n\n- Boundary conditions and extensions\n  - Stress-test with 32–64 experts, longer inputs, and multi-turn problems (e.g., with tools/retrieval). Report stability and latency/accuracy curves.\n  - For structured outputs (code, formal proofs), does the aggregator need task-specific constraints? Without them, what failure rate increases?\n\n- Forward-looking\n  - Can you distill the skill-profile router into a compact student to further reduce multi-model calls? Show initial results or discuss feasibility.\n  - What “meta-abilities” make a good aggregator (fallacy detection, counterfactual reasoning, long-context reconciliation)? Can auxiliary benchmarks predict the best aggregator per task a priori?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2fYmDsiRy8", "forum": "RYrFUkraWM", "replyto": "RYrFUkraWM", "signatures": ["ICLR.cc/2026/Conference/Submission21562/Reviewer_mHRt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21562/Reviewer_mHRt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761380001784, "cdate": 1761380001784, "tmdate": 1762941836859, "mdate": 1762941836859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a gradient-free Mixture-of-Experts framework that chooses pre-trained experts for each task based on skill tags associated with each expert. The framework involves (1) first using a \"Keyword LLM\" to tag skills for each question in a validation set, (2) building model profiles of skills tag for each expert by evaluating them on the validation set and tracking accuracy, (3) selecting models for each question during inference based on the model profiles and skills tagged to each question, (4) using another model aggregator selected based on how well it can distinguish correct from incorrect responses. The framework is evaluated on various reasoning benchmarks and over both single and multi-model baselines, and achieves good empirical performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed framework is intuitive: each question requires various skills to solve, and each model has different skills profile, hence performing routing based on skills tagging could yield good performance.\n\n- The paper demonstrated the framework's consistently good performance across various benchmarks of different domains (math, sciences and medical). \n\n- The framework provides a simple, adaptive instance-level mechanism that could help achieve good performance even for out-of-distribution task instances if the skills tag remain consistent and the \"Keyword\" tagging LLM is sufficiently capable."}, "weaknesses": {"value": "- Intuitively, the performance of the framework would be very dependent on the quality of the skill tags assigned by the \"Keyword LLM\". While there is some ablation on the sensitivity to the LLM used (table 14), there is no discussion on sensitivity to the description of keywords (e.g. influenced by the prompt). For example, the range/set of keywords that can be used could be important, as well as the granularity of the skills (e.g. Math vs Calculus vs PDEs). Sensitivity analysis and better explanation of how the keywords could be set would be helpful.\n\n- Similarly, the choice of aggregator also seems to be important, as indicated in Table 3, where swapping out an aggregator could result in significantly worse performance below that of simple baselines such as self-consistency. Providing ablations on whether most of the performance gains are actually from the better aggregator would be useful (e.g. applying the aggregator only to self-consistency or other baselines, or applying the routing framework but with majority voting aggregation).\n\n- The framework, while intuitive, is heuristics based and may not be able to generalize well to tasks where it is not clear how to clearly assign skill tags to each question, especially when there may not be clear domain-specific LLM variants and when the benchmarks do not contain a large mixture of these sub-categories of questions unlike the ones being considered (e.g. MMLU, GPQA). It would be useful if there were additional experiments on benchmarks where such clear 'subject-based' sub-categories are not obvious, e.g. in logical reasoning, non-math reasoning tasks.\n\n- The framework also seems to be quite dependent on the validation dataset, e.g., if the validation set only contains math questions and hence only math-subject tags are generated, the model profiles would not be useful for other context.\n\n- The authors could consider discussing and/or comparing with related work that single model, multi-instance LLM ensembles where diversity is based on the prompts to each instance [1-3]. These approaches have been shown to be more effective than the self-consistency baseline considered in the paper, which seems to have very strong performance (2nd-best performance across several baselines, and beating the framework without the selected aggregator).\n\n[1] Hu et al, Dipper: Diversity in Prompts for Producing Large Language Model Ensembles in Reasoning Tasks\n\n[2] Naik et al, Diversity of Thought Improves Reasoning Abilities of LLMs\n\n[3] Zhang et al. Prefer: Prompt Ensemble Learning via Feedback-Reflect-Refine"}, "questions": {"value": "Please see the concerns and suggestions in the Weakness section. It would also be useful to provide assessment of the failure modes of the framework as this may help address some of the concerns mentioned -- e.g. the primary cause of failure being whether the wrong skills were assigned, poor selection of experts (i.e. other experts in the pool have the right response),  aggregator LLM failure (experts' answers are correct but final aggregated output is wrong)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Q5tGqZAS7", "forum": "RYrFUkraWM", "replyto": "RYrFUkraWM", "signatures": ["ICLR.cc/2026/Conference/Submission21562/Reviewer_GGut"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21562/Reviewer_GGut"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783106550, "cdate": 1761783106550, "tmdate": 1762941836611, "mdate": 1762941836611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SYMBOLIC-MoE is a test-time, skill-aware mixture-of-experts framework: a “keyword LLM” extracts discrete skills and, from a small validation set, builds per-model skill profiles. At inference, it extracts a sample’s skills, matches them to these profiles, probabilistically recruits a few experts by combining local skill fit with global competence, and uses a task-selected aggregator to synthesize their chain-of-thought outputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\t**Clear problem formulation and intuitive approach.** Figures 1 and 2 vividly contrast “task-level fixed agents” with “instance-level, skill-based routing.”\n2.\t**Reporting with variability**: Results include standard deviations, which strengthen the empirical claims and improve interpretability and reproducibility."}, "weaknesses": {"value": "1.\t**Missing comparison to RL-based ensembles/routing.** The paper argues for SYMBOLIC-MoE mainly against all-LLM ensembles and multi-agent baselines, but does not compare to lightweight RL/bandit routing or RL-trained ensembling (e.g., [A] or [B]) that already offer strong cost–quality trade-offs.\n2.\tAggregator not validated vs stronger fusion. Try the GenFuser [C] as the aggregator and provide an ablation to see if gains stem from fusion rather than routing.\n3.\t**Unclear online behavior with small/no batches.** In an online production setting with mixed, bursty traffic, how does your expert-grouped batching behave when batches are small or fail to form?\n\n\n[A] Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning, NeurIPS 2025.\n\n[B] Efficient Dynamic Ensembling for Multiple LLM Experts, IJCAI 2025.\n\n[C] LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion, ACL2023."}, "questions": {"value": "see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PXnJHAJGlm", "forum": "RYrFUkraWM", "replyto": "RYrFUkraWM", "signatures": ["ICLR.cc/2026/Conference/Submission21562/Reviewer_zGjf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21562/Reviewer_zGjf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897617536, "cdate": 1761897617536, "tmdate": 1762941836329, "mdate": 1762941836329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a model ensemble method that aggregates the outputs of multiple models to solve specific problems. The authors design a skill-based method to select appropriate LLMs for each instance. Specifically, every candidate LLM is assigned a profile containing scores for solving different kinds of tasks (skills), based on its performance on a validation set. When a new test instance is introduced, it is also labeled with relevant skills. The LLMs that have high scores on these specific skills are then selected to process the instance. Finally, another LLM is used to aggregate the outputs from the selected models and generate a final answer. Experiments reportedly show that this proposed ensemble method outperforms both single LLMs and previous ensemble methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work addresses the critical routing problem within model ensemble or multi-agent scenarios. The proposed skill-based routing is interesting and novel, and it seems more appropriate than traditional task-level routing."}, "weaknesses": {"value": "1. **Unfair Experimental Setting**: The experimental settings appear unfair. The authors include reasoning models in their candidate pool (e.g., QwenR1, LlamaR1) but compare them only against non-reasoning baselines (e.g., deepseek-v3, llama3-70B). This comparison is problematic, as reasoning models have a strong inherent advantage on tasks like AIME. As shown in Table 1, QwenR1 alone outperforms the non-reasoning baselines. Given this setup, the claim that the proposed method allows smaller LLMs to outperform models with more parameters is not well-supported. For a fair comparison, the authors should either include reasoning models in the baselines or remove them from the candidate pool.\n\n2. **Lack of Clarity in Profile Creation**: The model profile creation stage is not clearly described. The authors state that an LLM is used to annotate the skills needed for each query in the validation set; however, the prompt used for this annotation is not provided. It is unclear whether the LLM's prediction is based on a predefined keyword pool. If no such pool is used, the LLM's outputs for semantically identical skills might be slightly different (e.g., \"algebra\" vs. \"algebraic\"). Do the authors employ a normalization method for these outputs? Furthermore, the distribution of skills in the validation set is missing. The model profile's reliability depends on having a sufficient number of test queries for every skill.\n\n3. **Contradictory \"Global Competency\"**: The introduction of \"global competency\" seems contradictory to the principle of instance-based routing. If I understand correctly, global competency is not relevant to the current test instance; rather, it only reflects a model's general performance on the validation set. Why not rely solely on the local suitability score?\n\n4. **Expert Sampling Method**: The paper states that K experts are sampled **with replacement**, which means a single expert might be selected multiple times for one instance, effectively reducing the number of unique experts used. The softmax temperature is set to 0.5, which would make the expert selection probability distribution spiky. This setup would encourage the selection of only one or two experts with the highest probabilities, making the \"mixture-of-experts\" effectively a \"select-the-best-expert\" approach. Can the authors provide statistics on the average number of unique experts used per instance?"}, "questions": {"value": "Is the aggregator's selection ability also related to the skills required for each query? In lines 197-199, the authors state this is not a good choice, but they do not provide evidence to support this claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H5CsBU91Mx", "forum": "RYrFUkraWM", "replyto": "RYrFUkraWM", "signatures": ["ICLR.cc/2026/Conference/Submission21562/Reviewer_DD14"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21562/Reviewer_DD14"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970572721, "cdate": 1761970572721, "tmdate": 1762941836057, "mdate": 1762941836057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}