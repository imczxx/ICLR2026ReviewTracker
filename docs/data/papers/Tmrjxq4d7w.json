{"id": "Tmrjxq4d7w", "number": 7142, "cdate": 1758009493957, "mdate": 1763649955233, "content": {"title": "Secure Outlier-Aware Large Language Model Inference", "abstract": "Secure multiparty computation allows the client to secretly inference their sensitive inputs without acquiring the proprietary machine learning model weights. As the decoder-only transformer-based large language model becomes the popular paradigm, the desire of applying MPC in large language models is increasing. However, such inference usually leads to great amount of latency, which is due to nonlinear operations in the Transformer architecture. Recent works either focus on improving cryptographic primitives or re-architecting and re-training to make LLM MPC-friendly. We, on the other hand, observe that properly addressing outlier phenomena, which are unique yet universal properties existing across different LLMs, can effectively reduce the input domain and thereby design faster protocols for non-linear operations. Hence, we propose Secure Outlier-Aware Large Language Model Inference framework (SOAL), which accelerates the RMSNorm operation by nearly 2 $\\times$, SiLU by $2\\times$, and Softmax by more than 3$\\times$. SOAL maintains the same performance of the original model without any fine-tuning requirement.", "tldr": "", "keywords": ["Multiparty Computation", "Privacy Perserving Machine Learning", "Secure LLM Inference"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b70a3b786d86bfc3ae55d9e9fd5427fbb1081340.pdf", "supplementary_material": "/attachment/beab1c4eab8b7be1188b44e2e7738c0970225488.zip"}, "replies": [{"content": {"summary": {"value": "This work introduces a secure inference system that smooths out outliers in order to reduce complexity (bit length) in underlying secure computation. They start from empirical investigation of the outlier issues and show the possibility to constrain their values within a small range by prefixing the prompt. They then propose optimized MPC protocols to evaluate the non-linear primitives on small bit-length inputs. They also propose the \"conformant maxima\" to compute approximate max values in softmax, which is based on the observation that the most maxima positions locate in a limited number of places."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ I appreciate their effort to identify origins of the outlier issues (and also maxima positions) and propose simple and effective way to reduce such issues in inference, consequently leading to more efficient secure computation on smaller bit length.\n\n+ The proposed protocols seem valid.\n\n+ The proposal is evaluated on both ASS and FSS schemes, which exceeds my expectation."}, "weaknesses": {"value": "- Some claims have no reference support (or might even be invalid, see my questions below).\n\n- The usage of CrypTen is understandable, but it provides misleading results, which should be clearly identified and explained to the readers.\n\n- The experimental comparison is not complete."}, "questions": {"value": "- Give reference for your claim: \"Inferencing a 64-token input once using Llama2-7B (Gupta et al., 2022) via CrypTen (Knott et al., 2021) would take 169.76 seconds.\" \n\n- Your claim \"Such inefficiency stems from MPC’s difficulty in handling nonlinear layers\" is not completely correct. It is only meaningful because you consider everything implemented in CrypTen, which is not a fair library to use for two-party computation (SOAL focuses on standard two-party computation as you claim). CrypTen is NOT a pure 2PC library because it doesn't include faithful ways of generating 2PC multiplication triples. It should be classified as a \"2+1\" library where triples are generated by a trusted third party. In pure 2PC secure inference system, the linear layers (mainly matrix multiplication) actually occupies at least half of the overhead (check BOLT[1] or BumbleBee [2] for reference). \n\n- In Figure 1 (c), how can I read from the graph that special tokens are related to outliers?\n\n- For ASS instantiation, how is your 8-bit LUT implemented?\n\n- As I mention above, please clarify in the text that the evaluation results with CrypTen is not pure 2PC. For example, in Table 1, if we evaluate with a \"real\" 2PC library, the linear layers should contribute much more in the total overhead.\n\n- Should include some recent schemes into the comparison, such as BOLT [1] and BumbleBee [2]. Though these schemes are not implemented with CrypTen, it doesn't affect a direct comparison by taking their results from the papers. For example, communication amount is a metric that normally doesn't change in different libraries, and would give readers a more clear picture on the advantage of your solution.\n\n\n[1] https://eprint.iacr.org/2023/1893\n[2] https://eprint.iacr.org/2023/1678"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VS6xfTrmhR", "forum": "Tmrjxq4d7w", "replyto": "Tmrjxq4d7w", "signatures": ["ICLR.cc/2026/Conference/Submission7142/Reviewer_Hgte"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7142/Reviewer_Hgte"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761026618768, "cdate": 1761026618768, "tmdate": 1762919306932, "mdate": 1762919306932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SOAL focuses on improving the efficiency of LLM inference under MPC settings. In the past, the main reason for slow inference was the computational complexity of the nonlinear layer. SOAL leverages the statistical properties of activations, maintains the original model weights, and uses a designed trigger token to control the location of outliers, further narrowing the activation range."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "In the preparation stage, the distribution of nonlinear layer activations is pre-counted, outliers of special tokens are identified, and outliers are locked in predictable positions by prefixing; based on the previously shrunk input, the MPC protocol of RMSNorm, SiLU, and Softmax is reconstructed to achieve acceleration, and the advantages in time and communication volume are reported on gpt2, llama2-7b, and mixtral."}, "weaknesses": {"value": "**1. Idealistic Assumptions:**\n\nThe evaluation is conducted in a semi-honest setting, lacks discussion of malicious actors, and provides no security proof.\n\n**2. Lack of Robustness Assessment:**\n\nThis method relies on a stable association between activation distributions and specific tokens, but the paper does not verify whether this relationship holds in **multilingual, long-context, or fine-tuned models** scenarios. It also lacks robustness tests against outlier distribution drift and prefix failure."}, "questions": {"value": "1. Is the post-prefix outlier anchor position stable in complex scenarios such as multilingual, long-context, and retrieval-enhanced or multimodal? Has performance been tested under different sampling parameters ($temperature, top-p$)?\n\n2. After incremental training/instruction fine-tuning, is it necessary to re-count special tokens? How does the cost of the Preparation phase versus retraining affect the quantitative benefits of model changes?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LMvtMtE9cE", "forum": "Tmrjxq4d7w", "replyto": "Tmrjxq4d7w", "signatures": ["ICLR.cc/2026/Conference/Submission7142/Reviewer_8jL4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7142/Reviewer_8jL4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761058139997, "cdate": 1761058139997, "tmdate": 1762919306414, "mdate": 1762919306414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SOAL focuses on improving the efficiency of LLM inference under MPC settings. In the past, the main reason for slow inference was the computational complexity of the nonlinear layer. SOAL leverages the statistical properties of activations, maintains the original model weights, and uses a designed trigger token to control the location of outliers, further narrowing the activation range."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "In the preparation stage, the distribution of nonlinear layer activations is pre-counted, outliers of special tokens are identified, and outliers are locked in predictable positions by prefixing; based on the previously shrunk input, the MPC protocol of RMSNorm, SiLU, and Softmax is reconstructed to achieve acceleration, and the advantages in time and communication volume are reported on gpt2, llama2-7b, and mixtral."}, "weaknesses": {"value": "**1. Idealistic Assumptions:**\n\nThe evaluation is conducted in a semi-honest setting, lacks discussion of malicious actors, and provides no security proof.\n\n**2. Lack of Robustness Assessment:**\n\nThis method relies on a stable association between activation distributions and specific tokens, but the paper does not verify whether this relationship holds in **multilingual, long-context, or fine-tuned models** scenarios. It also lacks robustness tests against outlier distribution drift and prefix failure."}, "questions": {"value": "1. Is the post-prefix outlier anchor position stable in complex scenarios such as multilingual, long-context, and retrieval-enhanced or multimodal? Has performance been tested under different sampling parameters ($temperature, top-p$)?\n\n2. After incremental training/instruction fine-tuning, is it necessary to re-count special tokens? How does the cost of the Preparation phase versus retraining affect the quantitative benefits of model changes?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LMvtMtE9cE", "forum": "Tmrjxq4d7w", "replyto": "Tmrjxq4d7w", "signatures": ["ICLR.cc/2026/Conference/Submission7142/Reviewer_8jL4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7142/Reviewer_8jL4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761058139997, "cdate": 1761058139997, "tmdate": 1763699071228, "mdate": 1763699071228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problem of secure LLM inference using MPC technologies, specifically 2PC setting. The authors studied the phenomenon of outlier activations during LLM inference and proposed tailored methods to handle outliers for different non-linear functions.  In this way, the input values to non-linear functions, which are typically the bottleneck of MPC-based approaches, can be limited to a narrow range. The authors then used simpler approximations or LUT to improve the efficiency of these non-linear functions. Particullarly, the introduction of conformant maxima avoid the heavy maximum finding. The authors also conducted some experiments against prior works on GPT-2 and LLama-7B models to showcase the effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- Using MPC to secure LLM inference is an interesting and important direction.\n- The idea of handling activation by prefixing special tokens and using more efficient non-linear functions given a narrow input range is innovative. The introduction of conformant maxima is interesting.\n- Extensive experiments have been conducted."}, "weaknesses": {"value": "- The authors focus on 2PC ASS and FSS, lacking the discussion of SOTA HE-based solutions.\n- The ASS baseline is mainly Crypten, and lacks comprehensive comparison against SOTA works. For example, MPCFormer, SecFormer and PUMA [1]\n- The MPC-side contribution is somewhat limited. The idea is similar to SecFormer and more recent work Nimbus [2], which also proposes distribution-aware non-linear function evaluaiton.\n- The description for special-token outlier collection is ambigous. Could the author provide the 'special-toekns' for GPT-2 and Mistral 8x7B as well? According to Figure 1(c), the token '.' (which is believe is not a commonly-defined special token) is regarded as outlier token. What if multile '.' occurs in the prompt? Besides, if the prompt is a long sentence, does that mean all the '.' should be prefixed? In this case, what is the impact to the utility?\n- How are the PPL figures in Table 3 calculated? Do the SOAL and original use the same prefixed prompt as inputs?\n\n\n[1]: PUMA: Secure inference of LLaMA-7B in five minutes. https://arxiv.org/abs/2307.12533\n\n[2]: Nimbus: Secure and Efficient Two-Party Inference for Transformers https://arxiv.org/pdf/2411.15707"}, "questions": {"value": "- Could the author clarify whether the setting is exactly 2PC? I believe MPCFormer and Crypten using 2+1 setting, where a TTP is required to handle the generation of some correleated randomness.\n- Please refer to the weakness for more questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZfY2eFHMIp", "forum": "Tmrjxq4d7w", "replyto": "Tmrjxq4d7w", "signatures": ["ICLR.cc/2026/Conference/Submission7142/Reviewer_DABD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7142/Reviewer_DABD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761280039318, "cdate": 1761280039318, "tmdate": 1762919305948, "mdate": 1762919305948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SOAL (Secure Outlier-Aware Large Language Model Inference), a framework that accelerates privacy-preserving LLM inference under MPC (multi-party computation) by exploiting outlier distributions in non-linear layers. The authors observe that activations in normalization, activation (SiLU), and Softmax layers show heavy-tailed distributions where only a few “outlier” activations dominate. They propose to “control” these outliers by prefixing special tokens in the prompt, thereby reducing the input domain of non-linear MPC protocols. They then design optimized MPC protocols for RMSNorm, SiLU, and Softmax based on the narrowed input range and specific polynomial approximations. Experiments on GPT-2, Llama2-7B, and Mixtral 8×7B show about 2–3× latency reduction in secure inference with nearly no accuracy loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. SOAL requires no model retraining or architecture modification, increasing its compatibility with commercial LLM deployments.\n\n2. The paper demonstrates large latency reductions (≈2×–3× on major operations) while maintaining accuracy, which is significant for the secure inference community."}, "weaknesses": {"value": "- The main idea, i.e. using activation range reduction to simplify MPC protocols, draws heavily from prior work in quantization and outlier suppression (e.g., SmoothQuant, Outlier Suppression). The contribution is more engineering-oriented integration than a new theoretical principle.\n\n- The prefix-based outlier control is validated on a fixed set of prompts; the paper does not analyze robustness to domain shift (e.g., multi-language, code, long context). The approach may overfit the profiled activation statistics."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3TPBxba0mh", "forum": "Tmrjxq4d7w", "replyto": "Tmrjxq4d7w", "signatures": ["ICLR.cc/2026/Conference/Submission7142/Reviewer_RtFw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7142/Reviewer_RtFw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992090631, "cdate": 1761992090631, "tmdate": 1762919305249, "mdate": 1762919305249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}