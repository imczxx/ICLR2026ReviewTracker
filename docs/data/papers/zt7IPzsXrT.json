{"id": "zt7IPzsXrT", "number": 2845, "cdate": 1757274234830, "mdate": 1759898123429, "content": {"title": "Forget Many, Forget Right: Scalable and Precise Concept Unlearning in Diffusion Models", "abstract": "While multi-concept unlearning has shown progress, extending to large-scale scenarios remains difficult, as existing methods face three persistent challenges:  **(i)** they often introduce conflicting weight updates, making some targets difficult to unlearn or causing degradation of generative capability;  **(ii)** they lack precise mechanisms to keep unlearning strictly confined to target concepts, resulting in collateral damage on similar content;  **(iii)** many approaches rely on additional data or auxiliary modules, causing scalability and efficiency bottlenecks as the number of concepts grows.  To simultaneously address these challenges, we propose **Scalable-Precise Concept Unlearning (ScaPre)**, a unified and lightweight framework tailored for scalable and precise large-scale unlearning. ScaPre introduces a *conflict-aware stable design*, which integrates the spectral trace regularizer and geometry alignment to stabilize the optimization space, suppress conflicting updates, and preserve the pretrained global structure. Furthermore, the *Informax Decoupler* identifies concept-relevant parameters and adaptively reweights updates, ensuring that unlearning is confined to the target subspace without collateral damage. ScaPre yields an efficient closed-form solution, requiring no additional data or auxiliary sub-models, while maintaining both scalability and precision. Comprehensive experiments across large-scale objects, styles, and explicit content benchmarks demonstrate that ScaPre effectively removes target concepts while maintaining generation quality. It can forget up to **×5** more concepts than the best baseline within the limits of acceptable generative quality, and outperforms existing multi-concept approaches in precision and efficiency, achieving a new state of the art for large-scale unlearning.", "tldr": "", "keywords": ["machine unlearning", "large-scale unlearning", "diffusion model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc5de4d34e7e75fa23a4a44bb5ee19ee2d046745.pdf", "supplementary_material": "/attachment/a65546ad6eb22cafac4262024702be16cb33397b.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes ScaPre, a training‑free, closed‑form framework to erase many target concepts from text‑to‑image diffusion models while preserving quality on non‑targets. It stabilizes multi‑concept edits via a spectral trace regularizer (second‑order target statistics plus an SVD‑based overlap regulator), improves precision with a mutual‑information–guided \"Informax Decoupler\", and preserves global structure through Bures‑geometry alignment; the quadratic part reduces to a Sylvester system solved in one shot. On Stable Diffusion v1.4/1.5, ScaPre achieves state‑of‑the‑art large‑scale forgetting, e.g., Imagenette residual accuracy 0.8 (Table 1), Diversi50 3.9 (Table 3), Confuse5 overall 84.3 (Table 4), and unlearns 50 concepts in $\\sim$120 s on an A6000 (Fig. 3), while claiming up to x5 more concepts than baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Conflict‑aware spectral trace regularizer + SVD overlap control; MI‑guided channel reweighting; Bures‑alignment for geometry‑aware stability.\n\n- Convex quadratic core with unique Sylvester solution; clear derivations and proximal mapping.\n\n- Best residual accuracy/UQ on Imagenette, Diversi50, and Confuse5 with robust visuals.\n\n- 50‑concept unlearning in $\\sim$120s; low memory/time vs. baselines (Fig. 3)."}, "weaknesses": {"value": "- Construction of \"target/neutral\" inputs, threshold choice, and variance of MI estimates need clarification.\n\n- Heavy use of classifier‑based \"unlearn accuracy\" and CLIP; limited human/adversarial prompt evaluation.\n\n- Results limited to SD v1.4/1.5; unclear portability to SDXL/DiT architectures.\n\n- Deeper, main‑text ablations of S vs. R vs. Informax vs. geometry step and sensitivity of UQ's normalization would strengthen claims."}, "questions": {"value": "- How are \"target\" and especially \"neutral\" inputs instantiated for MI estimation without extra data? What sample size/thresholding is used, and how stable are channel scores across layers?\n\n- Which cross‑attention layers/branches (K vs. V) are edited, and how does performance vary layerwise?\n\n- Can you show results on SDXL/DiT to illustrate architectural generality or required modifications to S and R?\n\n- Do you test robustness to synonyms/negation/compositional prompts or other circumventions (e.g., multilingual)? A breakdown would substantiate \"precise\" forgetting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gVhWqdZe38", "forum": "zt7IPzsXrT", "replyto": "zt7IPzsXrT", "signatures": ["ICLR.cc/2026/Conference/Submission2845/Reviewer_4a91"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2845/Reviewer_4a91"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721610069, "cdate": 1761721610069, "tmdate": 1762916407773, "mdate": 1762916407773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ScaPre, a closed-form framework for scalable and precise concept unlearning in text-to-image diffusion models.\nIt addresses three challenges in large-scale unlearning: conflicting weight updates, imprecise forgetting causing collateral damage, and inefficiency from auxiliary modules."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The empirical validation is good, for example, ScaPre achieves the lowest unlearning accuracy on Imagenette while maintaining high CLIPcoco, outperforming other baselines.\n\n2. The algorithm is innovative and lightweight. The closed-form solution avoids iterative fine-tuning, and geometry alignment via Bures distance preserves global covariance structure better than L2 regularization. Also, the Informax Decoupler is reasonable.\n\n3. The benchmark construction. ImageNet-Confuse5 explicitly tests disentanglement of visually similar concepts (e.g., dog breeds), a realistic and challenging setting absent in prior work."}, "weaknesses": {"value": "1.  Some notation is ambiguous. For example, The symbol W is used for both the updated matrix and intermediate solution W⋆ without distinction in Sec. 4.3 (see Eq. (8)–(10)). In Appendix B.1, Eq. (11) redefines the objective with A = λI + S+R and B = diag(α), but these symbols are not introduced in the main text, breaking continuity.\n\n2. Random seeds, data splits for ImageNet-Diversi50/ImageNet-Confuse5, and prompt selection criteria are omitted (see Sec. 5.1), hindering replication.\n\n3. Fig. 3 reports GPU-hours and memory but omits per-concept scaling trends (e.g., time vs. number of concepts), critical for “scalable” claims (see Sec. 5.5)."}, "questions": {"value": "Please introduce A and B in the main text when first used in Eq. (8), aligning with Appendix B.1 notation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sVUPXrqCzG", "forum": "zt7IPzsXrT", "replyto": "zt7IPzsXrT", "signatures": ["ICLR.cc/2026/Conference/Submission2845/Reviewer_VrCc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2845/Reviewer_VrCc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825304382, "cdate": 1761825304382, "tmdate": 1762916406606, "mdate": 1762916406606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the challenges of conflicting weight updates, collateral damage to non-target concepts, and reliance on extra data in large-scale concept unlearning for text-to-image diffusion models, this paper proposes ScaPre (Scalable-Precise Concept Unlearning), a unified lightweight framework aiming for scalable and precise unlearning.\nScaPre integrates a conflict-aware stable design (spectral trace regularizer + geometry alignment) and an Informax Decoupler, achieving an efficient closed-form solution without extra data/sub-models; experiments show it can unlearn up to 5× more concepts than the best baseline while maintaining high generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper innovatively combines a conflict-aware stable design and an Informax Decoupler, which effectively solves the core problems of instability in large-scale unlearning and imprecise separation of target/non-target concepts, making up for the shortcomings of existing methods in large-scale scenarios.\n\nScaPre adopts a closed-form solution, ensuring high efficiency (unlearning 50 concepts in 120 seconds) and reproducibility without extra data or auxiliary modules; meanwhile, its experiments cover objects, styles, and explicit content benchmarks, with comprehensive and convincing results."}, "weaknesses": {"value": "1. Experimental benchmarks are mostly ImageNet-derived datasets (e.g., Imagenette, ImageNet-Diversi50), lacking evaluations on more complex and diverse real-world scenarios (e.g., dynamic concepts, cross-modal associated concepts), making it hard to verify the method’s practical generalization.\n\n2. It is suggested to explore ScaPre’s adaptability and performance changes on larger diffusion models (e.g., Stable Diffusion XL). \n\n3. It is suggested that the authors enhance and enrich the elaboration of the overview figure in this paper, as the current description of this figure is too simplistic, omits many details, and makes it difficult for readers to understand.\n\n4. In the proposed Imagenette benchmark, it is not explicitly specified what prompts are used for evaluation and whether these prompts are merely category names; thus, it is suggested that the authors use more complex prompts to evaluate the impact of prompts on the experimental results."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h8CIrZLzF3", "forum": "zt7IPzsXrT", "replyto": "zt7IPzsXrT", "signatures": ["ICLR.cc/2026/Conference/Submission2845/Reviewer_zBcc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2845/Reviewer_zBcc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926764701, "cdate": 1761926764701, "tmdate": 1762916405921, "mdate": 1762916405921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a challenge of large-scale concept unlearning in text-to-image diffusion models. While existing approaches can remove individual concepts, they struggle when scaling to multiple concepts simultaneously, facing issues with conflicting weight updates, imprecise unlearning boundaries, and computational scalability bottlenecks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured and easy to follow\n2. While the majority of other algorithms fine-tune the the entire model weights\n3. Addresses fundamental challenges through theoretically grounded components\n4. Comparison with other methods on a large-scale multi-concept unlearning (up to 50 concepts)\n5. Extensive appendix with math proofs and detailed results"}, "weaknesses": {"value": "1. No comparison with parameter efficient Unlearning methods (e.g., SEMU https://arxiv.org/abs/2502.07587)\n2. It would be beneficial to have the data from figure 3 in a from of a table in the appendix\n3. Based on the theoretical foundation the method should also work on other SOTA models (e.g., SDXL, Stable Diffusion 3.5, FLUX.1-dev, Qwen-Image); however, no experimental confirmation is presented in the paper"}, "questions": {"value": "see Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WbddLG1MTd", "forum": "zt7IPzsXrT", "replyto": "zt7IPzsXrT", "signatures": ["ICLR.cc/2026/Conference/Submission2845/Reviewer_RFw7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2845/Reviewer_RFw7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762178673642, "cdate": 1762178673642, "tmdate": 1762916405354, "mdate": 1762916405354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}