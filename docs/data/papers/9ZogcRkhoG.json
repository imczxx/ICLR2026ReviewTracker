{"id": "9ZogcRkhoG", "number": 23683, "cdate": 1758347166232, "mdate": 1759896801661, "content": {"title": "Representing local protein environments with machine learning force fields", "abstract": "The local structure of a protein strongly impacts its function and interactions with other molecules. Representing local biomolecular environments remains a key challenge while applying machine learning approaches over protein structures. The structural and chemical variability of these environments makes them challenging to model, and performing representation learning on these objects remains largely under-explored.  In this work, we propose representations for local protein environments that leverage intermediate features from machine learning force fields (MLFFs). We extensively benchmark state-of-the-art MLFFs—comparing their performance across latent spaces and downstream tasks—and show that their embeddings capture local structural (e.g., secondary motifs) and chemical features (e.g., amino acid identity and protonation state), organizing protein environments into a structured manifold. We show that these representations enable zero-shot generalization and transfer across diverse downstream tasks. As a case study, we build a physics-informed, uncertainty-aware chemical shift predictor that achieves state-of-the-art accuracy in biomolecular NMR spectroscopy. Our results establish MLFFs as general-purpose, reusable representation learners for protein modeling, opening new directions in representation learning for structured physical systems.", "tldr": "We show that embeddings from machine learning force fields provide rich, transferable representations of local protein environments, enabling zero-shot generalization and state-of-the-art downstream performance.", "keywords": ["Machine learning force fields", "structural biology", "NMR", "representation learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51e37d6f865ca42137e6238c108da6197d53ac46.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the use of MLFFs as general-purpose representation learners for local protein environments. Instead of relying on sequence-based or handcrafted descriptors, the authors repurpose latent embeddings from pretrained MLFFs (AIMNet, MACE, OrbNet, Egret) as compact, physics-grounded descriptors for atomic neighborhoods in proteins. The study benchmarks MLFF embeddings on diverse downstream tasks—secondary structure and amino acid classification, pKa prediction, and NMR chemical shift regression—and shows that these representations outperform or rival specialized baselines such as PropKa and pKa-ANI. Overall, the paper establishes MLFFs as reusable, physics-informed foundation models for structural biology."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The central insight, repurposing MLFF embeddings as general-purpose, transferable protein environment representations, is both novel and timely. Most prior MLFF applications focus on energy or force prediction for small molecules; extending them to protein representation learning is original and valuable. The paper effectively bridges quantum-chemistry-based potentials and protein machine learning.\n\n2. The experimental setup is comprehensive. The authors curate 165 k environments from 1048 proteins and evaluate four MLFF families on four biologically relevant tasks. Comparisons include classical and ML baselines (PropKa, pKa-ANI, UCBShift2-X). Results demonstrate meaningful improvements in both accuracy and interpretability. The inclusion of uncertainty quantification and physical consistency tests (e.g., ring-current effects) adds rigor.\n\n3. The methodology that extracting embeddings from pretrained MLFFs and mapping them to canonical residue-centered environments is sound and clearly motivated. Statistical reporting (mean absolute errors, standard deviations) is adequate, and all experiments appear well-controlled.\n\n4. The paper is well written and pedagogically organized. The introduction clearly motivates the challenge of representing local protein environments; figures (e.g., Fig. 1 and 2) effectively illustrate how embeddings are constructed and used. Terminology (canonical environment, focus residue, MLFF feature extraction) is consistent and accessible even to readers outside computational chemistry.\n\n5. This work could substantially influence both computational biology and machine-learning communities by providing a physics-consistent alternative to sequence-only protein language models. MLFF embeddings encode quantum-derived information unavailable in existing representations and show transferability to tasks requiring local chemical precision. The idea of using pretrained MLFFs as “foundation models for atoms” could be broadly significant."}, "weaknesses": {"value": "1. The study restricts environments to 5 Å radius regions; while suitable for local chemistry, it omits long-range electrostatic or conformational effects. For tasks like folding or binding prediction, this locality may be insufficient. A discussion or experiment extending to multi-scale contexts would strengthen generality claims.\n\n2. MLFFs such as MACE and OrbNet require quantum-level pretraining on millions of molecules. Although embeddings are reused, the computational barrier to obtaining them limits accessibility compared to pretrained sequence models (e.g., ESM, ProtT5). The paper could better address scalability and efficiency trade-offs.\n\n3. Qualitative case studies (e.g., helix → strand unfolding) are insightful but anecdotal. Quantitative metrics (e.g., correlation between embedding distances and RMSD/chemical similarity) would solidify interpretability claims.\n\n4. While the authors compare to physics-based predictors, they do not benchmark against modern structure-aware geometric encoders (e.g., GVP-GNN, ProteinNeRF, FrameFold). Including such baselines would clarify whether MLFF embeddings provide advantages beyond standard geometric message-passing."}, "questions": {"value": "1. Could the same MLFF-based representation transfer to nucleic acids or protein–ligand complexes, where local environments include non-canonical atoms and charges?\n\n2. MLFFs have multiple internal layers encoding different orders of interaction (0th, 1st, 2nd). Did the authors investigate which layer yields the most informative embeddings for downstream tasks?\n\n3. Since MLFF embeddings originate from networks with different scales and symmetries, how are they aligned or normalized across model families?\n\n4. The authors mention uncertainty-aware predictions. Is uncertainty derived from ensemble variance, likelihood width, or another calibration technique?\n\n5. Could MLFF embeddings serve as complementary features to pretrained protein language models, bridging physics-based and sequence-based representations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HWRYLMQkQf", "forum": "9ZogcRkhoG", "replyto": "9ZogcRkhoG", "signatures": ["ICLR.cc/2026/Conference/Submission23683/Reviewer_fbDS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23683/Reviewer_fbDS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761564400479, "cdate": 1761564400479, "tmdate": 1762942763567, "mdate": 1762942763567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Local protein environments contain highly relevant chemistry that affects its function and interactions. This paper benchmarks existing machine learning force field (MLFF) methods on their ability to understand local protein structures. They evaluate a suite of MLFFs on their ability to predict the protonation state, secondary structure, and amino acid types."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper rigorously evaluates multiple popular MLFF methods on across benchmarks and connect them to some protein structure related tasks.\nThey perform an interesting analysis on the chemical shift prediction."}, "weaknesses": {"value": "The evaluation benchmarks such as secondary structure, amino acid type prediction are a bit straightforward.\n\nWhile MLFF naturally learn the physics of local structural environments, there are other machine learning based approaches that reason over the local structure and are suitable to predict protonation state, secondary structure, and amino acid types. This work does not benchmark MLFF versus these methods on representing local structure.\n\n[1] Simulating 500 million years of evolution with a language model. Hayes et al.\n[2] 3D deep convolutional neural networks for amino acid environment similarity analysis. Torng et al.\n[3] Distilling Structural Representations into Protein Sequence Models. Ouyang-Zhang et al."}, "questions": {"value": "Are there any other interesting local protein structure properties to evaluate on?\n\nHow well do non-FF based local structure models, such as ESM3, perform on these benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lBpwnpZL5c", "forum": "9ZogcRkhoG", "replyto": "9ZogcRkhoG", "signatures": ["ICLR.cc/2026/Conference/Submission23683/Reviewer_RxQj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23683/Reviewer_RxQj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933578346, "cdate": 1761933578346, "tmdate": 1762942763307, "mdate": 1762942763307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes repurposing intermediate embeddings from pre-trained Machine Learning Force Fields (MLFFs) as \"physics-grounded\" feature representations for local protein environments. The authors benchmark embeddings from MACE, AIMNet, and others, claiming state-of-the-art (SOTA) results in $pK_{a}$ prediction (reportedly outperforming pKa-ANI) and NMR chemical shift prediction (reportedly outperforming UCBShift2-X). The method's physical realism is validated via case studies, such as aromatic ring current effects."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Repurposing MLFFs as \"foundation models\" for structural biology is in nnovative and valuable.   \n2. The design of the validation experiments (e.g., ring current effect , helix unfolding ) is a commendable standard for physical realism.   \n3.The paper effectively demonstrates the general-purpose nature of the embeddings for zero-shot clustering and generative guidance."}, "weaknesses": {"value": "1.The paper's central claims rest on comparisons against pKa-ANI and UCBShift2-X that appear to be factually incorrect or based on flawed implementations.\n\n2.The $pK_{a}$ evaluation is missing the entire 2024/2025 SOTA, invalidating its performance claims.\n\n3.The paper compares its structural embeddings against sequence (ESM) embeddings. It critically fails to benchmark against the most obvious and relevant competitors: other structural embeddings, namely those from the AlphaFold2 or ESMFold structure modules."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cfD0Tm3rgx", "forum": "9ZogcRkhoG", "replyto": "9ZogcRkhoG", "signatures": ["ICLR.cc/2026/Conference/Submission23683/Reviewer_dp5R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23683/Reviewer_dp5R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968480146, "cdate": 1761968480146, "tmdate": 1762942763027, "mdate": 1762942763027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes to leverage pretrained ML interatomic potential embeddings as canonical descriptors of local protein environments. Embeddings are extracted from a residue-centered neighborhood. Various applications including pKa prediction of titrable residues and protein NMR chemical shift prediction are explored, with MLFF-based embeddings show good representative power for downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Repurposing latent features of MLFFs as canonical protein descriptors is a timely, well-motivated idea that links quantum-level atomistic modeling with biomolecular representation learning.\n- The paper covers a solid range of downstream tasks tied to experimental observables and provides thorough analysis of the physical plausibility of its predictions."}, "weaknesses": {"value": "- Dataset and baselines: For the pKa and NMR shift tasks, the baselines are evaluated in conditions that differ from their intended use, whereas the MLFF-feature models introduced here are trained directly for the target objective. For instance, the pKa baselines are designed to predict experimental values, while the proposed methods are trained to reproduce a cheaper computational reference. This creates a benchmark mismatch, since the baselines are not optimized for the reference chosen in this work. Because the core question is whether MLFF features are useful, a more appropriate primary comparison would be a standard GCN with simple learned embeddings rather than prior task-specific baselines. This issue is compounded by the use of AFDB structures as inputs.\n- Experimental design: Some experiments are not very indicative of how the embeddings would be used in practice. Inferring amino acid type or secondary structure from full atomic coordinates (Section 4) is a trivial task under those inputs, and the distribution-shift analysis in Section 5 (Fig. 2) mainly shows that embeddings from energy-relaxed structures are similar, which is expected since both MLFFs and classical force fields approximate the same physical energy landscape. Several experiments currently placed in the appendix appear more compelling and better aligned with the paper’s motivation. I recommend reorganizing the manuscript so that the most informative use cases are in the main text, especially given that all experiments are listed as contributions but not all are presented in the main body."}, "questions": {"value": "- How sensitive are results to the MLFF layer chosen for embeddings?\n- MACE is an architecture, but there are several variants according to the dataset it is trained on. It seems like the MACE-OFF23 is used in this work, could authors elaborate?\n- For AF3-guided structure selection in Fig A14, could authors report the optimization statistics similarly to Fig A13? (What fraction of optimization result in the target structure, or whether it always lead to the target structure)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PcedQpzSMg", "forum": "9ZogcRkhoG", "replyto": "9ZogcRkhoG", "signatures": ["ICLR.cc/2026/Conference/Submission23683/Reviewer_AnCg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23683/Reviewer_AnCg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998332617, "cdate": 1761998332617, "tmdate": 1762942762774, "mdate": 1762942762774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response Part 1"}, "comment": {"value": "We thank all reviewers for their thoughtful and constructive feedback. Several common themes emerged across the reviews, in particular, the relationship between simulated and experimental targets, baseline coverage and ablation studies. We summarize our main clarifications and new experiments on these shared points below, and we have updated the manuscript accordingly so that the changes are visible there as well.\n\n## Comparisons with experimental values\n\n**Critique.** Reviewers AnCg and dp5R raised concerns about mismatches between our simulated training targets and experimental baselines.\n\n**Response.** \nFor pKa, we clarified in the text that Table 1 reports results on **simulated** targets, which makes the comparison conservative and biased against methods trained on experimental values, and we added new evaluations on the experimental datasets associated with PropKa and pKa-ANI (Tables B.6 and B.7). For PropKa, we compare on the experimental set used for its training; for pKa-ANI, we evaluate on the updated PKAD-R dataset, noting that the original work did not publish its exact train–test split. In both cases, the protocols are biased in favor of the baselines, yet our model outperforms PropKa on more than 50% of its entries and pKa-ANI on 3 of the 4 residue types considered. The comparison results are presented in the tables below.\nFor chemical shifts, our model is trained directly on **experimental** values from RefDB with sequence-similarity–based train/test clustering, and our comparison to UCBShift2 is biased in UCBShift2’s favor because we did not remove RefDB entries that may have been included in its training set.\n\n| PDB ID | Glu acid PropKa | Glu acid (Ours) | Asp acid PropKa | Asp acid (Ours) |\n|--------|------------------|----------------------|-----------------|----------------------|\n| 1PGA:A | 0.413            | **0.294**            | 0.362           | **0.292**            |\n| 1IGD:A | 0.489            | **0.182**            | 0.352           | **0.274**            |\n| 1A2P:A | **0.525**        | 1.046                | 0.426           | **0.403**            |\n| 4ICB:A | 0.579            | **0.546**            | **0.299**       | 1.018                |\n| 1BEO:A | –                | –                    | 0.730           | **0.511**            |\n| 4LZT:A | **0.629**        | 0.665                | 0.488           | **0.404**            |\n| 3RN3:A | 0.548            | **0.462**            | 0.423           | **0.412**            |\n| 2RN2:A | 0.274            | **0.185**            | 0.683           | **0.585**            |\n| 2OVO:A | 0.377            | **0.376**            | **0.180**       | 1.571                |\n| 1XNB:A | **1.001**        | 1.270                | **0.436**       | 1.020                |\n| 135L:A | 0.470            | 1.700                | **0.480**       | 1.043                |\n\n\n| Amino acid     | pKa-ANI | Ours  |\n|----------------|---------|-------------|\n| Glutamic Acid  | 0.696   | **0.695**   |\n| Aspartic Acid  | 0.978   | **0.831**   |\n| Lysine         | 0.648   | **0.360**   |\n| Histidine      | **0.899** | 0.925     |"}}, "id": "ScD8pnskx5", "forum": "9ZogcRkhoG", "replyto": "9ZogcRkhoG", "signatures": ["ICLR.cc/2026/Conference/Submission23683/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23683/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23683/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763659612787, "cdate": 1763659612787, "tmdate": 1763660160600, "mdate": 1763660160600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}