{"id": "elFNG9Qr6p", "number": 4657, "cdate": 1757737239695, "mdate": 1759898021320, "content": {"title": "Zero-Order Sharpness-Aware Minimization", "abstract": "Prompt learning has become a key method for adapting large language models to specific tasks with limited data. However, traditional gradient-based optimization methods for tuning prompts are computationally intensive, posing challenges for efficiency. We introduce ZOSA (Zero-Order Sharpness-Aware Minimization), a novel optimization framework that integrates zero-order optimization with sharpness-aware minimization to enhance prompt tuning. ZOSA employs Rademacher perturbation vectors to estimate gradients without requiring backpropagation. By incorporating sharpness-aware principles, it targets flat minima in the loss landscape, improving generalization. An adaptive learning rate, guided by loss variability, further ensures stable convergence. Experiments on few-shot learning tasks, such as text classification and natural language inference, show that ZOSA significantly outperforms existing methods. With its theoretical foundation and computational efficiency, ZOSA offers a practical solution for prompt-based learning in resource-limited settings.", "tldr": "ZOSA is a novel zero-order sharpness-aware minimization framework that integrates Rademacher perturbations and adaptive loss-variance scaling to enable efficient prompt tuning for large language models.", "keywords": ["Zero-order optimization", "Sharpness-aware minimization", "Prompt tuning", "Gradient estimation", "Generalization bounds"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36b3550c9c16a68f4d902612669d5f401037df3d.pdf", "supplementary_material": "/attachment/05d6b4ba6a1f5810e71a68064f07c74bb99f739b.zip"}, "replies": [{"content": {"summary": {"value": "The paper combines zero-order optimization with sharpness-aware minimization. Convergence and generalization guarantees are provided for the algorithm. Experiments on synthetic functions and a zero-order prompt tuning benchmark is provided."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The final method has good performance on synthetic functions and demonstrates on par performance with some selected baselines on a zero-order prompt tuning benchmark."}, "weaknesses": {"value": "- Introducing SAM to zero-order optimization seems like an engineering trick to try, but I don't believe that it is a novel idea.\n- SAM is introduced as an addition to zero-order optimization, but neither the theoretical results nor the experiments demonstrates strong evidence that this is a good addition. It is not clear which baselines differ from ZOSA in exactly just the SAM component. Seems like a missing ablation to me.\n- Figure 1 and 2 are not well made and very to parse.\n- The title is too general when the scope of the method is only two specific scenarios."}, "questions": {"value": "- The rate in FZOO for the dL term is O(1/T) (Theorem 3.6) instead of O(1/sqrt(T)), so it is a big strange that the current work, which is FZOO + SAM, only gets O(1/sqrt(T)). Is that a typo in FZOO?\n- The assumptions in Theorem 3.6 of FZOO and Theorem 4.3 in the paper is quite similar. The SAM related terms could also be from related work in the SAM literature. Can the authors clarify the novelty in the proof technique for Theorem 4.3?\n- Why are R-AdaZO and FZOO not baselines for Sec 5.2? If the paper improves upon these methods (as mentioned throughout the apper), they should be baselines so that we can understand if the SAM component is actually useful for the task.\n\nTypos:\n- (Malladi et al., 2023a) is the right citation for MeZO, not (Malladi et al., 2023b). This is repeated for all references to MeZO.\n- line 135: n-ZO should be N-ZO\n- line 814: simulateties"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j5cPtPcht3", "forum": "elFNG9Qr6p", "replyto": "elFNG9Qr6p", "signatures": ["ICLR.cc/2026/Conference/Submission4657/Reviewer_k6pv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4657/Reviewer_k6pv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761006799256, "cdate": 1761006799256, "tmdate": 1762917496803, "mdate": 1762917496803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ZOSA (Zero-Order Sharpness-Aware Minimization), a novel optimization framework designed to efficiently tune prompts for Large Language Models (LLMs) in resource-constrained environments where gradient information is unavailable or computationally expensive. ZOSA integrates zero-order optimization techniques with the principles of Sharpness-Aware Minimization (SAM). The method employs adaptive learning rates guided by loss variance (σ_t). Theoretical analysis establishes convergence guarantees and generalization bounds. Experiments on synthetic functions and few-shot learning tasks (GLUE benchmarks) demonstrate ZOSA's superiority over existing ZO methods in terms of convergence speed and performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clean integration of SAM into ZO using a two-point estimator and loss-std normalization, yielding a normalized-SAM view. This offers a principled way to bias toward flatter minima without backprop.\n\n2. The theoretical framework is well-developed with convergence analysis and generalization bounds.\n\n3. Empirical results on both synthetic non-convex functions and real-world GLUE prompt tuning tasks demonstrate superior convergence speed and higher accuracy/F1 scores compared to a comprehensive suite of ZO baselines and evolutionary algorithms. This validates the effectiveness of the sharpness-aware mechanism for enhancing generalization in practice."}, "weaknesses": {"value": "1. Limited novelty in components: Each individual component (batched Rademacher perturbations and variance reduction from FZOO, SAM-like perturbations from SABO) exists in prior work. The contribution is primarily in the combination, which while valuable, is somewhat incremental.\n\n2. The generalization bound in Theorem 4.4 assumes convexity of the loss function, which is restrictive for neural networks and conflicts with the non-convex assumptions elsewhere in the paper.\n\n3. Experimental comparisons are incomplete: 1) In LLM experiments, comparisons with recent methods like FZOO as well as recent black-box prompt tuning methods are missing. 2) No wall-clock time comparisons, only iteration counts. 3) Limited analysis of memory consumption in practice.\n\n4. The method introduces multiple hyperparameters $ \\rho, \\epsilon, m, \\eta$ requiring grid search over wide ranges. The sensitivity to these choices and guidelines for setting them are not thoroughly discussed.\n\n5. The paper doesn’t clearly state the base LLM(s), tokenizer, prompt templates, few-shot sampling protocol.\n\n6. All NLP tasks are classification; it would be informative to include generation tasks (e.g., mathematical reasoning, summarization) and instruction-following to test generalization beyond GLUE-style metrics.\n\n7. The conclusion states O(1/T) convergence, whereas Theorem 4.3 and the surrounding discussion align with $O(1/\\sqrt{T})$ in ZO settings. Please reconcile or correct."}, "questions": {"value": "1. You compare to SABO/evolutionary methods, but not to FZOO and recent black-box prompt-tuning methods. Can you add more baselines—or explain if there are incompatibilities?\n\n2. Please report wall-clock time, #forward passes, and peak memory in LLM experiments. Also clarify whether all methods share the same forward-pass budget and how you determine “convergence steps”.\n\n3. Which base model(s), prompt templates, and few-shot k did you use for each dataset? How does ZOSA scale to even larger models?\n\n4. Have you conducted ablations to isolate the contributions of (a) SAM-like perturbation, (b) variance-based adaptive scaling, and (c) batched Rademacher estimation (compared with Gaussian perturbation as in MeZO)? This would clarify which components contribute most to performance gains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UxMB9uBRKW", "forum": "elFNG9Qr6p", "replyto": "elFNG9Qr6p", "signatures": ["ICLR.cc/2026/Conference/Submission4657/Reviewer_yrgf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4657/Reviewer_yrgf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491708961, "cdate": 1761491708961, "tmdate": 1762917496020, "mdate": 1762917496020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ZOSA, a zeroth-order optimizer that combines: random-direction ZO,loss-std normalization(σ) to stabilize step magnitudes, and a SAM-style inner perturbation. The theory gives ZO-SGD-order convergence and argues a bias toward flatter minima (low trace Hessian). Experiments cover synthetic functions and zero-order prompt tuning for LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. minimal, clean SAM analogue in ZO with no gradient access; σ-normalization is used coherently for both the inner radius and the outer step.\n2.Standard nonconvex ZO rate with explicit dependence on m,d,$\\rho$ and a flatness argument (trace-Hessian bias). \n3.Batched Rademacher directions are GPU-friendly; the algorithm is simple to implement."}, "weaknesses": {"value": "1 Results are largely reported vs iterations, while ZOSA uses two probes/step and, on synthetic tasks, very large (m) and per-step query counts. There are no fixed-budget (equalized function queries) plots, no wall-clock comparisons, and no per-step cost breakdown, unlike recent fast ZO work that foregrounds efficiency.\n    \n2.Synthetic functions under-specified and potentially biased. High-dimensional success at (d=10^4) relies on very large query budgets per step, which likely masks estimator variance—this setting appears tailored to favor the method without demonstrating real compute efficiency.\n    \n3.The paper doesn’t map dimension d to architecture (e.g., soft-prompt length × embedding dim / low-rank adapter), nor specify batching rules (whether the same mini-batch is reused for inner/outer probes). Given σ is computed from batch-level losses and used twice (radius and step), batch size and reuse/mismatch are critical for stability.\n\n4.Claims about lower trace Hessian are not empirically corroborated."}, "questions": {"value": "Please add loss/accuracy vs (i) equalized function-query budgets and (ii) wall-clock time, and report per-step query counts for all methods. Current iteration-based plots do not isolate true efficiency.\n\nMove exact definitions/conditioning/noise into the main text. Justify the large (m)/queries per step at (d=10^4); also provide fixed-budget and wall-clock plots to avoid conflating early convergence with compute efficiency.\n\nPrecisely define dimension d .Provide a per-dataset hyper-parameter table,Clarify batch reuse between ($\\theta$) and ($\\theta+\\epsilon_{\\text{sam}}$). If different, show robustness to batch mismatch.\n\nFor (d=1000) in LLMs, run fixed-budget comparisons and show how m (or queries/step) must grow with d to maintain performance. If the method targets moderate d under practical budgets, state this scope explicitly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Mx71MGj2bd", "forum": "elFNG9Qr6p", "replyto": "elFNG9Qr6p", "signatures": ["ICLR.cc/2026/Conference/Submission4657/Reviewer_Yb7p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4657/Reviewer_Yb7p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905152665, "cdate": 1761905152665, "tmdate": 1762917495293, "mdate": 1762917495293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ZOSA, a zeroth-order optimization method that replaces gradient access with two forward passes: first it estimates a gradient using batched one-sided Rademacher perturbations, then takes an adaptive-scaled step in the direction of this estimated gradient (normalizing by estimated loss variance). It then repeats the gradient estimation process at the perturbed point, and makes its final step in the direction of this estimate. ZOSA outperforms other zeroth-order methods at tuning a continuous prompt vector for an LLM across several tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The experiments are reasonably broad, covering standard synthetic objectives and black-box prompt tuning on popular NLP benchmarks, comparing to several ZO baselines.\n\n- The method shows consistent gains over these black-box baselines in both convergence speed and downstream accuracy across tasks and dimensions."}, "weaknesses": {"value": "- I think the main weakness of the paper is limited theoretical novelty. I am by no means an expert on this area, so I am willing to be corrected on any/all of the following points and their technical difficulty:\n\n    - Properties 3.1 and 3.2 are proven in existing ZO literature\n\n    - The proof of Theorem 4.3 seems to follows standard smooth nonconvex analysis for ZO methods (e.g., in Ghadimi and Lan 2013), and obtains the same rate, with some modifications for the extra normalization term.\n\n    - The proposed \"equivalence\" to SAM does not seem rigorous---it is called an approximation in the appendix\n   \n    - The SABO paper includes a very similar PAC-Bayes sharpness-aware bound for a ZO method\n\n- The paper compares heuristically to MeZO (e.g., L48--L50, L78--L79) but never compares to it empirically.\n- I'm not sure about the proof of \"SAM-equivalence\" (see below)\n- [minor] The LLM experiments tune a continuous vector for an open-weights model, but don't compare to parameter-efficient fine-tuning methods that tune continuous parameters (e.g., LoRA, soft prompt tuning)"}, "questions": {"value": "- How does the proposed method compare empirically to MeZO?\n- Typo in conclusion: should say 1/sqrt(T) rate?\n- Why have Theorem 4.3 apply to smooth nonconvex fns but then assume convexity in Theorem 4.4?\n- The Nesterov and Spokoiny reference is not the correct paper?\n- [SAM-equivalence] What happened to epsilon in Eqn 9 / Eqn 66? Shouldn't it be $\\rho / \\epsilon$? Don't we need to carry this through the rest of the argument?\n- In eqn 31, isn't the second term 0 for Rademacher $u_i$'s?\n- Are the minima found by ZOSA actually flatter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zklmyxnIIS", "forum": "elFNG9Qr6p", "replyto": "elFNG9Qr6p", "signatures": ["ICLR.cc/2026/Conference/Submission4657/Reviewer_T8un"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4657/Reviewer_T8un"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762233805139, "cdate": 1762233805139, "tmdate": 1762917494689, "mdate": 1762917494689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}