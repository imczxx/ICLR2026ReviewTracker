{"id": "ukGxWd2aDG", "number": 13701, "cdate": 1758221124254, "mdate": 1759897418653, "content": {"title": "REAP the Experts: Why Pruning Prevails for One-Shot MoE compression", "abstract": "Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient pre-training and low latency but their large parameter counts create significant memory overhead, motivating research into expert compression. Contrary to recent findings favouring expert merging on discriminative benchmarks, we demonstrate that expert pruning is a superior strategy for generative tasks. We prove that merging introduces an irreducible error by causing a \"functional subspace collapse\", due to the loss of the router's independent, input-dependent control over experts. Leveraging this insight, we propose Router-weighted Expert Activation Pruning (REAP), a novel pruning criterion that considers both router gate-values and expert activation norms. Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP consistently outperforms merging and other pruning methods on generative benchmarks, especially at 50\\% compression. Notably, our method achieves near-lossless compression on code generation and tool-calling tasks with Qwen3-Coder-480B and Kimi-K2, even after pruning 50\\% of experts.", "tldr": "This paper argues that pruning experts is superior to merging them for compressing MoE LLMs and introduces a new method, REAP,  that achieves nearly lossless performance on generative tasks by preserving the model's functional structure.", "keywords": ["mixture-of-experts", "moe", "compresson", "expert pruning", "expert merging", "merging", "pruning", "LLM", "evaluation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/401bf654d374b32ba55468cbf154a9afb11394ac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper challenges the recent trend favoring expert merging for compressing Sparse MoE models, arguing instead that expert pruning is superior for generative tasks. The authors provide theoretical analysis showing that merging causes \"functional subspace collapse\" due to loss of independent router control over experts, while pruning preserves this control. They propose REAP (Router-weighted Expert Activation Pruning), a novel pruning criterion combining router gate values and expert activation norms. Experiments across models from 20B to 1T parameters demonstrate REAP's advantages on generative benchmarks, achieving near-lossless compression at 50% on code generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The theoretical insight in this work, that expert merging introduces irreducible error proportional to router policy variability, is insightful, novel, and well-presented.\n\n* Evaluation is extensive across many models and tasks\n\n* REAP is simple to implement and computationally efficient\n\n* The paper is overall well-structured with good motivation and intuitive explanations"}, "weaknesses": {"value": "* Theorem 1's proof assumes \"independence between the router policy and expert functions.\" This is barely possible in trained MoE models where routers and experts are jointly optimized. \n\n* The proposed method does not always outperform expert merging baselines, such as Qwen3-30B-A3B on WildBench at 50% ratio. \n\n* I would love to see more comparisons with other MoE-specific compression methods, such as quantization [1] [2].\n\n\n[1] Li, P., Jin, X., Tan, Z., Cheng, Y. and Chen, T., 2024. QuantMoE-Bench: Examining Post-Training Quantization for Mixture-of-Experts. arXiv preprint arXiv:2406.08155.\n\n[2] Duanmu, H., Li, X., Yuan, Z., Zheng, S., Duan, J., Zhang, X. and Lin, D., 2025. MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design. arXiv preprint arXiv:2505.05799."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "W8sOvEAp0D", "forum": "ukGxWd2aDG", "replyto": "ukGxWd2aDG", "signatures": ["ICLR.cc/2026/Conference/Submission13701/Reviewer_Yea5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13701/Reviewer_Yea5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708461139, "cdate": 1761708461139, "tmdate": 1762924251796, "mdate": 1762924251796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes REAP (Router-weighted Expert Activation Pruning) for pruning experts in Mixture-of-Experts model. They show thorough empirical validation that pruning outperforms merging methods on generative tasks. REAP prunes MoE experts based on router gates and activation norms. The central claim seems to be that merging causes \"functional subspace collapse\" by losing the router's input-dependent control, while pruning preserves independent expert modulation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) Well written introduction (and comprehensive related work) of concepts like model merging, expert pruning etc.\n2) The intuition is sound in section 3.1: merging forces a static convex combination when you need a dynamic one.\n3) I appreciate the effort to formalize the merging and pruning problem and provide mathematical intuition for why pruning outperforms merging.\n3) Strong/thorough empirical validation."}, "weaknesses": {"value": "In general, I understand the authors' core ideas behind REAP, and empirical results are convincing. My major concern is that the mathematical presentation has significant issues (in my view). Perhaps there needs to major rewording where authors state them as \"mathematical intuition\" rather than formal proofs, explicitly acknowledging the simplifying assumptions while emphasizing that the approach is validated by strong empirical evidence.\n\nMajor Weaknesses\n1) I am not convinced by Section 3.3 experiments and the related claim. Authors show PCA (PC1, PC2) plots to claim it as the empirical evidence for loss of independent control of \"degrees of freedom\" when one does model merging vs pruning. \n- PCA by definition is reducing from $n$ dimensions to $k << n$ dimension ($k=2$) and what if independent control is preserved in dimensions 3-100 but lost in the first 2 PCs? What about variance? (should report variance explained by PC1+PC2)\n- Since this is towards understanding functional subspace (potentially high dimensional) a more convincing setup/experiments would require quantitative metrics like: rank of expert output matrix, any information-theoretic metric etc. \n- Current setup would be stronger as \"preliminary empirical observations\" rather than conclusive evidence.\n- The real evidence should come from performance metrics and more direct measures of expert independence.\n\n2) Missing Rigor in \"Irreducible Error\" Claim in general: I have similar issues in this part as well as point 3 below. I understand the intuitive sketch but I think stating it as formal proof is a bit of stretch. For this to be a rigorous claim one needs to proof that 1) that this is a lower bound over all possible merged representations or 2) this is the optimal form or that no other parameterization could do better.\n\n3) Theorem 1: Despite stating the independence assumption I believe there are mathematical errors in this and subsequent sections. For example: authors write the $||\\Delta_{i,j}||^2$ outside the expectation, but $\\Delta_{i,j}$ depends on $x$, so that expression does not look correct (at-least without more explanation). Similarly the three terms in Equation 5 are not rigorous and one cannot generally factor this into three separate expectations without making strong independence assumptions (which are also not stated completely) . Equation 6 also seems to have similar issues.\n\n4) *However, since our goal is to prune unimportant experts, we can reasonably assume their gate-values are small w* --> At $50\\\\%$ compression, one is pruning half the experts - Some of these will NOT have small gate values - authors use the approximation to derive the metric, then apply it globally - so I believe the explanation in this section is not entirely correctly stated with missed statements.\n\n5) There is a disconnect between Section 3 and 4. Whatever the authors use in section 3 does not reflect in Section 4 (ex: no mention/use of expert differences, policy variability? among other issues.)\n\n6) Not sure if I missed this but it's hard to see in Figure 2 the claim of near-lossless compression at any task. Ex: wrt to EAN (another method). I saw that the difference between the two is not much? Since this is an impactful claim - it might be better or more cleanly demonstrated? \n\nMinor Weaknesses\n1) Section 3.1: the equations or the explanation in general in this section can be cleared a bit (especially *\"define the router’s input-dependent mixing ratio ...\"*)."}, "questions": {"value": "1) Section 5 (experimental setup):  *We compress models by pruning or merging 25% or 50% of experts in each layer* - are these experts selected at random (for non M-SMoEs)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2YsFv3FSBU", "forum": "ukGxWd2aDG", "replyto": "ukGxWd2aDG", "signatures": ["ICLR.cc/2026/Conference/Submission13701/Reviewer_5xpD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13701/Reviewer_5xpD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817107612, "cdate": 1761817107612, "tmdate": 1762924251414, "mdate": 1762924251414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates compression techniques for Sparse Mixture-of-Experts (SMoE) models, challenging recent claims that expert merging outperforms pruning. The authors provide theoretical analysis showing that merging introduces irreducible error through \"functional subspace collapse\" - essentially, when you merge experts, the router loses its ability to independently control them based on input. They propose REAP (Router-weighted Expert Activation Pruning), which selects experts to prune by considering both router gate values and activation norms. The method is evaluated across models ranging from 20B to 1T parameters, with testing on generative tasks like code generation, creative writing, and math reasoning. Results show REAP consistently outperforms merging methods on generative benchmarks, particularly at 50% compression ratios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors offer a clean, intuitive analysis showing that merging induces an irreducible error proportional to router policy variability. \n\n2. REAP’s design combines router gate strength with expert activation norms. It is conceptually simple yet empirically robust. It scales efficiently to trillion-parameter models and operates without fine-tuning.\n\n3. The experiments span multiple model families, from 20B to 1T parameters, and cover diverse benchmarks (code, creative writing, math, tool use). The generative evaluation is a standout—showing that REAP maintains performance where merging sharply degrades.\n\n4. The PCA visualizations in Figures 1 and A4 are interesting. They effectively demonstrate the functional collapse phenomenon and show how pruning maintains the geometric structure of the expert manifold while merging contracts everything toward the center."}, "weaknesses": {"value": "1. This paper focuses on one-shot compression, which is fair for the “no retraining” setting, but most real-world deployments fine-tune after merging. It’s unclear whether the theoretical and empirical gaps between merging and pruning persist once merged experts are lightly retrained.\n\n2. The independence assumption between router policy and expert function in the irreducible error derivation (Eq. 5) is strong and may not hold in practice, especially in late layers where experts co-adapt. This limits how far the theorem generalizes.\n\n3. Section 5.1 highlights that calibration data domain critically affects compression success. This introduces a sensitivity that could complicate real deployment, but the paper doesn’t analyze how REAP behaves under mismatched calibration domains."}, "questions": {"value": "1. Would merging still underperform pruning if merged experts were allowed limited post-merge fine-tuning? How much of the “functional subspace collapse” could be recovered through optimization?\n\n2. How sensitive is REAP to the assumed linearity between gate-values and activation magnitudes? Would a nonlinear scaling or normalization improve robustness across layers?\n\n3. Can REAP and merging be hybridized—e.g., pruning weak experts first and merging only within strongly correlated clusters—to achieve better compression-performance trade-offs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PNssUl91Fk", "forum": "ukGxWd2aDG", "replyto": "ukGxWd2aDG", "signatures": ["ICLR.cc/2026/Conference/Submission13701/Reviewer_KnfQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13701/Reviewer_KnfQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143600676, "cdate": 1762143600676, "tmdate": 1762924250838, "mdate": 1762924250838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}