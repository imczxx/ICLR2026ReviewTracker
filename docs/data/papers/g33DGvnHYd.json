{"id": "g33DGvnHYd", "number": 795, "cdate": 1756818364496, "mdate": 1759898241542, "content": {"title": "SSVPO: Effective Step-Level Credit Assignment for RL Training of Language Models", "abstract": "Language models have shown strong performance on mathematical reasoning tasks. Post-training with outcome-based reinforcement learning (RL) can further enhance reasoning but is inefficient because it relies solely on final rewards. Recent credit assignment–based RL methods provide intermediate feedback, yet they often struggle to fairly evaluate each step’s importance, especially in partially correct reasoning chains. We propose Sequential Shapley Value Policy Optimization (SSVPO), a step-level credit assignment framework inspired by multi-agent RL. SSVPO introduces an insertion MDP and Sequential Shapley Values (SSV), which measure each step’s marginal contribution by reordering reasoning steps into alternative chains, ensuring fair credit assignment to all possible steps. By identifying steps with zero credit, SSVPO can shorten reasoning chains to improve training efficiency. We further provide a theoretical proof that SSV fairness to allocate credits and demonstrate that SSV as the new advantage baseline is consistent with Proximal Policy Optimization (PPO). Across 7 benchmarks, SSVPO outperforms state-of-the-art RL methods, both outcome-based (RLOO, GRPO, DAPO) and credit assignment–based (VinePPO, SPO), achieving up to an 11.6\\% gain in accuracy, an 18.1\\% reduction in token usage, and a 1.6× improvement in reasoning efficiency over vanilla methods. Our findings highlight that SSVPO provides effective step-level credit assignment, advancing post-training LLM reasoning performance while reducing token budgets.", "tldr": "Step-level reward credit assignment for efficient reinforcement learning training of large language models", "keywords": ["Credit Assignment", "Reinforcement Learning", "Step-Level Reward", "Large Language Model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b206ebc0ed90574a2546faf3d68cffacfec4fa30.pdf", "supplementary_material": "/attachment/b0ff52d34a833ca4b7a90ba5554aa03897e87055.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents SSVPO, a reinforcement learning algorithm aimed at improving reward allocation strategies. By modeling the generation process as an insertion MDP and using SSV as an advantage estimator, SSVPO computes more equitable token-level rewards by reordering the reasoning steps and calculation the relative promotion gained from the insertion of each step. The authors theoretically define the properties required for fair reward allocation and prove that SSV satisfies these properties. Comparisons with existing RL baselines show that SSVPO yields greater gains in sample efficiency and overall performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. By introducing game-theoretic principles, SSVPO provides a novel and intuitive framework for fairly distributing step-level rewards.\n\n2.  The experimental analysis is quite thorough. It includes results on multiple benchmarks; comparisons with both outcome-based RL methods and RL methods focused on credit assignment; and evaluations of sample efficiency and diversity (pass@n), which essentially cover the main interests of reasoning-oriented RL.\n\n3. The implementation details and visualization such as Figure 5 and Figure 6, help clarify the procedure of the algorithm a lot."}, "weaknesses": {"value": "1. In the pass@K evaluation, there is no comparison against other RL baselines. In addition, pass@N is typically scaled exponentially (pass@{1,2,4,8,16}). Up to what extent can SSVPO maintain its exploration advantage?\n\n2. There is a lack of discussion of the training dynamics: how do key metrics such as val_score, response_length, and entropy evolve?\n\n3. The algorithm has certain limitations: the large number of sequences generated by reordering causes sample inflation, which in turn forces the number of reasoning steps to be capped at 3. To what extent does this affect training speed and performance?"}, "questions": {"value": "4. Could you analyze the effect of off-policy sampling (especially async sampling, aka parital rollout) on SSVPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dtbKZaKh1e", "forum": "g33DGvnHYd", "replyto": "g33DGvnHYd", "signatures": ["ICLR.cc/2026/Conference/Submission795/Reviewer_4L2k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission795/Reviewer_4L2k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761401450214, "cdate": 1761401450214, "tmdate": 1762915606189, "mdate": 1762915606189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a totally novel way of credit assignment when doing RLVR. The credit assignment is based on the marginal contribution of a specific step among many possible reorderings of all possible reasoning steps. This credit assignment is different from the traditional definition of the credit when doing RL which just indicates the change in value after an action. But, it captures how useful this action could have been in other sequences. Therefore, it detects more generally if an step is useful."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of the method is that it is very novel and it also provides empirical evidence for its proposed methods. The baselines, VinePPO, SPO, GRPO, and RLOO are appropriate baselines and evaluations on AIME'24 and AIME'25 are strong evaluation benchmarks. The method is introducing a completely new way to see what credit assignment means. As an RL person, I am used to \"credit assignment\" meaning the delta in values of states before and after an specific action and this paper is challenging that assumption."}, "weaknesses": {"value": "While the paper is novel and provides empirical evidence, the method is so radically new, that I think I need more theoretical and intuitions in accepting its soundness. I think the authors do not provide intuition or justification on the reason that their proposed method should work better than the baselines. VinePPO in particular, is able to detect useless steps: a useless step will have zero-advantage. Therefore, I cannot rely on just intuiting the strength of results of SSVPO based on assuming it detects useless steps more accurately.\n\nAdditionally, I need more explanation on how the credit is determined. The reward in verifiable reward tasks is based on the final answer in the last sentences. If I understand correctly, if we reorder sequences, it is only the last sentence that matters and dictates the reward. In other words, changing the order of all previous sentences should not change the reward that we get out of the current mathematical verifiable rewards. I think this has some problems? The response that is the result of such reordering has no meaning. Therefore, I don't understand how SSVPO can lean on permutations of steps as there is no mechanism that detects that permutation of steps is not a correct response. \n\nI think the paper fails to provide intuition on why it works beyond just providing the algorithm."}, "questions": {"value": "1-Inserting some steps in an existing solution might make the solution completely wrong as the inserted step might not logically follow from the previous one. However, this is going to be missed with the current verifiable rewards for MATH. Is that fine under SSVPO? Am I understanding this correctly?\n2-Permutation of steps of reasoning makes the reasoning useless. However, the SSV operates based on this. Is my understanding correct? If yes, how is this fine?\n3-I am wondering if the SSVPO can be used in normal RL settings like training a robotic hand. Is there a reason/paper that this works well? It feels like changing the meaning of credit assignment might not play well in all deep RL settings. If that is true, why is it not the case for LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4NXYtnnfoo", "forum": "g33DGvnHYd", "replyto": "g33DGvnHYd", "signatures": ["ICLR.cc/2026/Conference/Submission795/Reviewer_K1FX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission795/Reviewer_K1FX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974171730, "cdate": 1761974171730, "tmdate": 1762915606047, "mdate": 1762915606047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SSVPO, By using Shapley values, SSVPO fairly allocates credit to each reasoning step based on its marginal contribution, even in partially correct reasoning chains.\nThe key contributions include:\nCredit assignment through SSV to better estimate the utility of each reasoning step.\nValidation showing that optimizing the utility of each step through SSVPO leads to performance improvements in mathematical reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces an innovative credit assignment method using Shapley values (SSV) to estimate the utility of each reasoning step. \n2. The paper provides empirical evidence demonstrating the effectiveness of the SSVPO method in improving reasoning accuracy and computational efficiency."}, "weaknesses": {"value": "1. One important weakness is the lack of discussion on computational cost. The paper introduces SSV for step-level credit assignment, but methods like SMC require evaluating multiple reasoning sequences, which increases the computational load. If the goal is to improve training efficiency, the authors should compare the computational cost and ensure a fair comparison with baseline methods by keeping the training computational load consistent.\n\n2. I want to know the advantages of SMC over VinePPO, which uses search-based value estimation. A discussion on how SMC compares in terms of accuracy, efficiency, and computational cost would make its benefits clearer.\n\n3. The paper also lacks ablation studies to show how different hyperparameters affect SSVPO's performance. These would help assess the stability of the algorithm under different settings.\n\n4. Is SSVPO suitable for zero-RL training scenarios? All experiments in the paper are conducted on models that already have long-COT abilities. It would be useful to know how well the method performs on models without such prior capabilities"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zWH2dIVNzI", "forum": "g33DGvnHYd", "replyto": "g33DGvnHYd", "signatures": ["ICLR.cc/2026/Conference/Submission795/Reviewer_Xabj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission795/Reviewer_Xabj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977237219, "cdate": 1761977237219, "tmdate": 1762915605906, "mdate": 1762915605906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper adapts the concept of Shapley value into LLM reasoning trajectory, using Qwen model (and its distilled version) on math reasoning benchmarks for training. It proposes something called the SSV value to perform multi-step credit assignment in the reasoning process, and uses it as the advantage estimator in RL. The paper provides some explanation claiming that this estimator is unbiased and has lower variance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Applying Shapley value to LLM reasoning is indeed novel."}, "weaknesses": {"value": "1. The modeling is unreasonable: Shapley assumes independent and exchangeable participants, but the LLM reasoning steps are strongly dependent and order-sensitive. The SSV estimation method enumerates all orderings, which is unnatural and violates the basic premise of sequential language modeling. Under such ordering, the MDP transitions are arbitrary concatenations, not reflecting the real generative distribution.\n\n2. shapley value is 'fair' only ensures that each step gets an equal share in total reward decomposition, but does not mean the step truly causes the result. In other words, a logically irrelevant or even wrong step can still get positive credit due to statistical averaging over permutations.\n\n3. computationally intractable: ssv requires averaging over all permutations—combinatorial explosion. Even with 10 reasoning steps, there are already millions of permutations.\n\n4. The effectiveness of this credit assignment: compared with REINFORCE-style RL algorithms that use (normalized) outcome rewards as advantage, this method adds the variance of SMC estimation, introducing more sampling noise. The paper provides no analysis or experiments for this.\n\n5. empirically: scenarios and models are very limited. Using Qwen3 for training on a few math datasets is questionable. two groups of experiments, each using only one different backbone.\n\n6. On results: many benchmark improvements are meaningless, the absolute difference is within 2%. Qwen models already saturate these benchmarks."}, "questions": {"value": "Why would Shapley value work for multi-step reasoning credit assignment? \n\nWhy can token-by-token generation from the same policy be partitioned and treated as a multi-agent game?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "krY9s6B1mU", "forum": "g33DGvnHYd", "replyto": "g33DGvnHYd", "signatures": ["ICLR.cc/2026/Conference/Submission795/Reviewer_BBy3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission795/Reviewer_BBy3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762400722675, "cdate": 1762400722675, "tmdate": 1762915605798, "mdate": 1762915605798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}