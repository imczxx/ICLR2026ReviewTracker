{"id": "o15wN4JAyT", "number": 16782, "cdate": 1758268660696, "mdate": 1763722594592, "content": {"title": "Generalizing Supervised Contrastive learning: A Projection Perspective", "abstract": "Self-supervised contrastive learning (SSCL) has emerged as a powerful paradigm for representation learning and has been studied from multiple perspectives, including mutual information and geometric viewpoints. However, supervised contrastive (SupCon) approaches have received comparatively little attention in this context: for instance, while InfoNCE used in SSCL is known to form a lower bound on mutual information (MI), the relationship between SupCon and MI remains unexplored.\nTo address this gap, we introduce ProjNCE, a generalization of the InfoNCE loss that unifies supervised and self-supervised contrastive objectives by incorporating projection functions and an adjustment term for negative pairs. We prove that ProjNCE constitutes a valid MI bound and affords greater flexibility in selecting projection strategies for class embeddings. Building on this flexibility, we further explore the centroid-based class embeddings in SupCon by exploring a variety of projection methods.\nExtensive experiments on image and audio datasets demonstrate that ProjNCE consistently outperforms both SupCon and standard cross-entropy training. Our work thus refines SupCon along two complementary perspectives—information-theoretic and projection viewpoints—and offers broadly applicable improvements whenever SupCon serves as the foundational contrastive objective.", "tldr": "", "keywords": ["SupCon", "mutual information", "label embedding", "kernel estimator"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/905b3a5803f552eb549c85047f12ba83819fb7c0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to improve the generalization ability of Supervised Contrastive Learning (SupCon) by rethinking how positive and negative pairs are sampled and weighted. The authors argue that standard SupCon assumes a fixed class-wise similarity structure, which limits transferability and robustness. To address this, they propose a generalized supervised contrastive loss that adaptively reweights pair contributions based on inter-class similarity, learned from data rather than predefined by labels. The method is evaluated on several datasets (CIFAR-10, CIFAR-100, Tiny-ImageNet, and STL-10), showing consistent gains over the standard SupCon and related methods such as Triplet, InfoNCE, and Center Contrastive Loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper explains well why standard SupCon may not generalize well — because it treats all same-class samples as equally positive and all other-class samples as equally negative. This rigid assumption can hurt performance in fine-grained or hierarchical datasets.\n- The proposed method keeps the same overall framework as SupCon but introduces an adaptive reweighting term. It’s easy to implement and can be seen as a drop-in replacement for the SupCon loss.\n- The paper provides results on several benchmarks with consistent, though moderate, improvements. It also includes ablations that analyze the influence of the weighting term and similarity estimation.\n- The writing is clear and organized, and the figures and tables are easy to read. The motivation, formulation, and results flow naturally."}, "weaknesses": {"value": "- The adaptive reweighting idea is related to earlier metric-learning works such as ProxyAnchor and SoftTriple. The main contribution may be seen as incremental.\n\n- All evaluations are done on small-scale datasets like CIFAR and Tiny-ImageNet. It is not clear if the proposed method still works well on larger and more realistic benchmarks (e.g., ImageNet-1K).\n\n- Figure 1 shows that using a larger β in the proposed projNCE makes the embeddings more spread out, which demonstrates the intended effect of the weighting. However, the overall embedding quality appears worse than SupCon in Figure 1(a): SupCon produces cleaner class clusters with clearer boundaries, while the proposed method shows more overlap between classes.\n\n- The paper does not study how the results depend on batch size or temperature, even though self-supervised and contrastive methods are known to be sensitive to these factors. Larger batch sizes could reduce the observed improvements,\n\n- The paper does not share code. This makes it difficult to verify the claimed improvements."}, "questions": {"value": "My main concern should refer to the weakness section, and I am willing to change the rating if the weakness is being explained or addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FdI5p6d85U", "forum": "o15wN4JAyT", "replyto": "o15wN4JAyT", "signatures": ["ICLR.cc/2026/Conference/Submission16782/Reviewer_2LFY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16782/Reviewer_2LFY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758305110, "cdate": 1761758305110, "tmdate": 1762926822783, "mdate": 1762926822783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper first unifies self-supervised contrastive loss (InfoNCE) with supervised contrastive loss (SupCon) by introducing projection operation, resulting a projection-based InfoNCE.  Further decomposition of mutual information shows that for supervised cases where positive and negative projection differs, the adjustment term does not canceled out, and might result poor bound on mutual information. Based on this observation, they proposed a new objective regarding the adjustment term, with several suggestions on projection itself, one giving the nice approximation of optimal projection (projNCE-perp), and some giving better generalization (projNCE-med, projNCE-MLP). They showed empirically projNCE performs better at image/audio datasets, and especially when the label is noisy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- proposition 2.1 is quite interesting result. Its derivation is simple, but it generalizes InfoNCE (so infoNCE bound is preserved) while points out that the adjustment term is missing in SupCon. \n- The adjustment term has intuitive meanings where they pull $g_-(c_k)$ to the $f(x)$, while push $g_+(c_k)$ from the $f(x)$. Which is implicitly performed in SupCon but without explicit guidance.\n- Strong empirical results. The method seems achieving significant improvement from previous methods."}, "weaknesses": {"value": "- I think the concept of label representation is already presented in other papers. [1,2] Authors should include this concept and explain why ProjNCE is different from these methods.\n\n[1] Label Supervised Contrastive Learning for Imbalanced Text Classification in Euclidean and Hyperbolic Embedding Spaces, Khalid et al.\n[2] Supervised contrastive learning over prototype-label embeddings for network intrusion detection, Lopez-Martin et al."}, "questions": {"value": "- I can agree that this derivation is interesting, but I am not sure this concept (having label representation and having some additional objective that controls feature representation) is novel. I wish to see the connection of ProjNCE to these methods (see weakness), and comparison.\n- As far as I know, $I_{NWJ}(X; C)$ gives tighter bound (so theoretically favored) but not used in practice because of its instability. The paper actually decompose $I_{NWJ}(X; C)$ to get bound to mutual information - how stable the framework is, and if it is stable, is there any hypothesis why in this case the framework is more stable? [3,4]\n\n[3] Combating the Instability of Mutual Information-based Losses via Regularization, Guo et al. \n[4] Tight Mutual Information Estimation With Contrastive Fenchel-Legendre Optimization, Choi et al."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MsMMpF9qGB", "forum": "o15wN4JAyT", "replyto": "o15wN4JAyT", "signatures": ["ICLR.cc/2026/Conference/Submission16782/Reviewer_CpYY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16782/Reviewer_CpYY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764778174, "cdate": 1761764778174, "tmdate": 1762926822425, "mdate": 1762926822425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ProjNCE, a unified contrastive learning framework that bridges self-supervised (InfoNCE) and supervised contrastive learning (SupCon) through a mutual-information–based formulation. By introducing projection functions and an adjustment term for negative pairs, ProjNCE not only establishes a valid lower bound on mutual information but also enables flexible class embedding strategies such as centroid-based projections. Experiments on image and audio datasets demonstrate that ProjNCE consistently outperforms both SupCon and cross-entropy training, offering a framework that combines theoretical clarity with practical improvements in contrastive learning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper discusses how SupCon relates to mutual information, which is an unexplored and interesting topic.\n- The derivation of ProjNCE as a generalized form of InfoNCE is clearly presented and mathematically sound.\n- The idea of decoupling projection functions for positive and negative pairs is well motivated."}, "weaknesses": {"value": "- It remains unclear whether the observed improvements stem from the proposed MI-based formulation itself or merely from the additional parameters introduced by the projection functions.\n- The practical benefit of maintaining a “valid MI bound” is not convincingly demonstrated—there is no evidence that tighter MI bounds correlate with better downstream performance or robustness.\n- The comparisons omit several recent strong baselines trained on larger-scale datasets, which weakens the empirical significance of the reported gains.\n- The proposed bound does not empirically demonstrate reduced bias compared to existing methods."}, "questions": {"value": "Can you provide empirical evidence showing that the proposed bound indeed exhibits reduced bias relative to the true mutual information, compared to existing contrastive objectives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HXBRdhkId3", "forum": "o15wN4JAyT", "replyto": "o15wN4JAyT", "signatures": ["ICLR.cc/2026/Conference/Submission16782/Reviewer_YBd7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16782/Reviewer_YBd7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877783996, "cdate": 1761877783996, "tmdate": 1762926822094, "mdate": 1762926822094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to unify supervised and self-supervised contrastive learning through the ProjNCE loss. The idea behind this unification is to generalize the distance measure in the InfoNCE loss, replacing $⟨f(x_i), c_i⟩$ with $⟨f(x_i), g_{+}(c_i)⟩$ for positive pairs and $⟨f(x_i), g_{-}(c_i)⟩$ for negative pairs, where g₊ and g₋ are projection functions. In classical losses, these functions are typically the identity mapping or the class-average embedding (centroid). The authors also add an additional adjustment term, R(X, C), to establish a valid mutual-information lower bound. Experiments are conducted on vision datasets such as CIFAR, ImageNet, Caltech256, and Food101, as well as on audio datasets including CREMA-D and SpeechCommands. For linear-probe classification, ProjNCE consistently outperforms supervised cross-entropy and SupCon in accuracy across almost all datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. strong linear-probe results on both vision and audio datasets, outperforming CE and SupCon, with notable robustness to noisy labels.\n\n2. the paper evaluates multiple projection strategies (Orthogonal, Median, MLP), showing consistent gains.\n\n3. the derivation of ProjNCE as a proper MI lower bound has a good theoretical motivation."}, "weaknesses": {"value": "1. the paper does not compare against other self-supervised methods such as BYOL, Barlow Twins, DINO, VICReg, or PaCo, which limits its positioning relative to the state of the art.\n\n2. The paper does not include sensitivity studies on key hyperparameters such as $\\beta$, the choice of kernel bandwidth, or embedding dimensionality.\n\n3. I am not sure about correctness of the claim on line 154 that SupCon is mathematically equivalent to $I^{\\text{self-p}}_{NCE}(Z;C)$ is inaccurate. Substituting $g+(c_i)$ with a class-centroid embedding and $g-(c_j)$ with sample embeddings does not reproduce the pairwise SupCon loss. SupCon averages the negative log conditional probabilities over all *positive pairs*, whereas the projection-based variant contrasts each point with its class centroid. For example, if we have embeddings $a,b$ from class 1 and $c$ from class 2:  \n\nSupCon: $\\Big[\\log\\frac{e^{\\langle a,b\\rangle}}{e^{\\langle a,b\\rangle}+e^{\\langle a,c\\rangle}} +\\log\\frac{e^{\\langle b,a\\rangle}}{e^{\\langle b,a\\rangle}+e^{\\langle b,c\\rangle}}\\Big]$ while $I^{\\text{self-p}}_{\\mathrm{NCE}}= -\\Big[\\log\\frac{e^{\\langle a,(a+b)/2\\rangle}}{e^{\\langle a,b\\rangle}+e^{\\langle a,c\\rangle}}\n+\\log\\frac{e^{\\langle b,(a+b)/2\\rangle}}{e^{\\langle b,a\\rangle}+e^{\\langle b,c\\rangle}}\\Big]$\n\nThese coincide only when $a = b$ or under a trivial critic and in fact you can bound one to the other with Jensen inequality. The equivalence however holds under alignment and uniformity approximation as the authors motivated their choices with in lines 108-128 (for some common choices of $\\psi$)."}, "questions": {"value": "1. can you report quantitative results with and without the adjustment term $R(X, C)$ to isolate its contribution?\n\n2. Figure 1 provides qualitative analysis for $\\beta$ = 1, 5, 10, but quantitative accuracy trends are missing. How sensitive is performance to $\\beta$?\n\n3. What's the motivation for choosing NW over alternatives? How sensitive are results to the kernel bandwidth?\n\n4. in line 410 you state \\( K(t) = 1 − t^2 \\). What motivated this kernel, and how would other kernels affect convergence or accuracy?\n\n5. how does your proposed objective differ from other frameworks that unified InfoNCE and SupCon such as: UniCL [1], X-Sample Contrastive Loss [2], I-Con [3]?\n\n[1] Yang, Jianwei, et al. \"Unified contrastive learning in image-text-label space.\" CVPR 2022.\n\n[2] Sobal, Vlad, et al. \"X-Sample Contrastive Loss: Improving Contrastive Learning with Sample Similarity Graphs.\" ICLR 2025.\n\n[3] Alshammari, Shaden, et al. \"I-Con: A Unifying Framework for Representation Learning.\" ICLR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dlhG9lul6H", "forum": "o15wN4JAyT", "replyto": "o15wN4JAyT", "signatures": ["ICLR.cc/2026/Conference/Submission16782/Reviewer_5QFy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16782/Reviewer_5QFy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977393124, "cdate": 1761977393124, "tmdate": 1762926821735, "mdate": 1762926821735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}