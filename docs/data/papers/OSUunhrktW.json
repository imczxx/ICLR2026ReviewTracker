{"id": "OSUunhrktW", "number": 11878, "cdate": 1758204451095, "mdate": 1759897549244, "content": {"title": "THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?", "abstract": "Large Language Models (LLMs) are accelerating scientific idea generation, but rigorously evaluating these numerous, often superficial, AI-generated propositions for novelty and factual accuracy is a critical bottleneck; manual verification is too slow. Existing validation methods are inadequate: LLMs as standalone verifiers may hallucinate and lack domain knowledge (our findings show $~60\\%$ unawareness of relevant papers in specific domains), while traditional citation networks lack explicit causality and narrative surveys are unstructured, underscoring the absence of structured, verifiable, and causally-linked historical data of scientific evolution. To address this, we introduce $\\textbf{THE-Tree}$ ($\\textbf{T}$echnology $\\textbf{H}$istory $\\textbf{E}$volution $\\textbf{Tree}$), a computational framework that constructs such domain-specific evolution trees from scientific literature. THE-Tree employs a search algorithm to explore evolutionary paths using a novel $\\textbf{``Think-Verbalize-Cite-Verify''}$ process: an LLM proposes potential advancements and cites supporting literature, while each proposed evolutionary link is validated for logical coherence and evidential support by interrogating the cited literature. We construct and validate $88$ THE-Trees across diverse domains and release a benchmark dataset including up to $71k$ fact verifications covering $27k$ papers to foster further research. Experiments demonstrate that i) in graph completion, our THE-Tree improves hit@1 by $8\\%$ to $14\\%$ across multiple models compared to traditional citation networks; ii) for predicting future scientific developments, it improves hit@1 metric by nearly $10\\%$; and iii) when combined with other methods, it boosts the performance of evaluating important scientific papers by almost $100\\%$. By constructing explicit, verifiable pathways of scientific progression, THE-Tree provides a robust historical foundation for evaluating new hypotheses (human or AI-generated) and enables a computable science history, fostering evidence-based AI-driven scientific discovery.", "tldr": "", "keywords": ["Scientific discovery verification；Evidence-Based Verification；Scientific Evolution"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/14cdfd11a41d186e2b06d9ebeb9d27c62f1623f4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces THE-Tree (Technology History Evolution Tree), a computational framework for constructing structured, causally-linked representations of scientific evolution from literature. The authors employ Self-Guided Temporal Monte Carlo Tree Search (SGT-MCTS) with a Think-Verbalize-Cite-Verify (TVCV) methodology to build evolution trees where nodes represent papers and edges represent validated evolutionary relationships. The framework uses Retrieval-Augmented Natural Language Inference (RA-NLI) to verify relationships. The authors construct 88 THE-Trees across AI domains and demonstrate improvements in graph completion (8-14%), future development prediction (~10%), and paper evaluation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### **1. Originality**\n\n**1.1 Novel problem formulation and framework design.** The paper addresses a genuine bottleneck in AI-driven scientific discovery—validating AI-generated hypotheses—through an original lens of constructing structured, causally-linked historical evolution trees. The TVCV (Think-Verbalize-Cite-Verify) methodology represents a creative combination of Monte Carlo Tree Search, LLM generation, and retrieval-augmented validation that hasn't been previously explored for scientific knowledge graph construction. The explicit focus on causal evolutionary relationships rather than simple citation links is a meaningful departure from traditional bibliometric approaches.\n\n**1.2 Innovative validation mechanism.** The RA-NLI (Retrieval-Augmented Natural Language Inference) component addresses a critical weakness in LLM-based systems—hallucination—by grounding proposed relationships in actual textual evidence from papers. The reported reduction of phantom citations from 21.46% to 0% represents a significant technical contribution to ensuring factual accuracy in automated knowledge graph construction.\n\n---\n\n### **2. Quality**\n\n**2.1 Substantial dataset contribution.** The construction of 88 THE-Trees covering 27k papers with 71k verified relationships represents a significant effort. The dataset spans diverse AI research areas and includes expert refinement with reasonable inter-annotator agreement (0.82 Cohen's κ), providing a valuable resource for the research community.\n\n**2.2 Comprehensive multi-task evaluation.** The paper evaluates THE-Tree across multiple downstream tasks (graph completion, future prediction, paper evaluation) rather than a single application, demonstrating breadth of potential impact. The cross-conference validation (NeurIPS, ICLR, ICML, CVPR) in Table 11 strengthens generalizability claims within the AI domain.\n\n**2.3 Rigorous construction pipeline.** The SGT-MCTS algorithm with temporal constraints and the multi-stage validation process (NLI model + LLM fallback + expert refinement) shows methodological sophistication. The explicit handling of temporal consistency and the composite reward function balancing node importance, path coherence, and link validity demonstrate careful system design.\n\n---\n\n### **3. Clarity**\n\n**3.1 Well-structured presentation.** The paper provides clear motivation, systematic methodology description, and extensive experimental results. Figure 1 effectively communicates the high-level approach and contrasts with existing methods. The formal edge definition (Section 3.1) with explicit temporal and validation constraints provides mathematical precision.\n\n**3.2 Comprehensive appendices.** The extensive supplementary material (Sections A-C) provides implementation details, model architectures, dataset statistics, and case studies that support reproducibility. The worked examples in Appendix C (Neural Pfaffians, Graph Contrastive Learning) help readers understand practical application.\n\n---\n\n### **4. Significance**\n\n**4.1 Addresses timely problem.** As AI systems increasingly generate scientific hypotheses, the need for automated validation becomes critical. The paper tackles this important challenge with a structured approach that could influence how the research community thinks about AI-assisted scientific evaluation.\n\n**4.2 Practical improvements demonstrated.** The reported improvements—8-14% in graph completion, ~10% in future prediction, and substantial gains in paper evaluation—suggest potential real-world utility. The case studies show concrete examples where THE-Tree augmentation improved paper quality assessment.\n\n**4.3 Foundation for future research.** By releasing the dataset and framework, the paper enables follow-up work on scientific knowledge representation, automated reviewing, and computational science history."}, "weaknesses": {"value": "### **W1. Circular Reasoning in Benchmark Construction and Evaluation (Critical)** \n\n**Issue:** The benchmark is constructed using LLMs (TVCV with LLM proposals, RA-NLI with LLM fallback for ambiguous cases) and expert refinement of LLM outputs, then used to demonstrate that LLM-based evaluation improves when augmented with this benchmark. This circularity undermines claims about THE-Tree capturing \"authentic patterns\" of scientific evolution versus capturing what LLMs believe evolution looks like.\n\n**Specific concerns:**\n\n- Expert refinement starts from LLM proposals (anchoring bias) rather than independent construction\n- Ground truth for graph completion/future prediction is \"papers retrospectively included in THE-Tree,\" not independently validated milestones\n- Improvements may reflect LLMs recognizing their own construction patterns rather than genuine scientific understanding\n\n**Requested evidence:**\n\n**W1.1.** Ablation where experts build THE-Trees independently without seeing LLM outputs, measuring agreement between expert-first vs. LLM-first construction\n\n**W1.2.** Validation against independent benchmarks (e.g., test-of-time award papers, highly-cited papers 5 years post-publication)\n\n**W1.3.** Analysis of whether different LLMs produce consistent THE-Trees for the same domain (inter-model reliability)\n\n**Suggested fix:** Section 4 should include independent validation experiments that don't rely on LLM-generated or LLM-refined ground truth.\n\n------\n\n### **W2. Relationship Type Assignment: Incomplete Documentation (Critical)** \n\n**Issue:** The paper introduces three semantic relationship types {causal, enabling, foundational} as a core advantage over citation networks but never explains how these types are assigned, validated, or distinguished.\n\n**Missing information:**\n\n- No operational definitions differentiating the three types with concrete criteria\n- No description of whether types are LLM-generated, expert-assigned, or algorithmically determined\n- No inter-annotator agreement specifically for relationship type classification\n- No confusion matrix or error analysis for type assignments\n- No justification for why three types are sufficient versus more granular taxonomies (e.g., \"extends,\" \"refutes,\" \"optimizes,\" \"applies-to-new-domain\")\n\n**Impact on claims:** Without clear type assignment methodology, the claimed semantic superiority over citation networks cannot be properly evaluated. If types are LLM-generated without validation, this represents a significant quality gap.\n\n**Requested addition:** Add Section 3.3.2 explicitly describing:\n\n**W2.1.** How relationship types are determined at each stage (LLM generation, RA-NLI, expert refinement) \n\n**W2.2.** Clear definitions with examples distinguishing each type\n\n**W2.3.** Inter-annotator agreement for type classification\n\n**W2.4.** Discussion of type taxonomy design choices\n\n\n------\n\n### **W3. Unfair Experimental Comparisons (Major)**\n\n**Issue:** The comparisons between THE-Tree and traditional citation networks are uncontrolled for information content, making it impossible to isolate THE-Tree's specific structural contribution.\n\n**Specific problems:**\n\n*Graph Completion (Table 8):*\n\n- THE-Tree: typed edges, importance scores, confidence scores, retrieved evidence passages\n- Citation network: binary edges only\n- This is comparing information-rich vs. information-sparse representations\n- Better performance may simply reflect having more features, not superior causal structure\n\n*Paper Evaluation (Tables 1-2):*\n\n- LLM + THE-Tree: receives historical context, related papers, evolutionary paths\n- LLM alone: only title + abstract\n- Any form of historical context (RAG retrieval, survey paragraphs, citation network paths) might provide similar improvements\n\n**Missing baselines:**\n\n**W3.1.** Citation networks augmented with paper importance scores (from PageRank, citation counts, or Semantic Scholar metrics)\n\n**W3.2.** LLM + RAG retrieval of related papers (matching THE-Tree's information content)\n\n**W3.3.** LLM + relevant survey paragraphs about the topic\n\n**W3.4.** Citation networks with citation context text (available from tools like Semantic Scholar)\n\n**Requested experiments:** Table comparing:\n\n- THE-Tree (full)\n- THE-Tree with random edge types (ablation)\n- Citation network + importance scores\n- Citation network + citation contexts\n- RAG baseline with equivalent information\n\nThis would isolate whether improvements stem from THE-Tree's causal structure versus simply providing more context.\n\n------\n\n### **W4. Temporal Leakage in Future Prediction (Major)**\n\n**Issue:** The \"future prediction\" experiment (Section 4.4, Table 9) claims to predict developments after year Y using data up to Y, but the LLMs used were trained on data beyond Y, creating temporal leakage.\n\n**Specific problem:**\n\n- THE-Tree built \"up to 2019\" to predict 2020-2023\n- But Qwen2.5-72b, GPT-4, Claude-3.5-Sonnet were all trained on data including 2020-2023\n- These models already \"know\" what came after 2019 from their training\n- This isn't true prediction but retrofitting/recognition\n\n**Additional concerns:**\n\n- Very low baseline (citation network Hit@1: 18.31%) suggests task setup issues\n- Ground truth is papers \"retrospectively included in THE-Tree,\" not independently validated breakthroughs\n- No discussion of this limitation in the paper\n\n**Required clarifications:**\n\n**W4.1.** State the training data cutoffs for all LLMs used (Qwen2.5-72b, Gemma-7b, etc.)\n\n**W4.2.** If training data includes the prediction period, acknowledge this as a limitation\n\n**W4.3.** Redesign experiment using models with training cutoffs before prediction period, or\n\n**W4.4.** Design truly prospective evaluation: make predictions, wait for future developments, evaluate against actual outcomes\n\n**Alternative formulation:** Frame as \"retrospective pattern matching\" rather than \"future prediction\" and acknowledge the limitation."}, "questions": {"value": "# Questions and Suggestions for Authors\n\n## **A. Semantic Relationship Types (Critical Clarification Needed)**\n\n### **A1. How are relationship types assigned?**\n\nThe paper states edges have types {causal, enabling, foundational} but never explains how these are determined.\n\n**Questions:**\n\n- Is the relationship type generated by the LLM during the TVCV \"Think\" step?\n- Do experts validate or modify relationship types during refinement?\n- What are the operational definitions distinguishing \"causal\" vs \"enabling\" vs \"foundational\"?\n- What is the inter-annotator agreement specifically for relationship type assignments?\n\n**Why this matters:** Without clear type assignment methodology, the claimed semantic advantage over citation networks cannot be evaluated.\n\n**Suggestion:** Add a subsection explicitly describing the relationship typing process with examples and validation metrics.\n\n------\n\n### **A2. Are three types sufficient?**\n\n**Questions:**\n\n- How was the three-type taxonomy derived? Why not more granular (e.g., \"extends,\" \"refutes,\" \"optimizes,\" \"applies-to-new-domain\")?\n- Did experts request additional relationship types during refinement?\n- Can you provide confusion matrix or disagreement analysis for the three types?\n\n**Why this matters:** If types frequently overlap or are ambiguous, the semantic structure may not provide meaningful improvement over simpler representations.\n\n------\n\n## **B. Circular Reasoning and Validation Independence (Major Concern)**\n\n### **B1. How do you address potential circular reasoning?**\n\n**The concern:**\n\n```\nTHE-Tree built by: LLM proposals → RA-NLI validation (NLI + LLM) → Expert refinement of LLM outputs\nTHE-Tree used for: Improving LLM-based evaluation\n```\n\n**Questions:**\n\n- Have you measured whether improvements simply reflect LLMs \"recognizing their own construction patterns\"?\n- Can you provide ablation where experts build THE-Trees independently (without seeing LLM outputs) and compare agreement?\n- What percentage of expert refinements are additions vs. modifications vs. deletions of LLM proposals?\n\n**Why this matters:** If THE-Tree primarily captures \"what LLMs think evolution looks like\" rather than \"actual evolution,\" its value for LLM evaluation is questionable.\n\n**Suggestion:** Conduct independent expert construction experiment and report agreement metrics.\n\n------\n\n### **B2. RA-NLI validation - how robust is it?**\n\n**Questions:**\n\n- What NLI model is used? Is it fine-tuned on scientific text or general domain?\n- For the LLM fallback (when NLI score is 0.6-0.8), what percentage of edges require this?\n- Can a cited paper \"acknowledge contribution\" without representing intellectual lineage (e.g., \"Like [AlexNet], we use CNNs\" vs. actually building on AlexNet's ideas)?\n- Can you provide false positive/negative analysis of RA-NLI with expert evaluation?\n\n**Why this matters:** If RA-NLI has high false positive rate (accepting spurious relationships), the validation isn't actually filtering noise.\n\n**Suggestion:** Report precision/recall/F1 of RA-NLI against expert judgments on a held-out validation set of relationship proposals.\n\n------\n\n## **C. Experimental Design and Fair Comparisons**\n\n### **C1. Graph completion - is the comparison fair?**\n\n**The issue:** THE-Tree has typed edges, importance scores, confidence scores, evidence passages. Citation networks have binary edges.\n\n**Questions:**\n\n- Can you compare against citation networks augmented with similar metadata (e.g., citation context, paper importance scores from Semantic Scholar)?\n- What happens if you give the model THE-Tree structure but random edge types - does performance degrade?\n- Is the improvement from THE-Tree's specific causal structure, or just from having richer information?\n\n**Why this matters:** Comparing information-rich vs. information-sparse representations doesn't isolate the value of your specific approach.\n\n**Suggestion:** Add ablation studies:\n\n- THE-Tree with random edge types\n- Citation network + paper importance scores\n- Citation network + citation context text\n\n------\n\n### **C2. Future prediction - how do you avoid temporal leakage?**\n\n**The concern:** You build THE-Tree \"up to year Y\" but use LLMs trained on data beyond Y, which already \"know\" what came after Y.\n\n**Questions:**\n\n- What is the training data cutoff of the LLMs used (Qwen2.5-72b, etc.)?\n- If LLM training includes 2020-2023 data, how can it fairly \"predict\" 2020-2023 developments?\n- Can you re-run experiments using LLMs with training cutoffs before the prediction period?\n- What constitutes \"correct\" future prediction - papers added to THE-Tree retrospectively, or papers that actually had impact?\n\n**Why this matters:** If models already know the future, this isn't prediction but retrofitting.\n\n**Suggestion:**\n\n1. Clearly state LLM training cutoffs\n2. Design truly prospective evaluation (make predictions, wait for future, evaluate)\n3. Or use historical holdout with appropriate model cutoffs\n\n------\n\n### **C3. Paper evaluation - what's the fair baseline?**\n\n**The issue:** LLM+THE-Tree vs. LLM alone isn't isolating THE-Tree's value - any historical context might help.\n\n**Questions:**\n\n- How does performance compare to:\n  - LLM + citation network paths (same number of papers)?\n  - LLM + RAG retrieval of related papers?\n  - LLM + relevant survey paragraphs?\n- The baseline GPT-4o achieves 0% Oral identification for NeurIPS 2024 - why is this so low? Is the prompt or task setup problematic?\n- Can you show the actual prompts used for with/without THE-Tree conditions?\n\n**Why this matters:** Need to establish whether THE-Tree's structure provides unique value vs. any form of historical context.\n\n**Suggestion:** Add comparison conditions isolating THE-Tree's contribution:\n\n- Same information content, different structures\n- Ablate specific THE-Tree components (types, confidence scores, evidence)\n\n------\n\n## **E. Generalization and Scope**\n\n### **E1. Paradigm shifts and revolutionary science**\n\nYou mention handling paradigm shifts (Section 3.1) but provide no empirical validation.\n\n**Questions:**\n\n- Can you show examples of THE-Tree successfully capturing a paradigm shift?\n- How does the framework handle papers that fundamentally break from prior trajectories?\n- Does the survey-based initialization bias against revolutionary work not yet covered in surveys?\n\n**Suggestion:** Demonstrate on historical paradigm shift (e.g., attention mechanisms replacing RNNs, AlexNet reviving deep learning).\n\n------\n\n## **H. Clarifications on Methodology**\n\n### **H1. Expert refinement protocol**\n\n**Questions:**\n\n- How many experts per THE-Tree? What are their qualifications?\n- How much time did experts spend per tree (you say 2-4 hours, but what's the distribution)?\n- What specific instructions were experts given?\n- For the 15.3% coherence improvement - how is \"coherence\" measured objectively?\n\n------\n\n### **H2. Node importance scores**\n\n**Questions:**\n\n- You define S(v) = γ·Sgraph(v) + (1-γ)·SLLM(v) - how is γ chosen?\n- Is γ constant across domains or tuned per THE-Tree?\n- How sensitive are results to γ values?\n\n------\n\n### **Summary: Most Critical Questions for Rebuttal**\n\n1. **How do you address circular reasoning** (LLMs building benchmarks to evaluate LLMs)?\n2. **How are relationship types assigned** and what's the validation?\n3. **Can you provide fair comparisons** (THE-Tree vs. citation networks with equal information)?\n4. **How do you avoid temporal leakage** in future prediction experiments?\n5. **Can you demonstrate value beyond AI domains** with non-AI THE-Trees?\n\nThese responses would significantly impact my assessment of the paper's contributions and validity.\n\n---"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gJPASJb8Gy", "forum": "OSUunhrktW", "replyto": "OSUunhrktW", "signatures": ["ICLR.cc/2026/Conference/Submission11878/Reviewer_MHaW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11878/Reviewer_MHaW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735859497, "cdate": 1761735859497, "tmdate": 1762922895489, "mdate": 1762922895489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces THE-Tree (Technology History Evolution Tree), a computational framework for constructing structured, verifiable representations of scientific evolution from literature. The approach combines Self-Guided Temporal Monte Carlo Tree Search (SGT-MCTS) with a novel Think-Verbalize-Cite-Verify (TVCV) methodology and Retrieval-Augmented Natural Language Inference (RA-NLI) to build domain-specific evolution trees that capture causal relationships between scientific papers. The authors construct 88 THE-Trees across diverse domains and demonstrate improvements in graph completion, future prediction, and paper evaluation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is generally well-written and organized, with clear motivation and comprehensive experiments. The main concepts are explained well, and the figures effectively illustrate the framework.\n2. The challenge of evaluating AI-generated scientific ideas is timely and important, especially given the proliferation of LLM-based research tools. The demonstrated improvements in paper evaluation tasks (especially for identifying high-impact papers) have clear practical applications for peer review and research assessment.\n3. The RA-NLI mechanism effectively reduces hallucination rates (from 21.46% to 0% for phantom citations), addressing a critical issue in LLM-based approaches.\n4. The paper includes extensive experiments across multiple tasks, datasets, and venues."}, "weaknesses": {"value": "1. The approach requires existing survey papers as starting points, which may not be available for emerging or niche research areas. This dependency could limit applicability.\n2. The ground truth seems to rely on expert annotation with potential biases. While acknowledged, the paper doesn't provide sufficient mitigation strategies beyond inter-annotator agreement.\n3. Missing ablation studies on key components like:\n    - The impact of different weighting schemes in the reward function\n    - The contribution of individual components in TVCV\n    - The effect of different confidence thresholds\n4. The paper doesn't adequately discuss when THE-Tree might fail or produce incorrect evolutionary paths, particularly for paradigm shifts or interdisciplinary work.\n5. While the paper evaluation results are impressive, the broader claim of enabling \"AI-driven scientific discovery\" is not sufficiently validated. The experiments primarily focus on paper assessment rather than hypothesis generation or discovery. \n\nMinor: I have noticed that the computational cost (4.73 hours per tree on 8×A100 GPUs) is high, which could limit accessibility."}, "questions": {"value": "1. How does THE-Tree approach handle papers that contribute to multiple research threads or interdisciplinary work that doesn't fit cleanly into a single evolutionary path?\n2. How would THE-Tree perform in rapidly evolving fields where survey papers quickly become outdated?\n3. What is the sensitivity of the results to the choice of threshold parameters (e.g., θ_min = 0.7, similarity threshold ≥ 0.75)?\n4. Can you provide concrete examples of cases where THE-Tree failed to capture important evolutionary relationships?\n5. How does the approach handle conflicting evolutionary narratives that might exist in contentious research areas? I am very curious about this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jwo3Kr9Zl0", "forum": "OSUunhrktW", "replyto": "OSUunhrktW", "signatures": ["ICLR.cc/2026/Conference/Submission11878/Reviewer_CMH3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11878/Reviewer_CMH3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925864547, "cdate": 1761925864547, "tmdate": 1762922894811, "mdate": 1762922894811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes THE-tree, a way of constructing augmented scientific knowledge graphs which helps address the problem of evaluating AI-generated scientific ideas. One issue is that manual verification of these ideas is labour intensive and slow, whereas LLMs may miss related literature when judging novelty of an idea. They propose a process to generate domain-specific evolution trees from the scientific literature as an alternative to traditional citation networks. In the THE-tree framework, each paper is represented by a node containing metadata about that paper, and each node is associated with an importance score (weighted combination of its structural significance within the citation graph and semantic relevance as assessed by an LLM). Edges between papers represent different types of relationships, but importantly they represent significant contributions or influence of one paper on another rather than just one citing another. They construct THE-trees for 88 topics and assess the constructed trees with graph completion tasks as compared to baseline knowledge graphs, as well as performance in identifying accepted papers and oral/spotlight papers from Neurips 2024."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**S1**: I think the problem that this paper is targeting is very timely and relevant as there is an increasing interest in using LLMs for scientific ideation, and LM-generated papers are being increasingly submitted to conferences. The construction of THE-trees or similar approaches could help LMs evaluate whether proposed ideas (whether generated by LMs or humans) are novel and help with LM-as-a-judge approaches.\n\n**S2**: The tree construction method is detailed in great depth, and I think the ideas used to construct the trees generally make sense. Even if not used in their exact form, I think these ideas could potentially inform future designs of scientific citation networks."}, "weaknesses": {"value": "Unfortunately, I think that there are some significant issues in the experimental setup of this paper, and once fixed this paper could be a much stronger contribution. That's why I presently recommend rejection, but I would like to say that I think the paper has potential.\n\n**W1**: The paper is framed as a way to improve scientific idea evaluation, but the contribution of THE-tree seems significantly more narrow from the methods. Of the experiments, the most convincing ones compare THE-tree to traditional citation networks, whereas the remaining experiment focuses on predicting acceptance/rejection decisions for a conference, which while somewhat correlated with idea quality, is also a proxy for the motivating usage (improving evaluation of scientific ideas). \n\n**W2**: From Table 1 vs Table 2, it seems like the gains from including THE-tree are mixed and model dependent, with some models having a much more modest improvement (e.g. Qwen2.5 72b goes from 25.7 -> 26.03). Additionally, the larger gains on models such as Deepreviewer 7B and Claude Sonnet 3.5 seem to be driven by higher rejection accuracy while the acceptance accuracy actually goes down. This seems to be because models are predicting accept on almost everything by the numbers on the Acc% column? This raises questions about the prompt given to LMs (see questions). I recommend that the prompt be revised and that F1 score or brier score be used instead.\n\n**W3**: Currently, THE-tree has many different components and it is unclear to me which parts of the methodology drive gains in performance over a traditional citation network. It would be helpful to show ablations for the different modules such as the RA-NLI edges, node importance, TVCV and MCTS.\n\n**W4**: The details of the traditional citation network compared with are not clear to me. In addition to adding more details, I think the authors should also compare with something like semantic scholar’s “highly influential citations” as that also aims to isolate more meaningful citations. (Valenzuela et al 2015)"}, "questions": {"value": "- What was the prompt given to models for the accept/reject and paper status (oral/spotlight/poster) classification? It seems like in Table 1 and 2 the LLMs are voting to accept almost everything\n\n- Which traditional citation networks did you compare with and are there any details about them or how they were constructed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tFdq7uteU5", "forum": "OSUunhrktW", "replyto": "OSUunhrktW", "signatures": ["ICLR.cc/2026/Conference/Submission11878/Reviewer_Xdgz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11878/Reviewer_Xdgz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966501453, "cdate": 1761966501453, "tmdate": 1762922894266, "mdate": 1762922894266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the THE-Tree, a computational framework  that aims to make scientific reasoning and verification more evidence-based by reconstructing causally linked, temporal evolution trees of scientific ideas from literature. \n\nThe motivation stems from two issues: (i) LLMs hallucinate and lack awareness of historical contexts, and (ii) Citation networks lack explicit causal and logical coherence. \n\nTHE-Tree addresses these by representing each paper as a node with metadata and an importance score. Connecting papers via edges and using the Retrieval Augmented NLI mechanism to validate that an edge has textual, factual support. \n\nOverall, the paper argues that structured, causal historical modeling can ground AI-driven scientific discovery."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. __Ambitious Vision:__ The problem tackles an important bottleneck in AI4Science: how to verify machine-generated hypotheses. \n2. __Methodological Coherence:__ Integrates Monte-Carlo search, LLM Reasoning, and NLI-based factual grounding in a coherent pipeline.\n3. __Dataset:__ One of the largest structured attempts to model scientific evolution."}, "weaknesses": {"value": "1. __Potential Data Leakage:__ Experiments evaluate LLMs on past conference papers while using training data drawn from broad scientific corpora. Given that many papers already appear on preprint servers months before review, there is a leak of data leakage --- THE-Tree or its LLM components may have already seen these texts. The authors do not report any leakage check. \n2. __Ground Truth Causality:__ \"Causal\" relations are defined linguistically (NLI entailment) rather than via experimental or citation-intent evidence. Hence, the framework captures semantic relatedness, not necessarily genuine causal influence. The validation by experts is small and does not demonstrate that the resulting graph encodes true intellectual lineage rather than sophisticated co-citation semantics. \n3. __Miscellaneous:__ \n   - Metrics like \"Hit@k\" for graph completion are borrowed from link-preduction and not fully aligned with the notion of scientific evolution.\n   - Constructing one tree requires approximately 5 hrs on 8xA100 GPUs, which is heavy. The authors ignored the maintenance cost, as domains may evolve monthly.\n   - Phrases like \"computable science history\" are sometimes overstating claims; at its core, this is a structured bibliometric + LLM system."}, "questions": {"value": "1. How were causal labels validated by experts? Was disagreement quantified beyond the reported kappa? \n2. What happens if we remove RA-NLI or use plain MCTS without LLM guidance? \n3. Can incremental updates be done - eg, adding 2026 papers without rebuilding trees from scratch? \n4. How does the framework prevent reinforcing historical biases or stifling paradigm shifts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F0FAzx0hdi", "forum": "OSUunhrktW", "replyto": "OSUunhrktW", "signatures": ["ICLR.cc/2026/Conference/Submission11878/Reviewer_7fD9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11878/Reviewer_7fD9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992524778, "cdate": 1761992524778, "tmdate": 1762922893786, "mdate": 1762922893786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}