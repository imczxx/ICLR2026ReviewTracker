{"id": "vfDJiI0dbu", "number": 12972, "cdate": 1758212288631, "mdate": 1759897473438, "content": {"title": "STM4D: 4D Occupancy Forecasting with 2D and 3D Spatio-Temporal Modeling", "abstract": "Vision-based 4D occupancy forecasting enables autonomous vehicles to predict future 3D semantic scenes from historical multi-view images, which is critical for driving safety. While current methods show promising results, the potential of simultaneous 2D and 3D spatio-temporal modeling and leveraging temporal cues from 2D multi-view image sequences to improve 4D occupancy prediction remains\nunexplored, presenting a critical bottleneck for advancing performance. To address this gap, we introduce STM4D, a novel framework for 4D occupancy prediction that jointly models temporal dynamics in both voxel-based representations and multi-view image sequences, while explicitly incorporating feature interaction between the two complementary branches. Our framework incorporates three core components: 1) A 3D Spatio-Temporal (3DST) module that learns volumetric dynamics from historical voxel states to predict future voxel states; 2) A 2D Spatio-Temporal (2DST) module employing an auxiliary segmentation forecasting task to enhance temporal semantic consistency; 3) A Spatio-Temporal Interaction Modeling (STIM) module that enables camera-agnostic feature interaction between 2D and 3D representations. The unified architecture is trained end-to-end and establishes new state-of-the-art performance on both Occ3D-nuScenes and Cam4DOcc benchmarks.", "tldr": "", "keywords": ["4D Occupancy", "Spatio-temporal Modeling", "Semi-Supervised"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82bf4aef0220c5c17132eb95f86a59ccf47e3efb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "STM4D is a framework for 4D occupancy forecasting that predicts future 3D semantic scenes from videos by jointly modeling spatiotemporal dynamics in both 2D image space and 3D voxel space through a Spatio-Temporal Interaction Module (STIM). The STIM integrates 2D feature representations with refined 3D features, enabling end-to-end training. Experiments conducted on the Occ3D-nuScenes and Cam4DOcc benchmarks demonstrate competitive, and often state-of-the-art, performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses an important problem—vision-only 4D occupancy forecasting. This topic holds significant practical relevance for autonomous driving. The paper is clearly written, and the diagrams are intuitive and easy to understand."}, "weaknesses": {"value": "The experimental results are somewhat limited, as they are restricted to the Occ3D-nuScenes and Cam4DOcc datasets. Additional evaluations on more datasets (e.g., Lyft Level-5) will strengthen the validation of the proposed approach. While the joint 2D–3D design is reasonable, similar insights have already been explored and verified in prior occupancy-related literature. Compared with Cam4DOcc, this work represents an incremental improvement rather than a fundamentally novel formulation."}, "questions": {"value": "1. Please add experiments and comparisons on the Lyft Level-5 to assess the performance.\n(Lyft Level-5 Perception Dataset, 2019)\n2. Please compare STM4D against EfficientOCF (CVPR 2025).\n3. EfficientOCF explores spatiotemporal decoupling; what are the principal conceptual and architectural differences, and under what conditions would each be preferable?\n4. The authors concatenate historical image features along the channel dimension. How do you mitigate spatial misalignment across views and time (e.g., due to ego-motion)? Is there any warping or learned alignment?\n5. Since STIM relies on the quality of the generated BEV features, how do you ensure the geometric correctness of those features?\n6. In Table 1, why does STM4D (supervised by 2D Labels & 3D Occ) underperform OccLLAMA-F (supervised only by 3D Occ) on Avg. IoU?\n7. How does STM4D remain robust when objects become occluded over time?\n8.Since 2D labels are auto-generated by SAM, how do you map SAM masks/classes to the occupancy taxonomy, and how do you reduce semantic drift between 2D labels and 3D occupancy classes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FEDSJS4tsK", "forum": "vfDJiI0dbu", "replyto": "vfDJiI0dbu", "signatures": ["ICLR.cc/2026/Conference/Submission12972/Reviewer_KpCi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12972/Reviewer_KpCi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807598859, "cdate": 1761807598859, "tmdate": 1762923726848, "mdate": 1762923726848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents STM4D, a unified deep learning framework for 4D occupancy forecasting in autonomous driving scenarios. STM4D integrates two complementary spatio-temporal modeling modules: a 3D Spatio-Temporal (3DST) module for capturing volumetric dynamics; and a 2D Spatio-Temporal (2DST) module employing an auxiliary segmentation forecasting task to enforce temporal semantic consistency in the image space. The approach also introduces a Spatio-Temporal Interaction Module (STIM) to facilitate cross-modal interactions between the 2D and 3D representations without explicit reliance on camera parameters. Experiments on the Occ3D-nuScenes and Cam4DOcc benchmarks demonstrate state-of-the-art results, with ablations validating each component."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper conducts extensive evaluations on two leading benchmarks and sets the new SOTA.\n2. The qualitative comparisons offer clear visual evidence that STM4D produces more temporally consistent and geometrically plausible predictions over multi-second horizons."}, "weaknesses": {"value": "1. The framework design lacks detailed explanations. For instance, in the section on the 3DST module: What are the shapes of the intermediate features? Along which dimension are the 3D features of past and current frames concatenated? Why does the spatial encoder take the concatenated 3D features as a whole as input, yet produce spatial features for multiple future frames? Are these features consistent across frames, or is there some kind of autoregressive module involved? Similar questions also arise regarding the 2DST module, and we hope further clarifications can be provided in the subsequent rebuttal.\n2. The ablation analysis could go deeper. For instance, the 3DST module employs two LSTM modules for temporal forecasting and feature refinement, based on the argument that this \"enhances their temporal consistency.\" However, this design choice is insufficiently validated by the experimental results.\n3. Despite the abundant set of modular designs incorporated into the model, the performance improvement over previous SOTA methods appears somewhat limited. For example, the average mIoU improvment on Occ3D-nuScenes is only about 3% compared to PreWorld+."}, "questions": {"value": "1. Given the number of components in STM4D, it is notable that its FPS remains comparable to PreWorld. To better understand this efficiency, could the authors provide a detailed breakdown of the latency introduced by each module?\n2. We note from Table 7 that STM4D does not achieve SOTA prediction performance across all semantic categories. Could the authors provide some analysis into which categories underperform and the potential reasons behind this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kGuJMrd8ub", "forum": "vfDJiI0dbu", "replyto": "vfDJiI0dbu", "signatures": ["ICLR.cc/2026/Conference/Submission12972/Reviewer_Y9Bu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12972/Reviewer_Y9Bu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924486540, "cdate": 1761924486540, "tmdate": 1762923726491, "mdate": 1762923726491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STM4D, a unified framework for vision-based 4D occupancy forecasting that addresses the limitation of previous methods by simultaneously modeling spatio-temporal dynamics in both 3D voxel-based representations and 2D multi-view image sequences. The core idea is to leverage the temporal cues inherent in the 2D image sequences to improve the 4D occupancy prediction. The entire architecture is trained end-to-end."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The technical design is sophisticated and well-justified. The model combines classical techniques like ConvLSTM (extended to 3D) for sequence prediction with modern mechanisms like cross-attention for feature interaction\n\n2. The paper is clearly written, well-structured, and easy to follow."}, "weaknesses": {"value": "1. The paper acknowledges that STM4D underperforms PreWorld+ in terms of the raw IoU metric on Occ3D-nuScenes (Table 1). Furthermore, even with the inclusion of additional 2D labels, the mIoU improvement remains marginal and not sufficiently superior to existing baselines.\n\n2.The proposed 2D/3D spatio-temporal modeling lacks novelty, as similar architectures and strategies have been widely adopted in prior occupancy flow and 4D occupancy forecasting works.\n\n3. While the paper claims that the 2DST module explicitly models spatio-temporal relationships across frames through 3D convolution-based upsampling and refinement, the actual mechanism appears to depend primarily on frame concatenation and a standard 2D CNN (DenseNet). This design choice raises questions about whether the temporal dynamics are being effectively captured, especially compared to the ConvLSTM-based 3DST branch, which is more inherently suited for sequential modeling.\n\n4. There is a lack of comparison and discussion with relevant 4D occupancy forecasting baselines, particularly Let Occ Flow [1]. Since both works share the central idea of leveraging temporal cues from 2D image sequences to enhance 4D occupancy prediction, the claim that this direction “remains unexplored” appears inaccurate or overstated.\n\n5. The architecture introduces two separate LSTM modules to process temporal features, which increases model complexity without a clear ablation demonstrating the necessity or complementary nature of both components.\n\nReference:\n[1] Liu Y, Mou L, Yu X, et al. Let Occ Flow: Self-Supervised 3D Occupancy Flow Prediction. arXiv preprint arXiv:2407.07587, 2024."}, "questions": {"value": "Please see the weakness for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Le2C3at8i", "forum": "vfDJiI0dbu", "replyto": "vfDJiI0dbu", "signatures": ["ICLR.cc/2026/Conference/Submission12972/Reviewer_qeo6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12972/Reviewer_qeo6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972846439, "cdate": 1761972846439, "tmdate": 1762923726068, "mdate": 1762923726068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the task of 4D occupancy forecasting, aiming to predict future 3D scene occupancy states based on past observations. The authors propose a model that performs temporal feature interaction at both 3D and 2D levels, as well as cross-level interaction between 2D and 3D features. Additionally, they introduce an auxiliary task that predicts future-frame 2D segmentation maps to enhance temporal modeling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The writing is clear, and the architecture’s structure can be easily understood.\n\n2. The exploration of temporal information in 4D occupancy prediction is meaningful and relevant to current research trends."}, "weaknesses": {"value": "1. The paper does not provide incremental theoretical or conceptual insights to the field. While it applies convolutional operations on 2D and 3D temporal features and uses cross-attention to fuse spatiotemporal information between them, the approach remains largely a straightforward technical combination. There is no clear theoretical analysis or empirical evidence demonstrating the intrinsic advantages or depth of the proposed mechanism.\n\n2. The method performs worse than recent SOTA approaches. For example, II-World [3] achieves 18.97 mIoU on the Occ3D-nuScenes benchmark, roughly twice the performance of this paper’s model. On Cam4DOcc, the improvement over other baselines in the comparison table is only marginal.\n\n3. Although the paper focuses on leveraging temporal information for occupancy forecasting, it should better discuss other recent works that explicitly address temporal fusion in occupancy prediction, such as [1] and [2].\n\nMinor:\nKey implementation details such as loss functions and supervision signals should be clearly specified in the Method section to improve understanding.\n\n[1] Rethinking Temporal Fusion with a Unified Gradient Descent View for 3D Semantic Occupancy Prediction, CVPR 2025\n\n[2] GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction, CVPR 2025\n\n[3] II-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting, ICCV 2025"}, "questions": {"value": "1. Eqs (3), (4), and (5) appear to be somewhat redundant. In Eq (5), V_{3d} is repeatedly used. From my understanding, could the sum of T_{3d} and T_{3d}' be represented as a residual connection, eliminating the need to explicitly separate the two networks LSTM_{AR} and LSTM_{Refine}? The final residual-connected output could then be concatenated with V_{3d}.\n\n2. In typical occupancy prediction tasks, temporal fusion methods [1, 2] rely on ego-motion information for frame-to-frame spatial alignment. However, the spatial-temporal modeling described in Sec. 3.2 does not appear to use ego-motion cues. Is it challenging to learn spatial-temporal relationships purely from voxel features across frames without motion alignment?\n\n3. L 266–269 mention applying 3D convolution on 2D features. On which additional dimension is this convolution performed, the temporal dimension or another axis? This should be clearly clarified in the paper.\n\n[1] Rethinking Temporal Fusion with a Unified Gradient Descent View for 3D Semantic Occupancy Prediction, CVPR 2025\n\n[2] GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction, CVPR 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "M6EMRzkA1X", "forum": "vfDJiI0dbu", "replyto": "vfDJiI0dbu", "signatures": ["ICLR.cc/2026/Conference/Submission12972/Reviewer_Uizt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12972/Reviewer_Uizt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991761352, "cdate": 1761991761352, "tmdate": 1762923725564, "mdate": 1762923725564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}