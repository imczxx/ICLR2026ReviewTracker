{"id": "MbMzoQ91Gk", "number": 1003, "cdate": 1756827748886, "mdate": 1759898232504, "content": {"title": "ChronoEdit: Towards Temporal Reasoning for In-Context Image Editing and World Simulation", "abstract": "Recent advances in large generative models have significantly advanced image editing and in-context image generation, yet a critical gap remains in ensuring physical consistency, where edited objects must remain coherent. This capability is especially vital for world simulation related tasks. In this paper, we present ChronoEdit, a framework that reframes image editing as a video generation problem. First, ChronoEdit treats the input and edited images as the first and last frames of a video, allowing it to leverage large pretrained video generative models that capture not only object appearance but also the implicit physics of motion and interaction through learned temporal consistency. Second, ChronoEdit introduces a temporal reasoning stage that explicitly performs editing at inference time. Under this setting, target frame is jointly denoised with reasoning tokens to imagine a plausible editing trajectory that constrains the solution space to physically viable transformations. The reasoning tokens are then dropped after a few steps to avoid the high computational cost of rendering a full video. To validate ChronoEdit, we introduce PBench-Edit, a new benchmark of image–prompt pairs for contexts that require physical consistency, and demonstrate that ChronoEdit surpasses state-of-the-art baselines in both visual fidelity and physical plausibility.", "tldr": "", "keywords": ["image editing", "generative models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/13dc303f1b947004c48aee326d378d6707b8a79c.pdf", "supplementary_material": "/attachment/210a187a014863d6cb19feafb475e640de9bc80f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes modeling the physical realism of image editing using video generation models. By leveraging the strong temporal, physical, and motion consistency capabilities of video generation models, the approach achieves impressive editing results. Furthermore, the introduction of a temporal reasoning token to simulate intermediate video frames is a very intuitive idea, and the experimental results are remarkable."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Utilizing video generation model to model the image editing task, which introducing great physical prior, achieving great results.\n2. The Temporal Reasoning Token simulate the intermidiate step of video and it fills the gap of modeling intermediate changes in image editing, resulting in stronger interpretability.\n3. After distilling, the result is still great and the speed is nearly comparable with image editing model."}, "weaknesses": {"value": "I do not have many concerns regarding the content of the paper itself. However, I am curious about how the video generation based model would perform in multi-turn editing scenarios involving different user instructions, where each round would introduce an additional input frame, which is an interesting attempt for future development.\nAdditionally, I want to know the true performance of trained video generative model in video generation. This would provide insights into how demanding the requirements are for training video generation models to achieve high-quality image editing. Please show the result in video generation benchmark like VBench series."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mHDM9FKd1e", "forum": "MbMzoQ91Gk", "replyto": "MbMzoQ91Gk", "signatures": ["ICLR.cc/2026/Conference/Submission1003/Reviewer_kjZH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1003/Reviewer_kjZH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760974075687, "cdate": 1760974075687, "tmdate": 1762915654224, "mdate": 1762915654224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method termed ChronoEdit using latent diffusion/flow video models for editing images based on text prompts and a reference image. Three variants of the method are proposed in the paper. (1) ChronoEdit - generations of the edited image as a video having two frames (the reference image and the output image); (2) ChronoEdit-Think - where additional \"reasoning\" frames/tokens are added between these two frames in order to model better physical consistency across time; (3) ChronoEdit-Turbo - distilled version of ChronoEdit for few-step sampling. The paper also presents PBench-Edit, a new benchmark for image editing designed to assess editing in physically grounded contexts. The authors compare the 3 variants to various baselines from the literature on two benchmarks and show an ablation study for various aspects of their method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The main novelty of this paper, as I understand it, since I am not familiar with the literature in this area, resides in the idea to frame the problem of image editing based on text prompts as a video generation task using diffusion/flow models. By doing so, one can get better physical consistency. Indeed, I find this idea novel and interesting.\n* To gain better control and interpretability, another novel part of the method is to introduce the so-called reasoning tokens, which show that the authors took an extra step in the modeling process of the problem.\n* The authors introduce a new benchmark, which I suppose can be valuable for research in this area.\n* The method seems to improve over baseline methods on both benchmarks, both quantitatively and qualitatively. \n* For the most part, the paper is written clearly."}, "weaknesses": {"value": "* While I do appreciate the idea of using video generation models for image editing and the other improvements to the method, in terms of the overall contribution and novelty, to me, this work seems limited. To properly assess its contribution, I believe it will be valuable if the authors could further elaborate on how this work lays a new foundation for future models in this area and if it opens new avenues of research.\n* Regarding the experiments:\n  - To me, the quantitative results are not clear enough. Specifically, are the numbers in the table bounded between [0,5]? If not, are they bound in another region? How exactly are they measured? In addition, in order to understand the significance of the results, std information should be reported.\n  -  The authors present only successful cases for their model, but what are some failure cases of it? Are there common situations in which it systematically fails?\n  - One experiment I found missing is showing the performance of the method when using out-of-the-box video generation models (without training), where the first and last frames are set as in this paper.\n  - Regarding reproducibility, code wasn't provided. For me, this is a weakness in an empirical paper.\n* Minor:\n  - In Fig. 3, what is the prompt?\n  - In line 311, do you mean $N=8$?\n  - It is not clear which version of ChronoEdit, ChronoEdit-Turbo distills.\n\nFor now, my tendency is to reject the paper; however, as I am not an expert in this field, I would like to see the author's response, assessment of my fellow reviewers, and the AC before making a final decision."}, "questions": {"value": "* In line 201, how is it ensured that F', c, and w are integers?\n* Was the model from which you generated the training dataset (\"video data curation\") trained (either in the pre-training stage or fine-tuning) on PBanch?\n* All baseline methods seem recent; why is there such a gap in the number of parameters? Specifically, I assume that at least part of the baseline methods also use latent diffusion/flow models. What causes the difference then? And can you evaluate your method using exactly the same network and number of parameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ui4whNQWBr", "forum": "MbMzoQ91Gk", "replyto": "MbMzoQ91Gk", "signatures": ["ICLR.cc/2026/Conference/Submission1003/Reviewer_rm73"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1003/Reviewer_rm73"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761209265374, "cdate": 1761209265374, "tmdate": 1762915654116, "mdate": 1762915654116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ChronoEdit, a novel framework designed to instill physical consistency in generative image editing, a capability deemed crucial for \"world simulation\" applications (e.g., robotics, autonomous driving). ChronoEdit achieves this by reframing image editing as a two-frame video generation problem, thereby leveraging the learned temporal prior from pre-trained video generative models. For enhanced coherence, the framework features a Temporal Reasoning Stage (ChronoEdit-Think) during inference. Here, \"reasoning tokens\" (imagined intermediate video frames) are jointly processed with the input to implicitly constrain the final edit trajectory to one that is physically plausible. The authors validate their approach with the new PBench-Edit benchmark, demonstrating SoTA performance in both visual quality and physical realism over existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of repurposing the powerful temporal mechanisms of video generation models to solve a fundamental deficiency (physical inconsistency) in static image editing is a good idea, providing a principled method for incorporating dynamic laws.\n2. By focusing on physical consistency, ChronoEdit addresses an important bottleneck for real-world applications like autonomous systems, where geometric or physical inconsistencies are unacceptable.\n3. The Temporal Reasoning Stage is cleverly implemented to optimize efficiency by limiting the reasoning steps ($N_r$) and discarding the tokens early in the denoising process. Furthermore, the visualized reasoning trajectory offers a degree of interpretability into the model's \"thinking process.\"\n4. The creation of the PBench-Edit benchmark is a valuable contribution, establishing a much-needed standard to evaluate models on physical and temporal coherence, pushing research beyond purely aesthetic metrics.\n5. Figure 4 looks great"}, "weaknesses": {"value": "1. Despite efforts to optimize, the ChronoEdit-Think variant still introduces a measurable inference overhead compared to non-reasoning baselines.\n2. The framework’s success depends entirely on the video model's ability to perfectly encode and enforce physical laws within its high-dimensional latent space. If the latent representation merely captures statistical correlations of motion rather than strict physical principles, errors in complex or novel dynamics will inevitably persist."}, "questions": {"value": "1. What editing operations fundamentally cannot benefit from this framework?\n2. Is rejection sampling effective for generating physically plausible results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hOKA8t7RU0", "forum": "MbMzoQ91Gk", "replyto": "MbMzoQ91Gk", "signatures": ["ICLR.cc/2026/Conference/Submission1003/Reviewer_BTgq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1003/Reviewer_BTgq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996428091, "cdate": 1761996428091, "tmdate": 1762915653907, "mdate": 1762915653907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes a ChronoEdit method for image editing, which reframes the editing problem as a video generating problem. It introduces a temporal reasoning mechanism at inference to perform editing, wherein reasoning tokens are designed to imagine plausible editing trajectory. A novel benchmark dataset is proposed to evaluate the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe idea of solving image editing using video generation models is interesting.\n-\tExperimental results on the benchmark datasets show good performance."}, "weaknesses": {"value": "-\tThe proposed method is prone to the video generation models. Is the upper bound of the proposed method is limited to the performance of the video generation models?\n-\tThe computational complexity of the proposed method vs. other image editing methods is expected to be justified.\n-\tThe ambiguity of the “edited” image. As mentioned by the author in L216-232, the author formulates the image editing as a T-frame video generation problem, wherein 0- and T-th frames are defined as the input and edited image. Can the (T-1)-th frame or other frames also be considered as the “edited” image? If not, what are the key differences between the (T-1) and T frames?\n-\tThere are methods [1-3] that apply chain-of-thought techniques to the image editing problem, which also attempt to add an intermediate process into the editing problem. Can you discuss and justify the proposed paradigm vs these methods? \n\n[1]. Enhancing Image Editing with Chain-of-Thought Reasoning and Multimodal Large Language Models\n[2]. ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding"}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Om8mVWde76", "forum": "MbMzoQ91Gk", "replyto": "MbMzoQ91Gk", "signatures": ["ICLR.cc/2026/Conference/Submission1003/Reviewer_dEVi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1003/Reviewer_dEVi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094047463, "cdate": 1762094047463, "tmdate": 1762915653732, "mdate": 1762915653732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}