{"id": "OXo4Tp4LI0", "number": 6839, "cdate": 1757997634837, "mdate": 1763014492186, "content": {"title": "ReasonNav: Human-Inspired Global Map Reasoning for Zero-Shot Embodied Navigation", "abstract": "Embodied agents often struggle with efficient navigation because they rely solely on partial egocentric observations, which restrict global foresight and lead to inefficient exploration. In contrast, humans plan using maps: we reason globally first, then act locally. We introduce ReasonNav, a human-inspired framework that operationalizes this reason-then-act paradigm by coupling Multimodal Large Language Models (MLLMs) with deterministic planners. ReasonNav converts a top-down map into a discrete reasoning space by room segmentation and candidate target nodes sampling. An MLLM is then queried in a multi-stage process to identify the candidate most consistent with the instruction (object, image, or text goal), effectively leveraging the model’s semantic reasoning ability while sidestepping its weakness in continuous coordinate prediction. The selected waypoint is grounded into executable trajectories using a deterministic action planner over an online-built occupancy map, while pretrained object detectors and segmenters ensure robust recognition at the goal. This yields a unified zero-shot navigation framework that requires no MLLM fine-tuning, circumvents the brittleness of RL-based policies and scales naturally with foundation model improvements. Across three navigation tasks, ReasonNav consistently outperforms prior methods that demand extensive training or heavy scene modeling, offering a scalable, interpretable, and globally grounded solution to embodied navigation.", "tldr": "In this paper, we introduces ReasonNav, a novel framework for zero-shot embodied navigation, leveraging the strengths of MLLMs for high-level hierarchical global reasoning, while relegating low-level control to robust, deterministic planners.", "keywords": ["Large Language Model", "Multimodalitiy", "Embodied navigation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/63da2f0ee22398092c42747000ad6bd0c96c80d7.pdf", "supplementary_material": "/attachment/d254fb2ca9f83e0463ec7db5c157dbe4929c2f25.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes ReasonNav, a zero-shot embodied navigation framework that separates high-level reasoning and low-level control. A multimodal LLM performs global reasoning on a top-down floor map through a two-stage discrete selection process (room-level then point-level), producing a global goal for a deterministic planner (A* + VFH*). The system supports ObjectNav, ImageNav, and TextNav within a unified, training-free pipeline. Experiments on HM3D demonstrate competitive success rates and state-of-the-art path efficiency (SPL), with ablations validating design choices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a clear reasoning–planning decomposition, combining LLM-based semantic reasoning with deterministic path planning.\n2. The two-stage discrete selection design effectively improves goal localization and navigation efficiency.\n3. Experiments show strong zero-shot performance and competitive SPL on standard benchmarks.\n4. The paper is well written and reproducible, with detailed prompts, pseudo-code, and ablations."}, "weaknesses": {"value": "1. The paper’s “unified” claim covers only goal-conditioned navigation (ObjectNav, ImageNav, TextNav) and omits instruction-following VLN tasks that require sequential reasoning. Related work such as SmartWay: Enhanced Waypoint Prediction and Backtracking for Zero-Shot Vision-and-Language Navigation (Shi et al., IROS 2025) should be discussed to clarify the scope.\n2. Despite the emphasis on reasoning, the proposed method mainly performs static selection (choosing a room and a point) rather than explicit multi-step or interpretable reasoning. The reasoning process is not analyzed or visualized.\n3. The approach relies on numerous heuristic components (map segmentation, candidate sampling, prompt templates, ensemble voting), making it highly engineered and potentially sensitive to environment changes.\n4. The assumption of an ideal, pre-known floor map limits the method’s applicability in realistic embodied settings.\n5. While technically solid, the contribution is primarily system-oriented and lacks deeper insight into reasoning or learning mechanisms."}, "questions": {"value": "1. How would the system perform under incomplete or noisy maps?\n2. Would online re-planning or iterative reasoning improve robustness in complex environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "weKbCRetQM", "forum": "OXo4Tp4LI0", "replyto": "OXo4Tp4LI0", "signatures": ["ICLR.cc/2026/Conference/Submission6839/Reviewer_Zfnt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6839/Reviewer_Zfnt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761426869546, "cdate": 1761426869546, "tmdate": 1762919098906, "mdate": 1762919098906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "oPHclxSkdx", "forum": "OXo4Tp4LI0", "replyto": "OXo4Tp4LI0", "signatures": ["ICLR.cc/2026/Conference/Submission6839/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6839/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763014491369, "cdate": 1763014491369, "tmdate": 1763014491369, "mdate": 1763014491369, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ReasonNav, a human-inspired embodied navigation framework that integrates MLLMs for high-level global reasoning and deterministic planners for local control. The system transforms navigation into a discrete reasoning problem, enabling zero-shot navigation without reinforcement learning or fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The reason-then-act paradigm is well-motivated and effectively addresses the limitations of both end-to-end RL methods and reactive exploration. The analogy to human map-based reasoning is conceptually reasonable.\n- The discrete node-based reasoning with hierarchical MLLM prompting, deterministic local planning, and ensemble verification are technically coherent. \n- The framework performs competitively without fine-tuning or task-specific training, highlighting scalability with the evolution of foundation models."}, "weaknesses": {"value": "- The method assumes access to a high-quality top-down map, which is unrealistic for many real-world settings where such maps are unavailable or noisy. The authors acknowledge this limitation but should discuss how the system could integrate online SLAM or partial map construction to relax this assumption.\n- The reasoning step is performed once at the beginning. The agent does not revise or refine its global plan based on egocentric semantic feedback during navigation. This limits robustness when the MLLM’s initial reasoning is suboptimal. Future work could incorporate dynamic reasoning triggers.\n- While the simulation results are comprehensive, there is no demonstration in real robotic environments. Given the deterministic planning nature and reliance on pretrained vision models, an analysis of sim-to-real transfer or physical robot feasibility would strengthen the paper."}, "questions": {"value": "Is it possible to evaluate the model’s performance on popular VLN simulation benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mB2ChrJzJp", "forum": "OXo4Tp4LI0", "replyto": "OXo4Tp4LI0", "signatures": ["ICLR.cc/2026/Conference/Submission6839/Reviewer_ecbZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6839/Reviewer_ecbZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839468814, "cdate": 1761839468814, "tmdate": 1762919098295, "mdate": 1762919098295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ReasonNav, a framework for zero-shot embodied navigation that decouples global reasoning from local execution. The key innovation is using VLMs to perform hierarchical reasoning over top-down maps. The framework is evaluated on the HM3D benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and easy to follow. \n- The appendix provides detailed prompts, supporting reproducibility."}, "weaknesses": {"value": "- The paper fundamentally requires a top-down 2D map with wall information. The authors acknowledge this limitation in the Appendix but don't adequately address how realistic this assumption is. Perhaps in most real-world cases, this assumption would not hold.\n- In Table 1, the results of most baselines on ImgNav and TextNav are missing. Given that ReasonNav is not achieving the best SR on both ObjectNav and ImgNav, more results comparison would give a clearer map of the performance of ReasonNav."}, "questions": {"value": "- Computational costs like token usage should be provided.\n- Could authors provide more failure case studies? I am wondering what the most common failure modes are."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "avWIYAIsTB", "forum": "OXo4Tp4LI0", "replyto": "OXo4Tp4LI0", "signatures": ["ICLR.cc/2026/Conference/Submission6839/Reviewer_PSXv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6839/Reviewer_PSXv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983011822, "cdate": 1761983011822, "tmdate": 1762919097713, "mdate": 1762919097713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}