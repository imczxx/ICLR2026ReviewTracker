{"id": "aemqAxScl9", "number": 3609, "cdate": 1757486306015, "mdate": 1759898078804, "content": {"title": "SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation", "abstract": "Large scale robot learning has recently shown promise in enabling robots to perform complex tasks by integrating perception, control, and optionally language understanding into a unified framework. However, they continue to struggle with long-horizon, contact-rich manipulation tasks, such as the handling of deformable objects, where supervision from demonstrations is often inconsistent in quality. In such settings, reward modeling offers a natural solution: by providing grounded progress signals, it can transform noisy demonstrations into stable supervision that generalizes across diverse trajectories. In this work, we introduce a task stage-aware, video-based reward modeling framework that jointly predicts high-level task stages and fine-grained progress within each stage. Reward labels are automatically derived from natural language subtask annotations, enabling consistent progress estimation across variable-length and heterogeneous demonstrations. This design overcomes the limitations of frame-index-based labeling, which collapses in long, variable-duration tasks such as folding a T-shirt. Our reward model demonstrates robustness to demonstration variability, generalization to out-of-distribution scenarios, and strong utility for downstream policy training. Building upon this reward model, we propose the Reward-Aligned Behavior Cloning (RA-BC) framework, which selectively filters high-quality data and reweights training samples according to reward estimates. Extensive experiments show that the reward model alone outperforms baselines on human demonstration validation and out of distribution real robot policy rollouts. When integrated into RA-BC, our approach achieves 83\\% success on folding T-shirts from the flattened state and 67\\% from the crumpled state—dramatically surpassing vanilla behavior cloning, which attains only 8\\% and 0\\% success under the same training dataset, respectively. Overall, our results highlight reward modeling as a key enabler for scalable, annotation-efficient, and robust imitation learning in long-horizon robotic manipulation.", "tldr": "", "keywords": ["Imitation Learning", "Reward Modeling", "Robotics Manipulation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/93cd726d3370deba7ff89b4304c56765315a0f0c.pdf", "supplementary_material": "/attachment/f8dfc5c4e65cf6b4b33331d56f58186ef25c4e17.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a reward-modeling approach that supplies stable, dense feedback for long-horizon manipulation. It decomposes progress estimation into two parts: (1) a high-level predictor that identifies the current stage of the task and (2) a fine-grained regressor that measures progress within that stage. This two-stage design reduces sensitivity to variable demonstration lengths (e.g., in cloth folding) and yields more reliable rewards. Building on this signal, the authors also introduce an imitation-learning scheme that up-weights higher-quality demonstrations. Experiments show consistent gains over single-stage progress-prediction baselines, with real-robot results on cloth-folding validating the approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Strong real-world gains: On a challenging cloth-folding task, a naive fine-tuning SOTA behavior policy (Pi0) fails 0/12 trials, while the proposed method succeeds in 8/12—highlighting the efficacy of their good reward modeling.\n- The paper analyzes performance across (i) high-level stage-prediction accuracy, and (ii) end-to-end policy rollouts, helping readers to understand where the method adds value.\n- Consistent improvements: The performance improvement holds across different rollout outcomes (success/partial/failure progress prediction) and task difficulty, from simple cloth pick-and-place to full folding."}, "weaknesses": {"value": "**Novelty of the stage-aware reward modeling.**\n  + Stage-aware reward modeling for long-horizon manipulation is not new per se. Please clarify what is distinct, and compare directly to prior work (e.g., Drs [1], REDS [2]).\n\n**Clarity on data-quality literature.**\n\n  + L76–79 state data quality is hard to assess beyond simple heuristics (e.g., duration). This could be misleading: recent work proposes stronger proxies. Please cover this literature (like [3] and [4]) and briefly position your method relative to these proxies.\n\nPlease clarify the literature on stage-aware reward modeling and data quality, state how your approach differs from prior work, and—if possible—add experiments showing superiority over these to strengthen your paper’s contribution and rigor.\n\n### References\n- [1] Mu et al., \"Drs: Learning reusable dense rewards for multi-stage tasks\", ICLR 2024.\n- [2] Kim et al., \"Subtask-Aware Visual Reward Learning from Segmented Demonstrations\", ICLR 2025.\n- [3] Belkhale et al., \"Data Quality in Imitation Learning\", NeurIPS 2023\n- [4] Dass et al., \"DataMIL: Selecting Data for Robot Imitation Learning with Datamodels\", arXiv preprint arXiv:2505.09603 2025"}, "questions": {"value": "- [Q1] Is the low performance due to underfitting?\n    + In Table 2, ReWiND improves with longer training (e.g., 20K Medium: 1/12 → 40K Medium: 6/12). Does this indicate the model hasn’t converged yet and could achieve higher performance with more training? (possibly outperform your model as well?)\n\n- [Q2] Model architecture parity\n    + Are ReWiND and SARM matched in model size? Since original ReWiND leverages large-scale data (OXE), it may use a larger model, which could overfit on your 200-hour dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ijFBzvCc3R", "forum": "aemqAxScl9", "replyto": "aemqAxScl9", "signatures": ["ICLR.cc/2026/Conference/Submission3609/Reviewer_WSRa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3609/Reviewer_WSRa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761226891312, "cdate": 1761226891312, "tmdate": 1762916869314, "mdate": 1762916869314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SARM, which introduces stage information about a task to improve reward predictions given a real world robot trajectory/video. The results show this outperforms prior work like ReWiND and aligns closer to the ground truth progress rewards. The learned reward model in SARM further is used to reweight training samples for better performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Introduces a novel use of stage prediction to get improved auto generated reward labels of a dataset. This contrasts with prior work that simply used frame indices of a video as the label of the reward progress, which is limited since different stages of the task may proceed at different speeds.\n- This work further introduces some useful tricks that improve performance and leads to easier to learn reward models that are much more smooth.\n- Strong improvement upon prior work is shown with ablations indicating how the different proposed ideas improve SARM."}, "weaknesses": {"value": "- Compared to prior work this method requires more annotation of datasets (subtask labels).\n- There doesn't seem to be an ablation on the reward-model based weighting of training samples, which I believe would be fairly important given the amount of space allocated to describe the RA-BC method. The discussion in 4.3 suggests that a bad reward model would cripple RA-BC results since it may lead to poor reweighting, it may be possible that not doing any re-weighting may end up working better."}, "questions": {"value": "See above\n\nGenerally think this work is solid and should be accepted. Happy to raise score if the question about the reward reweighting is addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZoMsII01t7", "forum": "aemqAxScl9", "replyto": "aemqAxScl9", "signatures": ["ICLR.cc/2026/Conference/Submission3609/Reviewer_3zBM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3609/Reviewer_3zBM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941706487, "cdate": 1761941706487, "tmdate": 1762916869077, "mdate": 1762916869077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A core challenge in imitation learning literature (e.g. behavior cloning) is learning from diverse demonstration data: multi-modal action distributions, varying teleoperation strategies and proficiency, human errors, as well as sometimes wildly different trajectory lengths depending on the teleoperator and initial/goal configurations. This paper aims to automatically estimate task progress in demonstration data regardless of factors such as trajectory length (recent method ReWiND is prone to fail in this setting), and then subsequently using estimated task progress (reward) to weigh samples during BC policy learning. To do so, the proposed method estimates stage and subtask progress from image observations + joint states + language annotations using a transformer backbone, as well as a number of engineering decisions and heuristics detailed in the paper. Experiments are conducted on a real-world T-shirt folding task using a dual-arm setup, and results indicate that the proposed framework (SARM / RA-BC) improves task progress estimation and downstream policy performance compared to strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "My initial assessment of this paper leans positive overall; it is generally well written, timely, and technically sound. Specifically:\n- I believe that this paper studies a relevant and timely problem (learning from \"diverse\" demonstrations wrt. manipulation strategy, trajectory length etc.), and is likely to be of interest to the community. The problem is clearly motivated in the introduction, and shortcomings of existing work (ReWiND in particular) is described in the related work section. The paper is generally well written and easy to follow, and the illustrations (Figure 1, 2) are helpful for understanding the technical contributions.\n- The proposed method seems technically sound and appears to rely fairly little on engineering or manual labor which is definitely a plus (besides language annotation of the demonstration data, I didn't see much discussion on this in the paper but I imagine it may be rather time-consuming). The approach is rather simple and intuitive (I consider this a strength), which makes me believe that it could feasibly be transferred to other tasks and potentially entirely different task domains provided that they can be separated into stages / subtasks.\n- I appreciate the focus on a difficult real-world task such as T-shirt folding, and the empirical results on this task are rather compelling. In particular, the qualitative comparison with ReWiND in Figure 3 clearly demonstrates the shortcoming of ReWiND: it is not very robust to varying trajectory lengths and, as a result, struggle with predictions at either end of the completion spectrum. The paper also includes a limited set of results on a dish unloading task (appendix A.6) as well as RL in simulation (pick cube) and the conclusions seem to be rather similar across these tasks and problem settings."}, "weaknesses": {"value": "I do not have any major concerns with the paper in its current form. However, I do believe that there is room for improvement in several ways:\n- The writing is a bit odd, with large parts of the experiment section moved to the appendices entirely. It would be helpful if the authors can be more clear about the experimental setup (T-shirt folding, dish unloading, RL finetuning in simulation) and perhaps summarize the results across these different settings in the main paper rather than only discussing them in the appendices; it is easy for readers to miss otherwise.\n- I am a little concerned about this particular engineering choice: *\"During annotation, only the subtasks defined by the protocol were labeled, and any trajectory that did not contain the complete sequence of subtasks specified by the protocol was discarded. Annotators watched the top-view video and segmented each trajectory into subtasks by recording the start and end frame indices. If a mistake occurred during execution, its start and end frames were also labeled; trajectories containing mistakes were excluded from subsequent model training\"* (L184). If one of the main claims of the paper is that SARM / RA-BC can learn from diverse demonstration data, why exclude trajectories that *e.g.* contain a human error somewhere in the trajectory? If the approach was truly robust, it should be able to infer that such actions are indeed errors that then subsequently are corrected by the operator. It would be helpful if the authors can clarify this particular aspect of their approach, and potentially include some experimental results in which data is not filtered as aggressively before any learning; that should give readers a better indication on what the key steps in the data pipeline are.\n\nMinor comments that need to be addressed but are insignificant wrt my overall assessment of the work:\n- The abstract is quite long which feels unnecessary, I would suggest revising it to be more concise.\n- It would be helpful if the authors could introduce the considered tasks somewhere in the main paper along with e.g. illustrations of what those tasks look like.\n- There are a few typos and grammatical errors throughout the paper. While they do not affect understanding, it would be a good idea to proof-read the paper before publication."}, "questions": {"value": "I would really appreciate it if the authors can address my comments in the \"weaknesses\" section above using written arguments and potentially additional experimental results. My main concerns / questions pertain to writing and the data filtering process."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "idT70dAByO", "forum": "aemqAxScl9", "replyto": "aemqAxScl9", "signatures": ["ICLR.cc/2026/Conference/Submission3609/Reviewer_fc5R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3609/Reviewer_fc5R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952727755, "cdate": 1761952727755, "tmdate": 1762916868673, "mdate": 1762916868673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors introduce a stage-aware, video-based reward modeling framework that jointly predicts high-level task stages and fine-grained progress within each stage. This design effectively addresses the limitations of frame-index-based labeling, which often fails in long, variable-duration tasks such as T-shirt folding. The learned reward models are further utilized to filter high-quality data and reweight training samples based on the estimated rewards. Extensive experiments demonstrate that the proposed reward model outperforms existing baselines in out-of-distribution real-robot rollouts and human demonstration evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The authors investigate a very important and timely direction, i.e., how to acquire high-quality data for training imitation learners on long-horizon, contact-rich manipulation tasks such as T-shirt folding. Quantifying demonstration quality is challenging because it depends on latent factors such as action consistency and contact stability, which cannot be directly measured. In this work, the authors propose a video-based reward modeling framework that leverages natural language annotations to assign progress labels and enable stable reward estimation, serving as a mechanism to filter high-quality data and improve policy performance in both simulation and real-world settings. The proposed method is timely and presents a novel idea for learning from suboptimal demonstrations, making it a valuable contribution to the community.\n\n2. The authors provide a comprehensive analysis of the proposed SARM reward model, comparing it against existing methods. The proposed approach demonstrates superior performance across multiple benchmarks. Furthermore, the authors evaluate the framework on  dish unloading, which further validates its effectiveness and generalization capability.\n\n3. By leveraging SARM, the RA-BC policy surpasses both BC baselines by a significant margin on medium and hard tasks. The experiments show that RA-BC effectively exploits diverse datasets by filtering out high-quality data frames, enabling the policy to learn robust, long-horizon manipulation strategies.\n\n4. The authors empirically demonstrate that RA-BC with SARM achieves substantially higher success rates than RA-BC with ReWiND, achieving 83% vs. 50% on medium tasks and 67% vs. 25% on hard tasks, clearly highlighting the advantage of the proposed reward model."}, "weaknesses": {"value": "**Overall Assessment:**\n\nThe paper provides a well-justified motivation for the proposed reward model and demonstrates its utility in selecting high-quality data for policy learning. However, several aspects remain unclear and merit further clarification.\n\n1. **Applicability to existing datasets:**\n    \nThe proposed reward model has shown promise in filtering high-quality data, but it remains unclear how well it generalizes to existing cloth manipulation datasets, such as **DROID* [1] and **Flat’n’Fold** [2]. How would the model perform if applied to these datasets for filtering or reweighting demonstrations?\n \n[1] DROID: A Large-Scale In-the-Wild Robot Manipulation Dataset, 2024\n[2] Flat’n’Fold: A Diverse Multi-Modal Dataset for Garment Perception and Manipulation, 2024\n    \n2. **Robustness to visual variations:**\n    \nHow does the proposed reward model handle changes in lighting and illumination conditions? Since real-world environments are often visually diverse and dynamic, it would be valuable to understand the model’s robustness and reliability under such variations.\n    \n3. **Baseline comparison:**\n    \nThe recent work by **Hung et al. (ICLR 2025)** [3] introduces an alternative strategy for learning reward models in long-horizon manipulation tasks. Although the learning objectives differ from those of SARM, a direct comparison between the two approaches under similar long-horizon, contact-rich settings would provide stronger empirical grounding and highlight the unique advantages of the proposed framework.\n\n[3] Hung et al., VICtoR: Learning Hierarchical Vision–Instruction Correlation Rewards for Long-Horizon Manipulation, ICLR 2025"}, "questions": {"value": "1. Generalization: How well does the proposed reward model generalize when applied to large-scale, real-world cloth manipulation datasets such as DROID and Flat’n’Fold?\n\n2. Robustness: How robust is the model to lighting and illumination changes, which commonly occur in real-world robotic settings?\n\n3. Comparative Evaluation: How does the proposed method compare with VICtoR (ICLR 2025) in terms of reward modeling effectiveness and policy performance on long-horizon, contact-rich manipulation tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aWBYaX7Tm8", "forum": "aemqAxScl9", "replyto": "aemqAxScl9", "signatures": ["ICLR.cc/2026/Conference/Submission3609/Reviewer_pgZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3609/Reviewer_pgZ9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762102716505, "cdate": 1762102716505, "tmdate": 1762916866806, "mdate": 1762916866806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}