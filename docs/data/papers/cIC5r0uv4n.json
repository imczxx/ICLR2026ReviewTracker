{"id": "cIC5r0uv4n", "number": 1677, "cdate": 1756903809597, "mdate": 1759898195990, "content": {"title": "CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP", "abstract": "Spatial grounding is crucial for referring image segmentation (RIS), where the goal of the task is to localize an object described by language. Current foundational vision-language models (VLMs), such as CLIP, excel at aligning images and text but struggle with understanding spatial relationships. Within the language stream, most existing methods often focus on the primary noun phrase when extracting local text features, undermining contextual tokens. Within the vision stream, CLIP generates similar features for images with different spatial layouts, resulting in limited sensitivity to spatial structure. To address these limitations, we propose COPATCH, a zero-shot RIS framework that leverages internal model components to enhance spatial representations in both text and image modalities. For language, COPATCH constructs hybrid text features by incorporating context tokens carrying spatial cues. For vision, it extracts patch-level image features using our novel path discovered from intermediate layers, where spatial structure is better preserved. These enhanced features are fused into a clustered image–text similarity map, COMAP, enabling precise mask selection. As a result, COPATCH significantly improves spatial grounding in zero-shot RIS across RefCOCO, RefCOCO+, RefCOCOg, and PhraseCut (+ 2–7 mIoU) without requiring any additional training. Our findings underscore the importance of recovering and leveraging the untapped spatial knowledge inherently embedded in VLMs, thereby paving the way for opportunities in zero-shot RIS.", "tldr": "We discover a novel path in CLIP layers to create intuitive, efficient image-text similarity map useful for zero-shot referring image segmentation.", "keywords": ["Zero-Shot Referring Image Segmentation", "CLIP"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b5ba865b130a10c79a4777bf40829a38da0aef1.pdf", "supplementary_material": "/attachment/74cd3d80fc702ae017c5dba6d45fe9fe1e2f3f34.zip"}, "replies": [{"content": {"summary": {"value": "CoPatch is a training-free, zero-shot referring image segmentation (RIS) method. It aims to improve spatial grounding in CLIP by (1) constructing hybrid text features that augment the primary noun with context tokens (CT) (e.g., \"woman smiling\"), and (2) building a patch-level spatial map, CoMap, from intermediate CLIP visual layers. The CoMap is generated by computing patch-text cosine similarity, crucially applying a negation (sign-flip) to the patch embeddings before projection to correct for \"opposite visualization\" artifacts . This map is then clustered and used to re-rank candidate masks generated by an off-the-shelf model. The approach shows improved zero-shot mIoU over baselines on RefCOCO/+/g and PhraseCut. Its Top-3 performance is exceptionally strong (e.g., +16.28 mIoU over Top-1 on RefCOCOg val), suggesting the CoMap is highly effective at identifying good candidates, even if the final Top-1 selection is imperfect."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Simple and well-motivated method**: Based on the experimental results, tapping intermediate layers for spatial knowledge and using a simple negation to fix inverted saliency maps are highly effective.\n\n2. **Strong performance**: Significant boost from Top-1 to Top-3 results (Table 1) validates the core claim - CoMap is a superior guide for scoring and ranking mask proposals compared to standard global-local similarity.\n\n3. **Good component analysis**: e.g. the ablations in Table 3(b) effectively demonstrate that each component—Context Tokens (CT), Top Candidate (TC) re-ranking, and Spatial Coherence (SC)—provides a measurable benefit."}, "weaknesses": {"value": "1. **Concerns on Architectural Generalizability:** The method's generalizability is a key concern, as its effectiveness appears tightly coupled to the specific architecture and training objective of ViT-based CLIP models. It remains unclear if the CoMap framework can be extended to convolutional backbones, such as the convnext_large_d_320 variant of OpenCLIP. Furthermore, the \"negation trick,\" which seems tailored to correct an artifact of CLIP's contrastive loss, may not be relevant or effective for models trained with different objectives, like the sigmoid-based loss used in ViT-SO400M-14-SigLIP-384. I would like to see experiments with these different variants.\n\n2. **Missing quantitative ablation:** \n    1. *Negation on/off*—The negation step is central to making CoMap work (Section B.2, Figure 11). However, there is no mIoU/oIoU ablation showing performance with versus without this negation. This is a critical piece of evidence needed to validate the component's true impact.\n    2. *No “no-clustering” baseline*—How does the method perform if masks are scored directly with the raw CoMap (no connected-component clustering)?\n\n3. **Confounded main comparisons:** The evaluation of CoPatch against key baselines like HybridGL and Global-Local is unfair. CoPatch is benchmarked using mask proposals from Mask2Former, while the baselines rely on proposals from SAM or FreeSOLO. Since the choice of mask generator significantly influences performance, this discrepancy prevents a fair assessment. While the appendix (Section C.2) attempts to address this issue, the analysis is undermined by the use of different evaluation metrics. Specifically, the results in Table 1 are reported in mIoU, whereas Table 6 uses oIoU, rendering the tables incomparable. To ensure a fair and transparent evaluation, the data from Tables 1, 6, and 7 should be reorganized into a unified analysis that uses a consistent metric across all methods. This direct comparison is crucial for understanding the true performance of CoPatch and should be presented in the main body of the paper—either in the primary results table or as a dedicated ablation study—rather than being relegated to the appendix.\n\n4. **Lack of Discussion of Relevant Works:** The paper omits discussion and comparison with recent, highly relevant works—a significant oversight for a SOTA-focused paper. These works must be addressed in the related work section and included in the Table 2 comparisons.\n    1. *Feature Design for Bridging SAM and CLIP (Ito et al., WACV 2025)*—A trained CLIP↔SAM bridge that mirrors CoPatch’s proposal + CLIP scoring pipeline.\n    2. *Mask Grounding for Referring Image Segmentation (Chng et al., CVPR 2024)*—Supervised method that aligns word tokens to pixels, similar to same fine-grained text-to-pixel goal as CoPatch.\n\n5. **Minor nits**:\n    1.  Figure 13 Caption: \"CLIP ViT-B/316\" is a typo for \"ViT-B/16\".\n    2.  Consistency: The method name should be unified (e.g., CoPatch without dash in the title vs. Co-Patch in Figure 5 and Figure 7)."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "veNutYgocD", "forum": "cIC5r0uv4n", "replyto": "cIC5r0uv4n", "signatures": ["ICLR.cc/2026/Conference/Submission1677/Reviewer_ehKc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1677/Reviewer_ehKc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760849080645, "cdate": 1760849080645, "tmdate": 1762915853500, "mdate": 1762915853500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method of leveraging spatial knowledge in the pretrained model to improve zero-shot referring image segmentation. In particular, the proposed CoPatch constructs hybrid text features by considering context tokens that carry spatial cues. In addition, CoPatch extracts patch-level image features using a novel path found from intermediate layers. Then, these enhanced features are employed to calculate image-text similarity map."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "(1) The motivation is well presented of leveraging spatial knowledge in the pretrained model to improve zero-shot referring image segmentation.\n\n(2) The illustrations is mostly clear and intuitive of the context tokens, the patch-level spatial map, the fusion and the reranking of top candidate masks."}, "weaknesses": {"value": "(1) The contribution is limited. In particular, the task setting is not clearly presented where this zero-shot setting is not well distinguished with the supervised setting since a pretrained model is employed. In addition, the experimental comparison seems to be unfair and problematic.\n\n(2) On Page 6, Line 292-293, the comparison is unfair since the proposed CoPatch uses Mask2Former as pretrained model while all the SOTA methods does not.\n\n(3) In Table 1, Mask2Former Upper Bound performs much better than Mask2Former +CoPatch, it is wired and there has no explanation.\n\n(4) On Page 7, Line 347-350, the oracle performance is not well explained which makes the comparison and discussion very confusing. In addition, the results are not well organized to show a clear comparison.\n\n(5) On Page 7, Line 377, still on the oracle performance issue, the low performance is not mainly due to pretrained models. It seems to have nothing to do with the main experimental results."}, "questions": {"value": "No."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jAzSDZdQI1", "forum": "cIC5r0uv4n", "replyto": "cIC5r0uv4n", "signatures": ["ICLR.cc/2026/Conference/Submission1677/Reviewer_veLK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1677/Reviewer_veLK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830317198, "cdate": 1761830317198, "tmdate": 1762915853348, "mdate": 1762915853348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces COPATCH, a training-free framework for zero-shot referring image segmentation (RIS) that enhances spatial grounding by (i) augmenting CLIP’s local text features with context tokens and (ii) extracting patch-level image features from an intermediate visual layer to build a context-aware spatial map (CoMap). The method clusters high-similarity patches and uses the clusters to re-rank candidate masks from a pre-trained segmenter, then applies a spatial coherence filter when explicit positional cues are present. The authors argue that CLIP is not intrinsically “spatially blind”; rather, spatial information exists in intermediate representations and can be surfaced without the need for gradient or attention maps. Experimentally, COPATCH achieves strong zero-shot mIoU across RefCOCO/+/g and PhraseCut, often surpassing prior zero-shot and even weakly/pseudo-supervised baselines; top-1 improvements range roughly +2–7 mIoU, with much larger margins when reporting top-3 oracle metrics. The approach is hyperparameter-light (exit layer, threshold, and spatial coherence weight) and is claimed to require only a single forward pass for the map. Qualitative examples demonstrate better localization than Grad-CAM-based pipelines, and ablation analysis attributes the gains to both context-token text features and CoMap-guided candidate selection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Leveraging intermediate CLIP patch embeddings, along with lightweight clustering, to recover spatial cues is elegant and avoids fine-tuning or the use of expensive gradient maps. The paper provides evidence (in the form of layerwise similarity trends and latency comparisons) that this approach is both practical and efficient. \n- Results on RefCOCO/+/g and PhraseCut demonstrate state-of-the-art zero-shot performance across multiple CLIP/DFN backbones. Ablations isolate the contributions of context tokens (CT), top-candidate (TC) re-ranking via CoMap, and spatial coherence (SC). \n- Visualizations demonstrate a sharper focus on the referred object than Grad-CAM-based IteRPrimE; the spatial/non-spatial subset analysis strengthens the claim that both spatial grounding and semantics improve."}, "weaknesses": {"value": "- Many baselines utilize SAM or FreeSOLO proposals, whereas COPATCH employs Mask2Former and also reports a Mask2Former upper bound. Without re-running baselines under the same proposal pool, some gains may reflect differences in candidate quality rather than scoring alone. A stronger fairness control would standardize the mask generator across methods. \n- The paper frequently highlights considerable “Top-3” mIoU improvements, which reflect whether the correct mask appears among the top-k, not whether the method selects it. Unless coupled with an interaction or an automatic picking rule, these numbers are diagnostic rather than directly actionable. \n- The negation of patch embeddings to fix an “inversion artifact,” the clustering threshold δ, and selection logic (including negative text features + SC) are plausible but somewhat ad-hoc; theoretical motivation or broader sensitivity analysis (beyond the 10% val tuning on RefCOCOg) would improve confidence in robustness. \n- The extraction of “primary noun + context tokens” hinges on chunking/parsing rules and English expressions; the paper does not show multilingual or out-of-grammar robustness, and it is unclear how failures in chunking affect CoMap and mask scoring. (Appendix references and tool citations suggest rule-based tokenization choices, but details are light in the main text.)"}, "questions": {"value": "- Can the authors re-run the strongest prior zero-shot baselines using the same Mask2Former proposal set to isolate scoring improvements from proposal quality? If not, can they at least show COPATCH scoring on SAM proposals to bridge the comparison? \n- How sensitive is the context-token extraction to parsing errors or non-English phrasing (e.g., multilingual RefCOCO variants)? Are there any failure analyses that show when CT helps versus when it hurts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hQ9Lihukx0", "forum": "cIC5r0uv4n", "replyto": "cIC5r0uv4n", "signatures": ["ICLR.cc/2026/Conference/Submission1677/Reviewer_MWds"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1677/Reviewer_MWds"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986358834, "cdate": 1761986358834, "tmdate": 1762915853205, "mdate": 1762915853205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}