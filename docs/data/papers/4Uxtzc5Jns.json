{"id": "4Uxtzc5Jns", "number": 891, "cdate": 1756822085250, "mdate": 1759898237271, "content": {"title": "G-KV:  Decoding-Time  KV Cache Eviction  with Global Attention", "abstract": "Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. Experiments on AMC 23 and AIME 24 show G-KV achieves SOTA performance, with a 19.15\\% improvement in pass@1 under a budget of 256 tokens. The code of this paper is available on:https://anonymous.4open.science/r/G-KV-B3C0 .", "tldr": "", "keywords": ["KV Cache Compression; Large Language Models; Reinforcement Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4e33989f5c206fc038a5097b128bc0933d2d0aa0.pdf", "supplementary_material": "/attachment/48454fd95b3a373738365dc4a3ea63760f72877b.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces G-KV, a decoding-time KV Cache eviction method designed to address the computational and memory bottlenecks that large language models face during long-sequence reasoning tasks. The authors argue that existing methods, which often rely on local attention scores to determine which tokens to evict, fail to capture the long-term importance of tokens. To solve this, G-KV introduces a \"global score\" that combines the current local attention score with historical scores, allowing for a more accurate assessment of a token's long-term value. Furthermore, the paper explores post-training techniques, including reinforcement learning and distillation, to better adapt the model to a compressed KV Cache environment. Experiments on the AMC and AIME mathematical reasoning benchmarks demonstrate that G-KV achieves significant performance improvements over existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core motivation—that local attention is insufficient for capturing long-term token importance—is both intuitive and critical. The experiment in Section 4 (Figure 1), which convincingly demonstrates that the set of attended tokens shifts across different time windows, provides strong empirical support for the proposed “global score.”\n\n2. I appreciate the paper’s writing style, especially the clarity of its figures and the *Observations* presented throughout, which make the motivation and overall argument easy to follow."}, "weaknesses": {"value": "1. Limited Benchmark Coverage: The evaluation is primarily focused on mathematical reasoning tasks. While this effectively demonstrates the model's capabilities in long chain-of-thought scenarios, it may not fully prove the generalizability of the G-KV method to other types of long-text tasks. For instance, many state-of-the-art methods like SnapKV are also tested on benchmarks such as **LongBench** (for comprehensive long-text understanding) and **Needle-in-a-Haystack** (for long-range information retrieval). Including experiments on these more general long-context benchmarks would strengthen the paper's conclusions.\n\n2. Insufficient Analysis of Method Overhead: The G-KV method introduces a \"global score,\" which requires storing historical scores. Although the paper mentions in Appendix G that this overhead is negligible, it does not provide detailed empirical data to support this claim. Does calculating the global score introduce additional computational latency at each compression step? It would be beneficial for the authors to provide more specific experimental data, such as: **(1) What is the exact increase in memory (VRAM) usage for G-KV compared to methods that only use a local score? (2) On the same hardware and with the same batch size, what is the time cost of the G-KV algorithm itself (i.e., the scoring and sorting process)?** This would help readers more fully assess the method's efficiency."}, "questions": {"value": "1. Regarding the benchmarks: Have you considered evaluating G-KV on more general long-context benchmarks like LongBench or Needle-in-a-Haystack? This would help validate your method's effectiveness on tasks beyond mathematical reasoning.\n\n2. Regarding efficiency overhead: Could you provide more detailed data on the computational and memory overhead introduced by the global score calculation? Specifically, what is the added latency and extra memory consumption per compression step compared to a local-score-only method?\n\n3. Regarding the hyperparameter $\\alpha$: Figure 4 shows that the method's performance is quite sensitive to the decay factor $\\alpha$. For new models or tasks, do you have any recommendations or heuristics for setting this hyperparameter, other than extensive experimental search?\n\n4. Regarding token distribution: The finding in Figure 5 is very interesting—it shows G-KV retains tokens more uniformly across the entire sequence, whereas other methods are biased toward the end. Could you elaborate on why this uniform distribution is beneficial? Does it suggest that G-KV is better at preserving key information from the initial prompt, thus preventing context loss during long-range generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZH9gMGEM2z", "forum": "4Uxtzc5Jns", "replyto": "4Uxtzc5Jns", "signatures": ["ICLR.cc/2026/Conference/Submission891/Reviewer_tGhE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission891/Reviewer_tGhE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760664307731, "cdate": 1760664307731, "tmdate": 1762915636714, "mdate": 1762915636714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a KV cache selection strategy that comprehensively considering both local and historical attention contexts to enhances accuracy after cache compression. Furthermore, they introduce an additional post-training optimization step designed to adapt the model to the compressed KV Cache, which subsequently yields further performance improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-structured and easy to follow.\n- The improvement achieved through the post-training optimization is noteworthy."}, "weaknesses": {"value": "- The experimental results are unstable and require validation on a broader range of models, particularly larger-scale models (e.g., 32B and 70B parameters).\n    * Specifically, while the performance gain for the DeepSeek-R1-Distill-Qwen-7B model in Table 1 appears acceptable, the improvements for the DeepSeek-R1-Distill-Llama-8B model in Table 2 are very inconsistent.\n- The experimental results indicate that significant performance improvements are only achieved under very low cache budgets(like 256), which are likely impractical or unusable in real-world scenarios. The improvement is marginal in large budget scenarios.\n- Lack of Novelty.\n- The post-training optimization appears largely disconnected from the proposed global score mechanism, suggesting an ad hoc addition. \n    - Fine-tuning a model to adapt to the sparsity inherent in cache compression is a general optimization technique and is not specifically tied to the authors' KV cache compression algorithm.\n- The proposed method will likely impact the Time-to-First-Token(TTFT) performance. The authors need to include an experimental analysis of the TTFT overhead."}, "questions": {"value": "- Performance and concern: In Table 4, the results show that G-KV (R-KV w/ global score ) achieves some performance gain over plain R-KV. Since the consideration of a \"global score\" is expected to introduce additional computational overhead, could the authors explain why G-KV still maintains an advantage?\n- Data Inconsistency between Tables: The data presented in Table 1 and Table 3 does not align. Specifically, the \"Untrained\" baseline data in Table 3 does not match the \"G-KV\" data in Table 1. An explanation for this discrepancy is required."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RRPYKTWqW8", "forum": "4Uxtzc5Jns", "replyto": "4Uxtzc5Jns", "signatures": ["ICLR.cc/2026/Conference/Submission891/Reviewer_Uc4V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission891/Reviewer_Uc4V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700627740, "cdate": 1761700627740, "tmdate": 1762915636573, "mdate": 1762915636573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "G-KV proposes a decoding-time KV-eviction rule that forms a per-token global score by combining a normalized local window score with an attenuated historical score (via a hard max), plus an optional RL fine-tuning objective; evaluation is largely math-reasoning with 7B/8B models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The proposed global scoring mechanism is intuitive, which improves over R-KV at 256 tokens."}, "weaknesses": {"value": "**Q1.**  The related-work section names several 2025 SOTA methods (R-KV, CAKE, KVzip) but they are not used as baselines in the main tables; RocketKV and ShadowKV from ICML’25, Raas ACL’25 are ignored for baseline comparison.   \n\n**Q2.** Novelty is incremental relative to temporal/union-aware and redundancy-aware methods.\n The global score (Eq. 3) is max(α·F_{t−1}, normalized S_t). a form of temporal accumulation via attenuation and a hard max. CAKE explicitly models temporal shifts and layer preferences; R-KV adds redundancy scores to avoid keeping near-duplicates; ShadowKV/KVzip reconstruct/repurpose KV context. G-KV’s “historical-local max” feels like a special case of these broader families and needs a stronger differentiation (e.g., theory or empirical wins against them).\n\n**Q3.**  Authors state the RL objective “simplifies GRPO,” but the description aligns with REINFORCE w/ baseline; also, arguing the online setup “eliminates the need for clipping” is not supported—PPO/GRPO-style stability typically relies on clipping/trust-region constraints. Please justify with analysis or ablations. In general, I am not sure if, in practice, these RL work and deliver promising results.\n\n\n**Q4.** Heuristic design issues:\n\nThe hard max in Eq. (3) may overreact to outliers; compare to (i) EMA / weighted average, (ii) max-pool w/ temperature, (iii) top-k smoothing. No ablation is shown.\n\n\nSensitivity to α, w (window), and s (interval) is missing; these hyper-params directly govern stability and recall. (Authors define them but don’t study sensitivity.)\n\n\n**Q5.** Benchmarks are math-centric (AMC-23, AIME-24) with distilled 7B/8B models; The following ones are ignored GSM8K, MATH-500, CSQA, LiveCodeBench, and a long-context retrieval suite (LongBench/BabiLong).\n\n\n**Q6.** Authors state decoding time is ~40% of Full-KV and ~90% memory reduction at 16k (appendices), but no system-level comparisons vs optimized kernels (e.g., FlashAttention-3 baselines in SeerAttention-R, or ShadowKV’s throughput).\n\n**Q7.** Missing ablations/analyses \n\nA) Hard-max design must be validated against smoother alternatives.\n Add an ablation that replaces the hard max with: (a) exponential moving average (EMA) of local and historical scores; (b) a convex combination with a learned or tuned mixing weight; and (c) a temperatured max/LogSumExp. For each, report pass@1, average KV-retention, and variance of retention across steps to assess robustness to attention spikes.\n\nB) Aslo, the author should run structured sweeps: window size (w from {64, 128, 256}), compression interval (s from {8, 16, 32}), attenuation (α from {0.8, 0.9, 0.95, 0.99}), and budgets (b from {128, 256, 512, 1024, 2048}). Plot accuracy vs. retention and accuracy vs. decode-time to identify stable operating regions.\n\nC) Head-wise analysis (are some heads persistently favored/evicted?) to contrast with HeadKV\n\n\n**Typos:**\n\n“PREILIMINARY” to  “PRELIMINARY”.\n\n (Eq. 2): The summation bounds (k=0 to w) imply w+1 elements, but the division is by w. This should be clarified (e.g., k=0 to w-1)."}, "questions": {"value": "I already indicated them in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pDUOgG0gTD", "forum": "4Uxtzc5Jns", "replyto": "4Uxtzc5Jns", "signatures": ["ICLR.cc/2026/Conference/Submission891/Reviewer_h1av"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission891/Reviewer_h1av"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930082261, "cdate": 1761930082261, "tmdate": 1762915636444, "mdate": 1762915636444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes G-KV, a decoding-time KV cache eviction method that introduces a global scoring mechanism combining historical and local attention scores to better preserve long-term token importance. It further enhances performance via post-training techniques—reinforcement learning with sparse attention masks and knowledge distillation. The method is evaluated on challenging mathematical reasoning benchmarks (AMC-23, AIME-24) using strong reasoning LLMs, showing significant gains over prior SOTA, especially under tight KV cache budgets (e.g., +19.15% pass@1 at 256 tokens).\n\nThe work is well-motivated, empirically solid, and addresses a critical bottleneck in long-context reasoning LLMs. The global scoring idea is simple yet effective, and the integration with existing methods (e.g., R-KV) demonstrates strong generalizability. The training-aware extensions (RL-Sparse, Distill) are thoughtfully designed to close the train-inference gap.\nHowever, while the empirical results are compelling, the technical novelty is incremental, and several methodological and evaluation concerns limit the strength of the contribution for a top-tier conferences."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Strong Empirical Results:**\n  - Clear and consistent improvements over strong baselines across multiple models (Qwen-7B, Llama-8B), datasets (AMC, AIME), and KV budgets.\n  - Gains are especially pronounced under low budgets (256–512 tokens), which are practically relevant for deployment.\n\n**Insightful Motivation via Empirical Observation:**\n  - Figure 1 provides compelling evidence that token importance is non-stationary across decoding windows—justifying the need for global scoring.\n  - The analysis of token retention distribution (Figure 5) convincingly shows that G-KV preserves more diverse context than local methods.\n\n**Practical and Modular Design:**\n  - The global score is a drop-in replacement for local scores in existing eviction frameworks (H2O, SnapKV, R-KV).\n  - Minimal computational overhead (confirmed by throughput results in Table 4).\n\n**Comprehensive Evaluation:**\n  - Includes efficiency metrics (throughput, memory, decoding time), ablation on α and λ, cross-model validation, and qualitative case studies (Appendix J).\n  - Post-training methods (RL-Sparse, Distill) are well-motivated and show meaningful gains.\n\n**Reproducibility:**\n  - Code, distilled data, and environment configs are provided."}, "weaknesses": {"value": "**Limited Technical Novelty:**\n   - The global score (Eq. 3) is essentially an exponentially weighted moving average (EWMA) of normalized attention scores—a well-known technique in online learning and signal processing. While effective, it lacks deep algorithmic innovation.\n  - The core idea resembles “heavy-hitter” tracking with decay, similar in spirit to H2O but with historical memory.\n\n**Evaluation Scope is Narrow:**\n  - Experiments are restricted to mathematical reasoning on two datasets. No evaluation on general QA, coding, or open-ended generation.\n  - All models are distilled reasoning models from DeepSeek-R1. Performance on standard LLMs (e.g., Llama-3, Qwen2) or non-reasoning tasks is unverified.\n  - No comparison to non-eviction compression methods (e.g., quantization, low-rank) that may offer better trade-offs.\n\n**Hyperparameter Sensitivity:**\n  - Performance heavily depends on α (decay) and λ (redundancy weight). While tuned, the paper doesn’t provide robustness analysis or automatic tuning strategies.\n - The optimal α ≈ 0.8–1.0 suggests historical scores dominate—raising questions about the marginal utility of local scores.\n\n**Ambiguity in Training Protocol:**\n  - RL-Sparse uses a sparse attention mask during training, but it’s unclear how this interacts with RoPE (rotary embeddings), which assumes full positional context. This could introduce positional bias.\n  - Distillation uses teacher outputs from full KV, but student trains with sparse attention—a distributional mismatch not fully addressed.\n\n**Claimed SOTA May Be Overstated:**\n  - The 19.15% improvement is vs. R-KV under 256 tokens—but R-KV itself may not be the strongest baseline (e.g., CAKE, LightThinker are mentioned but not compared in main results).\n  - No comparison to recent methods like StreamingLLM or Infini-Attention that handle long contexts differently."}, "questions": {"value": "- How does G-KV perform on non-reasoning tasks (e.g., narrative continuation, summarization) or with standard LLMs (e.g., Llama-3-8B) without reasoning distillation?\n\n- Besides math related task, what about those extremely long context reasoning task in coding domain like SWE bench? Besides, could you provide a computing and memory cost for different context length such as 4K, 8K, 16K, 32K, 64K, etc?\n\n- Since G-KV evicts early tokens, how does the model handle positional information for retained early tokens under RoPE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lHe9BZ8DqE", "forum": "4Uxtzc5Jns", "replyto": "4Uxtzc5Jns", "signatures": ["ICLR.cc/2026/Conference/Submission891/Reviewer_ziNN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission891/Reviewer_ziNN"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762259394332, "cdate": 1762259394332, "tmdate": 1762915636302, "mdate": 1762915636302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}