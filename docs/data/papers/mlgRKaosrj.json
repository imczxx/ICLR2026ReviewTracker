{"id": "mlgRKaosrj", "number": 15188, "cdate": 1758248796644, "mdate": 1759897322586, "content": {"title": "Rethinking Scale: How Multi-Agent Collaboration Enables Smaller Models to Rival GPT-4 in Video Understanding", "abstract": "The rapid development of large language models (LLMs) has brought new perspectives to the field of video understanding. However, existing methods often rely on large-scale proprietary models, such as GPT-4, to achieve competitive performance. This paper challenges the notion that scale is the primary driver of capability by introducing RIVAL, a framework demonstrating how multi-agent collaboration enables smaller open-source models (72B or fewer) to rival their large-scale counterparts. RIVAL consists of two key components: a Multi-stage React Planner (MSRP) for structured stepwise reasoning and Multi-agent Debate Refinement (MADR) for collaborative answer generation. MSRP enhances instruction-following through precise control, while MADR improves answer quality via multi-perspective debate. Using a 72B model, our framework sets a new state-of-the-art on the EgoSchema subset with 66.8\\% accuracy, surpassing prior GPT-4 based methods by 6.6\\%. Furthermore, we demonstrate that even smaller open-source models (0.6B to 32B) across the Qwen 2.5 and 3 series achieve competitive performance with RIVAL. We also demonstrate competitive performance on the Next QA benchmark. Highlighting its efficiency, RIVAL can process over 28 hours of continuous video input using limited computational resources.", "tldr": "", "keywords": ["Video Understanding", "Large Language Model", "Agent"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ef3ab8a0d6ece9fc4c5030ecd4ead6aa79b44a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes U-CSA—an unsupervised cross-modal semantic anchoring framework to match aerial imagery with vector maps. Instead of aligning image↔image, U-CSA first asks a multimodal LLM (Qwen2.5-VL) to produce structured “semantic anchors” (JSON over 11 attributes + a ≤40-word summary) for each image–map pair; then (i) a dual-branch visual encoder is trained with anchored contrastive learning against the text anchors; and (ii) an adversarial matching head with a prototype library refines the decision boundary. The authors also introduce MSTcons, a 18,907-pair benchmark built from WHU (Christchurch) and Inria (Austin, Chicago, Kitsap, Vienna, West Tyrol), with 256×256 tiles and explicit splits. On MSTcons, U-CSA beats unsupervised SAM-MCD and several adapted change-detection baselines in ROC-AUC/F1, with ablations supporting the contribution of anchors and prototypes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Originality. A clean combination of staged ReAct planning with adversarial multi-agent debate targeted at long-video QA; explicit tool APIs (stop, add/delete by frame ID, CLIP text query) make the control flow concrete.  ￼  ￼\nQuality. Strong headline numbers on EgoSchema and Next-QA (incl. per-subset breakdowns) and a long-video stress test (28h). Comparisons include GPT-4/VideoAgent/LLoVi families and same-model re-implementations.  ￼  ￼  ￼\nClarity. The pipeline and roles are well illustrated; termination conditions and planner/debate prompts are spelled out; implementation details (captioner/CLIP/serving) are given.  ￼  ￼  ￼\nSignificance. If claims hold under controlled settings, the result that smaller open LMs with orchestration can match/beat prior GPT-4-based agents is practically meaningful for privacy/cost-sensitive deployments."}, "weaknesses": {"value": "Potential option-conditioning bias. Frame retrieval uses both the question and the answer options (Ia = Top-k Sim(I, A)), which risks label-peeking and unfairly advantaging multiple-choice setups. Please add ablations that retrieve only from Q (no options), or retrieve before reading options, and report accuracy deltas.  \nFair-comparison controls. Several baselines differ in LLM scale, context, and tools. While you re-implement VideoAgent with Qwen-2.5 (Appendix C), the tables still mix methods with non-comparable compute/budgets. Please provide same-LLM apples-to-apples runs (VideoAgent/LLoVi/RIVAL all on Qwen-3-32B with matched token budgets, retrieval limits, and tool calls) and report token & wall-clock costs.  \nAblation depth on MSRP/MADR. The contribution attribution is under-specified. Add: MSRP→single-stage ReAct; MSRP without enforced state transitions; MADR→self-consistency / majority-vote; debate with no tools; and variance across #rounds/threshold α. Report accuracy, calls/round, tokens, and failures. \nReliance on captioner/CLIP & leakage audit. Results hinge on LaViLa/CogAgent (captioner) and EVA-CLIP-8B+ (retriever). You remove EgoSchema overlaps in LaViLa, which is good, but please add captioner/CLIP swaps and a leakage audit across both benchmarks (e.g., retrieve-then-blind the captioner to options; test different CLIP checkpoints)."}, "questions": {"value": "No-options retrieval? What happens if frame retrieval is conditioned only on Q (not options), or performed before seeing the options? Please quantify.\nBudget & efficiency. Can you report average #tool calls / debate rounds / prompt tokens / latency per question, and compare to VideoAgent/LLoVi under matched settings? \nAblation breadth. Could you add MSRP/MADR ablations described above and release prompt templates and judge criteria used for win/consensus decisions? \nCaptioner/CLIP sensitivity. How sensitive are results to swapping LaViLa↔CogAgent (cross-benchmark) and EVA-CLIP↔other CLIPs? Any drop-in if the captioner context is limited? \n8-hour scenario. How do you partition the 28h stream internally (sliding windows? chunked debates)? Please report failure modes and variance for the long-video setting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WiMjyordEL", "forum": "mlgRKaosrj", "replyto": "mlgRKaosrj", "signatures": ["ICLR.cc/2026/Conference/Submission15188/Reviewer_LuFc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15188/Reviewer_LuFc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707833373, "cdate": 1761707833373, "tmdate": 1762925493622, "mdate": 1762925493622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RIVAL, a training-free framework showing that *multi-agent collaboration* can enable smaller open-source LLMs (≤72B) to approach or surpass GPT-4–based systems on long-video understanding. RIVAL has two core components:\n\n- Multi-Stage ReAct Planner (MSRP): decomposes reasoning into *OBSERVE → THINK → ACT* stages, with explicit state transitions and a fixed toolset (stop search, delete/add by frame ID, add by text via CLIP), iterating until a quality threshold is met.  \n- Multi-Agent Debate Refinement (MADR): after MSRP forms an initial answer, affirmative and opposing agents debate once per turn (with one tool call each), and a judge selects or revises the final answer; debate stops on agreement, a win, or max rounds.\n\nOn EgoSchema, RIVAL with Qwen-2.5-72B reaches 66.8% on the subset (SOTA in their comparison; +6.6 over GPT-4 baselines) and 56.4% on the full set. With Qwen-3-32B, it reaches 65.0/57.2 (subset/full). On NExT-QA, RIVAL attains 74.4% (72B) and 73.2% (32B) on validation and 66.5% / 63.7% on ATP-Hard, surpassing prior GPT-4–based agent methods in the reported comparisons. The system also processes a 28-hour concatenated long video under limited compute (≤15k token context; dual A100s for 72B), arguing for privacy-preserving, resource-constrained deployment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Shows that *careful orchestration* (MSRP) plus *adversarial verification* (MADR) can reduce dependence on very large proprietary LLMs for long-video QA.\n- Explicit stage transitions, fixed tool APIs, and stopping rules make the agent loop auditable and easier to reproduce conceptually.\n- SOTA subset performance on EgoSchema (66.8% with 72B), and NExT-QA gains over VideoAgent/LLoVi in the reported tables.\n- Maintains accuracy on a 28-hour concatenated video where a single-agent baseline degrades substantially.\n- Operates within a 15k token window and on commodity accelerators (72B split over 2×A100), aligning with realistic deployment; privacy angle is well-motivated.\n- Improves upon GPT-4–centric VideoAgent and text-only aggregation like LLoVi, while aligning with the trend toward streaming arbitrary-length video."}, "weaknesses": {"value": "- Tables focus on GPT-4 baselines circa 2024; it would help to benchmark against the most recent proprietary/open VLMs that handle arbitrary-length streams (e.g., streaming VLLMs) to solidify the “rivals GPT-4” claim.\n- The pipeline’s quality hinges on CLIP retrieval and the image/video captioners; retrieval bias or caption hallucinations could mislead the debate, and ablations on retrieval quality (e.g., different CLIP backbones, top-k) are limited in the main text.\n- MSRP/MADR rely on an internal evaluator score (60/40 criteria) and a threshold to trigger debate; while there is some analysis, deeper calibration/robustness checks (e.g., agreement with human judgments, sensitivity to α) would strengthen soundness.\n- Claims of efficiency would benefit from a cost breakdown: number of tool calls, frames read, average tokens per step, wall-clock latency vs. baselines. Current hardware details are provided, but *end-to-end* throughput comparisons are sparse.\n- Source code is not yet released (pending security review); although pseudocode and prompts are promised, this limits verification and adoption pre-camera-ready.\n- Results are strong on EgoSchema/NExT-QA; adding diverse long-video tasks (e.g., instruction following, temporal localization) would clarify generality."}, "questions": {"value": "1. How sensitive are results to the accuracy/completeness weights (60/40) and the debate threshold α? Can you report Kendall/ Spearman correlation of evaluator scores with correctness, and success rates per score bin?\n2. You set 3 rounds based on a peak at 66.8%. What is the marginal accuracy gain vs. added latency per round on both datasets? Provide a Pareto curve (accuracy vs. seconds/$$).\n3. How do different CLIP variants and top-k selections affect accuracy and runtime? Can you quantify failure modes where retrieval misses key evidence?  \n4. For EgoSchema you use LaViLa with overlap removal; for NExT-QA, CogAgent. Could you provide cross-captioner results and any leakage checks? \n5. Can you add a head-to-head vs. recent streaming long-video VLLMs or updated GPT-4-class systems to contextualize “rival GPT-4” beyond 2024-era baselines?\n6. Please report average #tool calls, frames retrieved, tokens consumed, and end-to-end latency per query, and contrast with VideoAgent and LLoVi at similar compute."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FBC5Xn7hs8", "forum": "mlgRKaosrj", "replyto": "mlgRKaosrj", "signatures": ["ICLR.cc/2026/Conference/Submission15188/Reviewer_jHSX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15188/Reviewer_jHSX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726001972, "cdate": 1761726001972, "tmdate": 1762925493250, "mdate": 1762925493250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RIVAL, a *training-free* agentic framework for long-video question answering that aims to “rethink scale”: instead of relying on very large proprietary models, it orchestrates smaller open LLMs via two modules:\n\n-Multi-Stage ReAct Planner (MSRP): enforces explicit OBSERVE → THINK → ACT stages, produces a structured tool-usage plan (Stop Searching; Delete/Add by Frame ID; Add by Text via CLIP), and stops when a score threshold or max steps is reached. This reduces reasoning/action drift and keeps context within ~15k tokens.\n\n-Multi-Agent Debate Refinement (MADR): after an initial answer, *affirmative* and *opposition* agents debate with limited tool calls; a judge either declares agreement, a winner, or halts at max rounds.\n\nOn EgoSchema, RIVAL with Qwen-2.5-72B/3-32B reports 66.8/65.0 on the subset and 56.4/57.2 on the full set, surpassing GPT-4–based baselines in their table. On NExT-QA, it reaches 74.4/73.2 on val and 66.5/63.7 on ATP-Hard.\nUnder an ≈28-hour concatenated-video stress test, RIVAL degrades less than VideoAgent (e.g., on a 1.5B model: 33.8 vs 23.4).\n\nCompared to VideoAgent (LLM-tool agent with proprietary backends) and LLoVi (dense captioning + LLM reasoning), RIVAL argues better *privacy* (local open models) and *resource* efficiency; against streaming VLMs it offers a systems alternative grounded in retrieval + agent debate."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+Competitive long-video QA with open models under a 15k token budget and modest GPUs.\n\n+Enforced OBSERVE/THINK/ACT stages + fixed tools and stop criteria reduce “free-form” LLM drift and make loops auditable.\n\n+MADR’s affirmative/opposition/judge structure with Frame-ID/Text queries is a neat twist on multi-agent debate for video evidence-seeking.\n\n+Consistent gains over VideoAgent from 0.6B–72B (incl. large margins on EgoSchema subset) and solid NExT-QA results.\n\n+Smaller degradation on the 28-hour test relative to VideoAgent supports the scalability story."}, "weaknesses": {"value": "-Results would be more conclusive with head-to-head against *arbitrary-length* streaming VLMs (e.g., VideoStreaming, StreamingVLM) at comparable compute, not only GPT-4–centric agents. \n\n-The end-to-end quality hinges on EVA-CLIP-8B+ retrieval and LaViLa/CogAgent captioners; failure modes (missed key frames, caption hallucination) are not deeply dissected.\n\n-The evaluator’s 60/40 criteria and α=5 gate are plausible, but more human-agreement/calibration plots (e.g., ROC/AUC vs. correctness) and sensitivity to α would strengthen soundness.\n\n-Hardware is stated, but per-query metrics (#tool calls, frames retrieved, tokens, wall-clock) and *compute-normalized* comparisons vs. VideoAgent/LLoVi are sparse.\n\n-The method is validated on EgoSchema/NExT-QA; extensions to open-ended grounding, temporal localization, or instruction following would clarify generality.  \n\n-The paper reads reproducibly at the concept level, but full code/prompts would be needed for wider adoption; timelines aren’t specified."}, "questions": {"value": "1. Could you report accuracy vs. *seconds/tokens/tool-calls* per query (and per MADR round), and compare against VideoAgent/LLoVi at matched budgets?\n2. How stable are results under different α thresholds or 60/40 weight splits? Any human-agreement stats (e.g., Kendall τ between evaluator scores and correctness)?\n3. What is the impact of swapping EVA-CLIP-8B+ for a lighter/heavier CLIP, or changing Top-k and similarity thresholds? Fail-case taxonomy?\n4. Cross-captioner results (LaViLa ↔ CogAgent) on both datasets, plus leakage checks for LaViLa (you mention overlap removal).\n5. Can you add direct comparisons to VideoStreaming/StreamingVLM (or other 2025 long-context VLMs) to contextualize where RIVAL wins/loses?\n6. Any early results on open-ended QA or temporal localization tasks to probe beyond multi-choice settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f1ZpO1a3g4", "forum": "mlgRKaosrj", "replyto": "mlgRKaosrj", "signatures": ["ICLR.cc/2026/Conference/Submission15188/Reviewer_bSpo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15188/Reviewer_bSpo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729374316, "cdate": 1761729374316, "tmdate": 1762925492487, "mdate": 1762925492487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RIVAL, a video understanding framework built on small open-source LLMs (≤72B), aiming to rival GPT-4-level proprietary methods. RIVAL consists of (1) MSRP (Multi-stage ReAct Planner) for structured reasoning with explicit sub-states (OBSERVE → THINK → ACT) and tool-calling, and (2) MADR (Multi-Agent Debate Refinement) for adversarial multi-role answer refinement. The system retrieves key frames via CLIP and performs iterative information augmentation plus debate-based correction. Experiments on EgoSchema and Next-QA show strong results, surpassing GPT-4 baselines on subsets, and showing robustness on extremely long (28h) concatenated video."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong empirical results. RIVAL achieves substantial gains over prior GPT-4-based VideoAgent/LLoVi on EgoSchema subset (+6.6%) and competitive Next-QA performance.\n\n2. Multi-agent debate refinement is effective and well-motivated. MADR empirically corrects initial errors and is demonstrated clearly with case study.\n\n3. Very long video case study is interesting. Handling 28h concatenated input with minimal degradation is a good stress test."}, "weaknesses": {"value": "1. Clarity & ablations missing. The current writing does not sufficiently quantify how much performance comes from CLIP retrieval, MSRP decomposition, and MADR debate individually. Ablation will greatly strengthen causal attribution.\n\n2. Significant engineering heuristics. Many parts of MSRP are manually structured and rely on prompt templates / tool definitions — unclear robustness to domain shift or tasks not fitting stepwise logic.\n\n3. Scalability beyond QA not validated. RIVAL is only evaluated on video QA benchmarks; unclear if this paradigm generalizes to open-ended summarization / event boundary detection / reasoning beyond MCQ.\n\n4. Some baselines may not be strictly comparable. For Next-QA, several older entries are pre-CLIP/2024-era; more recent strong open models could be added for fairness."}, "questions": {"value": "1. Can the authors include ablations isolating (a) no MSRP, (b) no MADR, (c) no CLIP key-frame retrieval, to quantify contribution of each component?\n2. Could the authors report results on free-form open-ended summarization tasks to illustrate generality beyond MCQ style QA?\n3. Given that the video is often reduced to textual descriptions, does RIVAL degrade on videos with non-linguistically describable cues (e.g., spatial geometry, implicit physics)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xnbQH25yra", "forum": "mlgRKaosrj", "replyto": "mlgRKaosrj", "signatures": ["ICLR.cc/2026/Conference/Submission15188/Reviewer_DJH9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15188/Reviewer_DJH9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991534922, "cdate": 1761991534922, "tmdate": 1762925491780, "mdate": 1762925491780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}