{"id": "TpL2nXanru", "number": 6196, "cdate": 1757957780168, "mdate": 1759897930497, "content": {"title": "AutoQVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization", "abstract": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model's quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce AutoQVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, AutoQVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model's VRAM while maintaining 98.9% of its original performance and achieving a 1.49$\\times$ speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released.", "tldr": "We present the first systematic analysis of quantization challenges in Vision-Language-Action (VLA) models, designing and implementing a comprehensive quantization framework specifically for this architecture.", "keywords": ["Vision-Language-Action Models", "Embodied AI", "Model Quantization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a7e69c62bff9a97b11c4bc18c420049aac58fb7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes AutoQVLA, a VLA quantization method that reduces memory requirements and inference latency of VLAs by employing channel-wise bit allocation and pruning which optimize for reduced degradation of action quality. The authors show that naive uniform-bit quantization methods used for LLMs underperforms when applied to VLAs since they do not take into account how errors compound across trajectories when the VLAs are deployed as policies. The authors propose a more sophisticated framework that first analyzes the action sensitivity of different channels within network layers and then runs a greedy demotion algorithm to iteratively demote channels from full precision to lower precision until a target bit budget is met. Experimental results with OpenVLA and OpenVLA-OFT policies in the LIBERO simulation benchmark demonstrate nearly 4x VRAM reduction and 1.5x faster inference speed, with only up to 1% absolute drop in average success rate, outperforming prior quantization methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* The paper is well motivated and generally easy to follow.\n* The paper achieves significant reduction in memory requirements (about 4x) and improvement in inference latency (about 1.5x) while preserving most of the accuracy of full-precision VLAs. These efficiency results surpass those of prior methods (SmoothQuant, OmniQuant, AWQ), and the degradation in policy performance is substantially lower. This suggests that the method can be practically useful for low-resource deployment of VLA policies.\n* There is good breadth in the experiments: The analyses cover weights-and-activations and only-weights quantization, layer- vs channel-wise quantization, temporal error accumulation across different quantization schemes, and 0-bit (\"pruning\") and uniform-bit quantization.\n* The analysis of quantization is performed on two VLA instances that vary substantially in their action generation schemes (OpenVLA which does vanilla autoregressive decoding of tokens and OpenVLA-OFT which does parallel decoding of continuous actions). Findings hold across both instances and overall support the authors' claims."}, "weaknesses": {"value": "Major:\n* The experiments are limited to one benchmark of robot tasks in simulation (LIBERO). Results in another simulator (e.g. RoboCasa or CALVIN) or real-world robotic manipulation tasks would further strengthen the paper.\n* The temporal error accumulation analysis in Section 4.3 and Figure 3 could benefit from greater detail. It is unclear under which evaluation setting the curves in the plot are computed (e.g. what is the task here, what is the policy being analyzed?). In addition, some discussion on how the differences in cumulative MSE affect the episode outcomes can be helpful (e.g. what do these differences mean in terms of success rate)?\n* Qualitative analysis of how the AutoQVLA-quantized VLA policies behave in comparison to the full-precision policies or the policies quantized using prior methods is missing and can further strengthen the paper. I.e., beyond success rate differences, how do the AutoQVLA policies actually behave when rolled out as policies, and how do they succeed/fail? Are there example rollouts that can illustrate why the other quantization approaches are less effective?\n* In Section 3.3.1, the authors write, \"In practice, we find that the rankings of channel sensitivities produced by $s_{l,c}^{(b)}$ and $S_{l,c}^{(b)}$ are highly consistent. This crucial finding allows us to leverage the computationally cheaper single-step metric $s_{l,c}^{(b)}$ for guiding the bit allocation process, while using the more comprehensive cumulative metric $S_{l,c}^{(b)}$ to validate that our approach successfully extrapolates to long-horizon performance.\" This finding is important, yet evidence supporting it is missing. Details on how this conclusion was made would be helpful.\n* There is lack of detail on the calibration set used for LIBERO and how it is curated.\n* It is unclear how many trials/random seeds are used for the evaluations (e.g. Table 1 and 2).\n\nMinor:\n* Figure 1(b): The separation of vision encoder, projector, language model, and action head is not very clear. For example, where does the language model section start exactly in this plot?\n* Figure 2 step 1: \"tagrt bit\" typo?\n* Table 4: \"The row (3)\" typo? Shouldn't it be \"The row (4)\" instead?"}, "questions": {"value": "See questions in Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "laOB2hErTb", "forum": "TpL2nXanru", "replyto": "TpL2nXanru", "signatures": ["ICLR.cc/2026/Conference/Submission6196/Reviewer_RtD1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6196/Reviewer_RtD1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952629533, "cdate": 1761952629533, "tmdate": 1762918538880, "mdate": 1762918538880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AutoQVLA, a novel quantization framework specifically designed for VLAs. The key insight is that naively applying uniform quantization methods from LLMs/MLLMs is suboptimal for action-driven models, as minor action deviations can compound into catastrophic failures in embodied control. This paper introduces a highly granular, channel-wise bit allocation strategy to quantize each channel to various bit-widths. Comprehensive experiments show that AutoQVLA maintains 98.9% of original performance while using only 29.2% of VRAM and achieving 1.49$\\times$ speedup compared with the original OpenVLA-OFT model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Conducted a detailed analysis of the sensitivity of parameters across different channels to action generation;\n2. Significant memory reduction and speedup with minimal performance loss make large VLA deployment feasible."}, "weaknesses": {"value": "All experiments are conducted on LIBERO benchmarks, where there is a lack of research on model quantization in real-world tasks."}, "questions": {"value": "Would the presence of greater noise in real-world observations affect the sensitivity of channel parameters to quantization errors and action generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t8nYzb0oNV", "forum": "TpL2nXanru", "replyto": "TpL2nXanru", "signatures": ["ICLR.cc/2026/Conference/Submission6196/Reviewer_2rUj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6196/Reviewer_2rUj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989837595, "cdate": 1761989837595, "tmdate": 1762918538313, "mdate": 1762918538313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AutoQVLA, a specialized framework for quantizing Vision-Language-Action (VLA) models. The framework addresses the limitations of traditional quantization methods that struggle to maintain output precision in VLA models. It proposes an action-space-based sensitivity quantization approach. The core of AutoQVLA lies in its fine-grained, per-channel bit allocation strategy, which measures each channel’s sensitivity to the final action output during quantization. Unlike conventional uniform-bit quantization methods, AutoQVLA introduces adaptive per-channel precision adjustment to optimize quantization efficiency while ensuring robustness in action generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper introduces a fine-grained approach to quantifying errors by isolating individual channels, allowing for a more precise evaluation of the error for each channel at different bit-widths, ensuring better control over the quantization process.\n2.Unlike traditional global bit allocation, the paper proposes a per-channel adaptive bit allocation strategy and employs a greedy search algorithm to optimize bit allocation. The algorithm dynamically adjusts the bit-width for each channel based on its sensitivity, effectively balancing performance and computational efficiency."}, "weaknesses": {"value": "1. The sensitivity evaluation and bit allocation are only conducted on a few VLA models in the paper. It remains unclear whether the same process needs to be applied to other VLA models, and if so, whether it will lead to significantly higher computational resource consumption. A more generalizable analysis across a broader range of VLA models such as UniVLA[1] would provide better insight into the scalability of the proposed method.\n2. Inadequate comparison with relevant baselines. Since the VLA architecture is structurally most similar to Vision-Language Models (VLMs), it is essential to compare against VLM-specific quantization methods, such as VLMQ [2] and Q-VLM [3], rather than unimodal LLM quantization approaches. A direct comparison with these VLM quantization baselines would provide a more meaningful assessment of the proposed method’s effectiveness in the multimodal setting.\n3. Limited Novelty. The proposed channel-wise mixed-precision quantization scheme lacks significant novelty. Similar channel-wise quantization strategies have already been explored in prior work, notably in GPTQ [4]. Moreover, the importance-aware quantization design is also conceptually aligned with discussions in GPTQ.\n\nMinor Weaknesses:\n\n4.The pipeline diagram could be clearer. While it outlines the workflow of the method, its current presentation may not be intuitive or detailed enough for readers to fully grasp the process. Improving the clarity and labels in the diagram would enhance understanding, especially when explaining the interactions between different steps in the quantization process.\n\n[1]Bu, Qingwen, et al. \"Univla: Learning to act anywhere with task-centric latent actions.\" arXiv preprint arXiv:2505.06111 (2025).\n\n[2]Xue, Yufei, et al. \"Vlmq: Efficient post-training quantization for large vision-language models via hessian augmentation.\" arXiv preprint arXiv:2508.03351 (2025).\n\n[3]Wang, Changyuan, et al. \"Q-vlm: Post-training quantization for large vision-language models.\" Advances in Neural Information Processing Systems 37 (2024): 114553-114573.\n\n[4]Frantar, Elias, et al. \"Gptq: Accurate post-training quantization for generative pre-trained transformers.\" arXiv preprint arXiv:2210.17323 (2022)."}, "questions": {"value": "Further questions:\n\n1. According to the analysis in SmoothQuant and OmniQuant, quantizing activations to 4 bits (A4) is significantly more challenging than quantizing weights to 4 bits (W4). For instance, OmniQuant reports approximately a 10% performance drop under A4 quantization (see Table 2 in OmniQuant [1]). In contrast, the method proposed in this paper appears to focus exclusively on weight quantization and does not introduce any specialized treatment for activation quantization. Surprisingly, the reported results show negligible performance degradation even under aggressive A4 quantization. This discrepancy raises serious concerns about the validity of the experimental claims. To ensure reproducibility and credibility, the authors should release both the training code and the quantized pretrained models, and provide a detailed ablation study addressing why their method seemingly avoids the well-documented pitfalls of low-bit activation quantization. Without such evidence, the reported results remain unconvincing.\n\n2. To the best of my knowledge, data types such as 2-bit and 4-bit are not natively supported in mainstream frameworks like PyTorch. As discussed by prior works like AWQ, achieving actual memory savings from low-bit quantization typically requires custom CUDA kernels design. It is therefore unclear whether the memory reduction reported in this paper reflects real hardware-level savings or is merely a theoretical estimate based on idealized bit-width calculations. \n\nMoreover, in the W4A4 setting described in the paper, activations and weights may be quantized to heterogeneous bit-widths (e.g., 2-bit, 8-bit), and the paper implies that they are first upcast or aligned to 4-bit before matrix multiplication. Such a process would inevitably introduce computational overhead and degrade inference speed. Yet the paper claims improved inference throughput—how was this speedup measured? Did the authors implement a custom CUDA kernel specifically optimized for mixed-precision W4A4 matrix multiplication? Without clarification on these implementation-level details and empirical profiling results (e.g., latency, memory usage on actual hardware), the efficiency claims lack substantiation.\n\n3. In your paper, you mention that the sensitivity is calculated through the error in the action space. Could you clarify whether this metric is strongly dependent on specific tasks or data distributions? When transitioning to new tasks or environments, is it necessary to recalibrate or adjust the way this metric is calculated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NqIq5RuiYg", "forum": "TpL2nXanru", "replyto": "TpL2nXanru", "signatures": ["ICLR.cc/2026/Conference/Submission6196/Reviewer_9cSM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6196/Reviewer_9cSM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990311746, "cdate": 1761990311746, "tmdate": 1762918537895, "mdate": 1762918537895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AutoQVLA, a novel action-centric quantization framework for embodied control. They allocate different bitwidths for different channels based on a per-channel action sensitivity. To overcome the computational challenge of computing the sensitivity metric for all channels and bit-widths, they first derive a linear approximation of the channel amplification factor using Taylor expansion and then do a fine-grained measurement. Then they propose a greedy algorithm to assign bit-widths to different channels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The algorithm defined in the paper is novel way of quantizing Video Language Action models\n2. They efficiently allocate the computation budget for identifying the sensitivity of different channels\n3. They empirically show that their algorithm achieves performance close to the full precision model, while reducing the VRAM required and  achieving speedup"}, "weaknesses": {"value": "1. The evaluation is focused on one class of models and a single benchmark; it is not clear how the method generalizes across models and benchmarks\n2. No detailed ablation studies highlighting the contribution of different parts of the method, like the calibration set sizes, gate ratio selection etc.,\n3. No theoretical analysis of the choices made in the paper"}, "questions": {"value": "Please see the weaknesses I mentioned."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vs3s9YosKJ", "forum": "TpL2nXanru", "replyto": "TpL2nXanru", "signatures": ["ICLR.cc/2026/Conference/Submission6196/Reviewer_kUNf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6196/Reviewer_kUNf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990685047, "cdate": 1761990685047, "tmdate": 1762918537168, "mdate": 1762918537168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}