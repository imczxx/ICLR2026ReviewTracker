{"id": "K42xtH1sqG", "number": 14433, "cdate": 1758235250729, "mdate": 1759897370350, "content": {"title": "Attacking Combinatorial Bandits: Beyond Bounded Rewards", "abstract": "Combinatorial Multi-Armed Bandits (CMABs) are a widely adopted tool to address online learning problems with a combinatorial nature. Adversarial attacks, on the other hand, represent a significant threat to machine learning algorithms, where a malicious entity intentionally manipulates data or feedback to deceive learning algorithms, undermining their performance and reliability. While CMABs and adversarial machine learning received extensive attention as distinct subjects, CMABs under adversarial attacks are still underinvestigated. We propose algorithms to attack CMABs, providing theoretical guarantees regarding success and cost in three different scenarios. Each scenario differs in the assumptions on the rewards. First, we study attacks when rewards are bounded and means are positive. Then, we consider two extensions in which rewards have unbounded support, distinguishing between positive and arbitrary means. For each scenario, we design two attack strategies. First, we assume that the attacker is omniscient, i.e., knows the problem instance, then we extend the attack to a more realistic setting where the learner and the attacker have the same knowledge of the problem. We show that our attack strategies are successful, i.e., the learner will select a target superarm for $T - o(T)$ times, except for some degenerate cases. We also show that in most settings the attack cost is sublinear in $T$. Finally, we validate our theoretical results via numerical experiments on synthetic instances.", "tldr": "", "keywords": ["online learning", "multi-armed bandits", "adversarial attacks"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cdd9ef1a1c469495a5450824f8be5586e5490a76.pdf", "supplementary_material": "/attachment/aa9a506a5fe74da966757a1ad502ce316d47b195.zip"}, "replies": [{"content": {"summary": {"value": "This paper designs attack policies for combinatorial bandits, considering three types of reward environments. For each environment, the paper identifies some concern cases where a sublinear attack is infeasible, and proposes attack policies for CUCB algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing of this paper is easy to follow."}, "weaknesses": {"value": "Overall, the reviewer leans towards a mild negative evaluation of this paper, with the following concerns:\n\n1. The most concerning issue of this paper is the novelty. The new parts of this paper are mainly Conditions 3.1, 3.2, and 3.3. However, from the reviewer’s perspective, these conditions can be straightforwardly derived from the CMAB’s standard structure without any technical challenges. If the authors are not agree with that, please add a discussion on the technical difficulties of deriving them.\n2. For the attack on Condition 3, the authors claim that a linear budget is unavoidable. How about set the $\\epsilon$ in Eq(3), as $\\frac{1}{T}$ or $\\frac{1}{\\log T}$?\n3. For the two attacking policies in Eq. (9) and Eq. (10), as in the realistic case, the attacker also does not know whether Condition 3.3 holds or not. How does it decide which attacking policy to pick?\n4. In all the budget analyses, only the orders in terms of $T$ are given, leaving the dependence on gaps unclear. Given that the bandit literature is on the theoretical side, it would be better to derive and present fine-grained analysis."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RaIfKwGxsJ", "forum": "K42xtH1sqG", "replyto": "K42xtH1sqG", "signatures": ["ICLR.cc/2026/Conference/Submission14433/Reviewer_gEy2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14433/Reviewer_gEy2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761268104462, "cdate": 1761268104462, "tmdate": 1762924839141, "mdate": 1762924839141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of attacking combinatorial bandits under different conditions: 1) bounded rewards with positive means, 2) unbounded rewards with positive means and 3) unbounded rewards with signed means. It identifies necessary and sufficient structural conditions for attacks to be successful. Attack algorithms are given for the oracle case where the attacker knows the means of all the base arms, and the realistic case where it has the same information as the learner. The algorithms are proved sound and validated empirically on toy examples."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper formulates and solves an interesting problem of attacking combinatorial bandits where the rewards can be unbounded and may be negative. I appreciate the careful division into multiple cases and their analysis.\n\nThe analysis shows that unless some structural conditions hold, the bandits are not attackable. In the case where they are attackable, specific attack algorithms are given and are analyzed. \n\nThe empirical results validate the correctness of the algorithms."}, "weaknesses": {"value": "Despite the above strengths, There are some problems with the specific claims and proofs in the paper. On the whole the paper needs careful reworking. Also the empirical results will be more impressive on less toy-like problems.  \n\nConfusing notations. \nAlgorithm 1. Clarify that Xs_t and As_t are vectors and Xs_t + As_t is a vector addition (not sets or numbers). \nBut then, below the Algorithm 1 you write As_t = \\sum_i Ai_t and C(T) = \\sum As_t. This does not make sense unless As_t is a number.\n\nC2 is convoluted. it is better stated as\"There exists a superarm S \\subset S* s.t. \\forall j \\in S*/S, \\mu_j =0.\"\nHowever, I think that this needs to be generalized to cover all negative cases as I show below. \n\nIn the proof of Proposition 3.3. the definition of S^{1,2} is not proper. I think you mean (S*\\S) \\bigcap {i | \\mu{i) > 0} =/= \\Phi, i.e., \nFor all S, S*\\S has at least one arm with non-zero mu. If this is the case, when the adversary sets all arms in S/S* to have zero means, S* would be the best arm and will be chosen.  I suggest that you add this explanation to the main text.\n\nHowever S^{1,2} is not the same as [ \\not C1 \\wedge \\not C2]. Suppose C1 and C2 are both false. In particular assume that there is no S where S subset S* or S* \\subset S.  Let S* = {a, b} S ={b,c} and mu(a)=0. The algorithm does not seem to work, since setting mu(c) to 0, would keep both S and S* to be optimal. \n\nOne way to fix this is to make C2 weaker. Perhaps: \"There exists a superarm S, where S*/ S is non-empty and \\forall j \\in S*/S, \\mu_j =0.\" , i.e., S need not be a strict subset of S*. \n\nIn this case S^{1,2} seems equal to [\\not C1 \\wedge \\not C2] and the proofs seem to go through. \n\nEquation (8) I suspect  that A_{i,t} should be set to -(\\hat{\\mu}_i(t)+\\beta(N_i(t)+\\epsilon) if i \\in S_t\\S*. \n\nThe reason is that \\hat{\\mu} might have statistical error and the real \\mu may be higher by \\beta. Equation 9 does this for example. \n\nPage 14. Proof of Proposition 3.6. The same equation is repeated twice. It appears that epsilon should come in the first part of the repeated equation and not on the second part. \n\nPage 15, Proof of Proposition 4.2. < sign in Eq 23 is turned into >= in Eq 24. Why?\nIt appears that equation is squared on both sides and summed, but if so \\epsilon should have turned into epsilon squared. This step should be explained. As I said earlier, I think that Equation (8) is wrong. If so the proof would be wrong too. \n\nIn Proposition 4.3. N_{i*}(t) is not defined. Equation 10 should be explained and Proposition 4.4 should be proved. \n\nAlso, Lemma 4.1. claims that the expectation of an event E is > 1-\\delta. Your propositions that rely on 4.1 should include this caveat (only true with probability 1-\\delta.\n\nTypo level comments. \nLine 183. \"be addressED\"\nLine 220. \\mu* and \\mu*_S* are both used. Are they the same?\nL 228. \"this conditions\" -> \"these conditions\"\nL 241. \"null mean\" -> \"zero mean\"\nL 365. \"begins the after\" -> \"begins after\"\nL 376. \"dependent by\" -> \"dependent on\""}, "questions": {"value": "1. Answer my question about the correctness of C2 and S^{1,2} above. \n\n2. Is Equation 8 correct? What about the proof of proposition 4.2.? \n\n3. Explain Equation 10 and justify Proposition 4.4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kULBniwtRA", "forum": "K42xtH1sqG", "replyto": "K42xtH1sqG", "signatures": ["ICLR.cc/2026/Conference/Submission14433/Reviewer_4PRb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14433/Reviewer_4PRb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761520095400, "cdate": 1761520095400, "tmdate": 1762924838546, "mdate": 1762924838546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies adversarial attacks on combinatorial multi-armed bandits (CMAB) under a linear reward model. It formalizes attack cost and impact trade-offs under different conditions and proposes an attack strategy designed to degrade the performance of a CUCB learner. The theoretical analysis provides regret and cost bounds under several variants of attack settings. The authors also present numerical results to illustrate the attack’s effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The study of adversarial attacks in bandit settings continues to be relevant, especially for complex decision-making environments such as CMABs.\n2) The paper systematically presents its assumptions, theoretical analysis, and experimental validation across different CMAB attack settings.\n3) The experimental results, though limited in scope, effectively illustrate the theoretical findings and demonstrate the attack’s impact under the assumed linear reward setting."}, "weaknesses": {"value": "1) The entire analysis relies on the assumption of a linear reward model. While this simplifies analysis, it restricts the generality of the results. The linear model is not representative of general CMAB settings, where nonlinear or triggering-based reward dependencies are common. It is unclear whether the techniques can extend to non-linear or triggered CMAB models.\n2) Given the linear reward assumption, the results in Section 3 seem expected and primarily incremental. The theoretical findings do not appear to introduce fundamentally new challenges beyond adapting existing bandit-attack analyses (e.g., [Jun et al., 2018]) to the CMAB domain.\n3) The discussion of linear attack costs is somewhat trivial. It offers limited insight into more realistic attack models where costs are non-linear or constrained.\n4. The attack proposed in Section 4 appears conceptually similar to the general attack framework of [Liu et al., 2019]. It would be valuable to investigate whether a more tailored attack strategy could be developed for CUCB, analogous to the specialized designs for UCB in [Jun et al., 2018]."}, "questions": {"value": "1) Can the authors clarify how the linear reward assumption affects generality? Can the attack design be adapted to specific CMAB structures (e.g., matching, subset selection) to show the generality of the approach?\n2) What are the main technical challenges compared to prior attack analyses in [Jun et al., 2018] or [Liu et al., 2019]?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OdWZfaA1VO", "forum": "K42xtH1sqG", "replyto": "K42xtH1sqG", "signatures": ["ICLR.cc/2026/Conference/Submission14433/Reviewer_riyv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14433/Reviewer_riyv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998905006, "cdate": 1761998905006, "tmdate": 1762924838109, "mdate": 1762924838109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}