{"id": "yv15C8ql24", "number": 11367, "cdate": 1758197544751, "mdate": 1759897579620, "content": {"title": "pySpatial: Generating 3D Visual Programs for Zero-Shot Spatial Reasoning", "abstract": "Multi-modal Large Language Models (MLLMs) have demonstrated strong capabilities in general-purpose perception and reasoning, but they still struggle with tasks that require spatial understanding of the 3D world. To address this, we introduce pySpatial, a visual programming framework that equips MLLMs with the ability to interface with spatial tools via Python code generation. Given an image sequence and a natural-language query, the model composes function calls to spatial tools including 3D reconstruction, camera-pose recovery, novel-view rendering, etc. These operations convert raw 2D inputs into an explorable 3D scene, enabling MLLMs to reason explicitly over structured spatial representations. Notably, pySpatial requires no gradient-based fine-tuning and operates in a fully zero-shot setting. Experimental evaluations on the challenging MindCube and Omni3D-Bench benchmarks demonstrate that our framework pySpatial consistently surpasses strong MLLM baselines; for instance, it outperforms GPT-4.1-mini by 12.94% on MindCube. Furthermore, we conduct real-world indoor navigation experiments where the robot can successfully traverse complex environments using route plans generated by pySpatial, highlighting the practical effectiveness of our approach. Our project website will be available at https://pySpatial.github.io.", "tldr": "We introduce pySpatial, a visual programming framework that flexibly composes spatial tools (e.g., 3D reconstruction, camera movements, and novel view synthesis) to enable MLLMs to explicitly reason in 3D space for diverse spatial reasoning tasks.", "keywords": ["Multi-Modal LLMs", "Spatial Reasoning", "3D Vision"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3934db7ac2c9194974a4c9abcb852a1be7d27d5b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an agentic prompting method to improve spatial understanding tasks with explicit 3D representations and achieves suprior performance than existing MLLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a novel agentic method, incorporating state-of-the-art 3D foundation models for spatial understanding tasks.\n- The proposed method achieves suprior performance comparing to other methods.\n- Real world application demonstrates the effectiveness of the whole system."}, "weaknesses": {"value": "- Since the paper requires the model to explicitly reconstruct the 3D scenes, render novel views, and navigates inside the reconstructed scene. Is it sensitive to the reconstruction methods, *i.e.* incorporating 3D foundational models other than VGGT? \n\n- Is the proposed method sensitive to the rendering quality? In Figure 2 we see holes in the rendered images. Is VLMs sensitive to those kind of inputs? Is there anyway to compensate the visual gap? Would gaussian splatting be a better choice? \n\n- Is the method sensitive to the code agent choice? Table 4 mainly compares the performance of the GPT family. What about other VLMs, will coding VLMs perform better than general purpose VLMs?"}, "questions": {"value": "The explicit 3D modeling method for spatial understanding may be limited to static environments. While in real-world robot navigation tasks, how much will changes in the environment affect the overall performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GwvKaNoQn8", "forum": "yv15C8ql24", "replyto": "yv15C8ql24", "signatures": ["ICLR.cc/2026/Conference/Submission11367/Reviewer_FeNP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11367/Reviewer_FeNP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901436845, "cdate": 1761901436845, "tmdate": 1762922495744, "mdate": 1762922495744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces pySpatial, a visual programming framework designed to enhance the 3D spatial reasoning abilities of multimodal large language models (MLLMs). While existing MLLMs perform well on general perception and reasoning tasks, they struggle with understanding spatial relationships in three-dimensional environments. pySpatial addresses this limitation by allowing MLLMs to generate and execute Python programs that call spatial tools to transform 2D image inputs into explorable 3D scenes. This enables models to reason explicitly over geometric structures without additional fine-tuning, operating entirely in a zero-shot manner. The method builds on recent advances in feed-forward 3D reconstruction and visual programming, integrating them into a unified, interpretable API for spatial reasoning. Experiments on the MINDCUBE and OMNI3D-BENCH benchmarks show that pySpatial outperforms strong MLLM baselines like GPT-4.1-mini. The framework also demonstrates practical effectiveness in real-world indoor navigation tasks, where a robot can successfully generate and follow route plans derived from pySpatial’s reasoning outputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies a relevant gap between current MLLMs’ implicit, imagination-based spatial reasoning and the need for explicit geometric grounding. The motivation is reasonable and reflects an active research direction in improving spatial understanding for embodied and multi-view settings.\n\n2. The proposed visual programming framework is well-structured and methodologically sound. Its modular API design offers a clear and interpretable mechanism for integrating 3D reasoning tools with MLLMs. This contributes to transparency and reproducibility, though the overall paradigm follows existing visual programming approaches.\n\n3. The framework demonstrates consistent zero-shot performance gains on both multi-view (MINDCUBE) and single-view (OMNI3D-BENCH) benchmarks. These improvements, achieved without any fine-tuning, indicate that the method is generally effective as a plug-and-play enhancement for existing MLLMs.\n\n4. The experimental evaluation is thorough, with comparisons spanning open-weight models, proprietary systems, and prior visual programming baselines. Additional ablation and failure analyses help clarify the contribution of each component and provide a more complete understanding of the system’s strengths and limitations.\n\n5. The experimental evaluation is thorough, with comparisons spanning open-weight models, proprietary systems, and prior visual programming baselines. Additional ablation and failure analyses help clarify the contribution of each component and provide a more complete understanding of the system’s strengths and limitations."}, "weaknesses": {"value": "1. While the integration of 3D spatial reasoning within a visual programming framework is well-executed, the core concept of using generated Python code as an intermediate reasoning layer is not entirely novel. Prior works such as VisProg, ViperGPT, and VADAR have explored similar paradigms for visual reasoning. The novelty here primarily lies in extending this paradigm to 3D tools rather than introducing a fundamentally new reasoning mechanism. Consequently, the conceptual contribution may be perceived as incremental in scope.\n\n2. The real-world navigation experiment, though interesting, is relatively limited in scale. It consists of a single scenario under controlled conditions. Additional experiments across varied environments, different robot embodiments, or more complex navigation tasks would significantly strengthen the claims regarding real-world applicability and robustness."}, "questions": {"value": "1. Could pySpatial be integrated with or further enhance the spatial reasoning capabilities of specialized spatial MLLMs (e.g., VLM-3R or Space-Qwen)? Specifically, would combining explicit program-based reasoning with models already trained on 3D-augmented data yield additional improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1J2Sab5EqS", "forum": "yv15C8ql24", "replyto": "yv15C8ql24", "signatures": ["ICLR.cc/2026/Conference/Submission11367/Reviewer_Xvqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11367/Reviewer_Xvqg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960585825, "cdate": 1761960585825, "tmdate": 1762922495185, "mdate": 1762922495185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes pySpatial, a zero-shot visual programming method to perform multi-view spatial reasoning techniques. The model calls a set of pre-existing APIs best on the reasoning of MLLM to output the final answer for spatial reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "In general the paper is written quite clearly. The method is well described, and Figure 1 and is quite clear in terms of describing the differences with most spatial mental models and pySpatial. The model is compared on several datasets (MindCube, Omni3D-Bench) to show the effectiveness.\n\nThe problem of spatial reasoning with MLLMs is also an important and relevant task in the community."}, "weaknesses": {"value": "I have some questions on the actual effectiveness of the visual program set up. From my understanding, and from all the results shown in the paper, pySpatial lists out the procedures of calling external APIs. It does not do additional complex actions (e.g., loops, if/else, etc) beyond a sequence of API calling. Are there cases where the question answer requires more than a linear sequence of API calling and if so can we see several of these examples? If not, this makes me question whether explicitly converting it to program language code really helps with the set up or just a list of action items is fine.\n\nIn addition, one of the main differences between VADAR and pySpatial is that VADAR generates the APIs required for a particular task, while in your case a set of pre-defined APIs are set. I wonder if the set of API limits the type of spatial reasoning tasks you can perform. For example, I wonder if the model would still perform better on tasks such as absolute metrics such as distances and volumes estimation, where the reconstruction itself isn’t explicitly providing more information. It would also be great to see the results on more benchmarks, e.g., MMSI-Bench, All-Angle Bench, etc. It would also be great if you can highlight a bit more on the architectural difference of VADAR and pySpatial."}, "questions": {"value": "Please refer to the weakness section for my main concerns. I would like to further understand the limitations of VADAR in terms of the complexity of the programs that it can generate, as well as more benchmarks to show the range of 3D spatial reasoning tasks it can perform."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VQYG9AAZlW", "forum": "yv15C8ql24", "replyto": "yv15C8ql24", "signatures": ["ICLR.cc/2026/Conference/Submission11367/Reviewer_Rgza"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11367/Reviewer_Rgza"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980395899, "cdate": 1761980395899, "tmdate": 1762922494545, "mdate": 1762922494545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents pySpatial, a visual programming framework that enables MLLMs to perform explicit 3D spatial reasoning. The core contribution is a Python-based API that allows MLLMs to compose spatial tools—including feed-forward 3D reconstruction, camera pose estimation, and novel view synthesis—through code generation. Given sparse image views and natural language queries, the system generates executable programs that transform 2D observations into explorable 3D representations, enabling geometric reasoning rather than relying on implicit mental models.\nThe framework operates in a zero-shot setting without gradient-based fine-tuning. Experimental results demonstrate substantial improvements over MLLM baselines on MindCube (+12.94% over GPT-4.1-mini) and Omni3D-Bench benchmarks. The authors also provide qualitative validation through real-world robot navigation experiments. The interpretability of generated programs and modular tool composition are key practical advantages."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and Well-Motivated Problem Formulation\n\nThe paper addresses a clearly identified limitation in current MLLMs regarding 3D spatial reasoning from limited views (Section 1, lines 54-65). While recent works like SpatialVLM and SpatialRGPT focus on single-view spatial understanding, this work tackles the more challenging multi-view setting where models must reason across perspectives. The visual programming paradigm is well-suited to this problem, allowing flexible composition of spatial tools through a clean Python API (Code 1, Section 3.2).\n\n2. Strong and Consistent Empirical Results Across Multiple Benchmarks\n\nOn the challenging MindCube benchmark (Table 1), pySpatial achieves 58.56% overall accuracy, substantially outperforming the best open-weight model DeepSeek-VL2-Small by 10.94% and the strongest proprietary baseline GPT-4.1-mini by 12.94%. The results are particularly impressive on the \"Among\" category (60.54%), where all baseline approaches fail to exceed 50%—this category requires reasoning about how a central object relates to all surrounding objects, a challenging spatial task. Notably, pySpatial outperforms VLM-3R by 16.5% while operating entirely zero-shot. On MindCube-1k (Table 2), pySpatial with GPT-4o reaches 62.67%, outperforming spatial mental models by ~20%. The framework also generalizes to single-view settings, achieving state-of-the-art on Omni3D-Bench among visual programming approaches (+3.8% over VADAR, Table 3) and even surpassing GPT-4o on total score.\n\n3. Zero-Shot Operation with Broad Generalization\n\nA key practical advantage is that pySpatial requires no gradient-based fine-tuning (lines 47-48, 106-107). The ablation study (Table 4, Section 4.5) demonstrates that augmenting different MLLMs with pySpatial consistently yields substantial improvements: GPT-4o improves from 42.29% to 62.67%, GPT-4.1-mini from 43.34% to 58.19%, and GPT-4.1 from 44.67% to 63.42% on MindCube-1k. This plug-and-play nature makes the framework broadly applicable without requiring expensive task-specific training, and the consistent gains across different code agents validate the robustness of the approach. The framework also successfully transfers from multi-view (MindCube) to single-view (Omni3D-Bench) settings, demonstrating versatility.\n\n4. Interpretability, Transparency, and Reproducibility\n\nUnlike black-box MLLM approaches, pySpatial generates executable Python programs (Figure 2) that are human-readable and include well-structured comments explaining the reasoning process (e.g., \"# Step 2: rotate the camera to the right from viewpoint 2 to see what is on the right side of the black chair\"). This interpretability allows researchers to inspect, debug, and modify generated programs. The API is thoroughly documented (Code 1 in main paper, full specification in Appendix A Code 2), and prompts are provided in Appendix B (lines 892-1079). The modular design separates high-level reasoning from low-level execution (lines 220-223), and the paper commits to releasing code upon acceptance (line 266). Implementation details are comprehensive, including specific model choices (VGGT, CUT3R), rendering backend (Open3D), and computational setup (single A6000 GPU, Section 4.1).\n\n\n5. Comprehensive Experimental Evaluation and Analysis\n\nThe evaluation is thorough and multi-faceted: (1) Benchmark diversity: Tests on both multi-view (MindCube) and single-view (Omni3D-Bench) benchmarks; (2) Baseline breadth: Compares against open-weight MLLMs, proprietary models, specialized spatial models, and prior visual programming approaches (Tables 1-3); (3) Qualitative analysis: Figure 2 shows diverse examples with generated programs, 3D reconstructions, and outputs; (4) Ablation study: Table 4 validates effectiveness across different code agents; (5) Failure analysis: Manual examination of ~100 samples (Section 4.5, Figure 4) attributes errors to program generation (6%), 3D reconstruction (13%), and final reasoning (20%), providing transparency about system limitations; (6) Efficiency analysis: Reports 7.45s per query on MindCube-1k, competitive with VADAR's 17.25s (Section 4.5); (7) Real-world validation: Indoor robot navigation experiment (Figure 3, Section 4.4) demonstrates practical applicability."}, "weaknesses": {"value": "1. Limited and Non-Rigorous Real-World Validation\nWhile Section 4.4 presents robot navigation as evidence of practical effectiveness, the evaluation is limited: (a) Qualitative only: No quantitative success rates, path efficiency metrics, or safety margins are reported; (b) Single environment: Testing appears confined to one 50m² two-room laboratory; (c) Manual intervention: High-level position commands are \"manually converted into temporal velocity targets\" (lines 375-377), reducing the autonomy claim; (d) Limited comparison: Only one baseline (GPT-4.1) is tested, and failure modes are not systematically analyzed. The paper states the robot \"successfully traverses complex environments\" (line 46), but Figure 3 shows only one trajectory. To strengthen claims of \"practical effectiveness,\" the authors should provide: success rates over multiple trials/environments, collision avoidance validation, automated planning-to-control integration, and discussion of failure recovery mechanisms.\n\n\n2. Reconstruction Quality as Performance Bottleneck with Insufficient Analysis\nThe failure analysis (Section 4.5, Figure 4) reveals that 13% of errors stem from 3D reconstruction quality, and the paper acknowledges this dependency. However, the analysis lacks depth: (a) No sensitivity studies: How does performance vary with 2 vs 4 vs 8 input views? What about different view overlap percentages or baseline distances? (b) Scene characteristics unexplored: Which scene properties (low texture, reflective surfaces, repetitive patterns, dynamic objects) most affect reconstruction quality? (c) Backend comparison incomplete: VGGT is used for most experiments, CUT3R for navigation, but no systematic comparison between VGGT/DUSt3R/CUT3R on the same multi-view tasks; (d) Mitigation strategies absent: Are there ways to detect reconstruction failures (e.g., uncertainty estimation) or fallback mechanisms when reconstruction quality is poor? The statement \"advances in 3D reconstruction... hold the potential to further enhance our performance\" (lines 469-470) is true but sidesteps the question of current robustness limits.\n\n\n3. Heavy Reliance on Proprietary Models Limits Reproducibility\nMain results depend critically on GPT-4o/GPT-4.1-mini (Table 1 uses GPT-4.1-mini \"due to budget constraints,\" line 273; Table 2 and most analyses use GPT-4o). While Table 4 shows the framework works with different code agents, all are GPT-4 series models. The paper lacks: (a) Open-source code generation results: No experiments with CodeLlama, DeepSeek-Coder, or other fully open models for program synthesis; (b) Reproducibility concerns: Proprietary model APIs can change behavior across versions, making exact reproduction difficult; (c) Cost analysis: No discussion of API costs versus open alternatives; (d) Sensitivity to model choice: Beyond the three GPT-4 variants shown, how much does performance degrade with weaker but open code generators? This dependency limits the \"zero-shot\" and \"plug-and-play\" accessibility claims for researchers without API access or budgets.\n\n\n4. Insufficient Statistical Rigor and Evaluation Details\nThe quantitative results lack statistical validation: (a) No confidence intervals: Tables 1-3 report point estimates without error bars or standard deviations; (b) No significance testing: Claims of improvement (e.g., \"+12.94% over GPT-4.1-mini\") are not tested for statistical significance; (c) Limited failure analysis scale: Only \"about 100 samples\" manually examined (line 460), representing <1% of MindCube and <10% of MindCube-1k; (d) Baseline parity unclear: No discussion of whether baselines use identical prompts, temperature settings, or number of trials; (e) Prompt sensitivity unstudied: The approach relies heavily on in-context examples (Appendix B), but no ablation on prompt variations or example selection is provided. Given that results are based on sampling from language models (which can be stochastic), statistical rigor is essential for verifying claimed improvements are robust rather than within noise margins.\n\n\n5. Underspecified Calibration and Scale Handling\nCritical details about camera calibration and scale are missing or unclear: (a) VGGT scale ambiguity: VGGT produces \"up-to-scale\" reconstructions (line 182), but Section 3.2's camera description assumes computing actual displacements (equations 1, line 191). How is scale determined or normalized for motion categorization? (b) Intrinsics estimation: Camera intrinsics K appear in Equation 1, but it's unclear whether these are estimated by the reconstruction model, extracted from metadata, or assumed as default values; (c) Coordinate frame alignment: How are multiple views aligned into a consistent world frame, especially for scenes without significant overlap? These technical details are important for understanding when the approach will succeed or fail and for enabling reproduction."}, "questions": {"value": "1. Your main results use GPT-4o/GPT-4.1-mini. Can you report results using fully open-source code generation models (e.g., CodeLlama-34B, DeepSeek-Coder-33B, or recent Qwen-Coder models) for both program synthesis and final answer generation? This would: (a) Improve reproducibility for researchers without API access; (b) Clarify how much performance depends on proprietary model capabilities; (c) Identify minimum model requirements for the framework to be effective. Even negative results would be valuable for the community.\n\n2. Please clarify the technical pipeline for camera parameters: (a) For VGGT (up-to-scale), how do you handle scale ambiguity in Equation 1 and the camera motion description (Section 3.2)—is scale normalized per scene, or are only relative directions used? (b) How are camera intrinsics K determined—estimated by reconstruction models, from metadata, or assumed? (c) For navigation with CUT3R, what is the sensitivity to metric scale errors (e.g., ±10-20%)? (d) How are camera extrinsics aligned to a consistent world frame for scenes with limited overlap? This is critical for reproducibility.\n\n3. Can you provide: (a) Confidence intervals or bootstrapped standard errors for the main benchmark results (Tables 1-3)? (b) Statistical significance tests comparing pySpatial against key baselines (e.g., GPT-4.1-mini, VADAR)? (c) Ablation on prompt design—how sensitive is performance to the choice of in-context examples, API documentation verbosity, or structured output formatting? (d) Results with multiple random seeds or temperature settings to quantify variance? This would increase confidence that reported improvements are robust.\n\n4. Given that 13% of failures stem from reconstruction quality (Section 4.5), can you provide systematic analysis of performance under varying input conditions? Specifically: (a) Performance curves as a function of number of input views (e.g., 2, 4, 6, 8 views); (b) Impact of view overlap percentage on reconstruction and final task accuracy; (c) Comparison between VGGT, DUSt3R, and CUT3R on the same MindCube subset; (d) Scene characteristics that most correlate with reconstruction failures (texture, lighting, clutter level)? This would help users understand when pySpatial is reliable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YksDcF01pd", "forum": "yv15C8ql24", "replyto": "yv15C8ql24", "signatures": ["ICLR.cc/2026/Conference/Submission11367/Reviewer_ZMiG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11367/Reviewer_ZMiG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044815432, "cdate": 1762044815432, "tmdate": 1762922494168, "mdate": 1762922494168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}