{"id": "imzDS4N8CW", "number": 8659, "cdate": 1758093881161, "mdate": 1759897771305, "content": {"title": "When Models Know When They Do Not Know: Calibration, Cascading and Cleaning", "abstract": "When a model knows when it does not know, many possibilities emerge. The first question is how to enable a model to recognize that it does not know. A promising approach is to use confidence, computed from the model’s internal signals, to reflect its ignorance. Prior work in specific domains has shown that calibration can provide reliable confidence estimates. In this work, we propose a simple, effective, and universal training-free method that applies to both vision and language models, performing model calibration, cascading, and data cleaning to better exploit a model's ability to recognize when it does not know. We first highlight two key empirical observations: higher confidence corresponds to higher accuracy within a single model, and models calibrated on the validation set remain calibrated on a held-out test set. These findings empirically establish the reliability and comparability of calibrated confidence. Building on this, we introduce two applications: 1. Model cascading with calibrated advantage routing and 2. Data cleaning based on mixture of experts. Using the routing signal derived from the comparability of calibrated confidences, we cascade large and small models to improve efficiency with almost no compromise in accuracy, and we further cascade two large state-of-the-art models to achieve performance beyond either model alone. Leveraging multiple experts and their calibrated confidences, we design a simple yet effective data-cleaning method that balances precision and detection rate to identify mislabeled samples in ImageNet and Massive Multitask Language Understanding (MMLU) datasets. Our results demonstrate that enabling models to recognize when they do not know is a practical step toward more efficient, reliable, and trustworthy AI.", "tldr": "a general framework that enables a model to know when it does not know and to make use of this property", "keywords": ["Model Calibration", "Model Cascades", "Data Cleaning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/15f153d32eedccd6d5def2e3c98e8ad6d40eaffb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a training-free, unified framework leveraging model calibration to enable two capabilities: 1. Model cascading based on calibrated confidence advantage, routing samples between small and large models for efficiency or accuracy gains. 2. Data cleaning via a mixture-of-experts agreement scheme that flags mislabeled examples using calibrated confidences. Experiments span ImageNet, MMLU, GSM8K, MBPP, and BigCodeBench, showing that (i) calibrated confidence monotonically correlates with accuracy and generalizes from validation to test, (ii) cascading can balance cost and accuracy, and (iii) confidence-based cleaning surpasses Confident Learning in precision."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "I like this paper, it is a complete empirically study, despite the lack of theoicaitcal study.\n1. Clarity and simplicity: The framework is easy to understand and reproduce.\n2. Cross-domain applicability: Demonstrated on both vision and language tasks.\n3. Empirical breadth: Includes multiple benchmarks and models, from EVA-02 to LLaMA/Qwen.\n4. Training-free nature: Practical for deployment without retraining routers.\n5. Solid comparisons: Includes random baselines and Confident Learning."}, "weaknesses": {"value": "1. Limited novelty: Essentially an aggregation of calibration and routing concepts; lacks new algorithms or theory.\n2. No OOD or corrupted-data tests; calibration reliability is only shown in-distribution. The assumption that \"better calibration on validation set results in better calibration in test set\" may not hold in this setting. Especially in LLM scenario, LLM face domain shift a lot. This should be discussed.\n3. Minor improvements: Accuracy and ECE gains are small in some cases. For example the MMLU and imagenet. The explanation that imagenet models are well calibrated so performance are close. But why language task still perform the same? (uncalibrated and calibrated)"}, "questions": {"value": "Why don't try some other calibration methods? maybe these models are not well calibrated under such methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9jFncytBG5", "forum": "imzDS4N8CW", "replyto": "imzDS4N8CW", "signatures": ["ICLR.cc/2026/Conference/Submission8659/Reviewer_tASX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8659/Reviewer_tASX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761401255034, "cdate": 1761401255034, "tmdate": 1762920477322, "mdate": 1762920477322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes training-free frameworks to leverage model confidence for applications in both vision and language domains. The paper formalizes a unified view of model calibration, empirically showing that calibrated confidence is a reliable cross-model predictor of accuracy. They introduce a model cascading method that uses a calibrated advantage signal to route inputs between small and large models for efficiency, or between two large models to achieve stronger performance. They also present a data cleaning method using an MoE approach, where disagreement between highly confident models and the ground-truth is used to identify mislabeled data in datasets like ImageNet and MMLU."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The proposed methods for cascading and data cleaning are simple, intuitive, and don't require complex training of auxiliary models. The framework can apply to both computer vision and natural language processing tasks, demonstrating its generalizability."}, "weaknesses": {"value": "1. The evaluation results about the confidence-accuracy relationship in Sec. 3.1 is of minor contribution, since they are common model calibration characteristics.\n2. The calibrated and uncalibrated results in Figure 2(a,b) are almost identical, showing insufficient evaluation. The authors are suggested to evaluate on a truly uncalibrated model.\n3. The difference of calibration is minimal for benchmarks other than GSM8K and MMLU in Figure 3.\n4. Both the confidence cascading and data cleaning frameworks are verified with limited comparisons, e.g., there are confidence routing methods which should also be included for comparison.\n5. Minor: typo around Line 389: \"as show\"."}, "questions": {"value": "1. What is the key novelty of the paper? The data cleaning approach is based on the established idea that confident model predictions that disagree with a label indicate a likely error, while the proposed model cascading is very similar to model routing methods.\n2. What is the role of calibrated confidence in these frameworks? Currently, the improvement by calibration seems marginal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wJwYFOLbpo", "forum": "imzDS4N8CW", "replyto": "imzDS4N8CW", "signatures": ["ICLR.cc/2026/Conference/Submission8659/Reviewer_xhYV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8659/Reviewer_xhYV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554952394, "cdate": 1761554952394, "tmdate": 1762920476937, "mdate": 1762920476937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified framework for model inference and data cleaning. Based on the rule of confidence calibration, the author presents two primary applications named calibrated model cascading and mixture-of-experts data cleaning. Extensive experiments demonstrate that the proposed method shows superior precision and recall over baselines like confident learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThe proposed framework is easy to deploy. The author presents the proposed method as a universal method that can be applied to both vision and language tasks."}, "weaknesses": {"value": "1.\tThe key findings lack novelty. The author highlights that “higher confidence corresponds to higher accuracy within a single model, and models calibrated on the validation set remain calibrated on a held-out test set.” However, these findings are basic rules in confidence calibration. The author uses too many sections to illustrate these facts.\n2.\tThe claim for \"training-free\" is misleading. The author claims that the proposed cascading method is training-free. However, post-hoc calibration still explicitly requires optimizing parameters on a held-out validation set using NLL loss.\n3.\tThe notation is ambiguous. The author uses many $K$ to represent different items, such as \"selection budget\" in the cascading algorithm (Alg. 1) and the \"threshold \" in the data cleaning algorithm (Alg. 2). \n4.\tThe author points out that disagreement-based methods depend on the \"independence between experts\". However, several key experiments use models from the same family, such as LLAMA and EVA-02.\n5.\tThe use of mixture-of-experts (MoE) is confused. The paper repeatedly uses MoE to describe its data cleaning method. However, MoE usually refers to a specific architecture with a dynamic routing mechanism.\n6.\tLack of comparison to other multi-model methods. The small-large cascade is motivated by efficiency. However, the paper only compares it to random routing and an uncalibrated variant. I recommend that the author compare their method with the traditional Mixture-of-Experts (MoE) architecture composed of multiple small uncalibrated models. It is unclear how the proposed method compares in its accuracy-efficiency trade-off to MoE."}, "questions": {"value": "1.\tWhy use different calibrations for vision and language tasks? Temperature scaling is often used in LLM calibration [1-2].\n\n[1] Can LLMs express their uncertainty? NeurlPS, 2023.\n\n[2] Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator. NeurlPS, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "riJwWXcv7u", "forum": "imzDS4N8CW", "replyto": "imzDS4N8CW", "signatures": ["ICLR.cc/2026/Conference/Submission8659/Reviewer_nfCf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8659/Reviewer_nfCf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557968262, "cdate": 1761557968262, "tmdate": 1762920476545, "mdate": 1762920476545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission leverages the calibration property of both vision and language models for two applications: 1) Model cascading and 2) Data cleaning. The main motivations are: higher confidence corresponds to higher accuracy within a single model, and models calibrated on the validation set remain calibrated on a held-out test set. Based on the two observations, this submission can conduct the model cascade based on the calibrated confidence, aiming to improve efficiency. In the meantime, the disagreement of two models can be used to identify mislabeled samples in ImageNet and MMLU. The experiments show validate the usage of a calibrated confidence score works for both applications."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ It is interesting to study the calibrated confidence score for the following applications. The model cascading is a simple and useful example\n\n+ The writing is very clear. The method is well-introduced. The settings of both data clearning and model cascading are clear."}, "weaknesses": {"value": "+ [**Clarification on the two observations**] The main points are: higher confidence corresponds to higher accuracy within a single model, and models calibrated on the validation set remain calibrated on a held-out test set. The first point has been reported in [a], which could be useful to include. The second point should be specified in the context of the in-distribution test set, as the calibration will fail when it comes to out-of-distribution shifts [b]. \n\n    [a] Predicting with confidence on unseen distributions\n\n    [b] Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift\n\n+ [**No Consideration of Out-of-Distribution (OOD) Settings**] Following up on the above point, the framework is evaluated entirely on in-distribution datasets like ImageNet and MMLU. There’s no evaluation on distribution shifts (e.g., ImageNet-C, ImageNet-R, or adversarial MMLU subsets), where calibration often fails. It would strengthen the paper to either (1) provide evaluations under OOD conditions, or (2) narrow the scope of claims to reflect this limitation more precisely.\n\n+ [**Clarify the usage of Platt scaling (for language)**] Given that Platt scaling is traditionally designed for binary classification, how does it generalize to sequence-level tasks in language models? Is there any connection with the token confidence of the predicted sequence?\n\n+ [**The improvement is somewhat limited**] According to Figures 2 and 3, the improvement of Calibrated over Uncalibrated is limited. Please clarify the advantage. Please clarify what specific advantages calibrated confidence provides in these cases. Furthermore, since the cascade performance depends on the top-K bin selection, how robust is the approach to this hyperparameter?"}, "questions": {"value": "- Please clarify that if involving more than two models helps\n- If self-reported confidence work in language tasks?\n- How sensitive are the cascading and cleaning results to the number of bins or the top-K selection thresholds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "G2x3OMyPHc", "forum": "imzDS4N8CW", "replyto": "imzDS4N8CW", "signatures": ["ICLR.cc/2026/Conference/Submission8659/Reviewer_mxpA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8659/Reviewer_mxpA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567557709, "cdate": 1761567557709, "tmdate": 1762920476111, "mdate": 1762920476111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}