{"id": "U7pWkp90qA", "number": 25517, "cdate": 1758368817992, "mdate": 1759896717707, "content": {"title": "DyCodeExplainer: Explainable Dynamic Graph Attention for Multi-Agent Reinforcement Learning in Collaborative Coding", "abstract": "We propose \\textbf{DyCodeExplainer}, a novel multi-agent reinforcement learning (MARL) framework that integrates dynamic graph attention with explainability techniques to improve collaborative coding. Existing MARL systems typically depend on static communication protocols which are not flexible and transparent in performing coding tasks that are more complicated. The above method suffers from this limitation by treating the interaction of agents in the form of a time-evolving graph in which the nodes represent coding agents, and edges indicate messages exchanged between them. A dynamic graph attention network (DGAT) dynamically prioritizes the messages considering contextually relevant message, whereas hard attention gate eliminates noises and helps improve decision-making efficiency. Furthermore, the framework includes gradient-based attention attribution and rule-based post-hoc explanations to explain message prioritization for providing interpretable budgetary information about the collaborative process. The policy and critic networks use Transformer-XL and graph neural networks respectively for managing the long-range dependencies and assessing the memory argument of the joint state values. Experiments show DyCodeExplainer to be more accurate in terms of code correctness and collaborative efficiency than traditional MARL baselines. The novelty of the system is the simultaneous optimization of thresholds for dynamic attention and explainability rules to bridge an important gap in transparent multi-agent coding systems. This work will move the field forward by providing a scalable and interpretable solution for collaborative software development.", "tldr": "", "keywords": ["Collaborative Coding"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/249d7b76e9fb81e4a5ae577b6181697594ce1191.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes **DyCodeExplainer**, a framework designed to enhance both the performance and interpretability of multi-agent reinforcement learning systems for collaborative programming. The model uses a **dynamic graph attention network** to model agent interactions, where edges are gated by a sparsity-inducing hard threshold. It also introduces a **hybrid explainability module** that combines gradient-based attribution with symbolic rule extraction, aiming to produce interpretable reasoning traces for each coding action. The experiments demonstrate improved correctness and communication efficiency on two collaborative coding benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Tackles an emerging and relevant problem—explainable multi-agent collaboration for code generation.\n\n* The dynamic attention mechanism is conceptually straightforward and may encourage efficient communication.\n\n* The combination of symbolic rules and neural explanations is an interesting hybrid approach that could inspire follow-up work.\n\n* The paper is clearly structured and includes good qualitative visualizations of attention maps."}, "weaknesses": {"value": "* The hard gating function is non-differentiable, yet the paper does not describe how gradients are approximated (e.g., STE, Gumbel-softmax).\n\n* The rule learning process is underdefined—there is no description of the search space or optimization mechanism for symbolic rules.\n\n* The human evaluation used to justify the explainability claims lacks methodological rigor (no inter-rater reliability, sample size, or blinding).\n\n* Experimental validation is narrow (two datasets) and lacks cross-language or cross-agent generalization tests.\n\n* The proposed approach, while creative, seems only partially implemented and evaluated."}, "questions": {"value": "1. How is the threshold parameter for the hard gate trained or tuned?\n\n2. How are the symbolic rules parameterized and optimized alongside the neural model?\n\n3. Can you provide more methodological details on your human evaluation protocol?\n\n4. Have you tried the method on different coding environments or programming languages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gpc87E8a8o", "forum": "U7pWkp90qA", "replyto": "U7pWkp90qA", "signatures": ["ICLR.cc/2026/Conference/Submission25517/Reviewer_PFrr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25517/Reviewer_PFrr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761204076680, "cdate": 1761204076680, "tmdate": 1762943459899, "mdate": 1762943459899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DyCodeExplainer, a framework that uses dynamic graph attention and explainability techniques to improve multi-agent reinforcement learning for collaborative coding."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This paper appears to be AI-generated and lacks genuine research depth. It loosely applies MARL concepts to LLM-based agent collaboration without a clear problem definition or credible technical grounding. The proposed framework mixes unrelated components like Transformer-XL, GNN, and explainability modules in a way that feels incoherent and artificial. There are essentially **no meaningful strengths**, as the work lacks novelty, theoretical justification, and real experimental validity."}, "weaknesses": {"value": "1. The paper mixes multi-agent reinforcement learning, dynamic graph attention, and explainability mechanisms without establishing any clear theoretical connection, making it appear as an arbitrary combination of unrelated concepts.\n2. The collaborative coding task is not naturally suited for MARL modeling and aligns more closely with LLM-based agent systems. The use of a reinforcement learning framework feels forced and unjustified, giving the impression that the work was written by someone unfamiliar with both fields who attempted to merge them superficially.\n3. Although the experimental data and results appear complete, they lack real significance and reproducibility. No implementation details or accessible code are provided, making the reported findings highly questionable.\n4. The writing is excessively templated, repeatedly using terms such as “dynamic graph attention” and “explainability framework,” and lacks the logical flow expected in genuine academic writing.\n5. The proposed joint optimization objective and explainability modules have no theoretical grounding or derivation; the equations are merely formal decorations without substance.\n6. The so-called “collaborative coding” task is poorly defined, with no clear description of the environment or the concrete interaction mechanisms among agents.\n\nOverall, this paper appears to be an AI-generated pseudo-academic text that is formally structured but substantively empty, lacking genuine research foundation, methodological rigor, and technical credibility."}, "questions": {"value": "This paper is clearly an AI-generated and fabricated manuscript with empty content, incoherent logic, and no genuine experiments or theoretical grounding. The text shows extensive signs of patching and template-based generation, with irrelevant citations, mismatched methods and tasks, and even indications of fabricated experimental data and results. It wastes the reviewers’ time and undermines the seriousness and integrity of the academic review process. It is strongly recommended that the conference committee verify the authors’ identities and the source of the submission and hold them accountable for this misconduct."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JSQwM4sCNP", "forum": "U7pWkp90qA", "replyto": "U7pWkp90qA", "signatures": ["ICLR.cc/2026/Conference/Submission25517/Reviewer_mHmD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25517/Reviewer_mHmD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652489557, "cdate": 1761652489557, "tmdate": 1762943459699, "mdate": 1762943459699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DyCodeExplainer, a novel multi-agent reinforcement learning (MARL) framework that integrates dynamic graph attention with explainability techniques to enhance collaborative coding. The idea is innovative and addresses critical challenges in message prioritization and decision-making transparency within collaborative coding environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The integration of dynamic graph attention networks (DGAT) with hybrid explainability techniques is a novel approach that effectively captures the evolving nature of agent interactions in collaborative coding tasks.\n\nThe combination of gradient-based attribution and rule-based post-hoc explanations provides both precise importance scoring and intuitive rationale generation, addressing the semantic gap between low-level attention weights and high-level coding logic."}, "weaknesses": {"value": "The dependency on predefined rule templates for post-hoc explanations requires manual engineering effort, which may limit scalability and adaptability to new coding domains or languages."}, "questions": {"value": "No"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KRh5sD2AEx", "forum": "U7pWkp90qA", "replyto": "U7pWkp90qA", "signatures": ["ICLR.cc/2026/Conference/Submission25517/Reviewer_k1wM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25517/Reviewer_k1wM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707450315, "cdate": 1761707450315, "tmdate": 1762943459514, "mdate": 1762943459514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a computational framework to combine dynamic graph attention and explainability to improve multi-agent performance in a collaborative coding task. The authors evaluate their proposed method with baseline MARL methods which indicate that DyCodeExplainer achieves better performance in terms of correctness and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The idea of combineing dynamic graph attention and explainability in MARL is relatively novel."}, "weaknesses": {"value": "The literature review section misses important work in the field when identifying the gap. In Section 2.3 the references are survey papers. I would recommend citing the exact empirical work. Attached are a few references to start with. There is work using an attention mechanism [6] or gating function [1-2] to selectively communicate given the task context. In the emergent communication community, researchers have also been working on improving communication interpretability by rewarding agents to communicate in a semantically meaningful space [5] or directly aligning the agent communication space with the human natural language space [3-4].\n\n[1] Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks\n\n[2] Interpretable learned emergent communication for human-agent teams\n\n[3] Multi-agent cooperation and the emergence of (natural) language\n\n[4] Language grounded multi-agent reinforcement learning with human-interpretable communication\n\n[5] Emergent discrete communication in semantic spaces\n\n[6] Multi-agent graph-attention communication and teaming\n\nThe target task “collaborative coding” is not explained before being used. I would recommend properly defining the task space (number of agents, form of communication, observation and action space) and the RL learning objective in a separate problem formulation section.\n\nThe proposed framework is not clearly explained in the method section. Details are missing to reproduce the work. Figure 1 should be extended to include interaction with external task environments. It would be helpful to provide a concrete interaction example between agents in the target task context.\n\nHuman evaluation details are missing. For example, how many annotators were recruited? What instructions and explanation materials were shown to participants for the evaluation? Are the reported differences statistically significant?\n\nIt's not clear to me why adding the explainability objective improves performance in the ablation study. My understanding is that explanations are generated for humans and are basically an auxiliary task that does not directly contribute to the main task objective. Could the author elaborate more on this?"}, "questions": {"value": "It's not clear to me why adding the explainability objective improves performance in the ablation study. My understanding is that explanations are generated for humans and are basically an auxiliary task that does not directly contribute to the main task objective. Could the author elaborate more on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fD45SmRxAA", "forum": "U7pWkp90qA", "replyto": "U7pWkp90qA", "signatures": ["ICLR.cc/2026/Conference/Submission25517/Reviewer_YmaW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25517/Reviewer_YmaW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765578422, "cdate": 1761765578422, "tmdate": 1762943458694, "mdate": 1762943458694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}