{"id": "RcxM3e59ga", "number": 4332, "cdate": 1757664055258, "mdate": 1759898039121, "content": {"title": "Who Said Neural Networks Aren't Linear?", "abstract": "Neural networks are famously nonlinear. However, linearity is defined relative to a pair of vector spaces, $f$$:$$\\mathcal{X}$$\\to$$\\mathcal{Y}$. Is it possible to identify a pair of non-standard vector spaces for which a conventionally nonlinear function is, in fact, linear? This paper introduces a method that makes such vector spaces explicit by construction. We find that if we sandwich a linear operator $A$ between two invertible neural networks, $f(x)=g_y^{-1}(A g_x(x))$, then the corresponding vector spaces $\\mathcal{X}$ and $\\mathcal{Y}$ are induced by newly defined addition and scaling actions derived from $g_x$ and $g_y$. We term this kind of architecture a Linearizer. This framework makes the entire arsenal of linear algebra, including SVD, pseudo-inverse, orthogonal projection and more, applicable to nonlinear mappings. Furthermore, we show that the composition of two Linearizers that share a neural network is also a Linearizer. We leverage this property and demonstrate that training diffusion models using our architecture makes the hundreds of sampling steps collapse into a single step. We further utilize our framework to enforce \nidempotency (i.e. $f(f(x))=f(x)$) on networks leading to a globally projective generative model and to demonstrate modular style transfer.", "tldr": "We define vector spaces where networks are linear, then apply linear algebra to tasks like one-step diffusion.", "keywords": ["Linear", "flow", "idempotent", "diffeomorphism", "vector-space"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b2c9da76166a4e6a440934804f213dd4db08dbd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors reinvent a classical mathematical technique which I know under the name transport of structure: https://en.wikipedia.org/wiki/Transport_of_structure. This method typically an exercise when vector spaces are introduced."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "There are experiments that seem to show some practical value of the approach. The paper is also easy to read."}, "weaknesses": {"value": "Theoretically, the whole approach is beyond trivial. Saying that this method is well known would be an understatement of epic proportions. \nThis idea is quite literally in every introduction course to linear algebra and functional analysis. It is central in topology,  and geometry. This induced vector space is the whole reason tangent spaces are a thing, hence all of differential geometry is based on it. It is absolutely inconceivable to me that the authors have never come across transport of structure before."}, "questions": {"value": "I am sorry, I do not have any questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xWnyR2tCkc", "forum": "RcxM3e59ga", "replyto": "RcxM3e59ga", "signatures": ["ICLR.cc/2026/Conference/Submission4332/Reviewer_Yo18"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4332/Reviewer_Yo18"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761030293752, "cdate": 1761030293752, "tmdate": 1762917303077, "mdate": 1762917303077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies maps that are the composition $g_y^{-1}\\circ A\\circ g_x:X\\to Y$ of an invertible neural network $g_x$, a linear map $A$ and an inverse neural network $g^{-1}_y,$ where the neural networks depend only on the spaces $X$ and $Y$.  In other words, it studies maps from $X$ to $Y$ that can be represented as linear map after the spaces are transformed in a suitable non-linear  way. This makes it possible to bring several tools of linear algebra in a new way to machine learning.  For example  projections, semigroups, singular value decomposition are then formulated for potentially non-linear operators. This is an interesting idea. The paper can be consider also a technically simple way (not in the sense that it is trivial, but easily implementable) to do manifold learning and learn maps between manifolds."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper has a clear idea that is well developed. The question when maps or families of maps can be coded or represented as linear maps is simple but the idea is developed in creative and quite deep way. The paper is very well written."}, "weaknesses": {"value": "The families of the maps which can be represented in the form  $g_y^{-1}\\circ A\\circ g_x$ may be quite small. In fact, the authors discuss this well and give examples of maps for which their method does not work. However, the authors give also several interesting examples of problems to which they can apply their architecture and this shows that their results are widely applicable."}, "questions": {"value": "1. What you mean by  \"space\", can it be it a finite or infinite dimensional vector space, an affine space, or a topological space. If can can be an infinite dimensional vector space, what is its topology? \n\n2. Are $g_x$ and $g_y$ and their inverse maps continuous?\n\n3. When $g_x$ and $g_y$ are represented as neural networks or neural operators, what kind of architecture you can use to ensure that  these maps are bijections?\n\n4. Do you need uniform continuity of the inverses of the maps $g_x$ and $g_y$ to make the $\\oplus$ and $\\odot$ operations continuous?\n\n5. On Lemma 3: Is the function $g_y$  fixed in the space $Y$ when you consider families of functions $X\\to Y$?\n\n6. In formula (14), is the norm the $g$-norm?\n\n7. Do we have any universal approximation results (or approximation results in some restricted class of functions or families of functions) for the linearizers?\n\n8. In general, please explain in more detail what functions in the linearizer are trained. For example if you consider one function from $X$ to $Y$, do you find all the maps $g_x$, $g_y$ and the linear operator\n$A$ by training? How training is done when you have a family of functions between the same spaces $X$ and $Y$?\n\n9. In formula (15), what are the properties of  the function $f(x,t)$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uvR2XcQHcK", "forum": "RcxM3e59ga", "replyto": "RcxM3e59ga", "signatures": ["ICLR.cc/2026/Conference/Submission4332/Reviewer_e6qV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4332/Reviewer_e6qV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761259083, "cdate": 1761761259083, "tmdate": 1762917302748, "mdate": 1762917302748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a methodology for linearizing nonlinear neural networks by applying linear space theory. First, it introduces an algebra (vector operations and a metric) that endows curved spaces with the structure of inner product spaces using invertible neural transformations. Using this, it treats neural networks as linear mappings on curved vector spaces transformed by these invertible networks. As applications, this paper proposes one-step inference via flow-matching, interpolation for style transfer, and the construction of idempotent networks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Several studies have explored deep learning models that perform linear operations in latent spaces, such as Mixup and state-space models. However, few have provided a theoretical framework for such operations. This paper offers a theoretical framework that reframes neural networks as linear mappings in curved spaces. To my knowledge, no other paper has framed neural networks in this manner."}, "weaknesses": {"value": "1. As the authors point out, the proposed linearizable neural networks have limitations in their expressive power. Therefore, I question whether they are theoretically useful architectures.\n2. The experiments are limited to generation tasks on relatively small image datasets, such as MNIST and CelebA. Therefore, their significance is limited from the perspective of practical usefulness.\n3. I would like more clarification on the benefit of reframing deep learning models as linear mappings in curved spaces. If all neural networks could be interpreted this way, it would be convincing. However, as this paper itself points out, this is not the case. Therefore, it seems more reasonable to view this paper as proposing a specific type of architecture that combines invertible networks with linear mappings."}, "questions": {"value": "See the Weaknesses section, particularly the third point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kTTTg3k7Xz", "forum": "RcxM3e59ga", "replyto": "RcxM3e59ga", "signatures": ["ICLR.cc/2026/Conference/Submission4332/Reviewer_TT5Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4332/Reviewer_TT5Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889327080, "cdate": 1761889327080, "tmdate": 1762917302393, "mdate": 1762917302393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to use invertible neural networks as non linear coordinate transformations of vector spaces R^n and to consider the linear structure within one such coordinate representation, leading to a non classical linear structure in the other and to a non-linear structure w.r.t. the standard operations on R^n. The authors thereafter consider the linear operations with respect to the deformed addition and scalar multiiplication. When seeing the results from the angle of non linear coordinates of a vector space, the results of the given Lemma are pretty straight forward, but valuable for less mathematically oriented readers. \nInterestingly, the authors can then employ their linearization approach in learning image synthesis style transfer. The FM generation paths that usually require the solution of a non linear ODE can, in the linearized representation be represented by a learned linear map that is obtained as the time integral of a linear vector field A_t v which is represented by a neural network. When the solution operator to A_t is computed once, this can be used for a essentially lossless one step generation.\nThe authors demonstrate the viability of their approach for MNIST and Celebrity Faces image generation tasks and neural style transfer. The results are decent and show the potential of the author's point of view."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "* the paper provides a really original idea to consider invertible neural networks in vector spaces and then build machine learning tasks on linear methods in the constructed latent space\n* The authors show that this approach can be combined with generative learning tasks in the image domain, in particular that the time integration of the FM vector field can be effectively represented by a linear solution operator obtained as the time integration of (time dependent) linear vector fields.\n* The theoretical documentation is rather complete and easy to follow"}, "weaknesses": {"value": "- The core idea of the paper - looking at vector spaces in non linear coordinates - could be presented somewhat clearer. Many results given in the lemma would then just be clear and in parts superfluous (like Lemma 4). Also all the linear algebra checked in detail would follow from one structural argument. \n- Better visualizations would be really helpful for a better representation of the main idea. \n- For me the missing part is a detailed investigation of the interaction of the chosen 'coordinate maps' g with the learning task. Obviously not any g would do (e.g. not the identity or linear g's).\n- Related to this, I would appreciate a much more extensive analysis of the training details, especially with regard to the previous point.\n- Also the interaction between the learning task and the expressivity of the linearizers would be of interest. Is the reproduction of the FM time integration with a linear flow compatible with other approaches that do not have as 'stiff' vector fields as FM - take e.g. a likelihood based trained neuralODE. Similar questions arise for the style transfer task.\n- The title, due to its generality, somehow promises more than is kept by the rather specific application domain.\n- The authors address flow matching as diffusion. As an ODE based method it is not a diffusion process, unlike SDE based DDPM. In a mathematically based paper, such inaccuracies should be avoided."}, "questions": {"value": "- Can you give a detailed and reproducible account on how the coordinate maps g are obtained in the respective application context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3t2IRdDLF7", "forum": "RcxM3e59ga", "replyto": "RcxM3e59ga", "signatures": ["ICLR.cc/2026/Conference/Submission4332/Reviewer_ojRH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4332/Reviewer_ojRH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992607816, "cdate": 1761992607816, "tmdate": 1762917302048, "mdate": 1762917302048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}