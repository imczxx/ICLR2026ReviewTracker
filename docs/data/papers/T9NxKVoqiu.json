{"id": "T9NxKVoqiu", "number": 15733, "cdate": 1758254572900, "mdate": 1763460447148, "content": {"title": "PhysCodeBench: Benchmarking Physics-Aware Symbolic Simulation of 3D Scenes via Self-Corrective Multi-Agent Refinement", "abstract": "Physics-aware symbolic simulation of 3D scenes is critical for robotics, embodied AI, and scientific computing, requiring models to understand natural language descriptions of physical phenomena and translate them into executable simulation environments. While large language models (LLMs) excel at general code generation, they struggle with the semantic gap between physical descriptions and simulation implementation. We introduce PhysCodeBench, the first comprehensive benchmark for evaluating physics-aware symbolic simulation, comprising 700 manually-crafted diverse samples across mechanics, fluid dynamics, and soft-body physics with expert annotations. Our evaluation framework measures both code executability and physical accuracy through automated and visual assessment. Building on this, we propose a Self-Corrective Multi-Agent Refinement Framework (SMRF) with three specialized agents (simulation generator, error corrector, and simulation refiner) that collaborate iteratively with domain-specific validation to produce physically accurate simulations. SMRF achieves 67.7 points overall performance compared to 36.3 points for the best baseline among evaluated SOTA models, representing a 31.4-point improvement. Our analysis demonstrates that error correction is critical for accurate physics-aware symbolic simulation and that specialized multi-agent approaches significantly outperform single-agent methods across the tested physical domains.", "tldr": "", "keywords": ["Physical Simulation", "Code Generation", "Multi-Agent Systems", "Benchmark Dataset"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b305d9f23ffefd05222422879903d7ca96e5a924.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a benchmark for evaluating the quality of code-generation systems for generating accurate physics simulations. A training set is developed for this benchmark, which is then used to fine-tune local models in a new \"self-corrective multi-agent refinement framework\" that outperform SOTA proprietary models on this task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The topic of using code-gen agents to generate physics simulations is very relevant to the AI community, and the benchmark proposed in this paper is a solid contribution to the field.\n- I appreciate the user study, which validated the use of ClipScore as an evaluation metric.\n- The SMRF training pipeline follows standard practices (SFT and DPO), and the ablation study demonstrates the utility of each component."}, "weaknesses": {"value": "- The pipeline for creating the dataset requires significant human oversight, including (1) creating initial seed prompts, (2) filtering AI-generated prompts, (3) validating simulation code, and (4) adding metadata and preference scores. I think this is excellent for validation/test sets, but this somewhat limits the scalability of the training set.\n- Figure 2 is hard to parse without zooming in significantly; the inner text and images are too small."}, "questions": {"value": "- At inference time, each model is provided with 100K tokens of documentation, but the paper also says that the maximimum context length is 32K tokens for the local models. What is actually done at inference time for these local models? Furthermore, it is commonly reported that LLM capabilities degrade at long contexts (along with being wasteful due to the quadratic attention of self-attention); have you tested reducing the length of the documentation or some tool-use system to provide a more manageable context length?\n- Given the success of ClipScore in evaluation, have you tried using it (or other VLMs) as a filtering mechanism for outputs of SOTA models? In other words, generate k different outputs from a SOTA LLM, choose the output that gives valid code and has the best ClipScore. That seems like it could significantly boost scores for proprietary models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rdgxo2xEJ5", "forum": "T9NxKVoqiu", "replyto": "T9NxKVoqiu", "signatures": ["ICLR.cc/2026/Conference/Submission15733/Reviewer_TMKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15733/Reviewer_TMKC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859192598, "cdate": 1761859192598, "tmdate": 1762925973579, "mdate": 1762925973579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel benchmark for simulation curation coding. The benchmark uses genesis simulator and test the ability to produce corresponding physical simulatin given the text prompt. The evaluation proposes a corresponding metric with a combination of code accuracy and visual accuracy. it also provides a dataset, curateing after semi-automatic process, which involves human expert labeling.\nIn addition to the benchmark, the method proposes a framework for finetuning LLM for the simulation coding task. It involves agenerator, a error corrector and involves DPO to improve based on human preference.\nExperiment suggest that the benchmark is challenging and existing pipeline can solve simple cases like water drops."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to understand.\n2. The problem setup is well-defined and the tools for benchmarking is well-provided."}, "weaknesses": {"value": "1. The tasks are relatively simple. For benchmarking, when should include more challenging environment or stratify them to different difficulty level.\n2. The metric for coding / visual is more qualitative. Looking similar does not emphasize accuracy of the code. For example, if i want to have a robot with 4 legs and it give me 5 legs. One should certainly punish such serious errors.\n3. Limited to Genesis. A comprhensive benchmark should also consider other platforms like IsaacSim."}, "questions": {"value": "1. How large is the dataset? How much effort is cost to build the dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fAzeCxtLcO", "forum": "T9NxKVoqiu", "replyto": "T9NxKVoqiu", "signatures": ["ICLR.cc/2026/Conference/Submission15733/Reviewer_ezgr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15733/Reviewer_ezgr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861213922, "cdate": 1761861213922, "tmdate": 1762925973127, "mdate": 1762925973127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to solve the \"semantic gap\" that Large Language Models (LLMs) face when translating natural language descriptions into physically accurate 3D simulation code. The authors point out that code generated by existing LLMs often leads to simulation failures, bugs, or incorrect physical parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- First-of-its-Kind Benchmark: PhysCodeBench is the first comprehensive benchmark in this domain. It provides not only a dataset but also detailed metadata (like difficulty and physical laws), laying a foundation for future research .\n- Comprehensive Evaluation: The paper conducts not only quantitative analysis but also uses qualitative comparisons (Figure 5) to demonstrate SMRF's superiority in simulating fluid ripples and complex collapse dynamics . Furthermore, a user study with 10 participants was conducted, validating SMRF's lead in physical accuracy and code usefulness."}, "weaknesses": {"value": "- Reliance on High-Level APIs: The entire benchmark and framework are heavily dependent on a single physics engine named \"Genesis\". The model primarily learns how to correctly call this specific library's API, rather than how to implement physics simulations from first principles (e.g., physical equations) .\n- Superficial Evaluation Metrics: The PhysCodeEval evaluation framework (100 points) may fail to measure true physical accuracy.\n  - Code Quality (50 points): Only evaluates whether the code can \"successfully execute\" and \"generate files\", not code efficiency or structural quality.\n  - Simulation Fidelity (50 points): Relies on proxy metrics. S_clip measures \"semantic similarity\" between the video and text (e.g., if \"ball\" and \"trampoline\" are present), not physical correctness (e.g., if the ball's bounce follows Hooke's Law) . S_motion only assesses \"motion smoothness\" ; a simulation that is completely wrong physically (e.g., no acceleration) could still be \"smooth\"."}, "questions": {"value": "- Generalization: If the SMRF framework were applied to a completely new physics engine (e.g., MuJoCo or PyBullet), would it fail completely? How much of the knowledge learned by the framework is \"physics logic\" versus \"Genesis API syntax\"?\n- Verification of Physical Laws: Given that PhysCodeEval relies on proxy metrics, to what extent do the simulations generated by SMRF truly adhere to core physical laws (e.g., conservation of energy, conservation of momentum)? Has any quantitative verification of this been performed?\n- Robustness of Correction: How does the Error Corrector (EC) handle errors that are \"physically unreasonable but syntactically correct\" (e.g., setting gravity to an unrealistic value)? How strong is its diagnostic capability for this type of semantic error?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NGlBaLQZEs", "forum": "T9NxKVoqiu", "replyto": "T9NxKVoqiu", "signatures": ["ICLR.cc/2026/Conference/Submission15733/Reviewer_xguB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15733/Reviewer_xguB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896981737, "cdate": 1761896981737, "tmdate": 1762925972616, "mdate": 1762925972616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PhysCodeBench, a benchmark for evaluating physics-aware symbolic simulation, a code gen problem where models must generate executable simulation code from natural language descriptions. The benchmark provides a dataset of 700 human-selected prompts and corresponding code for physical simulation scenarios across rigid and soft body simulation, fluid dynamics, and mechanics. The authors further propose the Self-Corrective Multi-Agent Refinement Framework (SMRF), which includes 3 modules: the Simulation Generator (generates initial code), Error Corrector (fix execution errors), and Simulation Refiner (improve code). The authors test SMRF on PhysCodeBench, where it achieves better performance compared to zero-shot and single-agent finetuned baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The Simulation Generator, Error Corrector, and Simulation Refiner framework is interesting, and their respective optimization procedures are presented clearly\n- The data collection procedure is covered in detail in the appendix\n- The authors evaluate against a number of relevant baselines, demonstrating improved performance with the multi-agent refinement framework"}, "weaknesses": {"value": "- Baselines include zero-shot and finetuned models, and ablations remove the Error Corrector or Simulation Refiner individually, however the baselines do not include a refinement framework which leverages a single model to fix and refine the generated code given error descriptions\n- The human preference study has a fairly small sample size (10 participants)"}, "questions": {"value": "- How well does a single-agent iterative refinement framework perform compared to the SMRF framework?\n- The authors write, \"Robotics applications like SimGen (Zhou et al., 2024), VoxPoser (Huang et al., 2023), and Code as Policies (Arenas et al., 2024) generate simulation environments for robot task planning.\" However, to my knowledge, neither of these works generate simulation environments. Code as Policies writes policies with hierarchical code generation, and VoxPoser extracts affordance and constraint maps via LLM-generated code. Can the authors clarify what is meant by this statement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qh00h9VM5g", "forum": "T9NxKVoqiu", "replyto": "T9NxKVoqiu", "signatures": ["ICLR.cc/2026/Conference/Submission15733/Reviewer_LEZS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15733/Reviewer_LEZS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993192065, "cdate": 1761993192065, "tmdate": 1762925972270, "mdate": 1762925972270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}