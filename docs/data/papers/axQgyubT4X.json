{"id": "axQgyubT4X", "number": 23489, "cdate": 1758344569055, "mdate": 1759896812143, "content": {"title": "On the Role of Preference Variance in Preference Optimization", "abstract": "Direct Preference Optimization (DPO) has emerged as an important approach for learning from human preferences in aligning large language models (LLMs). However, collecting human preference data is costly and inefficient, motivating methods to reduce the required annotations. In this work, we investigate the impact of \\emph{preference variance} (PVar), which measures the variance in model preferences when comparing pairs of responses, on the effectiveness of DPO training. We provide a theoretical insight by establishing an upper bound on the DPO gradient norm for any given prompt, showing it is controlled by the PVar of that prompt. This implies that prompts with low PVar can only produce small gradient updates, making them less valuable for learning. We validate this finding by fine-tuning LLMs with preferences generated by a reward model, evaluating on two benchmarks (AlpacaEval 2.0 and Arena-Hard). Experimental results demonstrate that prompts with higher PVar outperform randomly selected prompts or those with lower PVar. We also show that our PVar-based selection method is robust, when using smaller reward models (1B, 3B) for selection. Notably, in a separate experiment using the original human annotations from the UltraFeedback dataset, we found that training on only the top 10\\% of prompts with the highest PVar yields better evaluation performance than training on the full dataset, highlighting the importance of preference variance in identifying informative examples for efficient LLM alignment.", "tldr": "", "keywords": ["Direct Preference Optimization", "LLM Alignment"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49b0b04d65f043b683a2703e02d5d62b6bcc5190.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the role of *preference variance (PVar)*, which measures the variance of pairwise reward gaps among responses to the same prompt. The authors hypothesize that higher PVar indicates greater disagreement or uncertainty in the reward model’s judgments and thus corresponds to more informative training samples. They present theoretical analysis showing the connection between PVar and the gradient norm in DPO, and demonstrate that selecting prompts with high PVar can improve data efficiency and performance in preference optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper has a clear and intuitive idea. The notion of using the variance of preference scores to assess the information value of samples is straightforward but meaningful, and the motivation is well explained.\n2. Theoretical analysis is carefully developed. The connection between PVar and the DPO gradient provides a solid justification for why such a measure can guide data selection. The proofs and reasoning are sound and complete.\n3. The paper is clearly written and well structured, making it easy to follow the logic from definition to empirical validation."}, "weaknesses": {"value": "### \n\n1. It is not entirely clear whether the observed advantage of PVar generalizes beyond conversational or instruction-following settings. The authors could discuss whether the same approach would still be effective for other domains such as coding or mathematical reasoning, where the response space is less diverse.\n2. The analysis and experiments are all based on DPO. It would be helpful to discuss whether PVar could also be applied in other preference alignment algorithms, such as KTO, SimPO, or ORPO.\n3. The filtering seems to be done at the prompt level: once a prompt is selected as top-PVar, all its response pairs are retained. This is a reasonable first step, but it may not be as fine-grained as pair-level filtering methods. Prior work such as *Filtered Direct Preference Optimization (Morimura et al., 2024)* performs selection directly at the response pair level and might serve as a stronger comparison baseline.\n4. The current formulation uses an external reward model to measure uncertainty. There is related work, such as *Uncertainty-Aware Iterative Preference Optimization for Enhanced LLM Reasoning (ACL 2025)*, which instead estimates uncertainty from the model’s own predictions. Discussing this contrast between external and internal uncertainty estimation would strengthen the related work section."}, "questions": {"value": "Please see the “Weaknesses” section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CfPAHhKlvC", "forum": "axQgyubT4X", "replyto": "axQgyubT4X", "signatures": ["ICLR.cc/2026/Conference/Submission23489/Reviewer_UBZj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23489/Reviewer_UBZj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621978266, "cdate": 1761621978266, "tmdate": 1762942681262, "mdate": 1762942681262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed a data filtering mechanism based on Preference variance (PVar) for selecting high impact sample for preference optimization. The authors proved that low PVar prompts have low upper bound for gradient norm and become less efficient samples in preference optimizations. Through experiments, PVar is proved to be robust and effective in filtering highly effective samples using Llama 3.1-8B-Instruct as base model on AlpacaEval 2.0 and Arena Hard."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Theorems 1 and 2 formalize the intuition that prompts yielding diverse responses are particularly effective for training, which is reflected in the derived inequalities.\n* The implementation of this PVar based data filtering is easy for practitioners to implement\n* Comprehensive experiments covering two base models and reward models of different sizes."}, "weaknesses": {"value": "* I agree that low PVar implies low gradient norm and could be filtered out for more efficient training. However, the theoretical upper bounds could only say that High PVar implies potentially high gradient norms. Though the experiments suggests that filtering out low PVar samples improves performance, the theoretical framework only explains why we should drop low PVar samples but not why should we keep high PVar samples, limiting the theoretical contribution.\n\n* There's a lack ablations study on the number of samples for computing PVar and the data split ratio. The authors choose 5 samples per prompt, 50% cutoff for standard data and 10% for human annotated data. A few sets of experiments tweaking these hyper-parameters would be helpful.\n\n* Line 418-419 is a bit unclear, \"This baseline selects the top 50% of prompts with the\nlargest reward difference between any two responses\", is the difference computed on the same set of responses as the PVar filtering?"}, "questions": {"value": "Please see Weaknesses sections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2LgnrqiQzP", "forum": "axQgyubT4X", "replyto": "axQgyubT4X", "signatures": ["ICLR.cc/2026/Conference/Submission23489/Reviewer_bZmS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23489/Reviewer_bZmS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961943454, "cdate": 1761961943454, "tmdate": 1762942680878, "mdate": 1762942680878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the impact of preference variance on DPO training of large language models (LLMs). The authors mathematically demonstrate that preference variance (PVar) can serve as an upper bound of the DPO gradient, and empirically show that filtering preference data based on high PVar values leads to better performance than using the entire dataset for DPO training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed concept is easy to adapt across various scenarios, making it a practical. It does not require a large computing cost and is not mathematically complex.\n\n2. The authors conduct experiments on a wide range of models, including LLaMA and Mistral, as well as datasets such as UltraFeedback and Chatbot Arena, with evaluations on major benchmarks like AlpacaEval 2.0 and Arena-Hard. These results demonstrate that the proposed method is not limited to a specific model or dataset, but rather represents a general and robust approach applicable to diverse alignment settings."}, "weaknesses": {"value": "1. While the paper claims that PVar determines the upper bound of the DPO gradient, this does not necessarily guarantee higher gradients in practice. Moreover, the paper presents faster loss convergence as empirical evidence, but this cannot be considered direct proof of the claim. The convergence of loss is also influenced by factors such as gradient direction consistency and data sample distribution. Without additional analyses on these aspects, the paper’s argument appears somewhat overstated or incomplete. Additional analysis is needed to clarify the underlying mechanism through which PVar enhances data filtering performance.\n\n2. There is a lack of baseline. For example, other statistical indicators similar to PVar, such as perplexity, are also known to be effective for data filtering and often exhibit similar tendencies to variance-based measures. It would strengthen the paper to include comparisons with such well-established statistical metrics, providing a clearer understanding of what makes PVar distinct or superior.\n\n3. The paper lacks hyperparameter search. If PVar indeed influences the DPO gradient, then optimal hyperparameters (e.g., learning rate, beta value for DPO) are likely to vary depending on the dataset. Since the experiments were conducted using a single hyperparameter configuration, it is difficult to conclude whether the PVar-filtered data is intrinsically better or simply better suited to that specific setup."}, "questions": {"value": "1. I am curious how effective the PVar-filtered data would remain when applied to a different model from the one used during the filtering process?\n2. Can PVar be applied in SFT learning?\n3.  I also wonder whether the proposed method would remain effective for other preference learning algorithms, such as SimPO or similar approaches beyond DPO."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1uI0vBaOIK", "forum": "axQgyubT4X", "replyto": "axQgyubT4X", "signatures": ["ICLR.cc/2026/Conference/Submission23489/Reviewer_YE9r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23489/Reviewer_YE9r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966656122, "cdate": 1761966656122, "tmdate": 1762942680497, "mdate": 1762942680497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Recent work in RLHF for LLMs has highlighted the importance of reward variance as a key metric for achieving effective alignment. Building on this idea, this paper introduces the concept of variance in preference strength, defined under a given reward model as the variance of the logistic-transformed difference between rewards of different generations. The paper provides theoretical justification for using this variance as a selection mechanism, helping identify promising pairs of samples for offline alignment methods such as DPO, thereby improving sample efficiency. Empirically, the proposed approach outperforms baselines that use either random selection or subsets with lower variance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is overall fairly easy to parse, and I appreciate the proofs, mostly following easy statistical results (which is not a bad thing). I further appreciate the real subset selection simulation for alignment with DPO using humans as labels."}, "weaknesses": {"value": "### Weaknesses and Suggestions\n\nWhile the paper presents an interesting and well-motivated idea, there are several experimental limitations that currently restrict the overall scope of the contribution. The following suggestions could help strengthen the work:\n\n1. **Code Data Evaluation**  \n   Including code datasets such as **HumanEval** could meaningfully enhance the contribution. Code generation tasks often exhibit higher reward variance, making them well-suited for the proposed method. Moreover, prior works such as **CodeDPO**, **Focused-DPO**, and **Code-Optimise** have shown measurable improvements on HumanEval through preference-based training alone. Adding this comparison would help position the paper more competitively.\n\n2. **Ablations and Correlation Studies**  \n   It would be valuable to compute the correlation between the proposed variance metric and training loss after a few warm-up steps for each datapoint. This could reveal whether the metric aligns with early learning dynamics. Additionally, an experiment replacing the external reward model with an intrinsic LLM reward could offer insight into generality.  \n   The authors could also report **sample efficiency curves** (e.g., from 10% to 50% of the data), as is common in active learning literature, instead of only presenting results at a single data fraction.\n\n3. **Data Subset Selection Baselines**  \n   The paper would benefit from comparing against diversity-based subset selection methods—such as **coreset selection** or **clustering-based approaches**—to better contextualize the proposed variance-based selection strategy.\n\n4. **Correlation Analysis**  \n   It would be informative to analyze how strongly **reward difference** correlates with the proposed **Preference Variance (PVar)** metric. This could clarify whether PVar captures additional signal beyond what is already reflected in the raw reward differences."}, "questions": {"value": "Refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f3bp0HgWg4", "forum": "axQgyubT4X", "replyto": "axQgyubT4X", "signatures": ["ICLR.cc/2026/Conference/Submission23489/Reviewer_JLpC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23489/Reviewer_JLpC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762034487448, "cdate": 1762034487448, "tmdate": 1762942680064, "mdate": 1762942680064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}