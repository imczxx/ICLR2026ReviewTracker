{"id": "e5OrurYW07", "number": 14487, "cdate": 1758237104970, "mdate": 1759897367478, "content": {"title": "PULSE: Benchmarking Large Language Models for ICU Time Series Classification", "abstract": "Large language models (LLMs) are increasingly used on multimodal clinical data, yet their performance on high-stakes ICU time series lacks a standardized evaluation. We introduce PULSE, a comprehensive LLM benchmark for intensive care unit (ICU) time series classification. PULSE evaluates 17 models, strong conventional learners, deep learning, and instruction-following LLMs under nine prompting/agentic strategies, across three datasets (HiRID, MIMIC-IV, eICU) and three clinical endpoints (mortality, sepsis, acute kidney injury). Across settings, well-tuned conventional models, especially gradient-boosted trees, remain the most robust and best-performing, reaching AUROCs up to 0.906. Frontier LLMs approach this level (best 0.889 AUROC, OpenAI o3) but show higher variance and sensitivity to prompting. A hybrid agentic approach that grounds LLM reasoning in a conventional model’s predictions consistently narrows the gap while also providing human-readable explanations at inference time. Notably, on several inferences, LLMs are competitive with classic models despite not being trained on multimodal time-series data. PULSE provides all code, configuration files, and a public results dashboard to enable transparent, reproducible comparison and rapid community extension. We expect PULSE to serve as a common yardstick for developing safer, more reliable LLMs for multimodal time series data, in critical care.", "tldr": "The First LLM Benchmark for ICU Time Series", "keywords": ["icu", "intensive care unit", "clinical", "benchmark", "large language model", "llm", "agents", "evaluation", "classification"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1516d76a8697b1dd30bbc354afae38a80a4ffa4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a framework to evaluate the ability of LLMs to predict clinical outcomes in the ICU based on preprocessed clinical time series data. It leverages an existing multicentre harmonisation pipeline and adds an LLM prompt module. Referencing known issues with LLM calibration, the study proposes novel metrics to evaluate LLM predictions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The study builds on top of an established benchmark across multiple hospitals and clinician-designed clinical outcomes.  \n- The authors evaluate a range of state-of-the-art proprietary LLMs as well as open source alternatives."}, "weaknesses": {"value": "- The motivation for this study is unclear. If data has already been extensively preprocessed, is it surprising that a specifically trained XGBoost outperforms the LLM?\n- If—as the discussion alludes to—the motivation is instead grounded in the potential off-the-shelf performance of LLMs to be used until enough local data is available to train a dedicated XGBoost model, then the authors should include experiments that thoroughly investigate this aspect. The multicentre nature of the underlying YAIB framework would allow for such experiments. \n- If—as the last section of the results alludes to—the motivation lies in obtaining humand-readable explanations, then the authors should include a thorough assessment of the model explanations that goes beyond a not further described qualitative analysis of the Hybrid Reasoning Agent, most of which appeared to defer to the underlying XGBoost score as an explanation."}, "questions": {"value": "- The presented Summary Agent and Hybrid Reasoning Agent aren't agents or agentic approaches under standard definitions. They operate on predefined rules and have a hard-coded control flow. They do not interacte with the environment or themselves decide on tool use. For an agent, the LLM would itself decide which data to fetch, if and which conventional ML model to call, etc. \n- I do not understand why the PULSE score and its arbitrary combination of AUC, AUPRC, and MCC was introduced. I also couldn't find a statement on which weights were used in the experiments or why.\n- The performance metrics for some models warrant scrutiny. For example in Figure 2, GRU performs particularly bad for sepsis (AUC ~0.33) while its close sibling LSTM is best-in-task. This is unexpected and differs  from the original YAIB results, where LSTM and GRU are relatively closely matched across most tasks. To give another example from Figures 32  and 33, Gemma, Mistral, and to a lesser extent Llama all have poor AUC of ~0.5 for predicting sepsis in eICU but a corresponding AUPRC ~0.5, which is an extremely high AUPRC given the low prevalence of sepsis and AUPRC < 0.1 seen in other models. It is not clear how this can be the case."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kxPTPlCahy", "forum": "e5OrurYW07", "replyto": "e5OrurYW07", "signatures": ["ICLR.cc/2026/Conference/Submission14487/Reviewer_eFcc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14487/Reviewer_eFcc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761043608009, "cdate": 1761043608009, "tmdate": 1762924888353, "mdate": 1762924888353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper measures the accuracy of conventional ML methods, deep learning, and LLMs for the task of predicting various labels for patients based on ICU time series."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The logic behind the research is sound, and the design of methods and experiments appears valid. There are no obvious mistakes. The paper is written well and easy to read, with only a very few English errors."}, "weaknesses": {"value": "The findings are not surprising, nor are the methods. The authors did not take the opportunity to go beyond a correct but standard and relatively superficial understanding of the ICU prediction domain.\n\nLine 418 has a valid and interesting observation: \"because one-shot and few-shot LLM approaches require no site-specific training, they are a pragmatic “day-zero” option for hospitals with limited labeled data or MLOps capacity.\" For ICU prediction to be useful outside research, it must be applicable in hospitals not used for research. LLMs can make predictions for such hospitals. Conventional ML models can do so also, but when they are trained on data from specific hospitals, other hospitals are out-of-domain. The work in this paper would be more interesting with cross-hospital results for the conventional ML methods. If a model is trained on two of the three HiRID, MIMIC-IV, and eICU, how accurate is it on the out-of-domain third? Can the 200 eICU hospitals be separated, to evaluate out-of-domain accuracy on each after training on the 199 others?\n\nThe submission praises the previous benchmark YAIB but says it was not designed to accommodate LLMs. This is true, but why not extend YAIB? That would increase comparability with previous results, compared to creating a new benchmark, as done in this submission.\n\nThe paper should be more incisive in highlighting the limitations of LLMs for prediction. When given GBDT predictions as input, the LLMs provide plausible-sounding explanations, but their accuracy is worse than the accuracy of the GBDT predictions provided as input! The explanations appear sensible, but the null hypothesis has to be that they have no actual factual value.\n\nLike most AI papers nowadays, this one has very few references that are more than just a few years old. The lessons of older research are neglected, or assumed to be recent. In particular, the finding that conventional ML methods are more accurate than linguistic methods mirrors findings that date back many decades that humans are less accurate than data-driven methods. See among many other papers:\n\nDawes & Corrigan (1974), “Linear models in decision making,” Psychological Bulletin\n\nWG Baxt. “Prospective validation of artificial neural network trained to identify the presence of acute myocardial infarction.” The Lancet. 1996."}, "questions": {"value": "Please explain where you disagree with the weaknesses discussed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iJzqpeDNeE", "forum": "e5OrurYW07", "replyto": "e5OrurYW07", "signatures": ["ICLR.cc/2026/Conference/Submission14487/Reviewer_9EQn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14487/Reviewer_9EQn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761437862463, "cdate": 1761437862463, "tmdate": 1762924887614, "mdate": 1762924887614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents PULSE, a benchmark to compare conventional ML, deep learning, and large language models on ICU time‑series for three clinically important tasks: hospital mortality, acute kidney injury, and sepsis. It standardizes datasets, preprocessing, prompting/agent workflows, metrics (including a reliability‑aware PULSE Score) and reports cost/latency trade‑offs alongside accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation, the work fills the gap between text‑centric LLM benchmarks and tabular ICU benchmarks by a head‑to‑head evaluation on structured time‑series. \n-  A strong side is the pipeline that prevents leakage: harmonization, hourly resampling, explicit missingness indicators, forward‑fill plus train‑set imputation, and chronological splits that can reflect realistic deployment.\n- The benchmark covers a fair number of datasets, 3 public ICU cohorts, and a wide range of models (17).\n- Hybrid Reasoning Agent grounds the LLM in XGBoost risk and top features which is an interesting approach. \n- Operational realism, the paper reports token usage, latency, throughput, and a cost-performance Pareto analysis that helps choose optimal deployments. \n- Artifacts, configurations, predicted outputs, and an interactive leaderboard are planned for public release."}, "weaknesses": {"value": "- All most important results use a limited test subset (100 stays x 10 windows per dataset), which reduces statistical power and may change rankings and cost Pareto frontiers on full test sets.\n- Figures lack uncertainty quantification, no CI or error bars.\n- PULSE score inconsistency: the metric penalizes models that are LLMs differently from no-LLM, the authors have to elaborate on that. \n- Baselines can be under‑tuned, ConvML and ConvDL mostly use defaults without dataset‑specific HPO which may understate baselines or distort comparisons. \n- The codebase was not included in the supplementary materials of the review, therefore I was unable to assess how easy it is to run the benchmark which is an important aspect for the community."}, "questions": {"value": "- How does the PULSE Score’s CCF asymmetry affect cross-family comparisons and leaderboard fairness? Please consider comparing the 3 policies and reporting their impact or discuss them: (a) discard cases where an LLM’s label contradicts its probability, (b) ignore the label text and evaluate all models solely on probabilities (derive labels server-side at a fixed threshold), (c) add a unified consistency/calibration penalty that applies to all models. \n- What α/β/γ weights were actually used in the PULSE metric and how sensitive is the leaderboard to them? \n- How do LLM results change if you serialize timestamps (possibly truncated or selected) instead of min/mean/max summaries under the same token budget? Now the Hybrid Agent takes XGBoost probabilities computed from raw numeric inputs, while LLMs see only aggregated values. For fairness, could you test serialized numeric tables or raw-value snippets for LLMs so comparisons are apples-to-apples?\n\nMinor:\n- Line 144, typo: basline -> baseline\n- Line 365 makrer -> marker"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x6vBCCUF81", "forum": "e5OrurYW07", "replyto": "e5OrurYW07", "signatures": ["ICLR.cc/2026/Conference/Submission14487/Reviewer_BTyh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14487/Reviewer_BTyh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946723933, "cdate": 1761946723933, "tmdate": 1762924886972, "mdate": 1762924886972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}