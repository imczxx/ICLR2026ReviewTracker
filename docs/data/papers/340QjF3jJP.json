{"id": "340QjF3jJP", "number": 1352, "cdate": 1756874894270, "mdate": 1759898213582, "content": {"title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head", "abstract": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. **Linear attention** offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution and few self-attention blocks) that defeat the original purpose. In this work, we identify a key failure mode in these methods: **global context collapse**, where the model loses representational diversity. To address this, we propose **Multi-Head Linear Attention (MHLA)**, which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a **3.6%** improvement on ImageNet classification, a **12.6%** gain in image generation tasks with the same computational complexity and a **2.1%** improvement on NLP.", "tldr": "", "keywords": ["Linear Attention", "Model Architecture", "Efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8981f9a29c9e36a9eb09aaafeb559d9cc5c2f53d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses a key limitation in linear attention models, termed global context collapse, where the model’s representational diversity diminishes as sequence length increases.\nTo overcome this, the authors propose Multi-Head Linear Attention (MHLA), which partitions the token sequence into multiple blocks and computes local key–value summaries within each.\nBy introducing a multi-head mixing mechanism that adaptively combines these local summaries, MHLA restores query-conditioned diversity while maintaining linear computational complexity.\nExtensive experiments demonstrate that MHLA achieves significant improvements over existing linear attention baselines in image classification (ImageNet-1K), image generation (DiT and SANA models), and NLP benchmarks, with minimal computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Insightful analysis of linear attention – The authors conduct a rigorous examination of why linear attention underperforms, identifying global context collapse as the root cause. Their analysis of rank deficiency and entropy in attention maps provides a strong theoretical foundation.\n\n2. Innovative multi-head design – MHLA introduces a token-level multi-head mechanism that mixes local key–value summaries through learnable coefficients. This design cleverly restores query-dependent diversity and sparsity while keeping linear complexity.\n\n3. Strong empirical results – On ImageNet classification, MHLA improves accuracy by 3.6% over standard linear attention, achieving state-of-the-art results when integrated into DeiT and VLT architectures. On image generation, it delivers up to 12.6% gains over baselines while matching or surpassing softmax attention performance in large-scale DiT models. On NLP tasks, MHLA yields 2.1% improvements over existing linear attention methods, even without additional positional embeddings."}, "weaknesses": {"value": "### 1. Limited and Inconclusive Experimental Evaluation\nThe major limitation of this paper lies in its lack of comprehensive experiments to support its central claim—improving long-term modeling capability.\nIn the vision domain, experiments are restricted to image classification on 224×224 inputs, which are too short and fail to reflect long-range dependencies.\nIn the NLP domain, the evaluation covers only three small-scale reasoning benchmarks (ARC-c, WinoGrande, CoPA), which do not assess long-context understanding, open-ended text generation, or large-scale language modeling.\nConsequently, it remains unclear whether MHLA truly benefits long-sequence scaling.\nMoreover, the NLP model used (340 M parameters trained on 5 B tokens) is relatively small; while suitable for controlled experiments, it is insufficient to demonstrate MHLA’s effectiveness in large-scale language models, where long-context efficiency becomes critical.\nTo better validate the method’s claims, I recommend incorporating:\n\nVision tasks such as image captioning, VQA, or video captioning, which require multi-modal and temporal reasoning.\n\nNLP datasets such as LongBench, PG19, or BookSum, which directly evaluate scalability and diversity preservation in long-sequence scenarios.\n\n### 2. Incomplete Discussion of the Computational Trade-off\nThe proposed method essentially trades extra computation for improved representational rank.\nThis trade-off is reasonable when the additional $O(M^2d^2)$ cost remains small, but it does not fundamentally solve the scalability problem for very long input sequences. Linear attention mechanisms are expected to handle arbitrarily long contexts, yet MHLA’s computational complexity scales with block size $M$, limiting its practicality for extremely long sequences.\nThe paper should more clearly analyze this scaling limitation and discuss how MHLA behaves as sequence length increases.\n\n### 3. No Throughput or Speed Analysis\nThe paper does not report throughput, latency, or efficiency comparisons.\nAlthough MHLA maintains linear complexity in theory, the method introduces an additional $O(M^2d^2)$ term due to multi-head block mixing.\nThis overhead may be negligible in small-scale settings like image classification or ARC-c, but it could become significant in truly long-sequence scenarios.\nWithout explicit runtime or memory benchmarks, the paper fails to demonstrate that MHLA preserves an effective balance between accuracy and computational cost under long-context conditions.\n\n### 4. Weak Literature Review and Terminology Issues\nThe paper’s literature review is notably shallow.\nThe introduction and related work sections lack a comprehensive discussion of the extensive prior research on linear attention, a highly active field with numerous variants and analyses.\nThe authors should consult and reference existing survey papers on linear attention to position MHLA more accurately within the broader landscape.\nAdditionally, the writing style introduces several non-standard or self-coined terms—such as “adapt to each query individually” and “KV summary”—which are not widely adopted in the community.\nThe authors are encouraged to revise these descriptions using standard terminology and provide clearer, community-recognized definitions to improve readability and academic rigor."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gtTUCQPH6w", "forum": "340QjF3jJP", "replyto": "340QjF3jJP", "signatures": ["ICLR.cc/2026/Conference/Submission1352/Reviewer_2bkY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1352/Reviewer_2bkY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783435613, "cdate": 1761783435613, "tmdate": 1762915745880, "mdate": 1762915745880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a failure mode of many kernelized/linear-attention variants namely “global context collapse,” where compressing all keys/values into a single global summary limits rank and drives attention toward near-uniform distributions as sequence length grow . \n\nAutores proposes Multi-Head Linear Attention (MHLA), which partitions tokens into non-overlapping blocks and learns a block-wise mixing of local KV summaries via a nonnegative, normalized coefficient matrix, restoring query-conditioned diversity while keeping linear-time scaling . The authors derive rank bounds showing MHLA can achieve substantially higher attainable rank than global linear attention and empirically observe lower entropy (sharper sparsity) of attention maps ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear diagnosis of “global context collapse” with complementary theoretical indicators, e.g. rank upper bound ≤d for global linear attention; MHLA’s additive blockwise rank potential;  and empirical entropy analyses .\n\n\n- Simple, hardware-friendly construction: blockwise summaries + learned nonnegative mixing; retains linear-time leading term and is compatible with chunkwise/streaming execution .\n\n\n- Strong cross-domain results: (i) ImageNet improvements over linear attention baselines and competitive with transformer/mamba-style models, (ii) sizable FID gains across DiT scales (including XL) with near-linear-attention throughput, and (iii) modest yet positive NLP benchmarks without positional embeddings .\n\n\n- Useful ablations: initialization vs learnable mixing and head-number sensitivity give practical guidance (e.g., M=16 works well) ."}, "weaknesses": {"value": "- Results are mostly single-number comparisons; there is no reporting of multiple seeds, confidence intervals, or significance tests. For diffusion FID/IS/sFID and ImageNet top-1, please consider report mean±std over ≥3 seeds and specify sample counts and evaluation protocols used for FID/IS (e.g., 50k samples, classifier, resize method) to support the claims of consistent improvements .\n\n\n- Language modeling evaluation is narrow. The 0.3–0.34B model trained on 5B tokens is assessed on a small suite (ARC-c, Wino, CoPA). Please add perplexity on standard corpora and long-context stress tests to directly evaluate the purported benefits at long sequence lengths, and compare to strong linear/SSM baselines under matched budgets .\n\n\n- Complexity & memory overhead clarity. The method introduces an M×M mixing step with O(M²d²) cost. While argued negligible when M²≤N, the paper would benefit from explicit profiling of wall-clock time and peak memory versus N and M across hardware and tasks, and from reporting the realized M values used in each experiment (classification/generation/NLP) .\n\n\n- Streaming/causal claims are not empirically demonstrated. The text asserts compatibility with chunkwise parallel training and streaming/stateful execution; please provide experiments demonstrating long-context streaming inference throughput/latency versus linear attention baselines under causal masking .\n\n\n- Comparisons breadth in T2I/C2I. The DiT/DiG comparisons are strong, but additional comparisons to other recent linear or hybrid attention mechanisms in diffusion backbones would strengthen the case, especially where prior methods also raise rank or sparsify attention (the paper cites some but not all are evaluated directly) .\n\nAdditionally, there are minor typos (e.g., “diversty”, “Iamge”)."}, "questions": {"value": "How are the mixing coefficients parameterized to ensure nonnegativity and row normalization in practice (softmax per row, or other), and how sensitive are results to this choice and to the locality-biased initialization? What values of M were used per model and task, and how does wall-clock time and peak memory scale with N and M on H100 across DeiT/DiT/NLP? Please include profiles showing the point where O(M²d²) becomes non-negligible .\n\n\nAlso, can you provide multi-seed runs (≥3) with mean±std for ImageNet accuracy and DiT FID/IS/sFID, including sample counts and evaluation details?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OoNFH1nojT", "forum": "340QjF3jJP", "replyto": "340QjF3jJP", "signatures": ["ICLR.cc/2026/Conference/Submission1352/Reviewer_uTES"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1352/Reviewer_uTES"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824580144, "cdate": 1761824580144, "tmdate": 1762915745690, "mdate": 1762915745690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper raises a failure mode in linear attention, which the authors term \"global context collapse.\" They argue that compressing the entire sequence into a single, shared key-value summary leads to a severe bottleneck in representational capacity, which they quantify through rank and entropy analysis of the attention matrix. To address this issue, this paper proposes Multi-Head Linear Attention (MHLA), a new mechanism that partitions the input sequence along the token dimension into multiple blocks. MHLA computes local summaries for each block. For each query block, it then constructs a query-conditioned context by computing a learnable, weighted mixture of all local summaries. The authors validate MHLA across diverse domains, including image classification, image generation, and natural language processing, demonstrating competitive performance gains over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The diagnosis of \"global context collapse,\" supported by a concise analysis of rank deficiency and entropy elevation, provides intuitive motivation for the work. It clearly articulates why previous linear attention models often underperform.\n2. The paper presents extensive experiments across multiple domains (computer vision and NLP)."}, "weaknesses": {"value": "1. The choice of the term \"Multi-Head\" is confusing and conflicts with the well-established definition from original Transformer, which refers to splitting the channel dimension. In this paper, \"heads\" are defined along the token/spatial dimension. This non-standard usage could lead to significant confusion in the community. \n2. The paper misses some important comparisons with highly relevant prior work such as FLASH[1] and VOLO[2], which also employs a block-wise strategy combining quadratic and linear attention. For example, VOLO-D1 (comparable to DeiT-S) achieves 84.2 on ImageNet, which substantially higher than the MHLA's 81.0. It is worth noting that VOLO-D1 was proposed four years ago.\n\n\n[1] Transformer Quality in Linear Time, ICML 2022\n\n[2] VOLO: Vision Outlooker for Visual Recognition TPAMI"}, "questions": {"value": "1. How does MHLA compare, conceptually and empirically, to other block-wise attention strategies? For instance, how does its \"soft mixing\" of all block summaries contrast with the approach in FLASH, which uses a hybrid of quadratic intra-block and linear inter-block attention?\n2. The authors report the “Throughput of DiT-S/2 with 4096 resolution” in Figure 1(d), but no experimental generation results at 4096×4096 resolution are provided in the paper. Presenting throughput for a resolution that was not actually evaluated risks implying overclaimed contribution and should be supported with real experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v0bgbYvp5F", "forum": "340QjF3jJP", "replyto": "340QjF3jJP", "signatures": ["ICLR.cc/2026/Conference/Submission1352/Reviewer_WAkW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1352/Reviewer_WAkW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970057484, "cdate": 1761970057484, "tmdate": 1762915745493, "mdate": 1762915745493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}