{"id": "y6yrEPkSKM", "number": 8257, "cdate": 1758076552308, "mdate": 1759897795892, "content": {"title": "Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning", "abstract": "Vector Quantization (VQ) has recently emerged as a promising approach for learning discrete representations of graph-structured data. However, a fundamental challenge, i.e., codebook collapse, remains underexplored in the graph domain, significantly limiting the expressiveness and generalization of graph tokens. In this paper, we present the first empirical study showing that codebook collapse consistently occurs when applying VQ to graph data, even with mitigation strategies proposed in vision or language domains. To understand why graph VQ is particularly vulnerable to collapse, we provide a theoretical analysis and identify two key factors: early assignment imbalances caused by redundancy in graph features and structural patterns, and self-reinforcing optimization loops in deterministic VQ. To address these issues, we propose RGVQ, a novel framework that integrates graph topology and feature similarity as explicit regularization signals to enhance codebook utilization and promote token diversity. RGVQ introduces soft assignments via Gumbel-Softmax reparameterization, ensuring that all codewords receive gradient updates. In addition, RGVQ incorporates a structure-aware contrastive regularization to penalize the token co-assignments among dissimilar node pairs. Extensive experiments demonstrate that RGVQ substantially improves codebook utilization and consistently boosts the performance of state-of-the-art graph VQ backbones across multiple downstream tasks, enabling more expressive and transferable graph token representations.", "tldr": "", "keywords": ["Vector Quantization", "Graph Neural Networks", "Graph Representation Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1436a639584be1ff9ec0278fc09fd0590a5f3f16.pdf", "supplementary_material": "/attachment/0be0c8e88dc56736b9c0024eaf3a6e54e0cf60a8.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates why vector quantization (VQ) methods often fail when applied to graph representation learning, exhibiting severe *codebook collapse*, where most tokens remain unused.\nThe authors identify two intrinsic causes:\n(1) graph-structured data exhibit strong feature redundancy and non-i.i.d. sampling, which biases early token assignments, and\n(2) the standard VQ training objective reinforces this imbalance via a self-amplifying loop.\n\nTo address this, the paper proposes Regularized Graph Vector Quantization (RGVQ).\nThe method introduces (i) a Gumbel-Softmax reparameterization to allow differentiable soft token assignment, ensuring all codewords receive gradient updates, and (ii) a structure-aware regularization that leverages graph topology and feature similarity to encourage balanced token utilization.\nExtensive experiments on standard benchmarks demonstrate that RGVQ can improve codebook utilization and downstream graph learning performance over existing Graph-VQ methods.\n\nOverall, the paper offers a clear theoretical analysis of why codebook collapse arises in graph domains and provides an elegant, empirically supported remedy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* **Clear motivation and theoretical insight:** The authors provide a compelling and formal explanation of why VQ collapses more severely on graphs than on i.i.d. data, linking structural redundancy and non-independent sampling to early token imbalance.\n* **Strong empirical validation:** RGVQ consistently improves codebook perplexity and downstream performance across multiple benchmarks and graph architectures.\n* **General applicability:** The approach is modular and can be integrated into various Graph-VQ frameworks.\n* **Theoretical–empirical alignment:** The analysis of token-usage distribution supports the proposed mechanism and the effectiveness of the regularization."}, "weaknesses": {"value": "* **Scalability limitations:** The experiments are conducted on medium-sized graphs (e.g., Cora, PubMed, Amazon). It is unclear whether RGVQ can scale to very large graphs or industrial settings where full pairwise structural regularization becomes costly.\n* **Limited baselines:** Although several Graph-VQ methods are compared, broader baselines such as tokenization-based GNN compression or self-supervised graph representation models could provide stronger context.\n* **Ablation depth:** The paper demonstrates the importance of each component, but further analysis (e.g., computational overhead, impact of temperature τ on performance, not only perplexity) would strengthen the empirical claims.\n* **Generality of the “natural regularization” claim:** The idea that “GRAPH ISANATURALREGULARIZATION” is conceptually appealing, but its formal definition and theoretical boundary could be discussed more precisely.\n* The appendix includes a useful computational complexity analysis for the structure-aware regularization, which helps clarify scalability concerns. However, the current appendix hyperlinks seem broken or unreferenced, making it difficult for readers to locate those details. It would be helpful to fix cross-references and explicitly summarize the main complexity results in the main text for better readability."}, "questions": {"value": "1. Have the authors considered approximating the structure-aware regularization through local sampling or hierarchical neighborhood aggregation to improve scalability on very large graphs?\n3. Could the proposed RGVQ framework be extended to heterogeneous or dynamic graphs where structural relations evolve over time?\n4. The Gumbel-Softmax temperature τ appears crucial to token utilization—how sensitive is model performance to τ, and can it be adaptively learned instead of tuned manually?\n5. Would integrating RGVQ into large Graph Foundation Models (e.g., GROVER, GraphMAE) yield consistent benefits, or are there observed limitations when scaling up?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7oXuCXVl5n", "forum": "y6yrEPkSKM", "replyto": "y6yrEPkSKM", "signatures": ["ICLR.cc/2026/Conference/Submission8257/Reviewer_8u7x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8257/Reviewer_8u7x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760613829117, "cdate": 1760613829117, "tmdate": 1762920198322, "mdate": 1762920198322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper looks at codebook collapse in graph vector quantization. The authors argue that when quantize node embeddings into a discrete codebook, only a tiny handful of codewords actually get used, and this is especially bad on graphs since many nodes look locally redundant. They claim this hurts any attempt to build reusable “graph tokens” for transfer or for Transformer-style models on graphs. To fix it, they propose RGVQ and claim that it can both increases codebook perplexity (so the dictionary is actually used) and improve downstream accuracy when plugged into recent graph-token pipelines like GFT and GQT. After reviewing this paper,  I give a **weak reject rating (4) since some core pieces of the training recipe, baselines, and theoretical link are not yet fully convincing.**"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Problem focus is well motivated. Treating collapse itself as the main object of study (instead of just reporting final accuracy) feels important for the emerging “graph as tokens” paradigm. The paper makes a decent case that collapse is systematic in graph VQ, not just an odd failure case.\n\n2.  A simple, generally pluggable fix. RGVQ is made of two ideas that our community already understand (Gumbel-Softmax instead of hard argmax, plus a structure-aware contrastive regularizer), and the paper shows that dropping this module into GFT and GQT not only boosts codebook usage but also leads to better transfer accuracy on standard graph benchmarks. I think that practicality is attractive if it really generalizes."}, "weaknesses": {"value": "1. **Theory-to-method gap.** The theoretical part (Thm 1) gives a lower bound saying that nodes with similar features and local computation trees are very likely to get mapped to the same codeword under standard VQ. This motivates why graphs collapse. But the constants are not instantiated in a way that proves the bound is non-vacuous on real data, and the theorem analyzes hard VQ while the proposed method uses Gumbel-Softmax and a contrastive regularizer. There is no formal argument that these two ingredients actually break the self-reinforcing loop identified by the theorem. Right now the theory reads more like intuition than a guarantee.\n\n2. **Ambiguity in the actual training loss.** The paper first defines a soft assignment distribution $\\tilde{p}_i$ and the soft quantized embedding $\\tilde{z}_i = \\sum_j \\tilde{p}_i(e_j|h_i) e_j$, but later reuses the classic VQ loss with codebook and commitment terms between $h$ and $z$. It is not stated clearly whether $z$ there is $\\tilde{z}$, a sampled hard codeword, or something else, nor where stop-gradients are applied. Since the main claim is “inactive codewords finally get gradients,” I think the paper needs a precise forward/backward description or pseudocode. Otherwise reproduction (and trust in the mechanism) is shaky.\n\n3. **Structure-aware contrastive term underspecified.** The method builds a positive set (neighbors or top-$k$ most similar nodes) and a negative set (non-neighbors and not in top-$k$), then uses an InfoNCE-style loss over assignment distributions. However, details like how $k$ is chosen, how similarities are computed, how negatives are sampled, and how this scales beyond citation-size graphs are not fully spelled out in the main paper and appendix. The claim that RGVQ is an easy drop-in module would be much stronger if these knobs were made explicit and justified.\n\n4. **Baseline fairness concerns.** The paper reports that plain Graph VQ and multiple known anti-collapse tricks can end up with perplexity near 1.0 even with a 512-codeword dictionary. If that is true, reconstruction should be terrible because essentially everyone maps to the same codeword. I would like to see evidence that these baselines were genuinely trained to convergence, not just stuck in a bad local optimum or stopped early. Similarly, for downstream tasks we only see “GFT vs GFT+RGVQ” and “GQT vs GQT+RGVQ,” but not “GFT + EMA/codebook reset/etc.” or “GQT + SimVQ.” Without those ablations, it is hard to isolate how much of the reported gain is specific to solving collapse versus just adding one more regularizer.\n\n5. **Scalability and clarity.** The paper positions this as relevant for large-scale graph foundation modeling, but experiments are still mostly on medium-size benchmarks (citation graphs, Amazon co-purchase graphs, WikiCS, Roman-Empire). There is no quantitative discussion of memory or runtime overhead from doing structure-aware positive/negative sampling, which in the naive form sounds at least quadratic if you keep nearest-neighbor sets globally. This weakens the generality claim."}, "questions": {"value": "1. In the final loss $L_{\\text{VQ}}$, do you backprop through $\\tilde{z}_i$ (the soft combination of all codewords via Gumbel-Softmax), or through a hard assignment $z_i$? Please give exact forward/backward steps, including where gradients are stopped. A short pseudocode block would help.\n\n2. Can you report “GFT + EMA,” “GFT + codebook reset,” “GQT + SimVQ,” etc., not just vanilla vs RGVQ? Otherwise it is impossible to tell whether RGVQ is uniquely effective for collapse or just another regularizer that any strong baseline could also benefit from.\n\n3. In Table 1, where vanilla Graph VQ shows perplexity $\\approx 1.0$ for a 512-size codebook, what are the reconstruction losses and downstream accuracies at that point? Are these runs actually converged? If they are already unusably bad, then the gap to RGVQ might just reflect that you fixed a degenerate run, not that you improved an actually competitive baseline.\n\n4. Them 1 is used to argue that structural redundancy in graphs drives collapse. Can you make that bound non-vacuous by instantiating the constants on real datasets, and then explain concretely how Gumbel-Softmax and the structure-aware contrastive term attack the mechanism identified by the theorem?\n\n5. How does the positive/negative sampling scale on large graphs? Do you approximate nearest neighbors, cache neighborhoods, or subsample batches? Please quantify runtime and memory overhead relative to plain Graph VQ, since you claim RGVQ as broadly applicable to “graph foundation models.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N3M6fBjr59", "forum": "y6yrEPkSKM", "replyto": "y6yrEPkSKM", "signatures": ["ICLR.cc/2026/Conference/Submission8257/Reviewer_JNRk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8257/Reviewer_JNRk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655138113, "cdate": 1761655138113, "tmdate": 1762920197857, "mdate": 1762920197857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies codebook collapse in vector-quantized (VQ) tokenizers for graphs and proposes RGVQ, which combines (i) Gumbel-Softmax reparameterization for soft assignments, and (ii) a structure/feature-aware contrastive regularizer that encourages similar nodes to share token distributions and discourages co-assignment for dissimilar nodes."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clear. The paper explains that codebook collapse is systemic in Graph VQ and empirically shows large gaps between ideal and observed perplexity across datasets and codebook sizes.\n- Results show substantial perplexity gains.\n- The paper is well-written."}, "weaknesses": {"value": "- The two building blocks (Gumbel–Softmax and contrastive regularization) are well known; the novelty lies in how they’re used for VQ in graphs and justified by the analysis. This is meaningful but not very novel.\n- This paper gives zero attention to mathematical notation. This paper uses indiscriminately all types of letters for all types of elements (sets, vectors, matrices, scalars). Even though one can understand, this hurts the soundness of the paper, and makes the paper not publishable at ICLR.\n- The current theoretical discussion is weak. Please (i) instantiate the bounds on real datasets and compare the predicted vs observed codebook perplexity in Fig. 1, and (ii) decompose the bounds’ two terms, quantifying their individual contributions. This would clarify whether the bounds can fully explain the measured perplexities and which factor is the primary driver of imbalance.\n- The co-assignment bound is ill-defined and not sound. Under nearest-neighbor quantization, the assignment $z_v$ is a deterministic function of the embedding $h_v$, and $h_v$ itself is deterministic given fixed inputs and parameters; thus $\\Pr[z_{v_1}=z_{v_2}]$ is undefined unless a source of randomness is explicitly specified. Moreover, Eq. (27) appears to apply a tail bound (via Markov) and then incorrectly drops the expectation, replacing $\\mathbb{E}\\,\\|h_{v_1}-h_{v_2}\\|$ with $\\|h_{v_1}-h_{v_2}\\|$, which invalidates the inequality. Finally, the geometric implication is stated in the wrong direction: $\\|h_{v_1}-h_{v_2}\\|\\le \\delta$ does not guarantee the same codeword in general; the valid statement is that if two nodes share the same token, then their embeddings lie within the cell diameter (e.g., $\\|h_{v_1}-h_{v_2}\\|\\le diam_i$ for codeword $i$).\n- Sec 6.2 is not an ablation study but a sensitivity analysis. Is this paper doing model selection with the test set?\n-  The “ablation study” suggests that lower Gumbel temperatures yield higher codebook perplexity. This contradicts the paper’s motivation for using Gumbel-Softmax rather than a hard estimator: if lower temperatures help, the STE limit ($\\tau \\to 0$) should be competitive or even superior. As written, Table 5 indicates that pushing $\\tau$ toward zero could further improve perplexity, which contradicts the stated rationale.\n- Appendix B shows hyperparameters, but train/validation/test splits, number of seeds, and evaluation protocol for each dataset family are not explicit."}, "questions": {"value": "- Is the observed collapse related to oversmoothing in deeper GNN encoders? Please provide a study of codebook perplexity vs. the number of GNN layers.\n- Please clarify precisely how you measure the distance between (L−1)-layer computation trees in (7).\n- In (27), why is the expectation dropped? One cannot do that."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8U563tyNH5", "forum": "y6yrEPkSKM", "replyto": "y6yrEPkSKM", "signatures": ["ICLR.cc/2026/Conference/Submission8257/Reviewer_Bsb5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8257/Reviewer_Bsb5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659019641, "cdate": 1761659019641, "tmdate": 1762920197422, "mdate": 1762920197422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The article is a work aimed at contributing to the design of graph foundation model, by studying carefully how vector quantisation can be used to learn graph tokens. For that, the authors study limitations of previous use of vector quantisation (VQ) for graph representations, showing that their main limits is codebook collapse and that it’s caused by 2 things: imbalances in early assignments to codewords (due to similarities in attributes or structures), and  the training dynamics which removes non-used (and even less-used) codewords and does not allow them to be used again. The authors provide both theoretical insights and experimental evidences on that.\n\nTo alleviate these difficulties, the authors design a new approach of VQ training which includes a regularisation over the graph, so that the codebook collapse doesn’t appear. The method is relevant and elegant. A section with numerical experiments study the obtained gains and performance, and its is shown that, on classical examples, the proposed RGVQ representation works better than previous solutions to avoid codebook collapse, and that it has transferability properties (hence paving the way to graph foundation models)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of the article are : \n\n1. A very good analysis of the limitations of VG for graphs as used before, with numerical experiments reporting the codebook perplexity and which is far from optimal in existing methods.\n\n2. This analysis comes also with a nice theoretical insight (with Theorem 1) to explain that, in addition to insights about the self-reinforcing training which removes less-used codewords from the training and enforces this collapse by preventing unused codeword to be considered again. \n\n3. The proposed method, called RGVQ (for regularised graph) is not overly complex yet it does the work of preventing the codebook collapse. \n\n4. The numerical experiments show that the method works well, better than previous propositions for a better VQ for graphs. \n\nAll in all, I found that the contributions are correct, and that there is enough novelty and insight to be presented at this conference. However, it comes with some weakness, especially in the presentation, as stated underneath."}, "weaknesses": {"value": "The main weakness of the article are about the presentation, and here are the points which should be improved according to me:\n\n1. For me, section 3 about the VQ construction was hard to follow. Specifically, the notation $z_i = \\delta_j \\mathbf{C}$ used for eq. (3) had me go back to referenced work to understand how the sg operator comes in to eq. (3).  I think that presenting first the loss of eq. (4) with the sg operator and explaining then how sg works and why we need these three parts in the loss, should be clearer.\n\n2. Also, the presence of D and \\sigma in eq. (5) is not really commented. Are specific forms assumed ? The authors refer to articles about other forms of losses. It would be good to known whether the present work could use various forms of D, other losses, and possibly other ways to encode A (why use \\sigma(z z^T) necessarily?).\n\n3. I am not certain of the use of the notation in eq. (10) : $\\pi  =  - \\Vert h_i - \\mathbf{C} \\Vert^2$. I would have expected that $\\pi$ is here some p_i(e_j \\ h_i) and I would not have written it the way it is in eq. (10)  \n\n4. The proposed RGVQ is compared against other methods with VQ with mitigation of the difficulties. But the baselines include no other attempts at graph foundation models, which currently achieve baseline close (or slightly better) than the proposed RGVQ with GFT. Two examples could be (chosen because I have seen their results and because they have some of their dataset in common with the present work): ULTRA by Galkin et al. ICLR 2024, or SCORE, by Wang and Luo, 2024.\n\nThis is not to say that the present contribution is not valuable ; this is more to show that existing (and different) approaches to graph foundation models provide almost similar performance, at least on node classification.\n \n5. The graphs in Figure 4 are too small to be read on papers; please enlarge the legends (and use symbols). \n\n6. Section 6.4 does not strike me as an ablation study, more as a study on the impact of the hyperparameters. Also I have questions: when n increases, the perplexity decreases, why ? Why also is there a dependency on the dataset in 4 (b) ? For figure 4 (c) : why not report also perplexity for this numerical experiment ? For 4(d) about diversity: what procedure is used to vary the obtained perplexity ?\n\n\nReferences:\n\n\nKai Wang and Siqiang Luo. Towards graph foundation models: The perspective of zero-shot reasoning on knowledge graphs. arXiv preprint arXiv:2410.12609, 2024.\n\n\nM Galkin, X Yuan, H Mostafa, J Tang, and Z Zhu. Towards foundation models for knowledge graph reasoning. International Conference on Learning Representations, 2024."}, "questions": {"value": "Some additional questions:\n\n* You could remind the bounds of perplexity and its link to entropy in 3. Also, one often uses distorsion to quantify the use of a quantizer and of codebook; would it make sense here ?\n\n* I have carefully checked the proof of theorem 1 and it seems ok for me. The bound in eq. (23) could be commented: is it expected to be tight or realistic on average ? or often an overestimation ?  This would help to know whether the bound of the theorem (or eq. (28))) is really a conservative estimate or not. \n\n* About theorem 1: it could be interesting to comment about the two limits when B_x goes to zero and when \\Delta_l becomes large.\n\n* In section 5, provide at least one reference for InfoNCE, and possibly some rationale for using it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "izwRLpUNmN", "forum": "y6yrEPkSKM", "replyto": "y6yrEPkSKM", "signatures": ["ICLR.cc/2026/Conference/Submission8257/Reviewer_f6SV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8257/Reviewer_f6SV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762122741346, "cdate": 1762122741346, "tmdate": 1762920196958, "mdate": 1762920196958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}