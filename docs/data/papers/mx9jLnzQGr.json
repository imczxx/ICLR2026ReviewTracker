{"id": "mx9jLnzQGr", "number": 23136, "cdate": 1758340076822, "mdate": 1762968133793, "content": {"title": "Motion Generalist: Multimodal Motion Generation", "abstract": "Conditional motion generation has been extensively studied in computer vision, yet two critical challenges remain. First, while masked autoregressive methods have recently outperformed diffusion-based approaches, existing masking models lack a mechanism to prioritize dynamic frames and body parts based on given conditions. Second, existing methods for different conditioning modalities often fail to integrate multiple modalities effectively, limiting control and coherence in generated motion. To address these challenges, we propose Motion Generalist, a multimodal motion generation framework that introduces an Attention-based Mask Modeling approach, enabling fine-grained spatial and temporal control over key frames and actions. Our model adaptively encodes multimodal conditions, including text and music, improving controllability. Additionally, we introduce Text-Music-Dance (TMD), a new motion dataset consisting of 2,153 pairs of text, music, and dance, making it twice the size of AIST++, thereby filling a critical gap in the community. Extensive experiments demonstrate that Motion Generalist surpasses state-of-the-art methods across multiple benchmarks, achieving a 15% improvement in FID on HumanML3D and showing consistent performance gains on AIST++ and TMD.", "tldr": "", "keywords": ["Text-to-Motion", "Music-to-Dance", "Human Motion Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/808d7e4f93b1b32db71c9d06976d28094594efce.pdf", "supplementary_material": "/attachment/20f41751b2e112ca6861091b894e21d36c0d81b0.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies multimodal motion generation conditioned on text and music. The authors identify a limitation in masked-autoregressive methods: masks are applied randomly and do not adapt to the conditions, which prevents the model from focusing on condition-relevant motion tokens and degrades generation quality. To address this, they propose a dynamic, condition-aware masking strategy (derived from attention between condition and motion) and introduce a pseudo-labelled text+music→motion dataset for evaluation. The method shows strong results on HumanML3D and demonstrates promising control when both text and music are provided."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors propose and release a new text+music→motion dataset — a valuable resource that will likely benefit future multimodal motion-generation research.\n\n2. The proposed condition-aware masking is an interesting direction: it directly targets the failure mode of random masking (failure to learn key tokens) and the empirical results (notably on HumanML3D) are convincing."}, "weaknesses": {"value": "1. The overall pipeline strongly resembles recent works (e.g., MoMask, MoGenTS, SALAD): the mask mechanism + spatial–temporal attention design appears to be a combination of those prior ideas. The paper does not clearly disentangle what is genuinely novel versus what is a re-combination of existing components. Without a clearer comparison, the architectural novelty looks limited.\n\n2. The multi-modality generalization claim (that fusing modalities improves generation) lacks sufficient ablation. Table 8 only compares w/o text vs. text+music conditions. The paper should include cross-task ablations (train on text only → test on text; train on music only → test on music; train on text+music → evaluate with each single modality and with both) to quantify how modalities help or interfere."}, "questions": {"value": "1. In Figure 3(b), after the VQ-VAE encoder produces tokens and masks are applied, are the masked tokens fed into the Temporal Adaptive Transformer or not? The figure and text are ambiguous on this point — please clarify the exact token flow.\n\n2. The idea of deriving masks from the attention map between condition and motion is interesting. Can the authors characterize which motion tokens are typically masked by this mechanism (e.g., short transient frames, repeated/redundant frames, onset/offset frames)? A qualitative example or a visualization of masked vs. unmasked frames on real clips would help understand what the mask is selecting.\n\n3. The first claimed contribution mentions “autoregressive masked methods,” yet the paper appears to adopt only a masked prediction training objective and does not switch to an autoregressive generation paradigm. Why did the authors not adopt an autoregressive generation strategy? What are the practical or theoretical reasons for that choice?\n\n4. The VQ-VAE component is not described in sufficient detail. Please provide architecture specifics (encoder/decoder structure, codebook size, commitment loss weighting, whether codebooks are shared per joint/frame or global, tokenization stride, etc.).\n\n5. For different conditioning modalities, the paper specifies how conditioning is injected in the spatial and temporal attention blocks (e.g., using self-attention or cross-attention). However, I wonder whether the authors have conducted any ablation studies to evaluate the impact of these different conditioning injection strategies on model performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ofw5NYhczb", "forum": "mx9jLnzQGr", "replyto": "mx9jLnzQGr", "signatures": ["ICLR.cc/2026/Conference/Submission23136/Reviewer_rkcE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23136/Reviewer_rkcE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739821192, "cdate": 1761739821192, "tmdate": 1762942527263, "mdate": 1762942527263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "rokDCwMxxi", "forum": "mx9jLnzQGr", "replyto": "mx9jLnzQGr", "signatures": ["ICLR.cc/2026/Conference/Submission23136/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23136/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762968133018, "cdate": 1762968133018, "tmdate": 1762968133018, "mdate": 1762968133018, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-modal motion generation framework that combines text-to-motion and music-to-motion generation into a unified model. Additionally, the paper introduces a TMD dataset for multi-modal motion generation. The authors present quantitative experiments and some visualizations to support their claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of text-to-motion and music-to-motion into a single framework is technically sound and may provide a foundation for future multi-modal motion generation research.\n\n2. The paper is well-written and the overall method is presented clearly.\n\n3. The idea of guiding the conditional modality to select key frames (as shown in Figure 4) is intuitive and could be valuable if demonstrated more thoroughly."}, "weaknesses": {"value": "1. From the perspective of task novelty or network architecture, the contributions are kind of incremental. The main novelty is technically combining text and music modalities, which has been explored a lot. Therefore, the broader impact on the community is unclear.\n\n2. Figure 4’s visualization of key-frame selection is not fully convincing on its own. Without corresponding demos or more detailed explanation (e.g., map dimensions, color bar interpretation), it is hard for the reader to judge whether the model truly identifies key frames.\n\n3. The TMD dataset is claimed as a third contribution, but its description in the main text and appendix is very limited. The supplementary material does not include any data examples. Key aspects such as music clip length, diversity, coverage, and the use of Stable Audio Open for generating music are not thoroughly detailed. The paper only mentions human expert assessment without providing quantitative or visual evidence. Experiments on TMD are also quite limited, and some ablation studies could be conducted on TMD to better validate its quality. Without this, the claimed contribution of TMD is questionable.\n\n4. Supplementary videos show artifacts such as mesh penetration and foot sliding. There is no direct visual comparison with baseline methods; improvements reported via metrics alone may not reflect real qualitative gains. Besides, given the large number of existing motion generation methods, the paper provides very limited comparisons and visualizations. From my own experience running some baselines, reported improvements may not translate to significant perceptual differences. Therefore, it is difficult to evaluate the actual contribution of this paper based on the current submission.\n\n5. No failure cases or limitations are discussed."}, "questions": {"value": "The implementation details are insufficient. It’s unclear how the model was trained, on which datasets, how many models were trained, and whether there is a separate model for each modality.\n\nWithout detailed descriptions, it is hard to evaluate the paper's contribution and reproduce it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "659Ez6P4i6", "forum": "mx9jLnzQGr", "replyto": "mx9jLnzQGr", "signatures": ["ICLR.cc/2026/Conference/Submission23136/Reviewer_S3cR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23136/Reviewer_S3cR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768969206, "cdate": 1761768969206, "tmdate": 1762942526988, "mdate": 1762942526988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Motion Generalist, a unified framework for multimodal human motion generation that supports text-, music-, and text-plus-music-conditioned synthesis. The method introduces an Attention-based Mask Modeling mechanism within an autoregressive framework to emphasize key frames and joints guided by multimodal cues. Two core modules—the Temporal Adaptive Transformer (TAT) and Spatial Aligning Transformer (SAT)—align motion both temporally and spatially with the conditioning signals. The authors also contribute a new Text-Music-Dance (TMD) dataset with 2,153 triplets, twice the size of AIST++. Experiments on HumanML3D, KIT-ML, AIST++, and TMD demonstrate consistent performance gains, achieving up to a 15% FID improvement over prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel formulation: The attention-guided masking strategy effectively overcomes the limitations of random masking and enhances controllability.\n\n2. Comprehensive evaluation: Extensive quantitative and ablation studies strongly support the method’s effectiveness.\n\n3. Valuable dataset: The proposed TMD dataset fills a gap in multimodal motion research and provides a solid benchmark for future work.\n\n4. State-of-the-art performance: The method achieves leading results on HumanML3D and KIT-ML datasets."}, "weaknesses": {"value": "1. The architectural details of the Temporal Adaptive Transformer and Spatial Aligning Transformer (Figure 3) are not clearly explained, making it difficult to grasp their design and interaction.\n\n2. The authors do not evaluate whether the proposed TMD dataset complements existing datasets. Does training on TMD improve generalization to HumanML3D or KIT-ML?\n\n3. Part of the TMD music is synthesized rather than real, which could introduce bias or limit generalization to natural musical inputs."}, "questions": {"value": "1. Are there qualitative failure cases, especially for text-music synchronization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IZRibANs8B", "forum": "mx9jLnzQGr", "replyto": "mx9jLnzQGr", "signatures": ["ICLR.cc/2026/Conference/Submission23136/Reviewer_qXj9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23136/Reviewer_qXj9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919882581, "cdate": 1761919882581, "tmdate": 1762942526632, "mdate": 1762942526632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Motion Generalist, a unified framework for multimodal motion generation that handles text-, music-, and text&music-conditioned motion within one model. It introduces an attention-based mask modeling strategy to focus on key frames and body parts guided by the conditioning modality, implemented via two Temporal Adaptive Transformer and Spatial Aligning Transformer. A new Text-Music-Dance (TMD) dataset is also established. Extensive quantitative experiments on HumanML3D, AIST++, and TMD show consistent performance gains and better controllability than prior works."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The new text-music-dance dataset enrich good quality paired data sources for multimodal motion generation, which support further exploration along this direction.\n\n1. The proposed framework effectively integrates text-to-motion, music-to-dance, and text-music-to-dance generation into a single model, demonstrated with quantitative and qualitative results. The qualitative demos of the generation results are impressive, which is a highlight of this paper."}, "weaknesses": {"value": "1. Some key implementation details are missing in the paper, e.g. the motion representation, the design of VQ-VAE and motion tokenization.  For example, what exactly the “spatial dimension” stands for when operating the motion tokens here? The Spatial Aligning Transformer is claimed to align spatial pose with spatial condition by masking and restoring the tokens along the “spatial dimension”. However, it’s actually confusing how the motion features of different body parts can be explicitly / implicitly represented along this “spatial dimension” without any related definition.\n\n1. The training pipeline is not clearly illustrated. What kind of objective functions is the proposed model trained with? Are the tokens masked at different positions in each different spatial / temporal layer? If so, what would be account for the final predictions for calculating the objective function? Similarly, the inference pipeline is not clear neither.\n\n1. The qualitative figures (e.g. Figure 5, Figure 6) stack too many overlapping SMPL frames, which makes it hard for the readers to evaluate the generated motion details. Consider bigger intervals with fewer frames when sampling the motion sequences. Also, the axe or reference for temporal direction should be mentioned alongside.\n\n1. There are still several aspects of the design choice untouched in the ablation studies. For example, can the masking ratio of the attention-based masking be a scheduled varying ratio instead of a fixed one? What if the order of stacking the spatial transformer and temporal transformer is inverted?\n\n1. Currently, the generated dances of the two tasks seem to be biased to hiphop, krump and breakdance style. The text prompts given are also biased to describe hiphop and street dance movements. Besides, the generated musics are very similar, both of 120-130 bpm, similar beats and styles. It would be more convincing to show broader range of musics and text prompts."}, "questions": {"value": "Why is the length for text-to-motion and music-to-motion fixed to 6 sec per clip, while when applied to 4D avatars it is extended to 10 sec? Is there any particular design choice or algorithm to deal with variable length generation?\n\nPlease refer to the weaknesses section for more details. The actual score I would like to give for now is around 5. So if the identified issues and questions are properly addressed, I would consider revising my score to a more positive level."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EHDw1Toa1n", "forum": "mx9jLnzQGr", "replyto": "mx9jLnzQGr", "signatures": ["ICLR.cc/2026/Conference/Submission23136/Reviewer_NtEu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23136/Reviewer_NtEu"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954893822, "cdate": 1761954893822, "tmdate": 1762942526398, "mdate": 1762942526398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}