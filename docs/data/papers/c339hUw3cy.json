{"id": "c339hUw3cy", "number": 970, "cdate": 1756825798603, "mdate": 1763727512654, "content": {"title": "ConCuR: Conciseness Makes State-of-the-Art Kernel Generation", "abstract": "GPU kernel generation by LLMs has recently experienced rapid development, leveraging test-time scaling and reinforcement learning techniques. However, a key challenge for kernel generation is the scarcity of high-quality data, as most high-quality kernels are proprietary and not open-source. This challenge prevents us from leveraging supervised fine-tuning to align LLMs to the kernel generation task. To address this challenge, we develop a pipeline that generates and curates high-quality CUDA kernels with reasoning traces, motivated by a critical observation that concise yet informative reasoning traces result in robust generation of high-performance kernels. Using this pipeline, we construct our dataset ConCuR and introduce our model KernelCoder, which is the first model trained on a curated dataset consisting of PyTorch, reasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup, our model achieves significant improvements over the existing top-performing model, QwQ-32B, and outperforms all open-source models fine-tuned for kernel generation, as well as frontier models such as DeepSeek-V3.1-Think and Claude-4-sonnet. Finally, we show that the average reasoning length can serve as a metric to assess the difficulty of kernel generation tasks. The observations, metrics, and our data collection and curation pipeline can help obtain better data in the kernel generation task in the future.", "tldr": "", "keywords": ["CUDA kernel generation", "datasets", "data curation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4eb022ae07728ebcbcad1fa7aea46290667e637f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper makes a solid and well-motivated contribution to kernel generation. It introduces the ConCuR dataset based on the novel observation that concise reasoning traces improve kernel generation, and presents KernelCoder, a state-of-the-art model capable of producing correct and efficient CUDA kernels. The work is clearly written, experimentally strong, and supported by thorough ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a substantive and well-motivated contribution to the challenging task of kernel generation.\n\n1. The authors make an insightful observation that concise reasoning traces lead to better kernel generation performance. Building on this finding, they construct a new dataset (ConCuR) specifically designed around this principle.\n2. The proposed model, KernelCoder, demonstrates strong technical quality. It is a state-of-the-art model capable of generating correct and efficient CUDA kernels.\n3. The paper is clearly structured and well-presented. The motivation, methodology, and results are logically connected, making it easy to follow the authors’ reasoning and understand the contributions.\n4. The experimental results show that KernelCoder consistently outperforms both frontier and fine-tuned models on KernelBench Level 1 and Level 2 benchmarks. The inclusion of an ablation study further strengthens the work by demonstrating the effectiveness of the dataset curation pipeline.\n\nOverall, the paper offers novel insights and practical advancements for the field of kernel code generation. Through the creation of the ConCuR dataset and the development of KernelCoder, the authors provide a meaningful contribution that can inspire future research on reasoning-based code generation."}, "weaknesses": {"value": "1. DeepSeek-R1 appears to be a very strong general-purpose reasoning model. When compared with KernelCoder, which is specifically trained for kernel generation, the performance gap does not seem particularly large. \n2. The paper should provide more information about the *Correctness Analysis* setup — including how many random inputs were used for validation, and what tolerance thresholds were applied when judging correctness.\n3. It is unclear why KernelBench Level 3 experiments were not included. A justification for this maybe helpful.\n4. In Section 3.4, for the second part of the dataset, the authors select samples achieving 5× speedup. The rationale for this specific threshold is not explained. It would be interesting to know how results might change with alternative thresholds (e.g., 3× speedup).\n5. It would be valuable to include the performance of DeepSeek-R1-0528 in Table 5 for a more complete comparison.\n6. Why choose Kevin to create dataset. possible to use DeepSeek-R1-0528? Or rounded to use kernelCoder to create dataset for next round training.\n7. The dataset was created using Kevin, but the motivation for choosing this model over others (e.g., DeepSeek-R1-0528) is not fully explained. And is it possible to use KernelCoder itself to create dataset for next round training."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "frQ14SVqTX", "forum": "c339hUw3cy", "replyto": "c339hUw3cy", "signatures": ["ICLR.cc/2026/Conference/Submission970/Reviewer_2Vjz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission970/Reviewer_2Vjz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620463156, "cdate": 1761620463156, "tmdate": 1762915650520, "mdate": 1762915650520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose data generation and curation strategy for kernel generation using LLMs. Authors emphasize on generating chain-of-thought representation along with kernel code for supervised finetuning (SFT) of LLMs. Authors further demonstrate the efficacy of this strategy for SFT on opensource models. Authors point out two key and non-intuitive observations: 1) shorter reasoning lengths lead to better performance and 2) generated kernel performance improvement is not correlated to reasoning length. Authors compare their results with both open and closed source frontier models. KernelCoder, LLM finetuned with ConCuR strategy, outperforms frontier models in generating correct kernels though it lacks in obtaining performant ones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. ConCuR leverages generated chain-of-thoughts (CoT) for better refinement of LLMs.\n2. To avoid overthinking related issues, shortest reasoning lengths are selected out of datapoints with highest speedup.\n3. To account for high speedup cases, authors also select data points with >5x speedup over baseline."}, "weaknesses": {"value": "- Illustration in Figure 1 is not completely clear. I would encourage authors to refer other published papers for improving this.\n- In section 3.3, it is not clear which model was used (under what conditions) to generate the data.\n- In section 3.3, out of 90K total collected kernels, did authors find only 24K kernels to be correct?\n- In section 3.5, only 4,892 samples are reported to be used for finetuning. It is not clear what happened to those 90K or 24K data points.\n- In section 4.2 the definition of pass@k evaluation (lines 318, 319). Authors should refer to Chen et. la., 2021 human eval (https://arxiv.org/abs/2107.03374) paper for more clarity.\n- The data curation pipeline does not inspire scientific innovation. \n- Moreover, authors have not shown any way to determine the correctness of CoT steps. The bare assumption that generated CoTs are correct (even though the corresponding kernel maybe correct) isn't the right approach.\n- Performance improvement of generated kernels is of vital importance to justify the cost spent in training/inference/generations. Lacking the details on hardware feedback processing does not inspire confidence in obtaining performant kernels.\n- Case study shows naive details and does not show novel generated kernels that truly reflect thinking/reasoning from LLMs.\n- Authors' approach achieves very low speedups in general. Other methods/approaches have demonstrated far more speedups in comparison.\n- If authors' approach does not produce correct AND performant kernels more often then this approach does not contribute any innovation to the field."}, "questions": {"value": "- How does this approach scale to low-resource languages such as Triton?\n- Authors have not described in detail with evidence why is fast1 for L1 is worse than that of L2 in-spite of having easier problems in L1.\n- Also refer to weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CuxmDd4TYS", "forum": "c339hUw3cy", "replyto": "c339hUw3cy", "signatures": ["ICLR.cc/2026/Conference/Submission970/Reviewer_k3BA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission970/Reviewer_k3BA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717971038, "cdate": 1761717971038, "tmdate": 1762915650281, "mdate": 1762915650281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents ConCuR, an SFT curated dataset that pairs PyTorch code, reasoning traces, and corresponding CUDA kernels, claimed to be the first of its kind. Built through an automated synthetic pipeline, ConCuR enables training of KernelCoder, a model fine-tuned specifically for kernel generation. The dataset facilitates stronger reasoning-to-code alignment, leading to state-of-the-art performance on the KernelBench benchmark and outperforming both open-source and proprietary models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The work is well-motivated, addressing the high cost and expertise required for developing efficient GPU kernel data.\n* The evaluation setup includes a reasonable number and balanced distribution of models, providing adequate coverage for assessing the proposed method’s effectiveness.\n* The work presents interesting insights into how reasoning can enhance kernel code generation, highlighting the potential benefits of integrating structured reasoning traces into low-level code synthesis."}, "weaknesses": {"value": "* Some sections of the manuscript, particularly those describing the data synthesis and curation pipeline, would benefit from language refinement and stylistic polishing to improve clarity and readability.\n* The evaluation setup is somewhat limited, relying on a single fine-tuned model and one benchmark (KernelBench). Expanding the evaluation to include additional benchmarks, such as TritonBench, and more diverse fine-tuned models would strengthen the empirical validation and demonstrate broader applicability.\n* While the related work section covers key benchmarks and datasets relevant to kernel generation, it lacks a comparative table or structured analysis that clearly contrasts the proposed benchmark with existing ones. Including such a comparison, highlighting differences in dataset size, performance, cost, and methodology, would make the contribution’s advantages more explicit and easier to assess."}, "questions": {"value": "* Could the authors clarify how task difficulty was considered or quantified during the dataset construction? The manuscript discusses reasoning length as an indicator of difficulty but does not clearly explain how this factor influenced data curation (if it did). Clarifying the role of task difficulty and its potential impact on the reported results would help strengthen the connection between the dataset design and the discussion section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ItXLgDlZk7", "forum": "c339hUw3cy", "replyto": "c339hUw3cy", "signatures": ["ICLR.cc/2026/Conference/Submission970/Reviewer_JCxH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission970/Reviewer_JCxH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123666850, "cdate": 1762123666850, "tmdate": 1762915649953, "mdate": 1762915649953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a dataset of reasoning traces for kernel generation that can be used as the basis of future SFT work on kernel llms.\nIt observes that for traces generated with current-generation LLMs, long traces correlate with _worse_ generation accuracy, and thus suggests to curate the dataset by selecting short traces that resulted in correct kernels."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* An open dataset like the one proposed would be valuable to the community\n* The filtering based on trace length seems like an interesting approach.\n* The reclassification of kernel bench tasks into more sensible difficulty classes is useful."}, "weaknesses": {"value": "KernelBench is _not_ a reliable benchmark. Many of its shapes are too small, and the choice of pytorch eager as a baseline means you're mostly profiling against overhead.\nTo get a meaningful interpretation of the speed of the generated kernels, it would be important to calculate their speed-of-light, i.e., how long they would take to execute based on the GPUs flops and memory bandwidth, and then show what percentage of this speed is achieved.\n\nFurthermore, I'm worried about about dataset contamination. The kernels in KernelBench were chosen because they are common operations, and I'd be surprised if among the 9,789 tasks you selected from kernel book, there was not a significant overlap.\nThis isn't a bad thing for the dataset to be released in itself (if I want to train a kernel llm, I do want all the popular kernels in the training set, after all), but it makes it hard to trust any of the metrics reported in the paper.\n\nI would also say that the central observation about conciseness should be stated a bit more carefully. When generated by current-generation LLMs, short traces work better than long traces. That might not be the case for better future LLMs that don't \"wait\" so often, or for human-generated traces."}, "questions": {"value": "(How) have you ensured that the tasks tested on in kernel bench are not part of the training set from kernel book.\n\nHave you validated that the shapes for which speed-ups are reported are actually of a meaningfully large size. How do they compare against torch compile? What percentage of the speed-of-light is achieved?\n\nFor getting the fastest of 10 generations, do you select the time of the fastest run, or do you select the code of the fastest run, and re-time it again independently?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vLDg4fdqS2", "forum": "c339hUw3cy", "replyto": "c339hUw3cy", "signatures": ["ICLR.cc/2026/Conference/Submission970/Reviewer_QzMf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission970/Reviewer_QzMf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140957504, "cdate": 1762140957504, "tmdate": 1762915649824, "mdate": 1762915649824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of reviews"}, "comment": {"value": "Thanks to all reviewers for providing insightful feedback and suggestions. We conclude reviews and respond to the common concerns or questions here.\n\nTo sum up, we are encouraged to find all reviewers agree that:\n\n>1. Our paper shows reasonable and strong motivation, addressing the high cost and expertise required for developing efficient GPU kernel data. (Reviewer JCxH)\n2. Our *'insightful observation that concise reasoning traces lead to better kernel generation performance'* (Reviewer 2Vjz), *'is an interesting approach'* (Reviewer QzMf), and helps the construction of the dataset, ConCuR (Reviewer k3BA).\n3. Our *'open dataset like the one proposed would be valuable to the community'* (Reviewer QzMf) and our model KernelCoder, '*demonstrates strong technical quality. It is a state-of-the-art model capable of generating correct and efficient CUDA kernels*' (Reviewer 2Vjz).\n4. *'The inclusion of an ablation study further strengthens the work by demonstrating the effectiveness of the dataset curation pipeline.'*\n\nMoreover, reviewers raise a common concern, and we address them here:\n\n> Requesting more information about our training/inference/test configurations. (Reviewer QzMf, Reviewer JCxH, Reviewer k3BA, Reviewer 2Vjz)\n\n**A:** We have added information and details as per request, to make our paper more reproducible. Overall, we have added the following content to our revised paper.\n1. We revised the overview figure to improve clarity. (Reviewer k3BA)\n2. We carefully revised the description of the data curation pipeline in Section 3 to ensure accuracy (Reviewer QzMf) as well as clarity and readability (Reviewer JCxH).\n3. We added Section 4.3 and Table 3, which compare the efficiency of training samples and computational resources, highlighting the efficiency of our dataset and model. (Reviewer JCxH)\n4. We added Section 5.2 and Table 5, reporting Pass@10 results for different base models and their fine-tuned versions, thereby strengthening empirical evidence and demonstrating the broader applicability of our ConCuR dataset. (Reviewer JCxH)\n5. We revised Table 7 to include the results of DeepSeek-R1-0528. (Reviewer 2Vjz)\n\nAdditionally, we noticed that Reviewer `k3BA` has proposed 11 weaknesses, but 3 of them are just questions (W2-W4) which **we have clearly claimed in our original paper**. \n\nTo conclude, the domain of (CUDA) kernel generation is still emerging, and the community will undoubtedly benefit from more comprehensive benchmarks and standardized evaluation protocols. This paper represents an early exploratory effort, and we believe it lays an important foundation for future research and the broader automation of kernel generation. We once again thank all reviewers for their time and insightful feedback."}}, "id": "roEvXTgJVQ", "forum": "c339hUw3cy", "replyto": "c339hUw3cy", "signatures": ["ICLR.cc/2026/Conference/Submission970/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission970/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission970/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763732628258, "cdate": 1763732628258, "tmdate": 1763732628258, "mdate": 1763732628258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}