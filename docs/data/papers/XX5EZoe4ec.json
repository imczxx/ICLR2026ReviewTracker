{"id": "XX5EZoe4ec", "number": 21558, "cdate": 1758318951795, "mdate": 1759896915788, "content": {"title": "RETRIEVALFORMER: TRANSFORMER-QUALITY RECOMMENDATIONS WITH EFFICIENT ANN RETRIEVAL AND COLD-START RESILIENCE", "abstract": "We propose RetrievalFormer, a novel two-tower neural recommender architecture that bridges the gap between transformer accuracy and retrieval efficiency\nfor sequential recommendation. RetrievalFormer employs a transformer encoder\nto model user interaction sequences while using a lightweight item tower to encode items from their content features, enabling efficient approximate nearest\nneighbor (ANN) retrieval at serving time. The key innovation is an attentionbased heterogeneous feature encoder that enriches both user and item representations by learning to weight and combine different feature modalities. By sharing\nembedding tables across towers and leveraging feature-rich representations, our\nmodel achieves three critical capabilities: (1) transformer-level recommendation\naccuracy while avoiding expensive full-catalog softmax computation, (2) immediate recommendation of new items without retraining, and (3) dramatic inference speedup through ANN search. On standard benchmarks (Amazon Beauty,\nAmazon Toys & Games, MovieLens-1M), RetrievalFormer achieves competitive\nperformance, reaching 86-91% of established transformer baselines’ Recall@20\nwhile delivering up to 288× speedup at inference for large catalogs. In cold-start\nexperiments with held-out items, RetrievalFormer successfully recommends completely unseen items while baseline models fail entirely. Our approach enables\npractical deployment of efficient recommendations at scale, offering a compelling\ntrade-off between model accuracy and serving efficiency.", "tldr": "A flexible two-tower architecture that combines transformer-based user modeling with efficient retrieval, matching state-of-the-art performance while maintaining sub-millisecond inference latency.", "keywords": ["recommendation systems", "two-tower models", "transformers", "user modeling", "neural networks", "deep learning", "information retrieval"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91cc25b565b0f4b08b5ae7ac5d20ec3df4bd63f0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose RetrievalFormer, it is a two-tower model that combines transformer-based sequence modeling with efficient ANN-based retrieval for sequential recommendation. It uses shared embeddings and a multimodal feature encoder to achieve competitive accuracy while enabling fast inference and handling item cold-start. Experiments on Amazon and MovieLens datasets show 86–91% recall of transformer baselines with up to 288× speedup, along with effective recommendation of unseen items."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors conducted numerous experiments to verify the effectiveness of the method."}, "weaknesses": {"value": "The first argument posited by this paper is that existing transformer-based sequential recommendation models incur high computational costs. However, the explanation of what causes such high costs remains unclear. Is it due to the quadratic complexity of the self-attention mechanism in the transformer architecture? What are the existing solutions proposed to mitigate this issue?\n\nThe second argument claims that classical transformers struggle with the item cold-start problem. To the best of my knowledge, this issue is primarily data-related rather than model-related, as it stems from insufficient interaction history for new items.\n\nThe authors did not adequately elaborate on how their proposed method addresses the aforementioned issues. In lines 50–60, they devote extensive space to describing implementation details, yet these descriptions fail to substantiate their earlier claims. There is a lack of evidence demonstrating that their approach effectively tackles the efficiency issue of transformers or the cold-start problem.\n\nMoreover, the paper appears hastily prepared and suffers from multiple technical and presentational shortcomings. Below are several examples:\n\n1. The title seems unusual—what does “ANN” refer to? It is not a standard academically recognized term in this context.\n\n2. Question marks appear on lines 260 and 798, and an unclear character “L” is found on line 363.\n\n3. Table 2 exceeds the prescribed layout boundaries.\n\n4. The term “transformer accuracy” is ambiguous and undefined.\n\nIn summary, while the paper introduces numerous technical implementation details, it provides no compelling evidence to support its two central claims regarding efficiency and cold-start performance. Additionally, the overall presentation is unpolished and contains numerous errors. In my assessment, the manuscript requires substantial further work before it can be considered acceptable. Therefore, I strongly recommend rejection."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YG7kXH416X", "forum": "XX5EZoe4ec", "replyto": "XX5EZoe4ec", "signatures": ["ICLR.cc/2026/Conference/Submission21558/Reviewer_r8bb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21558/Reviewer_r8bb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841357809, "cdate": 1761841357809, "tmdate": 1762941834971, "mdate": 1762941834971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a dual encoder architecture for sequential recommendation that learns user action sequence embedding and item embedding separately and explicitly handles the recommendation as similarity search via dot product, hence enabling the use of vector search tools such as IVF and PQ for fast and large-scale search. The experiment highlights the query efficiency with near precision/recall. The overall proposed method makes sense on an intuitive level, although some claims are not well supported. Presentation, especially equations, can be improved."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Although not new, the authors studied a practically useful problem that moves beyond a static recommender system, where the sequential user interaction isn't utilized. The topic itself is impactful and is of broader interest to the machine learning community.\n\n2. The idea is presented clearly, first formulating a recommendation as a similarity search between embeddings and logically bringing vector search tools such as IVF and PQ. This is a practically useful insight despite its engineering nature.\n\n3. Evaluations are completed with many ablation studies, including feature fusion, LOOC, and others.\n\n4. Most of the designs are well motivated and make sense on an intuitive level, despite their simplicity. To my best knowledge, bringing IVF-PQ style vector search in a sequential recommendation system is new and novel."}, "weaknesses": {"value": "Major concerns:\n\n1. It seems most of the gains claimed in the paper are from building an IVF+PQ structure for ANN search, which is not novel and is a well-known approach for recommender systems. The novelty of using this technique in a sequential recommendation system is very limited, and how IVF-PQ is incorporated into the proposed system is somewhat trivial.\n\n2. One of my major concerns comes from the evaluation fairness: I found it unconvincing to state that previous works, such as SASRe, can not benefit from approximate vector search techniques; if this is the case, it needs to be examined more closely.\n\n3. Recall/precision is results are not optimal; the author claims the main benefit is serving efficiency, but that claim is not well supported due to major concern#2. Also, the serving efficiency experiments feel underwhelming, as only one pretty old baseline is considered.\n\nOther concerns:\n\n1. Having separate embedding for items and users is not new in the recommendation systems, for sequential or non-sequential cases. Even for a fully transformer model, such as SASRec, this is done implicitly. \n\n2. Alignment and uniformity statements regarding InfoNCE are strange and feel out of place. It is unclear what it means and what it brings to the paper. How uniformity brings good results is not clear and not well supported.\n\n3. The data flow and model design figure is hard to understand. What exactly is AttentionFusion?\nIVF-PQ details not given, how many inverted lists are there, and what is the configuration for the product quantization?\n\nSmall issues:\n\n1. Citation issue: (?) appear in citation \n\n2. “A fundamental challenge….” paragraph duplicated.\n\n3. The LOOC term is used before defined.\n\n4. Overall, the paper feels rushed and is in need of more polishing."}, "questions": {"value": "1. Can Authors isolate and do an ablation study on the effect of IVF+PQ?\n\n2. Why vector search techniques can’t be used in baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xfmbr4A1l5", "forum": "XX5EZoe4ec", "replyto": "XX5EZoe4ec", "signatures": ["ICLR.cc/2026/Conference/Submission21558/Reviewer_naFe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21558/Reviewer_naFe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861196748, "cdate": 1761861196748, "tmdate": 1762941834635, "mdate": 1762941834635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents RetrievalFormer, a two-tower recommender system combining a transformer-based user tower with a feature-based item tower to enable ANN-based retrieval. The item tower encodes heterogeneous item features via an attention fusion mechanism and shared embedding tables, making new item recommendation possible without retraining. The user tower models enriched interaction sequences with static profile features. The training uses InfoNCE with mixed negative sampling to learn a shared embedding space. Experiments show competitive accuracy relative to transformer baselines but with large efficiency gains (up to 288× speedup) and cold-start capability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear, thorough organization of the paper, with extensive architectural description and ablation studies.\n- Experimental evaluation is broad (accuracy, cold-start, efficiency), includes comparisons against strong baselines, and covers real-world cases."}, "weaknesses": {"value": "- The central “two-tower” design with shared embeddings is not new — similar architectures have been widely studied and deployed in the recommender systems literature.\n- The attention fusion for heterogeneous features, while useful, is a minor variation on existing multi-head/self-attention feature interaction or set transformer designs, not a fundamentally new mechanism.\n- The main claimed innovation is the framework combination rather than original algorithmic or modeling novelty, and the “efficient double-tower” approach is already common in related fields.\n- As a result, the paper’s contribution lies mainly in engineering integration of known ideas rather than introducing new techniques."}, "questions": {"value": "- How does RetrievalFormer’s attention fusion differ in measurable novelty from other feature interaction methods already used in two-tower models?\n- Can the authors clarify in what sense the shared embedding design here is materially different from previous unified embedding table approaches?\n- Would using standard two-tower architectures with similar feature fusion yield comparable results without the transformer component?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mEMvqdPUI2", "forum": "XX5EZoe4ec", "replyto": "XX5EZoe4ec", "signatures": ["ICLR.cc/2026/Conference/Submission21558/Reviewer_ewbd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21558/Reviewer_ewbd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898753914, "cdate": 1761898753914, "tmdate": 1762941834121, "mdate": 1762941834121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RetrievalFormer, a dual-encoder architecture for sequential recommendation, aiming to combine the accuracy of Transformer-based models (e.g., SASRec, BERT4Rec) with the serving efficiency of approximate nearest neighbor (ANN) retrieval. The model consists of a Transformer-based user tower that encodes user interaction sequences, and a feature-based item tower that encodes heterogeneous item attributes. Both towers share embedding tables to maintain semantic consistency. The paper’s key novelty lies in an attention-based heterogeneous feature fusion mechanism that dynamically integrates diverse feature modalities (text, category, numeric, etc.) for both user and item representations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper addresses a practically important and relatively underexplored problem—improving inference efficiency in Transformer-based sequential recommenders—by integrating retrieval mechanisms into the modeling framework.\n(2) The work provides both experimental and theoretical support, including ablation studies, detailed architecture analysis, and discussion of InfoNCE training dynamics, which lend credibility to the proposed approach.\n(3) The paper is well-structured overall, with clear sectioning, comprehensive appendices, and reproducibility details that make it easier to follow the methodology and replicate the experiments."}, "weaknesses": {"value": "1. Fragmented problem formulation and insufficiently articulated contributions.\nThe paper identifies two key challenges—low retrieval efficiency and item cold-start—but these are treated as independent objectives rather than as parts of a unified framework. The proposed dual-tower design and ANN-based retrieval address efficiency, while feature-based item encoding targets cold-start resilience. However, these components appear loosely coupled, lacking a cohesive theoretical or algorithmic integration.\n\n2. Weak formalization and unclear writing.\n(1) Unclear notation: Equations (5–7) introduce undefined and nonstandard terms such as AttentionFusion, ItemFeatures, and InteractionContext, which are not formally explained at their first occurrence.\n(2) Formatting and typographical issues: e.g., Line 260 “enforces uniformity (?)”; Line 461 “while achieving ? 90%”; Line 326 “averaged over five runs with std. < 0.001 not reported.”\n(3) Lack of rigor in description: e.g., Line 81 “while achieving 288× speedup at 10M items” does not specify the comparison baseline; Line 285 ambiguously uses “ML-1M” without clarifying it refers to MovieLens-1M.\n(4) Unclear visualization: Figure 1 does not explain the color-coded “Categorical Feature” elements and fails to depict the central ANN retrieval process, a key claimed contribution.\n\n3. Incomplete experimental validation.\n(1) For RQ3 (Cold-start Item Recommendation), the analysis compares only the model’s own performance under LOO vs. LOOC settings, omitting classical cold-start baselines (e.g., content-based or attribute-aware methods).\n(2) For RQ4 (Efficiency and Latency), efficiency is compared solely against SASRec, excluding stronger Transformer-based baselines such as AttrFormer and DIF-SR, which achieve higher accuracy (Table 1) and are more relevant for evaluating the claimed efficiency–accuracy trade-off.\n\n4. Insufficient efficiency analysis.\nThe paper does not explicitly compare inference time under softmax-based scoring vs. ANN-based retrieval. Such analysis would clarify whether efficiency gains stem from the model design or simply from substituting softmax with ANN—a change that could, in principle, also accelerate conventional models like SASRec.\n\n5. Unclear handling of heterogeneous feature types.\nThe paper claims support for diverse modalities (text, categorical, numerical, and interaction features) but only briefly mentions embedding features with “shared lookup tables” (Line 159). It remains ambiguous how these modalities are processed—e.g., whether textual features use pretrained embeddings, token averaging, or learned representations.\n\n6. Missing comparison with traditional two-tower retrieval models.\nWhile the Related Work section distinguishes RetrievalFormer from matrix-factorization-based dual encoders, no empirical comparison is provided with classical two-tower retrieval baselines. Such results are necessary to substantiate the claimed benefit of introducing Transformer-based sequence modeling into a retrieval-oriented framework."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hc29BTtZyU", "forum": "XX5EZoe4ec", "replyto": "XX5EZoe4ec", "signatures": ["ICLR.cc/2026/Conference/Submission21558/Reviewer_vn8j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21558/Reviewer_vn8j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923946015, "cdate": 1761923946015, "tmdate": 1762941833743, "mdate": 1762941833743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}