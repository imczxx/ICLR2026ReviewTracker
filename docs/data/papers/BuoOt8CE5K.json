{"id": "BuoOt8CE5K", "number": 23248, "cdate": 1758341288788, "mdate": 1759896824414, "content": {"title": "WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless Communications with Reinforcement Learning", "abstract": "Large language models (LLMs) excel at general mathematical reasoning but fail catastrophically on specialized technical mathematics. In wireless communications, where problems require precise manipulation of information-theoretic bounds, optimization constraints, and signal processing formulations, even state-of-the-art models struggle to achieve competent performance. We present \\textbf{WirelessMathLM}, demonstrating that compact models (0.5B--7B parameters) can match or exceed much larger models through domain-specific reinforcement learning with verifiable rewards.\nOur key insight is that wireless mathematics problems possess a unique property—verifiable correctness—that enables effective reinforcement learning without human feedback. We construct WirelessMathBench-XL, a comprehensive benchmark of 4,027 problems from 970 papers. Using Group Relative Policy Optimization (GRPO) with binary verification rewards, we train models directly from base checkpoints without supervised warm-start. \nOur 7B model achieves 39.5\\% accuracy on WirelessMathBench-XL, approaching GPT-4o (40.4\\%) while using $\\approx$100× fewer parameters than DeepSeek-R1 (671B, 57.4\\%). Remarkably, GRPO training nearly doubles performance across all model scales (0.5B: +11\\%, 3B: +103\\%, 7B: +81\\%), with positive transfer to general mathematics benchmarks—our models gain +8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and AIME without any training on these tasks. \nWe will release our benchmark, models, and codes to accelerate research in efficient, specialized AI for technical domains.", "tldr": "We train 0.5B-7B models with GRPO on wireless math, achieving 39.5% accuracy (near GPT-4o) with only 7B parameters and unexpectedly gaining 8.4 points on general math benchmarks using WirelessMathBench-XL (4,027 problems).", "keywords": ["large language models", "mathematical reasoning", "wireless communications", "reinforcement learning", "benchmark datasets"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc4f789c6aecbdbe5dea870b65815c6946f4b652.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces WirelessMathBench‑XL, a benchmark of 4,027 problems from 970 papers covering wireless‑communications mathematics (optimization, information theory, signal processing, etc.). Problems come in three tiers—MCQ, progressive fill‑in‑the‑blank (25/50/75%), and full equation completion. On top of the dataset, the authors train compact models GRPO with binary, verifiable rewards, without supervised warm‑start. Surprisingly, GRPO on this domain‑specific dataset improves general math performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The field lacks specialized math datasets for wireless; the paper addresses this with a scalable, verifiable benchmark that aligns with realistic derivations and notations used in comms papers. The pipeline and rubric are detailed, with examples and QA procedures.\n2. Leveraging verifiable correctness unlocks RL without costly human feedback. The hierarchical reward (format + answer verification; semantic equivalence for hard expressions) and simple, reproducible training recipe are well specified.\n3. The 7B model approaches GPT‑4o on the new benchmark and GRPO delivers substantial lifts over base models across 0.5B/3B/7B.\n4. The observation that domain‑specific GRPO improves MATH/Minerva/OlympiadBench/AMC/AIME (average +8.4) is interesting and contrary to standard “forgetting” concerns."}, "weaknesses": {"value": "1. Reliance on LLMs for QA and evaluation introduces bias. Although experts do a second pass, the first‑stage filtering uses GPT‑4o, and semantic equivalence for complex expressions uses GPT‑4.1‑mini. This can bias both dataset selection and scores toward models similar to the evaluator.\n2. Limited analysis on why transfer emerges. The paper notes positive transfer but offers limited causal evidence. Ablations that hold training tokens constant while varying domain specificity (how many domain-specific math questions are included in the training) would strengthen the claim."}, "questions": {"value": "See my main concerns above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2IRPguJcUi", "forum": "BuoOt8CE5K", "replyto": "BuoOt8CE5K", "signatures": ["ICLR.cc/2026/Conference/Submission23248/Reviewer_UFY2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23248/Reviewer_UFY2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650148627, "cdate": 1761650148627, "tmdate": 1762942573644, "mdate": 1762942573644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes WirelessMathLM, training compact LLMs (0.5B–7B) for wireless-communications math using verification-based reinforcement learning without massive scale or extensive supervision. They used GPT-4o and DeepSeek-R1 to construct WirelessMathBench-XL from the arXiv papers , a comprehensive benchmark of 4,027 problems from 970 papers. They trained models directly from the base checkpoints by Group Relative Policy Optimization (GRPO) with binary verification rewards, without supervised warm-start. The 7B model achieves 39.5% accuracy on WirelessMathBenchXL, approaching GPT-4o (40.4%) while using ≈100× fewer parameters than DeepSeek-R1 (671B, 57.4%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe WirelessMathLM 7B model achieves 39.5% accuracy on WirelessMathBenchXL, approaching GPT-4o (40.4%) while using ≈100× fewer parameters than DeepSeek-R1 (671B, 57.4%). And WirelessMathBench-XL, the training model and the GRPO training framework have been publicly released.\n2.\tThe paper demonstrate that verification alone enables efficient domain specialization. GRPO training from base models, without supervised warm-start or human feedback. This challenges the assumption that reinforcement learning requires extensive pre-training.\n3.\tThe paper show that specialized training develops transferable mathematical reasoning, suggesting that learning domain-specific mathematics strengthens fundamental capabilities."}, "weaknesses": {"value": "1.\tDespite the gains, 39.5% overall accuracy may still be too low for high-stakes engineering use; reporting task-wise reliability and confidence intervals would strengthen claims.\n2.\tThe WirelessMathBench-XL construction uses GPT-4o filtering and DeepSeek-R1 extraction, which could bias style/notation and favor certain models"}, "questions": {"value": "1.\tCould you report pre-vs-post domain-RL performance on broad, general-purpose benchmarks (e.g., knowledge, commonsense, reading comprehension, math outside wireless, programming) to show there is no regression in foundational abilities?\n2.\tHow does reliance on GPT-4.1-mini for semantic equivalence checking affect grading validity and reproducibility, and can you quantify bias/error rates or provide an open verifier to reduce dependence on a proprietary model?\n3. Topic coverage is skewed (e.g., deep learning and convex optimization dominate);  please report per-subdomain performance and discuss rebalancing to avoid overfitting to prevalent areas."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b3zgh3U7sW", "forum": "BuoOt8CE5K", "replyto": "BuoOt8CE5K", "signatures": ["ICLR.cc/2026/Conference/Submission23248/Reviewer_qHbW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23248/Reviewer_qHbW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829449029, "cdate": 1761829449029, "tmdate": 1762942573309, "mdate": 1762942573309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. Builds WirelessMathLM, adapting 0.5B–7B LMs to wireless-communications mathematics via GRPO using verifiable rewards only (no SFT / no human feedback). Reported gains are large: 7B goes from 21.9%→39.5% on WirelessMathBench-XL, “approaching GPT-4o (40.4%)” while using far fewer params than DeepSeek-R1 (671B) (57.4%)\n2. Introduces WirelessMathBench-XL: 4,027 problems from 970 papers, with a three-tier format (MCQ, progressive fill-in at 25/50/75%, and full eqn completion), plus a dual-layer QA process (LLM screening + expert review; 78% pass rate at ≥3/5) \n3. Main results: on their test set, 7B GRPO = 39.5% overall; per-type gains esp. on fill-in (14.3%→37.0%); comprehensive baseline table includes proprietary and OSS models (DeepSeek-R1, Qwen-Math, Llama-3.3, etc.)"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple, reproducible-ish training recipe: GRPO with binary verification is appealing, avoids SFT/human annotations, and the training details are reasonably specified (objective, α/β/G, hardware, epochs) \n2. Timely & valuable problem: formal, verifiable math in wireless comms is genuinely under-served; a curated benchmark at real scale is useful to both comms and reasoning communities. The pipeline description (Fig. 2) is clear and covers crawling, extraction, question generation and QA"}, "weaknesses": {"value": "1. Designing and collecting such a domain-specific dataset is a contribution, but I am not sure if posttraining it on 3B/7B models is a great contribution.\n2. I know there are some issues with Qwen-3 base model where its math training datasets could be polluted, but have you tried to posttrain on top of Qwen-3?\n3. Train a strong SFT on the same training set; compare to GRPO at equal token/computation budgets. This will isolate “verification-only RL” value."}, "questions": {"value": "Several baselines are instruction-tuned (e.g., Qwen-Math), while authors fine-tune base models with RL. A natural SFT baseline on WirelessMathBench-XL (and DPO/ORPO) is missing; this weakens the claim that “verification alone enables efficient specialization.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xsb9a23kFu", "forum": "BuoOt8CE5K", "replyto": "BuoOt8CE5K", "signatures": ["ICLR.cc/2026/Conference/Submission23248/Reviewer_uN8k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23248/Reviewer_uN8k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883078216, "cdate": 1761883078216, "tmdate": 1762942573121, "mdate": 1762942573121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To advance LLMs' ability to address mathematical problems in wireless communication, this paper introduces WirelessMathBench-XL, a benchmark dataset containing 4,027 problems collected from 970 papers through paper collection, mathematical extraction, and quality assurance. This dataset supports three tasks: multi-choice question-answering, fill-in-the-blank, and full equation completion. This paper further trains WirelessMathLM on these tasks using the GRPO scheme to enhance wireless mathematical reasoning performance. Evaluations on WirelessMathBench-XL demonstrate the superior performance of WirelessMathLM (with small LLM base models) compared to models trained without the GRPO scheme."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "There are two main strengths in this work: 1. A new benchmark.  The paper assembles a broad-scope corpus and constructs a comprehensive, high-quality benchmark for wireless mathematics. The breadth and curation appear solid and relevant to the community.\nPositive experiment results.  2. This paper demonstrates that GRPO fine-tuning can yield significant improvements over the base model, indicating the method’s effectiveness for this domain. It also shows improvements on general math benchmarks, suggesting that domain-specific training can strengthen fundamental mathematical abilities beyond the target domain."}, "weaknesses": {"value": "There are a few weaknesses in this work. 1. Novelty/positioning. The overall pipeline and training method appear to reuse existing approaches, primarily transferring them to the wireless domain. As written, the contribution reads more like solid engineering than methodological innovation. Please clarify the novel elements (e.g., benchmark design, verification tooling, reward shaping) and position them against prior work.  2.  According to Section 2, the pipeline uses GPT‑4o and DeepSeek‑R1 to generate and use an LLM for semantic checking. This introduces potential selection and evaluation bias, weakening the “verifiable ground truth” claim. 3. This work showcases the effectiveness of GRPO; other fintuing methods (e.g., SFT/LoRA on the same corpus) should also be investigated, and detailed ablation studies should also be presented. 4. There are several writing issues and typos."}, "questions": {"value": "1. During the paper filtering process, how is the relevance score assigned to each paper? Is this score automatically generated by the LLM or explicitly calculated based on predefined metrics?\n2. Given that the human effort is still required to review each question, what advantages does the LLM-based automated evaluation provide?  Moreover, it is encouraged to develop a fully end-to-end, automated evaluation framework without human intervention. \n3. Compared to general mathematical reasoning problems, what are the main challenges in training LLMs on wireless mathematical problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wUJ7DCSLbY", "forum": "BuoOt8CE5K", "replyto": "BuoOt8CE5K", "signatures": ["ICLR.cc/2026/Conference/Submission23248/Reviewer_GMo4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23248/Reviewer_GMo4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965370568, "cdate": 1761965370568, "tmdate": 1762942572953, "mdate": 1762942572953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}