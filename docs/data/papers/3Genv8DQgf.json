{"id": "3Genv8DQgf", "number": 18030, "cdate": 1758283052586, "mdate": 1759897138312, "content": {"title": "EAST: Early Action Prediction Sampling Strategy with Token Masking", "abstract": "Early action prediction seeks to anticipate an action before it fully unfolds, but limited visual evidence makes this task especially challenging. We introduce EAST, a simple and efficient framework that enables a model to reason about incomplete observations. In our empirical study, we identify key components when training early action prediction models. Our key contribution is a randomized training strategy that samples a time step separating observed and unobserved video frames, enabling a single model to generalize seamlessly across all test-time observation ratios. We further show that joint learning on both observed and future (oracle) representations significantly boosts performance, even allowing an encoder-only model to excel. To improve scalability, we propose a token masking procedure that cuts memory usage in half and accelerates training by 2× with no accuracy loss. Combined with a forecasting decoder, EAST sets a new state of the art on NTU60, SSv2, and UCF101, surpassing previous best work by 10.1, 7.7, and 3.9 percentage points, respectively. We support future research by releasing efficient training implementations and pre-trained models.", "tldr": "", "keywords": ["early action prediction", "token masking", "video analysis", "efficient training"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6f96e2e8b86df412f56e9a926a8b13d022665454.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reseaches on action early recogntion (or action prediction, coined by ealier work) in videos. Insteading of pursuing complicated techniques, this paper actually advocates and demonstrates that simple model architecture and training schema would be sufficient to have good results, as well as be sufficient to accomendate a varitety of inference requirements (e.g., observation ratios in this use case). Strong empirical results support the claim."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A. Very impressive evaluation results - consistently improving on popular benchmarks by notable margin.\n\nB. The overall approach bears a good balance of simplicity - the model architecture being transformers (i.e., a very general and prior-free model family), the optimization strategy (i.e., the training loss is a standard cross-entropy loss), the design for being agnostic against observation ratios with one unified approach, and lastly the hyper-parameters also being straightforward and easy to implement."}, "weaknesses": {"value": "A. This review has strong concern on the assumptions incoporated and implemented in the token masking module, which is proposed for the purpose of the effienct model design - repetitive frame masking. This strategy, though indeed reduces FLOPS and speeds up training/inference, pre-assumes that similar tokens should be ignored and largely different tokens in feature space should be attended to more, which is arguable when it comes to a boarder scope of action early recongtion task. Imagine fast action video clips such as \"world chase tag\" competition where the frames are mostly motion intense, have much less idle motions and even the idle period is quite important. It is very unlikely that the 50% frame masking strategy would hold all essential information. Even more, let's assume 50% masking rate is appropriate, the masking decision comes from the token-wise similarity captured by feature space difference. This strategy has inherent flaws, such as big background motion will be reckoned as more valuable compared to small actor related motion under current fashsion. From the experimental results, it seems that using full frame (or token) still bring the best results, which makes the masking less critical. To sum up, this reviewer appreciate the efforts on making slim model, but disagree on doing so depending on any prior assumptions, such as feature space difference.   \n\nB. Can the authors further clarify the protocol difference between the proposed EAST method and the most recent work, e.g., Tempr? This reviewer found an evaluation tip, i.e., multi-crop inference, a training tip, i.e., data augmentaion, and a scaled learning rate schedule, in the paper and wonders if the comparison method follow the same procedure. Sometimes, training and evaluation details can lead to big difference. It would be nice to keep the protocol the same on both side. This reviewer conjure that difference in optimization could has strong impact on final results.\n\nC. How is the VideoMAE trained in Table 5? Is it just using the observed frames at a certain ratio to predict the action label? Is it also a observation ratio agnostic model? Why the results indicate the impact of \"sampling strategy\"? What is the loss and input here?\n\nD. This reviewer is particularly interested in Table 5 and would like to observe more on the performance gain when being compared to VideoMAE from other dataset. Actually, VideoMAE is more comparable to EAST than other early action recognition work (e.g., Tempr) in terms of model architecture similarity and loss objectives. It will be a big help if authors can add such comparison to all benchmarks and spot any interesting patterns. One thing noticed by this reviewer from the current Table 5 is that the 0.9 observation ratio accuracy from EAST_{epson} is deteriorated, i.e., 60.7 vs. 63.4. And this makes reviewer wonder if current VideoMAE is biased on later frames (i.e., focus on 80th - 90th percent frames of a video clip). If so, it seems logically reasonable that the proposed training strategy depicted in figure 1 resolves such bias to some extent, or say learn a more generalized classifier across time - The L^{pred} will object when L^{oracle} optimize later frames too much. The learning dynamic here can be a very interesting investigation. Maybe plotting the L^{oracle} of 80th-90th percent frames vs L^{pred} on the validation set during training would be insightful, which also fits the taste of ICLR (more machine learning analysis)."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tONFX0x8Ci", "forum": "3Genv8DQgf", "replyto": "3Genv8DQgf", "signatures": ["ICLR.cc/2026/Conference/Submission18030/Reviewer_mSTD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18030/Reviewer_mSTD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761512292543, "cdate": 1761512292543, "tmdate": 1762927820857, "mdate": 1762927820857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents EAST, a simple yet effective framework for early action prediction (EAP) that enables a single model to generalize across multiple observation ratios. The method introduces a randomized temporal sampling strategy to jointly train on both observed and unobserved video segments, combined with a token masking technique that removes redundant patches for more efficient training. Extensive experiments on NTU60, Something-Something V2, and UCF101 demonstrate state-of-the-art accuracy while reducing training time and memory by half."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "S1. Simple yet effective: The proposed sampling and masking strategies are conceptually simple but yield strong performance gains across benchmarks.\n\nS2. In-depth analysis: Extensive ablation studies support each design choice and show consistent improvements.\n\nS3. Comprehensive evaluation: EAST is evaluated on multiple datasets, demonstrating generality and efficiency."}, "weaknesses": {"value": "W1. Weak motivation & significance: The paper does not clearly elaborate which specific limitations in prior EAP methods EAST addresses or why its components are crucial. While the sampling strategy is intuitive and well-justified, the roles of the compound forecasting loss and token masking are underexplained. Moreover, the significance of the tackled problem is not sufficiently emphasized. Clarifying both the motivation and broader impact of the proposed framework, along with the necessity of each component, would substantially strengthen the paper’s contribution.\n\nW2. Outdated baselines: Many comparisons are against older methods; TemPr is the only recent baseline, but its architectural and training differences make fair comparison difficult. Additional controlled analysis on temporally challenging datasets (e.g., Something-Something) would clarify the contribution.\n\nW3. Missing inference speed comparison: Since EAP is latency-sensitive, comparing inference speed with prior works (e.g., TemPr, ERA) would strengthen the claims.\n\nW4. Limited masking comparison: Token masking is only compared with random masking. Including modern masking methods such as VideoMAE [a] or MAR [b] would better validate the approach.\n\nW5. Missing results on Epic-Kitchens 100: Although Epic-Kitchens 100 is a standard benchmark for action anticipation and applicable to EAP (as demonstrated in TemPr), the paper omits it. Including this dataset would better validate generalization in real-world scenarios.\n\n[a] Tong et al., “VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training,” NeurIPS, 2022. \\\n[b] Qing et al., “MAR: Masked Autoencoders for Efficient Action Recognition,” Transcations on Multimedia, 2023."}, "questions": {"value": "Please refer to the weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uTc0yZPJjQ", "forum": "3Genv8DQgf", "replyto": "3Genv8DQgf", "signatures": ["ICLR.cc/2026/Conference/Submission18030/Reviewer_9qPd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18030/Reviewer_9qPd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570094799, "cdate": 1761570094799, "tmdate": 1762927819086, "mdate": 1762927819086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses early-action prediction, where the model must recognise an action from only a partially observed video. The authors propose EAST, a training framework with three main ideas:\n\n1.  Randomised observation-ratio sampling: During training, the method samples an observation ratio ρ ∈ {0.1,…,0.9} and splits each video into an “observed” prefix Vᵒ and an “unobserved” suffix Vᵘ. A single model is then trained to operate across all ρ, rather than training one specialised model per ρ (as prior work like TemPr typically does).\n\n2.  Compound loss on present vs. full video: The model is trained both to classify from the partial clip (via a decoder that forecasts future features) and to classify the full clip (oracle). Optimising both prediction logits (from predicted future features) and oracle logits (from full context) is argued to make the encoder’s features discriminative even under very small observation ratios.\n\n3. Token masking for efficiency: The authors introduce a difference-based token masking strategy that drops ~50% of spatio-temporal tokens deemed redundant across time. This reduces training memory and wall-clock time while preserving accuracy.\nEAST reports strong gains on NTU60, Something-Something v2 (SSv2 and SSsub21), and UCF101, on both VideoMAE and MoViNet backbones. The paper also includes ablations: (i) with/without the decoder, (ii) with/without each loss term, (iii) different masking ratios, and (iv) per-ρ specialised models vs. one unified model. The authors claim that EAST achieves new state-of-the-art results while requiring fewer GPUs and no per-ρ retraining."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Overall, even if the ideas are incremental individually, the combination is interesting and could serve as a practical baseline for early-action prediction. Below are the main strengths of this paper.\n\n1. Practical relevance and framing. Early action prediction is important for any system that must react before an action is complete (e.g., driving, human-robot interaction). The paper motivates this scenario well.\n\n2. Single model across all observation ratios. Prior work often trains a different model for each observation ratio ρ. This is expensive and awkward for deployment. EAST’s randomised sampling of ρ at training time directly addresses that, and Figure 2 suggests that such a unified model generalises well across all ρ, whereas per-ρ specialists do not transfer. This is an important engineering insight for the field.\n\n3. Compound loss improves robustness at low observation ratios. The combination of predicting from partially observed clips and supervising with an “oracle” view of the full clip encourages discriminative features even at 10%–20% observation. The ablations (Table 7) confirm that using both loss terms (L_pred and L_oracle) improves accuracy over using either alone.\n\n4. Training-time efficiency via token masking. The masking procedure drops ~50% of “low-difference” tubelets and roughly halves peak memory usage, while maintaining accuracy (Table 6). This is a concrete, useful contribution, and the authors do report memory/TFLOP numbers and epoch times.\n\n5. Strong experimental results. EAST achieves new state-of-the-art top-1 accuracy across a range of observation ratios on SSv2, SSsub21, NTU60, and UCF101, using both VideoMAE and MoViNet backbones. Comparisons include modern baselines like TemPr (CVPR 2023), ERA (ECCV 2022), and RACK (TIP 2023).\n\n6. Reproducibility and ablations. The paper discloses hyperparameters, training schedule, backbone initialisations (VideoMAE pretraining, MoViNet), and even wall-clock timings. The ablation test: decoder types (direct vs autoregressive), loss design, shared vs separate classifier, masking ratios, etc."}, "weaknesses": {"value": "1. Clarity and notation issues in the method section: the construction of Vᵒ (observed) and Vᵘ (unobserved) around a sampled ratio ρ is central, but the notation is underspecified. The text states: “The sampled clip V = Vᵒ ∥ Vᵘ consists of 2T evenly spaced frames centered at ρ·T_d,” implying |Vᵒ| = |Vᵘ| = T. However, T is not defined before use, and “centered” conflicts slightly with “concatenation of Vᵒ and Vᵘ with no gap.” This makes it unnecessarily hard to reproduce the exact temporal sampling logic.\nThe encoder paragraph (“Tokenizer T splits the input clip frames into N_t … tubelets of size d×p×p=2×16×16 … The transformer encoder V extracts features…”) mixes general notation and hard-coded hyperparameters in a colloquial way. Key variables (T, p, d, N_t) are introduced abruptly, and not all are defined rigorously. For a core technical section of an ICLR paper, this could be tighter.\n\n2. Limited theoretical insight: The paper argues that combining (i) a loss on predicted future features from partial clips (L_pred) and (ii) a loss on oracle full-clip features (L_oracle) makes the encoder discriminative under all observation ratios. While plausible, this is only described qualitatively; no analysis (e.g., feature visualisations, calibration curves, entropy at low ρ) is provided.\n\n3. Missing qualitative results: The paper does not include any qualitative timeline-style figure showing how the predicted class probabilities evolve as more of the video is seen, nor any visual comparison to ground truth. This is fairly standard in early \naction anticipation literature to illustrate how “early” the model can correctly commit to an action. Adding even one such figure would greatly improve the interpretability and trustworthiness of the qualitative claims.\n\n4. Comparisons to the most recent literature: The comparisons include TemPr (CVPR 2023), ERA (ECCV 2022), and RACK (TIP 2023), which are strong baselines. However, there are also recent approaches that model early action recognition via prototypical representations or transformer decoders and report top-1 accuracy vs. observation ratio on SSv2 / SSsub21 / EPIC-Kitchens in late 2023 (e.g., “Early Action Recognition with Action Prototypes”, a.k.a. Early-ViT-style models). These methods also aim at efficient online recognition from partial clips. EAST does not cite or compare against that family, and it would strengthen the impact claim to position EAST relative to those more recent models.\n\n5. Evaluation metrics: The paper reports only top-1 accuracy across multiple observation ratios ρ, which is indeed standard in early action prediction work such as TemPr and other pre-2024 baselines. Still, reporting additional metrics like top-5 accuracy, AUC across ρ, or calibration / time-to-correct-prediction curves would make the evaluation more comprehensive, especially since the method is motivated by safety-critical, real-time applications.\n\n6. Slightly overstated claims: The authors state they achieve “9× less compute,” “2× faster training,” and training on “consumer-grade hardware.” These claims are mostly motivated by: \n\n   a.  Not needing to train nine separate models (one per ρ). \n\n   b.  ~2× memory savings from their masking scheme (Table 6). 3. Wall-clock per-epoch measurements (80s/epoch vs 173s/epoch for TemPr on the same 4× RTX A6000 setup).\n\nWhile these are promising, the paper could be more precise: RTX A6000s are still high-end GPUs; and the ‘9× less compute’ claim is inferred rather than measured in total GPU-hours. This is more of a tone issue than a scientific flaw, but worth noting."}, "questions": {"value": "1. Clarify the temporal sampling: Please explicitly define T, T_d, Vᵒ, Vᵘ, and the operator ∥. When you say “The sampled clip V = Vᵒ ∥ Vᵘ consists of 2T evenly spaced frames centered at ρ·T_d,” does that mean |Vᵒ| = |Vᵘ| = T, with Vᵒ covering frames strictly before ρ·T_d and Vᵘ strictly after ρ·T_d, and no overlap? Or does “centered” mean you actually take a symmetric window around ρ·T_d and then label the pre-cut portion “observed” and the post-cut portion “future”? A precise definition is important for reproducibility.\n\n\n2. Generalisation across ρ: You state that “training at fixed observation ratios produces models that are suboptimal at other values of ρ.” You partially support this claim with Figure 2 (specialist models vs EAST). Could you quantify how much performance drops when a ρ-specialist is evaluated out-of-distribution (e.g., a model trained at ρ=0.3 evaluated at ρ=0.7)? Are all these models trained with exactly the same backbone, number of epochs, and data seen per step, so that the comparison is fair?\n\n3. Loss design and theory: The compound loss combines L_pred (classification on forecasted features from partial observation) and L_oracle (classification on oracle full-clip features). Can you provide intuition for why this helps so much at low observation ratios (ρ=0.1, 0.2)? For example, is the model learning to align early-frame features with full-clip features, or is it essentially acting like a teacher-student relationship inside the same network? Did you try scheduling L_oracle to zero out later in training (to avoid overfitting to full context)?\n\n4. Token masking: The masking method selects tubelets with a large temporal difference based on an L1 distance across time. Can you include a short figure or pseudocode showing this ranking process? The current text around Eqs. (5–7) is quite dense and abruptly introduces p, d, and N_t. At inference time, do you also apply masking? If not, is there any distribution shift between masked training and unmasked inference? Does masking ever remove fast motion in the observed portion, hurting the ability to predict very early?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper uses standard, publicly available video datasets (SSv2, NTU, UCF101). There is no sensitive data collection or obvious misuse scenarios presented beyond typical action recognition—anticipation use cases."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZlXOAhgV2N", "forum": "3Genv8DQgf", "replyto": "3Genv8DQgf", "signatures": ["ICLR.cc/2026/Conference/Submission18030/Reviewer_SbPM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18030/Reviewer_SbPM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931575258, "cdate": 1761931575258, "tmdate": 1762927818566, "mdate": 1762927818566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EAST (Early Action prediction Sampling strategy with Token masking), an encoder-decoder method for early action prediction. To improve classification performance, the encoder is trained considering both the previous and all the frames of the current action and employing a shared classifier. Moreover, training is performed by considering different observation ratios via sampling. This helps in having a single model able to handle various observation ratios. Finally, a token masking strategy is employed that leads to almost identical performance while only using 50% of the tokens, significantly improving the training efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is interesting and well-justified. The joint training of the encoder on both previous and future frames is intuitive and appears to contribute positively to the performance.\n- The proposed training strategy, which considers random sampling of action observation ratio, makes the method robust against different observation ratios at test time. By using a single model for different observation ratios, the equivalent training time is significantly reduced.\n- The proposed token masking strategy further improves training efficiency by using up to 50% fewer tokens without any significant performance degradation.\n- Multiple ablations are considered, including the choice of the decoder, the use of shared vs separate classifiers, and various combinations of the loss functions."}, "weaknesses": {"value": "- Although the introduction highlights the importance of inference time efficiency in various applications, only the training time and efficiency are discussed in the text. It is crucial to provide more details regarding the inference time of the proposed method.\n- Token masking is frequently considered in similar applications. Hence, although it most certainly helps in reducing the training efficiency, it cannot be considered a main contribution introduced in this work.\n- Related to both previous comments, masking at inference time is not considered in this work. It would be interesting to explore also this direction, which could contribute to the reduction of the processing time also during inference."}, "questions": {"value": "- What is the inference time of the proposed method?\n- Have inference time masking strategies been considered?\n- Why only sample at fixed increments of 0.1 and not uniformly in (0,1)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cdO8v9taxP", "forum": "3Genv8DQgf", "replyto": "3Genv8DQgf", "signatures": ["ICLR.cc/2026/Conference/Submission18030/Reviewer_oxAh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18030/Reviewer_oxAh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762343233605, "cdate": 1762343233605, "tmdate": 1762927818053, "mdate": 1762927818053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}