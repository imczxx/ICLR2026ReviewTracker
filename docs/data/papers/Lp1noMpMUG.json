{"id": "Lp1noMpMUG", "number": 15695, "cdate": 1758254054034, "mdate": 1763682377477, "content": {"title": "Decoding Inner Speech with an End-to-End Brain-to-Text Neural Interface", "abstract": "Speech brain–computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end BraIn-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text ’24 and ’25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio-LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization.", "tldr": "", "keywords": ["brain-computer interfaces", "neuroscience", "neural decoding", "self-supervised learning", "multimodal", "transformer", "masked modeling"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/290c16157e0c2362c0e2852dde46cff2bcc8b37f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces BIT, an end-to-end brain-to-text speech decoding framework that translates intracortical neural activity into text by combining a transformer-based, cross-task/cross-species pretrained neural encoder with an audio large language model (LLM) decoder. The authors design an architecture that supports both cascaded and end-to-end decoding by leveraging self-supervised masked modeling pretraining on extensive human and monkey datasets, phoneme-level finetuning, and contrastive learning for cross-modal neural-text alignment. The method is evaluated on the Brain-to-Text Benchmark '24 and '25 for both attempted and imagined speech."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The manuscript is well written and easy to follow. Numerous tables and figures provide detailed results, ablations, and interpretability analysis, helping readers understand model choices and results. The supplemental materials provide many clarifying details and results.\n\n2. Based on the well-tuned architecture in Time-Masked Transformer [7], MAE-based pre-training further improves `~0.5%` in two speech decoding datasets [2,8] in Table 1.\n\n3. Explore the effects of audio LLMs for end-to-end speech decoding, instead of LLMs used in [9]."}, "weaknesses": {"value": "1. The significant performance drop in the end-to-end model (Fig. 2A) casts doubt on the effectiveness of the proposed Audio-LLM Decoder (Fig. 1C). This finding is consistent with prior work [4].\n\n2. The pre-training approach lacks neuroscientific justification. It aggregates intracranial data from different brain regions (e.g., motor [1] vs. speech production [2]) and species, without explaining how such disparate neural computations can share a common latent representation. Perhaps applying more data augmentation (e.g., timing jittering) during pre-training and using a single subject’s data would be enough to compensate for the reported performance differences (Figure 2). Besides, the motivation for removing time masking during fine-tuning is unclear (Line 223), which is a core module in Time Masked Transformer [7].\n\n3. Using audio models for supervision is not novel [3], where the results indicate that audio latent guidance is less effective than sequential phoneme labels under phoneme error rate (PER) evaluation.\n\n4. The use of Masked Autoencoder (MAE) pre-training is a well-established technique and does not represent a significant methodological novelty [5,6].\n\n5. The proposed BIT model appears to have a significantly higher training cost than the Time-Masked Transformer baseline. To better contextualize this, the authors should include a runtime comparison, detailing the reported 260-hour pre-training time and the computational overhead of the Audio-LLM Decoder.\n\n**References**:\n\n[1] Willett F R, Avansino D T, Hochberg L R, et al. High-performance brain-to-text communication via handwriting[J]. Nature, 2021, 593(7858): 249-254.\n\n[2] Willett F R, Kunz E M, Fan C, et al. A high-performance speech neuroprosthesis[J]. Nature, 2023, 620(7976): 1031-1036.\n\n[3] Metzger S L, Littlejohn K T, Silva A B, et al. A high-performance neuroprosthesis for speech decoding and avatar control[J]. Nature, 2023, 620(7976): 1037-1046.\n\n[4] Jiang W B, Wang Y, Lu B L, et al. NeuroLM: A universal multi-task foundation model for bridging the gap between language and EEG signals[J]. arXiv preprint arXiv:2409.00101, 2024.\n\n[5] Ye J, Collinger J, Wehbe L, et al. Neural data transformer 2: multi-context pretraining for neural spiking activity[J]. Advances in Neural Information Processing Systems, 2023, 36: 80352-80374.\n\n[6] Kapoor J, Schulz A, Vetter J, et al. Latent diffusion for neural spiking data[J]. Advances in Neural Information Processing Systems, 2024, 37: 118119-118154.\n\n[7] Feghhi E, Kaasyap S, Hadidi N, et al. Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding[J]. arXiv preprint arXiv:2507.02800, 2025.\n\n[8] Card N S, Wairagkar M, Iacobacci C, et al. An accurate and rapidly calibrating speech neuroprosthesis[J]. New England Journal of Medicine, 2024, 391(7): 609-618.\n\n[9] Feng S, Liu H, Wang Y, et al. Towards an end-to-end framework for invasive brain signal decoding with large language models[J]. arXiv preprint arXiv:2406.11568, 2024."}, "questions": {"value": "See concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VmO7zd8L9b", "forum": "Lp1noMpMUG", "replyto": "Lp1noMpMUG", "signatures": ["ICLR.cc/2026/Conference/Submission15695/Reviewer_6BdX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15695/Reviewer_6BdX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760698948371, "cdate": 1760698948371, "tmdate": 1762925946030, "mdate": 1762925946030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the use of self-supervised learning (SSL) combined with Transformer architectures for decoding inner speech from EEG signals. The authors pre-train an SSL model on a large-scale EEG dataset and evaluate it on several downstream decoding tasks. The work aims to bridge the gap between recent advances in representation learning and neural decoding applications."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important and technically challenging problem: decoding inner speech from EEG, a task that remains largely open and scientifically valuable.  \n- The experimental design is systematic and the paper provides clear descriptions of the model, datasets, and training procedures.  \n- Empirical results show consistent improvements over baseline methods, indicating that the approach is effective in leveraging large-scale pre-training for EEG representation learning."}, "weaknesses": {"value": "1. **Potential Data Leakage in Pre-training**  \n   The SSL model was pre-trained on a large EEG dataset that includes data from some subjects who also appear in the test set. Although SSL does not rely on explicit labels, this overlap means that data from the test set contributed to the model’s representational space. As a result, the reported improvements might partially reflect familiarity with certain subjects rather than genuine generalization to unseen data. Clarifying or mitigating this issue would be important for ensuring the validity of the conclusions.\n\n2. **Characterization of Model Novelty**  \n   The Introduction suggests that prior work \"does not explore modern architectures, such as Transformer.\" This statement may be somewhat overstated. The Transformer architecture has been extensively studied across many fields, including neural data modeling. The novelty here lies more in its application to *inner speech decoding* rather than in the architecture itself. The framing could be refined to more accurately reflect this distinction.\n\n3. **Limited Theoretical Innovation**  \n   The paper primarily combines existing components—SSL pre-training, Transformer encoders, and standard EEG decoding techniques—on publicly available data. While the integration is well executed and empirically valuable, the theoretical innovation appears limited. The contribution may therefore be more in engineering practice than in advancing fundamental understanding in machine learning or neuroscience."}, "questions": {"value": "Since the contribution is primarily empirical, it would be helpful to better understand the theoretical or conceptual motivation behind combining SSL and Transformer.   For instance, what properties of EEG data make Transformer architectures particularly suitable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1xWAZj5SWo", "forum": "Lp1noMpMUG", "replyto": "Lp1noMpMUG", "signatures": ["ICLR.cc/2026/Conference/Submission15695/Reviewer_1Ni1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15695/Reviewer_1Ni1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896349505, "cdate": 1761896349505, "tmdate": 1762925945482, "mdate": 1762925945482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents BIT, an end-to-end framework for brain-to-text speech decoding aiming to translate neural activity directly into coherent sentences. The core contribution include a transformer-based neural encoder pretrained on large-scale, cross-task and cross-species neural datasets, integration with Audio-LLM. The proposed method achieved a lower word error rate (WER) on the test set compared to the baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: This paper builds an end-to-end architecture, integrating neural signals into a multimodal LLM and performing modality alignment.\nS2: This paper conducts additional analyses, such as ablation studies across different models and comparisons of the representational geometry of neural embeddings against that of LLMs.\nS3: This paper provides a relatively thorough discussion of its limitations."}, "weaknesses": {"value": "W1: The evaluation metrics only involve WER, which seems insufficiently comprehensive.  Although the paper introduces finetuning for sentence-level decoding, it evaluates performance solely at the word level. Incorporating sentence-level evaluation metrics, such as semantic similarity, would likely provide a more complete assessment. In addition, a more detailed analysis of the error words should be provided. For example, does the same word sometimes appear correctly and sometimes incorrectly? Are common words recognized more accurately? Are words that appear infrequently in the training data more prone to errors?\nW2: The paper introduces cross-species (monkey) data for training. Although this approach is relatively novel, the paper lacks further discussion on how the monkey data affect the training performance. I am also interested in understanding the impact of different proportions of monkey and human data on the results.\nW3: The end-to-end architecture proposed in the paper appears to underperform cascade systems in terms of both effectiveness and efficiency in practical applications, which raises doubts about the necessity of adopting an end-to-end framework for the brain-to-text task. In particular, given the limited amount of data available in this domain, it is questionable whether an LLM-based end-to-end architecture can be effectively trained."}, "questions": {"value": "Q1: In the BIT loss, the cross-entropy loss and the contrastive loss are simply added together. What would be the effect if the weighting between these two loss terms were adjusted?\nQ2: The training details in Appendix O show that hundreds of epochs are used for training the different components. Is there any theoretical justification for such a large number of epochs, and does this raise concerns about potential overfitting?\nQ3: The motivation for using an Audio-LLM requires further explanation. Why not adopt a VLM or Omni model instead?\nQ4: The reason for adopting LoRA is not entirely clear. Would full-parameter training yield better performance?\nQ5: The concepts of attempted speech and imagined speech are mentioned multiple times early in the paper, but they are not explained until Section 4.1. It would improve readability to introduce and clarify these concepts earlier in the text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nAhaicB3uS", "forum": "Lp1noMpMUG", "replyto": "Lp1noMpMUG", "signatures": ["ICLR.cc/2026/Conference/Submission15695/Reviewer_pds5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15695/Reviewer_pds5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970401214, "cdate": 1761970401214, "tmdate": 1762925945028, "mdate": 1762925945028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Advancing speech brain-computer interfaces (BCIs) to restore communication for individuals with paralysis, this paper (under review at ICLR 2026, titled *Decoding Inner Speech with an End-to-End Brain-to-Text Neural Interface*) addresses the key limitation of existing cascaded frameworks—where recurrent neural networks (RNNs) decode phonemes first before assembling sentences with n-gram language models (LMs), preventing end-to-end optimization and often causing mismatches between phoneme error rate (PER) and word error rate (WER)—by proposing the end-to-end BraIn-to-Text (BIT) framework, whose core is a transformer neural encoder pretrained via self-supervised masked modeling on 367 hours of Utah array data (98 hours of human speech/handwriting recordings and 269 hours of monkey motor task recordings) to enable cross-task (attempted and imagined speech) and cross-species representation transfer; BIT supports two decoding configurations: a cascaded setting (pairing the pretrained encoder with an n-gram LM, achieving state-of-the-art (SOTA) word error rates (WERs) of 5.10% on the Brain-to-Text ’24 holdout set and 2.82% on the ’25 holdout set with model ensembling, outperforming prior methods like Feghhi et al. (2025)) and an end-to-end setting (integrating the encoder with audio large language models (LLMs) such as Aero1-Audio 1.5B, optimized via contrastive learning for cross-modal alignment between neural and text embeddings, which reduces the prior end-to-end WER from 24.69% (Feng et al., 2024) to 10.22% with ensemble, while small-scale audio-LLMs outperform text-based LLMs and BIT aligns neural embeddings of attempted and imagined speech to enable cross-task generalization—especially valuable for low-data imagined speech tasks); the work also notes limitations including slower end-to-end inference (~0.95 seconds per sentence vs. 0.24 seconds for cascaded methods), bidirectional attention unsuitable for real-time decoding, and reliance on limited human data, but overall advances a differentiable, integrable framework for direct neural activity-to-coherent text translation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel end-to-end BIT framework: Replaces traditional cascaded systems (phoneme decoding followed by n-gram language models) with a single differentiable neural network, solving the dissociation between phoneme error rate (PER) and word error rate (WER) while enabling joint optimization of all stages.\n2. Cross-task and cross-species pretrained encoder: Pretrained via self-supervised masked modeling on human (speech-related tasks) and monkey (motor tasks) Utah array data, it outperforms RNNs and Transformers trained from scratch, and sets a new state-of-the-art (SOTA) on the Brain-to-Text benchmarks.\n3. Audio LLM integration with contrastive alignment: Integrates audio large language models (LLMs) in the end-to-end setting, and uses contrastive learning to achieve cross-modal alignment between neural and text embeddings; small-scale audio LLMs show better performance in end-to-end decoding than text-based LLMs.\n4. Strong cross-task generalization: Aligns the embeddings of attempted and imagined speech (leveraging shared semantic structure), enabling effective decoding of the imagined speech task, which has scarce labeled data.\n5. Outstanding performance: In the cascaded setting (especially with model ensembling), it achieves SOTA results on the holdout sets of the Brain-to-Text benchmarks; the end-to-end setting significantly narrows the performance gap with cascaded frameworks."}, "weaknesses": {"value": "1. Slow end-to-end inference: The end-to-end approach is less efficient than the cascaded one, making it unsuitable for real-time brain-computer interfaces (BCIs).\n2. Incompatibility with online decoding: The neural encoder uses bidirectional attention to maximize performance, which cannot be applied to online decoding; switching to causal attention will sacrifice some decoding accuracy.\n3. Limitations of large models in on-device use: Larger audio large language models (LLMs) cannot run on devices, further restricting real-time applications.\n4. Limitations of pretraining data: Human data contribute more to performance gains than cross-species transfer (monkey reaching data are less relevant to speech), and access to private human data for pretraining is limited.\n5. Room for improvement in LLM decoders and long-term BCI adaptation: The LLM decoder needs better modality alignment and prompt design; there is also a lack of solutions for non-stationarity, neural plasticity, and user-interface co-adaptation required for long-term BCI use."}, "questions": {"value": "1. Why include monkey motor task data in encoder pretraining? No clear link to human speech neural activity is given.\n2. For imagined speech, is \"data size difference\" controlled when comparing BIT-All and BIT-Cross-Task-Only?\n3. Without removing monkey data in ablation, is it proven such cross-species data is essential for pretraining?\n4. Why prioritize audio LLMs over text ones? Their bias for neural-to-text decoding lacks clear support.\n5. When comparing cascaded/end-to-end frameworks, are LLM differences (size, data) controlled for fairness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2OTAN1ij2P", "forum": "Lp1noMpMUG", "replyto": "Lp1noMpMUG", "signatures": ["ICLR.cc/2026/Conference/Submission15695/Reviewer_GRt5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15695/Reviewer_GRt5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177136074, "cdate": 1762177136074, "tmdate": 1762925944454, "mdate": 1762925944454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to All Reviewers"}, "comment": {"value": "Thank you for your feedback. We have started addressing each reviewer’s comments below. For minor points, such as clarifications, we acknowledge them and will incorporate the changes into the final manuscript. For more in-depth comments, we have added new experiments and analyses to the appendix of the revised manuscript and provided detailed responses when addressing individual reviewer comments. Please let us know if we haven’t fully addressed your questions."}}, "id": "Q1cKhwR3tu", "forum": "Lp1noMpMUG", "replyto": "Lp1noMpMUG", "signatures": ["ICLR.cc/2026/Conference/Submission15695/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15695/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission15695/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763684475013, "cdate": 1763684475013, "tmdate": 1763684475013, "mdate": 1763684475013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}