{"id": "xQRAo9YUQ3", "number": 6403, "cdate": 1757980717475, "mdate": 1759897916731, "content": {"title": "RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement Learning in Ride-Hailing", "abstract": "Ride-hailing platforms face the challenge of balancing passenger waiting times with overall system efficiency under highly uncertain supply–demand conditions. Adaptive delayed matching, which decides whether to assign drivers immediately or hold requests for batching, creates a fundamental trade-off between matching delay and pickup delay. Because these outcomes accumulate over long horizons and depend on stochastic, evolving supply–demand states, reinforcement learning (RL) is a natural framework for this problem. Yet existing approaches often oversimplify traffic dynamics, misrepresenting congestion effects, or employ RL models with shallow encoders that fail to capture complex spatiotemporal patterns.\n\nWe introduce the Regime-Aware Spatio-Temporal Mixture-of-Experts framework (RAST-MoE), which formalizes adaptive delayed matching as a regime-aware MDP and equips RL agents with a self-attention MoE encoder.  Instead of relying on a single monolithic network, our design allows different experts to specialize automatically, improving representation capacity while keeping per-sample computation efficient. A physics-informed congestion surrogate preserves realistic density–speed feedback while enabling millions of efficient rollouts. An adaptive reward scheme further guards against pathological strategies by dynamically penalizing service-quality violations.\n\nDespite its modest size of only 12M parameters, our framework consistently outperforms strong baselines. On real-world Uber trajectory data from San Francisco, it improves total reward by over 13%, reduces average matching delay by 15%, and reduces pickup delay by 10%. In addition, it demonstrates strong robustness across unseen demand regimes, stable training without reward hacking, and validated expert specialization. These findings demonstrate the broader potential of MoE-enhanced RL for large-scale decision-making tasks with complex spatiotemporal dynamics and large action spaces.", "tldr": "", "keywords": ["Deep Reinforcement Learning", "Mixture of Experts", "Urban Mobility", "Ride-Hailing"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a4c761f180fd8d895a682de20d412c03aaed36b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper formulates adaptive delayed matching as a Regime-Aware Spatio-Temporal MDP (RAST-MDP) with zone-wise binary actions and introduces RAST-MoE-RL, a PPO-based learner using a lightweight Transformer + sparse MoE encoder. A physics-informed congestion surrogate (hourly, MFD-consistent OD travel-time table) supplies fast, realistic pickup times; an adaptive reward with an online Lagrange multiplier discourages reward hacking. Experiment on 2019 CPUC Uber/Lyft traces for San Francisco dataset shows that the best configuration (16 experts, top-4) improves total reward by ~13% and reduces matching and pickup waits by ~10% and ~15%, respectively."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a new Regime-Aware Spatio-Temporal MDP (RAST-MDP) formulation tailored for dynamic urban decision-making. It combines transformer-based state encoding with MoE policy for adaptive regime specialization. \n\n2. It incorporates a physics-informed, MFD-consistent travel-time surrogate, a novel hybrid between transportation modeling and data-driven RL, and designs an adaptive reward mechanism with an online Lagrange multiplier to mitigate reward hacking. \n\n3. Real-world ride-hailing data is used to show the performance of the proposed approach."}, "weaknesses": {"value": "There are several design choices that require clarification or further justification:\n\n1. The policy factorizes over zones with Bernoulli heads. In congested networks, however, zone decisions are inherently interdependent due to spillback effects and shared fleet dynamics. Independent Bernoulli sampling may therefore under-capture coordinated batching behaviors, particularly near zone boundaries or along major corridors.\n\n2. While the adaptive penalty design is conceptually intuitive, its theoretical properties remain unexamined. The authors could clarify whether any stability or convergence guarantees (e.g., in a primal-dual or constrained RL sense) exist, and whether the adaptive coefficient λ exhibits oscillations or instability under non-stationary regimes.\n\n3. Routing based solely on the pooled global state could down-weight important local regime cues, potentially limiting the model’s ability to capture fine-grained spatial heterogeneity.\n\n4. The PPO training process for the MoE architecture requires further elaboration, particularly regarding how the learning pace among experts is balanced. If the sampling process disproportionately favors certain experts across iterations, it could lead to uneven expert training and degraded interpretability, since infrequently selected experts may appear “inactive” merely due to insufficient gradient updates rather than genuine specialization.\n\n5. The experiments focus exclusively on intra-city, cross-time evaluation. Given that “regime-awareness” is central to the proposed framework, a cross-city or out-of-distribution evaluation (e.g., holiday surges or incident disruptions) would provide stronger evidence of generalization and robustness."}, "questions": {"value": "1. The policy factorizes over zones with independent Bernoulli heads. How does this design capture inter-zone dependencies in congested networks, where decisions are spatially coupled? Have you considered structured or hierarchical policies to model coordinated batching?\n\n2. The adaptive penalty mechanism is intuitive, but its theoretical behavior is unclear. Are there any stability or convergence guarantees for the Lagrange coefficient λ, and how sensitive is training to its update rate?\n\n3. Sparse MoE architectures can suffer from unbalanced expert utilization. How do you ensure all experts are sufficiently trained during PPO updates, and do you use any load-balancing regularization or monitoring of routing diversity?\n\n4. Since expert routing is based on a pooled global embedding, how do you preserve fine-grained local regime information? Would token-level or multi-scale gating improve local sensitivity?\n\n5. Have you evaluated the model’s ability to generalize across cities or under unseen regimes (e.g., demand spikes, incidents, holidays)? Such tests seem central to validating “regime-awareness”.\n\n6. Can you provide evidence that different experts correspond to meaningful traffic or demand regimes? For example, do routing patterns or activation maps align with recognizable spatial-temporal behaviors?\n\n7. How does RAST-MoE-RL scale in terms of training time and memory compared to dense Transformer, especially when applied to larger urban areas?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SMUYhi8cov", "forum": "xQRAo9YUQ3", "replyto": "xQRAo9YUQ3", "signatures": ["ICLR.cc/2026/Conference/Submission6403/Reviewer_NJ8z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6403/Reviewer_NJ8z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866578998, "cdate": 1761866578998, "tmdate": 1762918807188, "mdate": 1762918807188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RAST-MoE-RL, a regime-aware spatio-temporal MoE framework for reinforcement learning in ride-hailing. It models adaptive delayed matching as a Regime-Aware MDP with a physics-informed congestion surrogate and adaptive reward. On Uber data from San Francisco, RAST-MoE-RL improves total reward by 13% and reduces delays by up to 15% over strong RL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1、Introduces regime-aware MoE in RL for dynamic control, bridging spatio-temporal modeling and policy optimization. \n2、Adaptive reward and anti-hacking mechanisms make the approach reliable and generalizable. \n3、Achieves strong performance with a compact model, showing effective use of sparse MoE capacity."}, "weaknesses": {"value": "1、Results are based only on San Francisco data.\n2、Adding results from optimization or heuristic methods would make the evaluation more comprehensive.\n3、Some ablation results are summarized but lack statistical significance reporting."}, "questions": {"value": "1、\tHow sensitive is RAST-MoE-RL to the number of experts and top-K routing under different city scales?\n2、\t Could the adaptive reward scheme destabilize if demand regimes shift abruptly (e.g., special events)?\n3、\t Have the authors compared against GNN-based policy encoders, which are also spatio-temporal but non-MoE?\n4、\tCould the MFD-based surrogate generalize to multi-city transfer without retraining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HujpgFSvT9", "forum": "xQRAo9YUQ3", "replyto": "xQRAo9YUQ3", "signatures": ["ICLR.cc/2026/Conference/Submission6403/Reviewer_8FE2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6403/Reviewer_8FE2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904770830, "cdate": 1761904770830, "tmdate": 1762918806772, "mdate": 1762918806772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RAST-MoE-RL, a regime-aware spatio-temporal Mixture-of-Experts framework for deep reinforcement learning in ride-hailing. It addresses challenges like non-stationary traffic, training instability, and limited policy expressiveness via: (1) A physics-informed RAST-MDP environment with adaptive delayed matching; (2) An anti-hacking reward scheme with online constraint adaptation; (3) A compact MoE encoder (12M parameters) that routes inputs to specialized experts for different demand-supply regimes. Evaluated on real Uber data, it improves total reward by 13% and reduces matching/pickup delays compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The framework is well motivated. MoE architecture enables specialized experts for distinct traffic/demand patterns (e.g., peak vs. off-peak), validated via utilization analysis.\n- Sparse expert activation maintains low FLOPs despite high representational capacity.\n- Robust Reward Design: Adaptive penalty multiplier prevents reward hacking and stabilizes training.\n- Macroscopic travel-time surrogate enables efficient simulation for millions of RL rollouts."}, "weaknesses": {"value": "- While tested on unseen demand, reliance on precomputed OD travel times may limit adaptability to unforeseen urban dynamics.\n- Rarely activated experts (e.g., off-peak specialists) are critical but may underutilize capacity."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sHdIyJSzV4", "forum": "xQRAo9YUQ3", "replyto": "xQRAo9YUQ3", "signatures": ["ICLR.cc/2026/Conference/Submission6403/Reviewer_ep2N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6403/Reviewer_ep2N"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962139915, "cdate": 1761962139915, "tmdate": 1762918806397, "mdate": 1762918806397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the adaptive delayed matching problem in ride-hailing platforms by proposing the RAST-MoE-RL framework. The framework integrates three core components through a regime-aware spatio-temporal MDP: a congestion surrogate model based on Macroscopic Fundamental Diagrams, an adaptive reward scheme, and a lightweight spatio-temporal mixture-of-experts encoder (12 million parameters). Experiments on San Francisco Uber data show that the framework achieves a 13% improvement in total reward, a 10% reduction in matching delay, and a 15% reduction in pickup delay compared to baseline methods, while also demonstrating robust performance under unseen demand regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1）The RAST-MDP formulation directly addresses real-world pain points of ride-hailing platforms, such as non-stationary traffic congestion and long-horizon decision instability. The physics-informed congestion surrogate strikes a balance between realism and scalability, which is critical for industrial deployment.\n2）The RAST-MoE encoder achieves superior performance with only 12M parameters—outperforming larger dense models and shallow architectures. This highlights the efficiency of sparse expert specialization over parameter expansion, a valuable contribution to resource-constrained city-scale RL tasks."}, "weaknesses": {"value": "1）Most baseline RL algorithms are relatively old (e.g., DQN 2015, A2C 2016, PPO 2017, ACER 2017), with only GRPO (2024) representing recent progress. The manuscript fails to compare against state-of-the-art MoE-RL or ride-hailing-specific RL methods recently (such as from 2023–2024). \n2）The “dynamic pickup time modeling” (MFD-based surrogate + hourly OD lookup) is a conventional extension of static congestion assumptions rather than a disruptive innovation. Similar MFD-based traffic modeling has been widely used in transportation research, and the manuscript does not introduce novel tweaks to improve its accuracy or adaptability to real-time events (e.g., accidents, weather).\n3）The adaptive reward scheme is compared only against extreme fixed-weight ratios (1:1, 4:1, 8:1, 1:4, 1:8). These ratios are unrealistic for real-world ride-hailing operations, where platforms typically use moderately balanced weights (e.g., 2:1, 3:1) that already mitigate pathological strategies. Without testing against such practical fixed weights, the manuscript cannot fully demonstrate the adaptive scheme’s added value over industry-standard reward designs.\n4）Figures should be mentioned and explained in the main text, e.g., Figure 4 and Figure 5."}, "questions": {"value": "1）Why were no 2023–2024 MoE-RL or ride-hailing RL methods included as baselines? How would RAST-MoE-RL perform against these newer methods in terms of reward, delay, and training stability? \n2）The current congestion surrogate uses hourly fixed speeds and precomputed static routes. How would the framework perform if faced with real-time traffic anomalies (e.g., road closures, rush-hour accidents)? Has any sensitivity analysis been conducted to test the model’s robustness to deviations from precomputed hourly congestion patterns? \n3）The service quality violation tolerance (set to 5%) and penalty update step are critical to the adaptive reward. Were these parameters tuned via sensitivity analysis (e.g., testing different tolerance levels or update steps)? How do they align with typical ride-hailing platform service level agreements (SLAs)? \n4）While expert masking experiments confirm the value of both high- and low-frequency experts, the manuscript does not explain how the router maps specific supply–demand–congestion regimes to experts. Are there quantitative (e.g., feature clustering) or qualitative (e.g., activation heatmaps) analyses to reveal this mapping?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EwRflc6PT5", "forum": "xQRAo9YUQ3", "replyto": "xQRAo9YUQ3", "signatures": ["ICLR.cc/2026/Conference/Submission6403/Reviewer_jLCz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6403/Reviewer_jLCz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762340608903, "cdate": 1762340608903, "tmdate": 1762918805665, "mdate": 1762918805665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}