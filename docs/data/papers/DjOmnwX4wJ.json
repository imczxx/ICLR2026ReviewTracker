{"id": "DjOmnwX4wJ", "number": 7472, "cdate": 1758023645093, "mdate": 1763728526254, "content": {"title": "Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling", "abstract": "Test-time scaling has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs) by allocating additional computational resources during inference. However, this paradigm is inherently inefficient due to the generation of redundant and repetitive reasoning traces, leading to significant computational overhead. Speculative decoding offers a promising avenue for mitigating this inefficiency, yet its efficacy in the structured and repetition-rich context remains unexplored. To bridge this gap, we introduce the first comprehensive benchmark designed to evaluate speculative decoding methods in LLM test-time scaling. Our benchmark provides consistent experimental protocols across representative test-time scaling paradigms (e.g., Best-of-N sampling and multi-round thinking), enabling a fair comparison of three major categories of speculative decoding: model-based, training-based, and n-gram-based methods. Extensive experiments reveal that simple n-gram-based methods effectively capture repetitive patterns, demonstrating unique potential in accelerating test-time scaling. This phenomenon demonstrates the value of integrating n-gram-based methods with model-based or training-based approaches to benefit both repetitive and diverse reasoning in test-time scaling. We hope this benchmark spurs further research on speculative decoding for test-time scaling, enabling faster and more practical reasoning in LLMs through better handling of repetitive and diverse reasoning paths.", "tldr": "", "keywords": ["Speculative Decoding", "Test-Time Scaling"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f8ec66df5e81f42fa3f8fa03f8a3548ce551f2b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents comprehensive benchmark experiments evaluating three types of speculative decoding methods -- model-based, training-based, and N-gram-based -- in the context of LLM test-time scaling. The empirical results reveal several notable findings: (a) training-based methods exhibit robustness to sampling temperature, (b) N-gram-based methods are effective at identifying repeated patterns at the token level, and (c) hybrid approaches can leverage the strengths of both. These findings offer insights into the behavior of different approaches for reasoning models and may inspire further research on more effective methods for test-time scaling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Empirically evaluates an important setting for the use of speculative decoding, i.e., test-time scaling for reasoning models. This setting can particularly benefit from more efficient inference, and understanding what works and why is highly valuable.\n\n2. Evaluates a wide range of speculative decoding methods including model-based, training-based, N-gram-based, and hybrid methods.\n\n3. Reports several notable findings, suggesting that hybrid approaches may be particularly promising."}, "weaknesses": {"value": "1. The datasets used for the benchmark experiments are mostly focused on math reasoning. Evaluating how the findings extend to other types of data, e.g., coding, general reasoning, social reasoning, would provide more valuable insights into the methods.\n\n2. Limited model scales. Results for models of scales other than 8B would strengthen the findings.\n\n3. While the paper reports multiple notable findings regarding how different types of methods perform on the benchmarks, technical novelty (e.g., proposing a better working hybrid method) is rather limited."}, "questions": {"value": "Q. Have the authors experimented with models of other scales, other types of reasoning benchmarks, etc.?\n\nQ. How large is the variance for the performance numbers, e.g., reported in Tables 3 and 4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NRp2MbNniu", "forum": "DjOmnwX4wJ", "replyto": "DjOmnwX4wJ", "signatures": ["ICLR.cc/2026/Conference/Submission7472/Reviewer_Hr8t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7472/Reviewer_Hr8t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895369316, "cdate": 1761895369316, "tmdate": 1762919588022, "mdate": 1762919588022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents the first benchmark for evaluating speculative decoding methods in test-time scaling of LLMs, offering standardized protocols across paradigms like Best-of-N sampling and multi-round thinking. Experiments show that simple N-gram-based methods efficiently handle repetitive reasoning patterns, suggesting their integration with model- or training-based approaches can make test-time scaling both faster and more effective."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackes an important problem of evaluating speculative decoding approaches in test-time scaling scenarios, and provide a formal benchmark.\n2. The authors perform extensive evaluations ranging in nine different speculative decoding methods, four datasets, and two LLMs.\n3. The authors provide a comprehensive analysis of the experiment results, along with clear takeaways for the readers."}, "weaknesses": {"value": "1. As far as the reviewer understands, the reasoning traces were sampled sequentially in best-of-N scenarios, using batch size 1 (L264). However in realistic scenarios, these traces would be sampled in parallel (i.e. using batch decoding). This suggests a critical gap between the benchmark performance and real-world benefits.\n\n2. The benchmark does not consider test-time tree search (e.g. beam search or DVTS [1]), which is another major test-time scaling strategy. (Note: This weakness is not critical, as the reviewer understands that it might add too much complexity to the benchmark.)\n\n[1] Beeching et. al., Scaling Test Time Compute with Open Models"}, "questions": {"value": "Would the Best-of-N speedups be different if the reasoning traces were samples in a batch? (e.g. The amount of overlap would be much smaller for N-gram based models)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qfABNl91aK", "forum": "DjOmnwX4wJ", "replyto": "DjOmnwX4wJ", "signatures": ["ICLR.cc/2026/Conference/Submission7472/Reviewer_eh5y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7472/Reviewer_eh5y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926680953, "cdate": 1761926680953, "tmdate": 1762919587499, "mdate": 1762919587499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses different methods of speculative decoding and tests them across reasoning benchmarks to measure MAT/wall clock speed up. \nThe paper is a little limited in scope and could have been a bit more thorough. If the authors wouldn't mind running this on some larger benchmarks with different scaled models, I believe this would make a very nice paper. I'm still content with the paper, so I recommend acceptance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It's a nice paper with useful results. \n2. The Takeaways are a pretty good analysis of what's going on with each speculative decoding method. \n3. Presentation is pretty nice. \n4. Practical results."}, "weaknesses": {"value": "1. It's not really a benchmark. It's just running the different methods on common benchmarks. Not a huge deal since it's still good science. \n2. WOuld have been nice to scale these across difficult model scales. The results may have been different. \n3. Datasets are tiny, like 120 total samples. Could have done this on more deata. \n4. Error bars could have been nice if you could rerun this multiple times to see what happens.\n5. Varying batch size would have been nice."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DDyJ2Ufx0T", "forum": "DjOmnwX4wJ", "replyto": "DjOmnwX4wJ", "signatures": ["ICLR.cc/2026/Conference/Submission7472/Reviewer_mU4f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7472/Reviewer_mU4f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938251126, "cdate": 1761938251126, "tmdate": 1762919586806, "mdate": 1762919586806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a benchmark paper that targets on accelerating test time scaling method by speculative decoding. It compares model-based, training-based, and N-gram approaches.\nThe key finding is that simple N-gram-based methods (like SAM) are highly effective at capturing this redundancy. A hybrid method, SAM[EAGLE-3], achieved the best overall speedup by combining N-gram's repetition capture with a training-based method's semantic prediction. N-gram methods also show \"progressive acceleration,\" getting faster across multiple reasoning turns , though they are sensitive to temperature"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem this paper targets seems relevant, and the method is intuitive and motivated.\n\n- There are many interesting findings and the experiments seem to be solid. For example, the most interesting finding is that simple N-gram-based methods, particularly SAM, are highly effective at capturing the redundancy in Best of N or Multi Turn answers. The findings are comprehensive."}, "weaknesses": {"value": "- The one concern I have right now is the novelty of the method. Maybe this is because this is my first time reviewing on the dataset and benchmark field.\n\n- Whether other complex reasoning frameworks such as Tree-of-thought or MCTS-based search algorithms could also benefits from speculative decoding?\n\n- Also, would the approach generalized to other field such as code generation and summarization?\n\n- The paper correctly identifies that hybrid methods (like SAM[EAGLE-3]) are a promising direction and achieve the best performance. However, it also admits that \"current hybrid strategies remain heuristic\" and that the potential of N-gram matching is \"underexploited\" due to \"suboptimal integration strategies\". The paper benchmarks one such simple strategy but does not propose or experiment with more dynamic or refined strategies to truly unlock this potential."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qBSTUCrFhD", "forum": "DjOmnwX4wJ", "replyto": "DjOmnwX4wJ", "signatures": ["ICLR.cc/2026/Conference/Submission7472/Reviewer_7EPL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7472/Reviewer_7EPL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964677199, "cdate": 1761964677199, "tmdate": 1762919586106, "mdate": 1762919586106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}