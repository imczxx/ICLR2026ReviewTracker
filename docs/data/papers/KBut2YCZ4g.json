{"id": "KBut2YCZ4g", "number": 3479, "cdate": 1757442365219, "mdate": 1763735627630, "content": {"title": "Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning", "abstract": "While scaling laws for large language models (LLMs) during pre-training have\nbeen extensively studied, their behavior under reinforcement learning (RL) post-\ntraining remains largely unexplored. This paper presents a systematic empirical\ninvestigation of scaling behaviors in RL-based post-training, with a particular fo-\ncus on mathematical reasoning. Based on a set of experiments across the full\nQwen2.5 dense model series (0.5B to 72B), we characterize how model scale,\ndata volume, and computational budget interact to shape performance. Our anal-\nysis leads to four key findings: (1). Under a fixed computational budget, larger\nmodels trained for fewer steps consistently outperform smaller models trained\nfor more steps. (2). Given a fixed amount of training data, larger models achieve\nsuperior sample efficiency, yielding lower loss. (3). In data-constrained regimes,\nrepeated reuse of high-quality data proves highly effective, as final performance\nis primarily governed by the total number of optimization steps rather than the\nuniqueness of samples. (4) These scaling behaviors are robust across both base\nand instruction-tuned models, which share similar learning dynamics (e.g., larger\nmodels show faster convergence) even while differing in absolute accuracy. We\nfurther show that the relationship between test loss, compute, and data can be\nmodeled by a predictive power-law with an analytic learning efficiency term k(N )\nthat demonstrates an efficiency saturation effect as model size increases. Collec-\ntively, these results provide a principled foundation and practical guidelines for\nefficiently scaling the reasoning capabilities of LLMs through RL post-training.", "tldr": "", "keywords": ["large language model", "neural scaling law", "reinforcement learning", "LLM reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4126d66e0f93818357328acad0c5ef10c8fba412.pdf", "supplementary_material": "/attachment/2cfcd3f515b477e3b2e10e1a373e3691548a088c.zip"}, "replies": [{"content": {"summary": {"value": "The paper conducts an empirical analysis of the effect of different choices in RL fine-tuning for mathematical reasoning. The authors conduct various experiments, varying the model size, compute budget, data size and data repetition. The authors find that larger models train faster and require less data compared to smaller ones, and that data repetition is effective in the data-constrained regime."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper focuses on scaling properties of RL for mathematical reasoning, covering some core questions that are of primary interest in the field.\n- The authors conduct a rigorous study of different aspects of scaling the RL training, including model size, compute budget and dataset size.\n- I find some findings interesting, including the effect of data repetition and number of rollouts."}, "weaknesses": {"value": "- The main issue I see with the paper is that, while the authors run a large set of experiments, the analysis lacks an overall unifying meaningful conclusion. The authors attempt to suggest that the analysis results in some power laws for e.g. compute or data vs. loss, but the data mostly seems very noisy and deviates significantly from the predicted law. In particular, it seems that summarizing the finding as \"larger models train faster\" captures the more meaningful results of this study, and fitting scaling laws doesn't seem to contribute a lot.\n- Another problem in the overall setting is the fact that, since RL is performed on pretrained models (and not \"from scratch\" like in \"standard\" scaling law analysis), larger models have better initial accuracy. This means that the larger models will observe more positive rewards from the beginning, and therefore it is expected that they will train faster. I think that an experiment that controls for the initial accuracy, e.g. trains models of different sizes with the same/similar initial accuracy, would be more appropriate for this type of study. Alternatively, the authors could study the same models but on a task where they all have similar initial accuracy (if such a task exists).\n- In the introduction of the scaling law analysis, it was not completely clear to me whether the compute considered is only the RL compute, or does it include the pertaining compute as well? I believe that the analysis does not take into account the compute required for pertaining, which makes the overall conclusion unclear. \n- Some fitted curves (e.g. Figure 5 and Figure 7) really do not fit the data at all, and it's unclear what they are adding to the analysis.\nMore minor comments:\n- Dataset setting: \"We further sort the problems by increasing difficulty (decreasing pass rate)\" - according to which model are these sorted?\n- Lines 72-74 are hard to read and understand.\n- Line 46-47: \"scales power-law\" => \"scales like a power-law\"?\n- Line 96 - \"the key hyperparameter rollout number\"\n- Line 210 - \"making more challenging loss reduction.\" => \"making the loss reduction more challenging\"\n- I'm not sure why the authors choose to use the term \"test loss\" instead of \"accuracy\" or \"error\", as \"loss\" is typically used to discuss something like e.g. cross-entropy loss. Using \"zero-one loss\" would also be better in my opinion."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SQ4BQJ116J", "forum": "KBut2YCZ4g", "replyto": "KBut2YCZ4g", "signatures": ["ICLR.cc/2026/Conference/Submission3479/Reviewer_xzeJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3479/Reviewer_xzeJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761336692835, "cdate": 1761336692835, "tmdate": 1762916747228, "mdate": 1762916747228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies scaling RL post training in models under various regimes (compute constrained, data constrained) for mathematical reasoning. The authors do post training using Qwen 2.5 across all possible sizes offered on the mathematics subset of the guru-RL-92k and evaluate on several benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The main strengths of this paper are:\n- the authors run the evals at several model sizes (from 500M to 14B), which can be considered reasonably large for an academic setting\n- the authors ablate over the number of rollouts in GRPO which has been shown in literature to be very important for performance\n- if I understand correctly, in line 169 the authors say they repeated each config 3 times across seeds - this is very much appreciated since in RL in general the seed can cause quite large differences"}, "weaknesses": {"value": "The main weaknesses:\n- as far as I understand, the authors did not sweep over hyperparameters. It is not clear to me why these exact values for the learning rate etc were used\n- the metrics reported are insufficient. From my understanding, the authors only report pass@1. For a more comprehensive overview, it would have been better to see pass@64, and majority@1 and majority@64\n- I am not sure why the plots do not have standard deviations plotted as well as it seems to be possible from the collected data\n\nAs a separate point, my main concern is on section 3.4. Unless I am misunderstanding the figures, 100 steps of training is too few training steps to draw any conclusion about the behaviour. I am curious if these plots would still hold at much longer training times and if the trends would swap. It seems from figure 4 that you do have enough data available to train for much longer."}, "questions": {"value": "- In section 3.4, for tau=100, given that you train for 100 steps, does this mean you repeat a single batch 100 times? If so, for how many rollouts?\n- In section 3.3, what does it mean that you train (referring to post-training in this setting) to convergence? Are you referring to doing GRPO on the guru-RL-92k dataset until the average reward hits 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fckSOFTMcN", "forum": "KBut2YCZ4g", "replyto": "KBut2YCZ4g", "signatures": ["ICLR.cc/2026/Conference/Submission3479/Reviewer_Wme4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3479/Reviewer_Wme4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975317114, "cdate": 1761975317114, "tmdate": 1762916746267, "mdate": 1762916746267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "Dear AC and Reviewers,\n\nWe sincerely thank you for your detailed and constructive comments and feedback on our manuscript. We have carefully reviewed your valuable suggestions and identified the primary concerns, which centered on the generality of our experimental scope, **the predictive power of the scaling laws, the clarity of our main conclusions**, and the need for a more rigorous statistical analysis.\n\nTo directly address these points and strengthen our findings, we have performed an revision of our work, incorporating new experiments and deeper analysis:\n- **Expanded Experimental Scope to 72B**: We expanded our experiments to include the **32B** and **72B** models (both Base and Instruct models). Our study now covers the entire Qwen2.5 dense model series.\n- **Analytic Form for Learning Efficiency $k(N)$ Revealing Saturation**: We proposed an analytic form for the learning efficiency coefficient $k(N)$ base on deeper analysis of comprehensive data. It demonstrates that the learning efficiency gains from model scale follow a saturation curve towards a maximum limit $K_{\\text{max}}$, as shown in Section 3.1, Section 3.2 and Figure 11 in Appendix C.2. \n- **Validated the predictive power of the empirical scaling law**:\n    - Inter-Model Extrapolation: Successfully predicting the training efficiency of the 72B model by fitting on 0.5B-32B model data (as shown in the new Figures 2a/2b and 3a/3b in Section 3.1 and 3.2)\n    - Intra-Model Forecasting: Accurately projecting a single model's future training trajectory based on its initial performance data (as shown in the new Figures 2c/2d and 3c/3d in Section 3.1 and 3.2)\n- **Enhanced Statistical Analysis**: We added new statistical analysis for the test loss and fitted parameters, which is now detailed in the Appendix C.3.\n\nWe also utilized new data to update and refine the discussion and figures in Section 3.3 and 3.5, as well as other accompanying appendices/figures/tables.\n\nAll updates are colored in red in the revised PDF.\n\nThese updates allow us to **strengthen our previous contribution by following**:\n- **Efficiency Saturates**: With given analytic K(N) formula, we can see that larger models do consistently exhibit higher learning efficiency, but the gain brought by model parameter follows a predictable saturation curve to max learning speed $K_{max}$.\n- **The Law is Predictive**: The log-log scale formula describing scaling behaviors is predictive, allowing for reliable extrapolation.\n\nThanks for reading our rebuttal and looking forward to your reply."}}, "id": "ZDE08nMmq8", "forum": "KBut2YCZ4g", "replyto": "KBut2YCZ4g", "signatures": ["ICLR.cc/2026/Conference/Submission3479/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3479/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3479/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763729942596, "cdate": 1763729942596, "tmdate": 1763729942596, "mdate": 1763729942596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "Dear AC and Reviewers,\n\nWe sincerely thank you for your detailed and constructive comments and feedback on our manuscript. We have carefully reviewed your valuable suggestions and identified the primary concerns, which centered on the generality of our experimental scope, **the predictive power of the scaling laws, the clarity of our main conclusions**, and the need for a more rigorous statistical analysis.\n\nTo directly address these points and strengthen our findings, we have performed an revision of our work, incorporating new experiments and deeper analysis:\n- **Expanded Experimental Scope to 72B**: We expanded our experiments to include the **32B** and **72B** models (both Base and Instruct models). Our study now covers the entire Qwen2.5 dense model series.\n- **Analytic Form for Learning Efficiency $k(N)$ Revealing Saturation**: We proposed an analytic form for the learning efficiency coefficient $k(N)$ base on deeper analysis of comprehensive data. It demonstrates that the learning efficiency gains from model scale follow a saturation curve towards a maximum limit $K_{\\text{max}}$, as shown in **Section 3.1, Section 3.2** and **Figure 11** in Appendix C.2. \n- **Validated the predictive power of the empirical scaling law**:\n    - Inter-Model Extrapolation: Successfully predicting the training efficiency of the 72B model by fitting on 0.5B-32B model data (as shown in the new Figures 2a/2b and 3a/3b in **Section 3.1 and 3.2**)\n    - Intra-Model Forecasting: Accurately projecting a single model's future training trajectory based on its initial performance data (as shown in the new Figures 2c/2d and 3c/3d in **Section 3.1 and 3.2**)\n- **Enhanced Statistical Analysis**: We added new statistical analysis for the test loss and fitted parameters, which is now detailed in the **Appendix C.3**.\n\nWe also utilized new data to update and refine the discussion and figures in Section 3.3 and 3.5, as well as other accompanying appendices/figures/tables.\n\n**All updates are colored in red in the revised PDF**.\n\nThese updates allow us to **strengthen our previous contribution by following**:\n- **Efficiency Saturates**: With given analytic K(N) formula, we can see that larger models do consistently exhibit higher learning efficiency, but the gain brought by model parameter follows a predictable saturation curve to max learning speed $K_{max}$.\n- **The Law is Predictive**: The log-log scale formula describing scaling behaviors is predictive, allowing for reliable extrapolation.\n\nThanks for reading our rebuttal and looking forward to your reply."}}, "id": "ZDE08nMmq8", "forum": "KBut2YCZ4g", "replyto": "KBut2YCZ4g", "signatures": ["ICLR.cc/2026/Conference/Submission3479/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3479/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3479/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763729942596, "cdate": 1763729942596, "tmdate": 1763740417367, "mdate": 1763740417367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses establishing scaling laws for reinforcement learning (RL) post-training on LLMs for mathematical reasoning tasks. Across 54 Qwen2.5 experiments (0.5B–14B), the authors find that larger models are both more compute- and data-efficient, and that performance depends mainly on total optimization steps rather than data uniqueness, making data reuse highly effective. RL improves in-domain math reasoning but transfers poorly to other domains, and scaling behaviors remain consistent across base and instruct models. Overall, the work highlights the following: favor larger models, reuse high-quality data, and tune rollout size to compute budget."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Establishing general and actionable scaling laws for the RL post-training setting for LLMs is an important problem.\n- The authors repeat three runs for each configuration in their results.\n- The specificity of the data reuse results, and the observation about the gap between dense models and mixture of experts models are interesting."}, "weaknesses": {"value": "- The work’s findings are confined to a single model family (Qwen2.5) and math dataset with an assigned curriculum. The extent of the generality of the specificity of their findings (eg. the fitted scaling law) since RL post-training behavior is highly sensitive to reward design, data composition, and curriculum—unlike the more standardized and robust scaling behavior seen in pretraining.\n- Furthermore, it’s unclear what the actionable takeaways from this work are; like the authors write in the introduction, “these laws … are invaluable... because they provide actionable guidance on how to distribute scarce computational resources most effectively”. The observations made in each section are too broad to be actionable (perhaps with the exception of Observation 4) and already reflect the general sentiment that if one has the computational resources, starting RL fine-tuning from a larger model is more beneficial. \n- While this paper identifies empirical scaling trends for RL post-training, I believe the real practical value of scaling laws for RL lies in whether they’re able to provide predictive insights. To be more specific, whether one can forecast when additional rollout compute will continue to yield gains or when performance will plateau. This work fits power-law curves post-hoc but does not test whether these fits can extrapolate to unseen compute budgets or reliably guide future training runs.\n- I believe the work would benefit from analyzing additional metrics like pass@k for their runs; it is also of interest to see whether model capability is improving under this metric (eg. are larger models reliably able to solve more problems that their base model previously couldn't have even under multiple samples)?\n\nMinor:\n- Line 313: toal -> total"}, "questions": {"value": "- Is the normalized error rate used for the scaling law calculated with respect to greedy sampling?\n- How does group size and the mini batch size used for the GRPO actor update relate to the batch size of 512? Specifically how many offline update steps are being taken with respect to the rollout policy? \n- What do the pass@k (or equivalent reward-based) scaling curves look like under your setup? Do they follow similar power-law behavior as the test loss curves you report?\n- Could the authors elaborate on what they see as the main conceptual or practical value of these results? In particular, how should practitioners interpret their findings when deciding how to allocate RL compute or data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iXWqmUeUx5", "forum": "KBut2YCZ4g", "replyto": "KBut2YCZ4g", "signatures": ["ICLR.cc/2026/Conference/Submission3479/Reviewer_5U2P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3479/Reviewer_5U2P"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154589757, "cdate": 1762154589757, "tmdate": 1762916745973, "mdate": 1762916745973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an empirical study of reinforcement-learning (RL) post-training for mathematical reasoning across Qwen2.5 models from 0.5B to 14B parameters. Using a consistent GRPO setup and ~50k curated math problems, the authors probe three regimes—compute-constrained, data-constrained, and data-reuse—over 50+ controlled runs. The central findings are: (i) under fixed compute, larger models trained for fewer steps achieve lower test loss and steeper loss–compute slopes; (ii) with fixed unique data, larger models are more sample-efficient and follow a power-law loss–data relation; (iii) when trained to convergence on sufficiently large data, performance improves monotonically with model size, though small models deviate from the idealized fits; and (iv) with steps held fixed, moderate data repetition has little impact, while extreme reuse leads to overfitting. An ablation on GRPO rollout group size shows larger groups improve sample efficiency, but the compute-optimal choice depends on budget. The paper’s stated contribution is practical guidance for allocating compute, data, and reuse when performing RL post-training on reasoning tasks. While the execution is careful and the results are useful for practitioners, the study primarily re-validates familiar scaling intuitions (bigger models, more data, cautious repetition) and offers limited technique-level insight specific to RL post-training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. All experiments use one model family (Qwen2.5) and one RL algorithm (GRPO) with matched tokenization, prompting, and reward extraction. This reduces design variance and makes cross-run comparisons credible, letting readers attribute differences to scale/data rather than to confounders (architecture swaps, reward model changes, or prompt drift).\n\n2. The compute-constrained, data-constrained, and reuse-constrained formulations are clearly defined and answered with targeted sweeps. Practitioners get concrete guidance: prefer bigger models at fixed FLOPs, expect diminishing returns from unique data vs. moderate repetition, and tune rollout group size to budget. These are decisions teams regularly face in RL post-training.\n\n3. The paper does not stop at in-distribution accuracy; it measures transfer to code/logic/STEM, reports degradation cases, and includes an efficiency ablation (rollout size). This candor about failure modes and trade-offs increases the trustworthiness and operational value of the study."}, "weaknesses": {"value": "1. The main results mirror known pretraining/data-centric scaling laws: larger models win under fixed compute; power-law data scaling; moderate repetition is fine until it isn’t. And the \nThe paper offers little RL-specific theory (e.g., KL/reward-landscape analysis, credit-assignment diagnostics) to explain why these slopes arise, so the contribution feels incremental.\n\n2. Although the paper tries to figure the scaling laws for reinforcement learning, there lacks of experiments to demonstrate the effectiveness in \"predictable scaling\" of the proposed laws and whether it can help the scaling trends of RL training.\n\n3. All results come from a single dense family ≤14B on math-centric data. Without diversity in model families (MoE, larger dense), domains, or data curricula, it’s unclear whether the conclusions persist beyond the studied slice.\n\n4. The core “test loss” is derived from Pass@1 extraction on a small (≈500) in-distribution holdout and may be sensitive to parsing/prompt choices. Confidence intervals and variance across random seeds are sparse; compute/data accounting under heavy reuse isn’t fully audited."}, "questions": {"value": "Please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u5WAH0ScOU", "forum": "KBut2YCZ4g", "replyto": "KBut2YCZ4g", "signatures": ["ICLR.cc/2026/Conference/Submission3479/Reviewer_MN3w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3479/Reviewer_MN3w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762400129604, "cdate": 1762400129604, "tmdate": 1762916745123, "mdate": 1762916745123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}