{"id": "kipQYpoZf1", "number": 1350, "cdate": 1756874795853, "mdate": 1759898213595, "content": {"title": "PhysicsMinions: Winning Gold Medals in the Latest Physics Olympiads with a Coevolutionary Multimodal Multi-Agent System", "abstract": "Physics is central to understanding and shaping the real world, and the ability to solve physics problems is a key indicator of real-world physical intelligence. Physics Olympiads, renowned as the crown of competitive physics, provide a rigorous testbed requiring complex reasoning and deep multimodal understanding, yet they remain largely underexplored in AI research. Existing approaches are predominantly single-model based, and open-source MLLMs rarely reach gold-medal-level performance. To address this gap, we propose PhysicsMinions, a coevolutionary multi-agent system for Physics Olympiad. Its architecture features three synergistic studios: a Visual Studio to interpret diagrams, a Logic Studio to formulate solutions, and a Review Studio to perform dual-stage verification. The system coevolves through an iterative refinement loop where feedback from the Review Studio continuously guides the Logic Studio, enabling the system to self-correct and converge towards the ground truth. Evaluated on the HiPhO benchmark spanning 7 latest physics Olympiads, PhysicsMinions delivers three major breakthroughs: (i) Strong generalization: it consistently improves both open-source and closed-source models of different sizes, delivering clear benefits over their single-model baselines; (ii) Historic breakthroughs: it elevates open-source models from only 1–2 to 6 gold medals across 7 Olympiads, achieving the first-ever open-source gold medal in the latest International Physics Olympiad (IPhO) under the average-score metric; and (iii) Scaling to human expert: it further advances the open-source Pass@32 score to 26.8/30 points on the latest IPhO, ranking 4$^\\text{th}$ of 406 contestants and far surpassing the top single-model score of 22.7 (ranked 22$^\\text{nd}$). Generally, PhysicsMinions offers a generalizable framework for Olympiad-level problem solving, with the potential to extend across disciplines.", "tldr": "PhysicsMinions is a coevolutionary multimodal multi-agent system that achieves gold-medal performance in latest physics Olympiads, significantly outperforming single-model baselines and advancing open-source models to human-expert levels.", "keywords": ["physics Olympiad", "multi-agent system", "coevolutionary framework", "multimodal model"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/32d2221815fe484f7740e3ed53bb70de2b0a7a90.pdf", "supplementary_material": "/attachment/33ae425d5b96c718e9f850d263c500e27f92b666.zip"}, "replies": [{"content": {"summary": {"value": "This work introduces a multi-agent framework for solving olympic-level physics problems. The proposed framework PhysicsMinions consists of a visual studio that converts the images into structured representation, a logic studio that provides the analysis and solution to the problem, and a review studio that check the solution from the perspective of physics-related format and correctness of the solution. The interplay happens between the logic and review module to support refinement. Results show that such method achieves the gold medal level performance on all olympic-level physics exam benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Gold medal performance with a light multi-agent framework is exciting.\n- The comparison of different visual representation format reveals the preference of existing model in processing the reasoning information figures and images."}, "weaknesses": {"value": "- The key feature of such a multi-modal multi-agent system for challenging physics reasoning problems would be the ability to interpret the scientific figures. The information and findings regarding this is limited. Besides, the interpretation is fixed during the iterative process (perhaps related to the benchmark features).\n- The comparison between physicsminions and best-of-N is not well elaborated. Are they restricted with the same number of solution attempts or similar number of tokens? Specially considering that Best-of-N and self-MoA achieves on-par performance with PhysicsMinion in EuPhO and PanMechanics."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YsOgW6svWR", "forum": "kipQYpoZf1", "replyto": "kipQYpoZf1", "signatures": ["ICLR.cc/2026/Conference/Submission1350/Reviewer_9HJu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1350/Reviewer_9HJu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801597247, "cdate": 1761801597247, "tmdate": 1762915744943, "mdate": 1762915744943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing single-models fail to solve Physics Olympiad tasks because it requires both complex reasoning and multimodal understanding. In this paper, the authors propose PHYSICSMINIONS, a framework of three connected studios: Visual, Logic, and Review, which refine solutions through repeated feedback. This coevolutionary process unites structured perception, reasoning, and verification, enabling the first open-source gold medal and near human-level results in the latest International Physics Olympiad."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper clearly identifies the limitations of the single model architecture in Physics Olympiad tasks (fail to handle both complex reasoning and multimodal understanding).\n2. They evaluate on a real and challenging benchmark to ensure fair and representative evaluation and significantly improve the performance (i.e. approaching human-expert level).\n3. The proposed system includes three components and delegate the tasks. This coevolutionary multi-agent system allows agents to cooperate, reflect, and verify each other's outputs, overcoming the limits of single model reasoning."}, "weaknesses": {"value": "1. I think it would be good to report the computation cost since the coevolution loop likely adds some iterations for the communications. I want to see a comparison between the single model and multi agent system.\n2. If I understand correctly, all agents use the same LLM, I think the system lacks heterogeneity and diversity of reasoning. LLM from different model family could have different aspect of reasoning, visualization abilities. What's the performance to combine different LLMs? I'm curious if this can further improve the results.\n3. Related work section does not cover the research on MLLM-based chart and figure understanding. I think the authors could add an section to talk about the visualization ability of current LLM on understanding the chart and figure (for example, CharXiv https://arxiv.org/abs/2406.18521, an evaluation benchmark)."}, "questions": {"value": "In this paper, the authors mention that the dual-verifier design assumes that consistent verification implies correctness. I'm curious what mechanisms exist to detect or prevent \"false consensus\", where both verifiers reinforce the same error?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EL9UIUNs6j", "forum": "kipQYpoZf1", "replyto": "kipQYpoZf1", "signatures": ["ICLR.cc/2026/Conference/Submission1350/Reviewer_KQyp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1350/Reviewer_KQyp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976612444, "cdate": 1761976612444, "tmdate": 1762915744717, "mdate": 1762915744717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission introduces PhysicsMinions, a coevolutionary multimodal multi-agent framework designed to elevate AI performance on challenging Olympiad-level physics problems. The system is structured into three specialized studios: a Visual Studio for diagram interpretation, a Logic Studio for solution generation and refinement, and a Review Studio for dual-stage verification. The agents interact through iterative feedback loops, combining structured visual extraction, explicit stepwise reasoning, and systematic error detection/correction. The framework is evaluated on the HiPhO benchmark (across 7 recent physics Olympiads), using four state-of-the-art multimodal language models. Results indicate consistent improvements for both open and closed-source models, including reaching gold medalist-level performance previously unattainable for open-source systems."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The manuscript is notably clear, demonstrating substantial effort in both visualization and written presentation.\n\n2. The study employs rigorous evaluation metrics (gold-medal thresholds and Pass@k) across seven recent and distinct Physics Olympiad competitions, comparing open-source and closed-source models to establish consistent performance benefits. Tables 1 and 2 convincingly show that *PhysicsMinions* elevates multiple models beyond the gold-medal line and achieves a historic milestone for open-source benchmarks.\n\n3. The ablation studies systematically examine the impact of the Visual Studio, the Review Studio (including physics-specific and general verification), and the key hyperparameter—Consecutive Verification (CV). The results support the claim that each architectural component is necessary to attain the highest performance tier.\n\n4. Section 5.2 candidly acknowledges current shortcomings in precise visual data extraction, pinpointing specific tools and failure cases within the system or available toolkits (see Fig. 7). This openness establishes a solid and honest foundation for future methodological improvements.\n\n5. The design rationale—encompassing visual, logical, and review components—is well-motivated, with clearly defined and differentiated roles for diagram parsing, symbolic reasoning, and correctness verification. The iterative co-evolution process and explicit feedback loops are logically coherent and empirically well-supported."}, "weaknesses": {"value": "### The Generalizability of the Paper is Limited\n\nDespite claiming generalizability, the system is only evaluated using physics Olympiad problems derived from a single benchmark (HiPhO). No evidence is presented for transfer to related domains (such as mathematics Olympiads, chemistry Olympiads, or general science Q&A), nor are conceptual or empirical justifications provided for broader applicability beyond physics. I **would like to see evaluations targeting large-model agents on real scientific research problems** (even if sometimes limited to the physics domain, that would still be a significant breakthrough).\n\nMoreover, the experiments are heavily designed around the structure of the HiPhO Olympiad (including structured JSON extraction for diagrams that conform to this benchmark’s conventions), creating a risk of benchmark-specific optimization. The paper could be improved by incorporating cross-benchmark generalization tests or, at the very least, providing deeper error analyses on out-of-distribution tasks/problems.\n\n---\n\n### Lack of Fairness in Experimental Comparison\n\nIn fairness comparisons, the paper uses a unified evaluation benchmark and scoring standard, ensuring result comparability. However, the experiments do not disclose hardware configurations for each method during runtime, total reasoning time, overall GPU usage, or token costs—only a single example’s relative consumption ratio is provided in the hyperparameter analysis. Given that the proposed method is a multi-stage, multi-round generation-and-verification system, **its inference overhead is clearly higher than that of single models and other baselines**. It is recommended that the authors supplement the final version with resource consumption comparisons for each method to allow readers to fully assess the real-world application cost. **If the strong results are not achieved under equal token or time consumption, they are meaningless and unacceptable.** If resources can be used without limits, then comparing with a single agent or single LLM is pointless—frequent agent calls will be hard to inspire either product development or academic research. (I acknowledge that many LLM-Agent papers have this issue, but it is still something that must be carefully considered.)\n\n---\n\n### Many Large-Model Prompts May Have Been Generated by LLMs\n\nIn the appendix, I noticed numerous prompts that, when tested on GPTZero, are highly likely to have been generated by an LLM. Extremely long prompts can cause the model’s attention to collapse, and I am not certain that Markdown is the best choice for prompt formatting. I cannot definitively confirm the prompts were LLM-generated, but the design philosophy and generation process still need to be described in detail in the paper. After all, if each task requires such an extensive prompt design process, usability will be very poor."}, "questions": {"value": "1. It is necessary to clarify whether the potential research significance of your proposed method lies primarily in advancing the general development of large language models (LLMs), in contributing specifically to the \"LLM for Science\" direction, or in facilitating agent-based application development. At present, what I observe is essentially a finely crafted problem construction for a specific scenario. An adequate response should explain how your study can provide broader insights — for example, how it could inspire rapid design strategies when facing novel and challenging problems in future large models.\n\n2. Please provide quantitative details regarding the computational overhead (in terms of both time and token usage) incurred by your proposed method. In typical use cases, how many times greater is the overhead compared to the Single Model approach illustrated in Figure 1? How does the time consumption compare under these conditions?\n\n3. For *PhysicsMinions*, how is the decision made to completely reinitialize (i.e., discard and regenerate) a solution instead of continuing iterative refinement? Is this decision based on a fixed threshold, or is it determined adaptively?\n\nThe review scores may vary depending on the quality of your responses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I observed extensive and direct use of the “Minions” character design and name, which are copyrighted and trademarked by Universal Pictures/Illumination Entertainment. I hope the author will carefully consider whether this may pose unnecessary copyright risks."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wdmFZOi77A", "forum": "kipQYpoZf1", "replyto": "kipQYpoZf1", "signatures": ["ICLR.cc/2026/Conference/Submission1350/Reviewer_YFh1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1350/Reviewer_YFh1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012797423, "cdate": 1762012797423, "tmdate": 1762915744418, "mdate": 1762915744418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the so-called Physics Minions workflow, with three main components: the Visual Image studio for image description and measurements, the Logic studio for solutions, and the Review studio for verification. Experiments show that the proposed workflow improve performance of multiple LLMs on multiple physics olympaids, boosting open-source LLM performance to gold-medal-level. Authors also conducted further Pass@K and ablation experiments to validate design choices. Overall, authors claim the main improvement to be with respect to the image dealing part and the reviewing part."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The experiments are conducted on the IPhO bench, showing descent improvement brought about by the two major components (i.e. improvement from image measurement and reviewing component)."}, "weaknesses": {"value": "The most significant flaw of this paper is its lack in contribution: the two proposed components have been known facts. The experience and intuition that, accurate image measurement and self-refinement can improve physics reasoning capability of Large Language Models has been proposed and validated in previous work. For example, [1] shows methods like BoN and self-refinement works well on physics problems; [2] shows images are important for some physics olympiad problems. Even similar, [3] shows that provided with an image expert tool and a review expert tool, state-of-the-art LLMs can perform quite well on physics olympiad. Some figures in this submission also resemble those in [3]: this similarity **at least** shows that \"accurate measurement and reviewing experts can boost performance in Physics Olympiad\" has been discussed in previous work.\n\nThe authors have indeed conducted experiments to test many LLMs' performance on HiPhO bench, which is consisted of multiple physics olympiads. However, it is HiPhO bench and its evaluation methods that made this evaluation possible, not this submission.\n\nOverall, this submission lacks novelty and contribution.\n\n\nReferenced papers in this review:\n\n[1] Improving Physics Reasoning in Large Language Models Using Mixture of\nRefinement Agents, arXiv 2412.\n\n[2] SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning, NeurIPS 2025.\n\n[3] Physics Supernova: AI Agent Matches Elite Gold Medalists at IPhO 2025, arXiv 2509."}, "questions": {"value": "As discussed in your paper, the VLM+agent architecture is better for image captioning and measurement, even better than some chart analysis tools tested, except the WebPlotDigitizer. The WebPlotDigitizer involves manual calibration of points measured. Could this manual calibration done by VLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NFtbqWs8m2", "forum": "kipQYpoZf1", "replyto": "kipQYpoZf1", "signatures": ["ICLR.cc/2026/Conference/Submission1350/Reviewer_Px8e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1350/Reviewer_Px8e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1350/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763032641713, "cdate": 1763032641713, "tmdate": 1763032641713, "mdate": 1763032641713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}