{"id": "pNpnqsn0Si", "number": 10324, "cdate": 1758167138787, "mdate": 1759897658591, "content": {"title": "Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space", "abstract": "Current approaches for scaling inference-time compute in transformers rely on training them to emit explicit chain-of-thought tokens before producing an answer. While these methods are powerful, they are limited because they cannot be applied during pretraining and rely on explicit verbalization to scale inference-time compute. In this work, we propose **Thoughtbubbles**, a transformer variant that natively performs parallel adaptive computation in latent space by learning to fork or delete residual streams. Thus, tokens that require a large amount of computation can form a \"bubble\" of cloned residuals in the middle of the network for additional thinking. Crucially, this behavior is learned during pretraining with only language modeling loss. **Thoughtbubbles** outperforms both standard decoder LMs as well as non-adaptive parallel computation approaches on OpenWebText and peS2o perplexity and in zero-shot evaluations such as HellaSwag and LAMBADA after pretraining across 150M to 772M parameter scales. The implicit nature of our method enables adaptive computation to be learned starting at pretraining time, paving the way to unify train and test-time behavior for reasoning models.", "tldr": "We propose a novel architecture that enables unsupervised parallel adaptive computation by forking residual streams, trained only using LM loss.", "keywords": ["adaptive computation", "thinking tokens", "pretraining", "architecture", "transformers"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40bdc00a4812a6643c5d69fe9b18f49f8d325058.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new framework, Thoughtbubbles, for latent parallel computation. They enable the dynamic allocation strategy to introduce residual streams, but delete or folk them based on judgments. With their folking decision methods and tailored structures, Thoughtbubbles achieve better perplexity and zero-shot accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written in overall.\n- Adaptive and dynamic strategy seems a good motivation for latent thinking approaches.\n- Overall method (approach) is well aligned with the intuitions, and potential issues (like RoPE) are also explained throughout the paper."}, "weaknesses": {"value": "- First of all, 2.5B tokens are too small to get valuable insights I believe. At least 20 times larger tokens to model size would be great to support the performance improvement.\n- Are all results averaging all residual streams? Do you have any result without output averaging to see if latent thoughts have good effects themselves?\n- Since longer sequences will introduce extra computation time, it'd be good to have those kinds of results for scaling effects.\n- I also curious about the batching inference latency itself. These dynamics will hurt the performance.\n- Could you elaborate 'log-space' in L.171?\n- Regarding Eq.8 and 9, are there any ablation study results? It seems like attention outputs are affected by square of P (cumulative scores), a little bit weird to me. \n- In Appendix D (Eq.13), the last fork (p = q) will get the same position embeddings as the previous step token, right?\n- Missing reference: Mixture-of-Recursions paper seems also be related to dynamic allocation of latent thoughts (vertical manner of recursion can be seen as horizontal manner.) The mechanism of folking or deleting inside seems different though. Nevertheless, it would be good to add discussion with that kinds of literature."}, "questions": {"value": "See above weakness parts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JdyhSylE2v", "forum": "pNpnqsn0Si", "replyto": "pNpnqsn0Si", "signatures": ["ICLR.cc/2026/Conference/Submission10324/Reviewer_Ck1v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10324/Reviewer_Ck1v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661503688, "cdate": 1761661503688, "tmdate": 1762921663273, "mdate": 1762921663273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Thoughtbubbles, a transformer architecture that dynamically allocate computational resources according to the learned cumulative scores. The scores are computed for each token across layers; authors use top-k selection to keep high-scoring tokens and weigh attention by these scores. The method is applied to language model pretraining with OpenWebText and peS2o and shows improvements in perplexity and some zero-shot tasks over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Authors propose a forking mechanism that is conceptually lightweight and also compatible with modern language model pretraining\n* The paper is well motivated and well written; despite complex notations it is very readable\n* Experiments are conducted over two pretraining sets and results on both perplexity and zero-shot tasks are reported in detail"}, "weaknesses": {"value": "* One major concern I have regarding the paper is its claim on \"reasoning\". The paper repeatedly claims \"latent computation\" and \"parallel thinking in latent space\" but the proposed method to its essence is a weighted transformer with token duplication, not a latent variable model with explicitly modeled \"reasoning\" process. Recent works demonstrate how explicit reasoning can be included in the pretraining process, for example [1][2]. I recommend the authors discuss these works, and either reframe the contribution as \"adaptive attention allocation\" or provide rigorous justification for why duplicated, masked token embeddings constitute \"latent\" computation.\n\n* Regarding experiment results -- the paper claims to enable reasoning but show little improvements on reasoning tasks. For example, PIQA is focused on physical reasoning, but the proposed method performs similar to baselines; BLiMP only beats parameter-matched baselines. For math reasoning, authors mention that GSM8k is \"too noisy at this scale\", but reasoning LMs of similar scale like [2] has demonstrated reasoning capabilities using the dataset. The model showed increase in performance mostly on tasks like Lambada, which are essentially context-retrieval-based test sets, and thus can only show mechanism improves attention bandwidth, but not helping with reasoning. \n\n* The experiments also lack baselines. The primary baseline is a \"Copy-N\" model that naively duplicates inputs. Authors should also incorporate results from related work (some already discussed in the paper) like [3][4].\n\n* The paper introduces a \"partial rotation\" scheme for forked tokens. However this seems more like an ad-hoc design and there is no theoretical justification for fractional position offsets. \n\n* The forking mechanism relies on hard top-k selection to determine which token streams survive at each layer. While this enables explicit pruning, I wonder if it creates a potential gradient flow problem, such that tokens whose scores fall just below the top-k threshold receive no gradient signal about how to improve. \n\n[1] Hao, Shibo, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. \"Training large language models to reason in a continuous latent space.\" arXiv preprint arXiv:2412.06769 (2024).\n\n[2] Kong, Deqian, Minglu Zhao, Dehong Xu, Bo Pang, Shu Wang, Edouardo Honig, Zhangzhang Si et al. \"Latent Thought Models with Variational Bayes Inference-Time Computation.\" arXiv preprint arXiv:2502.01567 (2025).\n\n[3] Raposo, David, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. \"Mixture-of-depths: Dynamically allocating compute in transformer-based language models.\" arXiv preprint arXiv:2404.02258 (2024).\n\n[4] Dehghani, Mostafa, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. \"Universal transformers.\" arXiv preprint arXiv:1807.03819 (2018)."}, "questions": {"value": "* Can the authors report results on GSM8K? I am curious about how it scales as the model size increases\n* Figure 5 claims the model allocates more computation to high-entropy tokens as evidence for intelligent computation allocation. I wonder if you remove forks from high-entropy tokens and add them to low-entropy tokens, does performance drop more than the reverse? \n* See additional questions in the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fh7xOBCcp4", "forum": "pNpnqsn0Si", "replyto": "pNpnqsn0Si", "signatures": ["ICLR.cc/2026/Conference/Submission10324/Reviewer_7Vnw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10324/Reviewer_7Vnw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762118158892, "cdate": 1762118158892, "tmdate": 1762921662847, "mdate": 1762921662847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Thoughtbubbles, a decoder-only transformer that adaptively allocates parallel compute by forking or deleting residual streams per token. A learned cumulative score governs when to fork or prune; low-score streams are attenuated (masked from attention and updates), while high-score streams are duplicated. After the stack, per-token distributions from all surviving streams are score-weighted averaged to produce the final logits. The system is trained only with standard LM cross-entropy (no extra supervision) and shows lower perplexity than (i) a parameter-matched GPT-2-style baseline and (ii) a non-adaptive, “Copy-N” computation-matched baseline on OpenWebText and peS2o; it also reports gains on LAMBADA and HellaSwag."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Architecture: The core idea of an unsupervised, adaptive, and parallel compute mechanism is conceptually clean, novel, and a valuable research direction.\n\n2. Perplexity Gains: The method demonstrates clear and consistent empirical wins on perplexity across all tested scales (150M–772M) and corpora when compared to the provided baselines (Table 1, Fig. 3).\n\n3. Insightful Analysis: The analysis in Figure 5, showing that the model allocates more compute to higher-entropy regions, provides compelling evidence that the adaptive mechanism is learning non-trivial, interpretable behavior."}, "weaknesses": {"value": "1) Reasoning Claims Are Not Supported by Appropriate Evaluations\nThe paper's most significant weakness is the disconnect between its framing and its results. The work is motivated as an advance toward \"thinking\" beyond Chain-of-Thought (CoT), yet it is not evaluated on any of the standard multi-step reasoning benchmarks (e.g., GSM8K, MATH, Big-Bench Hard) where such capabilities are measured. The authors acknowledge this as a limitation, but its omission is critical. Without this data, the central claim—that the \"bubbles\" constitute meaningful \"thinking\"—is unsubstantiated. \n\n2) Inadequate Computational Baselines and Cost Analysis\nThe paper's claims of outperforming \"computation-matched\" baselines are not robust, for three key reasons:\n\n(1) Weak FLOPs-Matched Baseline: The \"Copy-N\" baseline is a poor and non-competitive comparator. It measures the effect of applying a small model to more data, not the performance of a different model with an equivalent compute budget. A far more competitive and necessary baseline is a standard, non-adaptive GPT-2-style model that is scaled up (e.g., made wider or deeper) to have the same training and/or inference FLOPs as the \"Ours (k=4L)\" model. It is very plausible that a standard, larger transformer would outperform this complex, adaptive one.\n\n(2) Missing Wall-Clock Comparison: The paper omits a practical, wall-clock time comparison. The authors admit to \"low raw wall-clock efficiency\" due to the overhead of forking, gating, and averaging. A fair comparison requires measuring throughput/latency against all baselines (including a standard larger-model baseline) on a time-matched budget.\n\n(3) Missing Adaptive Baselines: The paper fails to compare against other, more established adaptive compute methods, such as layer-skipping, early-exits, or mixture-of-depths. Without these, it's impossible to know if this parallel-forking approach is superior to adaptive-depth.\n\n3) Key Ablations Are Missing\nThe paper does not provide sufficient ablation studies to isolate which components of the method are responsible for the gains. For example, what is the impact of the attenuation mechanism (masking updates/attention by score) versus the forking alone? How sensitive is the model to the placement of forking layers? What is the effect of the output-averaging design versus a simpler alternative (e.g., a learned router)?"}, "questions": {"value": "In zero-shot evaluations, do you use generation results or likelihood? Why there is a gap between PPL performance and zero-shot performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UFrF4YgaOy", "forum": "pNpnqsn0Si", "replyto": "pNpnqsn0Si", "signatures": ["ICLR.cc/2026/Conference/Submission10324/Reviewer_nkbZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10324/Reviewer_nkbZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762473079890, "cdate": 1762473079890, "tmdate": 1762921662395, "mdate": 1762921662395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a decoder-only Transformer called Thoughtbubbles, which forks residule streams at selected layers and prunes them under certain budget. Final logits in the architecture are then a score-weighted average over surviving streams. Experiments are conducted on OpenWebText and peS2o, and results indicate better performance (lower perplexity and better zero-shot metrics) than parameter-matched baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The \"thinking\" process is conducted in an unsupervised and adaptive way, which is also conceptually easy to understand\n* Modification on the attention mechanism is easily adaptable to modern LLM frameworks; figures are clear and informative\n* Empirical results indicate improvement over baselines on certain benchmarks"}, "weaknesses": {"value": "* Reasoning tasks like GSM8k are missing, and thus it is unclear how the proposed method helps with \"thinking\" in practice. While authors claim it is due to the small model size, this may raise concerns on whether the proposed architecture, if applied to continual training, can maintain its capabilities\n* In addition, results on physical reasoning like PIQA are mixed, this leads to further concerns on whether the architecture indeed enabled reasoning in LLMs\n* The paper requires additional ablation results. In the paper authors only provide results while forking at 3,7,11 layers -- what about forking at different layers? Also what if different score initialization schemes are applied? \n* Do different forks specialize in different aspects (syntax vs semantics, different possible continuations)? Figure 4 shows forks attend to their parent, but no analysis explores whether forks compute complementary versus redundant information. \n* The hard top-k selection scheme may create scenarios where tokens with high cumulative scores early get dropped later, receiving little or no gradient. Thus a deeper or more frequent forking will saturate gains at a fixed block budget. I wonder if the authors have tried other stochastic selection methods, and would other methods enable a more stable and deeper forking?"}, "questions": {"value": "See above in weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r7nGbyjC5F", "forum": "pNpnqsn0Si", "replyto": "pNpnqsn0Si", "signatures": ["ICLR.cc/2026/Conference/Submission10324/Reviewer_Anxs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10324/Reviewer_Anxs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762588276703, "cdate": 1762588276703, "tmdate": 1762921662020, "mdate": 1762921662020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}