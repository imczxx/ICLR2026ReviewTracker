{"id": "FDdOD3qwS7", "number": 22288, "cdate": 1758329087767, "mdate": 1759896874761, "content": {"title": "Beyond Uniformity: Sample and Frequency Meta Weighting for Post-Training Quantization of Diffusion Models", "abstract": "Post-training quantization (PTQ) is an attractive approach for compressing diffusion models to speed up the sampling process and reduce the memory footprint. Most existing PTQ methods uniformly sample data from various time steps in the denoising process to construct a calibration set for quantization and consider calibration samples equally important during quantization process. However, treating all calibration samples equally may not be optimal. One notable property in the denoising process of diffusion models is low-frequency features are primarily recovered in early stages, while high-frequency features are recovered in later stages of the denoising process. However, none of previous works on quantization for diffusion models consider this property to enhance the effectiveness of quantized models. In this paper, we propose a novel meta-learning approach for PTQ of diffusion models that jointly optimizes the contributions of calibration samples and the weighting of frequency components at each time step for quantizing noise estimation networks. Specifically, our approach automatically learns to assign optimal weights to calibration samples while selectively focusing on mimicking specific frequency components of data generated by the full-precision noise estimation network at each denoising time step. Extensive experiments on CIFAR-10, LSUN-Bedrooms, FFHQ, and ImageNet datasets demonstrate that our approach consistently outperforms state-of-the-art PTQ methods for diffusion models.", "tldr": "", "keywords": ["Post-training Quantization", "LLMs"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00c019b7d38c062123864056c8e9afc833ade2de.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper revisits the common practice of using uniform sampling in diffusion model training and identifies its limitations, especially in terms of efficiency and learning dynamics. The authors propose a non-uniform sampling strategy that focuses training more on informative or harder timesteps instead of treating all noise levels equally. They introduce a principled sampling distribution that improves convergence, sample quality, and training efficiency without modifying the model architecture. Experiments across various benchmarks demonstrate consistent performance gains over uniform sampling. Overall, the paper aims to optimize how diffusion models learn, rather than modifying their network or objective."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n\n1: Insightful Problem Identification: Highlights a commonly overlooked assumption in diffusion model training, uniform timestep sampling, and provides evidence of its suboptimality.\n\n2: General and Model-Agnostic: The proposed sampling strategy applies to multiple diffusion architectures (DDPM, latent diffusion, etc.) with no changes to model design.\n\n3:Strong Empirical Results: Demonstrates improvements in FID scores and convergence speed across datasets, validating practical impact."}, "weaknesses": {"value": "Weaknesses\n\n1: Additional Complexity in Training Setup: Non-uniform sampling introduces another hyperparameter and requires computing or estimating timestep importance, which may increase implementation difficulty.\n\n2: Limited Theoretical Guarantees: While the method is empirically effective, formal justification for optimality of the sampling distribution is not fully established.\n\n3: Focus on Image Generation Only: The experiments are mostly image-based; it remains unclear whether the approach extends well to text-to-image, video diffusion, or molecular diffusion models."}, "questions": {"value": "1: Generalization: How well does the proposed non-uniform sampling strategy extend to other modalities, such as text-to-image diffusion, video generation, or 3D diffusion models?\n\n2: Optimality of Sampling Distribution: Is there a theoretical way to derive the optimal sampling distribution instead of heuristically estimating timestep importance?\n\n3: Interplay with Noise Schedules: How does this sampling approach interact with different noise schedules (cosine, linear, VP/VE SDEs)? Could both be jointly optimized for further gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jaz0UjvnTA", "forum": "FDdOD3qwS7", "replyto": "FDdOD3qwS7", "signatures": ["ICLR.cc/2026/Conference/Submission22288/Reviewer_RZMH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22288/Reviewer_RZMH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761532822688, "cdate": 1761532822688, "tmdate": 1762942151346, "mdate": 1762942151346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a meta-learning method based on sampling and frequency weighting based on the existing post-training quantization (PTQ). They propose that the diffusion model recovers different frequency features at different timesteps, so the model can be made to focus on specific frequency components of the generated data at different timesteps by introducing the Discrete Wavelet Transform (DWT) and frequency component weights. The paper analyzes the timesteps of the diffusion model from the perspective of frequency, and this idea is innovative and worthy of further research and experimentation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This article innovatively puts forward the idea of optimizing the post-training quantization process of diffusion models based on frequency characteristics, and details the design of the loss function and optimization process for different training phases, so as to provide suitable weights for sampling at different timesteps. The method has good originality and is worthy of further research and analysis."}, "weaknesses": {"value": "1. Redundant description: A comprehensive set of parameters and optimization procedures are presented in the article, which is valuable for explaining the methodology theoretically. However, the detailed description of the parameter design and optimization steps in the Introduction seems a bit lengthy—content that might be more appropriately placed in the Experimental section, which may affect the clarity and impact of the paper. The focus could have been on the key decisions and results of the optimization, such as the rationale behind the chosen hyperparameters and the impact of these choices on the results. In addition, any trade-offs made during parameter tuning (e.g., computational cost vs. model performance) could be explicitly discussed and how these choices were justified.\n\n2. Insufficient experiments: the experimental part of the article is slightly insufficient as the only relatively new quantization method is TFMQ-DM, and more recent quantization methods (e.g., SVDQuant[1] or APQ-DM[2], etc.) could have been added. ** This time, I will strongly request that comparison experiments with SVDQuant and APQ-DM be included during the rebuttal period! ** You mentioned at NeurIPS Rebuttal that these would be added in the revised version, but I haven’t seen the supplementary experiments in the ICLR submission yet! Resubmitting without making revisions is a bad practice.\n\nIn addition, the authors did not list the detailed memory usage and inference speed of the quantization model compared with the full-precision model, and only compared the computational cost and hardware efficiency with TFMQ-DM, which can be considered to add more data in this area to improve the rigor.\n\n[1] Li, Muyang, et al. \"Svdqunat: Absorbing outliers by low-rank components for 4-bit diffusion models.\" arXiv preprint arXiv:2411.05007 (2024).\n\n[2] Wang, Changyuan, et al. \"Towards accurate post-training quantization for diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "questions": {"value": "Further questions:\n\n1. I am curious about (a) and (b) in Fig. 3 of the experimental part. The trends of the three high-frequency components of the frequency weights in (a) are very similar; do these three components have their own independent significance? Can they be combined? It is noted in the text that the later timesteps in (b) have more dispersed sampling weights, but the text does not further explain this, is it possible to further explore the pattern?\n\n2. The experimental part of the article only uses the bitwidths 4/32, 8/8 and 4/8, what is the purpose of choosing them? Are they representative enough for the quantization under different bitwidths?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "f1FAbp1c7C", "forum": "FDdOD3qwS7", "replyto": "FDdOD3qwS7", "signatures": ["ICLR.cc/2026/Conference/Submission22288/Reviewer_ruiM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22288/Reviewer_ruiM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997973078, "cdate": 1761997973078, "tmdate": 1762942151042, "mdate": 1762942151042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "With identifying the property that low-frequency features are primarily recovered in the early stages in the denoising process of diffusion models, this paper proposed a meta-learning method for PTQ in diffusion models, which jointly optimizes the contributions of calibration samples and the weighting of frequency components at each time step. Experimental results reveal the effectiveness of the proposed PTQ method for diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method well-solved the motivations claimed by authors. \n2. The formulation of equations are clear."}, "weaknesses": {"value": "1. For measuring the impact of frequency on model performance, authors propose a learnable frequency weight. I'm curious about simply apply a set hyper-parameter frequency weight, for example a weight ranges from [0, 1]. Will this simple method also achieves performance improvement? Since the learnable weights may involve quantization time cost, authors should compare their methods with such a naive weights to find a better trade-off between performance and quantization efficiency. \n2. It would be better to plot some figures (which are sampled from the calibration dataset) for better explaining the frequency motivation in the section of introduction."}, "questions": {"value": "See weaknesses. I would raise my score if my concerns are solved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "izI4XT7OaV", "forum": "FDdOD3qwS7", "replyto": "FDdOD3qwS7", "signatures": ["ICLR.cc/2026/Conference/Submission22288/Reviewer_gXhQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22288/Reviewer_gXhQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762399185152, "cdate": 1762399185152, "tmdate": 1762942150648, "mdate": 1762942150648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}