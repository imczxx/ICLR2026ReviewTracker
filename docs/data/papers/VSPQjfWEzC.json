{"id": "VSPQjfWEzC", "number": 17259, "cdate": 1758273953717, "mdate": 1759897187540, "content": {"title": "EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning", "abstract": "Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforcement learning with verifiable rewards and effective distillation that transfers competence across math, coding, and agentic tasks. Yet constructing generalizable synthetic verifiable data remains difficult due to hallucination-prone generation, and weak or trivial verification artifacts that fail to separate strong from weak solutions. Existing approaches often rely on task-specific heuristics or post-hoc filters that do not transfer across domains and lack a principled, universal evaluator of verifiability. In this work, we introduce an evolutionary, task-agnostic, strategy-guided, executably-checkable data synthesis framework that, from minimal seed supervision, jointly synthesizes problems, diverse candidate solutions, and verification artifacts, and iteratively discovers strategies via a consistency-based evaluator that enforces agreement between human-annotated and strategy-induced checks. This pipeline upgrades filtering into principled synthesis: it reliably assembles coherent, verifiable training instances and generalizes without domain-specific rules. Our experiments demonstrate the effectiveness of the proposed approach under both RLVR and model distillation training paradigms. The results show that training with our synthesized data yields significant improvements on both the LiveCodeBench and AgentBench-OS tasks, highlighting the robust generalization of our framework.", "tldr": "We present a task-agnostic framework that evolves strategies to synthesize reliable, verifiable data, boosting RLVR and distillation with strong gains.", "keywords": ["Verifiable data synthesis", "Evolutionary strategy", "RLVR", "Model distillation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f7c444989875d579e76f179587238b107b9270ac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents EvoSyn, an evolutionary data synthesis framework for constructing verifiable training datasets in tasks where correctness can be determined by executable tests. The method starts from a simple heuristic and then evolves data-filtering strategies, guided by a consistency-based evaluator that checks reliability on a small human-verified seed set. EvoSyn is evaluated on two executably-checkable domains — LiveCodeBench (for RL with verifiable rewards, RLVR) and AgentBench-OS (for model distillation). In both settings, EvoSyn-filtered data improve downstream model performance, leading to stronger reward learning and enabling smaller models to surpass their teachers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Treating reliable synthetic instance selection as a search over filtering strategies rather than fixed heuristics offers a clean, general abstraction applicable across verification-based learning setups.\n- The two consistency-based criteria (ensuring solvability and discriminative tests) address the main causes of unreliable verifiable data, and the Zero-Variance Pruning step provides an efficient quality control mechanism.\n- Applying the same pipeline to RLVR and distillation demonstrates strong generality.\n- The paper clearly describes the limitations of handcrafted, task-specific test-synthesis heuristics and positions EvoSyn as a more automated alternative, though execution-cost limitations still constrain scale."}, "weaknesses": {"value": "- Data scale is modest (231 RLVR; 673 distillation) from small seeds (51/129), in part due to the $O(MN)$ execution cost. The authors do not report variance across multiple evolutionary runs, so generality/reproducibility is hard to judge.\n\n- Baselines are mostly intra-method (random/relaxed). Adding strong hand-designed verification baselines would clarify the benefit of evolution.\n\n- The method selects for solvability and discriminativeness but does not report problem-level diversity/coverage/difficulty metrics. Quality is only inferred via downstream gains (and test-count statistics).\n\n- Positioning vs prior evolutionary program/data-search work could be sharpened."}, "questions": {"value": "- The authors report 231 (RLVR) and 673 (distillation) retained instances. Are these from a single evolutionary run/seed, or averaged over multiple runs?\n\n- Given the $O(MN)$ execution cost, can the authors quantify the wall-clock/compute cost for the reported configuration and describe how much parallelism was used in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Iendkm0ltp", "forum": "VSPQjfWEzC", "replyto": "VSPQjfWEzC", "signatures": ["ICLR.cc/2026/Conference/Submission17259/Reviewer_gDs6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17259/Reviewer_gDs6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965932343, "cdate": 1761965932343, "tmdate": 1762927209288, "mdate": 1762927209288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes EvoSyn, a task-agnostic evolutionary data synthesis framework designed to generate verifiable synthetic data for LLM training. Verifiable data (i.e., data with executable correctness checks) is crucial for reinforcement learning with verifiable rewards (RLVR) and distillation, yet remains expensive to curate manually and hard to generalize across domains.\n\nEvoSyn tackles this by evolving data filtering strategies that identify reliable problems, solutions, and verification artifacts (tests) through an evolutionary optimization process guided by consistency with a small human-verified seed dataset. The method iteratively refines filtering strategies based on two strict criteria ensuring alignment between human-annotated and model-inferred correctness.\n\nOnce a high-quality filtering strategy is obtained, EvoSyn synthesizes new tasks, candidate solutions, and tests, filters them using the evolved strategy, and trains models using this curated data. Experiments on LiveCodeBench (coding tasks, RLVR setting) and AgentBench-OS (agentic reasoning tasks, model distillation setting) demonstrate gains. EvoSyn-filtered data substantially improves model performance, enabling smaller distilled student models to outperform teacher models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The approach of synthesizing verifiable data is domain-agnostic, contrasting prior heuristic or task-specific filtering methods. Its evolutionary optimization of filtering strategies is broadly applicable.\n\n- The framework is evaluated on two different benchmarks (LiveCodeBench and AgentBench-OS) under both RLVR and distillation paradigms, showing performance gains.\n\n- The paper decomposes the pipeline (strategy evolution, synthesis, filtering, training), provides detailed ablations (e.g., effect of M, N, criteria sufficiency, pruning), and articulates trade-offs between data reliability, diversity, and computational cost.\n\n- The paper includes prompts, strategy variants, and explicit evaluation criteria in the appendix, thus focusing on reproducibility."}, "weaknesses": {"value": "- The paper is dense but not well-written and well-structured. For instance, throughout the introduction, the authors repeatedly emphasize developing a general framework for synthesizing verifiable data, yet the exact task formulation and problem statement remain vague. The objective is presented at a very high level without clearly defining the input-output structure of the task. Only by examining the experimental setup and the prompts in the appendix does it become apparent that the core task is test-case generation from NL problem descriptions. These descriptions, similar to those in competitive programming problems, may contain a few example test cases while the exhaustive test suite remains hidden. The framework also asks the LLM to generate several candidate solutions. Subsequently, EvoSyn performs cross-execution of the generated solutions and tests, for example, using TF-IDF-like, coverage-based, inverse filtering, or exclusion-based scoring approaches.\nHowever, since both the candidate solutions and test cases are generated by LLMs, they may both be unreliable or semantically inconsistent with the original NL description. How do the authors ensure that the generated test cases are meaningful and semantically faithful to the input description, rather than reflecting coincidental or spurious correlations?\n\n- The paper introduces a TF-IDF-like scoring mechanism where solutions that pass \"difficult\" tests receive higher scores, with difficulty defined as tests that are passed by only a few solutions. However, the underlying task—NL description to test-case generation, makes this assumption problematic. If a test case is passed by only a few solutions, it does not necessarily indicate that it is difficult; rather, it could simply be faulty or semantically misaligned with the problem description. In the absence of ground-truth verification or semantic alignment checks, there is no clear justification for treating such cases as valuable or discriminative. This undermines the reliability of the evolved scoring strategies and calls into question whether the \"difficulty\" metric genuinely correlates with test-case quality.\n\n- \"For example, RLVR-style training methods...\": please define any abbreviation before using it for the first time\n\n- There are typos in the paper e.g., \"synthsizing\" in line 146\n\n- For the prompt provided in Figure 6, what are the \"Problem 1\", \"Problem 2\", \"Problem 3\" being referred to? These are not defined anywhere in the prompt provided in Figure 6.\n\n- Instead of providing code-snippets in the paper and appendix, I would suggest providing algorithms that are typically more reader-friendly.\n\n- The metrics in the experimental section are not clearly defined. How is \"accuracy\" in Table 3 computed?\n\n- The evaluation covers only two benchmarks and model families. Given that the paper mentions that it focuses on developing a \"general framework\" for synthesizing verifiable data, broader tests on other verifiable domains (math reasoning, data-to-text, scientific QA) would better demonstrate true generality.\n\n- EvoSyn relies on a small set of human-verified seed data to guide consistency-based evaluation. The paper does not deeply explore how biases or poor coverage in this seed data affect the evolved strategies."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DvDSfAjyO9", "forum": "VSPQjfWEzC", "replyto": "VSPQjfWEzC", "signatures": ["ICLR.cc/2026/Conference/Submission17259/Reviewer_9iJM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17259/Reviewer_9iJM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762103071691, "cdate": 1762103071691, "tmdate": 1762927208698, "mdate": 1762927208698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes a method to generate verifiable synthetic data along with an automatic evolution based filtering technique to select useful datapoints from it. The evolution based filtering technique works by initializing a simple strategy and evolving it while trying to 'fit' to a small human-annotated dataset of synthetic datapoints. The overall approach works in 3 stages: (a) evolve a strategy iteratively (b) ask a strong model to generate problems, solutions and tests/oracles and filter them using the evolved strategy (c) train the model on this data. The paper evaluates this approach on LiveCodeBench using RLVR and on AgentBench-OS using distillation, and shows that the filtering helps train better models compared to randomly sampling synthetic data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes an approach to filter synthetic data without relying on task-specific heuristics, and evaluates the approach on two common approaches of post-training (RLVR and model distillation), and on two benchmarks providing some evidence of the generality of this approach."}, "weaknesses": {"value": "**Baseline**: The paper does not compare the approach with real baselines. The baselines used in the paper are simple/artificial. What would be great is if the paper can compare against other filtering approaches (heuristics or other automatic approaches) so as to compare the efficacy of this filtering approach over other filtering/synthetic-data-generation approaches. Being better than random baseline is not very meaningful as it is expected that randomly generated data without any kind of filtering will be very noisy, what would be a meaningful claim is if you can show that you get similar benefits as SOTA task-specific-heuristics without having to actually manually define them yourself.\n\nRelatedly, the \"related works\" section is also very sparse and the paper could benefit from broadening it significantly and actually comparing it with the approach in this paper.\n\n**Method clarity**: The other major concern I have is that the methodology is not entirely clearly described. E.g., how exactly is the strategy evolution happening? Perhaps you could describe the algorithm in more details/using pseudocode. An example of something that's not clear from the writing: how can a strategy that outputs a *ranked* list of (solutions, tests) for every problem be used for *filtering* the data (filtering is a boolean function) -- are you filtering based on a cutoff? How are you picking problems that go in the training set based on this ranked list? Just formally writing down the process would clarify the method significantly, and would greatly improve the paper."}, "questions": {"value": "Apart from the above major concerns I raised, I have a question about the criteria -- doesn't satisfying criterion 2 imply that criterion 1 is automatically satisfied?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D5doVekP0N", "forum": "VSPQjfWEzC", "replyto": "VSPQjfWEzC", "signatures": ["ICLR.cc/2026/Conference/Submission17259/Reviewer_vbBU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17259/Reviewer_vbBU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762891831780, "cdate": 1762891831780, "tmdate": 1762927208178, "mdate": 1762927208178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}