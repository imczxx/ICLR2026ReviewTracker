{"id": "inXScPz1gT", "number": 20760, "cdate": 1758309836053, "mdate": 1759896959993, "content": {"title": "Consistent Diffusion Language Models", "abstract": "Diffusion-based language models (DLMs) have emerged as compelling alternatives to sequential autoregressive generation, offering the promise of parallel decoding. Yet existing discrete diffusion models require hundreds of refinement steps for high-quality text, undermining the efficiency gains of parallelism. We introduce the Consistent Diffusion Language Model (CDLM), a new family of generative models that brings the benefits of consistency training---enforcing agreement across noise levels to enable one- or few-step generation---to the discrete domain. Our approach leverages an exact closed-form formulation of discrete posteriors, providing a rigorous analogue to the missing probability-flow ODE in discrete space. This yields a multi-path consistency objective that, as we show, unifies and generalizes popular diffusion, consistency, and distillation methods in a single view. To ensure stability at scale, we introduce a set of principled design choices that prevent training pathologies like mode collapse. On conditional and unconditional text-generation benchmarks, CDLM establishes new state of the art as a single-stage model, consistently outperforming both base and distilled DLMs across sampling budgets. These results position CDLM as a new paradigm for efficient, scalable, and high-fidelity discrete generative modeling. We will be updating the code base under https://anonymous.4open.science/r/dlm-135B", "tldr": "", "keywords": ["Diffusion Language Models", "Discrete Diffusion Models", "Consistency Models", "Generative Modeling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/34c09018fcc5005febd8f28856923edeaa82457f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel family of generative models for fast inference in diffusion LLMs, known as consistency diffusion language model (CDLM), through enforcing multi-path consistency of the denoiser model. The model is trained by the combination of a standard cross-entropy loss and a consistency loss for self-distillation, and during inference, it generates in the style of continuous consistency models. The authors discussed the design space of CDLM and evaluated it on various language modeling tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles an important issue of diffusion LLMs, which is the slow inference speed caused by many denoising steps. The proposed CDLM is an interesting approach to address this from the modeling perspective, and from the reported results, it achieves SOTA performance compared with other model structures, such as the vanilla MDLM, SDTT and DUO. Moreover, the loss does not require distillation from a teacher model (i.e., can be trained from scratch), which is a nice property. The authors also provide conceptual comparisons with related methods such as the vanilla masked diffusion model, consistency models, and distillation-based methods. The experimental results are comprehensive, covering various ablation studies."}, "weaknesses": {"value": "The main weakness of the paper is the confusing writing and presentation. Also, I don't fully understand the motivation and feel that the learning objective is not well defined. I won't lean towards acceptance unless these issues are sufficiently addressed in the rebuttal.\n\n1. The notations in the paper are mainly **token level** instead of **sequence level**. For instance, in section 2, $\\mathbf{x} _ 0\\in\\mathcal{X}$ is defined as a *sequence* of one-hot tokens (so I thought $\\mathcal{X}\\subset(\\\\{0,1\\\\}^V)^{|\\mathbf{x}|}$), but the transition matrix $\\mathbf{Q} _ t$ has dimensions of $V\\times V$, so I realize $\\mathbf{x} _ 0$ still represents a single token and $\\mathcal{X}\\subset\\\\{0,1\\\\}^{V}$. Everything after that (except in algorithm 2) seems to be all operating on token level. However, we all know that the actual language modeling task is on sequence level. While I understand this simplifies the presentation to some extent, it loses many important implementation details for dealing with sequences, and made me very confused when reading the paper. I hope the authors can clarify this point. I also suggest removing the one-hot representation and using token indices instead, as otherwise the matrix multiplications with $\\mathbf{Q} _ t$ would be hard to follow.\n\n2. The learning objective is also very unclear to me due to the token-level notation. First, in definitions 2 and 3, what's the range of $f$ when we input a partially masked sequence $\\mathbf{x} _ t$ and a time $t$? Is it $(\\Delta^V)^{|\\mathbf{x} _ t|}$ or something else? If it is $(\\Delta^V)^{|\\mathbf{x} _ t|}$, (i) is there any mechanism to model the correlation between different masked positions, as we are doing few-step generation that would involve unmasking multiple positions simultaneously, (ii) for non-mask dimensions in $\\mathbf{x} _ t$, will the output at those dimensions matter (because in the cross-entropy loss for training masked diffusion models, only the logit output at masked positions are used to compute the loss), and (iii) what's the explicit form of the loss $\\mathbb{D}(f _ \\theta(\\mathbf{x} _ t,t)||f _ {\\bar\\theta}(\\mathbf{x} _ s,s))$ in equation (9), considering the dimensions that are masked in $\\mathbf{x} _ t$ but unmasked in $\\mathbf{x} _ s$? I think clarifying these points will help readers better understand the learning objective. Also, it would be helpful if the authors could provide the code in the link to the anonymous repository so that we can better understand the implementation details, which is currently empty.\n\n3. I feel equation (8) is not well-defined. The optimal predictor $f ^ * $ is defined to satisfy the equality $f ^ * (\\mathbf{x} _ t,t)=\\mathbb{E} _ {q(\\mathbf{x} _ s|\\mathbf{x} _ t,\\mathbf{x} _ 0)}[f ^ * (\\mathbf{x} _ s,s)]$. The left-hand side does not depend on $\\mathbf{x} _ 0$, while the right-hand side does. There are infinitely many clean data sequences $\\mathbf{x} _ 0$ that can be masked into $\\mathbf{x} _ t$, so how can the equality hold for all possible $\\mathbf{x} _ 0$? Judging from lemma 2, it seems that we need to take an expectation over $\\mathbf{x} _ 0$ on the right-hand side as well, i.e., assume that I can swap the order of $\\mathbb{E}$ and $\\mathbb{D}$, we want to match $f ^ * (\\mathbf{x} _ t,t)$ with $\\mathbb{E} _ {q(\\mathbf{x} _ 0|\\mathbf{x} _ t)}[\\mathbb{E} _ {q(\\mathbf{x} _ s|\\mathbf{x} _ t,\\mathbf{x} _ 0)}[f ^ * (\\mathbf{x} _ s,s)]]=\\mathbb{E} _ {q(\\mathbf{x} _ s|\\mathbf{x} _ t)}[f ^ * (\\mathbf{x} _ s,s)]$ as the dependency on $\\mathbf{x} _ 0$ is marginalized out. I think this is a more reasonable definition of the optimal predictor, but whether this swap is applicable is unknown, so I am not sure if this is what the authors intended. Finally, does the optimal predictor have an analytical expression if we take $\\mathbb{D}$ to be some simple divergence such as KL or $L^2$? Of course, if the swap is valid, i.e., $f ^ * (\\mathbf{x} _ t,t)=\\mathbb{E} _ {q(\\mathbf{x} _ s|\\mathbf{x} _ t)}[f ^ * (\\mathbf{x} _ s,s)]$, we know it is the posterior distribution $\\\\{q(\\mathbf{x} _ 0|\\mathbf{x} _ t):\\forall\\mathbf{x} _ 0\\\\}$."}, "questions": {"value": "My main questions are detailed in the weaknesses above. In addition, I also have a few more minor comments:\n\n1. The presentation of figure 2 can be improved to be more clear and have better visual effects.\n\n2. I don't think it's necessary to present the transition matrix $\\mathbf{Q} _ t$. For masked diffusion models, the forward and backward probabilities as well as the posterior bridge operator can be defined directly.\n\n3. From equation (10), is it possible to finetune (distill) a pretrained masked diffusion model into a CDLM? This may be an interesting experiment to try.\n\n4. Assume $\\mathbb{D}$ is symmetric, is there any intuitive reason for preferring $\\mathbb{D}(f _ \\theta(\\mathbf{x} _ t,t)||f _ {\\bar\\theta}(\\mathbf{x} _ s,s))$ over $\\mathbb{D}(f _ {\\bar\\theta}(\\mathbf{x} _ t,t)||f _ \\theta(\\mathbf{x} _ s,s))$?\n\n5. When sampling from CDLM, the procedure described in section 3.1 can't correct the generated tokens at previous steps, as once a token is generated it will always be fixed under the posterior bridge operator. This is a little bit different from the continuous consistency model where the predicted $\\mathbf{x} _ 0$ will be renoised so that all dimensions are updated. Do you think including a correction mechanism would help improve the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gLVow36RJM", "forum": "inXScPz1gT", "replyto": "inXScPz1gT", "signatures": ["ICLR.cc/2026/Conference/Submission20760/Reviewer_Cqsk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20760/Reviewer_Cqsk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683094169, "cdate": 1761683094169, "tmdate": 1762934188306, "mdate": 1762934188306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Consistent Diffusion Language Model (CDLM), a new type of diffusion language model resembling Consistency Models in the continuous domain. At its core is the observation that the ground-truth posterior between two random time steps $s$ and $t$ is analytically tractable, and thus we can leverage this to perform consistency training, with accelerated model inference as a natural outcome. The authors demonstrated that, when combined with the CDLM training loss, MDLM models yield better generation quality than standard models like MDLM and DUO, and that CDLM is also amenable to SDTT distillation. The authors observed that CDLM+SDTT further improves generation quality and that CDLM+SDTT outperforms MDLM+SDTT and DUO+DCD."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The high-level idea is intuitive and straightforward, making the paper easy to follow. \n* The storyline of the paper is good.\n* The authors conducted a comprehensive analysis, both theoretically and empirically. \n* It is apparent that the authors put forward many engineering tricks that are crucial to the success of CDLM. I think these tricks are very valuable to the community."}, "weaknesses": {"value": "* The mathematical notation system is not rigorous enough for a top-tier machine learning paper. In Section 2, $\\mathbf{x}$ is explicitly stated to represent a token sequence. Suppose the token sequence has $L$ tokens, then $\\mathbf{x}$ should have the shape of $L \\times |V|$, and the $\\mathbf{Q}$ matrices should have the shape of $|V| \\times |V|$. If this is correct, then in equation 6, the term $\\mathbf{Q}\\_{s+1:t}^T\\mathbf{x}_t$ does not make sense. I believe there is either a typo or some fundamental issues, and I strongly advise the authors to do a double-check.\n* One of the core results of the paper is that CDLM training is better than MDLM alone, but there is a fundamental problem. Because the training loss contains another forward pass on the $\\mathbf{x}_s$ sequence as well, when the training steps are the same, the training FLOPs double. Hence, I do not believe the authors have conducted a fair comparison in their paper. I suggest the authors present results with comparable FLOPs to make the comparison fair.\n* It is unclear how well the model is trained since CDLM is not an inference-time only method. Crucial results, such as validation perplexity and zero-shot perplexity across different datasets, are missing.\n* Both SDTT and DUO presented results with cascaded distillation, i.e., several rounds of distillation, but in this paper, the authors only conducted experiments using one round of distillation, making the results less convincing.\n* The writing of the paper seems overly verbose and not logical. Definitions 2 and 3 are neither used in the algorithm nor in any of the subsequent theoretical analyses. Section 3.3 also seems overly verbose for the method part. A more succinct and mathematical presentation should be expected."}, "questions": {"value": "* As mentioned in the weakness part, how do the training dynamics evolve for CDLM and MDLM when the same computational budget is used?\n* What are the validation perplexity numbers and zero-shot perplexity numbers of CDLM and MDLM on different datasets?\n* How does the generation quality evolve for CDLM+SDTT, MDLM+SDTT, and DUO+DCD when multiple rounds of distillation is conducted?\n* Can the authors show any evidence that CDLM is better than existing methods in any of the data modalities other than natural language?\n* How does CDLM perform on downstream tasks?\n* How does CDLM perform against predictor-corrector samplers such as ReMDM [1] in the domain of fast generation? \n\n---\n**References**\n1. Wang, Guanghan, Yair Schiff, Subham Sekhar Sahoo, and Volodymyr Kuleshov. \"Remasking discrete diffusion models with inference-time scaling.\" arXiv preprint arXiv:2503.00307 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "97997zVRUZ", "forum": "inXScPz1gT", "replyto": "inXScPz1gT", "signatures": ["ICLR.cc/2026/Conference/Submission20760/Reviewer_bf5n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20760/Reviewer_bf5n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788337900, "cdate": 1761788337900, "tmdate": 1762934186791, "mdate": 1762934186791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CDLM, a consistency training algorithm for training a few-step masked diffusion model from scratch. CDLM considers consistency distillation-type objectives without leveraging concepts like PFODE and combines the vanilla MDM training loss to enable training a few-step discrete generative model from scratch. Experiments on GPT-2-level text generation tasks show promising performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper considers an important problem of training a few-step discrete generative problem, and provides a novel solution to it.\n- CDLM is one of the first successful consistency training algorithms for masked diffusion models applied to text generation.\n- The demonstrated empirical performance is compelling, and outperforms most of the existing baselines."}, "weaknesses": {"value": "- While motivations to derive the training loss are valid, the technical derivations are flawed. I am not convinced that Eq. 8 admits a unique or well-defined solution.  This essentially states that there exists an x0 prediction map f that behaves like a \"flow map\" for the MDM. Imagine x_t is a sequence full of masked tokens. How does f^* even exist in this case?\n- The distillation loss highly resembles the consistency loss for the continuous diffusion model, while there are no deterministic trajectories provided to ensure the existence of validity of the consistency loss. This makes me wonder about the theoretical guarantee of the method and if it is ready to scale to a larger size. \n- Some important experimental details are missing. For example, the paper does not discuss what CDLM-PPL Optimized is while still listing its performance in the table. Besides, optimized for ppl but loses diversity (entropy) makes me feel worried that the trained model will produce meaningless, low perplexity text frequently.\n- The given anonymous GitHub repo is empty, and therefore, I can't verify the technical implementation."}, "questions": {"value": "See weakness for most of the questions. The following are some additional ones.\n- In Table 1,  CDLMâ€“PPLOptimized produces better ppl than AR model. How is CDLM-PPLOptimized implemented and how is this possible?\n- In Table 1, CDLM-SDTT in general has the lowest entropy across all distilled methods. Why is that? And also, as a few-step generation model, why is progressive distillation with SDTT necessary?\n- In Table 2, why is only DUO with the greedy sampler listed, while DUO itself is not? The low diversity of DUO results seems to be related to the greedy sampler."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bw8LamlEtu", "forum": "inXScPz1gT", "replyto": "inXScPz1gT", "signatures": ["ICLR.cc/2026/Conference/Submission20760/Reviewer_jXUn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20760/Reviewer_jXUn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967434405, "cdate": 1761967434405, "tmdate": 1762934185943, "mdate": 1762934185943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the \"Consistent Diffusion Language Model\" (CDLM), which is trained with a new training objective that enforces consistency between denoising predictions at some state $x_t$ and denoising predictions at less noisy states $x_s$ that are drawn from the posterior $q(x_s|x_t, x_0)$. Training discrete diffusion models like this is inspired by consistency models from the continuous diffusion literature, and the authors find that CDLM can achieve improved results when generating with few sampling steps, compared to relevant baselines. The experiments consist of unconditional and conditional text generation benchmarks, measuring generative perplexity and entropy. The paper also includes ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Results:** CDLM achieves improved results (as measured in generative perplexity) when generating with few sampling steps compared to baselines.\n\n**Originality:** The proposed method is novel, to the best of my knowledge (although it is closely related to Self-Distillation Through Time (SDTT)).\n\n**Significance:** Accelerating diffusion language models is an important research direction that is currently getting a lot of attention, and the paper addresses this problem. Therefore, the work can be considered significant."}, "weaknesses": {"value": "**Presentation and relation to regular Consistency Models:** The paper motivates CDLM from the perspective of consistency models from the continuous diffusion literature, even calling their method \"*Consistent* Diffusion Language Model\". I think this is misleading, because the method really behaves very differently compared to regular consistency models. Since the posterior bridge involves an expectation over $x_s$, training CDLM necessarily always incorporates some form of averaging over different $x_s$ and the corresponding predictions, which prevents learning a perfect few-step denoiser. This is in contrast to the ODE-based consistency objective in regular consistency models, which does not incorporate such an expectation. For instance, I believe it is fundamentally impossible to learn an accurate one-step denoiser from all-mask states with CDLM's objective, while this is possible in regular consistency models. Hence, the way the work is presented is misleading, as readers may falsely get the impression that this is a direct generalization of consistency models to diffusion language models, but this is not the case. I would suggest the authors to better discuss this and present the method accordingly.\n\n**Theoretical justification:** Related to the above point, the paper misses a rigorous theoretical justification for the proposed objective. For regular consistency models, as discussed above, the perfect minimizer of the objective would be a perfect one-step generator, maintaining the original generated distribution. What would the perfect minimizer of the proposed objective here correspond to, given the averaging over $x_s$? Again, this is a very different situation compared to regular consistency models.\n\n**1-step results:** CDLM seems to completely fail in single-step generation (Figure 1). This is again evidence that the model behaves entirely different compared to regular consistency models, which can produce good single-step samples.\n\n**Relation to SDTT:** The method is similar to Self-Distillation Through Time, with the main difference being that here we are training from scratch, whereas SDTT uses a teacher.\n\n**Ad-how modifications and hyperparameters:** It seems the method can only be trained to high performance with additional modifications (mixing in a regular diffusion language model loss) and carefully chosen hyperparameters, like step size $\\delta$ and $\\kappa_{ms}$ schedules. The necessity for these adaptations calls into question how principled the approach is, aligned with the comments above.\n\n**Quality-diversity tradeoff:** The numerical results suggest that there is a quality-diversity tradeoff, also evidenced by the need to train two different models (CDLM vs. CDLM-PPLOptimized). While the method achieves strong generative perplexity, this seems to come with reduced entropy. The reason for this is not discussed in detail, and -- related to my above points -- this is also fundamentally different from regular consistency models, which learn the exact noise-to-data mapping without loss in diversity. This again shows that the proposed method is entirely different from consistency models."}, "questions": {"value": "The paper says it uses the $w(\\delta)=1/\\delta$ weighting *to help path length normalization*. Can the authors explain this better? According to my understanding $\\delta$ essentially corresponds to the path length in time. If we wanted *each unit of \"time\" on the corruption axis contribute equally* (quoting from line 268) then why use the *inverse* $\\delta$, i.e. $1/\\delta$ as weighting, and not $\\delta$ directly? The current formulation gives more weight to short paths (small $\\delta$) and less to longer ones. It would be great if the authors could explain this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4SpEmLjKYr", "forum": "inXScPz1gT", "replyto": "inXScPz1gT", "signatures": ["ICLR.cc/2026/Conference/Submission20760/Reviewer_Jjmk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20760/Reviewer_Jjmk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124687582, "cdate": 1762124687582, "tmdate": 1762934185078, "mdate": 1762934185078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}