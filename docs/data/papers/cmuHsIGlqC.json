{"id": "cmuHsIGlqC", "number": 24239, "cdate": 1758354515160, "mdate": 1763346353618, "content": {"title": "Sharpness of Minima in Deep Matrix Factorization: Exact Expressions", "abstract": "Understanding the geometry of the loss landscape near a minimum is key to explaining the implicit bias of gradient-based methods in non-convex optimization problems such as deep neural network training and deep matrix factorization. A central quantity to characterize this geometry is the maximum eigenvalue of the Hessian of loss, which measures the sharpness of the landscape. Currently, its precise role has been obfuscated because no exact expressions for this sharpness measure were known in general settings. In this paper, we present the first exact expression for the maximum eigenvalue of the Hessian of the squared-error loss at any minimizer in general overparameterized deep matrix factorization (i.e., deep linear neural network training) problems, resolving an open question posed by Mulayoff & Michaeli (2020). To complement our theory, we empirically investigate an escape phenomenon observed during gradient-based training near a minimum that crucially relies on our exact expression of the sharpness.", "tldr": "", "keywords": ["Sharpness", "Deep Matrix Factorization", "Deep Linear Networks"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/75804dd794af075ba48bc84caf15698700b7c9f5.pdf", "supplementary_material": "/attachment/b4f93a5e713a50c6c3599773338d2d025caa5ad4.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors study the optimization landscape of deep matrix factorization which is defined as minimizing the objective function\n\n$$f(W_1,…,W_L) = \\Vert M - W_L \\cdots W_1 \\Vert_F^2,$$\n\nwhere $M \\in \\mathbb R^{d_L\\times d_0}$ is a fixed ground-truth. The main contribution of the paper is an explicit formula for the worst-case sharpness of $f$, i.e., the operator norm of the Hessian, at an arbitrary point (W_1,…,W_L). Whereas existing work by Mulayoff & Michaeli (2020) already derived an explicit formula for the Hessian itself and a closed-form solution for its operator norm at sharpness minimizing global minima, they claimed that finding a closed-form expression of the worst-case sharpness is impossible in general. The authors of the present paper aim to refute this claim with their result."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Clearly written paper\n+ Rigorous result which might be useful for the study of generalization on deep matrix factorization"}, "weaknesses": {"value": "- Imprecise comparison to existing work\n- Main result is „only“ computing the derivative of a specific matrix function and as such rather feels like a technical lemma for further studies than a stand-alone theoretical result"}, "questions": {"value": "On the one hand, I acknowledge that having an explicit formula for the worst-case sharpness is a useful tool for further studies of generalization and implicit regularization in the toy setting of matrix factorization (or linear neural network training). On the other hand, I do not feel that it’s a stand-alone result for a publication yet. What exactly do we learn from having this representation? How can we use it? Consequently, I lean towards not accepting the paper.\n\nI furthermore spotted the following issues/unclear points:\n\n- l. 067: The description of Mulayoff & Michaeli (2020) is imprecise. In fact, they consider a slightly more general objective function in which input/output data of the linear network is included. Although they only consider full-rank data which means that the sets of global minimizers of both problems are equivalent, the data has still impact on the sharpness calculation.\n\n- Definition 1: For which $x_0$ (2) needs to hold?\n\n- l. 097: „… for a minimum $x_\\star$…“\n\n- l. 114: „… flatness/sharpness is measured…“\n\n- l. 121: Maybe I missed it, but is it possible that $\\lambda_\\max$ without argument has not been defined?\n\n- l. 221: To be precise, M does not denote the optimal parameters, but the product that any choice of optimal parameters will produce.\n\n- Eq. (22)-(23): It would help to add the dimensions of U_i and u_i in the optimization problem or at least in the explanation afterwards that u_i is the vectorization of U_i.\n\n- Eq. (29): Shouldn’t the non-negativity constraint on $x$ appear in the Lagrangian?\n\n- l. 316: „… on the hypersphere…“"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JcVC7rSwG8", "forum": "cmuHsIGlqC", "replyto": "cmuHsIGlqC", "signatures": ["ICLR.cc/2026/Conference/Submission24239/Reviewer_F5xk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24239/Reviewer_F5xk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719780178, "cdate": 1761719780178, "tmdate": 1762943011264, "mdate": 1762943011264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Main Revisions"}, "comment": {"value": "Dear reviewers,\n\nFirst of all, we would like to thank you for your valuable insights, and we apologize for submitting the supplementary material and the main manuscript separately, in case this caused any inconvenience during the review process. We would like to remark the main revisions in the manuscript as follows:\n\n**Empirical Evidence: A Parallel ICLR Submission.** We noticed a parallel submission to ICLR [1] that empirically shows sharpness correlates most with the spectral norm of weight matrices in two-layer linear networks, compared to Frobenius and nuclear norms of weight matrices. They further demonstrate that, in deep linear networks, sharpness correlates most with the spectral norms of products of weight matrices, compared to the Frobenius and nuclear norms of products of weight matrices. With this regard, our work is the first to provide a theoretical justification for these empirical findings. We have incorporated this observation into the Section 4 and the discussion section.\n\n**Extension to Local Minima.** We also extended our results to local minima (see Section 4 in the revised manuscript).\n\n**Discussion.** We substantially revised the discussion section.\n\n[1] Anonymous, “Cracking the hessian: Closed-form hessian spectra for fundamental neural networks,” in Submitted to The Fourteenth International Conference on Learning Representations, under review, 2025."}}, "id": "Ezby15CgpX", "forum": "cmuHsIGlqC", "replyto": "cmuHsIGlqC", "signatures": ["ICLR.cc/2026/Conference/Submission24239/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24239/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24239/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763234668327, "cdate": 1763234668327, "tmdate": 1763234668327, "mdate": 1763234668327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main contribution of this paper is the derivation of the maximum eigenvalue of the Hessian of the squared-error loss at any minimizer in general overparameterized deep matrix factorization. The authors refute the claim that deriving a closed-form expression for arbitrary global minima is intractable, which they demonstrate in Theorem 5. This contrasts with prior work, which characterizes the Hessian eigenvalues only at balanced minima (e.g., [1]), a setting that is significantly simpler than the general case. In addition, the authors empirically show that gradient descent consistently escapes dynamically unstable minima, regardless of how close the initialization is to such a point.\n\n[1] \"Learning Dynamics of Deep Linear Networks Beyond the Edge of Stability\", Avrajit Ghosh, Soo Min Kwon, Rongrong Wang, Saiprasad Ravishankar, Qing Qu. ICLR 2025."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "I find the paper well written and generally easy to follow. Since the authors derive a general closed-form expression for the maximum eigenvalue of the Hessian, something that to my knowledge has not been shown before, the paper does make a theoretical contribution. However, I do not believe that this derivation alone is sufficient to justify acceptance. The additional empirical claim that “GD always escapes from a dynamically unstable minimum, regardless of how close the initialization is to that minimum” has already been demonstrated in Ghosh et al. (2025). I discuss this further in the weaknesses section."}, "weaknesses": {"value": "As discussed previously, I believe the empirical observation has already been made by Ghosh et al. (2025). Their contour plots demonstrate that, when initialized at an arbitrary unbalanced point, gradient descent transitions away from dynamically unstable minima toward the flattest minimum (i.e., the balanced solution). The flattest minimum is sufficient to induce periodic oscillations, whereas the other minima, being sharper, cannot sustain them. If I am misunderstanding the distinction, could the authors clarify how their experiments differ from those presented by Ghosh et al. (2025)? Without a clear differentiation, I am concerned that the empirical component does not provide additional novelty.\n\nGiven this, it appears that the main contribution of the paper is Theorem 5. While the derivation is interesting, I am not convinced that it is sufficient on its own to justify acceptance. One possibility is that the authors could use the result from Theorem 5 to theoretically demonstrate why unstable minima cannot sustain oscillations, which would strengthen the empirical story and align it more closely with the observations from Ghosh et al. (2025). However, even with such an extension, I am uncertain whether this would constitute a sufficiently substantial contribution."}, "questions": {"value": "I have asked a few questions in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1cZ7pQlmTU", "forum": "cmuHsIGlqC", "replyto": "cmuHsIGlqC", "signatures": ["ICLR.cc/2026/Conference/Submission24239/Reviewer_BgPX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24239/Reviewer_BgPX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805144582, "cdate": 1761805144582, "tmdate": 1762943011002, "mdate": 1762943011002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper derive the explicit expression for the eigenvalues of Hessian of the deep matrix factorization problem. There are also experiments showing the behavior of GD when the learning rate is around the stability limit $2/\\lambda_{\\max}(Hessian)$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The largest eigenvalue of Hessian is a important concept in machine learning, and the authors derive the explicit expression of it although only for deep matrix factorization. This still provides some insights."}, "weaknesses": {"value": "The main concern is the results of this paper may not be enough for a conference paper. The main theoretical result is the derivation of the largest eigenvalue of Hessian. For the other claim, 'GD escaping from unstable minima' is more or less a known fact in dynamical systems, and there are also stable manifold if the authors do not specify the initial condition $x_0$. The experiments do not seem to introduce new things."}, "questions": {"value": "- Definition 1: Please specify the requirement or the dependency of learning rate $\\eta$, as well as $x_0$. The definition itself is not complete.\n- The authors claim that GD always escape from dynamically unstable minimum. However, even for unstable minima, there could still be stable manifold, although this is a lower dimensional manifold. This goes back to the problem of not specifying initial condition $x_0$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yqyAJsMDL5", "forum": "cmuHsIGlqC", "replyto": "cmuHsIGlqC", "signatures": ["ICLR.cc/2026/Conference/Submission24239/Reviewer_aQPP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24239/Reviewer_aQPP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848006800, "cdate": 1761848006800, "tmdate": 1762943010779, "mdate": 1762943010779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the maximum eigenvalue of the Hessian at global minimizers of deep matrix factorization. It provides exact, closed-form expressions for both deep overparameterized scalar and matrix factorizations. Using these expressions, the authors conduct controlled gradient descent experiments near minima and observe the escape phenomenon when $\\eta > 2/\\lambda_\\max$, consistent with results reported in prior special cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Understanding the loss landscape and its geometry is an important research problem.\n- This paper presents an exact formula for computing the largest eigenvalue of the Hessian at global minima in deep matrix factorization, which appears to be new."}, "weaknesses": {"value": "- The setting considered here appears to be limited to the well-specified case (where the minimum loss is zero). Is it possible to extend the analysis to more general cases or characterize the structure of local minima?\n- The experiments seems not too surprising that gradient descent escapes when the sharpness exceeds $2/\\eta$.\n\nOverall, the paper makes a valuable contribution by providing a formula for the largest eigenvalue of the Hessian at global minima. However, I believe further work may be needed before the paper is ready for publication, such as analyzing the full Hessian structure or extending the results to local minima."}, "questions": {"value": "-\tWhat additional insights can we obtain about the full spectrum of eigenvalues and eigenvectors Hessian or the Hessian at local minima?\n-\tThe current derivation from Eqs. (26)/(27) to (33) seems somewhat complicated. It appears that the result could follow directly from the Cauchy–Schwarz inequality, rather than being formulated as a constrained optimization problem.\n-\tTypo: In Eq. (42), there is an extra period before the last $I$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ryqqyK4wVU", "forum": "cmuHsIGlqC", "replyto": "cmuHsIGlqC", "signatures": ["ICLR.cc/2026/Conference/Submission24239/Reviewer_z1gb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24239/Reviewer_z1gb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969756637, "cdate": 1761969756637, "tmdate": 1762943010502, "mdate": 1762943010502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}