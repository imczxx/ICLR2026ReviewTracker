{"id": "pZQpoxvqdS", "number": 21757, "cdate": 1758321370204, "mdate": 1759896904703, "content": {"title": "Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs", "abstract": "Excel is a pervasive yet often complex tool, particularly for novice users, where runtime errors arising from logical mistakes or misinterpretations of functions pose a significant challenge. While large language models (LLMs) offer promising assistance by explaining formula errors, the automated correction of these semantic runtime errors remains an open problem. A primary challenge to advancing models for such scenarios is the severe lack of high-quality, comprehensive datasets for training and rigorous evaluation. This paper addresses this gap by introducing a novel approach for constructing a benchmark dataset specifically designed for Excel formula repair. We propose a data generation pipeline, which leverages a small set of curated seed samples from online forums to synthetically expand the dataset. Our pipeline integrates few-shot prompting with LLMs and employs a robust LLM-as-a-Judge validation framework, combined with execution-based checks to ensure the correctness and semantic fidelity of the generated data. This process produced a benchmark dataset of 618 high-quality samples, covering common runtime errors. Furthermore, we propose a context-aware baseline technique for Excel formula repair that utilizes LLMs to leverage both the faulty formula, and relevant spreadsheet context.\nWe evaluate the performance of various LLMs (GPT-5, GPT-4.1, GPT-4o, Phi-3, Mistral) on our newly generated benchmark using execution-based metrics. Our analysis demonstrates the dataset's quality through manual annotation and provides insights into error and function distributions. The proposed generation methodology is highly scalable and can be readily adapted to create evaluation benchmarks for similar code repair tasks in other low-resource programming languages.", "tldr": "", "keywords": ["Synthetic Data Generation", "Large Language Models", "Formula Repair"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1682d8ffdc5e86d12d1065d92d052a3450e94d18.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical gap in datasets and methods for repairing semantic runtime errors in Excel formulas, a common pain point for novice users lacking robust APR tools tailored to spreadsheets' unique tabular context. The authors introduce FOREPBENCH, the first large-scale benchmark dataset with 618 high-quality examples spanning five error types (#DIV/0!, #N/A, #NAME?, #REF!, #VALUE!), each including faulty/corrected formulas, spreadsheet context (e.g., cell values and headers), and user utterances. To construct it scalably, they propose BOOTSTRAP GENERATOR: a pipeline starting from 50 manually curated seed samples scraped from forums like MrExcel, expanded via one-shot LLM prompting (using GPT-4o), and rigorously validated through execution-based checks with Calc.ts and an LLM-as-judge framework employing chain-of-thought reasoning for semantic fidelity and difficulty annotation. They also present a context-aware baseline repair technique that prompts LLMs (e.g., GPT-4o, GPT-4.1, Phi-3, Mistral) with erroneous formulas, errors, and extracted table snippets, achieving moderate success on FOREPBENCH as evaluated via execution metrics. Overall, the work demonstrates strong dataset quality through human annotations and error/function distributions, offering a scalable methodology adaptable to other low-resource code repair tasks while highlighting LLMs' potential and limitations in intent-driven formula fixes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It is an interesting topic and promises to contribute to this area.\n\n- Synthetic dataset is an important research direction.\n\n- The method is straightforward to follow."}, "weaknesses": {"value": "- While the proposed synthetic data generation pipeline is creative, it raises concerns about potential bias and limited generalization. Starting from only 50 manually curated seed samples—sourced exclusively from a single forum (MrExcel), which is both small and unrepresentative—and expanding them via one-shot prompting (GPT-4o, temperature 0.64) risks severe mode collapse and hallucination. In such settings, the LLM is likely to replicate seed patterns mechanically rather than capture the true diversity of real-world spreadsheet errors. The validation process further relies heavily on LLM-as-judge (also GPT-4o), despite recent studies showing that such evaluators can be unreliable, often overestimating quality due to shallow pattern matching. Additionally, the Calc.ts check only verifies syntax and runtime correctness, overlooking edge cases involving intent alignment (e.g., ambiguous user instructions or complex table dependencies). Without incorporating a larger and more diverse seed set (e.g., from Stack Overflow or Reddit) or conducting large-scale human annotation beyond the initial seeds, the resulting FOREPBENCH dataset of 618 samples risks being more of an LLM-driven artifact than a robust benchmark, limiting its applicability in real-world scenarios.\n\n- The evaluation design appears overly simplistic and lacks depth. The chosen baseline (single-shot LLM prompting with minimal context) functions more as a strawman, falling far short of existing systems such as FLAME (2024) or neuro-symbolic approaches like LaMirage extensions, which integrate syntax constraints and search. The study reports only pass@1 execution rates on proprietary (GPT-4o/4.1) and open-source models (Phi-3, Mistral), without any ablation studies (e.g., impact of context extraction, prompt variations, or multi-turn repair), human baselines, or cross-dataset transfer (e.g., SpreadsheetCoder corpus), which limits the credibility of the findings. The “real-world alignment” (RQ1) analysis is largely qualitative, relying solely on forum distribution without quantitative measures (e.g., KL divergence over error types or function usage). Difficulty annotations further depend on unverified LLM judgments, which may diverge significantly from human assessments.\n\n- The scope of the benchmark is narrow, limiting its practical impact. It focuses exclusively on five runtime errors (#DIV/0!, #N/A, #NAME?, #REF!, #VALUE!), while ignoring syntax errors, logical bugs, and multi-formula interactions, according to the authors’ own analysis, which dominate real user queries. The dataset further omits critical spreadsheet features such as multi-sheet references, dynamic arrays (e.g., modern Excel’s SPILL errors), and differences between Excel and Google Sheets, which severely undermines its utility. Claims of scalability also lack substance: there is no detailed cost breakdown (only a vague mention of ~1K LLM calls), no efficiency comparison against rule-augmented approaches, and no evidence supporting adaptability to low-resource languages."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "scgI09xAq6", "forum": "pZQpoxvqdS", "replyto": "pZQpoxvqdS", "signatures": ["ICLR.cc/2026/Conference/Submission21757/Reviewer_muPL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21757/Reviewer_muPL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761072320339, "cdate": 1761072320339, "tmdate": 1762941921302, "mdate": 1762941921302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new benchmark for Excel formula repair that considers not only executability but also semantic correctness, i.e., alignment with user intent. The authors first collected seed examples from the web and manually verified them. In the second stage, the dataset is further scaled up using a bootstrap generator approach, by prompting an LLM to generate more problems with 1-shot demonstration of seed problems. Evaluation experiments are conducted on GPT models, Phi-3, and Mistral. Results show the synthetic benchmark is easier than seed real-world samples but useful for relative model comparison. The pipeline is cost-efficient and scalable."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed benchmark considers semantic correctness, which is an important aspect but overlooked by previous works in this domain.\n* The curation of the seed dataset is sound and rigorous.\n* Experiments and analysis for the benchmarking dataset are comprehensive and inspiring."}, "weaknesses": {"value": "* The seed dataset is a more reliable evaluation set, albeit having a small size. In contrast, the quality of the bootstrapped dataset is concerning. To explain, there is an LLM *examiner* who generates the problem along with a reference answer, and an LLM *examinee* who attempts to solve it. If we only think about generating the answer part, with the same underlying model, the *examiner* has no advantage over the *examinee*, except for the 1-shot demonstration (which is ideally not useful for solving the problem). Consequently, without considering randomness in sampling, there is no way for the *examiner* to come up with a problem that is both difficult enough to fail the *examinee* and solvable by the *examiner* itself. That explains why the LLM-generated problems are generally easier than the seed ones. The difficulty of those bootstrapped problems can never go beyond the capability of the LLM itself. It does not make much sense to me to evaluate an LLM on what it already can do. \n* Complementary to the above, I'm concerned about the quality of reference solutions generated by LLMs. Executability checks and LLM-as-judge do not guarantee functional correctness. Empirically, when evaluating the same gpt-4o model that was also used for problem generation, the execution match ratio is still well below 100%, indicating that the model is not confident about the answer across different runs. There's no reason to regard the answer generated during the bootstrap stage as more correct than that generated at test time, given that both use the same model and essentially the same context."}, "questions": {"value": "* What are the reasons for focusing on the task of Excel repair rather than NL2Excel generation from just the spreadsheet and a human instruction? The latter one seems more useful.\n* The inline citation format looks non-standard by missing parentheses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C6BL9IDiLO", "forum": "pZQpoxvqdS", "replyto": "pZQpoxvqdS", "signatures": ["ICLR.cc/2026/Conference/Submission21757/Reviewer_56zX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21757/Reviewer_56zX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761346774420, "cdate": 1761346774420, "tmdate": 1762941920986, "mdate": 1762941920986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FOREPBENCH, a practical benchmark dataset for context-aware Excel formula repair, focusing specifically on semantic runtime errors. The authors present a synthetic data generation pipeline that starts from a small set of curated seed samples and uses few-shot prompting with LLMs to expand the dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a significant gap in the literature. While LLMs have shown promise in code generation and repair for general-purpose languages, their application to spreadsheet formula repair has been underexplored.\n\n- The inclusion of context (table data, headers) and user intent (natural language utterance) is crucial for modeling realistic repair scenarios, moving beyond purely syntactic fixes."}, "weaknesses": {"value": "- **Baseline Method Simplicity:** The proposed baseline repair technique, while context-aware, is essentially a single-prompt engineering approach. It doesn't introduce a novel algorithmic or architectural contribution for repair.\n\n- **Scalability Claim:** The paper claims the methodology is \"highly scalable.\" However, the process relies heavily on a manually curated seed set (59 samples after rigorous manual filtering of forum posts). The scalability of the entire pipeline is therefore contingent on the availability of such high-quality seed data.\n\n- **LLM-as-a-Judge Subjectivity and Reliability:** The human evaluation (RQ2) reveals only moderate agreement (Cohen’s Kappa = 0.502) among annotators. This highlights the inherent subjectivity in judging the quality of formula repairs, especially concerning plausibility and intent alignment. If human experts disagree, the reliability of the LLM-as-a-judge becomes a major concern.\n\n- **Error Type Imbalance** Table 1 shows a significant imbalance in the final dataset, with #DIV/0! (244) and #N/A (140) dominating, while #REF! has only 12 samples. While this might reflect real-world prevalence, it could make the benchmark less effective for evaluating models on rarer error types."}, "questions": {"value": "Q1: How does the reliance on a small, manually curated seed set support the claim of a \"highly scalable\" methodology, and what are the practical limits to scaling this approach?\n\nQ2: Given the moderate inter-annotator agreement on repair quality, how reliable is the LLM-as-a-judge evaluation, and what steps were taken to ensure its judgments align with a consistent standard?\n\nQ3: “GPT-5” only appears in Table 2, is it a typo？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4qcq079W0z", "forum": "pZQpoxvqdS", "replyto": "pZQpoxvqdS", "signatures": ["ICLR.cc/2026/Conference/Submission21757/Reviewer_SEkX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21757/Reviewer_SEkX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890775559, "cdate": 1761890775559, "tmdate": 1762941920711, "mdate": 1762941920711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FOREPBENCH, a benchmark dataset for Excel formula repair, addressing the lack of datasets that capture semantic runtime errors along with spreadsheet context. The authors propose a synthetic data generation pipeline that bootstraps from manually verified seed samples scraped from online Excel forums. The pipeline integrates (1) few-shot LLM prompting for data expansion, (2) execution-based validation for correctness, and (3) an LLM-as-a-judge (LLM VALIDATOR) for semantic and difficulty filtering.\nThe final benchmark comprises 618 validated samples, each containing tabular context, a faulty formula, a repaired formula, and a natural-language utterance. The authors further propose a context-aware baseline repair approach using GPT-4/4o/4.1, Phi-3, and Mistral, evaluated through execution-based metrics: Syntax Validity, Can Execute, and Execution Match. This paper also includes cost analysis, showing the pipeline’s scalability and cost efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper identifies a genuine gap in semantic formula repair for spreadsheets, which differs significantly from syntax-only program repair tasks in structure and context dependency. By focusing on runtime errors and spreadsheet semantics, it opens a practically relevant and novel research direction.\n\n- The combination of manual seed verification, execution validation, and LLM-judge filtering ensures the generated dataset’s correctness and consistency. The detailed curation and verification pipeline provides solid methodological transparency.\n\n- The proposed BOOTSTRAP GENERATOR can potentially be adapted for similar low-resource domains like SQL or low-code environments, offering a modular framework for synthetic data generation under limited supervision."}, "weaknesses": {"value": "- While the dataset fills an important gap, its final size (618 samples) remains small relative to the diversity of real-world Excel usage. Moreover, the samples tend to be simpler than genuine user-generated errors, limiting the dataset’s stress-testing potential.\n\n- Overreliance on GPT-based validation may bias results.\nBecause both dataset generation and evaluation rely on GPT-4 variants (e.g., GPT-4o as generator and GPT-4/4.1 as baselines), the benchmark may be inadvertently tuned to GPT’s output distribution, reducing its neutrality for broader model evaluation.\n\n- The chosen execution-based metrics (Syntax Validity, Can Execute, Execution Match) are simple binary checks. They do not capture semantic equivalence, partial correctness, or user-intent fidelity, which are central to runtime error repair.\n\n- The paper does not explore failure cases where LLM VALIDATOR passes logically inconsistent or implausible examples (e.g., textual values in numeric columns), even though such issues are mentioned qualitatively."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fTD9XgvlBH", "forum": "pZQpoxvqdS", "replyto": "pZQpoxvqdS", "signatures": ["ICLR.cc/2026/Conference/Submission21757/Reviewer_w1ae"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21757/Reviewer_w1ae"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902249226, "cdate": 1761902249226, "tmdate": 1762941920487, "mdate": 1762941920487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}