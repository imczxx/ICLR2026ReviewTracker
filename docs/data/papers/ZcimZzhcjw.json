{"id": "ZcimZzhcjw", "number": 23909, "cdate": 1758350200618, "mdate": 1759896790997, "content": {"title": "Ada-Search: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning", "abstract": "Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has proven effective for building search agents.\nHowever, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content.\nThe central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary.\nPrior work attempted to address overuse by tying rewards and evaluations to the number of search calls. However, shaping rewards based on search-call counts often involves complex reward engineering and provides ambiguous guidance:\nagents may exploit it either by crafting stronger queries or by superficially reducing calls.\nNaively employing search-call counts as an evaluation metric also conflates necessary and unnecessary calls, obscuring the measurement of true adaptive behavior.\nIn this work, we propose **AdaSearch**, a simple outcome-driven RL framework that disentangles and jointly optimizes two objectives:\n(1) solving the task and\n(2) deciding whether to invoke search.\nExperiments with fine-grained metrics such as F1 show that **AdaSearch** improves boundary awareness of model knowledge, reducing unnecessary search calls while preserving task performance.", "tldr": "We propose a multi-task RL that bypass complex reward engineering for building better adaptive search agents.", "keywords": ["Large language models", "retrieval-augmented generation", "search agent"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d750ae2eae87729c5e44b42f233acf8e92102af7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of training large language model (LLM) agents to use an external search tool only when necessary, rather than over-relying on search for every query. Overuse of search can incur unnecessary cost and risk exposure to irrelevant or malicious content. Previous approaches tried to curb excessive tool use by adding penalties for each search call, but this required complex reward engineering and led to ambiguous learning signals, agents might game the reward by avoiding calls even when needed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a multi-task RL training framework (ADASEARCH) that explicitly optimizes both problem-solving and tool-use decision-making, instead of entangling them in a single reward.  It eliminates complex reward shaping – using a simple success/failure reward – yet achieves performance on par with or better than prior methods that used elaborate penalties."}, "weaknesses": {"value": "* ADASEARCH  mainly re-frames existing tool-RL setups by partitioning prompts (decision vs. solving) and using simple outcome-only rewards to introduce a “should I search?” gate; this reads as an engineering refinement rather than a fundamentally new paradigm and thus feels incremental rather than a paradigm shift.\n\n* ADASEARCH is consistently underperform by Search-R1 on EM across benchmarks. While the paper argues it reduces unnecessary tool calls, there is no accompanying cost/latency/token-use analysis (e.g., average tool calls per query, tokens per answer, wall-clock time, or $-cost) to demonstrate a concrete efficiency win that would justify the accuracy trade-off.\n\n* Lack of qualitative case studies or error analysis. The paper would benefit from worked examples showing when the model correctly abstains from search, when it mistakenly avoids or overuses search, and typical multi-hop failure modes—with decision rationales and retrieved evidence.\n\n* Limited evaluation breadth and realism. Testing on newer search benchmarks (e.g., BrowseComp, SimpleQA if applicable) and adding ablations with different retrievers and  responses numbers K would better demonstrate robustness."}, "questions": {"value": "*  Please quantify average tool calls, latency, tokens, and $-cost.\n*  Add a few worked examples showing correct abstentions, under-search, over-search, and typical multi-hop failures (with decision rationales).\n*  Evaluate on newer browsing/search benchmarks (e.g., BrowseComp; SimpleQA if applicable) and vary retrievers，K and threshold  for robustness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4VSjQJjcvy", "forum": "ZcimZzhcjw", "replyto": "ZcimZzhcjw", "signatures": ["ICLR.cc/2026/Conference/Submission23909/Reviewer_TLUY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23909/Reviewer_TLUY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761188845701, "cdate": 1761188845701, "tmdate": 1762942851003, "mdate": 1762942851003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission presents an RL framework designed to help LLMs decide when to rely on their internal knowledge and when to use external search tools. The authors’ primary goal is to reduce the number of unnecessary search calls, while maintaining task performance. \n\nTo solve this problem, the authors introduce AdaSearch, an RL framework that trains LLMs to both solve tasks and make decisions about when to search. AdaSearch proceeds in two stages. In the first stage (problem solving), the model is trained with two prompts: one for parametric reasoning and one for search-based reasoning. The model is rewarded when it produces a correct final answer, as measured by an exact match with the ground truth. In the second stage (decision making), the model learns to decide whether to use search for a given question. Using “pseudo-labels” generated from the first stage’s solve rates, they train a binary classifier to predict whether external search is required. The authors also introduce an end-to-end variant that joints trains the problem-solving and decision-making modules. \n\nThe authors empirically evaluate their framework using the Qwen2.5 and Llama3 models, conducting experiments using question-answering benchmarks, comparing to various baselines. For the search component, they use E5 on a 2018 Wikipedia page dump. \n\nThe main results are presented in Table 2. The metrics the authors report are EM (task accuracy) and F1 score. Across the different tasks, AdaSearch performs comparable to or slightly better than the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "AdaSearch is a clean RL framework for balancing an LLM’s internal knowledge with external search, without relying on hand-tuned reward functions. Their approach works by disentangling problem solving from deciding whether or not to search. I view the simplicity of their approach as a virtue and the main strength of this submission. Despite this simplicity, their method achieves comparable or slightly better performance across multiple QA benchmarks compared to more complicated baselines. Balancing search and internal knowledge is an important, and well-studied, problem. The paper writing is clear."}, "weaknesses": {"value": "While I like the simplicity of this approach, the middling results when compared to the baselines make it unclear when the authors' approach should be used. I understand that previous approaches rely on reward engineering, but a discussion on when to use this approach versus other alternatives would be appreciated."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yIyZhCrofv", "forum": "ZcimZzhcjw", "replyto": "ZcimZzhcjw", "signatures": ["ICLR.cc/2026/Conference/Submission23909/Reviewer_NEh8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23909/Reviewer_NEh8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968577988, "cdate": 1761968577988, "tmdate": 1762942850643, "mdate": 1762942850643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reinforcement learning framework aimed at training large language models to decide when to rely on parametric knowledge versus invoking external search tools. The approach uses a two-stage training process with simple binary outcome rewards instead of complex reward shaping. The authors claim to reduce unnecessary search calls while maintaining task performance. Experiments are conducted on small 3B-parameter models (Qwen2.5 and Llama-3.2)."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Reducing unnecessary search calls in tool-augmented large language models is important both, economically and for system reliability. \n- Empirical evaluation covers a diverse set of Q&A benchmarks. The decision-level F1 metric is a nice addition.\n- The method significantly reduces the number of search calls."}, "weaknesses": {"value": "- Missing ablation for the p-threshold and parameters of the reward-shaping based RL baselines. \n- Benchmarks do not directly reflect an advantage of reduced function calls as Search-R1 consistently outperforms AdaSearch. \n- Lack of hyperparameter settings (learning rate, ...), although promised to be in the appendix (page 6), limits reproducability. \n- There is no evaluation of AdaSearch-E2E. The only comparison is made in Table 5. I suggest the authors focus on one of the methods or extend the analysis in the paper. \n- The writing and formatting often seem rushed and incomplete. For example: \n\t- Table placement should be improved. Table 1 is mentioned on page 2 but appears on page 5. \n\t- Algorithm 2 is not mentioned at all. \n\t- Page 5: \"[...] as detailed in Appendix X.\"\n\t- Hyperparameters promised in the appendix are missing."}, "questions": {"value": "- Can you provide more evaluation results for AdaSearch-E2E?\n- Could you provide ablation studies for naive shaping and AdaSearch? How does the threshold for p affect performance and tool use? How does it compare to different values of lambda?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3gjS6PtreD", "forum": "ZcimZzhcjw", "replyto": "ZcimZzhcjw", "signatures": ["ICLR.cc/2026/Conference/Submission23909/Reviewer_jnAv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23909/Reviewer_jnAv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996894784, "cdate": 1761996894784, "tmdate": 1762942850039, "mdate": 1762942850039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of training LLMs using reinforcement learning to efficiently use search calls to leverage retrieval tasks. In particular, the paper focuses on eliminating the use of search calls when the model's internal knowledge is sufficient to solve the task. To do so, the authors propose a straightforward inference method in which the model is first prompted to decide whether to use its own parametric knowledge (i.e. no search calls), or to use search in its response. The model is then trained using RL to improve all three of these abilities (deciding whether to use search or not, solving without search, and solving with search). The authors show that the resulting method is roughly on par with standard search + RL methods (e.g. Search-R1) in accuracy, but improves in terms of the proportion of tasks that don't utilize search (f1 score)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a clear problem and the proposed method is easy to understand and well-presented. The experiments are also well-done and almost all baselines are considered."}, "weaknesses": {"value": "I found the paper to be primarily weak because of the lack of importance of the proposed problem setting. The paper focuses on reducing the proportion of tasks in which search is invoked, but I don't see a strong argument for why this is actually a useful distinction over prior work that reduces the average number of tool calls. For example, one could argue that not invoking tool calls on some prompts can reduce latency, but the proposed method requires an additional prompt to the LLM. Furthermore, because no existing method focuses on this problem setting, it's no surprise that the proposed metric of F1 score is better optimized by Ada-Search. Even then, there is a missing reward-shaping baseline that more sharply penalizes the transition between 0 and 1 tool calls."}, "questions": {"value": "1. Why is SubEM necessary for the decision reward? If there's actually a big difference with EM, then using SubEM for everything may be better. \n2. Stage 1 and 2 seems to be more of an ablation - I'd suggest keeping E2E as the main method, and introduce stage 1/2 only in the experiments section.\n3. Beyond F1 score, are there results for average number of tool calls per problem for all methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MfPLIKHNyG", "forum": "ZcimZzhcjw", "replyto": "ZcimZzhcjw", "signatures": ["ICLR.cc/2026/Conference/Submission23909/Reviewer_rGmT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23909/Reviewer_rGmT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147338617, "cdate": 1762147338617, "tmdate": 1762942849750, "mdate": 1762942849750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}