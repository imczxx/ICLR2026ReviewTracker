{"id": "go44bcifFA", "number": 18934, "cdate": 1758292121440, "mdate": 1763679097687, "content": {"title": "Can AI Perceive Physical Danger and Intervene?", "abstract": "When AI interacts with the physical world --- as a robot or an assistive agent --- new safety challenges emerge beyond those of purely ``digital AI\". In such interactions, the potential for physical harm is direct and immediate. How well do state-of-the-art foundation models understand common-sense facts about physical safety, e.g. that a box may be too heavy to lift, a hot cup of coffee should not be handed to a child, or that spilled liquid on the floor is a slip hazard? In this paper, our contributions are three-fold: first, we develop a highly scalable approach to continuous physical safety benchmarking of Embodied AI systems, grounded in real-world injury narratives and operational constraints. To probe multimodal safety understanding, we turn these narratives and constraints into photorealistic images and videos capturing transitions from safe to unsafe states, using advanced generative models. Secondly, we comprehensively analyze the ability of major foundation models to perceive risks, reason about safety, and trigger interventions; this yields insights into their deployment readiness for safety-critical agentic applications. Finally, we develop a post-training paradigm to teach models to explicitly reason about embodiment-specific safety constraints provided through system instructions. The resulting models generate thinking traces that makes safety reasoning interpretable and transparent, achieving state of the art performance in constraint satisfaction evaluations.", "tldr": "Comprehensive Physical Safety benchmarking of Foundation models evaluating ability to perceive risks, reason about safety, and trigger interventions", "keywords": ["AI Safety", "Robotics", "Embodied AI", "Evaluations", "Alignment", "Reasoning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a588dc3ddabf793fe0e0dd0539183880d1686cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose ASIMOV-2.0, a new benchmark dataset aimed at evaluating the physical safety awareness of foundation models, particularly for their application in Embodied AI. The benchmark is grounded in real-world injury narratives (NEISS) and industrial safety constraints. It consists of three components: ASIMOV-2.0-Injury (text-based risk assessment), ASIMOV-2.0-Constraints (image-based constraint adherence), and ASIMOV-2.0-Video (video-based intervention timing). The authors evaluate several state-of-the-art models, identify key \"gaps\" (modality, embodiment, latency), and propose a post-training paradigm to improve constraint adherence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Important and Timely Problem: The paper addresses the critical and increasingly relevant problem of physical safety for AI, a topic that is relatively under-explored compared to digital or content safety.\n\n- Multi-Modal Scope: The effort to create a benchmark that spans multiple modalities (text, static images, and video) is a noteworthy step towards evaluating the complex perceptual-reasoning required for physical safety.\n\n- Analysis of \"Thinking\" and Post-training: The paper provides an initial analysis of how inference-time compute (\"thinking\") and targeted post-training can impact safety adherence, which is a valuable area of investigation."}, "weaknesses": {"value": "While the goal of ensuring physical safety for AI is critical, this paper suffers from several fundamental flaws in its premise, methodology, and positioning within the field.\n\n**1. A Fundamental Misalignment with Embodied AI Principles**\n\n- The paper's central claim is that it \"comprehensively analyzes the ability of major foundation models to perceive risks, reason about safety, and trigger interventions\" for Embodied AI systems. This claim is fundamentally **misleading**. True embodied intelligence is defined by its dynamic, closed-loop interaction with a physical environment. This involves a continuous cycle of **Perception -> Planning -> Action -> Feedback.** The agent's actions change the state of the world, and this change  leads to a new cycle.\n- The proposed benchmark, however, evaluates none of this. The tasks in ASIMOV-2.0 are entirely disembodied and passive. They are, **in essence, static, multi-modal VQA or perception tasks:**\n    * ASIMOV-2.0-Injury: A text-based question-answering task.\n    * ASIMOV-2.0-Constraints: A visual question-answering (VQA) task that requires pointing to regions in a static image.\n    * ASIMOV-2.0-Video: A video perception task that requires predicting a single timestamp.\n- The paper merely assesses a model's ability to perceive potential risks in static media (text, images, videos). While perceptual safety is a component of embodied safety, it is not a sufficient proxy for it. A model might fail these perception-only tasks  but still perform safely in an embodied setup where it can actively query the environment. Conversely, a model that passes these static tests might catastrophically fail in a dynamic setting that requires real-time planning and adaptation. \n- The paper fails to provide any evidence that the \"safety risks\" identified in this passive setting are the same ones that exist in an active embodied agent framework.\n\n**2. Simplistic Task Design and Questionable Practical Utility**\n\nThis core flaw leads to the second major weakness: the tasks themselves are overly simplistic and their real-world utility is questionable.\n* In ASIMOV-2.0-Constraints, the task is to point to objects that violate a constraint. This is a monolithic and artificial task format. A more meaningful evaluation would embed this constraint within a larger, multi-step embodied task (e.g., \"Clean the room, but you cannot lift objects over 20kg\"). This would test whether the agent's planning and action sequence adheres to the constraint, which is far more critical than a simple VQA pointing-task.\n* In ASIMOV-2.0-Video, the task is to identify the \"last possible timestamp\" for intervention. It is entirely unclear what the real-world application of this is. A practical safety system does not ask for a post-hoc analysis of the \"last possible\" intervention point; it must proactively and continuously assess risk and intervene immediately when a threshold is crossed. This task format feels arbitrary and disconnected from any plausible engineering implementation of a safety intervention system.\n\n**3. Critical Omission of Relevant Embodied AI Safety Benchmarks**\n\nThe authors' claim to be filling a gap in \"physical safety for robots\" is made without acknowledging the large and growing body of literature that does exactly this, but in actual embodied environments. The related work  conspicuously ignores numerous highly relevant benchmarks.\n\n- The paper fails to cite, compare against, or differentiate itself from existing embodied AI safety benchmarks, such as SafeAgentBench，IS-Bench，EARBench，LabSafetyBench，Lota-Bench，SafePlan-Bench [1-5] and so on.\nThese benchmarks are explicitly designed to evaluate LLM-based agents in embodied environments. They directly test an agent's ability to follow safety rules, avoid hazards, not just passive perception.\n\n- By ignoring this entire line of relevant work, the authors fail to (a) properly motivate their own benchmark, (b) demonstrate its novelty, and (c) justify why a disembodied, perception-only benchmark is a superior or even necessary contribution to a field that has already moved on to more complex, interactive evaluations.\n\n**4. Over-reliance on Synthetic Data Generation**\n\nWhile grounding in NEISS reports is a good starting point, the entire benchmark pipeline relies on generative models (LLMs, Imagen, Veo3) to create the scenarios and all testing media. This means the benchmark is not evaluating safety in the real world, but evaluating safety perception on synthetic media. The \"long-tail\" risks identified might be artifacts of the generative models themselves, rather than representative of physical reality. This adds another layer of abstraction that distances the evaluation from true \"physical\" safety.\n\n\n**Reference:**\n\n[1] SAFEAGENTBENCH: A BENCHMARK FOR SAFE TASK PLANNING OF EMBODIED LLM AGENTS\n\n[2]Earbench: Towards evaluating physical risk awareness for task planning of foundation model-based embodied ai agents.\n\n[3]Hazard challenge: Embodied decision making in dynamically changing environments.\n\n[4]Is-bench: Evaluating interactive safety of vlm-driven embodied agents in daily household tasks.\n\n[5]LabSafety Bench: Benchmarking LLMs on safety issues in scientific labs."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Bt21U4JWAG", "forum": "go44bcifFA", "replyto": "go44bcifFA", "signatures": ["ICLR.cc/2026/Conference/Submission18934/Reviewer_oYkf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18934/Reviewer_oYkf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815352919, "cdate": 1761815352919, "tmdate": 1762930992566, "mdate": 1762930992566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a multimodal benchmark for physical safety across text, image, and video. The work systematically evaluates frontier models on latent risk identification, action outcome assessment, embodied constraint adherence, and the timing of video-based interventions. It further investigates how increased “thinking” budgets (chain-of-thought/reasoning compute) and post-training (SFT+RL) affect adherence to safety constraints, showing that small safety datasets can substantially reduce violation rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1、The benchmark Grounding in NEISS narratives and industrial safety standards strengthens real-world alignment and supports scalability. \n\n2、The generate–critic–refine pipeline improves data quality and enables continual dataset evolution.\n\n3、The paper is easy to follow."}, "weaknesses": {"value": "1、The heavy reliance on Imagen/VEO3-generated images and videos may limit external validity due to realism gaps and distribution shift. I recommend adding a small real-image/video validation slice and reporting cross-domain consistency, including absolute performance deltas and rank stability between synthetic and real subsets. \n\n2、The Action outcomes, severity, and intervention timing depending on contextual constraints (reachability, latency) may lead to the disagreement. Aggregating with 60–80% consensus thresholds discards informative ambiguity. I suggest preserving multi-annotator label distributions and evaluating with soft labels or proper scoring rules to test whether models show low confidence or cautious behavior on high-disagreement samples.\n\n3、The evaluation metrics lack sufficient interpretability.The constraint violation metric penalizes any point falling into a violating box, but task difficulty varies with constraint type, scene complexity, and occlusion. I think you could provide stratified curves by scene complexity (object count, occlusion, scale variation) and by the number of composed constraints to localize capability bottlenecks more precisely."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fPQYdkuvKF", "forum": "go44bcifFA", "replyto": "go44bcifFA", "signatures": ["ICLR.cc/2026/Conference/Submission18934/Reviewer_6BXZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18934/Reviewer_6BXZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874424622, "cdate": 1761874424622, "tmdate": 1762930926983, "mdate": 1762930926983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ASIMOV-2.0, a comprehensive benchmark suite designed to evaluate whether AI systems—especially multimodal and embodied ones—can perceive physical danger and intervene appropriately. The benchmark spans three modalities: text (ASIMOV-Injury), image (ASIMOV-Constraints), and video (ASIMOV-Video), grounded in real-world hospital injury narratives and operational safety standards. The authors assess leading proprietary models (GPT-5, Claude Opus 4.1, Gemini 2.5 Pro) and reveal three consistent gaps: a modality gap, an embodiment gap, and a latency gap (smaller, faster models perform worse). Finally, they propose a “thinking-for-safety” paradigm, where explicit reasoning traces and post-training improve safety constraint adherence and interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper aims to address an underexplored yet crucial domain, physical safety in embodied AI, through a multimodal, real-world-grounded benchmark.\n\n- The authors evaluate several state-of-the-art models and uncovers distinct modality, embodiment, and inference-budget gaps. They demonstrate that structured \"thinking\" traces and post-training can make safety reasoning more transparent and consistent across tasks."}, "weaknesses": {"value": "- The use of large models as critics in the data-generation loop raises concerns—without independent human auditing, bias and hallucination could propagate, undermining benchmark validity.\n\n- Although grounded in injury data, the benchmark’s images and videos are AI-generated, and may not capture real-world physics or social context faithfully; this limits external validity.\n\n- Evaluation excludes open-source and mid-tier models, restricting reproducibility and the generality of conclusions across the model landscape.\n\n- While post-training improves safety scores, the paper does not quantify potential degradation in general reasoning or task performance, raising the risk that safety gains may come at the expense of versatility."}, "questions": {"value": "- How is the critic model’s reliability validated during the generator–critic–refine loop? Are there calibration or cross-checking mechanisms with human reviewers?\n\n- How realistic are the AI-generated visual scenes? Have the authors measured perceptual realism (e.g., via human evaluation or physics consistency metrics)?\n\n- Why were open-source foundation models (e.g., LLaVA, Qwen-VL, or Kosmos series) excluded? Would the benchmark be publicly released to support reproducibility?\n\n- How does “thinking-for-safety” affect broader capabilities—e.g., general reasoning, speed, or creativity? Is there evidence that the approach scales without diminishing non-safety performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kaI2CouuB7", "forum": "go44bcifFA", "replyto": "go44bcifFA", "signatures": ["ICLR.cc/2026/Conference/Submission18934/Reviewer_gjnM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18934/Reviewer_gjnM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987729829, "cdate": 1761987729829, "tmdate": 1762930924791, "mdate": 1762930924791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a benchmark that evaluates whether AI models can perceive physical harm. The benchmark spans text, image and instructions, and video modalities, allowing comprehensive assessment across different forms of physical-risk understanding. The authors analyze how popular models perform on identifying risks over time, and propose a post-training paradigm that teaches models to reason about physical safety constraints, which improves physical safety performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem is practical and important, especially for real-world embodied AI systems. As far as I know, this is the first benchmark addressing physical safety across text, image, and video modalities.\n\n2. The dataset is grounded in reliable and authoritative sources. For example, the authors use the NEISS system, which collects injury reports from approximately 100 hospitals in U.S.. They also incorporate industrial safety standards such as ISO 10218-1:2025 and ISO/TS 15066:2016.\n\n3. The benchmark addresses operational safety, which is particularly significant for embodied AI systems.\n\n4. The authors evaluate several popular models on their abilities with the benchmark.\n\n5. The writing is clear and easy to understand."}, "weaknesses": {"value": "1. The generated images and videos might fail to relect real-world cases, especially for task-specific embodied AI systems. \n\n2. The data size is small, which might fail to capture edge cases in real-world scenarios. \n\n3. Embodied AI systems can handle complex tasks such as perception, reasoning, and physical actions, and therefore rely on different hardware such as cameras, and different sensors. Therefore, there may be a gap between the data an embodied system collects and the data used in the benchmark."}, "questions": {"value": "1. There may be a gap between the data an embodied system collects and the data used in the benchmark. A real embodied agent may leverage sensors to detect signals or subtle movements, instead of images or videos. However, such data are not explicitly represented in the benchmark. Could you provide more explanation on how the benchmark could be extended to match real-world use cases?\n\n2. The benchmark identifies physical safety issues. But how these evaluations relate to concrete tasks performed by real-world embodied agents? Could you provide some practical use cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9iNKXogv5y", "forum": "go44bcifFA", "replyto": "go44bcifFA", "signatures": ["ICLR.cc/2026/Conference/Submission18934/Reviewer_JKqL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18934/Reviewer_JKqL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762677062323, "cdate": 1762677062323, "tmdate": 1762930923767, "mdate": 1762930923767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}