{"id": "oIvIsK5AwB", "number": 15414, "cdate": 1758251090569, "mdate": 1759897308569, "content": {"title": "Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning", "abstract": "Multimodal large language models (MLLMs) promise enhanced reasoning by integrating diverse inputs such as text, vision, and audio. \nYet, despite their perceptual strengths, their reasoning ability across modalities remains underexplored, with conflicting reports on whether additional modalities help or harm performance. \nThese inconsistencies stem from a lack of controlled evaluation frameworks and analysis of models' internals to isolate when and why modality interactions support or undermine reasoning.\nWe address this gap through a logic-grounded evaluation framework that categorizes multimodal reasoning into six interaction patterns, varying how facts are distributed across modalities and logically combined. \nEmpirically, additional modalities enhance reasoning only when they provide independent and sufficient reasoning paths, while redundant or chained entailment support often hurts performance.  \nBesides, models recognize cross-modal facts reliably and always reason on text effectively.\nMoreover, reasoning degrades in three systematic ways: weaker modalities drag down overall performance, conflicts bias preference toward certain modalities, and joint signals from different modalities fail to be integrated effectively. Therefore, we identify two core failures: task-composition bottleneck, where recognition and reasoning cannot be jointly executed in one pass, and fusion bottleneck, where early integration introduces bias. \nFor further investigation, we find that attention patterns fail to encode fact usefulness, but a simple two-step prompting (recognize then reason) restores performance, confirming the task-composition bottleneck.\nMoreover, modality identity remains recoverable in early layers, and softening attention in early fusion improves reasoning, highlighting biased fusion as another failure mode.\nOverall, our findings show that integration, not perception, is the main barrier to multimodal reasoning, suggesting composition-aware training and early fusion control as promising directions.", "tldr": "", "keywords": ["Multimodal large language models", "Modality interactions", "Multimodal reasoning", "Model interpretation", "Attention probing", "Causal analysis", "Task composition", "Modality bias", "Logic-driven evaluation"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c85df98c56786fc658b5d1188e4ec51e4595e4d.pdf", "supplementary_material": "/attachment/f04c8bfd3e508f6101cda0986839b114ef2ea69f.zip"}, "replies": [{"content": {"summary": {"value": "Disclosure: Claude is used to refine this review.\n\nThis paper investigates when and why additional modalities help or hurt reasoning in multimodal large language models. The authors introduce a logic-grounded evaluation framework with six canonical interaction types (Equivalence, Alternative, Entailment, Independence, Contradictory, Complementary) that systematically vary how facts are distributed across modalities. Through controlled experiments on synthetic reasoning tasks, they identify two core bottlenecks: task-composition (inability to jointly perform recognition and reasoning) and fusion (biased early-layer integration). Simple interventions like two-step prompting and attention temperature adjustment substantially improve performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The six interaction types provide a principled, logic-grounded framework for isolating different aspects of multimodal reasoning. The controlled setup with synthetic data allows precise measurement of effects that would be confounded in naturalistic settings.\n- The paper goes beyond black-box evaluation to probe internal mechanisms through attention analysis and targeted interventions. The connection between external performance patterns and internal representations is clearly demonstrated.\n- The identification of task-composition and fusion bottlenecks is well-supported, and the proposed interventions (two-step prompting, attention temperature control) are simple yet effective. The paper convincingly shows that integration, not perception, is the main barrier.\n- The evaluation includes proper baselines, multiple models, and systematic ablations. The paper is well-written and easy to follow."}, "weaknesses": {"value": "- The main limitation is that the analysis is done with synthetic data. The entire evaluation uses controlled synthetic reasoning tasks. While this enables precise control, it raises serious questions about generalizability to real-world multimodal reasoning where visual complexity, ambiguity, and grounding matter significantly. The claim that these findings inform real MLLM behavior needs empirical validation on naturalistic tasks. \n- Only single-step reasoning tasks are considered. It's unclear whether findings hold for larger models, more complex reasoning chains, or diverse domains.\n- While attention probing and interventions demonstrate effects, the paper doesn't deeply explain why these patterns emerge or how they relate to training objectives. \n- Several related work has covered a subset of the findings in the paper, e.g., [1] [2]. The difference should be discussed.\n\n[1] Wu et. al. Mitigating Modal Imbalance in Multimodal Reasoning. COLM 2025 https://www.arxiv.org/abs/2510.02608\n[2] Fu et. al. Hidden in plain sight: VLMs overlook their visual representations. COLM 2025 https://arxiv.org/abs/2506.08008"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "76FofZX2zA", "forum": "oIvIsK5AwB", "replyto": "oIvIsK5AwB", "signatures": ["ICLR.cc/2026/Conference/Submission15414/Reviewer_Wn7A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15414/Reviewer_Wn7A"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955012926, "cdate": 1761955012926, "tmdate": 1762925692483, "mdate": 1762925692483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates when and why multimodal large language models (MLLMs) succeed or fail at logical reasoning across modalities. The authors introduce a logic-grounded evaluation framework with six canonical interaction patterns (Equivalence, Alternative, Entailment, Independence, Contradictory, Complementary) that systematically vary how facts are distributed across text, vision, and audio modalities. Through controlled experiments on four open-source MLLMs (Baichuan-Omni, Qwen2.5-Omni, MiniCPM-o, Phi-4), they find that additional modalities only help when providing independent reasoning paths, while redundant or chained evidence often hurts performance. The authors identify two core bottlenecks: (1) task-composition bottleneck where models struggle to jointly perform recognition and reasoning, and (2) fusion bottleneck where early-layer integration introduces modality bias. Probing and intervention experiments validate these bottlenecks and show that simple remedies like two-step prompting and attention temperature manipulation can alleviate these issues."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear writing and presentation**: The paper is well-written with clear motivation and systematic exposition.\n3. **Systematic evaluation**: Six logic-grounded interaction types enable controlled analysis of information distribution across modalities.\n4. **Controlled design with actionable insights**: Synthetic data with standardized renderings isolates reasoning mechanisms, and probing analysis provides causal understanding beyond black-box evaluation."}, "weaknesses": {"value": "1. Unrealistic modality representations: Your modalities use artificial encodings (GraphViz graphs, monotone TTS) rather than naturalistic isomorphic representations commonly seen in practice. IsoBench (Fu et al., 2024, https://arxiv.org/abs/2404.01266) uses real-world formats (e.g., math expressions as text vs. diagrams, chess as notation vs. images) and finds 28.7-point drops for Claude-3. Could you validate with similar real-world isomorphic representations?\n2. Oversimplified tasks: Single-template facts (\"subject + is + adjective\") and single-step reasoning lack complexity of real multimodal reasoning (spatial relations, temporal dynamics, multi-hop inference). Do findings generalize to realistic scenarios?\n3. Limited model scope: Only 4 small models (5-8B) are tested, leaving unclear whether findings generalize to larger systems. Could you add at least one large-scale model to validate the bottlenecks persist at scale?\n4. No statistical testing: Despite 1,000 instances per condition, no error bars or significance tests. Are modest differences (+1.7%, -5.7%) statistically meaningful?"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JOmZiQHnv3", "forum": "oIvIsK5AwB", "replyto": "oIvIsK5AwB", "signatures": ["ICLR.cc/2026/Conference/Submission15414/Reviewer_YhaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15414/Reviewer_YhaC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995845153, "cdate": 1761995845153, "tmdate": 1762925691605, "mdate": 1762925691605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses a pipeline to test cross-modal reasoning. Specifically, it tests the audio, vision, and text modalities and their combinations. The authors obtained their findings from controlled experiments. They have conducted several interesting experiments on different categories of reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The authors define different types of the logics to test the performance.\n\nWith the defined categories, the authors clearly identify the core bottlenecks."}, "weaknesses": {"value": "1. The study's conclusions are built on overly simplified, synthetic data. Facts are basic statements like \"Alice is awesome\", which are then turned into simple schematic graphs or clean TTS audio. This setup doesn't reflect the complexity of the real world, where models must reason over natural images, noisy audio, and complex text. This \"toy\" data makes it questionable if the findings will hold in more realistic scenarios.\n\n2. Many of the paper's \"findings\" are intuitive consequences of its data design. For example, It's not surprising that performance drops in the Complementary setting, where models must fuse necessary facts from different modalities. It's also expected that performance drops in the Independence setting when \"weaker\" audio or visual modalities are added, as the text is designed to be the clearest source of information (as vision and audio are converted from the text). The conclusion that text is the strongest modality seems pre-determined by this design rather than being a deep insight. This makes the paper's takeaways feel unsolid.\n\n3. The paper's writing and structure make its contribution unclear. The \"Related Work\" section is relegated to the appendix, so the reader cannot easily compare this benchmark to existing ones. The authors claim previous studies are \"anecdotal or domain-specific\" but never justify in the main paper why their own synthetic data is a better alternative. This omission makes it difficult to judge the paper's novelty or true contribution."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nNsWwiBLVk", "forum": "oIvIsK5AwB", "replyto": "oIvIsK5AwB", "signatures": ["ICLR.cc/2026/Conference/Submission15414/Reviewer_uT7W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15414/Reviewer_uT7W"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762075278453, "cdate": 1762075278453, "tmdate": 1762925690854, "mdate": 1762925690854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the mechanisms and limitations of Multimodal Large Language Models (MLLMs) in performing logical reasoning across diverse modalities (text, vision, audio). Addressing conflicting reports in literature regarding whether additional modalities aid or hinder MLLM performance, the authors introduce a controlled, logic-grounded evaluation framework. This framework categorizes multimodal reasoning into six interaction patterns: Equivalence, Alternative, Entailment, Independence, Contradictory, and Complementary.\n\nUsing a synthetic dataset where facts are rendered as text, synthesized audio, and schematic visuals, the study evaluates several open-source MLLMs (Baichuan-Omni, Qwen2.5-Omni, MiniCPM, Phi-4) and identifies several observations and bottlenecks. The authors validate these bottlenecks through interpretability probes and interventions, showing that two-step prompting (decoupling recognition and reasoning) and softening attention in early layers can mitigate these failures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The introduction of the six canonical interaction types (Equivalence, Alternative, Entailment, etc.) makes sense, which moves multimodal evaluation beyond simple accuracy metrics on varied tasks to a structurally grounded analysis of how information is combined.\n\n- The paper moves beyond just reporting performance drops. It offers diagnostic evidence for why these drops occur.\n\n- By identifying that integration, not perception, is the main barrier, the work offers actionable direction for future modeling (e.g., composition-aware training objectives, explicit fusion control mechanisms)."}, "weaknesses": {"value": "- The synthetic data generation process and dataset details are not clearly described in the paper.\n\n- The observations and conclusions made in the paper are based on synthetic data. Ideally, it would be nice to see how (some of) these observations could be made on more realistic settings with real data (e.g., OmniBench)."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZopDKMlh9n", "forum": "oIvIsK5AwB", "replyto": "oIvIsK5AwB", "signatures": ["ICLR.cc/2026/Conference/Submission15414/Reviewer_2P4W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15414/Reviewer_2P4W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159355213, "cdate": 1762159355213, "tmdate": 1762925690047, "mdate": 1762925690047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}