{"id": "9m5EU9QqNJ", "number": 3046, "cdate": 1757321129873, "mdate": 1759898111824, "content": {"title": "PASTEL: Panoramic Alignment for Monocular 4D Reconstruction", "abstract": "Reconstructing 4D scenes from casually captured monocular video is vital for applications in virtual reality (VR) and embodied AI. Recent advances in 4D reconstruction and novel view synthesis have significantly propelled this capability. However, all existing methods depend on pixel-level supervision and cannot recover regions beyond visible camera limits. Consequently, we introduce a new paradigm that reconstructs both the “visible regions\" from monocular input and the ``invisible regions\" beyond observable camera boundaries. The most intuitive solution is to leverage video generation models as ”generative priors\". However, their inherent stochasticity and large solution space prevent stable and consistent view synthesis. As a result, naively incorporating generative content into the reconstruction process often causes artifacts, especially when camera trajectories deviate significantly from the original video. To overcome these challenges, we present Panoramic Alignment for StraTegic ExpLoitation of Generative Priors (PASTEL) to mitigate inconsistencies brought by generative priors. Specifically, PASTEL first aligns the scene into a spherical panoramic representation. Within this space, it identifies trajectories that minimize deviation while maximizing exploration beyond observable boundaries. These trajectories enable generative priors for stable and consistent exploration beyond visible camera limits. PASTEL further designs a comprehensive view expansion with strategic 4D scene supervision to alleviate inconsistencies from generative priors. Experimental results show that PASTEL can not only extrapolate plausible scene content beyond the observable boundaries of input monocular videos, but also substantially boost monocular 4D reconstruction performance, outperforming the previous state-of-the-art method by 1.2dB in PSNR on the DyCheck Iphone dataset.", "tldr": "", "keywords": ["4D reconstruction", "video generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22388f5e2fde196a43d848a99c0a4557137839ee.pdf", "supplementary_material": "/attachment/3934842b5d9cb9903a894705c126f06b8d63d0a5.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes PASTEL, a panoramic alignment framework for monocular 4D reconstruction that extends scene coverage beyond visible camera regions while maintaining spatial-temporal consistency. The method integrates panoramic alignment through spherical projection, adaptive trajectory identification to regulate camera motion, and strategic 4D supervision to reduce generative inconsistencies when using pre-trained priors such as TrajectoryCrafter. By unifying these modules into a single pipeline, PASTEL improves novel-view fidelity and stability. Experiments on DyCheck and NSFF demonstrate consistent gains in PSNR, SSIM, and perceptual metrics over existing baselines, validating the effectiveness of panoramic alignment for monocular 4D reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This work addresses a clear and under-explored challenge, which is handling “invisible regions” in monocular 4D scenes.\n\n2.Panoramic alignment and trajectory identification are intuitive and practically effective.\n\n3.Comprehensive experiments on DyCheck and NSFF, including covisible and out-of-view regions, with strong PSNR/SSIM/LPIPS/FID performance.\n\n4.Ablation studies cover most modules and show consistent trends."}, "weaknesses": {"value": "1.While the method provides reasonably detailed descriptions of its panoramic trajectory design, view-expansion blending, and strategic-supervision weighting, several aspects of their motivation and behavior remain underexplored. In particular, Equations (6–11) define the optimization objectives but lack intuitive explanation or empirical analysis demonstrating why these components stabilize generative priors. Additional visualization or ablation-based interpretation, for example showing how trajectory adaptation improves consistency, would make the mechanism more convincing.\n\n2.The framework primarily combines existing paradigms, such as spherical reprojection, generative priors (TrajectoryCrafter), and 4DGS/MoSca representations, without introducing fundamentally new architectures or loss designs. While the panoramic alignment component is intuitively motivated, it can be viewed more as a geometric reparameterization than a novel learning mechanism, making the conceptual contribution appear more incremental than transformative.\n\n3.Performance relies heavily on accurate depth/pose/flow predictions and the generative quality of TrajectoryCrafter. While the sensitivity analyses help, this dependency somewhat limits generalizability."}, "questions": {"value": "The paper would benefit from more in-depth clarification of its core mechanisms. In particular, the panoramic trajectory design, view-expansion blending, and strategic-supervision weighting are introduced with equations but lack clear intuition or visualization showing how they stabilize generative priors. It would also help to further distinguish PASTEL from existing generative-alignment frameworks such as TrajectoryCrafter or MoSca, clarifying whether the improvements stem from new algorithmic design or from the integration of known components."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HQq5zYdHbE", "forum": "9m5EU9QqNJ", "replyto": "9m5EU9QqNJ", "signatures": ["ICLR.cc/2026/Conference/Submission3046/Reviewer_c8tk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3046/Reviewer_c8tk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731214351, "cdate": 1761731214351, "tmdate": 1762916524523, "mdate": 1762916524523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “PASTEL: Panoramic Alignment for Monocular 4D Reconstruction” proposes a framework that reconstructs 4D dynamic scenes from monocular videos by integrating panoramic alignment and generative priors. The method introduces a spherical panoramic representation to design optimal camera trajectories that balance deviation minimization and exploration beyond visible regions. It then employs a static-dynamic projection strategy to expand views and uses diffusion-based video generation (TrajectoryCrafter) as a prior for inpainting unseen areas. The approach achieves competitive results on DyCheck and NSFF, outperforming reconstruction- and generation-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper tackles an important challenge: recovering unobserved 4D regions from monocular videos, by leveraging generative diffusion priors in a geometrically guided way.\n+ The paper provides extensive evaluations on both synthetic and real-world datasets, ablation studies on panoramic alignment, trajectory alignment, and supervision masks, and sensitivity analyses on depth/pose errors."}, "weaknesses": {"value": "- Overly complicated and fragmented pipeline. The method involves multiple non-trivial steps: panoramic reprojection, trajectory identification, static-dynamic decomposition, diffusion-based video generation, Gaussian splatting refinement, and strategic supervision, which together make the system difficult to apply and obscure the core technical novelty.\n\n- Limited novelty beyond careful integration. While the panoramic alignment and strategic supervision are well-engineered, the approach mainly repackages existing components rather than introducing fundamentally new learning mechanisms or representations. The contribution feels more architectural than conceptual.\n\n- Weak justification and dependency on external priors. The framework heavily depends on pretrained generative priors and external depth/pose estimators (UniDepth, RAFT, Bootstapir). This reliance diminishes the originality and raises concerns about how much of the improvement comes from the proposed framework versus stronger pretrained modules."}, "questions": {"value": "Could the proposed panoramic alignment be simplified or integrated into an end-to-end learning model rather than relying on handcrafted reprojection and trajectory rules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J9ZQw7QIB1", "forum": "9m5EU9QqNJ", "replyto": "9m5EU9QqNJ", "signatures": ["ICLR.cc/2026/Conference/Submission3046/Reviewer_wtCL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3046/Reviewer_wtCL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978091941, "cdate": 1761978091941, "tmdate": 1762916524303, "mdate": 1762916524303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PASTEL, a method for reconstructing geometry- and motion-consistent 4D scenes from monocular video that explicitly addresses the challenging problem of synthesizing content in regions outside the observed camera coverage. The key innovation lies in transforming the reconstruction task into panorama space, which simplifies novel trajectory design and enables strategic use of generative priors to fill invisible regions while maintaining geometric consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- PASTEL introduces a novel framework for monocular 4D scene reconstruction that not only rebuilds visible regions but also consistently infers \"invisible regions\" beyond observable camera limits.\n- Also, the elegant dimensional reduction is quite interesting.  The panorama alignment is a genuinely clever contribution that reduces the high-dimensional camera extrinsic parameter search to a tractable 2D directional search using dm=(cos⁡(2πm/M),sin⁡(2πm/M))d_m = (\\cos(2\\pi m/M), \\sin(2\\pi m/M)), dm​=(cos(2πm/M),sin(2πm/M)). This makes trajectory sampling computationally feasible and interpretable.\n- The method leverages a panoramic alignment to identify optimal camera trajectories for generative priors, followed by a static-dynamic projection and strategic 4D scene supervision to mitigate inconsistencies."}, "weaknesses": {"value": "1. Generative prior domain shift: Diffusion models trained on large-scale datasets can introduce stylistic and illumination inconsistencies that conflict with the specific scene.\n- Have you experimented with scene-specific adaptation of the generative prior (e.g., LoRA fine-tuning on observed regions)? If so, what was the impact on generalization to unseen regions?\n\n2. Hyperparameter sensitivity: The method introduces several hyperparameters (MM M for trajectory directions, LL L for interpolation steps, ϵ\\epsilon ϵ for SSIM threshold).\n-  How sensitive are results to these choices? Is there an automated or principled way to select them?\n\n3. Performance gains primarily from static regions: While the direct warping approach is novel, a critical concern is that the reported performance improvements appear to be predominantly driven by static background regions rather than dynamic content.\n If the improvement is mostly in static regions, the contribution becomes less compelling for true 4D scene understanding, as static region reconstruction from multiple views is a more well-studied problem. The value proposition should be in handling dynamic content in unseen regions.\n\n4. Limited analysis of dynamic content generation: The core challenge in 4D reconstruction is handling dynamic objects with complex motion.\n- How well does the method generalize to highly dynamic scenes with significant occlusions and dis-occlusions? The provided examples seem to feature relatively simple motions."}, "questions": {"value": "N/A (Included in weakness)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QnUukdt9Ag", "forum": "9m5EU9QqNJ", "replyto": "9m5EU9QqNJ", "signatures": ["ICLR.cc/2026/Conference/Submission3046/Reviewer_pwmL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3046/Reviewer_pwmL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984863424, "cdate": 1761984863424, "tmdate": 1762916524160, "mdate": 1762916524160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}