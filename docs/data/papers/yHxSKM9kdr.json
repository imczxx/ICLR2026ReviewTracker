{"id": "yHxSKM9kdr", "number": 541, "cdate": 1756745197080, "mdate": 1759898254648, "content": {"title": "IceCache: Memory-Efficient KV-cache Management for Long-Sequence LLMs", "abstract": "Key-Value (KV) cache plays a pivotal role in accelerating inference in large language models (LLMs) by storing intermediate attention outputs, thereby avoiding redundant computation during auto-regressive generation. However, the cache's memory footprint scales linearly with sequence length, often resulting in memory bottlenecks on constrained hardware. While prior work has explored offloading KV-cache to the CPU and maintaining a reduced subset on the GPU, these approaches frequently suffer from imprecise token prioritization and degraded performance in long-generation tasks such as multi-turn dialogues and chain-of-thought reasoning. In this paper, we propose a novel KV-cache management strategy called IceCache, that integrates semantic token clustering with PagedAttention, a memory-efficient paging mechanism. By clustering semantically related tokens and organizing them into a hierarchical, dynamically updateable structure, our method improves cache hit rates and memory bandwidth utilization during CPU-GPU transfers. Experimental results show that IceCache achieves over 99\\% accuracy with a 256-token budget and still maintains 97\\% accuracy with only a 64-token budget, compared to the full KV-cache model. It outperforms existing baselines even while using just 25\\% of the KV-cache token budget, demonstrating its superior accuracy in long-sequence scenarios.", "tldr": "", "keywords": ["LLM Inference; KV-cahce Optimization; Sparse Attention"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a658ac07077db0edd6d2a18393ee42cc9f6973a7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces IceCache, a novel KV-cache management strategy designed to mitigate memory bottlenecks for LLMs processing long sequences. The core idea is to move beyond storing the KV-cache in its original token order. Instead, IceCache clusters tokens based on the semantic similarity of their key embeddings into a hierarchical structure called a DCI-tree. During inference, it employs a fast Approximate Nearest Neighbor (ANN) search algorithm (M-DCI) on the CPU to perform a query-aware, head-specific selection of the most relevant memory pages. These selected pages are then transferred from CPU to GPU for attention computation. The system is optimized with a pipelined workflow that overlaps the CPU-based page selection with GPU computations to hide latency. Experimental results across several benchmarks and models show that IceCache can maintain high accuracy (over 99% in some cases) with a significantly reduced token budget compared to the full KV-cache and outperforms other baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors compare IceCache against a strong and sufficient set of six recent state-of-the-art KV-cache optimization methods.\n\n- The paper is well-organized and clearly written. The motivation is well-established, the proposed method is described logically with helpful diagrams (e.g., Figure 1 and 2).\n\n- The application of a hierarchical ANN algorithm (M-DCI) to cluster and retrieve from the high-dimensional key-embedding space of the KV-cache is a novel and interesting approach."}, "weaknesses": {"value": "- The paper does not provide an analysis of how different parameter choices of M-DCI affect the final model accuracy and inference latency. Furthermore, the specific parameter values used in the experiments are not clearly described, which could hinder reproducibility.\n\n- The latency analysis in Section 5.5 focuses on \"Time to the second token\" (TT2T) and \"Time per output token\" (TPOT). While informative, it lacks a direct end-to-end runtime comparison against full attention.\n\n- A central claim is that grouping semantically similar tokens into the same memory pages improves efficiency. However, there is no ablation study to isolate and quantify this specific contribution. Since the index selection happens on the CPU, a key question arises: does this memory layout truly enhance efficiency, or is the performance bottleneck dominated by the CPU search and the CPU-GPU data transfer overhead?"}, "questions": {"value": "1.  Could you provide performance curves showing how the inference speed (e.g., tokens per second) scales with increasing context length?\n\n2.  The ANN search is currently performed on the CPU to overlap with GPU computations. Have you considered the feasibility of further accelerating the M-DCI query step by implementing it on the GPU?\n\n3.  The proposed method seems to be tightly integrated with a CPU offloading strategy. Is it possible to adapt IceCache for a non-offload scenario (i.e., where the full KV-cache fits in GPU memory, but sparsity is desired to accelerate computation)? Or does the method fundamentally rely on the CPU having access to the cache for the DCI-tree query to work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lT7jYsZdiw", "forum": "yHxSKM9kdr", "replyto": "yHxSKM9kdr", "signatures": ["ICLR.cc/2026/Conference/Submission541/Reviewer_815Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission541/Reviewer_815Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission541/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761034205497, "cdate": 1761034205497, "tmdate": 1762915543094, "mdate": 1762915543094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "IceCache introduces a new approach to memory-efficient KV-cache management for long-sequence LLMs. It uses a hierarchical DCI-tree that clusters key embeddings based on semantic similarity, grouping related tokens into memory pages. IceCache further employs bulk data loading and CPU-GPU pipelining to minimize latency. Tested on models like LLama-3.1-8B, Mistral-7B, Qwen3-32B, LongChat-7B on various tasks. It maintains over 99% accuracy with a 256 token budget."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. High efficiency and accuracy: while cutting KV memory usage, achieve high accuracy\n2. Fine-grained retrieval: per-head-query-aware selection, improve attention focus\n3. Efficient pipelining: CPU-GPU overlap computation with data movement to reduce latency"}, "weaknesses": {"value": "1. Limited analysis: missing ablations on index depth, page size and computational cost of clustering and updates\n2. Scalability uncertainty: effectiveness and efficiency on extremely long contexts or distributed multi-GPU settings\n3. Figure clarity and presentation issues: figure 1 & 4 are not well-explained or visually clear. lack of consistent notation and labeling."}, "questions": {"value": "1. What is the latency for prefill stage, since you introduce additional clustering and indexing \n2. Did you try on multi-turn conversation where context may change, since I assume the serial workflow in Figure 4 means multi-turn. What is the latency and accuracy.\n3. Figure 4 is lack of explanation and notations, hard to understand. \n4. Did IceCache select page for each decoding step or only once for the whole decoding phase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9pMBzEoAgT", "forum": "yHxSKM9kdr", "replyto": "yHxSKM9kdr", "signatures": ["ICLR.cc/2026/Conference/Submission541/Reviewer_Egcx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission541/Reviewer_Egcx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission541/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761433257466, "cdate": 1761433257466, "tmdate": 1762915542960, "mdate": 1762915542960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a KV-cache management method called IceCache, combining semantic token clustering (using a hierarchical DCI-tree) with the PagedAttention mechanism to optimize long-sequence LLM inference. The core idea is to group semantically related tokens into the same memory pages, enabling more accurate and efficient query-aware page selection during decoding, compared to methods that rely on the original token order. The paper demonstrates better results compared with baselines, maintaining over 99% of full-model accuracy with a significantly reduced KV-cache budget (e.g., 256 tokens) across a wide range of long-context benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Grouping tokens by semantic similarity in key-embedding space improves cache hit rates, which is powerful and clearly explained.\n2. Comprehensive evaluation with diverse benchmarks and multiple models with impressive results.\n3. Superior accuracy-latency trade-off compared to other high-accuracy, retrieval-based methods."}, "weaknesses": {"value": "1. Lack quantitative analysis of the computational cost of building and maintaining the DCI-tree on the CPU. How significant is the CPU utilization? Could this become a bottleneck on a system with a less powerful CPU or when running multiple instances?\n2. The storage cost of the DCI-tree indices themselves is not discussed. For a context length of 100k tokens per layer and per head, what is the memory footprint of the index on the CPU? \n3. Compared to OmniKV, the overhead is primarily from the DCI-query, the more complex data movement, or both? A direct comparison of these components with a leading baseline like OmniKV would be more informative.\n4. The description of M-DCI with P-DCI, but the algorithm in Page 4's pseudocode and Appendix B.1 uses a simpler promotion and parent-assignment scheme, the consistency should be improved.\n5. The entire method hinges on the quality of the key embeddings for clustering. How about the assumption of the clustering not hold?\n6. The promotion ratio r for the DCI-tree and the number of levels L are crucial hyperparameters. How were they chosen? How sensitive are the results to their values?"}, "questions": {"value": "The same as the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dfOJQiNstJ", "forum": "yHxSKM9kdr", "replyto": "yHxSKM9kdr", "signatures": ["ICLR.cc/2026/Conference/Submission541/Reviewer_SWPZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission541/Reviewer_SWPZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission541/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761784677177, "cdate": 1761784677177, "tmdate": 1762915542819, "mdate": 1762915542819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}