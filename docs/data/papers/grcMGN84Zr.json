{"id": "grcMGN84Zr", "number": 13918, "cdate": 1758225086491, "mdate": 1759897403704, "content": {"title": "Latent Debate: A Surrogate Framework for Interpreting LLM Thinking", "abstract": "Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. \nTo this end, we introduce \\emph{latent debate}, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference step. \nWe first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks.\nEmpirical studies demonstrate that latent debate is a faithful surrogate model that has highly consistent predictions with the original LLM.\nFurther analysis reveals strong correlations between hallucinations and debate patterns.\nThese findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.", "tldr": "We introduce Latent Debate, a framework for interpreting model predictions through the lens of implicit internal debates.", "keywords": ["Large Language Models", "Interpretability", "Argumentation"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7259d51530885a91e1e68e01f1c003a80cf9c27e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Latent Debate as an interpretability framework that treats layer-level and token-level hidden states as “arguments”, maps them via an Argument Interpreter to True/False polarity and strength, and uses QBAF “thinking module” to get a final decision."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The overall idea sounds interesting.\n2. The framework is training-free and has no intensive computational burden.\n3. The authors incorporated ablation studies and some hallucination analysis on their approach."}, "weaknesses": {"value": "1. **Evaluation looks toyish; baselines are too weak; no comparison to close related methods**\n\n   * Datasets are toy datasets (500 each) rather than large-scale, realistic NLP benchmarks; the setting feels **too toy** to support their claims.\n   * Baselines are **self-constructed and too simple**. Although the authors discussed DoLa / Logit Lens / Internal Consistency approaches briefly in text, which is good, they did not compare their method to these baselines.\n\n2. **Writing quality is poor; core ideas are hard to follow on first pass**\n   * Some keywords like \"debate\" and \"arguments\" are quite vague to understand and do not have concrete definitions. The paper probably needs a problem statement part up front (“What exactly is the concrete question you want to solve? ”). I even find this important question unclear for me on first pass.\n   * Key design (token-wise weights) is moved to Appendix, which is clearly not a good practice since reviewers and readers are not required to read beyond main text. \n   * Many details are missing. For instance, the crucial similarity component for token weight is not specified (which model? metric?). The appendix only states a generic “cross-encoder similarity model” without naming or detailing it. \n   * There are many typos, e.g., Sec. 3.2.1 uses “trasparent” (should be transparent).\n   * The illustrations are not so good as well. For example, figure 2, presented as the core framework overview, is quite ambiguous for the readers.\n\n3. **Poor reproducibility and missing details**\n\n   * Many key details for reproducibility are unclear, including: no specifics on the similarity backbone or metric (cosine? model name? pooling?); MLP classifier used in Section 5.1 completely lacks details on its architecture and training details.  \n   * To make matters worse, the authors did not provide their code. Combined with the missing details mentioned above, it is really difficult to reproduce the results, while ICLR has a high standard on reproducibility and encourages a reproducibility statement in submissions."}, "questions": {"value": "1.  Argument Interpreter uses a Logit Lens-like approach. However, Logit Lens (and the actual logits computation in models like Llama) typically **applies LayerNorm** (or RMSNorm) after hidden states and before the `W_unemb` projection. This normalization step is critical for maintaining the correct scale and distribution of logits. Could you please explain why you chose to omit this important normalization step? Would this issue affect your framework's interpretability?\n\n2.   All current experiments are about binary True/False tasks with clear answers. How would your framework apply to more complex tasks that lack clear binary outcomes, such as open-ended text generation or general QA?\n\n3.  To better illustrate the working mechanism of the framework, I strongly recommend providing concrete case studies to illustrate how your framework works under concrete scenarios.\n   \n4.  Please improve the presentation quality as the current version is not so easy to follow and many necessary details for reproducibility are missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JHO1Rdsk2O", "forum": "grcMGN84Zr", "replyto": "grcMGN84Zr", "signatures": ["ICLR.cc/2026/Conference/Submission13918/Reviewer_1f9Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13918/Reviewer_1f9Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761671111232, "cdate": 1761671111232, "tmdate": 1762924424613, "mdate": 1762924424613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a surrogate framework for interpreting models through argumentation frameworks. The authors instantiate it using hidden states and the unembedding matrix. The results show that (1) using QBAF as the reasoning module explains model predictions better than simple feature-based methods, and (2) MLPs trained on features extracted from debate patterns predict hallucinations more accurately than using individual features.\n\nAlthough I am not familiar with argumentation frameworks, I find the surrogate modeling approach conceptually interesting. The experiments demonstrate the potential of explaining LLM behaviors through the latent debate perspective. However, the writing could be clearer, and I have several questions about the methodology and experiments, which I detail below."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The conceptual framework is intuitive and well-motivated by argumentation theory.\n- While the logit lens technique used in instantiation is not novel, the experiments provide solid support for the argumentation-based approach."}, "weaknesses": {"value": "- For the instantiation, it is unclear why all tokens are treated as thinking steps (Line 288). In LLMs, only the final token attends to all previous ones. If earlier token representations lack full contextual information, it is questionable how they can form meaningful arguments about sentence correctness.\n- The writing lacks clarity. More descriptive figure captions would improve readability, rather than relying solely on definitions in the main text. The authors should also clearly define key terms (e.g., “hallucination”) in the context of the tasks.\n- More experimental details are needed for reproducibility, such as the specific versions of the models used.\n- The paper focuses mainly on knowledge-based toy tasks (e.g., fact judgment). It would be more valuable to examine whether reasoning questions can be explained under the same framework, which would make the work more convincing and significant."}, "questions": {"value": "- Please specify the model versions and provide more experimental details for better reproducibility.\n- Could you explain the rationale for defining each token as a thinking step? (see the first weakness)\n- What proportion of the questions produce hallucinations (are hallucinations incorrect answers in this task)? Is there an obvious imbalance between hallucinated and non-hallucinated samples when training the MLP?\n- Could you try to demonstrate the effectiveness of the proposed approach on reasoning tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OAUSKjQEGU", "forum": "grcMGN84Zr", "replyto": "grcMGN84Zr", "signatures": ["ICLR.cc/2026/Conference/Submission13918/Reviewer_MzgQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13918/Reviewer_MzgQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683024265, "cdate": 1761683024265, "tmdate": 1762924424016, "mdate": 1762924424016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper hypothesizes that models, 'internally debate' the truth of things.\nA framework is proposed where model activations are taken to act as arguments for or against the matter at hand or other arguments.\n\nThe framework is instantiated by using 'logit lens'-style probing in order to quantify to what degree each activation points into the 'true' or 'false' direction. Activations toward 'true' are considered arguments in favor and 'false' are against, with the degree of alignment acting as argument strength. From there it is hypothesized that 'arguments' on each token position argue for or against the 'argument' in the next token position, and that for the last token, the 'argument' argues for or against the 'argument' in the next layer. This structure is formalized as a graph, and a semantics is proposed that allows for propagating argument strengths through this graph to arrive at a final number that is hypothesized to predict the model's final prediction (between true and false).\n\nFinally, features of the graph are used to predict hallucinations."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written, results are presented clearly, and it has helpful diagrams.\n\nThe fact that the amount of disagreement between true/false directionality within a model is predictive of hallucination is a interesting finding."}, "weaknesses": {"value": "The results require better baselines: \n(1) using only the rightmost hidden state from layer L-1, since that is the 'closest' to where the actual prediction happens. \n(2) compute the consistency score for all arguments, and present the max.\nWithout that, I don't think the benefit is properly established: to be beneficial, the outcome of the QBAF procedure should be more predictive than the variables that go into it. \n\nThe same goes for the hallucination detection, where the results are not compared to any existing methods.\n\nThe language of debate/argument is not clearly established as metaphorical."}, "questions": {"value": "No question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fFPZvpoRfO", "forum": "grcMGN84Zr", "replyto": "grcMGN84Zr", "signatures": ["ICLR.cc/2026/Conference/Submission13918/Reviewer_HgTy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13918/Reviewer_HgTy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864073330, "cdate": 1761864073330, "tmdate": 1762924423486, "mdate": 1762924423486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}