{"id": "0OUkySEAf0", "number": 17319, "cdate": 1758274664761, "mdate": 1759897182595, "content": {"title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning", "abstract": "Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs’ strength derived from informal, natural language knowledge acquired during pre-training. In this work, we introduce DeepTheorem, a comprehensive informal theorem-proving suite exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes 1) a large-scale dataset of 121K\nhigh-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants; 2) adaptation of RL-Zero explicitly to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference; 3) comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps; and 4) a new informal theorem proving benchmark consoliadted from three established math competitions, formatted for automatic evaluation. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem’s potential to fundamentally advance automated informal theorem proving and mathematical exploration.", "tldr": "We introduce DeepTheorem, the first comprehensive suite for advancing LLMs' informal theorem proving, including a large-scale dataset, an adaptation of RL-Zero training method, a benchmark, and comprehensive evaluation metrics.", "keywords": ["natural language theorem proving", "large language models", "reasoning", "reinforcement learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d6b6c263e8babdefb1daff4ccd3ee4d933bb4bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces DeepTheorem, a suite for informal (natural‑language/LaTeX) theorem proving consisting of a 121K-sample dataset of IMO‑level natural‑language theorems with concise proof sketches, topic labels, and a family of truth‑preserving or contradictory variants per theorem to enable automatic checking; 2) an adaptation of RL‑Zero/GRPO to informal theorem proving that uses pass/fail signals derived from those variants as rewards; and 3) an evaluation framework with both outcome accuracy (truth classification across variants) and process scoring of proofs by LLM judges, with a small human study for calibration"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Data Scale:  The 121K IMO‑level, topic‑labeled problems with curated entailed/contradictory variants are materially larger than prior NL theorem/proof corpora (e.g., NaturalProofs/NaturalProver focused on smaller‑scale NL proofs). The variant family provides a neat mechanism for binary checking without a formal verifier, and Figure 4 (p.4) shows difficulty calibration that overlaps harder benchmarks (miniF2F, FIMO, Putnam, HMMT).\n\n2. Empirical gains: Table 3 (p.7) and Figure 6 (p.8) indicate consistent improvements of RL‑Zero over SFT across Qwen2.5 7B/72B models on the proposed outcome‑metric and process‑scores."}, "weaknesses": {"value": "1. The dataset is largely curated via web scraping existing problem sets and multiple LLM passes, with the final “step-by-step proofs” produced by black-box commercial models o3-mini. The RL part applies the naive 1/0 correctness reward extracted; and the process evaluation itself is performed by a black-box LLM judge (GPT-4o, with o3-mini as an alternative) rather than formal proof assistants such as lean. This pipeline raises concerns about originality (vs. prior RL-for-reasoning recipes), circularity, and reproducibility.\n\n2. Out-of-date baselines. The core experiments fine-tune Qwen2.5 model families and compare against a mix of 2024 models; however, several contemporaneous stronger families (e.g., Qwen3 dense / MoE series, GPT-5 ) and post-2024 formal provers (e.g., Seed-Prover variants) have significantly improved the frontier. Using pre-Qwen3 models limits the timeliness of the claims and their competitiveness at submission time.\n\n3. Unfair comparisons with formal theorem proving baselines. The test sets on table 4 from FIMO and PutnamBench are formal theorem-proving benchmarks but are manually converted to informal, variant-based NL evaluation; results are then compared against models for formal-theorem proving (such as DeepSeek-Prover) with formal accuracy (much more difficult than informal accuracies)Comparing an informal truth-assignment outcome to formal proof success under a verifier is not methodologically fair even if both are reported on the same underlying source problems.\n\n4. Decontamination is not documented w.r.t. time via temporal split. The paper details an embedding-based recall-and-justify decontamination and shows removed examples, but it does not report any timestamp/knowledge-cutoff filtering (e.g., train < test release date), nor quantitative false-positive/false-negative audits. Without a temporal split, leakage from recent competitions and public solutions remains a risk.\n\n5. Tiny human eval: the process scores depend on black-box GPT-4o/o3-mini judgments. While the paper reports high correlation between judges, LLM-based grading cannot ensure semantic correctness the way a formal checker can, and the human study covers only 10 items with two raters, which is far too small to validate reliability."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aAzT9wxEWQ", "forum": "0OUkySEAf0", "replyto": "0OUkySEAf0", "signatures": ["ICLR.cc/2026/Conference/Submission17319/Reviewer_Ua9x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17319/Reviewer_Ua9x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761418740076, "cdate": 1761418740076, "tmdate": 1762927248286, "mdate": 1762927248286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DeepTheorem, a large-scale dataset designed for informal theorem proving. The dataset is richly annotated with information such as difficulty levels, proof labels, topic categories, problem variants, and exampled proofs. Building on this resource, the authors adapt RL-Zero to train large language models on DeepTheorem and propose both outcome-based and process-based evaluation metrics, using LLMs as judges. Experimental results demonstrate that models trained on DeepTheorem achieve stronger performance on external theorem-proving benchmarks compared to baselines of the same model scale."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is overall well-written and easy to follow, with an interesting focus on informal theorem proving.  \n* The proposed dataset is large-scale and carefully annotated with detailed information such as problem variants and difficulty levels.  \n* The LLM trained on this dataset achieves better performance than other baselines of the same model scale."}, "weaknesses": {"value": "I think the major weakness of this paper lies in the fact that the entire dataset construction and evaluation pipeline is based solely on LLMs.\n\n* First, the original problem statements are drawn from existing datasets, which may have been included in the pretraining data of some LLMs. This raises concerns about potential contamination and data leakage.\n\n* Second, the generation of variants, the labeling of True or False, the construction of proofs, and even the assignment of difficulty levels and topics all depend heavily on existing LLMs—particularly o3-mini. In this sense, the dataset can be seen as a form of distillation of o3-mini onto a new dataset, rather than a fully independent benchmark.\n\n* Third, there is no human evaluation of the LLM annotations—not even on a sampled subset—to assess their quality. This absence raises serious concerns about the correctness of the proofs, labels, and overall dataset integrity.\n\n* Moreover, the evaluation relies on GPT-4o and o3-mini as LLM-as-a-judge models, with no manual inspection, even for a small portion of the data. Prior work has shown that such models may produce biased or inflated scores toward certain stylistic outputs (e.g., their own) [1]. Therefore, using a single LLM for evaluation, without any human verification, makes the results less reliable and less sound.\n\n* Finally, the baseline dataset appears to contain much less data, making the comparison seem somewhat unfair and potentially misleading.\n\nMinor: The question shown in Figure 2 appears incomplete, which further raises concerns about the dataset’s quality and attention to detail.\n\n[1] Gu, Jiawei, et al. “A Survey on LLM-as-a-Judge.” arXiv preprint arXiv:2411.15594 (2024)."}, "questions": {"value": "Could you manually evaluate a subset of the dataset to assess the accuracy and reliability of both the LLM-generated annotations and the LLM-as-a-judge evaluations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eSlZXIYVb9", "forum": "0OUkySEAf0", "replyto": "0OUkySEAf0", "signatures": ["ICLR.cc/2026/Conference/Submission17319/Reviewer_zq5f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17319/Reviewer_zq5f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761427252722, "cdate": 1761427252722, "tmdate": 1762927247956, "mdate": 1762927247956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DeepTheorem, a comprehensive framework for natural language theorem proving comprising four key components: 1) a large-scale dataset of 121K IMO-level informal theorems with high-quality proofs; 2) a novel adaptation of RL-Zero training methodology designed explicitly for informal theorem proving; 3) comprehensive outcome and process evaluation metrics for assessing proof quality; and 4) a newly constructed informal theorem proving benchmark with manually annotated theorem variants. This paper conducts experiments using both SFT and RL-Zero training methods on Qwen-2.5-Base across three sizes. Experiment results indicate that the model trained on DeepTheorem achieves leading performance compared to the baseline dataset. It also shows that the trained model obtains better results than other open-source models of similar size."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel idea of verifying NL proofs using LLM:** The paper proposes a logically sound verification method to evaluate natural language theorem proving by following its 4-step criteria. The idea to prove/disprove the variants of the problem and combine all the results together to justify the correctness of the theorem is novel to the field.\n2. **Leading benchmark results on open-source models:** The DeepTheorem dataset trained model achieves leading performance in open-source models and can even compete with large closed-source models. The experiment also thoroughly evaluates the difference between SFT and RL performance on different model sizes across different datasets to prove the effectiveness of DeepTheorem.\n3. **Useful benchmark and dataset proposed:** The benchmark and dataset proposed in this paper are valuable for training more effective models. The scale of the dataset and soundness of the benchmark are all valuable contributions to the field."}, "weaknesses": {"value": "Despite the above strengths, I have a few concerns regarding this paper:\n\n1. **Unclear motivation and positioning:** This paper’s narrative is confusing regarding its claim of formal theorem proving. While this work focuses on informal theorem proving, the major comparison datasets shown in Figure 1 are DS-Prover-V1 and Lean-WB, both of which are based on Lean4 formal proving rather than informal reasoning. Similarly, the experiment section presents results from DS-Prover, which is not designed for informal reasoning. Besides, this paper also compares with Lean4 reasoning in many parts of the paper; however, the scope of the paper still compares the final outcome results, which differ from real formal theorem proving, where every step is verifiable. This creates confusion about the paper's positioning and contributions in relation to existing work on formal versus informal theorem proving.\n2. **Insufficient experimental analysis:** The experimental evaluation lacks width and depth in the following ways:\n    1. No ablation studies are provided to validate the effectiveness of different components of the proposed dataset (e.g., how training data at different difficulty levels affects model performance, what is the scaling-law effect for the dataset).\n    2. The case study presents only examples without systematic analysis or insights.\n    3. Critical training details are missing, including the exact number of GPU hours and computational costs.\n    4. Given that RL-Zero is known for generating significantly longer responses, the paper should report generation length statistics, but this information is absent.\n3. **Limited evaluation of generalizability:** The paper only evaluates DeepTheorem’s result on Qwen-2.5-Base across different sizes. It does not evaluate on additional model families such as Qwen-2.5-Math, Llama-4, and Qwen-3. It would be necessary to demonstrate the generalizability and robustness of the proposed dataset.\n4. **Presentation quality issues:** Several figures suffer from poor quality or clarity problems. Specifically, Figures 3 and 7 contain blurred text and incomplete labels in the pie charts, making them difficult to interpret.\n\nWith the above weakness, I can only provide a relatively negative overall opinion of the paper. However, I am willing to increase the score if the raised concerns are properly settled."}, "questions": {"value": "Here are the questions for the authors:\n\n1. What is the connection between the proposed dataset and Lean-WB and DS-Prover-V1’s datasets? Is it necessary to compare them?\n2. What is the ablation study of each component of the proposed method? Are there more case studies and analyses? What is the GPU cost for Zero-RL and SFT training for the models? And what is the generated content length as the training goes on?\n3. How is the effect of DeepTheorem’s performance on SFT/RL on other leading open-source models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uF77jzs8RB", "forum": "0OUkySEAf0", "replyto": "0OUkySEAf0", "signatures": ["ICLR.cc/2026/Conference/Submission17319/Reviewer_xMBg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17319/Reviewer_xMBg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767675696, "cdate": 1761767675696, "tmdate": 1762927247563, "mdate": 1762927247563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DeepTheorem, a framework for informal theorem proving with large language models that combines a 121K-sample natural-language theorem dataset and reinforcement learning. By adapting RL-Zero to theorem proving with verifiable true/false rewards, the approach enhances reasoning beyond supervised fine-tuning. Evaluations on FIMO, Putnam, and HMMT benchmarks show that DeepTheorem-trained models outperform larger state-of-the-art systems in both proof accuracy and reasoning quality, highlighting the potential of natural-language-based reinforcement learning to advance LLM mathematical reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensive experiments across multiple representative model scales.\n- Introduction of a large, well-structured dataset for informal theorem proving.\n- Innovative adaptation of RL-Zero to the informal theorem-proving setting."}, "weaknesses": {"value": "- Many quality-control steps rely heavily on LLM judgments (e.g., contamination justification and difficulty annotation). A human evaluation of a sampled subset would strengthen confidence in the dataset’s integrity and annotation accuracy.\n- The process evaluation methodology remains somewhat unclear (even with the prompt shown in the appendix). Compared to formal proof verification (e.g., in Lean), LLM-based evaluation is inherently less reliable. A more detailed comparison against prior process-evaluation models, including an analysis of reliability and typical failure modes, would be valuable.\n\nMinor:\n- Figure 2 (left): the formula before ‘is given by’ is not quite right, I guess something like ‘\\pi / 2’ is missing."}, "questions": {"value": "- How does the dataset compare to Omni-Math?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EnFjOp6VqS", "forum": "0OUkySEAf0", "replyto": "0OUkySEAf0", "signatures": ["ICLR.cc/2026/Conference/Submission17319/Reviewer_EGQ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17319/Reviewer_EGQ7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762625532370, "cdate": 1762625532370, "tmdate": 1762927247240, "mdate": 1762927247240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}