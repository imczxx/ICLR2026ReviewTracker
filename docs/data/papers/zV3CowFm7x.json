{"id": "zV3CowFm7x", "number": 14741, "cdate": 1758242838934, "mdate": 1759897351847, "content": {"title": "DyGB: Dynamic Gradient Boosting Decision Trees with In-Place Updates for Efficient Data Addition and Deletion", "abstract": "Gradient Boosting Decision Tree (GBDT) is one of the most popular machine learning algorithm in various applications. However, in the traditional settings, all data should be simultaneously accessed in the training procedure: it does not allow to add or delete any data instances after training. In this paper, we propose DyGB (Dynamic GBDT), a novel framework that enables efficient support for both incremental and decremental learning within GBDT. To reduce the learning cost, we present a collection of optimizations for DyGB, so that it can add or delete a small fraction of data on the fly. We theoretically show the relationship between the hyper-parameters of the proposed optimizations, which enables trading off accuracy and cost on incremental and decremental learning. Empirical results on backdoor and membership inference attacks demonstrate that DyGB can effectively add and remove data from a well-trained model through incremental and decremental learning. Furthermore, experiments on public datasets validate the effectiveness and efficiency of the proposed DyGB framework and optimizations.", "tldr": "We propose DyGB (Dynamic GBDT), a novel framework that enables efficient support for both incremental and decremental learning within GBDT.", "keywords": ["Dynamic Learning", "Decremental Learning", "Incremental Learning", "Machine Unlearning", "Online Learning", "Gradient Boosting Decision Trees"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48217f49da6dcae5c0f59a174098292cbf0328a4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DyGB (Dynamic GBDT), a framework for efficient, in-place incremental (data addition) and decremental (data deletion) learning in GBDTs. Instead of retraining, DyGB traverses existing trees and checks if node splits are still optimal using statistics from the data delta ($D'$). If a split is suboptimal, it retrains only the subtree below that node. The framework's speed relies on several optimizations: (1) using stored statistics to avoid accessing the original dataset, (2) lazy derivative updates, (3) split candidate sampling ($\\alpha$), and (4) a robustness tolerance ($\\sigma$) to ignore minor, low-impact split changes. Extensive experiments show this approach is significantly faster than retraining while maintaining comparable accuracy, which is validated using backdoor attack and removal simulations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper addresses a critical and highly practical limitation of GBDTs. The static, batch-only nature of GBDT training is a major bottleneck in real-world systems that require models to adapt to new data or forget old data (e.g., for privacy compliance or addressing data drift).\n\nS2. The evaluation of the incremental and decremental quality is comprehensive. This includes functional validation via attack simulations (backdoor and membership inference), direct accuracy comparisons against retrained models, and functional similarity comparisons (Appendix K). Figure 3 is particularly impressive, showing that the incremental and decremental learning framework tracks the accuracy of a fully retrained model even when adding or removing large ratios of the dataset.\n\nS3. The proposed framework is unified and efficient. It is the first to support both incremental and decremental learning within a single, in-place mechanism for GBDTs."}, "weaknesses": {"value": "W1. The paper's method for feature discretization (Algorithm 4), a critical component for any GBDT implementation, is relegated to the appendix (Appendix C). This method is presented without sufficient experimental validation or theoretical proof of its superiority over other common techniques. A robust dynamic GBDT framework heavily depends on the stability of its feature histograms. The paper fails to compare its binning strategy against alternatives, such as naive sample-based discretization or the \"balanced robust histogram\" method used in related work like DeltaBoost. Such a comparison would be necessary to demonstrate the proposed method's balance and robustness to histogram shifts caused by data addition and deletion.\n\nW2. The paper presents the observation in Section 3.4 that \"For adding or deleting a single data point, the best split does not change in most cases\" as a novel finding motivating its optimizations. However, this is a known phenomenon that was theoretically proven in prior work (e.g., Theorem 3.1 in the DeltaBoost paper). The paper should explicitly acknowledge this and clarify what, if any, new insights its empirical findings add beyond what is already established.\n\nW3. The impact of the \"Adaptive Lazy Update for Derivatives\" (Sec 3.2) is not fully clear in the main text. The paper states that derivatives are updated \"only when retraining occurs,\" which implies that the decision to check a split (gain computation) might be made using stale derivatives. This is a key approximation. While Appendix Q touches on the resulting error, the main paper would be stronger if it discussed the impact of this specific approximation on the decision-making of the algorithm (i.e., does using stale derivatives cause the model to miss a necessary retrain, or retrain an unnecessary one?).\n\n**Minor Comments**\n\nC1. The text and values within Figure 1 are very small and low-resolution, which makes it difficult to follow the concrete example presented in Section 2.3.\n\nC2. The font size used in many of the tables (e.g., Table 2, 15, 16) is very small, making them difficult to read."}, "questions": {"value": "Q1. Does the incremental learning or decremental learning require access to the original full training set? This should be clarified in the problem statement.\n\nQ2. The experimental comparisons raise several questions about fairness and reliability.\n* First, why does DyGB consistently outperform highly optimized libraries like XGBoost in both full training time (Table 3) and accuracy (Figure 3)? Since DyGB is a prototype built on Robust LogitBoost, what is the nature of this significant improvement? Is it due to the algorithm itself, the C++ implementation, or the specific hyper-parameter choices used for all models?\n* Second, there appear to be discrepancies with prior work. For instance, the decremental learning time reported for DeltaBoost on the Covtype dataset seems significantly slower (e.g., ~20x) than the times reported in the original DeltaBoost paper. Can the authors explain this difference in experimental outcomes?\n* Finally, the efficiency evaluations (e.g., Table 2) do not report variance or standard deviations. Given the potential for high variance in dynamic updates, this makes it difficult to assess the reliability and consistency of the speedup claims. Were multiple runs conducted, and if so, what was the variance?\n\nQ3. What is the ratio of the changed nodes in each dataset? The appendix (e.g., Figure 11) presents the absolute number of retrained nodes, but not the ratio relative to the total number of non-terminal nodes in the ensemble. This ratio is the most important factor in evaluating whether the claimed efficiency improvement is reasonable. Can the authors provide these ratios for the experiments, as this metric directly connects the amount of avoided work (nodes not retrained) to the efficiency gains of incremental/decremental learning when compared to a full retraining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "joGYih2ljS", "forum": "zV3CowFm7x", "replyto": "zV3CowFm7x", "signatures": ["ICLR.cc/2026/Conference/Submission14741/Reviewer_CKeZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14741/Reviewer_CKeZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743597688, "cdate": 1761743597688, "tmdate": 1762925099787, "mdate": 1762925099787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DyGB, a new framework for dynamic gradient boosting decision trees (GBDT) that supports both incremental (adding new data) and decremental (removing data) learning in-place, without retraining the entire model. The authors present theoretical analyses on trade-offs between accuracy and computational cost, introduce several optimizations (e.g., adaptive lazy updates, split candidate sampling, robustness tolerance), and conduct extensive experiments comparing DyGB against strong baselines such as XGBoost, LightGBM, CatBoost, ThunderGBM, DeltaBoost, and MUinGBDT. Results show substantial speedups (up to 1200× in some settings) while maintaining accuracy close to retrained models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tTimely and important topic. Dynamic or “unlearning” capabilities are becoming crucial for privacy, continual learning, and compliance (e.g., GDPR). Extending GBDT to support both incremental and decremental updates efficiently is a valuable direction.\n2.\tComprehensive experiments. The paper evaluates DyGB across 10 datasets, with comparisons to several strong baselines and both incremental and decremental scenarios. The backdoor and membership inference attack studies are a creative way to validate unlearning ability.\n3.\tAlgorithmic innovations. The paper introduces meaningful technical contributions, such as adaptive lazy updates and split robustness tolerance, that help reduce computational costs without major accuracy loss.\n4.\tClarity of motivation. The introduction effectively argues why dynamic GBDT is relevant and distinct from existing online boosting or incremental tree methods."}, "weaknesses": {"value": "1.\tLimited novelty in core mechanism. While DyGB integrates several optimization strategies, the overall structure (updating splits, retraining subtrees, and incremental statistics updates) follows existing frameworks like MUinGBDT (Lin et al., 2023). The paper’s novelty seems incremental rather than groundbreaking.\n2.\tTheoretical analysis is shallow. The “robustness” definitions and proofs are mostly intuitive and not deeply rigorous. Theoretical guarantees (e.g., on convergence or bounds on model drift) are missing.\n3.\tExperimental reporting could be clearer. The results focus heavily on runtime speedups but offer limited insight into accuracy degradation, especially under frequent dynamic updates. A figure or table quantifying trade-offs between time and accuracy loss would strengthen the claims.\n4.\tPresentation issues. The paper is dense and sometimes difficult to follow, particularly in the algorithmic sections (Algorithms 2–3). Some pseudo-code lacks clarity in notation (e.g., subscripts and primes are inconsistently defined).\n5.\tMissing ablation details. Although Appendix S is mentioned for ablation studies, the main paper does not summarize which components (e.g., adaptive updates, split sampling) contribute most to efficiency."}, "questions": {"value": "1.\tHow does DyGB handle concept drift or non-stationary data distributions over time?\n2.\tCan DyGB support weighted unlearning or partial forgetting?\n3.\tWhat is the impact of hyperparameters α and σ on model stability—are there guidelines for tuning them automatically?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4L6gA9PXnR", "forum": "zV3CowFm7x", "replyto": "zV3CowFm7x", "signatures": ["ICLR.cc/2026/Conference/Submission14741/Reviewer_JhS1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14741/Reviewer_JhS1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808605818, "cdate": 1761808605818, "tmdate": 1762925098898, "mdate": 1762925098898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DyGB, a framework that performs in-place updates to trained GBDT models to support both incremental (adding data) and decremental (removing data) learning while keeping the model size (number of trees/parameters) unchanged. The key idea is to avoid touching the original training set by caching per-split statistics from training and then recomputing gains using only the delta dataset D’; affected nodes are selectively retrained and only those samples’ derivatives/residuals are updated for subsequent trees."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Clear problem and practical impact. The paper targets in-place updates that support both data addition and deletion—addressing real needs such as privacy-compliant unlearning and continual learning—while keeping the ensemble size fixed.\n\n- Simple, effective mechanism. By caching per-split gradient/Hessian statistics and applying localized residual/derivative updates, the method enables efficient in-place updates without rescanning the full training set.\n\n- Strong empirical evidence. Extensive experiments across multiple public datasets consistently demonstrate efficiency of the proposed approach.\n\n- Reproducibility. The authors provide an anonymized implementation and scripts, facilitating independent verification and reuse."}, "weaknesses": {"value": "- Introduction needs tightening. The current introduction spends too many paragraphs on broad background and related work (which are covered again later).\n\n- Clarify the “histogram” connection. Caching per-split gradient/Hessian statistics is effectively histogram binning à la LightGBM. I think the author should foreground this explicitly (use “histogram-based” terminology).\n\n- Approximation from lazy derivatives. Using outdated derivatives until a subtree retrains introduces approximation error that can grow with larger D’.\n\n- Accuracy parity missing. The experiments emphasize speed and memory but omit a head-to-head accuracy comparison against from-scratch retraining in XGBoost/LightGBM. \n\n- Clarify “robust” (Defs. 1 & 2). Why are Definitions 1 and 2 termed robust?"}, "questions": {"value": "- Full (global) derivative updates are costly; relying on lazy local derivatives incurs approximation error. Is there a systematic strategy to balance accuracy and cost?\n\n- Is there a parallel version (multi-node) for dyGB?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NVdbEshgsw", "forum": "zV3CowFm7x", "replyto": "zV3CowFm7x", "signatures": ["ICLR.cc/2026/Conference/Submission14741/Reviewer_pdB2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14741/Reviewer_pdB2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762329646781, "cdate": 1762329646781, "tmdate": 1762925098485, "mdate": 1762925098485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}