{"id": "WEuJlEJmX8", "number": 20267, "cdate": 1758304251524, "mdate": 1759896987286, "content": {"title": "CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual Comprehension and Symbolic Reasoning in Engineering Design Process", "abstract": "Engineering design operates through hierarchical abstraction from system specifications to component implementations, requiring visual understanding coupled with mathematical reasoning at each level. While Multi-modal Large Language Models (MLLMs) excel at natural image tasks, their ability to extract mathematical models from technical diagrams remains unexplored. We present \\textbf{CircuitSense}, a comprehensive benchmark evaluating circuit understanding across this hierarchy through 8,006+ problems spanning component-level schematics to system-level block diagrams. Our benchmark uniquely examines the complete engineering workflow: Perception, Analysis, and Design, with a particular emphasis on the critical but underexplored capability of deriving symbolic equations from visual inputs. We introduce a hierarchical synthetic generation pipeline consisting of a grid-based schematic generator and a block diagram generator with auto-derived symbolic equation labels. Comprehensive evaluation of six state-of-the-art MLLMs, including both closed-source and open-source models, reveals fundamental limitations in visual-to-mathematical reasoning. Closed-source models achieve over 85\\% accuracy on perception tasks involving component recognition and topology identification, yet their performance on symbolic derivation and analytical reasoning falls below 19\\%, exposing a critical gap between visual parsing and symbolic reasoning. Models with stronger symbolic reasoning capabilities consistently achieve higher design task accuracy, confirming the fundamental role of mathematical understanding in circuit synthesis and establishing symbolic reasoning as the key metric for engineering competence. Our synthetic pipeline code is available at \\href{https://anonymous.4open.science/r/CircuitSense-8AC7/README.md}{URL}", "tldr": "", "keywords": ["Mathematical Reasoning Benchmark", "Circuit Benchmark", "Symbolic Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3290c6b32cd2d7a6e840dfb368481ca0a4ffc15.pdf", "supplementary_material": "/attachment/4eedef566692d4d3efe7c88cf82c4df0229d3350.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces CircuitSense, a benchmark designed to evaluate the circuit-understanding capabilities of multimodal large language models (MLLMs). CircuitSense comprises over 8,000 problems organized into a six-level hierarchical structure. The paper also presents an evaluation of six state-of-the-art MLLMs, revealing that while these models demonstrate strong visual perception, they perform catastrophically poorly in the symbolic reasoning tasks essential for circuit analysis and design."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n\n2. Addressing multimodal engineering problems is an important and timely topic for current MLLMs.\n\n3. The proposed hierarchical synthetic generation pipeline is interesting."}, "weaknesses": {"value": "1. Lack of discussion on related work (EEE-Bench [1]).\nEEE-Bench is a comprehensive multimodal benchmark for electrical and electronics engineering, which is highly relevant to this paper. The authors fail to acknowledge or compare their work against it.\n\n2. Overstated claims and limited novelty.\nThe paper claims to introduce the first multi-level visual-to-analytical benchmark. However, EEE-Bench has already covered comprehensive circuit analysis problems. The authors do not clearly articulate the differences between CircuitSense and EEE-Bench. In my view, the main tasks (circuit-related problems) in this paper largely overlap with those in EEE-Bench, suggesting limited novelty. Moreover, EEE-Bench addresses a broader domain of electrical and electronics engineering.\n\n3. Insufficient evaluation.\nThe paper only evaluates six large scale models, which is not enough to provide a comprehensive assessment. More open-source small models and recent proprietary models—such as o3 and o4-mini—should be included.\n\n4. Lack of new insights in the experimental results.\nBeyond the introduction of the benchmark itself, the experiments do not provide any particularly novel findings. The observation that MLLMs excel in visual perception but struggle with symbolic reasoning is already well known.\n\n5. Missing implementation details.\nThe paper does not provide sufficient information about the evaluation setup, such as the prompts used for answer extraction or accuracy calculation or other experimental configurations.\n\n6. Inconsistent complexity-level design.\nThe benchmark defines multiple complexity levels, but the experimental results do not reflect the expected performance degradation with increasing difficulty. If model performance does not decrease with complexity, it raises questions about the validity and meaning of the defined complexity levels.\n\nReference\n\n[1] EEE-Bench: A Comprehensive Multimodal Electrical and Electronics Engineering Benchmark. CVPR 2025."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BThCu5tyRD", "forum": "WEuJlEJmX8", "replyto": "WEuJlEJmX8", "signatures": ["ICLR.cc/2026/Conference/Submission20267/Reviewer_Pqrx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20267/Reviewer_Pqrx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760480313836, "cdate": 1760480313836, "tmdate": 1762933747600, "mdate": 1762933747600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CircuitSense, a comprehensive hierarchical benchmark for assessing visual-to-symbolic reasoning in circuit understanding.  It focuses on the core engineering ability to translate schematic and system diagrams into mathematical equations, which has not been tested in previous multimodal benchmarks.\nCircuitSense has 8,006 problems organized into six hierarchy levels (resistor, RLC, small signal, transistor, block, and system) and three task categories (perception, analysis, and design).  The authors propose a synthetic generation pipeline based on SPICE simulation and Mason's gain formula for block diagrams to ensure ground-truth symbolic equations.\nExperiments on six cutting-edge MLLMs (GPT-4o, Gemini-2.5-Pro, Claude-Sonnet-4, InternVL3, Qwen2.5-VL, GLM-4.5V) show that while closed-source models achieve >85% accuracy on perception tasks, their performance on symbolic derivation remains <19%, indicating a severe gap between visual parsing and mathematical reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Novel Benchmark Scope  \nCircuitSense combines perception, analysis, and design in a single dataset that reflects real-world circuit design workflows. The six-level hierarchy captures domain progression from physical components to system architecture, and provides a uniquely structured view of how reasoning degrades with abstraction.\n\n-Rigorous Synthetic Generation Pipeline\nThe dual-stage pipeline (schematic and block diagram) ensures electrical validity and symbolic correctness. The use of SPICE simulation, Lcapy-based symbolic derivation, and Mason's gain formula ensures physics-consistent ground truths, not simply textual labels.\n\n- Comprehensive Evaluation and Diagnostics\nTesting six leading MLLMs with curated and synthetic problems (multiple-choice -- open-ended -- symbolic derivation) reveals how pattern-matching fails when symbolic reasoning is needed.\n\n- Fine-grained Task Taxonomy \nThe benchmark explicitly distinguishes perception subtasks (component detection, connection identification, function classification) from analytical derivation, allowing for the identification of the true bottleneck: symbolic manipulation.\n\n- Strong Domain Validity \nQuestions are sourced from reputable textbooks (Gray, Razavi, Holberg) and university courses to ensure educational and professional relevance."}, "weaknesses": {"value": "- Limited Novelty in Technical Methodology \nAlthough the domain is novel, the benchmark creation adheres to standard dataset synthesis paradigms (grid-based placement, random topology, template verification).  The originality is primarily in hierarchical symbolic labeling, not algorithmic innovation. Limited Novelty in \n\n-Imbalanced Data Composition \nThe dataset heavily favors analysis (approximately 7,000 samples) over design (~150 samples), making it difficult to statistically interpret design-task outcomes.\n\n- Synthetic Domain Gap \nThe significant drop in synthetic performance could be due to distribution shift rather than pure reasoning failure. The paper could better quantify the visual and algebraic difficulty of synthetic problems to address potential domain difference of the synthetic data.\n\n-Limited Evaluation Diversity \nAll tested models are general MLLMs, with no domain-specific baselines (such as SPICE-aware symbolic solvers or retrieval-augmented MLLMs) included.  This absense makes it unclear whether the failures are the result of poor general reasoning or a lack of domain knowledge."}, "questions": {"value": "- Annotation Quality and Validation\nHow were the 2,986 curated problems verified?  Were multiple domain experts involved, and how did the annotators agree? Were symbolic derivations manually re-checked after SPICE/Lcapy generation to avoid mismatch errors?\n\n- Synthetic Circuit Generator\nHow does the generator handle non-linear or transistor-level behavior that symbolic solvers cannot detect? Could the authors explain how \"adaptive timeouts\" were used to balance completeness and feasibility in symbolic derivation?\n\n- Data Balance and Hierarchical Sampling\nThe design subset (157 samples) is relatively small.  Do the authors intend to scale this portion using synthetic design tasks (for example, automated topology synthesis with constraints)? Are the counts roughly equal across hierarchy levels, or are some levels (such as transistor or system) underrepresented?\n\n- Table 7 shows a significant drop in output impedance derivation (8%).  Could you provide qualitative examples that demonstrate the specific algebraic or topological misunderstanding? Do models consistently misinterpret node labeling conventions (e.g., sign or orientation errors) or fail algebraic simplification?\n\n- Robustness in Equation Comparisons\nThe symbolic comparison pipeline (SymPy plus numerical validation) is great. What is the failure rate? Were there instances where algebraically correct but simplified forms (e.g., factorized expressions) were mistakenly classified as incorrect?\n\n- Potential Extensions: Do the authors intend to include temporal behaviors, such as time-domain simulations or frequency-sweep interpretation? Could integration with RAG/agent-based method serve as future baselines?\n\nAppreciate the author's work and contributions to the community!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k3ftDdoDP3", "forum": "WEuJlEJmX8", "replyto": "WEuJlEJmX8", "signatures": ["ICLR.cc/2026/Conference/Submission20267/Reviewer_222z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20267/Reviewer_222z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901006037, "cdate": 1761901006037, "tmdate": 1762933747184, "mdate": 1762933747184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CircuitSense, a hierarchical benchmark designed to evaluate multimodal large language models on circuit understanding and symbolic reasoning tasks across over 8,000 problems. The study shows that while models perform well on visual perception tasks, they struggle significantly with symbolic derivation, highlighting a critical gap between visual comprehension and mathematical reasoning in engineering design."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The dataset is highly diverse, encompassing various difficulty levels and a wide range of analog question types. This diversity could significantly benefit future research on large language models (LLMs) in analog reasoning.\n\n- The experimental evaluation is thorough, involving multiple LLMs with different capability levels, providing a comprehensive comparison."}, "weaknesses": {"value": "- Although the work is valuable, several existing benchmarks—such as AMSBench and MMCircuitEval—appear to share similar objectives and formulations. The differences between this work and those benchmarks are not clearly articulated.\n\n- In the supplementary materials, there are multiple zero-byte files (e.g., in the Perception/func directory), which may indicate missing or corrupted data.\n\nTypos:\n\nIn Table 4, “GPT-4O” should be corrected to “GPT-4o”."}, "questions": {"value": "How is the correctness of the LLM’s answers evaluated? For example, in the Synthetic Example Q5, there seem to be multiple mathematically correct equations that can represent the same diagram. How is such ambiguity handled during evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8bS1JM0Lmv", "forum": "WEuJlEJmX8", "replyto": "WEuJlEJmX8", "signatures": ["ICLR.cc/2026/Conference/Submission20267/Reviewer_bBsZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20267/Reviewer_bBsZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926989658, "cdate": 1761926989658, "tmdate": 1762933746906, "mdate": 1762933746906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CircuitSense, a large-scale (8,006 problems across six hierarchical abstraction levels spread across Perception, Analysis, and Design problems) benchmark designed to evaluate visual mathematical reasoning in circuit analysis and design. This remains an under-explored domain for multimodal large language models (MLLMs). \n\nThe authors construct a hierarchical synthetic data generation pipeline capable of automatically producing novel circuit schematics and block diagrams with guaranteed symbolic ground-truth equations. They achieved this via SPICE simulation and symbolic analysis via SymPy and Lcapy. This allows systematic testing of models’ ability to convert visual circuit representations into symbolic transfer functions and equations, which is a step beyond conventional recognition or numerical tasks that exist in the current VLM’s. \n\nEvals across six leading MLLMs such as GPT-4o, Gemini-2.5-Pro, Claude-Sonnet-4, InternVL3, etc. reveal a sharp contrast between perception and reasoning: while models achieve >85% accuracy on component recognition and topology understanding, symbolic derivation accuracy collapses below 19%, with catastrophic failure on novel synthetic circuits. This demonstrates the baseline reliability of the benchmark in evaluating SOTA models on these tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper tackles an unexplored domain: hierarchical visual symbolic reasoning in circuit systems, with high-quality richly annotated data. It seems to be the first benchmark to evaluate how well AI systems connect circuit visuals to mathematical models. Prior work such as ChipVQA only partially evaluates this capability. \n- Six abstraction levels mirror real analog design processes and provide a structured way to locate where model failures occur\n- SDG pipeline and datasets are open-sourced and readily reproducible and could constitute a major contribution to the community in solving the data scarcity problem\n- Failure analysis is fine-grained, highlighting, for example, precise points of reasoning breakdown (output impedance and input impedance)"}, "weaknesses": {"value": "- Sim2Real gap remains: while the synthetic generation pipeline is well engineered, the circuits are constrained to 12-15 components; data sourcing from internet data might result in contamination \n- The benchmark relies on Gemini as the judge, which may result in potential biases toward the Gemini family \n- The paper could benefit in an analysis on cross-domain transfer; for example, training on schematic-level circuits and testing on block diagrams \n- Lastly, the paper would benefit in adding human baseline performance"}, "questions": {"value": "- Beyond algebraic identity via SymPy, have you considered functionally equivalent but structurally different forms? This might influence the reported accuracy at <19%. \n\n- How closely do generated circuits reflect real textbook or industrial designs?\n\n- How did you ensure fairness when benchmarking Gemini models using Gemini as a judge?\n\n- Could CircuitSense outputs be integrated into simulation-based tools to test end-to-end utility for analog designers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vIGfwweGDT", "forum": "WEuJlEJmX8", "replyto": "WEuJlEJmX8", "signatures": ["ICLR.cc/2026/Conference/Submission20267/Reviewer_h46k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20267/Reviewer_h46k"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974315017, "cdate": 1761974315017, "tmdate": 1762933746568, "mdate": 1762933746568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}