{"id": "VqAhhF6av8", "number": 7645, "cdate": 1758030400461, "mdate": 1759897841608, "content": {"title": "Routing, Cascades, and User Choice for LLMs", "abstract": "To mitigate the trade-offs between performance and costs, LLM providers route user tasks to different models based on task difficulty and latency. We study the effect of LLM routing with respect to user behavior. We propose a game between an LLM provider with two models (standard and reasoning) and a user who can re-prompt or abandon tasks if the routed model cannot solve them. The user's goal is to maximize their utility minus the delay from using the model, while the provider minimizes the cost of servicing the user. We solve this Stackelberg game by fully characterizing the user best response and simplifying the provider problem. We observe that in nearly all cases, the optimal routing policy involves a static policy with no cascading that depends on the expected utility of the models to the user.\nFurthermore, we reveal a misalignment gap between the provider-optimal and user-preferred routes when the user's and provider's rankings of the models with respect to utility and cost differ. Finally, we demonstrate conditions for extreme misalignment where providers are incentivized to throttle the latency of the models to minimize their costs, consequently depressing user utility. The results yield simple threshold rules for single-provider, single-user interactions and clarify when routing, cascading, and throttling help or harm.", "tldr": "We develop a game theoretic model between LLM providers and users to study when model routing is beneficial or harmful for users", "keywords": ["LLM routing; human-AI interaction; game theory"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d46c0625737b28cee24b1e271130fe6d8051ff2a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies how LLM providers route requests to different models (in this case, non-reasoning vs. reasoning) to balance performance, cost, and latency. It models this as a Stackelberg game between a cost-minimizing provider and a utility-maximizing user, who may re-prompt or abandon a task if it fails. The theoretical findings show that optimal routing is often a simple, static policy, but there could be a misalignment if the provider is incentivized to slow down responses in order to save costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Instead of studying LLM routing in a vacuum, just from the perspective of the provider, it models the interaction as a Stackelberg game, which formally includes the reactive behavior of a user (who can re-prompt or abandon a task) in response to the provider's strategy.\n- It shows a misalignment between provider and user when providers decide to slow down their models to nudge users to reduce their use.\n- It derives simple and practical insights, like the fact that a static policy (routing to one model always) is better than cascading.\n- The writing and motivation are clear"}, "weaknesses": {"value": "I don’t see major weaknesses, but perhaps my main concern is how relevant these findings and formulation will be long term. The paper deals with two models, one more capable and slower than the other. However, it seems like the direction from the main LLM providers is moving towards models with different levels of thinking capabilities depending on the effort. There, the model is routing in a way, and I’m not sure the findings here apply directly since the providers and users don’t have the same amount of control."}, "questions": {"value": "- What do you think of your formulation in terms of models that automatically decide their effort based on each task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jVnxViqyLI", "forum": "VqAhhF6av8", "replyto": "VqAhhF6av8", "signatures": ["ICLR.cc/2026/Conference/Submission7645/Reviewer_QacU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7645/Reviewer_QacU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707956520, "cdate": 1761707956520, "tmdate": 1762919717619, "mdate": 1762919717619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the interaction between a large language model (LLM) provider that routes tasks across multiple models and a user who can choose to reprompt or abandon tasks depending on model performance. The authors model this as a Stackelberg game between the provider and a user, where the provider decides a routing and cascading policy, and the user optimizes their abandonment policy based on perceived utility. The setup assumes two LLMs — a standard and a reasoning model — that differ in accuracy, latency, and cost. The paper provides a closed-form characterization of the equilibrium by first solving for the user’s best response (Theorems 1–2) and then deriving provider-optimal routing policies (Theorems 3–5). The results show that: (1) static routing without cascading is optimal in most regimes; (2) misalignment gaps arise when the provider’s cost-based model ranking differs from the user’s utility-based ranking (Section 5); provider throttling (i.e., intentionally increasing latency) can emerge as an equilibrium when user churn penalties are low (Proposition 2, Section 6). Empirically, the authors visualize user responses, provider policies, and misalignment gaps across parameter regimes (Figures 3–5) and provide intuitive insights on user patience, value-dominated vs. latency-dominated models, and welfare trade-offs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Originality:The paper introduces a novel behavioral–economic perspective on LLM routing. While prior routing work (e.g., Chen et al., 2023; Ding et al., 2024; Hu et al., 2024) focuses on minimizing cost–latency trade-offs, this paper uniquely models strategic user response via a multi-round prompting game (Section 3). This Stackelberg formulation, with users as rational agents, represents a conceptual advance that bridges operations research and AI system design.\n\nQuality: The analysis is mathematically sound. The closed-form results (Sections 4.1–4.2) are carefully derived and supported by additional lemmas in the appendix.\n\nClarity: The exposition is clear and well-structured. Visuals (Figures 1–5) effectively summarize equilibrium regions and threshold rules. The notation is consistent. The intuitive explanations following each theorem (especially Theorem 2 and 5) help readability.\n\nSignificance: This work is relevant for LLM system design and AI governance. The analysis gives explicit conditions for misalignment and welfare loss, which is useful for researchers and policymakers analyzing these AI platforms."}, "weaknesses": {"value": "Empirical validation: The work is entirely theoretical. While this is appropriate for a conceptual contribution, the claims about user patience and latency manipulation (Figure 5 Right) would benefit from empirical support, e.g., simulations or user–provider experiments.\n\nLimited model diversity: The analysis considers only two models (standard vs. reasoning). While the authors acknowledge this in the conclusion (Section 7), the extension to $n$ models could meaningfully affect equilibrium behavior—especially when users can select between several public endpoints.\n\nSimplifying behavioral assumptions: The model assumes users observe provider cascade probabilities and adopt stationary abandonment policies (Section 3.1). In practice, users may have incomplete information or adaptive patience. A discussion of bounded rationality or stochastic user beliefs could improve this work."}, "questions": {"value": "Extension to multiple models: How would the equilibrium generalize to $n>2$ models? Would the threshold structure persist or collapse into pairwise comparisons?\n\nDynamic user learning: The analysis assumes fixed $p_i$. How might the equilibrium change if users learn about model success probabilities over repeated interactions?\n\nAlternative objectives: Have the authors considered a bi-level optimization where providers internalize user utility as part of a long-term revenue function?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PHAHZaLlsg", "forum": "VqAhhF6av8", "replyto": "VqAhhF6av8", "signatures": ["ICLR.cc/2026/Conference/Submission7645/Reviewer_pynF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7645/Reviewer_pynF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735775891, "cdate": 1761735775891, "tmdate": 1762919717141, "mdate": 1762919717141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Advances in AI systems raise interesting questions for model deployers --- for any given user query, what model should be presented to the user, when models have different capabilities and costs to querying? And with a proliferation of model deployers, users too have a choice --- which deployer to engage with? The authors lay out these questions and formalize the interplay between model routing and user participation as a game, to which they develop game-theoretic insights into how actors may behave under different cost settings and how such a game could be \"gamed\" by model deployers in potentially harmful ways to users."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "I very much enjoyed reading this paper! The authors' formalization of the problem of user and model deployer interaction as it relates to what model is served is a highly (and increasingly) important to today's consumer AI dynamic. I found the connection to game theory quite creative, and I learned quite a bit from reading the paper. The authors' novel insights about possible gameification that model deployers could engage in, given such a game (re throttling latency) are likely quite a valuable contribution in their own right. I appreciated that the authors were open about their limitations of their set-up of the problem as well. I imagine there could be a nice blossoming literature around this and related works to better understand and model people and deployers' choices in the setting of multi-model choice."}, "weaknesses": {"value": "I found quite few weaknesses in the work; however, I may have missed something in the mathematics. As someone a bit weaker on the theory-side, I did find some of the theoretical discourse quite dense and a little convoluted -- especially section 3 (but this may be my own naivete --- indicated in my lower confidence score). \n\nAs noted above, the authors seem quite upfront on their limitations (I would be interested in settings where the user may not be aware of the routing policy or s). \n\nMore minor but important -- I found some of the visuals a little confusing. There are quite a few colors in Fig 2 but it is not clear what they relate to. It would be helpful if the caption spelled out the assignment of colors to differences in the kind of computation. Figure 3 also indicates that Model 1 is shown with dashed lines, but I can't seem to see where this is?"}, "questions": {"value": "See above re: questions on interpreting figure 4 (where is the Model 1 dashed line?) \n\nI would be interested in the authors' extension to not just other models -- but the case where multiple model deployers are simultaneously competing for the same user."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ld2I5ig6fU", "forum": "VqAhhF6av8", "replyto": "VqAhhF6av8", "signatures": ["ICLR.cc/2026/Conference/Submission7645/Reviewer_d6sm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7645/Reviewer_d6sm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864138427, "cdate": 1761864138427, "tmdate": 1762919716746, "mdate": 1762919716746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper’s goal is to characterize operation regime of LLMs where providers offer models of different capability (and hence difference prices) and the users aim at minimizing their costs while getting the best utility out of the model. Some other variables of interest are the provider’s desire to minimize their inference costs and the user’s desire to minimize the latency. The paper studies multiple strategies, namely, routing to the best possible model and cascading through small models first and then sending to the larger model if the smaller model’s response is not good enough. The paper then proves different statements about optimal routing / cascading policies given different capability and latency ratios between the two models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The high level problem of learning optimal routing strategies is quite well motivated.\n2. The formalism is thorough, though it could be quite dense at some points."}, "weaknesses": {"value": "1. In general, I have quite a few concerns about how realistic the whole setup is. The tasks that the paper defines seem a bit different from what LLM users encounter in practice. The paper should describe (i) Why is the monetary cost to the user is not modeled? (ii) The user churning with some predefined probability seems plausible. But during deployment, users mostly cannot check the accuracy on each individual example. So is the framework here meant to be more suitable for scenarios when users are in the model testing phase? (iii) Why would the users keep trying the same model M1 if it doesn’t respond correctly? (iv) Users generally test on a set of independent queries. If the performance is bad on multiple successive queries, users could abandon the provider. How are the dynamics over multiple queries modeled? (v) Why is $s$ a number between 0 and 1. Shouldn’t it always be 1? If M1 fails, shouldn’t the provider always cascade to M2? (vi) The cascading step seems to assume that the model provider knows if the model answered the prompt correctly. Why is this a realistic assumption? In reality, the model providers are not even supposed to see the user inputs due to privacy and IP reasons. (vii) The notion of “value” that the user derives from the model is quite vague. Is this a real number or simply 0/1 accuracy. If it is the former, why?\n\n2. On a similar note, in Section 6, if the service costs are so high, why can the provider simply not raise the price rather than throttling users (disgruntled users might never return)? Not clear why this would be a rational policy.\n\n3. The value of the dense formalism is not clear and the insights that we draw looks to be derivable from simpler analyses. The main idea seems to be to compare the ratio between the accuracy and latency. The paper correctly points out that when the accuracies are the same, cascading doesn’t make sense since it add unnecessary latency by putting the smaller model in front of the bigger model. The results seem quite intuitive when considering this tradeoff. For instance, about the insight in Line 88, of course we expect users to stay if the net value is positive. The difficulty in practice is that it is very difficult to judge in advance for an unseen data point if the model will provide positive values, e.g., of the summary of an article will be good enough. Similarly, the insight in line 85 also seems straightforward. The main trouble is that we cannot predict when one model provides better value than the other (see weakness 1)."}, "questions": {"value": "Please see questions under weakness 1."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Reading the Appendix A and text in line 476, it seems that LLMs contributed very substantially to the ideation of the paper. For instance, the authors not that LLMs were used in \"(i) deriving proof steps or searching for counterexamples; (ii) preparing first drafts for sections and editing with feedback; and (iii) coding plots.\". For proofs, the process involved 2-10 prompts (Table 1).\n\n**Given that LLMs seem to have made fundamental contributions (to the extent to be considered authors), I would like to flag the paper for adherence to the ICLR policies.** The guidelines on the website do not clearly state what extent of LLM involvement is permitted."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b1utVGkUOW", "forum": "VqAhhF6av8", "replyto": "VqAhhF6av8", "signatures": ["ICLR.cc/2026/Conference/Submission7645/Reviewer_M77r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7645/Reviewer_M77r"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933647387, "cdate": 1761933647387, "tmdate": 1762919716299, "mdate": 1762919716299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary: Thanks for all reviews"}, "comment": {"value": "We thank all reviewers for their constructive and positive feedback:\n\n- Well-motivated and novel formulation of LLM routing with user response (`M77r, d6sm, pynF, QacU`)\n- Thoroughness of formalism and mathematical results (`M77r, pynF`)\n- Practical insights drawn from the framework (`d6sm, pynF, QacU`)\n- Quality & clarity of writing (`d6sm, pynF, QacU`)\n\n\nOur paper introduces the novel problem of the interaction between routed LLMs and users.  We build a stylized model of a prompting game between user and provider. This allows us to derive closed-form user/provider policies and identify the potential for misalignment between user and provider objectives. This is a fundamental first step towards understanding more empirical work on practicalities of LLM routing.\n\n\nWe received several common questions. We will revise the paper with the corresponding discussions:\n\n- **Partial information extension:** Our framework assumes the user knows $p_i$ and $s$. If $p_i$ is not available, users can still use a belief distribution of $\\xi_i(p_i)$ to estimate $E[\\xi_i]$ and correspondingly apply Theorem 1 and 2. This extension poses two next steps: (i) the provider optimal policy requires estimating the user's belief of $E[\\xi_i]$; and (ii) users may try an exploration/exploitation strategy over multiple task instances where they set low $q$ at first to estimate $\\xi_i$ with more observations. We still expect to observe potential misalignment.\n- **Extensions to multiple models:** The framework generalizes to $n>2$ models by modifying the Markov chain, re-defining absorption probabilities for $S_i, L_i, C_i$, and numerically optimizing the user/provider problems. We expect the key insights of our work to hold. However, there are $2^n$ scenarios because the user optimal response depends on which combination of models satisfy $\\xi_i > 0$. Consequently, we do not expect neat closed-form solutions for all settings.\n- **Realism of model and formal setup:** Our stylized model tries to give the most generalization while permitting closed-form theoretic results. We will include a more expanded motivating example to explain our design choices."}}, "id": "I78pC4w3qi", "forum": "VqAhhF6av8", "replyto": "VqAhhF6av8", "signatures": ["ICLR.cc/2026/Conference/Submission7645/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7645/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission7645/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763471929919, "cdate": 1763471929919, "tmdate": 1763471929919, "mdate": 1763471929919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}