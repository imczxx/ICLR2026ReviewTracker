{"id": "IBJtOltTbx", "number": 14375, "cdate": 1758234115941, "mdate": 1759897374236, "content": {"title": "Hybrid Training for Vision-Language-Action Models", "abstract": "Using Large Language Models to produce intermediate thoughts, a.k.a. Chain-of-thought (CoT), before providing an answer has been a successful recipe for solving complex language tasks. In robotics, similar embodied CoT strategies, generating thoughts before actions, have also been shown to lead to improved performance when using Vision-Language-Action models (VLAs). \nAs these techniques increase the length of the model's generated outputs to include the thoughts, the inference time is negatively affected. Delaying an agent's actions in real-world executions, as in robotic manipulation settings, strongly affects the usability of a method, as tasks require long sequences of actions. \nHowever, is the generation of long chains-of-thought a strong prerequisite for achieving performance improvements? In this work, we explore the idea of Hybrid Training (HyT), a framework that enables VLAs to learn from thoughts and benefit from the associated performance gains, while  enabling the possibility to leave out CoT generation during inference. Furthermore, by learning to conditionally predict a diverse set of outputs, HyT supports flexibility at inference time, enabling the model to either predict actions directly, generate thoughts or follow instructions. We evaluate the proposed method in a series of simulated benchmarks and real-world experiments.", "tldr": "Hybrid Training enables Vision-Language-Action models to learn from chain-of-thought reasoning traces, achieving high performance, while maintaining fast inference, by allowing direct action prediction at test time.", "keywords": ["vision-language-action models; chain-of-thought; robotic manipulation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6ed7cd0358faa13a46a651f77f4ce37d3a215b1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Hybrid Training (HyT), a framework that lets VLAs learn from CoT reasoning for better performance while allowing inference without generating thoughts. HyT flexibly enables direct action prediction, thought generation, or instruction following, and shows strong results in both simulation and real-world experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Performance-speed breakthrough: HyT resolves the key “reasoning vs. reflex” trade-off in VLAs, matching the success rates of CoT methods while maintaining high inference speed (e.g., 3 Hz vs. 1 Hz for ECoT)—a major step toward practical deployment.\n\n2. Robust empirical results: Extensive experiments show HyT’s clear advantages: strong data-scaling on ClevrSkills, state-of-the-art results on LIBERO (especially “Goal” and “Long” tasks), and real-world robot transfer. Its 54% OOD success vs. 29% baseline highlights superior generalization.\n\n3. Versatile and interpretable model: A single HyT-trained model supports fast “act,” interpretable “think,” and guided “follow” modes—allowing deployment, analysis, and human-robot collaboration without retraining."}, "weaknesses": {"value": "1.  HyT requires CoT or reasoning traces for training—using oracle thoughts on ClevrSkills and Gemma-2 9B-generated plans on LIBERO. This reliance on costly, curated reasoning data limits scalability and makes adaptation to new domains harder than learning directly from demonstrations.\n\n2. While “act” mode matches “think” mode performance, the paper doesn’t test harder, long-horizon, or abstract tasks where explicit reasoning might still be needed. Without such analysis, the boundaries of HyT’s internalized reasoning remain uncertain."}, "questions": {"value": "All of my qeustions are listed in the weakness section. If my concerns are well addressed, I will raise my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HJ1k9EYTmB", "forum": "IBJtOltTbx", "replyto": "IBJtOltTbx", "signatures": ["ICLR.cc/2026/Conference/Submission14375/Reviewer_LZtX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14375/Reviewer_LZtX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942814902, "cdate": 1761942814902, "tmdate": 1762924791932, "mdate": 1762924791932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Hybrid Training (HyT), a method that unifies thinking and acting behaviors in a single vision-language-action model via a learnable modality variable controlling “think/act/follow” modes. HyT trains one model to generate both reasoning traces and executable actions, using a Monte-Carlo sampling scheme to mix modalities during training. The method is evaluated across simulated (ClevrSkills, LIBERO) and real-robot (xArm6) environments, demonstrating competitive task success and faster inference compared to full chain-of-thought reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. HyT uses a modality variable to avoid maintaining separate reasoning and acting modules, simplifying deployment and enabling run-time mode control (think/act/follow) without model-switching.\n2. HyT shows good performance on the ClevrSkills benchmark, suggesting HyT can learn complex, compositional behaviors required there. And robotic experiments on an xArm6 support practical applicability beyond simulation."}, "weaknesses": {"value": "1. In Figure 4, reported task success and efficiency gains relative to ECoT are small; it remains unclear whether HyT achieves comparable performance at lower latency or merely approximates ECoT under similar compute.\n2. How does HyT compare to the π₀.₅ model (https://github.com/Physical-Intelligence/openpi) and OneTwoVLA (https://arxiv.org/pdf/2505.11917)? Can the BOA/BOR control mechanism (as in OneTwoVLA) achieve similar adaptive switching without retraining?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B1tA565ZH3", "forum": "IBJtOltTbx", "replyto": "IBJtOltTbx", "signatures": ["ICLR.cc/2026/Conference/Submission14375/Reviewer_zK8v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14375/Reviewer_zK8v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987975580, "cdate": 1761987975580, "tmdate": 1762924791602, "mdate": 1762924791602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hybrid Training (HyT), a framework for training Vision-Language-Action models that enables learning from chain-of-thought (CoT) reasoning traces while maintaining fast inference speeds. The key insight is that VLAs can internalize knowledge from thought supervision during training without needing to generate thoughts at test time. HyT trains a single model to conditionally predict three distributions based on a modality variable: direct actions (\"act\"), thoughts then actions (\"think\"), and following provided instructions (\"follow\"). The method uses Monte Carlo sampling during training for the three modes. Experiments on ClevrSkills, LIBERO, and real-world tasks demonstrate that HyT achieves performance gains similar to ECoT while maintaining standard VLA inference speeds (3Hz vs 1Hz for ECoT)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The hypothesis that models can internalize CoT reasoning without explicit generation is compelling and well-articulated through the System 1/System 2 cognition analogy.\n- HyT outperforms competitive baselines including MolmoAct (86.6% → 93.7%) and π0-FAST (85.5% → 93.7%) on LIBERO, while maintaining 3× faster inference than ECoT.\n- The paper includes extensive experiments across simulated (ClevrSkills, LIBERO) and real-world environments with ablations on data scaling.\n- The ability to maintain ECoT-level performance at standard VLA inference speeds addresses a critical deployment constraint for robotic systems."}, "weaknesses": {"value": "- The loss coefficients (wa:0.25, wτ:0.5, wf:0.25) appear arbitrary without ablation\n- The Monte Carlo sampling approach is not compared to direct weighted loss computation\n- Individual contributions of Lfollow and Lthink to overall performance are not analyzed\n- HiRobot with oracle thoughts often outperforms HyT with oracle, suggesting the \"follow\" mode may not be optimally implemented. \n- The paper lacks systematic investigation of which task characteristics benefit most from internalized CoT training versus standard VLA training.\n- Unclear if all baselines use the same VLM backbone (PaliGemma-2)\n- No inference speed comparison with π0-FAST (or other baselines)\n- Compatibility with other VLA architectures not explored"}, "questions": {"value": "- Why does HiRobot+Oracle outperform HyT+Oracle in Figure 4?\n- How were the loss coefficients (0.25, 0.5, 0.25) determined? What is the sensitivity to these values?\n- What is the ablation comparing Monte Carlo sampling versus direct weighted loss computation? Does the sampling variance help or is it just for computational efficiency?\n- Can you provide ablations showing individual contributions of Lthink and Lfollow? What happens with just Lact + Lthink?\n- What are the specific inference speeds compared to π0-FAST, and can HyT be combined with their efficient tokenization approach?\n- In what types of tasks (complexity, horizon length, reasoning requirements) does HyT show the largest improvements over standard VLA training?\n- Do all compared baselines (Table in LIBERO results) use the same PaliGemma-2 backbone for fair comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T5TeCTOEf4", "forum": "IBJtOltTbx", "replyto": "IBJtOltTbx", "signatures": ["ICLR.cc/2026/Conference/Submission14375/Reviewer_PRFm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14375/Reviewer_PRFm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009917584, "cdate": 1762009917584, "tmdate": 1762924791289, "mdate": 1762924791289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes that the benefits of chain-of-thought (CoT) reasoning in VLAs can be reaped without the very slow inference times that generating reasoning chains at test-time requires. Namely, they propose a method called HyT (hybrid training) in which they train the VLA to exhibit multiple modes of behaviors, either predicting actions directly, predicting a reasoning chain and then actions, or predicting actions from an oracle human-provided reasoning chain, with these modes selectable by configuring a modality variable. This hybrid training scheme allows the model to effectively learn from the chain-of-thought data at train time, not having to actually generate these reasoning chains at test-time to still get equally good performance. The authors test their approach against a standard VLA, a reasoning VLA, and a hierarchal VLA on the ClevrSkills simulated benchmark and find that their approach performs the best across multiple dataset sizes. On the Libero simulated benchmark, there approach, adapted off of the OFT-VLA model, performs the best when compared to many prior VLAs on Libero. And finally on eight tasks in the real-world, their model exhibits better performance than a standard VLA while requiring the same inference time (~3 Hz)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The authors tackle a very relevant problem in robotics, and arrive at a nice result in that the benefits of reasoning trace generation can be obtained without having to generate expensive reasoning chains at test time. The proposed hybrid training strategy is general and permits different manners of either conditioning on or not conditioning on reasoning at test time.\n\n(2) The experimental results are reasonably through, and include real-world results, which is nice. In particular, the authors’ method scores the highest on the Libero simulated benchmark when compared to other fine-tuned VLAs.\n\n(3) The paper is clear and easy to read."}, "weaknesses": {"value": "(1) The analysis of the results could be improved. In particular, it remains unclear what the specific reason is that reasoning trace generation can be avoided at test-time, but yet the benefits of reasoning can still be obtained. In their discussion section the authors wrote that “from analyzing the HyT framework, our understanding is that learning to generate CoT and learning to predict actions from CoT improves the agent’s understanding of the environment” —> could you expand on what it mean to improve the understanding of the environment? I.e., is it that the model can obtain better internal representations if trained with reasoning, or can follow language better, or something else?\n\n(2) Reasoning is perhaps the most appealing when the task involves some complex decision making/logical inference before picking what atomic action to take. It seems that the authors missed trying their approach out on these types of tasks. Indeed from the discussion: “we do not exclude that thoughts generation might be useful at test-time in more complex settings, but this would require evaluating on tasks that require advanced reasoning capabilities, which are currently sparse and rarely adopted in the robotics literature”. It would have been interesting to see whether the hybrid training scheme can bring benefits to these types of tasks as well, or if reasoning at test-time becomes strictly necessary."}, "questions": {"value": "(1) For the three different modes, the authors chose weights 0.25 for act, 0.5 for think, and 0.25 for follow. Did they ablate different values of these weights? Generally how important is a good selection of these weights to model performance? Which of the three modes matter the most/least?\n\n(2) HiRobot + Oracle seems to be doing the best, as shown in Figure 4. Am I correct in assuming that the main difference between HiRobot and the proposed approach is that two separate PaliGemma models are used? What leads to the improved performance? More capacity? Less feature sharing?\n\n(3) Why do you think ECoT performed worse than the proposed approach? They are very similar, and it would seem that ECoT should perform slightly better since it generates reasoning at test-time?\n\n(4) Why aren’t any of the ClevrSkills baselines compared against in Libero? E.g., ECoT or HiRobot?\n\n(5) For the real-world tasks, what do the reasoning traces look like and how were they generated? Were the types of reasoning traces generated such that reasoning might be particularly useful in the OOD setting?\n\n(6) There appears to be substantial overlap with a contemporaneous, peer-reviewed publication ([1], accepted Aug 1, 2025). Per the ICLR 2026 reviewer FAQ, I will not penalize the paper for missing a citation or comparison to contemporaneous work. However, as written, the submission does not clearly articulate contributions beyond that work. It would be great if the authors could clarify what is genuinely new here (theory, method, or empirical evidence) beyond this prior work.\n\n[1] Chen, W., Belkhale, S., Mirchandani, S., Mees, O., Driess, D., Pertsch, K., & Levine, S. (2025). Training Strategies for Efficient Embodied Reasoning. arXiv preprint arXiv:2505.08243."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YuCxFSG8JY", "forum": "IBJtOltTbx", "replyto": "IBJtOltTbx", "signatures": ["ICLR.cc/2026/Conference/Submission14375/Reviewer_cYRE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14375/Reviewer_cYRE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065972818, "cdate": 1762065972818, "tmdate": 1762924790917, "mdate": 1762924790917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}