{"id": "1cQChGqNX4", "number": 10287, "cdate": 1758166028144, "mdate": 1759897660907, "content": {"title": "MCP-R1: Generalized Real-World Task Agent Mastering Dozens of Tools", "abstract": "Modern agentic models require strong capabilities for orchestrating external tools to interact with complex environments. However, existing tool-integration approaches support only a narrow range of tools and lack a unified calling standard. Consequently, they devote little attention to real-world tasks and struggle to transfer to unseen tools. The emergence of the Model Context protocol (MCP) presents an open standard for two-way connections between external tools and agents. To this end, we introduce MCP-R1, a new paradigm designed to enhance models’ universal tool-interaction capabilities. We first construct a virtual-real integrated MCP tool system, supporting 17 MCP servers with 60+ tools, each sourced from real-world services to ensure diversity and authenticity during training. Based on the tool system, we further propose a scalable pipeline for generating multi-tool invocation data. In addition, going beyond rule-based rewards commonly used in QA tasks, we introduce a trajectory-based reward mechanism to evaluate the agent’s performance in goal-driven tasks. Thanks to the unified tool-interaction standard and our training pipeline, MCP-R1 has generic interacting ability across a broad set of tools, demonstrates strong performance on practical tasks across diverse scenarios, while flexibly adapting to unseen tools. Our experiments span several challenging domains including search (GAIA, WebWalker), general tool calling (MCP-Universe), and practical task execution. The strong performance of MCP-R1 underscores the effectiveness of our training paradigm, offering valuable insights and a scalable approach for developing general agentic models.", "tldr": "", "keywords": ["Large Language Model", "Agentic", "Model Context Protocol"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/08c22b9eceead64199346efb3a11c7c8849ca096.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MCP-R1, a training framework aimed at improving general multi-tool interaction capabilities of models. The main contributions include:\n\n1. Constructing a dataset of MCP tools, covering 17 MCP servers and over 60 tools (both real and self-constructed);\n2. Designing a data generation pipeline for MCP training, which produces answer-driven and goal-driven tasks, including automatically synthesized multi-tool, multi-step tasks;\n3. Enhancing the ability of large models to use MCP tools through a two-stage training process consisting of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments of this paper provide comparisons with advanced benchmarks. The paper conducts experiments on various methods, including RAG-based and RL-based approaches. In addition, experiments are conducted on tasks in three different scenarios, which thoroughly demonstrate the performance of MCP-R1 under different configurations, showing superior results in most cases.\n2. The method proposed in the paper is suitable for test time scaling. According to the experimental results, the proposed method demonstrates significantly better performance than the baselines under test time scaling."}, "weaknesses": {"value": "1. This method lacks sufficient innovation and insight: the data synthesis method proposed in this paper is relatively common, and there are no targeted improvements for SFT and RL. \n2. The paper lacks comparisons with existing research on API dataset generation, making it difficult to demonstrate the breakthrough of the proposed method.\n3. Some of the experimental results in this paper are insufficient to demonstrate the superiority of the proposed method. In the main experiments on deep search, the improvement of MCP-R1 over methods such as ARPO is relatively small, making it difficult to prove the advantage of MCP-R1 compared to existing work.\n4. The experimental setup of this paper lacks necessary ablation studies. The paper proposes two tasks and two training methods, but does not include ablation experiments to demonstrate whether both SFT and RL are necessary, or whether it is necessary to design two different tasks."}, "questions": {"value": "1. At present, there are a large number of MCP servers and tools available. Why does the paper only use around 61 tools, instead of constructing a large-scale training dataset based on more tools? If the number of tools is increased, would the performance of SFT and RL improve further?\n2. In the experiments on the GAIA benchmark mentioned in the paper, which toolset is used? Is it the same toolset used for training, or does it share another set of tools with other methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fyPHUsIiu4", "forum": "1cQChGqNX4", "replyto": "1cQChGqNX4", "signatures": ["ICLR.cc/2026/Conference/Submission10287/Reviewer_zaaL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10287/Reviewer_zaaL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545191999, "cdate": 1761545191999, "tmdate": 1762921640961, "mdate": 1762921640961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MCP-R1, a training framework for agentic models targeting generalized real-world tasks, aiming to address key limitations in current tool-using agents. The authors construct a Virtual-Real Integrated MCP Tool System, integrating 17 MCP servers and 60+ tools，and design a Scalable Data-generation Pipeline to systematically generate two types of tasks: answer-driven tasks and goal-driven tasks. Training follows a two-stage SFT + RL paradigm, where the Reinforcement Learning (RL) phase introduces a trajectory-based reward mechanism: an LLM evaluates the agent’s tool-call logic, sequencing, and goal achievement against a predefined rubric."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper achieves large-scale tool integration under the Model Context Protocol (MCP) and employs a virtual-real hybrid architecture: virtual tools ensure training safety and scalability, while real tools guarantee environmental fidelity, enabling zero-shot transfer.\n- It explicitly distinguishes between answer-driven and goal-driven tasks, moving beyond prior work that focuses almost exclusively on QA-style benchmarks.\n- For goal-driven tasks, it introduces a trajectory-based reward mechanism, allowing Reinforcement Learning (RL) to optimize tasks that lack a single ground-truth answer. It also employs Dynamic Server Sampling to train the model to identify and ignore irrelevant tools, thereby enhancing generalization.\n- Experiments demonstrate that a well-designed training paradigm + standardized interface can compensate for limited model scale outperforms much larger models."}, "weaknesses": {"value": "- Although MCP-R1 constructs an MCP training method for general real-world tasks, it does follow the current mainstream paradigm of agentic model training (i.e. , Data construction + SFT + RL)\n- The paper claims zero-shot transfer to real tools but provides no deployment experiments with real APIs and does not evaluate robustness to real-world issues such as API errors, authentication failures, or output drift.\n- Evaluation heavily relies on LLM-as-Judge, introducing subjectivity; the judge model (GPT-4.1-mini) may not align with human preferences, risking biased or inconsistent scoring.\n- On MCP-Universe, baseline models are not adapted to the MCP interface and instead call raw APIs directly, making comparisons unfair—MCP-R1’s advantage may stem partly from interface standardization, not superior policy learning.\n- While the paper emphasizes that MCP-R1 “avoids irreversible operations” in goal-driven tasks, it does not describe any explicit safety or error-recovery mechanism, nor does it show how such behavior is learned or enforced."}, "questions": {"value": "1. How is the fidelity of virtual tools quantified? Do they simulate real-world characteristics such as API latency, error rates, and output noise? Is there any error analysis comparing virtual vs. real tool behavior?\n2. Is the trajectory-based reward aligned with human preferences? Has the reliability of the LLM judge been validated via human evaluation?\n3. What is the exact strategy for Dynamic Server Sampling? Does randomly injecting 1–2 irrelevant MCP servers into the context cause confusion? Has an ablation study been conducted to justify this design?\n4. What is the diversity and difficulty distribution of tasks in MCP-RealWorld? Are the 199 tasks publicly released? Could the use of template-based generation lead to overfitting or lack of realism?\n5. How does the agent select relevant tools during inference in a setting with 60+ tools? What is the context-length overhead of including tool definitions, and how is tool selection efficiency maintained?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pDYQMt3UEH", "forum": "1cQChGqNX4", "replyto": "1cQChGqNX4", "signatures": ["ICLR.cc/2026/Conference/Submission10287/Reviewer_5P8B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10287/Reviewer_5P8B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724252693, "cdate": 1761724252693, "tmdate": 1762921640283, "mdate": 1762921640283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MCP-R1, an agentic framework that aims to improve general-purpose tool-interaction abilities in large language models by leveraging the Model Context Protocol (MCP) as a unified standard. The authors construct a “Virtual–Real Integrated MCP Tool System” with over 60 tools across 17 MCP servers, and a scalable data-generation pipeline to produce both answer-driven and goal-driven tasks. The training procedure includes supervised fine-tuning and reinforcement learning with a trajectory-based reward for goal-driven evaluation. Experiments on benchmarks such as GAIA, WebWalkerQA, MCP-Universe, and a self-constructed MCP-RealWorld benchmark show improvements over baseline models in tool-use performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors built an multi-tool environment (60+ tools) combining real and simulated MCP servers, which demonstrates strong engineering effort and scalability.\n\n2. The work explores the application of the MCP standard in large-scale tool-use training, aligning with emerging directions in agentic model research.\n\n3. The authors provide thorough empirical results on both public and self-constructed benchmarks, covering different task types (answer-driven and goal-driven)."}, "weaknesses": {"value": "1. The paper fails to convincingly justify why focusing on goal-driven tasks and environment state changes is more meaningful than answer-driven tasks. This shift is a central claim of the paper, yet it is not theoretically motivated or empirically validated beyond intuition. Without a clear argument or evidence, this framing seems arbitrary.\n\n2. The introduction of MCP into the training pipeline is primarily an engineering integration, not a new scientific insight. The paper does not clearly identify what new learning principle, algorithmic challenge, or research question is being addressed. The claim that a “unified standard” improves generalization is not rigorously analyzed and appears to be a software design choice, not a contribution to learning theory. In addition, the authors do not explain why the lack of a unified protocol is a significant research problem. The proposed solution seems to merely leverage an existing standard (MCP) rather than offering any novel algorithmic or modeling advance.\n\n3. The paper overlooks prior work that has similar goals and setups, especially [a], which also trained agents with multiple tools and generated large-scale tool-use trajectories (~20K). A direct comparison and discussion are necessary to assess novelty and performance relative to such baselines.\n\n[a] MULTI-MODAL AGENT TUNING: BUILDING A VLM-DRIVEN AGENT FOR EFFICIENT TOOL USAGE. ICLR 2025.\n\n4. While MCP-R1 uses 60+ tools, it is unclear how performance scales with tool diversity or whether similar results could be achieved with fewer tools. The system design and experimental results focus on scale rather than understanding what contributes to improvement."}, "questions": {"value": "See my weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RKeNc9Q2O4", "forum": "1cQChGqNX4", "replyto": "1cQChGqNX4", "signatures": ["ICLR.cc/2026/Conference/Submission10287/Reviewer_LgkF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10287/Reviewer_LgkF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906380226, "cdate": 1761906380226, "tmdate": 1762921639888, "mdate": 1762921639888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MCP-R1, a training framework for improving large language models' tool interaction in real-world tasks. Its main features are: an MCP Tool System integrating virtual and real environments with 17 servers and 60+ tools, a scalable data generation pipeline for answer and goal-based tasks, and a trajectory-based reward system. Tests on GAIA, WebWalkerQA, and other benchmarks show MCP-R1 outperforms baselines, adapting well to new tools and working effectively in practice."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Combines MCP protocol with hybrid virtual-real tool system to overcome limitations of existing tool-integrated models (narrow coverage, lack of standards). Integrates answer and goal-driven tasks with trajectory-based rewards, addressing prior single-task, answer-centric research gap.​\n\n-  Rigorous experiments across search, tool calling, and real-world tasks. Compares MCP-R1 to GPT-4o, Qwen3-235B, etc. Pass@K analysis shows scalability in dynamic interactions.​"}, "weaknesses": {"value": "1. The statement “but the open-source community still lacks sufficient attention to this matter” (lines 63-64) is inaccurate. Both open-source and closed-source communities have given significant attention to this topic, undermining the credibility of this claim.​\n\n2. The comparison with baselines is unfair. Many baselines support only 1-2 tools, whereas MCP-R1 uses over 60. The paper doesn't distinguish whether performance improvements come from the framework's design or the larger number of tools, undermining the validity of direct comparisons.​\n\n3. The authors' discussion on the distinction between answer-driven and goal-driven problem definitions is unclear and potentially misleading. It appears that both concepts inherently involve goal orientation, raising the question of whether the authors intended to differentiate between QA tasks and other Agent tasks instead. \n\nminor: The second paragraph reads more like a literature review than an introduction."}, "questions": {"value": "1. Will the sandbox, data, and training scripts for this work be open-sourced? The authors claim to be contributing to the open-source community, yet there is no mention in the paper of any plans to open-source the MCP SERVER, DATA, etc.​\n\n2. Could you provide explicit criteria for classifying tools as virtual or real, and explain how this division balances training safety, computational cost, and the authenticity of tool interactions?​\n\n3. To address baseline comparison fairness, have you conducted any ablative experiments (e.g., training MCP-R1 with a reduced set of tools matching baseline tool counts) to verify that performance gains are driven by the framework rather than tool quantity?​\n​\n4. How was the difficulty level of tasks in the MCP-RealWorld benchmark determined? Were tasks validated with human annotators to ensure they reflect real-world complexity and tool-use requirements?​"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ab7drsAQfX", "forum": "1cQChGqNX4", "replyto": "1cQChGqNX4", "signatures": ["ICLR.cc/2026/Conference/Submission10287/Reviewer_TTuS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10287/Reviewer_TTuS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974135582, "cdate": 1761974135582, "tmdate": 1762921639452, "mdate": 1762921639452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}