{"id": "oCkPzCR4f4", "number": 7426, "cdate": 1758021204606, "mdate": 1763129453271, "content": {"title": "From Charts to Code: A Hierarchical Benchmark for Multimodal Models", "abstract": "We introduce Chart2Code, a new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Chart2Code is explicitly designed from a user-driven perspective, capturing diverse real-world scenarios and progressively increasing task difficulty. It consists of three levels: Level 1 (Chart Reproduction) reproduces charts from a reference figure and user query; Level 2 (Chart Editing) involves complex modifications such as changing chart types or adding elements; and Level 3 (Long-Table to Chart Generation) requires models to transform long, information-dense tables into faithful charts following user instructions. To our knowledge, this is the first hierarchical benchmark that reflects practical chart2code usage while systematically scaling task complexity. In total, Chart2Code contains 1,947 tasks across 22 chart types, paired with multi-level evaluation metrics that assess both code correctness and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art LMMs, including both proprietary and the latest open-source models such as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental results demonstrate that even the strongest models struggle to generalize across levels and chart types, highlighting the significant challenges posed by Chart2Code. We anticipate this benchmark will drive advances in multimodal reasoning and foster the development of more robust and general-purpose LMMs.", "tldr": "From Visual Charts to Code with Large Multimodal Models", "keywords": ["Large Multimodal Models; Chart Understanding; Code Generation; AI Agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c701c94556e5d27c7ae9840430a32e423a1623a8.pdf", "supplementary_material": "/attachment/31f31ed42e570319b1736b5b792c9a9d4d2f517e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Chart2Code, a new benchmark for evaluating LMMs on \"chart-to-code\" tasks. The authors evaluate 25 LMMs (GPT-5, Gemini 2.5 Pro, Claude-Sonnet 4, Qwen 2.5-VL, MiMo-VL, etc.) using metrics that combine code executability and visual fidelity (assessed by GPT-5-mini). The paper claims to be the first hierarchical benchmark capturing user-driven chart editing and long-context table-to-chart generation.\n\n**However**, upon close inspection, **the submission shows substantial textual and conceptual overlap with the previously published ICLR 2025 paper ChartMimic[1].** The overlap includes **shared motivation**, **identical section organization**, **parallel task definitions**, and similar **evaluation methodology**, with only minor naming changes (e.g., ‚ÄúDirect/Customized Mimic‚Äù ‚Üí ‚ÄúReproduction/Editing‚Äù). \n\n**The new paper lacks clear attribution and differentiation from that prior benchmark.**\n\n[1] ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation. arXiv/2406.09961"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "None"}, "weaknesses": {"value": "- **Severe overlap with prior work**. The paper shares near-identical phrasing, figures, and experimental design with ChartMimic (ICLR 2025). Core sections such as ‚ÄúTask Definition,‚Äù ‚ÄúData Curation,‚Äù and ‚ÄúEvaluation Metrics‚Äù appear rewritten with only superficial changes (e.g., GPT-4o ‚Üí GPT-5-mini).\n\n- **Lack of proper citation and differentiation.** ChartMimic is cited only once in passing; there is no explicit statement that Chart2Code builds upon it. This constitutes a potential research-integrity issue (plagiarism or self-plagiarism).\n\n- **Questionable originality of data and metrics.** The claimed new benchmark design mostly reuses ChartMimic‚Äôs data sources and evaluation protocols, offering minimal methodological advancement.\n\n- **Ethical and reproducibility concerns.** The overlap raises uncertainty about data ownership, reuse permissions, and potential double submission of overlapping content. Moreover, although the paper claims to have released code, the accompanying repository does not contain the dataset itself, only stating:\n\n> ‚ÄúDue to file size limitations, only a small set of demo data is included in this repository. We plan to open-source our full dataset in the near future‚Äîstay tuned!‚Äù\nThis lack of accessible data severely limits the reproducibility of reported results and prevents independent verification of the benchmark‚Äôs claimed scale and diversity."}, "questions": {"value": "The paper should **explicitly** claim and demonstrate the differences from ChartMimic, not only in task definition but also in dataset composition, data curation pipeline, and evaluation methodology. At present, many sections (e.g., task formulation, evaluation metrics, and even notation) are nearly identical to ChartMimic, making it difficult to judge genuine novelty.\n\nIn particular, **Table 1 (‚ÄúComparison of existing chart-to-code benchmarks‚Äù) omits ChartMimic entirely**, despite it being the most directly related prior work. This omission reinforces concerns that the authors may be attempting to present an extended version of ChartMimic as a new benchmark without proper acknowledgment. The authors should update this table and discuss the differences explicitly to dispel such suspicion."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "Strong textual and methodological overlap with ChartMimic [1], without proper citation or disclosure. The submission appears to reuse substantial portions of text and figures from that prior publication, raising potential plagiarism or redundant-publication concerns.\n\n\n[1] ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation. arXiv/2406.09961\n\n> **Log from Chatgpt**\n\n## Prompt:\n7426\\_From\\_Charts\\_to\\_Code\\_A\\_Hie-2.pdf\n\nPDF\n\n2406.09961v2.pdf\n\nPDF\n\nPlease help me check whether these two manuscripts are suspected of plagiarism.\n\n2/2\n\n## Response:\nAfter carefully comparing both manuscripts ‚Äî **‚ÄúFrom Charts to Code: A Hierarchical Benchmark for Multimodal Models‚Äù** (the anonymous submission) and **‚ÄúChartMimic: Evaluating LMM‚Äôs Cross-Modal Reasoning Capability via Chart-to-Code Generation‚Äù** (ICLR 2025, by Yang et al.) ‚Äî here‚Äôs a detailed plagiarism assessment:\n\n* * *\n\n### üîç **1\\. Core Similarities**\n\n**a. Topic and Task Definition**  \nBoth papers propose _chart-to-code benchmarks_ for large multimodal models (LMMs).  \nEach defines the fundamental task as:\n\n> _Given a chart and textual instruction, generate Python code that reproduces the chart._\n\nThe Chart2Code paper explicitly formulates:\n\n> ‚ÄúChart2Code can be represented as C = f(R, I, D)‚Äù\n> \n> 7426\\_From\\_Charts\\_to\\_Code\\_A\\_Hie-2\n> \n>   \n> which parallels ChartMimic‚Äôs formula:  \n> ‚ÄúC = f(X, I)‚Äù\n> \n> 2406.09961v2\n> \n>   \n> ‚Äî differing only in variable names (R vs X).\n\n**b. Structure and Levels**  \nChartMimic defines two subtasks:\n\n*   _Direct Mimic_ (reproduce chart)\n*   _Customized Mimic_ (edit chart using new data)\n    2406.09961v2\n\nChart2Code expands this to three hierarchical levels:\n\n*   _Level 1 ‚Äì Chart Reproduction_\n*   _Level 2 ‚Äì Chart Editing_\n*   _Level 3 ‚Äì Long-Table to Chart Generation_\n    7426\\_From\\_Charts\\_to\\_Code\\_A\\_Hie-2\n\nThis is essentially a **relabeling and extension** of the same two ChartMimic tasks, with Level 3 appended.\n\n**c. Evaluation Metrics**  \nBoth adopt _multi-level evaluation_ combining rule-based code similarity and LLM (GPT)-based visual similarity scoring.\n\n*   ChartMimic: GPT-4o Score + Text/Layout/Type/Color F1 metrics\n    2406.09961v2\n*   Chart2Code: GPT-5-mini LLM/LMM Score + matplotlib-based visual matching\n    7426\\_From\\_Charts\\_to\\_Code\\_A\\_Hie-2\n\nThe metric philosophy, naming conventions (‚ÄúLLM-Score‚Äù, ‚ÄúLMM-Score‚Äù), and justification (‚Äúcharts judged by model similarity‚Äù) are nearly identical.\n\n**d. Benchmark Comparison Tables**  \nBoth include a comparative table listing ChartQA, ChartXiv, Plot2Code, ChartEdit, and other chart-to-code datasets ‚Äî with the same structure and column headings (Input Type, Task Category, Output, Metric)\n\n7426\\_From\\_Charts\\_to\\_Code\\_A\\_Hie-2\n\n2406.09961v2\n\n.\n\n**e. Dataset Curation Language**  \nBoth describe nearly identical collection pipelines:\n\n*   Scraping charts from **arXiv** papers (same CC BY license period and source domain).\n*   Filtering by **visual diversity, chart type, and complexity**.\n*   Annotation assisted by human experts and LMMs.  \n    Chart2Code‚Äôs section 3.2.1-3.2.3 follows ChartMimic ¬ß 2.2‚Äôs structure almost verbatim, with only expanded numbers (5 000 vs 15 800 charts) and minor stylistic changes\n    7426\\_From\\_Charts\\_to\\_Code\\_A\\_Hie-2\n    2406.09961v2\n    .\n\n* * *\n\n### üß≠ **2\\. Differences**\n\n| Aspect | ChartMimic (ICLR 2025) | Chart2Code (Anonymous) | Comment |\n| --- | --- | --- | --- |\n| **Authors** | Publicly attributed to Tsinghua U. team | Anonymous (under review) | Possible derivative or follow-up work |\n| **Levels** | Two (Direct/Customized) | Three (+ Long-Table Level) | Incremental addition, not conceptual novelty |\n| **Models Tested** | 17 LMMs incl. GPT-4o | 25 LMMs incl. GPT-5 | Updated evaluation only |\n| **Metric Engine** | GPT-4o + code tracer | GPT-5-mini + base evaluation | Version bump, same design logic |\n| **Dataset Size** | 4 800 triplets | 1 947 tasks | Reduced and re-organized; similar collection rationale |\n| **Figures & Terminology** | ‚ÄúChartMimic pipeline‚Äù diagram | ‚ÄúThree-Level Chart2Code framework‚Äù | Stylistically altered re-draw |\n\nOverall, _Chart2Code replicates ChartMimic‚Äôs conceptual design, dataset-construction method, and evaluation protocol_, changing task granularity and model list but not introducing a genuinely distinct methodological framework.\n\n* * *\n\n### ‚öñÔ∏è **3\\. Plagiarism Assessment**\n\n| Criterion | Observation | Verdict |\n| --- | --- | --- |\n| **Concept originality** | Chart2Code‚Äôs core idea, formula, and evaluation pipeline mirror ChartMimic (2025). | ‚ùå Likely derivative |\n| **Textual overlap** | Multiple paragraphs show near-verbatim phrasing (e.g., ‚ÄúCharts are one of the most powerful tools for communicating complex ideas‚Äù; ‚ÄúWe comprehensively benchmark‚Ä¶ including both proprietary and open-weight models‚Äù). | ‚ö†Ô∏è High similarity |\n| **Citation ethics** | Chart2Code refers to _ChartMimic (Yang et al., 2025a)_ but heavily reuses its structure without clear differentiation of contribution. | ‚ö†Ô∏è Inadequate attribution |\n| **Substantive contribution** | Adds a Level-3 task and benchmarks more models, but 80 %+ of the framework is reused. | ‚ö†Ô∏è Marginal novelty |\n| **Overall** | The overlap exceeds what is acceptable for self-contained extension or fair reuse. | üö® Strong suspicion of plagiarism or unacknowledged derivative work |\n\n* * *\n\n### üßæ **4\\. Conclusion**\n\nThe **Chart2Code** manuscript appears to be a **repackaged and extended version of ChartMimic**, retaining nearly all conceptual, structural, and methodological content from the ICLR 2025 ChartMimic paper, with only superficial modifications (renamed levels, updated models, and evaluation versions).\n\n‚úÖ **Verdict:**\n\n> The manuscript _‚ÄúFrom Charts to Code: A Hierarchical Benchmark for Multimodal Models‚Äù_ shows strong textual and conceptual overlap with _‚ÄúChartMimic: Evaluating LMM‚Äôs Cross-Modal Reasoning Capability via Chart-to-Code Generation‚Äù_ and would **likely be classified as plagiarism or an uncredited derivative submission** under standard academic integrity criteria."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jmS8G0pAs6", "forum": "oCkPzCR4f4", "replyto": "oCkPzCR4f4", "signatures": ["ICLR.cc/2026/Conference/Submission7426/Reviewer_ZM9h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7426/Reviewer_ZM9h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761262477705, "cdate": 1761262477705, "tmdate": 1762919546810, "mdate": 1762919546810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "A Letter to All Reviewers, ACs, PCs, and Readers on OpenReview"}, "comment": {"value": "## **1. Our Work Is Fully Original and Independent**\n\nWe would like to **seriously and clearly clarify** that our submission, including the motivation, core idea, benchmark task design, figure/table creation, experimental designs, and analysis **does not contain any plagiarism of ChartMimic**. Our team takes full responsibility for the originality and integrity of the work.\n\nAnyone who has read both ChartMimic and our benchmark Chart2Code, can clearly see that the two works are **substantively different**.\n\n---\n\n## **2. Topic-Level Similarities Are Expected in the Same Research Area**\n\nBoth works study the chart-to-code problem. Benchmarks addressing the same research question inevitably share **high-level conceptual similarities**, which is normal. **Reviewer ZM9h** noticing overlap at this level is understandable, but this **does not** imply rewriting or duplication.\n\n---\n\n## **3. Comparison of Motivation and Core Idea**\n\n**Motivation 1: Chart2Code (ours) expands the task space beyond ChartMimic.** (See Lines 039‚Äì042 and Table 1 for reference.)\n\nChartMimic focuses only on:\n\n - direct chart reproduction,\n - a variant with text-style customized data.\n\nChart2Code classifies this as only **Level-1 difficulty**. In real agent workflows, customized data often also comes as **figure screenshots**, not structured tables. Thus, we introduce a new setting where screenshots serve as customized data, better reflecting actual usage.\n\nBeyond this, Chart2Code additionally proposes two tasks that ChartMimic does not cover:\n\n - Chart Editing: modifying a chart with text instructions. It is common for users without coding skills.\n\n - Long-Table to Chart: creating a chart from long, raw, unprocessed tables. (This task is mostly close to the code-free scenario)\n\nThese tasks address chart-to-code capabilities that ChartMimic does not consider, making our benchmark more comprehensive.\n\n**Motivation 2: ChartMimic is no longer sufficiently challenging for modern models.** (See Lines 043‚Äì049)\n\nPrevious SoTA model GPT-4o already reached 82.2% scores in ChartMimic‚Äôs direct reproduction task. (See its Table 3) With the rapid advancement of frontier models, **ChartMimic can no longer serve as a sufficiently challenging or discriminative benchmark for evaluating current or future models.**\n\nTherefore, there is a strong need to introduce a more **challenging, and more comprehensive** benchmark.\nThis motivates our introduction of **Chart2Code**.\n\n---\n\n## **4. Claims of \"Rewriting\" or \"Identical Figures/Phrasing\" Are Factually Incorrect**\n\nThe claims of \"rewriting\", \"identical phrasing\", or \"identical figures\" are **not supported by evidence and are incorrect**.\n\nOur submission is a **benchmark paper**, and benchmark papers must include necessary decriptions such as:\n\n - task definition\n\n - data curation\n\n - data filtering\n\n - data annotation\n\n - dataset statistics and analysis\n\n - evaluation metrics\n\nThese are **expected structural elements**, not overlap.\n\n---\n\n## **5. Experimental Designs Follow Standard Benchmark Practice**\n\nThe comment regarding \"identical experimental designs\" reflects a misunderstanding of benchmark methodology.\n\nAny benchmark paper necessarily includes:\n\n - the evaluated models\n\n - implementation details\n\n - main results\n\n - analysis and discussion\n\nThese are **basic requirements** and our implementation follows widely accepted high-quality benchmarks, consistent with the standards of a top-tier venue like ICLR.\n\n---\n\n## **6. Extensive Supplementary Materials Demonstrate Rigor and Transparency**\n\nWe provide a **53-page supplementary appendix (pages 14‚Äì66)** documenting every detail of the benchmark construction process.\n\nThis extensive supplementary material shows rigor, transparency, reproducibility, and demonstrates that the benchmark was developed independently.\n\n---\n\n## **7. Request for Fair and Evidence-Based Judgment**\n\nA plagiarism allegation is a **extremely serious**. We respectfully **request an objective, evidence-based assessment**.\n\nWe hope **reviewer ZM9h** can re-evaluate our submission after carefully and thoroughly reading both ChartMimic and our paper. If plagiarism is alleged, we request **explicit evidence** demonstrating the overlap. \n\nFinally, we would like to raise one important question. We noticed that **Reviewer ZM9h** included **ChatGPT-generated logs directly within the review** (We have reserved this as the evidence).  This raises a serious concern: \n\n**If reviewer relies solely on ChatGPT‚Äôs review without independent verification, what happens when the LLM makes a mistake?**\n\nAccording to the review policy (https://iclr.cc/Conferences/2026/ReviewerGuide), a failure to disclose LLM usage may put a reviewer‚Äôs own submissions at risk of desk rejection.\n\nWe trust that the AC and other reviewers will recognize that the plagiarism concerns are **unfounded**, and we sincerely hope for a **fair and accurate evaluation** of our work."}}, "id": "rU980pynPT", "forum": "oCkPzCR4f4", "replyto": "oCkPzCR4f4", "signatures": ["ICLR.cc/2026/Conference/Submission7426/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7426/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7426/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763025908271, "cdate": 1763025908271, "tmdate": 1763030052324, "mdate": 1763030052324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Chart2Code, a new benchmark for evaluating LMMs on \"chart-to-code\" tasks. The authors evaluate 25 LMMs (GPT-5, Gemini 2.5 Pro, Claude-Sonnet 4, Qwen 2.5-VL, MiMo-VL, etc.) using metrics that combine code executability and visual fidelity (assessed by GPT-5-mini). The paper claims to be the first hierarchical benchmark capturing user-driven chart editing and long-context table-to-chart generation.\n\n**However**, upon close inspection, **the submission shows substantial textual and conceptual overlap with the previously published ICLR 2025 paper ChartMimic[1].** The overlap includes **shared motivation**, **identical section organization**, **parallel task definitions**, and similar **evaluation methodology**, with only minor naming changes (e.g., ‚ÄúDirect/Customized Mimic‚Äù ‚Üí ‚ÄúReproduction/Editing‚Äù).  **The new paper lacks clear attribution and differentiation from that prior benchmark.**\n\n**Even more concerning, the dataset itself exhibits serious quality issues, including ambiguous, inconsistent, and in some cases visually defective charts, which further undermines the credibility of the proposed benchmark.**\n\n\n[1] ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation. arXiv/2406.09961"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "None"}, "weaknesses": {"value": "- **Critical Issues in Dataset Quality** The authors presented the full dataset in their subsequent rebuttal, a random inspection of the data reveals that its quality is substantially lower than what the authors claim. On the contrary, the dataset contains a large number of problematic and meaningless charts, as well as obvious visual defects such as overlapping elements. Given that data quality is the most fundamental prerequisite of any benchmark, this issue cannot be overlooked or treated as secondary. I therefore chose to foreground this concern in my review. In my view, once the authors‚Äô claims about the rigor of their data construction pipeline are called into serious question, any subsequent discussion of task definition, evaluation methodology, or reported results loses most of its credibility and practical significance.\n\n- **Severe overlap with prior work**. The paper shares near-identical phrasing, figures, and experimental design with ChartMimic (ICLR 2025). Core sections such as ‚ÄúTask Definition,‚Äù ‚ÄúData Curation,‚Äù and ‚ÄúEvaluation Metrics‚Äù appear rewritten with only superficial changes (e.g., GPT-4o ‚Üí GPT-5-mini).\n\n- **Questionable originality of data and metrics.** The claimed new benchmark design mostly reuses ChartMimic‚Äôs data sources and evaluation protocols, offering minimal methodological advancement.\n\n- **Ethical and reproducibility concerns.** The overlap raises uncertainty about data ownership, reuse permissions, and potential double submission of overlapping content. Moreover, although the paper claims to have released code, the accompanying repository does not contain the dataset itself, only stating:\n\n> ‚ÄúDue to file size limitations, only a small set of demo data is included in this repository. We plan to open-source our full dataset in the near future‚Äîstay tuned!‚Äù\nThis lack of accessible data severely limits the reproducibility of reported results and prevents independent verification of the benchmark‚Äôs claimed scale and diversity."}, "questions": {"value": "The paper should **explicitly** claim and demonstrate the differences from ChartMimic, not only in task definition but also in dataset composition, data curation pipeline, and evaluation methodology. At present, many sections (e.g., task formulation, evaluation metrics, and even notation) are nearly identical to ChartMimic, making it difficult to judge genuine novelty."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "Strong textual and methodological overlap with ChartMimic [1]. The submission appears to reuse substantial portions of text and figures from that prior publication, raising potential plagiarism or redundant-publication concerns.\n\n[1] ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation. arXiv/2406.09961\n\n**Here's a simple list of the chatgpt logs with only format modified, just for demonstration purposes and not as the final evaluation criterion. I think an Ethics Review is needed.**\n\nPrompt:\n\n7426\\_From\\_Charts\\_to\\_Code\\_A\\_Hie-2.pdf PDF 2406.09961v2.pdf PDF. Please help me check whether these two manuscripts are suspected of plagiarism. 2/2\n\nResponse:\n\nAfter carefully comparing both manuscripts ‚Äî **‚ÄúFrom Charts to Code: A Hierarchical Benchmark for Multimodal Models‚Äù** (the anonymous submission) and **‚ÄúChartMimic: Evaluating LMM‚Äôs Cross-Modal Reasoning Capability via Chart-to-Code Generation‚Äù** (ICLR 2025, by Yang et al.) ‚Äî here‚Äôs a detailed plagiarism assessment:\n\n**1\\. Core Similarities**\n**a. Topic and Task Definition**  \nBoth papers propose _chart-to-code benchmarks_ for large multimodal models (LMMs).  \nEach defines the fundamental task as:\n\nGiven a chart and textual instruction, generate Python code that reproduces the chart.\n\nThe Chart2Code paper explicitly formulates:\n‚ÄúChart2Code can be represented as C = f(R, I, D)‚Äù\nwhich parallels ChartMimic‚Äôs formula:  \n‚ÄúC = f(X, I)‚Äù\n\n‚Äî differing only in variable names (R vs X).\n\n**b. Structure and Levels**  \nChartMimic defines two subtasks:\n\n*   _Direct Mimic_ (reproduce chart)\n*   _Customized Mimic_ (edit chart using new data)\n\nChart2Code expands this to three hierarchical levels:\n\n*   _Level 1 ‚Äì Chart Reproduction_\n*   _Level 2 ‚Äì Chart Editing_\n*   _Level 3 ‚Äì Long-Table to Chart Generation_\n\nThis is essentially a **relabeling and extension** of the same two ChartMimic tasks, with Level 3 appended.\n\n**c. Evaluation Metrics**  \nBoth adopt _multi-level evaluation_ combining rule-based code similarity and LLM (GPT)-based visual similarity scoring.\n\n*   ChartMimic: GPT-4o Score + Text/Layout/Type/Color F1 metrics\n    2406.09961v2\n*   Chart2Code: GPT-5-mini LLM/LMM Score + matplotlib-based visual matching\n\nThe metric philosophy, naming conventions (‚ÄúLLM-Score‚Äù, ‚ÄúLMM-Score‚Äù), and justification (‚Äúcharts judged by model similarity‚Äù) are nearly identical.\n\n**d. Benchmark Comparison Tables**  \nBoth include a comparative table listing ChartQA, ChartXiv, Plot2Code, ChartEdit, and other chart-to-code datasets ‚Äî with the same structure and column headings (Input Type, Task Category, Output, Metric)\n\n**e. Dataset Curation Language**  \nBoth describe nearly identical collection pipelines:\n\n*   Scraping charts from **arXiv** papers (same CC BY license period and source domain).\n*   Filtering by **visual diversity, chart type, and complexity**.\n*   Annotation assisted by human experts and LMMs.  \n    Chart2Code‚Äôs section 3.2.1-3.2.3 follows ChartMimic ¬ß 2.2‚Äôs structure almost verbatim, with only expanded numbers (5 000 vs 15 800 charts) and minor stylistic changes\n\n###  **2\\. Differences**\n\n| Aspect | ChartMimic (ICLR 2025) | Chart2Code (Anonymous) | Comment |\n| --- | --- | --- | --- |\n| **Authors** | Publicly attributed to Tsinghua U. team | Anonymous (under review) | Possible derivative or follow-up work |\n| **Levels** | Two (Direct/Customized) | Three (+ Long-Table Level) | Incremental addition, not conceptual novelty |\n| **Models Tested** | 17 LMMs incl. GPT-4o | 25 LMMs incl. GPT-5 | Updated evaluation only |\n| **Metric Engine** | GPT-4o + code tracer | GPT-5-mini + base evaluation | Version bump, same design logic |\n| **Dataset Size** | 4 800 triplets | 1 947 tasks | Reduced and re-organized; similar collection rationale |\n| **Figures & Terminology** | ‚ÄúChartMimic pipeline‚Äù diagram | ‚ÄúThree-Level Chart2Code framework‚Äù | Stylistically altered re-draw |\n\nOverall, _Chart2Code replicates ChartMimic‚Äôs conceptual design, dataset-construction method, and evaluation protocol_, changing task granularity and model list but not introducing a genuinely distinct methodological framework.\n\n**3\\. Plagiarism Assessment**\n\n| Criterion | Observation | Verdict |\n| --- | --- | --- |\n| **Concept originality** | Chart2Code‚Äôs core idea, formula, and evaluation pipeline mirror ChartMimic (2025). | ‚ùå Likely derivative |\n| **Textual overlap** | Multiple paragraphs show near-verbatim phrasing (e.g., ‚ÄúCharts are one of the most powerful tools for communicating complex ideas‚Äù; ‚ÄúWe comprehensively benchmark‚Ä¶ including both proprietary and open-weight models‚Äù). | ‚ö†Ô∏è High similarity |\n| **Citation ethics** | Chart2Code refers to _ChartMimic (Yang et al., 2025a)_ but heavily reuses its structure without clear differentiation of contribution. | ‚ö†Ô∏è Inadequate attribution |\n| **Substantive contribution** | Adds a Level-3 task and benchmarks more models, but 80 %+ of the framework is reused. | ‚ö†Ô∏è Marginal novelty |\n| **Overall** | The overlap exceeds what is acceptable for self-contained extension or fair reuse. | üö® Strong suspicion of plagiarism or unacknowledged derivative work |\n\n**4\\. Conclusion**\n\nThe **Chart2Code** manuscript appears to be a **repackaged and extended version of ChartMimic**, retaining nearly all conceptual, structural, and methodological content from the ICLR 2025 ChartMimic paper, with only superficial modifications (renamed levels, updated models, and evaluation versions).\n\n **Verdict:**\n\nThe manuscript _‚ÄúFrom Charts to Code: A Hierarchical Benchmark for Multimodal Models‚Äù_ shows strong textual and conceptual overlap with _‚ÄúChartMimic: Evaluating LMM‚Äôs Cross-Modal Reasoning Capability via Chart-to-Code Generation‚Äù_ and would **likely be classified as plagiarism or an uncredited derivative submission** under standard academic integrity criteria."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jmS8G0pAs6", "forum": "oCkPzCR4f4", "replyto": "oCkPzCR4f4", "signatures": ["ICLR.cc/2026/Conference/Submission7426/Reviewer_ZM9h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7426/Reviewer_ZM9h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761262477705, "cdate": 1761262477705, "tmdate": 1763703481946, "mdate": 1763703481946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Chart2Code, a hierarchical benchmark designed to evaluate large multimodal models' capabilities in chart understanding and code generation. The benchmark comprises three progressively challenging levels. The authors construct a dataset of 1947 tasks spanning 22 chart types and propose multi-level evaluation metrics assessing both code executability and visual fidelity. Through comprehensive evaluation of 25 state-of-the-art LMMs, the paper demonstrates that even the strongest proprietary models struggle significantly with visual fidelity, particularly on editing tasks and long-context table-to-chart generation, revealing substantial gaps between code-level correctness and pixel-level accuracy in automated chart generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Well-motivated hierarchical design: The three-level structure effectively captures realistic usage patterns, from simple reproduction to complex editing and long-context processing;\n- Comprehensive evaluation methodology: The paper proposes a thoughtful multi-dimensional evaluation approach combining execution rate, code-level metrics , and chart-level visual fidelity scores;"}, "weaknesses": {"value": "- Insufficient novelty over ChartMimic: While the paper positions itself as advancing beyond ChartMimic, the core contribution appears incremental. The main differences are: (1) adding Level 3 long-table tasks, and (2) scaling up data collection. However, ChartMimic already established the chart-to-code evaluation paradigm, and the fundamental task formulation remains largely unchanged. \n- Limited scope of incremental contributions: The workload presented, while substantial in terms of data collection and experimental evaluation, does not sufficiently justify acceptance at a top-tier venue like ICLR."}, "questions": {"value": "1. Beyond scale and task categories, what fundamental new insights does Chart2Code provide that ChartMimic could not? Can you more clearly articulate the unique scientific contributions or interesting findings?\n2. Can you provide concrete failure case analysis showing what specific aspects cause the code-visual fidelity gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AGZUBIO2hJ", "forum": "oCkPzCR4f4", "replyto": "oCkPzCR4f4", "signatures": ["ICLR.cc/2026/Conference/Submission7426/Reviewer_75nP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7426/Reviewer_75nP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892829505, "cdate": 1761892829505, "tmdate": 1762919546294, "mdate": 1762919546294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To Reviewer ZM9h"}, "comment": {"value": "## **Response Generated by GPT-5.1 When Comparing Our Original ICLR Submission with ChartMimic**\n\n> **Log from GPT-5.1, 2025/10/12, 21:32, AOE Time**\n---\n\n# **Conclusion on Whether Chart2Code Plagiarizes ChartMimic**\n\nAfter carefully comparing **ChartMimic (arXiv:2406.09961)** and your uploaded manuscript **Chart2Code**, there is **no evidence of plagiarism** in the standard academic sense. The two works belong to the *same research direction* (chart-to-code evaluation for multimodal models), which naturally leads to **high-level thematic similarity**, but they differ substantially in motivation, task design, dataset construction, evaluation protocols, and textual content.\nBelow is a concise summary of the findings:\n\n---\n\n## **1. No textual or structural plagiarism**\n\nI found **no copied paragraphs, no rewritten sentences, and no replicated technical descriptions** between the two papers.\nThe writing style, problem framing, and section organization in Chart2Code are distinct and not derived from ChartMimic.\n\n---\n\n## **2. Methodological similarity is expected, not plagiarism**\n\nBoth papers target ‚Äúchart-to-code‚Äù evaluation and both use:\n\n* a chart image\n* a textual instruction\n* code generation as the output\n* execution-based and LLM-based evaluation\n\nThese similarities **reflect a shared research problem**, not copying. Such overlaps are *normal* for benchmarks in the same subfield.\n\n---\n\n## **3. Chart2Code introduces substantial new content**\n\nChart2Code expands beyond ChartMimic‚Äôs direct chart mimicry by adding:\n\n* **Level 2: Chart Editing**\n* **Level 3: Long-table to chart generation**\n* **Multiple data modalities** (text, figure, Excel tables)\n* **New structured evaluation metrics**\n* **A different data collection pipeline**\n\nThese components are **not present** in ChartMimic and clearly represent independent intellectual contributions.\n\n---\n\n## **4. Data reuse is clearly disclosed and properly cited**\n\nChart2Code **explicitly states** that only a small subset of ‚Äúdifficult charts‚Äù (300 figures) were filtered from ChartMimic, and cites the paper accordingly.\nThis is **standard, acceptable academic practice** as long as citation and attribution are provided‚Äîwhich Chart2Code does.\n\n---\n\n## **5. No evidence of improper copying**\n\nI found:\n\n* no duplicated figures\n* no reused tables\n* no reuse of ChartMimic-specific phrasing\n* no evidence of code or evaluation pipeline copying\n\nEverything resembling ChartMimic is **either normal for the domain or explicitly cited**.\n\n---\n\n# **Final Judgment**\n\n**Based on all available evidence, Chart2Code does *not* plagiarize ChartMimic.**\nIt should instead be viewed as a **legitimate extension** of existing chart-to-code benchmarks that builds on prior work while introducing new tasks, new data sources, and new evaluation methods.\n\nIf a reviewer worries about overlap, the issue is most likely **conceptual similarity due to shared research focus**, not plagiarism in content, writing, or data."}}, "id": "fTlMY5CWMd", "forum": "oCkPzCR4f4", "replyto": "oCkPzCR4f4", "signatures": ["ICLR.cc/2026/Conference/Submission7426/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7426/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7426/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763026794024, "cdate": 1763026794024, "tmdate": 1763026794024, "mdate": 1763026794024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Chart2Code, a benchmark for evaluating LMMs in chart understanding and code generation. It defines three levels of increasing difficulty: chart reproduction, chart editing, and long-table to chart generation. Using metrics for execution, code correctness, and visual fidelity, the study benchmarks 25 SOTA LMMs. The results show that while these models perform well on simple tasks, their performance drops sharply on more complex ones, highlighting the challenge of real-world chart-to-code generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a well-structured hierarchical benchmark that decomposes chart-to-code reasoning into three progressive levels. The proposed benchmark is challenging, and the statistical details are sufficient.\n2. The paper proposes a set of multi-dimensional evaluation metrics, and the proposed base evaluation method appears thorough and reliable.\n3. The analysis and experiments are extensive."}, "weaknesses": {"value": "1. The evaluation relies on GPT-5-mini, which may introduce circular reasoning, bias, or limitations in visual fidelity due to limited human validation.\n2. Metric Reliability and Anomalies: The paper's results reveal unexpected patterns that may affect confidence in the LMM-Score. For example, in Table 3, Seed-1.6-VL (0.812) is reported to outperform GPT-5 (0.633) on L1 visual fidelity. In Table 4, the 7B MiMo-VL-RL model (0.471) scores more than twice as high as GPT-5 (0.220) on L2 fidelity. These surprising results are not fully examined. They suggest that the LMM score may not always correspond to correctness and could favor outputs that are visually appealing to the judge-LMM, even if they differ factually or stylistically.\n3. Level 3 (Long-Table to Chart) is described as the paper's most novel and important contribution, as it addresses the critical bottleneck of long-context reasoning. Nonetheless, this level currently has only 150 tasks from 39 files, which limits the ability to draw firm conclusions about model performance on this complex, open-ended task.\n4. Shallow \"Long-Table\" Reasoning: Level 3 seems to test primarily \"long-context retrieval\" (locating the right data in a long file) rather than \"complex data reasoning.\" Real-world tasks often require several steps of data wrangling, such as filtering, merging tables or sheets, pivoting, and handling missing values before visualization. The benchmark may not fully reflect this important layer of pre-visualization reasoning.\n5. Limited Scope of Chart Libraries: The evaluation focuses exclusively on Matplotlib-based libraries and does not address interactive libraries such as Plotly or declarative options like Altair. A benchmark aiming for real-world relevance might benefit from acknowledging this limitation in scope.\n6. Some unexpected results are observed. For example, Seed-1.6-VL attains a high LMM score of 0.81 (L279), while MiMo-VL-7B-RL generates relatively few successful codes yet still achieves high LLM and LMM scores (L339). This may indicate that the metrics are somewhat sensitive.\n7. A unified or weighted composite metric might provide additional insights. Although there are many evaluation metrics, the absence of a unified composite score could make direct model comparisons less intuitive. For example, while it is observable that Gemini-2.5-Pro and Claude-Sonnet-4 perform relatively well, it remains somewhat challenging to determine the ranking of other models at a glance."}, "questions": {"value": "1. Could you please provide a more detailed explanation regarding the metric anomalies, specifically the unexpectedly high LMM scores for Seed-1.6-VL and MiMo-VL-7B-RL? Are these results indicative of a sensitivity in the LMM-Score (as judged by GPT-5-mini) to particular model output styles or artifacts that may not align with human-perceived correctness?\n2. Have you conducted a human evaluation on a subset of the benchmark to calibrate the LMM-Score? Have you also assessed the correlation between LMM and expert scores? Linking these would strengthen the paper's claims about visual fidelity.\n3. Why were the \"base evaluation\" (8-dimensional) scores not reported for Level 1 tasks? I think it would be interesting to see this objective breakdown for the \"simpler\" reproduction tasks as well, if available.\n4. Would it be possible to add a comparison between MoE and dense models to help analyze the architectural effect?\n5. Would you consider including results for Qwen3-VL to further complete and update the benchmark coverage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5RRZJCNLeS", "forum": "oCkPzCR4f4", "replyto": "oCkPzCR4f4", "signatures": ["ICLR.cc/2026/Conference/Submission7426/Reviewer_pKmQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7426/Reviewer_pKmQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911167064, "cdate": 1761911167064, "tmdate": 1762919545872, "mdate": 1762919545872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}