{"id": "y8vwDwygeM", "number": 12997, "cdate": 1758212566169, "mdate": 1759897471812, "content": {"title": "Eliminating Agentic Workflow for Introduction Generation with Parametric Stage Tokens", "abstract": "In recent years, using predefined agentic workflows to guide large language models (LLMs) for literature classification and review has become a research focus. However, writing research introductions is more challenging. It requires rigorous logic, coherent structure, and abstract summarization. Existing workflows often suffer from long reasoning chains, error accumulation, and reduced textual coherence. To address these limitations, we propose eliminating external agentic workflows. Instead, we directly parameterize their logical structure into the LLM. This allows the generation of a complete introduction in a single inference. To this end, we introduce the Stage Token for Introduction Generation (STIG). STIG converts the multiple stages of the original workflow into explicit stage signals. These signals guide the model to follow different logical roles and functions during generation. Through instruction tuning, the model learns the mapping between stage tokens and text functions. It also learns the logical order and transition patterns between stages, encoding this knowledge into the model parameters. Experimental results show that STIG can generate multi-stage text in a single inference. It does not require explicit workflow calls. STIG outperforms traditional agentic workflows and other baselines on metrics of semantic similarity and sentence-level structural rationality. The code is provided in the Supplementary Materials.", "tldr": "", "keywords": ["Large Language Model; Agentic Workflow; Scientific Introduction"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a4203d1d4a69a8a7f3a1993d7602625c14760796.pdf", "supplementary_material": "/attachment/22ce11fda8c238e65fd7dd67a1f95c35c5e9e556.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes STIG (Stage Token for Introduction Generation), a model that eliminates external agentic workflows in scientific paper introduction generation by parameterizing writing logic into LLMs via 8 stage token pairs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Provides an incisive, task-specific analysis of key limitations in academic introduction generation ,such as cascading errors and high computational overhead of traditional agentic workflows."}, "weaknesses": {"value": "- Methodological novelty is insufficient. STIG merely transforms agentic workflows into single-step inference via training, sacrificing flexibility and generalization without achieving significant improvements in generation quality.  \n- SS and NQ fail to fully demonstrate the method’s advantages, raising doubts about the reliability of semantic similarity to the original text as a metric; GPT-2-derived perplexity for NQ lacks robustness, and sampling perplexity across multiple models plus human expert calibration are recommended.  \n- Experimental models and datasets are limited. Only small open-source models are tested, with no verification of adaptability to models of different parameter sizes, limiting result generalizability."}, "questions": {"value": "- Why is the no-citation constraint adopted in experiments? The rationale for this exclusion needs further explanation.\n- For baselines like Pure Prompt/ELABORATE Prompt, why were their original paper model settings not adopted? Why not compare with baselines on closed-source/large-parameter models (e.g GPT-4), as small open-source models may not reflect real baseline performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ImlJmGj6Yc", "forum": "y8vwDwygeM", "replyto": "y8vwDwygeM", "signatures": ["ICLR.cc/2026/Conference/Submission12997/Reviewer_Nvdz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12997/Reviewer_Nvdz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761715733253, "cdate": 1761715733253, "tmdate": 1762923745071, "mdate": 1762923745071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a pipeline (workflow) that employs a parametric stage-token–based prompting strategy on a fine-tuned STIG LLM for generating academic paper introductions. Experiments are conducted on a new dataset derived from 3,800 ACL papers, annotated with multi-stage writing structures corresponding to different rhetorical functions. By integrating workflow logic into trainable stage tokens, STIG unifies agentic reasoning and fine-tuned modeling within one pipeline. According to the reported results, it achieves higher semantic and structural quality with greater token efficiency than prompt-based and multi-agent baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper introduces an original idea that embeds workflow logic directly into model parameters via parametric stage tokens.\n\nS2. This approach reduces multi-agent dependency and improves inference efficiency in structured text generation.\n\nS3. The dataset of 3,800 ACL papers is large and well-structured. It represents a meaningful contribution for future research in the field.\n\nS4. Five multi-dimensional evaluation metrics comprehensively capture semantic, structural, and narrative quality.\n\nS5. Figures and examples illustrate the pipeline clearly, and the paper is generally well written and easy to follow."}, "weaknesses": {"value": "W1. The paper lacks clarity in distinguishing between the STIG framework and the STIG fine-tuned model.\nWhile the conclusion claims STIG eliminates agentic workflows, the framework still depends on them for data construction and stage definition.\n\nW2. Fine-tuning details are incomplete. No hyperparameter settings, training configurations, or sensitivity analyses are reported. \n\nW3. A hyperparameter study is essential to confirm the stability of stage-token fine-tuning. All five evaluation metrics are newly proposed, but no external validation or human correlation study is provided.\n\nW4. The main table and ablation experiments include too few baseline models. This makes comparisons less comprehensive.\n\nW5. It is unclear if the proposed approach can be generalized to other academic domains rather than ACL papers."}, "questions": {"value": "Q1. Is it possible to include an experiment or visualization that analyzes the internal weighting or influence of each stage token?\n\nQ2. Is it possible to add a hyperparameter study on the number of stages (e.g., four vs. eight) to understand whether performance depends on workflow granularity?\n\nQ3. Is it possible to expand the literature review to include prior research on LLM agentic workflows for paper writing? Current discussion on “LLM agents” and “LLMs for writing” is a bit general. \n\nQ4. Is it possible to conduct experiments with closed-source models (e.g., GPT-4 or Claude) on the new dataset and metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N7RcyJ1haF", "forum": "y8vwDwygeM", "replyto": "y8vwDwygeM", "signatures": ["ICLR.cc/2026/Conference/Submission12997/Reviewer_cWQU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12997/Reviewer_cWQU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913008529, "cdate": 1761913008529, "tmdate": 1762923744615, "mdate": 1762923744615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes STIG, a single-inference method that replaces multi-turn “agentic” pipelines for writing the Introduction section of CS papers. Instead of orchestrating separate agents sequentially, STIG embeds stage tokens directly into the LLM’s parameter space via supervised fine-tuning. An 8-stage token scheme forces the model to emit Background-outline, Background-content, …, Contributions-content in single decode. Trained on 3.8 k ACL papers and evaluated on 1.2 k ACL-2025 test papers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. STIG outperforms several training-free agentic baselines (AutoSurvey, Outline-Writing) in structural rationality, content coverage, while using fewer tokens.\n\n2. First work to parameterise an entire writing workflow into stage tokens. \n\n3. Contribute a customized dataset tailored for training and testing introduction generation, derived from over 3,800 ACL main conference papers."}, "weaknesses": {"value": "1. Trained only on ACL NLP papers; no CV, Theory or other domains. Claims “research introductions” but evidence is NLP-only (ACL).\n\n2.  The eight stage tokens are defined for the four subsections that appear tailored to research-style papers; however, ACL also contains many dataset papers whose introductions do not necessarily follow the Background–Problem & Limitations–Method & Experimental Results &  Contributions structure. \n\n3. The 'SR' metric is aligned with STIG’s own staged structure, making the comparison appear unfair."}, "questions": {"value": "1. Although stage-by-stage agentic workflows may suffer from error accumulation, they allow targeted evaluation at each stage, which can improve the final output. STIG's end-to-end generation seems to leave no room for intermediate refinement?\n\nOther questions refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rrbhDtl7Da", "forum": "y8vwDwygeM", "replyto": "y8vwDwygeM", "signatures": ["ICLR.cc/2026/Conference/Submission12997/Reviewer_YdqD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12997/Reviewer_YdqD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981882363, "cdate": 1761981882363, "tmdate": 1762923744306, "mdate": 1762923744306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Stage Token for Introduction Generation (STIG) to automatically write research introductions and eliminate the external agentic workflows. STIG converts multiple stages of the original workflow into explicit stage signals so that it can generate multi-stage text in a single inference. STIG uses the title, abstract, description of figures, description and table contents, and the abstracts of baseline references to guide structured output. To train the STIG model, the paper construct a dataset from ACL main conference papers. STIG outperforms traditional agentic workflow and other baselines on automatic evaluation metrics like semantic similarity and structural rationality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes STIG, a method that can combine multi-stage agentic workflow generation of research writing into a single inference pass.\n- The paper constructs a high-quality dataset from over 3,800 scientific papers from ACL main conferences, utilizing MinerU, GPT-4o, and the Semantic Scholar API."}, "weaknesses": {"value": "The evaluation metrics are insufficient. The reliance on automated metrics without human validation means we don't actually know if STIG produces good introductions. We only know that it produces text that scores well on these specific metrics. Furthermore, for scientific writing, one of the most important metrics that you should consider evaluating on is the factual accuracy (whether the claims in the introduction is accurate, whether there is fabricated content, etc). Furthermore, I am not sure BERTScore and perplexity from GPT-2 models are suitable for evaluating this task, because BERTScore may not capture long-form coherence needed for introduction writing that well, and GPT-2 is a very outdated model. (I do appreciate the example generations of the STIG models and AutoSurvey models in the appendix.)"}, "questions": {"value": "- Figure 3 seems a bit abrupt at the context. If possible, I strongly recommend you put a good example of STIG model’s generation here, as the purpose for this paper is to promote STIG, not criticize AutoSurvey.\n- Did you validate the quality of GPT-4o annotations?\n- Can you provide human evaluation results? Even a small-scale study (e.g., 50 papers rated by domain experts) would significantly strengthen the claims about generation quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XDgdqGpjOF", "forum": "y8vwDwygeM", "replyto": "y8vwDwygeM", "signatures": ["ICLR.cc/2026/Conference/Submission12997/Reviewer_gcqh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12997/Reviewer_gcqh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076373896, "cdate": 1762076373896, "tmdate": 1762923743871, "mdate": 1762923743871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}