{"id": "yu148f4aXm", "number": 20981, "cdate": 1758312340131, "mdate": 1759896948579, "content": {"title": "Universal Ordering for Efficient PAC Learning", "abstract": "We initiate the study of the \\emph{universal ordering} problem within the PAC learning framework: given a set of $n$ samples independently drawn from an unknown distribution $\\mathcal{D}$, can we order these samples such that every prefix of length $k \\le n$ yields a near-optimal subset for training a PAC learner? \nThis question is fundamentally motivated by practical scenarios involving incremental learning and adaptive computation, where guarantees must hold uniformly across varying data budgets. \nWe formalize this requirement as achieving anytime-valid PAC guarantees. \nAs a warm-up, we analyze the simple random ordering baseline using classical concentration inequalities. \nThrough a careful union bound over a geometric partitioning of prefixes, we establish that it provides a surprisingly strong universal guarantee, incurring at most an $O(\\log\\log n)$ overhead compared to a random subset of size $k$. \nWe then present a more powerful analysis based on the theory of test martingales and Ville's inequality, demonstrating that a random permutation achieves PAC guarantees for all prefixes that match the statistical rate of a random subset of size $k$, without the logarithmic overhead incurred by naive union-bound techniques. \nOur work establishes a conceptual bridge between universal learning on fixed datasets and the broader field of sequential analysis, revealing that random permutations are efficient and provably robust anytime-valid learners but opening the door to further improvements.", "tldr": "", "keywords": ["learning theory", "data ordering", "PAC learning"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1bd83bc6806be7c7302eccb06fb26c6e1d22a72e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Let H be a class of finite VC dimension and D be a realizable distribution. Standard PAC bounds imply that if we take a uniformly random sample S_n from D of length n, then with probability 1 - delta, every S_n-consistent hypothesis will have D-error at most O(log(1/delta)/n).\n\nIn the same setting, let S_k denote the k-length prefix of S_n for k = 1, ..., n.\n\nThe paper asks: for which sequences {eps_k}_{k = 1} one can show that with probability 1 - delta, for every k = 1, ..., n, we have that all S_k-consistent hypotheses have D-error at most eps_k?\n\nA trivial observation is that we can guarantee that for eps_k = (log n + log(1/delta)/k (I'm skipping O()-notation for better readibility) by applying standard bounds for every k = 1, ..., n with delta' = delta/n.\n\nA slightly less trivial but also a simple observation implies that instead of the additional log n term in the numerator, one can get log log n term. Indeed, let's use the standard bound for k that are powers of 2 and with delta' = delta/log n. By the union bound, we have with probability 1 - delta that for every k = 1, ..., n which is a power of 2, every S_k-consistent hypothesis has D-error at most eps_k = (log log n + log(1/delta)/k. What about non-powers of 2? If k is not a power of 2, take k_0 to the biggest power of 2, smaller than k. Observe that all S_k-consistent hypotheses are also S_{k_0}-consistent, which means that they all have D-error at most eps_{k_0} \\le 2 eps_k.\n\nThe main result of the paper is that this bound on eps_k is also true with no additional term that depend on n at all -- using martingale theory."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper motivates this problem by dynamic learning -- where the size of the data set is now known in advance. We might want to have good pac guarantees for all possible prefixes of the sample set (as good as they can get for such prefix size)."}, "weaknesses": {"value": "Most of the paper is mathematically confusing and needs careful re-writing (although results about log n and log log n overhead seem simple exercises and are restorable, see Summary).\n\nDefinition 3.4 is mathematically unclear. It is an ordering of what? What is the quantification over A? To be honest, it is not clear which object is defined to be ``Universally PAC-valid''. What makes more sense is the formulation from the Summary, so we can define something like ``a sequence {eps_k} of errors is universally pac-achiebable with delta if with probability 1 - delta...''\n\nIn Theorem 4.2, 4.3, what is eps? Why bounds on the error have n in the numerator, which makes that larger than 1? In the proof of Theorem 4.3 you don't need Lemma 4.2, see a simpler proof in the Summary section.\n\nI did not understand the proof of Theorem 5.1. Already in the definition H_0^h, H_1^h, these are random events, this is either true or false for a given hypothesis h, so what sense does it have to consider conditional probability involving it? The definition of M_k does not seem to depend on D, how do you deduce that M_k is large if some S_k-consistent hypothesis has large D-error?\n\nI think this paper has some potential, but it definitely needs a thorough rewritting."}, "questions": {"value": "no questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nawQHEai5K", "forum": "yu148f4aXm", "replyto": "yu148f4aXm", "signatures": ["ICLR.cc/2026/Conference/Submission20981/Reviewer_RSnn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20981/Reviewer_RSnn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761246899958, "cdate": 1761246899958, "tmdate": 1762999977541, "mdate": 1762999977541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the universal ordering problem in the PAC learning framework. The central question is whether, given $n$ i.i.d. samples from an unknown distribution, one can order these samples so that every prefix of length $k \\leq n$ forms a near-optimal subset for PAC learning. This can help with more efficient use of data in computation and memory constrained settings. The authors formalize this requirement using anytime-valid inference techniques and study how random permutations of the dataset can provide such guarantees. The authors first analyse a naive union-bound argument, showing that random orderings incur a  $O(\\log n)$ overhead for boosting the success probability. This is later improved to only a $O(\\log \\log n)$ overhead, since the authors exploit the fact that training on similar length prefixes (when their ratio is a small constant) is highly correlated and therefore suffices to only ensure a good performance at prefix lengths that grow exponentially. \nFor their main result, they leverage martingale-based techniques and Ville’s inequality to eliminate this overhead entirely, demonstrating that random permutations achieve optimal PAC rates uniformly across all prefix sizes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper formalizes a previously unexplored yet practically relevant problem and provides a starting point for its theoretical analysis. Conceptually, the paper builds a bridge between universal data ordering, sequential analysis, and safe anytime-valid inference, showing that random shuffling is not only convenient but also efficient."}, "weaknesses": {"value": "However, the paper’s focus is limited to random orderings, leaving important practical and algorithmic questions open. Furthermore, even though the stated motivation is to find near optimal orderings, this is not reflected in the results, as no lower bounds are provided to support that. Presentation wise, I would like to see more details of the paper’s contributions/results in the introduction."}, "questions": {"value": "- Do you conjecture that using random permutations is actually optimal, or could a deterministic or data-dependent ordering achieve strictly tighter bounds?\n- The analysis assumes i.i.d. data and a consistent learner in the realizable setting. How sensitive are the results to label noise, covariate shift, or approximate consistency? Could similar anytime-valid results be extended to the agnostic PAC setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SCUzrqrZqm", "forum": "yu148f4aXm", "replyto": "yu148f4aXm", "signatures": ["ICLR.cc/2026/Conference/Submission20981/Reviewer_5LRS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20981/Reviewer_5LRS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762246837172, "cdate": 1762246837172, "tmdate": 1762999977508, "mdate": 1762999977508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Rebuttal to All Reviewers"}, "comment": {"value": "We thank the reviewers for their insightful comments. We here address the common concern that examining the uniformly random ordering is insufficient. \nSpecifically, we here showcase that under the \\emph{task-agnostic} condition (1) a random ordering must always be better than a deterministic ordering, and (2) the uniformly random ordering achieves the optimal error against an adversary.\nWe formalize the notion of a task-agnostic ordering via a minimax game: the ordering algorithm must fix its distribution over permutations before the adversary selects the critical samples which reveal the true concept.\n\n**Random vs. Deterministic Ordering:** We first show that a deterministic data ordering is easily exploited and cannot be task-agnostic (and furthermore universally PAC valid).\nIt is intuitive that any committed to deterministic strategy can be exploited by an adversary that exactly counters this strategy.\nWe formalize this intuition as follows:\n\nLet $\\mathcal{A_det}$ be an algorithm for deterministic ordering of a dataset.\nWe proceed to construct a task which has maximal loss for the algorithm.\nLet $\\mathcal{H_adv} = \\{h_0, h_1\\}$ where $h_0(x) = 0$ and $h_1(x) = 1$ for all $x \\in \\mathcal{X}$ (a simple PAC-learnable hypothesis class with \\texttt{VC} dimension 1).\nFurthermore let $\\mathcal{X} = \\{x_A, x_B\\}$ where $x_A$ has label 0 and $x_B$ has label 1.\nAssume without loss of generality that $\\mathcal{A_det}$ sorts all instances of $x_A$ before $x_B$.\nWe then construct the adversarial task such that $h_1$ is the true hypothesis but is hard to discover: for $\\epsilon \\in (0, \\frac12)$ define the distribution $\\mathcal{D_adv}$ by $\\mathbf{Pr}(x_B) = 1-\\epsilon$ and $\\mathbf{Pr}(x_A) = \\epsilon$.\nNote that the true hypothesis would predict 1 for $x_B$ and 0 for $x_A$, but within the hypothesis class $\\mathcal{H_adv}$ the only hypothesis with error at most $\\epsilon$ is $h_1$.\nThe error of $h_0 = 1-\\epsilon$.\nNow, assume we have a consistent learner who draws an $n$ sample set from $\\mathcal{D_adv}$.\nLet $E$ be the event that this sample set contains at least one $x_A$ sample: $\\mathbf{Pr}[E] = 1 - (1-\\epsilon)^n$ which tends to 1 as $n \\rightarrow \\infty$.\nConditional on $E$, we have that the sampled set contains $k \\ge 1$ instances of $x_A$ and $n-k$ of $x_B$.\nThe algorithm applied to this set produces an ordering where the first $k$ elements are $x_A$. \nFor the prefixes up to $k+1$, the learner only sees samples with label 0 and returns hypothesis $h_0$.\nTherefore, the deterministic algorithm is easily exploited to achieve maximal error on the prefix and we have the result (note that we can reverse the task / algorithm wlog).\n\n**Optimality of Uniformly Random:** Further proving that the uniform random ordering is the unique optimal strategy relies on a similar construction and intuition.\n\nLet $S_k$ be the set of indices in the $k$-sized prefix of a dataset.\nFor any randomized ordering algorithm (which forms a distribution over the space of permutations $\\mathcal{P}$), the expected number of elements in $S_k$ must be exactly $k$.\nBy linearity of expectations, we have\n$$\\sum_{i=1}^n \\mathbf{Pr}_{\\pi \\sim \\mathcal{P}}[i \\in S_k] = \\mathbf{Ex}[|S_k|] = k$$\nwhich implies that the average inclusion probability for any data point in the set of size $k$ is $k/n$, regardless of the algorithm.\nSimilar to above, we consider the adversary who places a critical sample, $x^*$, (which fully reveals the hypothesis) at the index $i$ which minimizes the inclusion probability. The worst-case success probability is then given by: $\\arg\\min_i \\mathbf{Pr}[i \\in S_k]$.\nMoreover, because the average is fixed at $k/n$, this minimum is maximized if and only if all such probabilities are equal\n$$\\min_i \\mathbf{Pr}[i \\in S_k] \\le \\frac{k}{n}$$\nEquality holds iff the algorithm assigns equal marginal probability to every $i \\in [n]$.\nSince any non-uniform distribution must have some index $j$ such that $\\mathbf{Pr}[j \\in S_k] < k/n$, the adversary can exploit and increase the failure rate.\nThus, the uniformly random ordering is the unique best strategy.\n\n**We intend to include formalized versions of these results in the final version of our paper to better motivate the study. We thank the reviewers for their concerns as they lead us to these results!**"}}, "id": "zvQwt7tSQC", "forum": "yu148f4aXm", "replyto": "yu148f4aXm", "signatures": ["ICLR.cc/2026/Conference/Submission20981/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20981/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20981/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763667088697, "cdate": 1763667088697, "tmdate": 1763667088697, "mdate": 1763667088697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes and studies a problem called the ``universal ordering'' problem that is defined under the framework of classical PAC learning. That is, while traditional PAC learning characterizes the sample complexity as a fixed size of samples drawn from an unknown distribution, universal ordering considers a given large sample set and aims to achieve PAC guarantees on any prefix $k$ samples. The paper shows that random permutation can achieve this goal and provide satisfying universal guarantees. The paper first establish a baseline analysis of union bound on concentration inequalities and proves a $O(\\log\\log n)$ overhead. It then introduces a supermartingale analysis on the sequence of data that avoids taking union bound on all $n$ prefixes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes a very interesting problem of studying universal guarantees under the PAC learning framework, which is conceptually related to other fields of sequential analysis and safe testing. It could be of interest in many real-world applications when data sets are obtained and fixed, while the budget for learning is varying from time to time. It also provides a theoretical guarantee for the method of random permutation for data sets, which may often be used as a data preparation step in learning tasks."}, "weaknesses": {"value": "Comparing to its conceptual contribution and the statistical analysis for the universal learning guarantees, the technical contribution seems not so compelling. The traditional PAC learning theory already assumes i.i.d. sampling from the underlying distribution, hence, any $k$ leading samples are already satisfying the universal ordering criteria. Given this, a random permutation seems just a renascent of the i.i.d. assumption of PAC learning. From this perspective, the study could be more application driven, i.e. many real-world data sequences lack the necessary randomness in it, and random permutation is a valid resolution for inserting such randomness, benefiting learning guarantees."}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IqAqEHtweq", "forum": "yu148f4aXm", "replyto": "yu148f4aXm", "signatures": ["ICLR.cc/2026/Conference/Submission20981/Reviewer_Uvtz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20981/Reviewer_Uvtz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762415630956, "cdate": 1762415630956, "tmdate": 1762999977522, "mdate": 1762999977522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the novel **universal ordering** problem, which asks whether $n$ i.i.d. samples from an unknown distribution $\\mathcal D$ can be arrange in a fixed sequence so that every prefix subsequence of length $k \\geq n$ forms an approximately optimal subset for PAC learning. Using two different arguments, it establishes baseline results for the case when the samples are randomly arranged and the hypothesis class is either finite, or it has a finite VC dimension. The second argument (based on Ville's inequality) yields a strictly better bound than the first argument (based on a union bound). Both bounds provide PAC guarantees in comparison to the optimal statistical rate of a random subset of size $k$."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed **universal ordering problem** is a novel and potentially significant idea: arrange the training data so that we can directly compare models that fit any $k$-prefix of the dataset against each other.\n2. The second argument establishes a technique to work with randomly arranged data using anytime-valid inference and demonstrates that it allow us to obtain stronger results than using union bounds.\n3. The paper is clear and concise. The proofs are well-written and seem to be correct."}, "weaknesses": {"value": "1. Despite the promising premise of ordering training data in an optimal manner, the paper only deals with the case when we're arranging them in a uniformly random order.\n2. The paper lacks a demonstration, be it theoretical or empirical, of how models training with different dataset sizes can be compared against each other."}, "questions": {"value": "1. The statements of Theorem 4.1, 4.3, 4.4 should precisely state which terms are being referenced by the word *error*.\n2. There should be a brief explanation on why the bound on $\\epsilon_k$ on line 566 leads to the bound in Theorem 4.1. A similar suggestion can be made to the proof of Theorems 4.3.\n3. Assuming i.i.d. and finite training data, is there any way to arrange them that would meaningfully differ from a uniformly random arrangement? If the answer to the question is no then the paper would benefit from taking into consideration the non i.i.d. training data scenario.\n4. Given that random permutations already achieve such a strong universal guarantee (achieving the optimal statistical rate), what could be expected from another permutation of the training data? Why do you think this ordering is unlikely to be optimal?\n5. How was Figure 2 created?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LDNJ791c57", "forum": "yu148f4aXm", "replyto": "yu148f4aXm", "signatures": ["ICLR.cc/2026/Conference/Submission20981/Reviewer_F5au"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20981/Reviewer_F5au"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762744457249, "cdate": 1762744457249, "tmdate": 1762999977457, "mdate": 1762999977457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduced the universal ordering problem within the PAC learning framework. As a baseline, they analyzed random orderings using standard techniques. They then refined this analysis via Ville’s inequality, which yields a tighter bound that unexpectedly removes all logarithmic factors."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents original contributions.\n- While I have not verified every proof in full detail, the arguments appear correct.\n- I specifically like the Ville’s inequality idea.\n- The paper is well-written and easy to follow. I particularly appreciated the step-by-step progression of improvements.\n- The paper includes a comprehensive discussion section."}, "weaknesses": {"value": "- The main concern I have is that I’m not convinced of the importance of the problem. In particular, while I understand it on a mathematical level, the stated motivations did not resonate with me.\n- The scope of the paper is limited by its exclusive focus on random orderings, which leaves algorithmic aspects unexplored. In a setting where the goal is to minimize the expected error, random ordering is clearly optimal. The work would be substantially more impactful if the high-probability optimal ordering differed from the random ordering.\n- The discussion of related works could be strengthened. For example, the following paper appears relevant: https://arxiv.org/pdf/2202.05246"}, "questions": {"value": "- Could you elaborate on the key motivations for studying this problem?\n- Is there any reason to consider non-random orderings in your formulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AMO7xN78lj", "forum": "yu148f4aXm", "replyto": "yu148f4aXm", "signatures": ["ICLR.cc/2026/Conference/Submission20981/Reviewer_A7ii"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20981/Reviewer_A7ii"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20981/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763062137480, "cdate": 1763062137480, "tmdate": 1763062137480, "mdate": 1763062137480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}