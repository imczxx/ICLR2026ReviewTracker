{"id": "mh5388TZJ4", "number": 8813, "cdate": 1758098984895, "mdate": 1763003640760, "content": {"title": "ASO-LoRA: Attribution Scores-based Soft Orthogonality Low-Rank Adaptation for Large Language Model Continual Learning", "abstract": "Continual learning (CL) remains a critical challenge when applying large language models (LLMs) to real-world situations. On the one hand, billions of parameters for LLMs add a huge computing overhead to CL. Existing techniques, on the other hand, solely address catastrophic forgetting while ignoring the possibility of knowledge transfer between tasks. Facing these challenges, we propose Attribution Scores-based Soft Orthogonality Low-Rank Adaptation (ASO-LoRA), an effective and efficient framework that simultaneously facilitates knowledge transfer while mitigating catastrophic forgetting. Specifically, ASO-LoRA initially assigns task-specific parameter subspaces for new tasks utilizing multi-LoRA modules, enabling for efficient training and inference without relying on task labels. Then, ASO-LoRA leverages attribution scores to evaluate task similarity and suggests gradient steps in a soft orthogonal direction between task-specific subspaces, achieving a balance between knowledge transfer and preservation. Experiments are carried out on both the T5-large and the LLaMA2-7B, showing ASO-LoRA's suitability as a plug-in CL solution for general Transformer-based LLMs. Experimental results on CL benchmarks demonstrate that ASO-LoRA outperforms other strong baselines.", "tldr": "Attribution Scores-based Soft Orthogonality Low-Rank Adaptation (ASO-LoRA), an effective and efficient framework for Continual Learning", "keywords": ["Parameter-efficient-training", "LLM Efficiency", "Low-rank adaptation", "Soft Orthogonality"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/80f52f13efb75e1f2ba6f412587c74af228b3875.pdf", "supplementary_material": "/attachment/20828a4eb3726e15f48e741ff994625b62c3abb8.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose **Attribution Scores-based Soft Orthogonality Low-Rank Adaptation (ASO-LoRA)**, a method for continual learning. In a multi-LoRA adapter setting, ASO-LoRA utilizes attribution scores to evaluate task similarity. These scores are based on knowledge neurons: when a specific neuron contributes strongly to task-specific knowledge, its attribution score is high. Task similarity is then expressed through the similarity between their attribution score vectors. Gradient steps are subsequently taken in a softly orthogonal direction relative to task-specific subspaces.\n\nThe method is evaluated on short- and long-sequence continual learning benchmarks across T5-Large and LLaMA-2-7B. Additionally, the authors analyze knowledge transfer, forgetting, and output similarity."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper explores an idea based on attribution-guided orthogonality which is a relevant direction.\n\n---\n\n- The paper includes a nice and diverse set of baselines."}, "weaknesses": {"value": "### **1. Writing Quality and Clarity**\n\n- Numerous typos, formatting issues, and grammatical mistakes.\n- Overall poor writing quality makes the paper difficult to evaluate.\n- The methodology section is poorly structured and hard to follow.\n- The introduction mixes LoRA and multi-LoRA, creating confusion.\n- “continual” vs. “continuous” learning\n\n---\n\n### **2. Missing or Insufficient Definitions**\n\n- “Multi-LoRA” is introduced without explanation.\n- It is unclear what a knowledge neuron is.\n- FM and FWT are not explained nor is interpretation guidance provided.\n\n---\n\n### **3. Methodological Concerns**\n\n- The method essentially resembles adjusted O-LoRA.\n- The method requires storing many adapters, raising scalability concerns.\n- The proposed method sometimes outperforms what should be an upper bound, unclear why.\n- Comparison methods are not actually run, breaking comparability.\n\n---\n\n### **4. Related Work Weaknesses**\n\n- Related work section includes irrelevant content, it resembles more a background section.\n- It is not well connected back to the proposed method.\n\n---\n\n### **5. Figures and Presentation Issues**\n\n- Several plots appear stretched with distorted aspect ratios.\n- Figure labels and legend text are too small to read. They are sometimes missing or incomplete.\n- Figure 3 is unclear about what “current task” means.\n- Figure 2’s attribution score is not explained in the description of the figure."}, "questions": {"value": "1. Why does the forward transfer reduce with tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7O8hnmVomZ", "forum": "mh5388TZJ4", "replyto": "mh5388TZJ4", "signatures": ["ICLR.cc/2026/Conference/Submission8813/Reviewer_HTa5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8813/Reviewer_HTa5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747640180, "cdate": 1761747640180, "tmdate": 1762920582233, "mdate": 1762920582233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Z978we1U0q", "forum": "mh5388TZJ4", "replyto": "mh5388TZJ4", "signatures": ["ICLR.cc/2026/Conference/Submission8813/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8813/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763003640072, "cdate": 1763003640072, "tmdate": 1763003640072, "mdate": 1763003640072, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a LoRA-based continual learning method for LLMs, ASO-LoRA, which addresses the trade-off between forgetting mitigation as well as knowledge transfer. It computes attribution scores to evaluate task similarity during training and uses the scores as the loss regularization term. The mechanism makes the new task LoRA learning in the soft orthogonal directions between task-specific subspaces, balancing the knowledge transfer and preservation. Experimental results show that ASO-LoRA outperforms other strong baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. ASO-LoRA considers both mitigating forgetting and knowledge transfer, different from some existing continual learning, which only focuses on mitigating forgetting.\n\n2. The paper uses two different kinds of models to conduct experiments."}, "weaknesses": {"value": "1. It’s not clear for computing attribution scores in which training phase. From the statement in line 249, does it mean the interval of computing attribution scores during training is m=20? If so, the computation of ASO-LoRA is very high during training. ASO-LoRA is not efficient for LLMs.\n\n2. The motivation of the proposed method, ASO-LoRA, is not well explored, and some statements lack supporting analysis. For example, in line 180, “We hypothesize that the relationship between LoRA blocks is not strictly orthogonal”, which lacks the reason for discussing this. Also, in line 183, it lacks an empirical or mathematical explanation for why the dynamic soft orthogonality coefficient decreases as more knowledge is shared between LoRAs.”\n\n\n3. Experiments only utilize classification tasks and single metric average accuracy, which are simple for llama models. For example, in the cited paper, SAPT utilizes SuperNI benchmark, including generation tasks, to evaluate the performance. Furthermore, the ablation study is not sufficient, for example, scaling across different ranks for LoRA, scaling across different learning rates, scaling across different $\\lambda$ in the loss function, scaling across different approximation steps, etc. The paper does not show how the performance of ASO-LoRA varies in different settings.\n\n4. ASO-LoRA is not memory-efficient since it linearly grows with the number of sequential tasks.\n\n5. The related works section is too short and lacks analysis.\n\n6. Mathematical expressions in the paper lack consistency, for example, Eq (4) and Eq.(4) are both used."}, "questions": {"value": "1. Why does ASO-LoRA regard A as the core of the task-related subspace and regard B as the linear weighting coefficient of A? Is there any theoretical analysis or strong explanation to support this statement?\n\n2. Why does ASO-LoRA fix the approximation steps at 20? Why does ASO-LoRA not show the performance when choosing different steps? Can authors show the results of scaling across different approximation steps?\n\n3. How to distinguish ASO-LoRA and O-LoRA? Since O-LoRA also utilizes $\\lambda$ to control the orthogonality between A subspaces, which could be regarded as soft orthogonality. Does this $\\lambda$ not represent the degree of orthogonality? Can authors explain Figure 3 more clearly? It seems like the average range of attribution scores is from 0.1 to 0.3. What if fixing the attribution scores in this range, like 0.2, when computing training loss? It may reduce most computation during training.\n\n4. In Eq (13), what’s the definition of the “Sim” function? What’s the mathematical expression?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2NCrDcjHU8", "forum": "mh5388TZJ4", "replyto": "mh5388TZJ4", "signatures": ["ICLR.cc/2026/Conference/Submission8813/Reviewer_bvfs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8813/Reviewer_bvfs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921849359, "cdate": 1761921849359, "tmdate": 1762920581838, "mdate": 1762920581838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ASO-LoRA (Attribution Scores-based Soft Orthogonality Low-Rank Adaptation), a parameter-efficient framework built on multi-LoRA modules. ASO-LoRA first assigns task-specific low-rank subspaces (via separate LoRA blocks) to new tasks, keeping the LLM’s base weights frozen for efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. ASO-LoRA addresses a key limitation of hard orthogonality methods (e.g., O-LoRA) by using attribution scores to dynamically adjust subspace overlap. This enables intentional knowledge transfer between similar tasks.\n\n2. As a plug-in framework, ASO-LoRA adds no extra trainable parameters beyond multi-LoRA modules."}, "weaknesses": {"value": "1. The paper uses integrated gradients (with Riemann approximation) to compute attribution scores for knowledge neurons but provides no justification for critical choices—e.g., why 20 approximation steps (m=20) or how noise in gradient estimates affects score reliability. \n\n2. Key hyperparameters are set without empirical explanation. The paper does not test how λ affects the balance between transfer and forgetting, nor why Spearman’s correlation is preferred over other similarity metrics (e.g., Pearson’s).  \n\n3. Only evaluates classification tasks (sentiment, topic, NLI, QA). It does not test complex tasks like open-ended generation (summarization, dialogue) or reasoning (math, logic), where token-level frequency cues may behave differently.\n\n4. Only tested on T5 and LLaMA2. It is unknown if ASO-LoRA works for newer models (e.g., LLaMA3-8B, Qwen3-8B).\n\n5. In some cases, ASO-LoRA is worse than the baseline O-LoRA.\n\n6. The writing can be improved."}, "questions": {"value": "1. How sensitive is ASO-LoRA to the number of Riemann approximation steps (m) for attribution scores? Would increasing m improve score reliability, or introduce unnecessary computational cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V8jumYbeNc", "forum": "mh5388TZJ4", "replyto": "mh5388TZJ4", "signatures": ["ICLR.cc/2026/Conference/Submission8813/Reviewer_46in"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8813/Reviewer_46in"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004521959, "cdate": 1762004521959, "tmdate": 1762920581457, "mdate": 1762920581457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ASO-LoRA, a parameter-efficient continual learning framework for large language models that mitigates catastrophic forgetting while enabling knowledge transfer. By using multiple LoRA modules to assign task-specific subspaces and leveraging attribution scores to guide soft orthogonal gradient updates, ASO-LoRA achieves efficient adaptation without task labels and outperforms existing methods on standard benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The investigated problem of continual learning using low-rank adaptation is important.\n\n2. The experimental evaluation incorporates a wide range of baselines."}, "weaknesses": {"value": "1. The writing quality of the paper requires substantial improvement, as it contains many instances of non-standard usage. For example, not every equation should have a label, and each formula line must be followed by appropriate punctuation. The notation should also follow standard conventions, such as using `\\log` instead of *log*, and `\\operatorname{Sim}()` instead of *Sim()*.\n\n2. The notation in the paper is not clearly defined. For instance, what does $\\psi_{t, \\mathcal{T}}^{\\text {soft }}$ represent? Is it a scalar or a matrix?\n\n3. The improvements over previous works appear to be minor, as shown in Table 1. Moreover, no standard deviation is provided, and after conducting a t-test, the proposed method may not be significantly superior to others."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hAcoPaP8tN", "forum": "mh5388TZJ4", "replyto": "mh5388TZJ4", "signatures": ["ICLR.cc/2026/Conference/Submission8813/Reviewer_pyRx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8813/Reviewer_pyRx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762411322685, "cdate": 1762411322685, "tmdate": 1762920581001, "mdate": 1762920581001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}