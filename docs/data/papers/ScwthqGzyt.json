{"id": "ScwthqGzyt", "number": 21428, "cdate": 1758317480391, "mdate": 1759896922455, "content": {"title": "LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization", "abstract": "Retrieval-augmented generation (RAG) has become a popular approach to improving large language models (LLMs), yet trustworthiness remains a central challenge: models may produce fluent but incorrect answers, and retrieved context can amplify errors when irrelevant or misleading. To address this, we study how model internals reflect the interplay between parametric knowledge and external context during generation. Specifically, we ask: (1) can the correctness of a model’s output be inferred directly from its internal activations, and (2) do these internals reveal whether external context is helpful, harmful, or irrelevant? We introduce metrics grounded in intermediate activations to capture both dimensions. Across six models, a simple classifier trained on hidden states of the first output token predicts output correctness with nearly 75% accuracy, enabling early auditing. Moreover, our internals-based metric substantially outperforms prompting baselines at distinguishing between correct and incorrect context, guarding against polluted retrieval. These findings highlight model activations as a promising lens for understanding and improving the reliability of RAG systems.", "tldr": "The paper presents methods to predict the correctness of LLM outputs and the efficacy of external context by analyzing model-internal activations, enabling early detection of errors and better use of retrieval-augmented generation.", "keywords": ["Calibration", "Uncertainty", "Knowledge Tracing", "Probing"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a40c790ce88acf71906026d2b74294bd59afb44b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates whether LLMs internal activations can be used to predict the correctness of generated outputs and to assess the utility of external context. The authors utilize a simple classifiers trained on different types of inputs to predict correctness of the model predictions and show that they can predict correctness with about 75% accuracy. Further, they estimate the efficacy of external context\nalong w.r.t. correctness and relevancy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Simple and effective approach to generate signal about the correctness in generative tasks.\n\nIntroducing new metric for external context validation based on internal states.\n\nThe methodology is carefully described and clear."}, "weaknesses": {"value": "I think table 1 adds nothing useful. Besides, to me it seems obvious that LogitLens, TunedLens, and HiddenStates should have close performance since they are all different transformation of the hidden states. Also since there is no specific preference between these three maybe you could just keep one and bring the others in the Appendix as further exploration of different settings.\n\nPlease consider modifying the axes ticks and labels in Fig 3 to 6"}, "questions": {"value": "I was wondering whether it is possible to directly using the FFN or Attn outputs instead of HiddenStates or PKS etc.\n\nDid you analyse the sensitivity of the results to the parameter $\\lambda$?\n\nDid you try more complex classifiers to check if you can further improve the truthfulness detection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7pmHw1h1KG", "forum": "ScwthqGzyt", "replyto": "ScwthqGzyt", "signatures": ["ICLR.cc/2026/Conference/Submission21428/Reviewer_6JK6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21428/Reviewer_6JK6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761543099695, "cdate": 1761543099695, "tmdate": 1762941766133, "mdate": 1762941766133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper develops techniques to use model internals (hidden states, activations, etc.) to predict if a model’s answers are correct and assess the utility of retrieval-augmented context. They study these techniques on multiple different models and show their approach outperforms prompted evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and develops techniques that improve on prompted baselines to assess answer correctness and context utility. They apply this technique to several different small open-source models for TriviaQA and MMLU."}, "weaknesses": {"value": "While developing calibrated measures of model truthfulness is a critical space, there are multiple existing works that train classifiers on top of different model internals to assess answer correctness. It is unclear if this paper is adding something truly novel to the large existing body of work in this space. Several of these papers are referenced in the related work, but this paper does not benchmark their proposed method against these prior works. Additionally, although the baseline of prompting models to evaluate their correctness may not work well for the model sizes evaluated in the paper (<=13B), prompting alone may be sufficient for larger models (> 32B). Also, it is unclear if new classifiers need to be trained for each model, dataset pair to assess correctness – if so this may reduce the adoption of this technique. Overall the paper is suggesting a method that may not be too distinct from prior works and might not be relevant for models beyond a given size."}, "questions": {"value": "- section 2: Some more related works: Language Models Can Predict Their Own Behavior (https://arxiv.org/pdf/2502.13329), LLM-Check: Investigating Detection of Hallucinations in Large Language Models (https://proceedings.neurips.cc/paper_files/paper/2024/file/3c1e1fdf305195cd620c118aaa9717ad-Paper-Conference.pdf), Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs (https://aclanthology.org/2025.acl-long.304.pdf)\n\n- section 2: Your related work mentions several other “open-box internal-state approaches”. Beyond simple prompting techniques it would be good to include a few of these as baselines.\n\n- section 3.3 line 161: why are the hidden states taken from only the first token position?\n\n- section 3.4 line 168: why does confidence correlate with parametric knowledge used – provide more intuitions?\n\n- section 4.2 line 239-240: strong alignment between the context tokens and generated answer doesn’t necessarily mean that the model is relying more on the context than on parametric knowledge – parametric knowledge itself may also align with the answer\n\n- section 5 line 264: demonstrating this approach on MMLU and TriviaQA is a good starting point – adding 1-2 more datasets being more actively used in current literature such as GPQA would make the results more impactful.\n\n- section 5 line 341: does a separate classifier need to be trained for every model / dataset pair? Can we train a single classifier that could work across several datasets for a given model? Doing so could make this technique more useful.\n\n- section 5 line 322: Other works have shown that larger language models tend to be more calibrated than smaller language models. It’s possible that your prompting baselines may work well for larger models even though they don’t work for your scale. Experimenting with a larger model size (e.g. 32B+) could clarify this.\n\n- section 6.1 line 398: why is PKS predictive of correctness for open-ended QA and not MCQ?\n\n- section 8: Is there a single method that works well across models and datasets for assessing answer correctness and context utility? Explicitly noting this would be valuable for practitioners."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SHoa0fBJQy", "forum": "ScwthqGzyt", "replyto": "ScwthqGzyt", "signatures": ["ICLR.cc/2026/Conference/Submission21428/Reviewer_3wuj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21428/Reviewer_3wuj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903796408, "cdate": 1761903796408, "tmdate": 1762941764936, "mdate": 1762941764936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LLM Microscope to predict whether a language models answer will be correct using internal activations alone and whether external content is helpful or harmful. For the first hypothesis they train light weight classifier over features derived from intermediate representations Logit-Lens statistics, first-token hidden states, and a Parametric Knowledge Score.\nFor the second hypothesis (regarding the external content) they introduce an internals based proxy for contextual log-likelihood gain by combining an External Context Score via a scaling parameter.\nThe authors show experiment across multiple models and show correctness can be predicted via internals at 75% accuracy and a high AUC, and internal based signals outperform prompting when distinguishing correct vs incorrect context."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper clearly formulates the problem and cleanly formulates contextual log likelihood gain and relative utility and instanties an internal based proxy for context efficacy. Definitions and rationale are explicit. \n2) Random forests over per-layer logit lens statistics and first token hidden states is a well setup experiment and yield strong correctness predictions with analysis of feature importance and layer-wise trends. The observation that internal layers are as or more predictive than the final layer is compelling and practically useful for early auditing. \n3) The authors perform extensive experiments that covers 6 models. Table 2 is important and shows internals-based features consistently outperform prompting baselines."}, "weaknesses": {"value": "1) So the “Incorrect” contexts are produced by prompting GPT-4o to replace mentions of the gold answer with plausible but wrong alternatives, whereas “correct” contexts are sometimes GPT-4o-summarized to 500 tokens. This could introduce stylistic artifacts that models might pick up rather than genuine semantic correction signals. More diagnostics are needed to show that performance improvements are not due to such artifacts, such analysis is missing in the paper and is critical.\n2) Many generative answers extend beyond a single token and while the paper acknowledges this in Limitations, most RQ1 features are computed at the first output token and some RQ2 averages are token-averaged but layer-averaged later."}, "questions": {"value": "1) There is some inconsistency in the TriviaQA counts, in the experimental setup is states it retains 6557 questions but in D.1 the paper says quality filter yields 11683 examples."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hQtawhdo9N", "forum": "ScwthqGzyt", "replyto": "ScwthqGzyt", "signatures": ["ICLR.cc/2026/Conference/Submission21428/Reviewer_Q3yN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21428/Reviewer_Q3yN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965858924, "cdate": 1761965858924, "tmdate": 1762941763738, "mdate": 1762941763738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides an empirical study of how model internal states and other signals can be used for predicting: (i) if the model output is correct; and (ii) how much it relies on the external context. The results show that internal hidden states are effective at predicting correctness, and that features derived from external context matching and FFN activation strength are predictive of context relevance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The experiments are designed well and important aspects related to understanding LLM behaviors. The results are well presented and show clear trends in support of the claims made in the paper.\n- The paper does a good job of bringing together multiple lines of research around mechanistic interpretability, confidence elicitation from LLMs, contextual faithfulness and factuality. The findings presented are useful for driving further research in these areas."}, "weaknesses": {"value": "- All techniques in the paper are borrowed from prior works -- ECS and PKS scores (Sun et al, 2025), predicting factuality from hidden states (Azaria & Mitchell, 2023), verbalized confidence (Kadavath et al, 2022), Logit lens and Tuned lens. While the paper does a good job of contrasting and comparing them, it doesn't make any new methodological contributions.\n- Consequently, a lot of the main conclusions in the paper are already well known -- e.g., that hidden states are predictive of factuality, ECS and PKS scores are effective at measuring context vs parametric knowledge utilization.\n- Dealing with incorrect / conflicting contexts is a rich area with lots of papers. The paper misses out discussion of some important techniques, e.g., from ICLR 2025 [1].\n\n[1] Huang, Yukun, et al. \"To Trust or Not to Trust? Enhancing Large Language Models' Situated Faithfulness to External Contexts.\" The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "- Can you explain how the findings related to RQ1 here are different from those of Azaria & Mitchell (2023)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jDHMOwgEax", "forum": "ScwthqGzyt", "replyto": "ScwthqGzyt", "signatures": ["ICLR.cc/2026/Conference/Submission21428/Reviewer_p55B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21428/Reviewer_p55B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762293727269, "cdate": 1762293727269, "tmdate": 1762941762872, "mdate": 1762941762872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}