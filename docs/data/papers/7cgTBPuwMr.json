{"id": "7cgTBPuwMr", "number": 16065, "cdate": 1758259394383, "mdate": 1759897264489, "content": {"title": "Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Reward Design", "abstract": "This paper investigates approaches to enhance the reasoning capabilities of Large Language Model (LLM) agents using Reinforcement Learning (RL). Specifically, we focus on long-horizon multi-turn agent scenarios, which can be naturally modeled as Markov Decision Processes. Although RL algorithms such as Group Relative Policy Optimization (GRPO) and Proximal Policy Optimization (PPO) have been widely applied to train multi-turn LLM agents, they typically rely only on a sparse final reward and lack dense intermediate signals across multiple decision steps, limiting their performance on complex reasoning tasks. To bridge this gap, we propose a \\textit{turn-level reward design} strategy to enhance RL algorithms in multi-turn agent tasks. By integrating turn-level rewards, we extend GRPO and PPO to their respective multi-turn variants, enabling fine-grained credit assignment. We conduct case studies on multi-turn reasoning-augmented search agents, where we carefully design two types of turn-level rewards: verifiable and LLM-as-judge. Our experiments on multi-turn search tasks demonstrate that incorporating well-designed turn-level rewards enables RL algorithms to significantly outperform baseline methods with outcome-level rewards. Both training and validation reward curves illustrate that our method achieves \\textit{greater stability}, \\textit{faster convergence}, and \\textit{higher accuracy}. Numerical results across diverse question-answering datasets further show that our approach consistently delivers highest answer correctness and 100\\% format correctness.", "tldr": "", "keywords": ["multi-turn interaction", "reinforcement learning", "LLM Agent"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f20b8e83994757d523fbc5ef3487d666b2d036b0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "They propose a turn-level reward design strategy to enhance RL algorithms in multi-turn agent tasks. By integrating turn-level rewards, they extend GRPO and PPO to their respective multi-turn variants, enabling fine-grained credit assignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The algorithm is well clarified with a specific case study.\n- This paper studies a fundamental problem for multi-turn RL -- the use of turn-level reward.\n- The MT versions of PPO and GRPO show better performance compared to their counterparts: PPO and GRPO."}, "weaknesses": {"value": "- Lack of theoretical support.\n- Limited Baselines for Comparison: To provide a more comprehensive evaluation, additional baselines should be included, such as GRPO or PPO augmented with intrinsic rewards. The current comparisons are restricted to open-source LLMs and ablated variants of the algorithm, which may not fully benchmark the approach against state-of-the-art reinforcement learning methods in similar domains.\n- Omission of Concurrent Works: The discussion should address relevant concurrent research, such as the work on \"Context-lite Multi-turn Reinforcement Learning for LLM Agents,\" to highlight how the proposed method differentiates itself or builds upon these efforts."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FeRSUU8tG7", "forum": "7cgTBPuwMr", "replyto": "7cgTBPuwMr", "signatures": ["ICLR.cc/2026/Conference/Submission16065/Reviewer_JWKY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16065/Reviewer_JWKY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761449967405, "cdate": 1761449967405, "tmdate": 1762926254077, "mdate": 1762926254077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a turn-level reward design framework to improve reinforcement learning for multi-turn LLM agents. The authors extend GRPO and PPO into multi-turn variants (MT-GRPO and MT-PPO) that integrate intermediate rewards to enable finer credit assignment across reasoning steps. They evaluate the approach on search-based QA tasks using both verifiable and LLM-as-judge rewards. Experimental results with Qwen2.5-7B show that MT-PPO achieves more stable training, faster convergence, and better format correctness than PPO and GRPO. The paper highlights turn-level reward design as a promising direction for long-horizon agent training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper tackles an important problem: improving multi-turn reasoning in LLM agents through better reward shaping\n2. The distinction between single-turn and multi-turn MDP formulations is well presented and conceptually sound.\n3. The paper is clearly written and easy to follow, with consistent notation and illustrative examples."}, "weaknesses": {"value": "1. The main contribution, introducing turn-level rewards into PPO/GRPO, is conceptually straightforward and closely related to prior work on process reward models (PRM) and segment-level credit assignment. The paper overstates its originality by claiming to be the “first systematic study” without adequately discussing/comparing with these prior methods.\n2. The experiments are limited to search-based QA tasks, leaving it unclear whether the proposed framework generalizes to other multi-turn or open-ended domains such as code generation, dialogue, or planning.\n3. The reported improvement in answer accuracy (approximately +1.5% over PPO) is relatively modest compared to the additional implementation complexity required for designing and tuning intermediate rewards.\n4. The motivation for introducing MT-GRPO is weakly justified, and its evaluation is minimal.\n5. Novelty concern: Much of the paper’s content overlaps with existing work, and the contributions are largely incremental."}, "questions": {"value": "1. How does MT-PPO performance compare to prior PRM or step-level RL work?\n2. How sensitive are results to the chosen reward weights (retrieval, format, search penalty)?\n3. What is the computational overhead of MT-PPO relative to standard PPO (in terms of runtime, tokens, or sample efficiency)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o0mg8oZ6pR", "forum": "7cgTBPuwMr", "replyto": "7cgTBPuwMr", "signatures": ["ICLR.cc/2026/Conference/Submission16065/Reviewer_SxgQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16065/Reviewer_SxgQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768483582, "cdate": 1761768483582, "tmdate": 1762926253766, "mdate": 1762926253766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of training LLM agents for complex, multi-turn tasks using RL, proposing a novel turn-level reward design strategy. This strategy provides fine-grained credit assignment by rewarding the agent at each step of its multi-turn interaction. The authors extend both GRPO and PPO into multi-turn variants and conduct case studies on a reasoning-augmented search agent. Experiments demonstrate that their method achieves greater training stability, faster convergence, and higher accuracy compared to baseline methods that use only outcome-level rewards."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- 1.The work identifies and systematically tackles a fundamental flaw in applying RL to multi-turn LLM agents: the credit assignment problem. By shifting from sparse, end-of-task rewards to dense, turn-level rewards, the method provides the agent with much richer and more immediate feedback, which is crucial for learning complex sequences of actions.\n\n- 2.The paper offers a detailed and practical framework for designing turn-level rewards, which is a significant contribution. It introduces two distinct types of rewards: verifiable rewards and LLM-as-judge rewards. This dual approach ensures both precision and flexibility in guiding the agent's behavior. The authors not only create a multi-turn variant of GRPO but also develop MT-PPO to overcome MT-GRPO's computational limitations.\n\n- 3.Experiments on multiple question-answering datasets consistently show that their approach leads to more stable training, faster convergence, and superior performance in both answer correctness and output format adherence compared to strong baselines."}, "weaknesses": {"value": "- 1. **High Computational Complexity**: The proposed MT-GRPO method requires exponential trajectory samples, making it infeasible for long-horizon tasks. While MT-PPO reduces this cost via a critic model, it still introduces additional training overhead.\n\n- 2. **Fixed-Turn Constraint Limits Flexibility**: MT-GRPO mandates all rollout groups to have the same number of turns, enforced through system prompts. This rigid structure hinders adaptability to dynamic scenarios where tasks may require variable interaction lengths.\n\n- 3. **Reward Design Relies on Heuristic Priors**: Turn-level rewards are manually tuned without theoretical justification. This risks reward hacking and limits generalizability. The LLM-as-Judge approach also inherits biases from the judge model."}, "questions": {"value": "- 1. Is there a more reliable and theoretically-grounded method for reward design that enables adaptation to different tasks?\n- 2. The experiments focus on structured search tasks with clear turn boundaries. How would the method perform in less structured environments, such as open-ended dialogue or collaborative planning, where turns may involve unpredictable state transitions or partial observability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eaBb6QNtZ0", "forum": "7cgTBPuwMr", "replyto": "7cgTBPuwMr", "signatures": ["ICLR.cc/2026/Conference/Submission16065/Reviewer_CsNb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16065/Reviewer_CsNb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798215604, "cdate": 1761798215604, "tmdate": 1762926253454, "mdate": 1762926253454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}