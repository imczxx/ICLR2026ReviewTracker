{"id": "5TBemc6p5i", "number": 19591, "cdate": 1758297512475, "mdate": 1759897031400, "content": {"title": "NoMod: A Non-modular Attack on Module Learning With Errors", "abstract": "The advent of quantum computing threatens classical public-key cryptography, motivating NIST's adoption of post-quantum schemes such as those based on the Module Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a hybrid white-box cryptanalytic method that circumvents the challenge of modeling modular reduction by treating wrap-arounds as statistical corruption and casting secret recovery as robust linear estimation. Our approach combines optimized lattice preprocessing—including reduced-vector saving and algebraic amplification—with robust estimators trained via Tukey's Biweight loss. Experiments show NoMod achieves full recovery of binary secrets for dimension $n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful recovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) = (128, 3)$ and $(256, 2)$. We release our implementation in an anonymous repository https://anonymous.4open.science/r/NoMod-3BD4.", "tldr": "", "keywords": ["Learning with errors", "Modulo", "Neural network", "Outlier"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e6b4d94975b71e3536a20c6a5ae6c018b45d602.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles Module-LWE with a clever idea: treat modular wraparound as statistical noise rather than trying to learn it, and then use robust linear regression to extract the secret. On paper, that sounds different from SALSA, and the experimental numbers do look better (way fewer samples, way less compute). But the problem is that practically nothing revolutionary is actually happening here. Yeah, the modulus-as-error interpretation is a neat conceptual angle, but once you look at what's really going on under the hood, it's mostly just tweaking existing lattice reduction techniques—vector saving across BKZ tours, automorphism amplification, that kind of thing—which are solid engineering contributions but not exactly groundbreaking. The real killer, though, is that the entire experiment setup uses artificially sparse secrets that Kyber doesn't actually use; the real ML-KEM standard uses unconstrained CBD distributions, so this attack doesn't threaten actual deployed cryptography. You can frame this as \"security margin analysis,\" and that's actually a legitimate thing to do, but it's not the same as breaking the real system. The fundamental issue is that this feels like a cryptanalysis paper pretending to be machine learning research. There's no novel learning algorithm here, no theoretical advance, just domain-specific engineering applied to a relaxed problem variant. If this were submitted to crypto venues, it'd probably get an accept, or at least a borderline accept, because it's genuinely useful for understanding LWE's hardness assumptions and parameter selection, and for clever lattice preprocessing tricks applied to theoretical variants of problems that don't reflect real systems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's vector saving strategy across multiple BKZ tours and the automorphism amplification trick are good engineering moves that noticeably improve sample efficiency and computational cost. The experimental evaluation is thorough and well-executed, with comparisons against SALSA variants showing consistent improvements across different parameter regimes. The white-box linear recovery approach is conceptually cleaner than black-box transformer methods, providing immediate interpretability of the learned weights. Most importantly, even if the real-world threat to actual Kyber is limited, this work genuinely advances our understanding of LWE's security margins and provides a useful tool for cryptographers to evaluate parameter choices and post-standardization robustness."}, "weaknesses": {"value": "The most noticeable problem is that this paper has no real machine learning contribution to speak of. There's no novel learning theory, no algorithmic innovation that would matter to the ML community, just standard robust regression applied to lattice preprocessing, which makes it fundamentally misaligned with ICLR's scope. The experimental setup is built on a fiction: all the impressive results attack sparse secrets that actual Kyber doesn't use; real ML-KEM employs unconstrained CBD distributions, so this attack doesn't touch standardized cryptography and amounts to analyzing security margins on a theoretical variant rather than breaking anything real. While treating modular wraparound as statistical noise is conceptually different from SALSA's approach, the actual technical differences beyond that are hard to spot—the vector-saving and automorphism-amplification tricks are solid engineering but incremental optimizations of known lattice-reduction techniques, not breakthroughs. The paper basically says, \"if Kyber used sparse secrets like textbook LWE examples, we could attack it faster,\" which is useful for understanding hardness assumptions but doesn't constitute a meaningful cryptographic break. Fundamentally, this is a competent cryptanalysis paper masquerading as machine learning research, and it deserves to be at a cryptography conference."}, "questions": {"value": "If the method proposed in this paper were to attack real Kyber KEM parameters, what would be the approximate complexity in terms of time? If you had to guess roughly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YDIb4AbNB8", "forum": "5TBemc6p5i", "replyto": "5TBemc6p5i", "signatures": ["ICLR.cc/2026/Conference/Submission19591/Reviewer_KbXn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19591/Reviewer_KbXn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953144962, "cdate": 1761953144962, "tmdate": 1762931460450, "mdate": 1762931460450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a new attack on the Module Learning with Errors problem, a problem that is the basis for standardized post-quantum cryptographic schemes. \n* The “NoMod” ML-attack re-frames the problem into a noisy, but linear domain to enable efficient secret recovery through lattice reduction and regression.\n* The approach includes optimizations to preprocessing (including progressive BKZ, vector saving strategies, and amplifying a small set of reduced samples) with robust linear regression and Tukey’s Biweight loss. \n* The results show that the regression can recover dense binary secrets for n=350 and sparse secret recovery for CRYSTALS-Kyber parameters, improving upon prior work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper conducts comprehensive evaluations and comparisons to state-of-the-art AI attacks (SALSA, PICANTE, VERDE, FRESCA) on many parameter settings, showing that the NoMod provides improved performance and lower computational cost.\n* The authors provide multiple novel innovations on the preprocessing side that could be used for future work on LWE attacks. \n* The paper is well-presented and the authors provide sufficient background on the topic."}, "weaknesses": {"value": "* The method’s performance declines for denser secrets or in high-dimensional parameter settings (once n*k exceeds 150 for sparse secrets). \n* There is limited analysis on the tradeoffs between lattice reduction quality, parameter settings, and attack performance. Can you elaborate on these trends?\n* Although linear models are more interpretable, prior works have primarily focused on transformers due to their better performance. Can the preprocessing innovations be combined with transformer (or other model paradigms) for better performance?"}, "questions": {"value": "Can this attack be used in a real-world scenario, assuming we have a set of eavesdropped, unreduced samples? Or does it rely on some underlying structure in the initial data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ik6a8k6USc", "forum": "5TBemc6p5i", "replyto": "5TBemc6p5i", "signatures": ["ICLR.cc/2026/Conference/Submission19591/Reviewer_LswJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19591/Reviewer_LswJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956874531, "cdate": 1761956874531, "tmdate": 1762931459923, "mdate": 1762931459923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new attack for MLWE. The key ingredient appears to be not modeling the wrap-arounds but treating them as outliers. The authors rely on a combination of preprocessing and robust regression to identify the secrets.\nEmpirically, the approach outperforms related work."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The work uses simple white-box models with effective pre-processing\n- The main idea and modeling choices seem very interesting\n- Empirically, the method outperforms related work"}, "weaknesses": {"value": "- Potentially quite preprocessing sensitive. It is hard for me to say how difficult it is to get the preprocessing right for a new problem\n- Performance drop at higher dimensions\n- Underperforming SOTA approaches (but being more efficient)"}, "questions": {"value": "- Can you provide ablation studies for some parts of the pipeline? So that it is easier to get a feeling of the impact of the steps\n    - vector-saving\n    - ...\n- Could the interpretability of the model be used further? This would be a competitive advantage over the black box-related work"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5bPn8zTTw6", "forum": "5TBemc6p5i", "replyto": "5TBemc6p5i", "signatures": ["ICLR.cc/2026/Conference/Submission19591/Reviewer_rkL9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19591/Reviewer_rkL9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981992186, "cdate": 1761981992186, "tmdate": 1762931459011, "mdate": 1762931459011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies machine-learning-based attacks on the Module Learning With Errors (MLWE) problem. By exploiting the module structure, we develop several optimization techniques that improve upon existing ML-based attacks such as SALSA, SALSA PICANTE, and SALSA VERDE."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The effort to exploit the module structure as fully as possible to further optimize the attack is impressive. By leveraging structural properties of MLWE that previous ML-based attacks did not exploit, the paper amplifies the number of vectors used in the attack—an improvement that is technically meaningful. Moreover, the proposed method demonstrates superior performance on MLWE instances with small parameters compared to other ML-based attacks."}, "weaknesses": {"value": "Because this paper addresses the security of a cryptographic scheme, showing a meaningful improvement over prior ML-based attacks alone is not sufficient to fully assess its contribution. In particular, it is necessary to determine what impact the proposed technique would have on currently deployed cryptographic parameters. However, the current manuscript does not analyze the parameters that are used in practice as standards.\n\nAccordingly, to obtain community recognition of the proposed technique, the following analyses and estimates are required. First, the authors should state explicit assumptions for extrapolating attack costs to parameters used in real deployments of MLWE. To estimate attack runtimes for realistic (rather than toy) parameters, one needs a reasoned model of how the proposed attack’s runtime scales with input parameters. This model may be heuristic, but if so it must be accompanied by clear experimental evidence supporting the proposed scaling law. Given such a model and assumptions, the authors should provide estimated attack times for representative real-world parameters. These estimates are necessary to assess the concrete cryptanalytic impact on MLWE instantiations.\n\nAs it stands, the paper only reports experiments on toy parameters for which the attack runtime can be directly measured, so it is difficult to judge the cryptographic impact of the results. It would be helpful to refer to the following paper for guidance on how to properly analyze an attack algorithm.\n\nChen, Yuanmi, and Phong Q. Nguyen. \"BKZ 2.0: Better lattice security estimates.\" International Conference on the Theory and Application of Cryptology and Information Security. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011."}, "questions": {"value": "1. Can you derive a predictive formula that estimates attack runtime for real-world (practical) parameters?\n2. If so, what are the heuristic assumptions required for that extrapolation, and what experimental evidence supports those assumptions?\n3. Given those assumptions, what is the estimated attack runtime for realistic parameter sets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xoJVQlOgSe", "forum": "5TBemc6p5i", "replyto": "5TBemc6p5i", "signatures": ["ICLR.cc/2026/Conference/Submission19591/Reviewer_KqrX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19591/Reviewer_KqrX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762082437741, "cdate": 1762082437741, "tmdate": 1762931458479, "mdate": 1762931458479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}