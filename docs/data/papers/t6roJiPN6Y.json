{"id": "t6roJiPN6Y", "number": 19649, "cdate": 1758297972891, "mdate": 1759897028235, "content": {"title": "Decision Pre-Trained Transformer is a Scalable In-Context Reinforcement Learner", "abstract": "Recent progress in in-context reinforcement learning (ICRL) has demonstrated its potential for training generalist agents that can acquire new tasks directly at inference. Algorithm Distillation (AD) pioneered this paradigm and was subsequently scaled to multi-domain settings, although its ability to generalize to unseen tasks remained limited. The Decision Pre-Trained Transformer (DPT) was introduced as an alternative, showing stronger in-context reinforcement learning abilities in simplified domains, but its scalability had not been established. In this work, we extend DPT to diverse multi-domain environments, applying Flow Matching as a natural training choice that preserves its interpretation as Bayesian posterior sampling. As a result, we obtain an agent trained across hundreds of diverse tasks that achieves clear gains in generalization to the held-out test set. This agent improves upon prior AD scaling and demonstrates stronger performance in both online and offline inference, reinforcing ICRL as a viable alternative to expert distillation for training generalist agents.", "tldr": "", "keywords": ["in-context reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b9015e4970c84488237095f3447d0ded5c1143cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper extends the Decision Pre-Trained Transformer (DPT), a large-scale transformer model, to diverse reinforcement learning domains using a decision-transformer-style objective. DPT integrates a flow-matching policy head to better model multi-modal continuous-action distributions and is trained on a large multi-domain dataset. The authors evaluate DPT both in offline settings (fixed demonstration contexts) and online settings (autoregressive conditioning on recent transitions). The authors publish the used large-scale dataset with over 700m transitions. The paper also draws a connection, empirically, to the posterior-sampling interpretation of in-context learning, suggesting that DPT behaves as a Bayesian in-context learner following the reasoning of Lee et al. (2023). Empirically, DPT achieves strong performance across most domains and outperforms existing generalist RL models such as Vintix and REGENT on several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written, well-structured, and easy to follow. The dataset composition, architecture, and training setup are described clearly, and the figures are of high quality.\n- The scale of the dataset and experiments is high. The construction of the model, as well as the dataset, covering 10 domains is a significant engineering effort. The resulting benchmark could be a valuable community resource.\n- Replacing the Gaussian policy head with a flow-matching head for continuous actions is a natural and elegant design choice. \n- DPT performs consistently well on both seen and unseen tasks, with significant gains in the more complex domains."}, "weaknesses": {"value": "**Novelty**:\n\nThe paper would benefit from a clearer articulation of what exactly differentiates DPT from prior large-scale decision-transformer architectures such as REGENT and Vintix. Currently, the novelty seems to lie mainly in the use of the flow-matching policy head and the dataset scale. A comparison table summarizing architectural and algorithmic differences (policy head, encoding choice, inference procedure, etc.) would make the contributions clearer.\n\n**Ablation studies**:\n\nI would appreciate more detailed ablations that would allow a more finegrained assessment of what architectural choices contribute most to performance. Primarily the choice of using a flow-matching algorithm for policy heads comes to mind here, but also the choice of embeddings and context ordering, or context length.\n\n**Experiments**:\n\n- it seems that the selection of experiments makes it somewhat hard to draw quantitative conclusions from the chosen implementation choices, because most of the included domains appear to be solves by all tested models. Only few (Meta-World and Bi-DexHands) provide sufficient difficulty to distinguish algorithms. \n\n- Although the results are strong, I could not infer whether all baselines were re-tuned on the same dataset and training budget. In particular, Vintix and REGENT may have been evaluated under different protocols. Clarifying this would improve the fairness of the comparison."}, "questions": {"value": "- Could you provide ablations isolating the contribution of the flow-matching policy head relative to standard Gaussian heads?\n- Could you clarify the experimental protocol for the baselines? Were baselines retrained on your dataset or reproduced from prior work?\n- Do you plan to release the code publicly to support reproducibility and benchmarking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Bmg9VYHBAq", "forum": "t6roJiPN6Y", "replyto": "t6roJiPN6Y", "signatures": ["ICLR.cc/2026/Conference/Submission19649/Reviewer_KAK3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19649/Reviewer_KAK3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570125030, "cdate": 1761570125030, "tmdate": 1762931501555, "mdate": 1762931501555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents an in-context reinforcement learning (ICRL) method that extends the Decision Pre-trained Transformer (DPT) architecture with flow matching to model complex output distributions. An empirical study on a large cross-domain dataset demonstrates the improved performance of this design compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The writing is clear and detailed.\n- The empirical study is comprehensive and fair.\n- The method improves upon the original DPT without too much overhead or modification."}, "weaknesses": {"value": "- The paper lacks a background section to formalize RL, ICRL, and flow matching, which are the focus of this work. \n- I don't think the removal of the next observation $o'$ is well-justified, as it is crucial for capturing the dynamics of the environment, especially when the context is randomly permutated. Without $o'$, RL algorithms that optimize the policy based on the reward signal would not work unless it's a bandit setting. In this case, the DPT is likely merely learning by imitation and using the contextual information for task identification. Even in imitation learning, I doubt removing the next observation will leave the learner intact because, as I pointed out previously, $o'$ is crucial for capturing the environment dynamics. I suspect that the authors' claim that removing it will not impact the performance is due to that the dynamics of the testbeds are somewhat consistent across tasks."}, "questions": {"value": "- What is the source of the optimal actions used for training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UioWRdM4tt", "forum": "t6roJiPN6Y", "replyto": "t6roJiPN6Y", "signatures": ["ICLR.cc/2026/Conference/Submission19649/Reviewer_FmEz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19649/Reviewer_FmEz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886823824, "cdate": 1761886823824, "tmdate": 1762931501145, "mdate": 1762931501145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents the Decision Pre-Trained Transformer (DPT) with a flow-matching generative policy head as a scalable agent for cross-domain, in-context reinforcement learning. The authors create a new large, cross-domain dataset featuring over 700 million transitions across 10 domains, and successfully train the DPT agent to achieve high performance on unseen tasks in several domains. \n\n**Recommendation:**\\\nI recommend to reject. The paper is lacking in several fundamental areas, including the following: comparison with baselines, clarity of research methodology, and unconvincing results."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The analysis in Section 4.4 is potentially interesting. \n- A sizeable new cross-domain benchmark and dataset is created. \n- High performance on unseen tasks in 6 out of 10 domains."}, "weaknesses": {"value": "- Very little comparison with baselines\n- No mention of the significance of the results. Not mentioning number of seeds, the standard deviation or confidence intervals for any methods, makes it impossible to judge any level of significance. \n- Unclear research methodology. There is no mention of hyperparameter tuning or strict validation - test set splits.\n- Reproducability is lacking. The experimental details are very lacklustre. \n- Results not that strong. Only on a single domain (Meta-World) out of 10 does the performance on unseen tasks appear to be better than one of the baselines (Vintix), whilst appearing to be the same as the other baseline (REGENT). Furthermore, performance in the online and offline setting are overlapping in most domains, suggesting it doesn't actually learn in-context at all. This is corroborated by the mostly flat curves in Figure 5. \n- Overclaiming in the introduction. \n- Lack of background section makes it difficult to judge novelty."}, "questions": {"value": "- Which contributions are novel, and in what way exactly?\n- How was hyperparameter tuning performed? And how was contamination between validation and test sets avoided? \n- Several potential baselines were mentioned in the related works section, why did you not compare to them? Why are the existing baselines only evaluated on very few of the domains?\n- How did you test for significance of the results? \n- Figure 3 shows mostly overlapping performance between the online and offline setting, and Figure 4 shows mostly no change in performance as context size increases. Do these two things suggest your agent is not actually learning in-context? \n- Figure 4 shows concentration of the action distribution for longer contexts, but how does this actually relate to performance? \n\n\n**Things to improve that did not impact decision:**\n- Figure 2: What do the red lines and shaded regions indicate?\n- Figure 3: How is the performance for the online setting defined? \n- Line 437: Performance on the Industrial-Benchmark domain does not improve significantly with prompt size as the shaded regions are completely overlapping. \n- If each domain requires its own pre-trained encoder, is it really a cross-domain setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NeUGYS2JD5", "forum": "t6roJiPN6Y", "replyto": "t6roJiPN6Y", "signatures": ["ICLR.cc/2026/Conference/Submission19649/Reviewer_3WLW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19649/Reviewer_3WLW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921752986, "cdate": 1761921752986, "tmdate": 1762931500794, "mdate": 1762931500794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends pretrained Decision Transformer models to diverse multi-domain environments and integrates a flow-matching objective for action generation. The authors evaluate on 209 tasks across 10 domains and report that their approach (DPT with flow matching) outperforms baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and easy to follow.\n - The topic of leveraging past interaction data for generalizable policy learning is important and relevant to the community."}, "weaknesses": {"value": "- The primary contribution appears to be the incorporation of a flow-matching objective into a Decision Transformer framework. However, flow-matching and related diffusion-style generation objectives have been extensively explored in prior work, which makes the contribution feel incremental.\n - The paper does not clearly specify the types of observations used in each domain. Since the tasks span diverse settings, it is important to clarify whether inputs are proprioceptive states, images, or mixed modalities. If the experiments rely solely on low-dimensional proprioceptive inputs, the significance and applicability to high-dimensional tasks may be limited.\n - Although the motivation emphasizes multi-domain generalization, the architecture groups tasks and encodes them using group-specific encoders. This raises the question of how much knowledge is actually shared across domains. A comparison with models trained on single-domain data would help clarify whether multi-domain training provides measurable benefit."}, "questions": {"value": "- What observation modalities are used in each domain? Are the inputs entirely proprioceptive, or do any domains include image-based observations?\n - How does the performance of the multi-domain model compare to models trained separately for each domain? Is there measurable improvement from cross-domain data sharing?\n - Could the authors provide experiments or discussion on applying the model to high-dimensional visual inputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6qg1hy97OM", "forum": "t6roJiPN6Y", "replyto": "t6roJiPN6Y", "signatures": ["ICLR.cc/2026/Conference/Submission19649/Reviewer_rRdx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19649/Reviewer_rRdx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999568732, "cdate": 1761999568732, "tmdate": 1762931500331, "mdate": 1762931500331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}