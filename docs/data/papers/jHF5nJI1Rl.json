{"id": "jHF5nJI1Rl", "number": 5996, "cdate": 1757950071965, "mdate": 1759897940552, "content": {"title": "Revisiting Atomic Rounding with Vectorized Reparameterization for LLM Quantization", "abstract": "Large language models (LLMs) quantization predominantly relies on round-to-nearest (RTN) operations as the atomic operation to map floating point (FP) weights into quantization grids. Applied at tensor-, group-, or channel-level granularities, such non-element-wise rounding is sub-optimal, as it prevents error cancellation across elements. Adaptive rounding addresses this by assigning each weight an optimized rounding parameter, but existing methods introduce an auxiliary matrix of equal size to the weights, substantially inflating computation and memory costs. Thus, we propose VQRound, which re-parameterizes the rounding matrix via vector quantization (VQ) into a compact codebook, drastically reducing trainable variables while preserving quantization fidelity. We identify the critical role of the initialization of the rounding matrix, as a proper scheme minimizes the deviation from the FP model and facilitates efficient tuning of the rounding parameters. Beyond naive layer- or block-wise optimization, we introduce a lightweight end-to-end finetuning pipeline that requires only 128 samples and enables global optimization of codebooks across all layers. Moreover, VQRound can be used as a plug-and-play replacement for atomic rounding, complementing existing quantization techniques to further enhance accuracy. Experiments on billion-parameter models, including OPT, LLaMA, and Qwen, show that VQRound achieves competitive performance under 4-bit, 3-bit, or even 2-bit quantization with as few as 0.2\\% of the learnable parameters of prior adaptive rounding methods.", "tldr": "", "keywords": ["Adaptive Rounding", "Large Language Model", "Quantization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63fedb7609ddfada5a2b56655fb5b15ec913d10b.pdf", "supplementary_material": "/attachment/0db735e64920a0130bebaaade64651a884127ee7.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes VQRound, a memory-efficient method for adaptive rounding in post-training quantization (PTQ) of large language models (LLMs). To address the prohibitive memory and optimization cost of full-size learnable rounding matrices (as in AdaRound), the authors reparameterize the rounding matrix using vector quantization (VQ): they cluster small blocks of rounding decisions into a compact codebook, drastically reducing trainable parameters (to ~0.2% of the original). They further introduce a Hessian-aware initialization scheme and a lightweight end-to-end fine-tuning pipeline using small amounts of samples."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The core idea of using VQ to compress the rounding matrix is well-motivated and yields significant parameter savings (e.g., 7.34M vs. 6.48B for LLaMA-7B), making adaptive rounding feasible for billion-scale models.\n\n2) The method demonstrates non-trivial gains over strong baselines like GPTQ and QuaRot across multiple model families and bit-widths, including the challenging 2-bit regime where prior methods collapse."}, "weaknesses": {"value": "1) The novelty of the work is limited. The paper combines two well-established ideas—adaptive rounding (AdaRound, BRECQ) and vector quantization (VQ-VAE, AQLM)—without a fundamentally new insight. While the application to rounding matrices is novel, the technical contribution is largely engineering: replacing a dense matrix with a VQ codebook. The theoretical analysis (e.g., ∥E∥∞ vs. ∥E∥F) is sound but standard in approximation theory and does not constitute a conceptual leap. \n\n2) The end-to-end fine-tuning of large language models appears to partially undermine the memory efficiency advantage of VQRound, as it requires maintaining and backpropagating through the full model—even though only the small codebook parameters are updated.\n\n3) The individual contribution of the end-to-end fine-tuning strategy (as opposed to local, layer-wise optimization used in methods like GPTQ) and the role of knowledge distillation in improving quantization accuracy remain unclear; a more thorough ablation study is needed to disentangle their effects."}, "questions": {"value": "1) How about the results of the proposed method on reasoning models and multi-modal large models?\n\n2) I guess the work is not as plug-and-play as claimed, particularly when integrated with other quantization frameworks that already incorporate fine-tuning—such as OSTQuant[1] and FlatQuant[2].\n\n3) Can you go deeper into why VQRound counts so heavily on rounding initialization?\n\n4) What is the performance ceiling of VQRound?  That is, do not use vectorization to compress the auxiliary matrices.\n\n[1] Hu, Xing, et al. \"Ostquant: Refining large language model quantization with orthogonal and scaling transformations for better distribution fitting.\" ICLR 2025.\n\n[2]  Sun, Yuxuan, et al. \"Flatquant: Flatness matters for llm quantization.\" ICML2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9E0087LfcQ", "forum": "jHF5nJI1Rl", "replyto": "jHF5nJI1Rl", "signatures": ["ICLR.cc/2026/Conference/Submission5996/Reviewer_EE7v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5996/Reviewer_EE7v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761033940296, "cdate": 1761033940296, "tmdate": 1762918403442, "mdate": 1762918403442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article divides the rounding matrix of quantization errors into residual subvectors and proposes a VQ reparameterization-based rounding method called VQRound. By using codebooks, it solves the problem of the huge number of parameters in the traditional Adaround method and achieves better results than GPTQ."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This method innovatively expresses the coefficients of round through the reparameterization of the codebook, effectively reducing the overhead of round.\n2. Using Hessian to perceive residual initialization is beneficial for accelerating training convergence."}, "weaknesses": {"value": "1. This method is an improvement of adaround; however, no comparison has been made in terms of accuracy, which limits its...\n\n2. Both VQ-round and GPTQ can be regarded as quantization methods, which is similar to the relationship between Adaround and OBQ. However, in Table 5, VQRound+RNT is weaker than GPTQ, and furthermore, the effect when combined with GPTQ becomes even worse. These phenomena have heightened doubts about the effectiveness of VQRound.\n\n3. Global fine-tuning increases memory overhead, which may be the reason why the performance on larger models such as 70B has not been reported.\n\n4. There is also a risk of overfitting. For example, in Table 4, the ppl of global fine-tuning is even lower than that of fp16, but the acc in Table 5 fails to exceed that of floating-point.\n\n5. It fails to make comparisons with cutting-edge algorithms such as QuaRot, SpinQuant, OSTQuant, and FlatQuant."}, "questions": {"value": "1. Can you provide ablation studies on codebook size, annealing strategy, etc.?\n2.See WeakNess"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jMU6Hxo1ZX", "forum": "jHF5nJI1Rl", "replyto": "jHF5nJI1Rl", "signatures": ["ICLR.cc/2026/Conference/Submission5996/Reviewer_V8hy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5996/Reviewer_V8hy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640199782, "cdate": 1761640199782, "tmdate": 1762918401603, "mdate": 1762918401603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a PTQ method called VQRound, which re-parameterizes the rounding matrix via VQ into a compact codebook, to significantly reduce the number of learnable parameters. Beyond conventional layer- or block-wise reconstruction, the authors introduce a lightweight end-to-end fine-tuning pipeline that requires a small number of samples and enables global optimization of codebooks across all layers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The authors validate the effective of VQRound for various quantization configurations from 4-bit to 2-bit.\n2) They theoretically demonstrate why the vectorized re-parameterization can outperform other re-parametrization techniques.\n3) In contrast to layer-wise or block-wise optimizations, they implement VQRound’s end-to-end fine-tuning process."}, "weaknesses": {"value": "1. As mentioned in Abstract, the initialization of the rounding matrix is critical for VQRound. In Table 6, however, the authors look into the initialization of the rounding matrix only for OPT models, which are undertrained and thus easy to quantize [1]. To strengthen that VQ is superior to other methods such as SVD and Kronecker product decomposition, it would be necessary to investigate the initialization of the rounding matrix for recent LLMs such as Llama 3.2 or Qwen3.\n2. The baselines (RTN and GPTQ) are too limited to verify the efficacy of VQRound. To show that VQRound’s end-to-end fine-tuning process can be better than conventional layer-wise or block-wise optimization, it would be required to compare VQRound with block-wise quantization methods like OmniQuant [1].\n3. Either PPL or CSR accuracy is reported. So, it is hard to determine whether VQRound performs well or not. It would be more beneficial if the authors also explore more challenging tasks (e.g., MMLU) and generation tasks (e.g., IFEval, GSM8K) with recent LLMs such as Llama 3.2 or Qwen3.\n4. There are many distillation losses, but there is no ablation study about why the authors choose KL, which makes the novelty of VQRound seem marginal. It would be more beneficial if the authors try other distillation losses such as Jensen-Shannon Divergence (JSD).\n\n[1] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models, ICLR 2024."}, "questions": {"value": "In Table 4, VQRound+RTN sometimes works better than VQRound+GPTQ, but there is no explanation why this happens."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5NGmB05zAK", "forum": "jHF5nJI1Rl", "replyto": "jHF5nJI1Rl", "signatures": ["ICLR.cc/2026/Conference/Submission5996/Reviewer_E73v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5996/Reviewer_E73v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738789217, "cdate": 1761738789217, "tmdate": 1762918399926, "mdate": 1762918399926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper uses codebooks to guide the rounding phenomenon in quantization algorithms. This is in contrast to alternate method of adaptive rounding which employ a residual matrix that is added to the weights and is often in low-rank format."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The empirical results look good."}, "weaknesses": {"value": "- I strongly suggest the authors to change the name of the paper. \"Atomic\" in computer science is a reserved term for operation in multi-threaded execution that employ locks.\n- I understand the discussion below equation (3). However, I do not agree that controlling E_max gives a bound on E_F, it's the other way around as clearly shown by the inequalities in eq (3).\n- Note that the authors have been using E_max as notation to denote an L_infinity error, but on and after line 254, they use E_infinity. Please be consistent in the notation.\n- Multiple new notations are parachuted in equation (5) such as a weird inequality symbol with an tilde underneath, sigmas (which I assume denote singular values) and sigma followed by arrow followed by sigma_2 (is this a limit)?"}, "questions": {"value": "- What does the size of the balls indicate in Figure 1 left?\n- Since VQ is used to guide the rounding of the weights, why not just round the weights using VQ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ez1cf1FDdJ", "forum": "jHF5nJI1Rl", "replyto": "jHF5nJI1Rl", "signatures": ["ICLR.cc/2026/Conference/Submission5996/Reviewer_74XB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5996/Reviewer_74XB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943810145, "cdate": 1761943810145, "tmdate": 1762918399203, "mdate": 1762918399203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}