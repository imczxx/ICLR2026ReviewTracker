{"id": "69IxzbIiVb", "number": 10276, "cdate": 1758165740505, "mdate": 1759897661536, "content": {"title": "RAU: Reference-based Anatomical Understanding with Vision-Language Models", "abstract": "Anatomical understanding through deep learning is critical for automatic report generation, intra-operative navigation, and organ localization in medical imaging; however, its progress is constrained by the scarcity of expert-labeled data. A promising remedy is to leverage an annotated reference image to guide the interpretation of an unlabeled target. Although recent vision–language models (VLMs) exhibit non-trivial visual reasoning, their reference-based understanding and fine-grained localization remain limited. We introduce RAU, a framework for reference-based anatomical understanding with VLMs. We first show that a VLM learns to identify anatomical regions through relative spatial reasoning between reference and target images, trained on a moderately sized dataset. We validate this capability through visual question answering (VQA) and bounding box prediction. Next, we demonstrate that the VLM-derived spatial cues can be seamlessly integrated with the fine-grained segmentation capability of SAM2, enabling localization and pixel-level segmentation of small anatomical regions, such as vessel segments. Across two in-distribution and two out-of-distribution datasets, RAU consistently outperforms a SAM2 fine-tuning baseline using the same memory setup, yielding more accurate segmentations and more reliable localization. More importantly, its strong generalization ability makes it scalable to unseen datasets, a property crucial for medical image applications. To the best of our knowledge, RAU is the first to explore the capability of VLMs for reference-based identification, localization, and segmentation of anatomical structures in medical images. Its promising performance highlights the potential of VLM-driven approaches for anatomical understanding in automated clinical workflows.", "tldr": "", "keywords": ["Medical Image Analysis", "Anatomical Understanding", "Vision-Language Model", "Instance Segmentation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/634880fa6f714f5348b60fc4be76e27049c19e94.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes RAU, a reference-based anatomical understanding framework that combines a vision–language model with a segmentation foundation model (SAM2). The core motivation is to overcome the data scarcity problem in medical image analysis by leveraging a single annotated reference image to guide the interpretation of an unlabeled target image. The authors first show that VLMs can be trained to perform reference-based spatial reasoning for anatomical region identification through visual question answering (VQA) and bounding box prediction tasks. Then they integrate the VLM's spatial reasoning capabilities with SAM2's fine-grained segmentation to enable pixel-level anatomical region identification. Experiments on two in-distribution datasets (RAOS, Arcade) and two out-of-distribution datasets (LERA, CAMUS) show consistent improvements over SAM2 fine-tuning baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is overall clearly written and the method is adequately described.\n2. The proposed approach of using a single reference image to guide a VLM for anatomical understanding under limited labels is well-motivated.\n3. The hybrid paradigm that leverages VLM's relational reasoning with SAM2’s segmentation is logical and compelling."}, "weaknesses": {"value": "1. Limited novelty: The reference-based framework is not novel in the context of few-shot learning, retrieval-augmented generation (RAG), or in-context learning. Moreover, integrating VLMs with SAM has been explored in prior works such as LISA[R1] and Seg-Zero[R2].\n2. Insufficient baselines: For the VQA tasks, the paper only compares against MedFlamingo and Med-VLM-R1. How does RAU perform against stronger medical VLMs such as Lingshu-7B[R3] or HuaTuoGPT[R4]? For the segmentation task, the main baselines are SAM2-Memory and SAM2-Memory-Ref-SFT. However, more competitive and fair baselines are missing, such as few-shot medical segmentation methods and other VLM+SAM approaches like SAM4MLLM[R5], LISA[R1] and Seg-Zero[R2].\n3. Dependence on reference quality: the entire framework relies on a high-quality, well-annotated reference image. The method assumes relatively consistent topology between reference and target. How would performance change if (a) a different backbone (e.g., not DINOv2) were used for reference retrieval, (b) multiple reference images were retrieved, or (c) cases with large anatomical abnormalities that break the assumed structural consistency between reference and target?\n4. Poor baseline performance: The reported scores for SAM2-Memory-Ref-SFT appear remarkably low, even though it is fine-tuned on the target datasets. Can the authors clarify why this baseline underperforms so drastically?\n5. Evidence for VLM's relative spatial reasoning: The paper claims that CoT outputs show “convergence toward reference-guided relational reasoning,” but no examples are provided. Can the authors provide some CoT traces?\n6. Limited failure analysis: While successful cases are shown, there is minimal discussion of failure modes or scenarios where the approach breaks down, which is critical for understanding real-world applicability.\n7. Scalability concerns: Experiments are limited to datasets with limited anatomical classes. How would the method scale to more complex tasks like whole-body segmentation with 100+ structures?\n\n- [R1] Xin Lai, Zhuotao Tian, et al. Lisa: Reasoning segmentation via large language model. arXiv preprint arXiv:2308.00692.\n- [R2] Yuqi Liu, Bohao Peng, et al. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520.\n- [R3] Weiwen Xu, Hou Pong Chan, et al. Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning. arXiv preprint arXiv:2506.07044\n- [R4] Junying Chen, Chi Gui, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280.\n- [R5] YiChia Chen, WeiHua Li, et al. Sam4mllm: Enhance multi-modal large lan- guage model for referring expression segmentation. In Eu- ropean Conference on Computer Vision, 323–340. Springer."}, "questions": {"value": "Please see the \"Weaknesses\" above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "89wR6SdfFf", "forum": "69IxzbIiVb", "replyto": "69IxzbIiVb", "signatures": ["ICLR.cc/2026/Conference/Submission10276/Reviewer_YWeu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10276/Reviewer_YWeu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704840984, "cdate": 1761704840984, "tmdate": 1762921629434, "mdate": 1762921629434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper first demonstrates on visual question answering (VQA) and bounding box prediction tasks that, after training on a moderately sized dataset, a VLM can learn to identify anatomical regions via relative spatial reasoning between a reference and a target image. It then integrates the VLM-derived spatial cues with SAM2’s fine grained segmentation capability to achieve localization and pixel level segmentation of small anatomical regions. Across two ID and two OOD datasets, the proposed method outperforms an SAM2 fine tuning baseline and exhibits strong generalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Jointly training the VLM and SAM2, and leveraging the strengths of both, yields accurate identification of target structures in novel images given a reference image, demonstrating strong generalization. \n2. The figures and overall layout are carefully designed."}, "weaknesses": {"value": "**Main concerns**\n1. The authors state in Section 3.3: “Although our segmentation quality does not yet match a fully specialized nnU-Net (Isensee et al., 2021) trained with substantial task-specific labels, our approach, our approach [sic] requires far less annotation effort.” I would like to see a more detailed comparison between these two approaches, as well as comparisons with other segmentation backbones (e.g., MedSAM [1], SAT-Pro [2]), since they also exhibit strong generalization on OOD data. More concrete evidence is needed to highlight the advantages of VLM+SAM2, especially given that the authors use RAOS (650k) and train on 8 NVIDIA A100 GPUs (80 GB), which appears to be a nontrivial cost, and the training still requires precise segmentation mask ground truth.\n2. The authors describe their method as “Inducing reference-based spatial reasoning in VLMs for medical images” (#line 083). In my understanding, I do not clearly perceive “reasoning in VLMs”; it feels more like a few-shot setup where the VLM is used to convey information from the reference images to the decoder.\n3. The comparisons in Table 1 seem unfair: MedFlamingo and Med-VLM-R1 are essentially classification-level models without region-level training, whereas many existing methods do focus on region-level tasks in medical imaging, e.g., MedRegA [3], BiRD [4], and MedPLIB [5]. In particular, MedPLIB can also output masks. I believe the baselines should be updated to include more recent and more relevant methods.\nIn addition, when reporting MedFlamingo and Med-VLM-R1 results on RAOS-test-CT (ID), should the fine-tuned results also be reported to align with the proposed method?\n4. In Fig. 2(c), why are “Reference Image” and “Reference Image with Overlaid Annotation” not the same underlying image? “Reference Image with Overlaid Annotation” should be the “Reference Image” with the annotation overlaid; the two images should be identical apart from the overlay.\n\n**Minor issues**\n1. Lines 316–317 state: “indicating that spatial grounding via box-level alignment helps guide more accurate label assignment.” The increment attributable to box-level alignment should be measured from RL-VQA → RL-Box (74.68% → 78.16%), not 64.11% → 78.16%, because the latter also includes the improvement from SFT to RL, not just the box-level effect.\n\n[1] Segment Anything in Medical Images\n\n[2] Large-Vocabulary Segmentation for Medical Images with Text Prompts\n\n[3] Interpretable bilingual multimodal large language model for diverse biomedical tasks\n\n[4] A Refer-and-Ground Multimodal Large Language Model for Biomedicine\n\n[5] MedPLIB: Towards a Multimodal Large Language Model with Pixel-Level Insight for  Biomedicine"}, "questions": {"value": "1. Why does the first paragraph of the Section state “We train on two labeled datasets (RAOS-CT (Luo et al., 2024), Arcade-X-Ray),” while Table 1 lists Arcade (26) as an OOD dataset?\n2. In Table 2, the SFT-VQA result on RAOS is 64.11%. Does this correspond to the “SFT (5 epochs)” entry in Table 1? What exactly does RL-VQA refer to?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x6loXTaxh7", "forum": "69IxzbIiVb", "replyto": "69IxzbIiVb", "signatures": ["ICLR.cc/2026/Conference/Submission10276/Reviewer_QGk4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10276/Reviewer_QGk4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796142056, "cdate": 1761796142056, "tmdate": 1762921628830, "mdate": 1762921628830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework called RAU, which is a Reference-based Anatomical Understanding method. It employs a vision–language architecture, uses reference–target image pairs to improve anatomical localization and segmentation. By using relational prompts and a lightweight segmentation head built on a frozen VLM backbone, RAU captures inter-organ spatial dependencies and enhances generalization. Experiments on multiple medical datasets show performance gains over SAM2 and CLIP-based baselines in both accuracy and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Propose a reference-based learning paradigm, which sounds interesting and reasonable. Using a unique reference–target formulation for image understanding, this work enables the model to use anatomical correspondences between paired scans, and further helps capture inter-organ spatial relations and improves generalization.\n\n2. By combining a VLM with the segmentation model SAM2, RAU achieves precise anatomical region identification while retaining spatial reasoning capabilities. This design strengthens both semantic reasoning and pixel segmentation abilities.\n\n3. An interesting finding is that, the model demonstrates consistent performance improvements on OOD datasets, which suggests that reference-based reasoning enables better adaptability for downstream applications."}, "weaknesses": {"value": "1. The paper introduces DINOv2-based cosine similarity for template matching, but it never evaluates how reliable or consistent this retrieval step is. If the retrieval picks an anatomically dissimilar or low-quality reference, will the downstream reasoning and segmentation fail? \n\n2. The proposed pipeline involves multiple heavy components, such as: DINOv2 for feature extraction, cosine similarity over a large reference bank, multimodal VLM inference, and decoding. However, the paper provides no analysis of computational overhead, latency, which is practical for real deployment."}, "questions": {"value": "1. Does the method require the referene image even at the real-world inference time? If so, how to obtain or select a suitable reference image? And what are the standards? \n\n2. I wonder how robust the model is to mismatched references (e.g., differing anatomy, orientation, or disease states)?\n\n3. I did not find the details on the size, composition of the reference bank, the diversity, and quality of the reference bank directly affect retrieval accuracy on the reference image, further impacting generalization in evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OAtveuYYGH", "forum": "69IxzbIiVb", "replyto": "69IxzbIiVb", "signatures": ["ICLR.cc/2026/Conference/Submission10276/Reviewer_DJQe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10276/Reviewer_DJQe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990915166, "cdate": 1761990915166, "tmdate": 1762921628486, "mdate": 1762921628486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RAU (Reference-based Anatomical Understanding), a framework that couples a vision–language model (VLM) with SAM2 to achieve reference-guided recognition, localization, and segmentation of anatomical structures in medical imaging. The approach comprises three components: (1) constructing reference–target image pairs and fine-tuning a 7B QwenVL model via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), thereby endowing the VLM with the ability to infer spatial relationships and semantic correspondences from the reference image to identify the target region; (2) producing bounding boxes for multiple categories in the target image by regressing from special token embeddings and performing global assignment via optimal transport (OT); and (3) mapping VLM-generated tokens into query vectors for the SAM2 Memory Bank to drive fine-grained segmentation without explicit point/box prompts. Experiments span two in-distribution datasets (RAOS-CT, Arcade-X-ray) and two out-of-distribution datasets (LERA-X-ray, CAMUS-Ultrasound), reporting results under three task settings: VQA, bounding box prediction, and segmentation. The authors claim this is the first study to explore reference-based identification, localization, and segmentation with VLMs in medical imaging, and they present evidence of RAU’s scalability in annotation-limited scenarios and its cross-distribution generalization."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tNovelty: Using a reference image as intuitive visual context for anatomical understanding.\nThe paper treats the reference image as a visually grounded “in-context” prior that directly guides anatomical understanding on the target image. This is operationalized in two complementary ways: (i) multi-class bounding box assignment is cast as an optimal transport problem, leveraging global relational constraints across categories; and (ii) special tokens serve as soft language prompts whose embeddings are projected via an MLP into SAM2’s Memory Bank queries, enabling seamless adaptation from reference-guided localization to integration with SAM for fine-grained segmentation.\n2.\tTraining design: A systematic two-stage pipeline with task-aligned reward shaping.\nThe method adopts a coherent SFT→GRPO schedule and designs task-specific rewards across VQA, bbox prediction, and segmentation. Format-validity rewards ensure robust, parseable outputs; AP/IoU-based rewards drive spatial grounding for detection; and Dice/BCE losses (and their weighted combinations) enforce pixel-level accuracy for segmentation. This holistic reward design strengthens the VLM’s format correctness and spatial alignment capabilities for medical anatomical understanding, while maintaining a clear division of labor between high-level reasoning (VLM) and fine-grained perception (SAM2).\n3.\tGeneralization: Empirical evidence of cross-modality and cross-structure OOD gains, with RL initialization offering further benefits.\nExperiments demonstrate consistent improvements across modalities (CT→X-ray→ultrasound) and anatomical types (organs, bones, vessels), indicating that the approach generalizes beyond the training distribution. Notably, initializing the segmentation integration with the RL-tuned VLM yields stronger OOD performance than SFT initialization, suggesting that policy optimization imbues more transferable spatial reasoning. These findings provide constructive insights for building low-annotation pipelines in medical imaging, where leveraging visual reference priors and reward-driven spatial alignment can reduce annotation burden while preserving robustness."}, "weaknesses": {"value": "1.\tThe claims of novelty appear overstated, as the proposed architecture shares significant conceptual overlap with existing work.\nThe paper's central claim of being a \"first exploration\" seems to be an overstatement. The core innovation lies in fine-tuning a VLM for relative spatial reasoning and then using tokens as \"soft language prompts,\" which are projected via an MLP to query the SAM2 memory, thereby coupling \"language-spatial cues\" with \"visual-segmentation memory.\" However, this architectural design is conceptually analogous to existing \"language-segmentation interfaces\" such as EVF-SAM and LISA. Furthermore, its motivation to leverage external references for target localization overlaps with the principles of retrieval-augmented medical segmentation (e.g., RAG, In-Context Learning, Few-Shot learning with foundation models). Consequently, the paper's contribution is more of an incremental, \"combinatorial engineering innovation\" combined with novel task design and reward shaping, rather than a foundational, pioneering work.\nSuggestion: To better position the paper's contribution, I recommend that the authors conduct a more systematic comparison in the related work section. This should detail the differences in interface design and capability boundaries between the proposed method and prior works like EVF-SAM, LISA, MedSAM, as well as retrieval-augmented and in-context learning approaches in medical segmentation.\n2.\tThe baseline setup is weak and raises questions about the fairness of the experimental comparisons.\nThe experimental evaluation lacks rigor due to a limited selection of baselines. In the VQA experiments, the method is only compared against MedFlamingo and Med-VLM-R1. In the segmentation experiments, the only baseline is a \"memory-only\" version of the original SAM 2. The evaluation is missing comparisons against more SOTA models (e.g., MedSAM, TotalSegmentator), stronger baselines using SAM2 with its more common point/box/text prompts, or fusion-style approaches like EVF-SAM/LISA. Additionally, a comparison with classic registration pipelines (e.g., nnU-Net + VoxelMorph/Label Fusion) would provide a valuable reference point. The paper justifies fairness by claiming to \"use the same memory setting,\" but this is misleading. The proposed RAU method is augmented with language token injection and a retrieval mechanism, giving it access to information that the baseline SAM2 does not have, making the comparison inequitable.\nSuggestion: The authors should include a more comprehensive set of strong, state-of-the-art baselines and classic pipelines to provide a more convincing and credible assessment of the proposed method's improvements.\n3.\tThe evaluation of the retrieval component is insufficient and lacks depth.\nIf the reference images in the RAU are considered as in-context examples, the performance of the retrieval mechanism itself requires a more thorough investigation. The paper should discuss key retrieval metrics, such as the recall rate of the retrieved reference images and the impact of the number of references used. A crucial analysis is missing: an investigation into failure cases, such as instances where a more suitable reference image existed in the retrieval database but was not selected. For the OOD evaluation, the source of the \"reference library\" is not clearly specified. This raises a practical question: if deploying on a new dataset still requires at least one reference sample with ground truth, the \"minimal annotation\" assumption needs to be explicitly quantified (e.g., how many references are needed per disease, per modality, or per institution?).\nSuggestion: A more in-depth analysis and ablation study of the retrieval component is necessary. This should include retrieval performance metrics (e.g., recall) and a clear discussion quantifying the \"minimal annotation\" assumption required for practical deployment.\n4.\tThe manuscript's readability is poor, and it suffers from a lack of organizational clarity and academic rigor.\nThe paper is difficult to follow due to several presentational issues. The logical flow in the introduction and related work sections is disjointed, with the introduction lacking a clear narrative arc and the related work failing to establish a strong connection to the paper's core theme. The method and experiment sections are conflated, which prevents a detailed and clear exposition of the methodology; for instance, the data construction process is not mentioned. Key implementation details and hyperparameters are missing, such as the reward weights for RL (GRPO), the curriculum learning strategy, the design and number of tokens, the Memory Bank size, and the DINO retrieval threshold/strategy. Most concerningly, the paper contains multiple severe citation errors (e.g., citing GLIP as a 2006 chemistry paper and SEEM as a 2007 wireless sensor network protocol). This suggests a potential over-reliance on large language models for writing without careful human verification, which severely undermines the paper's credibility and adherence to academic standards.\nSuggestion: A major revision is required to improve the paper's structure, provide all missing implementation details, and meticulously correct all citations to meet the expected scholarly standards of a top-tier publication."}, "questions": {"value": "1.\tCould you clarify the construction of the reference library for OOD generalization, including the average number of new in-domain annotations required for datasets like LERA and CAMUS, and provide results using only the source-domain reference library to validate true cross-domain transferability?\n2.\tGiven that the model processes 2D slices from the 3D RAOS dataset, how are these slices selected, and how would you ensure cross-slice consistency of the Reference Attention Unit (RAU) when extending the model to full 3D volumes for clinical applications?\n3.\tWhat are the scale and update strategy of the memory bank, and how does it maintain robust retrieval and matching when faced with significant cross-domain mismatches, such as variations in viewpoint, scale, or mirroring between reference and target images?\n4.\tBeyond the \"slender vessel topology,\" could you provide a more detailed failure analysis for the bounding box stage on the Arcade dataset, including qualitative examples and potential improvements for challenging cases like small organs, thin vessels, occlusions, or low-contrast scenes?\n5.\tConsidering the reference image as a form of context, what is your hypothesis for the performance gap of the training-free approach compared to theoretical expectations, and could you discuss the potential of leveraging state-of-the-art large vision models (GPT-5 and Gemini2.5-pro) to enhance this capability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jQMS42LGbt", "forum": "69IxzbIiVb", "replyto": "69IxzbIiVb", "signatures": ["ICLR.cc/2026/Conference/Submission10276/Reviewer_Akfu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10276/Reviewer_Akfu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762178977330, "cdate": 1762178977330, "tmdate": 1762921627949, "mdate": 1762921627949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}