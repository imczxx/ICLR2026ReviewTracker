{"id": "m38vyzeoO0", "number": 1242, "cdate": 1756867301607, "mdate": 1763690166856, "content": {"title": "Inheriting Generalizable Knowledge from LLMs to Diverse Vertical Tasks", "abstract": "Large language models (LLMs) have demonstrated remarkable generalization across diverse tasks, suggesting the existence of task-agnostic, generalizable knowledge encoded within them. However, how to systematically extract and evaluate this knowledge remains unexplored. In this work, we innovatively propose MASA (Matrix-level Alignment and Scalable Adaptation), a unified framework for extracting and transferring generalizable knowledge from LLMs. MASA first introduces a lightweight set of gene matrices trained with a dual alignment strategy, combining output alignment and spectral alignment, to capture the generalizable knowledge encoded in the feed-forward networks (FFNs) of LLM. It then employs scalable adaptation to flexibly reshape these gene matrices to match the parameter dimensions of lightweight dense models of various sizes, enabling direct initialization of their FFN layers. To evaluate the inherited knowledge, we measure the downstream performance of lightweight models initialized with MASA across language understanding and dialogue generation tasks spanning diverse vertical domains. Experiments on both dense and Mixture-of-Experts (MoE) source LLMs show that MASA consistently outperforms baselines such as random initialization, pruning, and distillation, yielding lightweight models that achieve stronger performance, require less pre-training data, and converge faster. These results establish MASA as an effective and general framework for extracting and leveraging the generalizable knowledge within LLMs. The code is available at https://anonymous.4open.science/r/MASA-main-6083/.", "tldr": "", "keywords": ["NLP; DL"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1f0fd2afd78e9d354eca9348c3f4118005dcdadf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a technique to extract information stored in the feedforward layers from a pretrained LLM and uses it to initialize a smaller LLM of arbitrary dimension. The key idea consists of two steps: 1) extract square matrices from the FFN weight matrices of the LLM using a combination of output alignment that minimizes the distance between the outputs from the original layers and the square matrices and spectral alignment that minimizes the distance between singular values of the original and square matrices, and 2) adaptive scaling mechanism which reshapes the extracted square matrices and uses it to initialize a smaller LLM of arbitrary dimension. The remaining parameters of the smaller LLM are randomly initialized.  The paper applies the method to 3 different source LLMs: OLMo-7B, OLMoE-7B and DeepSeekMoE-16B. The paper shows that the proposed method can yield improvements in performance when compared to a number of baselines including training from scratch, distillation from a larger model and pruning. This is true for models of different sizes. The technique achieves 85% of the performance obtainable via full fine-tuning. It requires 2-5x less pre-training data when compared to training from scratch. It converges faster when fine-tuning on specific datasets. The paper reports an ablation analysis showing that both components of  the model - spectral alignment and adaptive scaling - play a critical role."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Proposes a novel technique to extract useful information from a pre-trained LLM which can used to initialize a smaller model of arbitrary dimension\n* Shows that the technique improves upon alternative initialization approaches such as training from scratch, distillation and pruning. \n* Shows that the method can reduce the amount of data for pre-training by a factor of 2-5x, and can reduce convergence time in fin-tuning."}, "weaknesses": {"value": "* The evaluation contains some baselines which are not comparable. See questions below.\n* The paper does not provide some details. See questions below."}, "questions": {"value": "* L016: The abstract contains the term 'gene matrices' which are previously undefined.\n* L170: Is the number of square matrices identical to the number of weight matrices in the LLM? \n* Evaluation: In Tables 1 and 2, what is the larger model used for Distillation? For proper comparison, the model for Distillation and Pruning-EEP should be identical to that used for MASA.  At the moment, a) The teacher model for Distillation is not specified and b) Pruning-EEP is based on OLMoE. Thus, it seems like Pruning-EEP can not be compared to MASA-OLMo or MASA-DeepSeek. Please clarify. \n* Evaluation: For Table 4, the paper states that \"In w.o Adaptive Scaling, the gene matrix is naively resized by random initialization or direct truncation\". When is random initialization chosen over truncation? or is the decision random?\n* Figure 4: What are the proportion of the different datasets : code/web/math?\n* L175: \"gene matrices are aligned with experts that are highly active across diverse tasks\" How do you quantify this? \n* L202: f_c(x_in), the x_in looks like a subscript. \n\nTypos:\n* L071: cross -> across"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aDz2OrUEYh", "forum": "m38vyzeoO0", "replyto": "m38vyzeoO0", "signatures": ["ICLR.cc/2026/Conference/Submission1242/Reviewer_KeEV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1242/Reviewer_KeEV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761337174108, "cdate": 1761337174108, "tmdate": 1762915715541, "mdate": 1762915715541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MASA, a method to extract \"generalizable knowledge\" from large language models (LLMs) via compact \"gene matrices\" aligned with Feed-Forward Network (FFN) layers using output and spectral alignment. An adaptive scaling technique is introduced to transfer these matrices to initialize smaller models. Experiments across multiple LLM architectures and tasks show that MASA-initialized models outperform random initialization, distillation, and pruning baselines in performance, data efficiency, and convergence speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper presents a method for transferring knowledge from large to small LLMs. \n- Experiments cover multiple model architectures (Dense, MoE) and a range of downstream tasks.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "- The core concept lacks strong novelty due to its significant overlap with the Learngene framework.\n- Insufficient evaluation of \"generalizable knowledge\". Most evaluations are on supervised fine-tuning tasks. Testing in zero-shot or few-shot settings would better demonstrate the transfer of generalizable knowledge.\n- The paper lacks of sensitivity study on the hyperparameters λ."}, "questions": {"value": "- Given the Learngene framework already established the core principle of inheriting condensed knowledge, what is the fundamental conceptual advance of MASA beyond its application to LLMs? Can you provide a detailed comparison?\n- To substantiate the claim of transferring \"generalizable knowledge,\" can you provide performance on standard zero-shot benchmarks for models initialized with MASA before any task-specific fine-tuning?\n- The spectral alignment loss uses a fixed hyperparameter λ. Can you provide a sensitivity analysis to show how critical this parameter is to the final performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uj5omo88JZ", "forum": "m38vyzeoO0", "replyto": "m38vyzeoO0", "signatures": ["ICLR.cc/2026/Conference/Submission1242/Reviewer_Nd6r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1242/Reviewer_Nd6r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982585585, "cdate": 1761982585585, "tmdate": 1762915715362, "mdate": 1762915715362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a method to extract knowledge from source LLMs and inherit the knowledge to lightweight models for downstream adaptation. Specifically, in knowledge extraction, the paper applied output reconstruction loss and singular value matching loss to make sure the gene matrices can extract the useful information from the FFN layers in the source LLM; to apply the gene matrices to lightweight models, the paper applied matrix column and row expansion based on the norm value of the left and right singular matrices. The proposed method was compared to other approaches including model distillation and pruning, and showed better task metrics on understanding and generative tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper is a fusion of different important aspects in LLM adaptation, including (1) how to distill large model to lightweight ones for better efficiency, (2) how to extract knowledge from source LLM to downstream adaptation for better accuracy, (3) how to address potential weight matrix mismatch problem between source and target LLM architectures."}, "weaknesses": {"value": "I have concerns in the following perspectives:\n- Novelty. In the knowledge extraction section, the paper proposed a method to align the output from the source LLM and the gene metrics based on MSE loss. These type of reconstruction objective has been well established in the deep neural network pruning community. Just list a few papers as reference [1] [2] [3]. Please add proper references to these previous works.\n- Technical clarity. (1) The theoretical justification about the single value alignment loss in the knowledge extraction section is not clear. Based on the provided justification in the appendix, the generalization is related to the flatness of the Hessian matrix, which is affected by the Trace of the Hessian matrix, which is the sum of eigen values of the Hessian matrix, but not the FFN weight matrices. Even we want to use the Rademacher bound to showcase that the spectral norms of the weight matrices affect generalization, based on the bound, I would say it is making more sense to create loss function based on minimizing the discrepancy of the Frobenums norm of the source weight matrix and gene matrix, or minimize the discrepancy of the nuclear norm of two matrices. The paper did not provide convincing ablation study. (2) In the knowledge inheriting section, the authors seem want to use some of the concept from matrix column set selection and importance score to determine how to expand the rows and columns of the gene matrix. However, based on the important score sampling in column subset selection theory, we should actually sample the original weight matrix's column based on the right singular weight matrix row-wise norm (i.e., importance score). The current implementation does not have clear theoretical justification.\n- Experiments. The results section misses an important baseline, which is pruning + SFT with distillation. The proposed approach extracts existing knowledge from source LLM, then apply to target LLM as initialization, and then SFT. Moreover, the two alignment loss acts like distillation to make sure the gene matrices can mimic the source LLM. Therefore, it is not suprising that the approach is better than training from scratch, distillation, or pruning. The real baseline should be applying pruning to get better initialization from the source LLM, apply SFT or other finetuning strategy with distillation loss so that the target LLM can mimic the source LLM, similar to the dual alignment loss proposed in the paper. Without this baseline, it is hard to make judgement on the real effectiveness of the approach.\n\n[1]  Channel pruning for accelerating very deep neural networks. ICCV 2017\n[2] Thinet: A filter level pruning method for deep neural network compression. ICCV 2017\n[3] Discrimination-aware channel pruning for deep neural networks. Neurips 2018"}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qlqEowdhZ5", "forum": "m38vyzeoO0", "replyto": "m38vyzeoO0", "signatures": ["ICLR.cc/2026/Conference/Submission1242/Reviewer_Wu8B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1242/Reviewer_Wu8B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762133400840, "cdate": 1762133400840, "tmdate": 1762915715216, "mdate": 1762915715216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MASA (Matrix-level Alignment and Scalable Adaptation), a two-stage framework to extract generalizable knowledge from large language models (LLMs) and use it to initialize smaller, lightweight models.\n\n- Knowledge Extraction: The method trains a lightweight set of \"gene matrices\" to capture the knowledge within the FFN layers of a frozen source LLM. This training uses a dual alignment loss: a standard output alignment loss ($L_{out}$) combined with a novel spectral alignment loss ($L_{spec}$). $L_{spec}$ aims to match the singular value distribution of the gene matrix to that of the source FFN weight matrix.\n- Knowledge Inheriting: The framework uses a scalable adaptation strategy to transfer the trained gene matrices to target models of arbitrary FFN dimensions. This SVD-based method trims or pads the singular vector matrices to match the target dimensions before reconstruction.\n\nExperiments show that models initialized with MASA converge faster and achieve stronger performance compared to baselines like random initialization, pruning, and knowledge distillation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Practical Problem: The paper addresses the significant and practical problem of efficiently transferring the vast knowledge of LLMs to smaller, more deployable models. \n- Technically Motivated: The methodology includes non-trivial technical ideas. The spectral alignment loss ($L_{spec}$) is a principled addition, moving beyond simple functional imitation (like in distillation) to match deeper structural properties of the weight matrices, with good theoretical motivation.\n\n- Principled Scaling: The SVD-based \"scalable adaptation\" is a principled and flexible method for transferring parameters across models of different FFN dimensions, superior to naive truncation."}, "weaknesses": {"value": "- Missing Baseline: It compares MASA against \"random initialization, pruning, and knowledge distillation\" but fails to include  relevant class of baselines: direct parameter transfer. A method like \"Weight Selection\", which initializes a smaller model by directly copying a subset of the larger model's FFN weights, is a crucial point of comparison. \n\n- Incremental Novelty: The paper's claim that this problem is \"unexplored\" is an overstatement. The paper explicitly adapts the \"Learngene\" framework from vision and builds on the known concept of FFNs as knowledge-stores. The novelty rests almost entirely on the addition of the $L_{spec}$ loss."}, "questions": {"value": "The \"Distillation\" baseline  is underspecified. Is this standard response-based distillation (matching output logits) or feature-based distillation? Given MASA operates on intermediate FFN outputs , a comparison to a modern feature-based distillation method  would be a more rigorous and appropriate baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4KHl2ecSEE", "forum": "m38vyzeoO0", "replyto": "m38vyzeoO0", "signatures": ["ICLR.cc/2026/Conference/Submission1242/Reviewer_yUAD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1242/Reviewer_yUAD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762239720778, "cdate": 1762239720778, "tmdate": 1762915715041, "mdate": 1762915715041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}