{"id": "LWYZ1nNkJl", "number": 9296, "cdate": 1758117863296, "mdate": 1759897732951, "content": {"title": "Rethinking Residual Errors in Compensation-based LLM Quantization", "abstract": "Methods based on weight compensation, which iteratively apply quantization and weight compensation to minimize the output error, have recently demonstrated remarkable success in quantizing Large Language Models (LLMs). \nThe representative work, GPTQ, introduces several key techniques that make such iterative methods practical for LLMs with billions of parameters.\nGPTAQ extends this approach by introducing an asymmetric calibration process that aligns the output of each quantized layer with its full-precision counterpart, incorporating a residual error into the weight compensation framework.\nIn this work, we revisit the formulation of the residual error.\nWe identify a sub-optimal calibration objective in existing methods: during the intra-layer calibration process, they align the quantized output with the output from compensated weights, rather than the true output from the original full-precision model. This leads to a sub-optimal calibration objective. Therefore, we redefine the objective to precisely align the quantized model's output with the original output of the full-precision model at each step. We then reveal that the residual error originates not only from the output difference of the preceding layer but also from the discrepancy between the compensated and original weights within each layer, which we name the 'compensation-aware error'.\nBy inheriting the neuron decomposition technique from GPTAQ, we can efficiently incorporate this compensation-aware error into the weight update process. Extensive experiments on various LLMs and quantization settings demonstrate that our proposed enhancements integrate seamlessly with both GPTQ and GPTAQ, significantly improving their quantization performance.", "tldr": "", "keywords": ["Large Language Models", "Quantization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/52a73489dd7711c28eb4759ad655e988e1eb539f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an improved residual error formulation for post-training quantization (PTQ) of large language models, building on frameworks like GPTQ and GPTAQ.\n\nThe key insight is that prior methods inaccurately model the calibration objective during the layer-wise iterative process. The authors identify that the residual error should not only account for the inter-layer propagated output error but also an intra-layer compensation-aware error arising from the weight update process itself."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper demonstrates significant strengths across key dimensions. Its originality lies in a nuanced reformulation of the residual error objective within established PTQ frameworks, identifying and incorporating the previously overlooked compensation-aware error. \n\nThe quality is high, evidenced by rigorous and extensive experiments across multiple model families (Llama 2/3), scales (1B-70B), and quantization settings (weight-only, weight-activation). \n\nThe clarity is commendable; the paper logically builds from the identified limitation to the proposed solution, with clear mathematical derivations and an efficiently described algorithm."}, "weaknesses": {"value": "In significance, while improvements are consistent, they are often marginal (e.g., <1% accuracy gains in Table 1), raising questions about practical impact. The added calibration memory overhead (Table 5) could be prohibitive for edge devices, yet this trade-off is underexplored. Addressing these points would strengthen the work's relevance and applicability."}, "questions": {"value": "Question 1 : Sensitivity to Calibration Data: Your experiments use a fixed 128 samples. How sensitive are the performance gains to the number and nature of the calibration samples? If the gains diminish significantly with fewer samples or change drastically with a different calibration dataset, it would impact the practical robustness of the method.\n\nQuestion2 : Generalizability Beyond Llama Architectures: The empirical validation is comprehensive but exclusively on the Llama family. Have you observed similar improvements on other prominent architectures, such as GPT-style models (e.g., Qwen, Gemma) or encoder-only models? A result on at least one non-Llama model would greatly bolster the claim of general applicability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WyfaNSOWNw", "forum": "LWYZ1nNkJl", "replyto": "LWYZ1nNkJl", "signatures": ["ICLR.cc/2026/Conference/Submission9296/Reviewer_mxfo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9296/Reviewer_mxfo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805570148, "cdate": 1761805570148, "tmdate": 1762920934669, "mdate": 1762920934669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that compensation-based PTQ methods for LLMs, such as GPTQ and especially GPTAQ, optimize against already compensated activations rather than the original full-precision (FP) layer output, which makes the target drift as we modify the weights inside the same layer. To fix this, the authors re-derive the per-layer objective so that every quantization and compensation step is directly aligned to the FP output, which naturally reveals a missing \"compensation-aware\" residual term capturing the error from earlier intra-layer updates, and they show that this term can be folded in using the same neuron-wise decomposition and Cholesky tools GPTAQ already uses, adding only modest memory/time overhead and delivering consistent quantization improvements for LLaMA 2/3 across especially in low-bit (3-2 bit) settings where quantization errors accumulate the most."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The core insight is simple and just proposes that we should match to the original full-precision output instead of a compensated one.\n- The derivation is intuitive because focusing on full-precision alignment, the missing compensation-aware term appears by default.\n- The method easily fits within GPTAQ style pipelines since it also uses the neuron-wise and Cholesky.\n- There are improvements in difficult low-bit and weight-plus-activation scenarios, which is exactly where we care.\n- The experiments cover several LLaMA variants and quantization settings."}, "weaknesses": {"value": "- Reported improvements are generally modest and in some cases the technique even leads to decreases in performance.\n- The method introduces additional memory and runtime overhead, which is not always justified by the size of the gains.\n- The writing of the paper has inconsistencies, e.g. Table 2 contains a bolding error on L3.1-8B-Inst for C4, where the proposed method is highlighted despite being worse than GPTAQ, which undermines the empirical presentation.\n- Table 1 applies bolding only to the authors’ method even when other techniques perform better (e.g. L3-8B), creating presentation bias in favor of the proposed approach.\n- The approach does not consistently dominate GPTAQ across models, on some models (again L3-8B) average accuracy even decreases in Table 1."}, "questions": {"value": "- The evaluation is limited to LLaMA/LLaMA-3 variants. Evidence of the technique across other families (e.g. Qwen, Phi, Gemma), especially those with different activation/normalization patterns?\n- It would be useful to extend Tables 5 and 6 to larger model to better understand the impact of the technique.\n- How sensitive is the method to calibration set size and distribution?\n- Are there quantization/deployment scenarios where aligning strictly to FP outputs is not the right target (e.g. fully quantized activation pipelines), and how would the method adapt there?\n- Can the authors correct the bolding inconsistencies and re-run significance checks to ensure that the reported improvements are not due to formatting or selection bias?\n\nI'd be happy to increase my score if the concerns are resolved!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lze88yDksr", "forum": "LWYZ1nNkJl", "replyto": "LWYZ1nNkJl", "signatures": ["ICLR.cc/2026/Conference/Submission9296/Reviewer_CWPT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9296/Reviewer_CWPT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936955006, "cdate": 1761936955006, "tmdate": 1762920934333, "mdate": 1762920934333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method that builds upon GPTQ and GPTAQ for efficient finetuning-free quantization. The main idea is to approximate the unquantized model’s activations using quantized weights during the quantization optimization process, aiming to reduce residual errors and improve quantization accuracy. The authors claim that this approach refines the calibration process and better aligns quantized activations with the full-precision model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The topic of efficient and finetuning-free quantization for large transformer models is timely and relevant.\n\n2. The authors provide a clear motivation to address accumulated quantization error across layers.\n\n3. The structure of the paper and the technical presentation follow a standard quantization analysis format."}, "weaknesses": {"value": "1. Sections 3.1 to 3.3 are almost entirely copied or rephrased from the GPTAQ paper. The derivations, notation, and even paragraph flow (e.g., the definitions of asymmetric calibration, residual error formulation, and inverse Hessian update) are reproduced with only superficial wording changes. This raises a serious concern of possible plagiarism.\n\n2. The paper merely extends the asymmetric calibration idea already introduced in GPTAQ. The supposed “rethinking of residual error” is essentially a restatement of GPTAQ’s asymmetric calibration mechanism."}, "questions": {"value": "1. Adding some figures might present the idea of this paper in a better way.\n2. The content of this paper is not self-contained. Without reading the GPTAQ paper, it is hard to understand this paper. For example, it is hard to understand why in Eqn. 4, $\\mathbf{\\tilde{X}}$ is used as the target. The purpose of the background should completely explain the idea of the paper without copying content from previous papers. Concepts in previous papers should be explained in a concise way."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fomxdgWok9", "forum": "LWYZ1nNkJl", "replyto": "LWYZ1nNkJl", "signatures": ["ICLR.cc/2026/Conference/Submission9296/Reviewer_xZwt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9296/Reviewer_xZwt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955907956, "cdate": 1761955907956, "tmdate": 1762920934057, "mdate": 1762920934057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper points out a defective implementation in the previously published GPTAQ method, proposes a corrected algorithm, and demonstrated the effectiveness of the correction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "+ Clear presentation.\n+ Sound analysis.\n+ Compelling empirical results. \n+ Practical significance."}, "weaknesses": {"value": "- I have but one issue with the wording when compared against GPTAQ.  If I understand correctly, the formulation of GPTAQ optimization in matrix form is exactly correct and not challenged by this paper; rather, the row-wise iterative algorithm implementation has been defective and is now corrected to truly align with the optimization problem.  So instead of leaving the reader the impression of this being yet another method, it should be clearly shown as a correction to a previously wrongly implemented existing method, which is no less significant."}, "questions": {"value": "* Minor question on data-efficiency: to achieve the same compensation outcome, does the corrected algorithm require more data than the original GPTQ/GPTAQ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9zkE95GndX", "forum": "LWYZ1nNkJl", "replyto": "LWYZ1nNkJl", "signatures": ["ICLR.cc/2026/Conference/Submission9296/Reviewer_shR7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9296/Reviewer_shR7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965898872, "cdate": 1761965898872, "tmdate": 1762920933558, "mdate": 1762920933558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}