{"id": "ihwAzktmWc", "number": 5758, "cdate": 1757932262587, "mdate": 1759897956097, "content": {"title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards", "abstract": "Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent's policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents.", "tldr": "", "keywords": ["LLM-based Agent", "Multi-agent System"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e5c061bbeb373600e8b1eede723b96667fc84601.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CoMAS (Co-Evolving Multi-Agent Systems), a novel framework that enables LLM-based agents to achieve self-evolution through pure inter-agent interactions without external reward signals. The key innovation lies in its adversarial reward design involving three interaction patterns: solution generation, critical evaluation, and scoring. The framework employs an LLM-as-a-judge mechanism where agents act as solvers, evaluators, and scorers in a zero-sum game, creating complementary rewards that encourage both correctness and critical thinking. Using REINFORCE++ for policy optimization, CoMAS supports heterogeneous agents in a decentralized training paradigm. \n\nThe authors conduct comprehensive experiments across 7 benchmarks (GSM8K, MATH-500, HumanEval, MBPP, SciBench, GPQA, MMLU) and 4 evaluation setups (Vanilla, Consistency, AutoGen, Debate), demonstrating consistent improvements over untrained agents (up to 19.80% in AutoGen setup) and competitive or superior performance compared to baselines (MAPoRL, TTRL). Ablation studies validate the necessity of the adversarial reward formulation and reveal promising scalability with increasing agent numbers and diversity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. **Clear Motivation and Problem Definition:** The paper articulates a compelling research question inspired by human collaborative learning, distinguishing itself from existing self-evolution approaches that focus on individual agents.\n\n2. **Novel Adversarial Reward Design:** The zero-sum game between solvers and evaluators, mediated by an independent scorer, is an elegant solution that prevents reward hacking. The ablation study (Figure 4) convincingly demonstrates that removing either evaluation or scoring leads to training collapse or reward exploitation.\n\n3. **Comprehensive Experimental Validation:** \n   - 7 diverse benchmarks covering math, coding, science, and general knowledge\n   - 4 different inference setups from single-agent to multi-agent collaboration\n   - Comparison with strong baselines (MAPoRL, TTRL)\n   - Extensive ablation studies on reward formulation and scalability\n\n4. **Scalability and Generalizability:** \n   - Performance improves with more agents (Section 4.3.2)\n   - Heterogeneous agents outperform homogeneous ones\n   - Decentralized training enables flexible system design\n   - Skills transfer across domains\n\n5. **Stable Training Dynamics:** Figure 3 shows that rewards converge around 0.5 and response lengths increase consistently, indicating healthy learning progress without collapse.\n\n6. **Strong Performance:** CoMAS achieves state-of-the-art or competitive results across most settings, particularly excelling in multi-agent setups (e.g., 19.80% improvement on AutoGen)."}, "weaknesses": {"value": "1. **Limited Baseline Comparisons:**\n   - MAPoRL baseline uses rule-based verifier instead of the specialized reward model from the original paper, which may not be a fair comparison\n   - Missing comparisons with other recent self-evolution methods mentioned in related work (e.g., REMA, Self-Rewarding LMs)\n   - Would benefit from comparison with debate-only methods without RL training\n2. **Scalability Limitations:**\n   - Experiments only go up to 4 agents; unclear how the framework scales to 10+ agents\n   - All experiments use 3B parameter models; performance with larger models (7B, 13B) is unknown\n   - Training cost analysis is missing (compute requirements, wall-clock time)\n3. **Generalization Questions:**\n   - All training data comes from verifiable domains (math, coding, science); unclear if the framework works for truly open-ended tasks like creative writing or complex reasoning\n   - The framework assumes problems have objectively evaluable solutions, limiting applicability"}, "questions": {"value": "1. **Baseline Fairness:** Why was MAPoRL evaluated with a rule-based verifier instead of its original reward model? How much does this change affect the comparison? Could you also compare with debate frameworks without RL training?\n\n2. **Statistical Significance:** Can you provide error bars or significance tests for the reported improvements? Some gains are modest (~1%), and it's important to know if they're statistically reliable.\n\n3. **Failure Modes:** In what scenarios does CoMAS fail? Can you provide examples of agent discussions where the adversarial mechanism breaks down?\n\n\n4. **Agent Initialization:** All agents start from the same pre-trained model in homogeneous settings. Does initializing with different checkpoints or different random seeds affect the co-evolution dynamics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yXZFWWjt8U", "forum": "ihwAzktmWc", "replyto": "ihwAzktmWc", "signatures": ["ICLR.cc/2026/Conference/Submission5758/Reviewer_qMBz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5758/Reviewer_qMBz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552803802, "cdate": 1761552803802, "tmdate": 1762918242849, "mdate": 1762918242849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the CoMAS (Co-Evolving Multi-Agent Systems) framework, mimicking the human learning mechanism of collaborative interaction and eliminating the need for external supervision. The framework first enables agents to engage in decentralized interactions following a \"solution generation-critical evaluation-structured scoring\" model, then leverages an LLM-as-a-judge mechanism to convert scores into zero-sum game-style intrinsic rewards (incentivizing both solution accuracy and rigorous evaluation), and finally uses the REINFORCE++ reinforcement learning algorithm to update each agent’s policy. Experiments show that across 7 benchmarks as well as single-agent and multi-agent scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper demonstrates good originality by addressing a critical limitation in LLM agent self-evolution. Prior RL-based methods rely on external rewards or single-agent intrinsic signals, while CoMAS introduces a novel \"interaction-driven co-evolution\" paradigm. By mimicking human collaborative learning (via decentralized solution-evaluation-scoring loops) and designing a zero-sum reward system to align agent incentives, it fills the gap of \"unsupervised, interaction-only evolution\" for LLM agents, rather than simply combining existing multi-agent or RL techniques.\n2. The paper is clear in presentation: it explicitly maps each component to solving prior limitations (e.g., format penalties prevent reward hacking), provides detailed formulas for key steps (e.g., reward calculation, advantage function), and uses ablation studies to validate the necessity of core modules."}, "weaknesses": {"value": "1.  The choice of Qwen2.5-3B-Instruct as the base model imposes a fundamental constraint on CoMAS’s evolutionary potential. 3B-scale models are well-documented to struggle with complex reasoning tasks, including multi-step mathematical operations, intricate programming logic, and nuanced open-ended problem-solving, even with advanced prompting. While the paper demonstrates marginal gains over the 3B baseline, it cannot verify whether the CoMAS framework can drive meaningful evolution for models already equipped with stronger inherent capabilities. For instance, Qwen2.5’s 7B variants outperform the 3B version significantly in code generation and mathematical reasoning ; without testing on these larger scales, it remains unclear if CoMAS’s interaction-driven paradigm can amplify the strengths of more capable models or merely compensates for the 3B model’s baseline deficiencies.\n\n2. While the paper shows performance scales with agent count (4>2>1), it lacks critical analysis of how interaction complexity impacts evolution efficiency and omits clear scalability boundaries. For example, it does not address whether increasing agents beyond 4 leads to diminishing returns (e.g., redundant discussions) or higher communication overhead that degrades training speed. Additionally, the \"recent κ rounds\" context truncation rule is set without justification, there is no ablation on how κ values affect discussion quality or reward reliability, leaving readers unsure if the chosen parameter is optimal or generalizable.\n\n3. While the current baselines (untrained agents, MAPoRL, TTRL) help validate CoMAS’s performance against non-interaction-driven methods, the scope of comparative methods appears relatively narrow. That said, I acknowledge uncertainty about the exact number of recently _published_ works that directly align with multi-agent evolution. As such, I recommend cross-referencing feedback from other reviewers on this point: if peers highlight additional relevant methods, expanding the baseline set to include these would strengthen the paper’s claims of superiority."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OCsuuU9MPe", "forum": "ihwAzktmWc", "replyto": "ihwAzktmWc", "signatures": ["ICLR.cc/2026/Conference/Submission5758/Reviewer_CxUs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5758/Reviewer_CxUs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660460787, "cdate": 1761660460787, "tmdate": 1762918242525, "mdate": 1762918242525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a multi-agent self-evolution framework called CoMAS. In this system, agents engage in an interactive cycle of solving–mutual evaluation–scoring, generating dialogue trajectories from which an LLM-as-a-judge extracts intrinsic rewards. Each agent then optimizes its policy through reinforcement learning, enabling unsupervised, verifier-free co-evolution. Across multiple benchmarks and reasoning settings, CoMAS achieves stable improvements over untrained agents and even reaches state-of-the-art performance in some configurations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n\n2. The research problem is well-defined, and constructing rewards from a multi-agent perspective is a promising direction."}, "weaknesses": {"value": "- From my perspective, the novelty is limited, obtaining rewards through multi-agent discussions is not a new idea.\n\n- The Related Work section lacks detailed and structured discussion of prior work, which blurs the paper’s contribution. The authors should provide a comprehensive overview of previous methods and explicitly highlight the differences between their method and existing ones.\n\n- Most comparisons are conducted under the paper’s unified pipeline rather than under each method’s native optimal setup, which may underestimate strong baselines.\n\n- The figures emphasize heterogeneous LLMs, but most experiments are based on homogeneous LLMs."}, "questions": {"value": "1. Could the authors systematically review recent works and clearly articulate the core contribution of this paper? At present, it is hard to see what differentiates it from existing methods? What specific shortcomings of prior works does CoMAS address?\n\n2. Please report token, wall-clock time, and memory curves for different agent counts; this is one of the main limitations of the current study.\n\n3. Besides the unified pipeline, please reproduce baselines in their native configurations and re-run CoMAS for fair comparison.\n\n4. According to my understanding, the proposed method and baselines do not use the same RL algorithm (e.g., CoMAS uses REINFORCE++ instead of GRPO). This inconsistency may introduce confounding factors. I suggest using the same RFT algorithm as the baselines for a fair comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XM6LOxQKNO", "forum": "ihwAzktmWc", "replyto": "ihwAzktmWc", "signatures": ["ICLR.cc/2026/Conference/Submission5758/Reviewer_F6Fu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5758/Reviewer_F6Fu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814376270, "cdate": 1761814376270, "tmdate": 1762918242200, "mdate": 1762918242200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CoMAS is a multi-agent system that maintains a population of LLM agents and iteratively finetunes them. In each fine-tune iteration, the agents act as three roles: a solver that generates solutions, an evaluator that evaluates the solution, and a scorer that generates a score based on the solution and the evaluation. The score serves as an adversarial reward that is then used to optimize the scorer and the evaluator with RL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The writing is clear and well-organized.\n- The proposed method is novel to my knowledge.\n- The evaluation is detailed, including comprehensive benchmark results and ablations.\n- In the evaluations, CoMAS establishes advantages over existing baselines in most scenarios.\n- Prompt templates and examples, as well as training hyperparameters are provided for reproducibility."}, "weaknesses": {"value": "- CoMAS generally outperforms other baselines. However, the performance gap is only significant in a minority of the tasks - mostly with AutoGen setup. Furthermore, both \"strong\" baselines, MAPoRL and TTRL, almost never have significant improvement over the naive untrained baseline. \n- I am not yet convinced that this reward formulation solves the reward hacking problem. This formulation does create an adversary between the solver and the evaluator, but it does not ensure that the generated contents are grounded, meaning both parties can be reward hacking at the same time. I doubt this will be a real issue, but it would be helpful to show some evidence or counterpoints."}, "questions": {"value": "- From the evaluations, the AutoGen setup result is siginificantly lower than the rest. Some insights to this would be helpful.\n\n### Some minor issues\n- Figure 2 is cute but I don't think it's necessarily information-condense. I don't see why you need to show three different threads, as all three parties are involved even in one of them. This is a minior issue and even personal preference.\n- The numbers in Figure 4 (left) and 5 are too small and hardly visible when printed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XWCr1O4Dcl", "forum": "ihwAzktmWc", "replyto": "ihwAzktmWc", "signatures": ["ICLR.cc/2026/Conference/Submission5758/Reviewer_bqfG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5758/Reviewer_bqfG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966833156, "cdate": 1761966833156, "tmdate": 1762918241991, "mdate": 1762918241991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}