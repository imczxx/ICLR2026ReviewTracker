{"id": "s5a8zBPFfe", "number": 19557, "cdate": 1758297256216, "mdate": 1759897032763, "content": {"title": "Pareto Variational Autoencoder", "abstract": "Incorporating robustness in generative modeling has enticed many researchers of the field.\nTo this end, we introduce a new class of multivariate power-law distributions---the symmetric Pareto (symPareto) distribution---which can be viewed as an $\\ell_1$-norm-based counterpart of the multivariate $t$ distribution.\nThe symPareto distribution possesses many attractive information-geometric properties with respect to the $\\gamma$-power divergence that naturally populates power-law families.\nLeveraging on the joint minimization view of variational inference, we propose the ParetoVAE, a probabilistic autoencoder that minimizes the $\\gamma$-power divergence between two statistical manifolds.\nParetoVAE employs the symPareto distribution for both prior and encoder, with flexible decoder options including Student's $t$ and symPareto distributions.  Empirical evidences demonstrate ParetoVAE's effectiveness across multiple domains through varying the types of the decoder. The $t$ decoder achieves superior performance in sparse, heavy-tailed data reconstruction and word frequency analysis; the symPareto decoder enables robust high-dimensional denoising.", "tldr": "", "keywords": ["Variational autoencoder", "Pareto distribution", "Information geometry", "Heavy-tail learning", "Heavy-tail Modeling"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e551ebe7a6df40a898907e9ceb663edd5328e369.pdf", "supplementary_material": "/attachment/2269efd7c3fdb4d5d054c2a92d93bfa8ca09b460.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel VAE based on the information geometry of $\\gamma$-divergence together with a multivariate symmetric Pareto (symPareto) distribution. Since the symPareto distribution is heavy-tailed and promotes sparsity, it improves both the performance and robustness of VAEs. A variety of experiments validate the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Introducing complex distributions into VAEs is difficult because one must compute the KL divergence between the encoder and the prior and also sample from the encoder via the reparameterization trick. This paper incorporates a complex distribution like symPareto into a VAE in a theoretically sound and tractable way. The empirical results are also strong."}, "weaknesses": {"value": "Please see the Questions section."}, "questions": {"value": "- In the proposed VAE, symPareto is used for the encoder and the prior, and either a t-distribution (L2) or symPareto (L1) is used for the decoder. Which component is particularly effective? For example, if the goal is to induce sparsity in the latent variables, using only a symPareto prior may be sufficient. If the KL can be computed, it even seems acceptable to keep the encoder Gaussian. On the other hand, when observations are noisy or sparsity is required, using a t-distribution or symPareto for the decoder appears effective. For the image denoising experiment, for instance, it seems a t-decoder should handle the task; yet the proposed method achieves higher performance than t-based VAEs on all datasets. Where does this performance gain come from?\n- Using toy data, can you visualize how the latent variables behave under a symPareto prior? For example, as in Figure 6 of [1], train on a four-dimensional one-hot vector dataset and visualize the two-dimensional latent variables. I believe this would make the effect of the symPareto prior easier to understand.\n- VAEs often suffer performance degradation due to over-regularization by the prior. Does employing a symPareto prior resolve this issue? Moreover, it is known that the aggregated posterior (a infinite mixture distribution of the encoder) can address this problem [2]. Could your method be combined with such approaches?\n\n[1] Mescheder, Lars, Sebastian Nowozin, and Andreas Geiger. \"Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks.\" International conference on machine learning. PMLR, 2017.\n\n[2] Tomczak, Jakub, and Max Welling. \"VAE with a VampPrior.\" International conference on artificial intelligence and statistics. PMLR, 2018."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "19xNDi49og", "forum": "s5a8zBPFfe", "replyto": "s5a8zBPFfe", "signatures": ["ICLR.cc/2026/Conference/Submission19557/Reviewer_iLe7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19557/Reviewer_iLe7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813594277, "cdate": 1761813594277, "tmdate": 1762931439165, "mdate": 1762931439165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript discusses changing enforcing the Gaussianity of the distribution in the latent space of the variational autoencoders (VAE).\nThe manuscript proposes  ParetoVAE, a probabilistic autoencoder that minimizes the γ-power, instead of the Gaussian assumption as in the classical VAE, divergence between two statistical manifolds. Also Student's t and symPareto distributions are considered. These types of VAEs show good results in sparse, heavy-tailed data reconstruction and word frequency analysis.\nThe symPareto decoder enables robust high-dimensional denoising."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The manuscript aims to develop a VAE that would be more appropriate for certain types of data.\n- The corresponding loss function for the new VAE is derived.\n- An application seems to be in denoising, when images are corrupted by Salt and Pepper noise"}, "weaknesses": {"value": "- Lack of theoretical development to justify why the new distributions are necessary in certain data representations. While the manuscript provides statistical distributions known in statistical analysis, it does not connect these with data reconstruction.\n- Experimental results are rather limited, especially in real data."}, "questions": {"value": "What is the performance of the ParetoVAE in the case when representing images corrupted by Gaussian noise?\n\nMore references should have been provided for the statistical derivations from the Appendices, in the supplementary material."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sP1kgmRJGK", "forum": "s5a8zBPFfe", "replyto": "s5a8zBPFfe", "signatures": ["ICLR.cc/2026/Conference/Submission19557/Reviewer_gNb7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19557/Reviewer_gNb7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835930057, "cdate": 1761835930057, "tmdate": 1762931438592, "mdate": 1762931438592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Pareto variational auto encoder. The model addresses the problem of modeling heavy tailed data within the VAE framework, which most typically use exponential family distributions. The paper offers a detailed and clear mathematical exposition for the multivariate Pareto distribution, uses information geometric tools to construct gamma flat manifold to analyze gamma power divergence, and presents detailed simulation results comparing to relevant baseline models on data with heavy tailed structure including graphs and language."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Strengths of this paper include:\n- The mathematical exposition was very nice. \n- The core motivation of modeling heavy tailed data was clearly borne out in experiments\n- The proposed model raised interesting challenges that were well resolved mathematically. \n- The approach was compared with a set of baselines that gave a clear sense of why the Pareto model was succeeding"}, "weaknesses": {"value": "Weaknesses include:\n- The introduction could have been more concrete sooner. Many of the arguments in the introduction would have been stronger if grounded in specific examples of data that are heavy tailed, specifically which models are under consideration, etc. The remainder of the paper answered these questions well. \n\n\nDetailed comments:\n- \"Incorporating robustness in generative modeling has enticed many researchers\nof the field.\" This sentence is not necessary. \n- \"To this end, ...\" To the end of enticing researchers into the field? \n- \"the γ-power divergence that naturally populates power-law families\" what does populate mean here?\n- \"Conventional VAEs typically employ exponential-family distributions, most notably the Gaussian, for their probabilistic model due to their mathematical tractability.\" This is typical but certainly not universally true. Therefore the next sentence \"To address these limitations,\" doesn't follow. It is necessary to list the exceptions to the convention and argue why they don't address the problem you want to solve. \n- \"exhibit heavy tails and extreme events\" What are the events you have in mind here? Is there evidence that standard VAEs haven't succeeded in modeling these events? Have modelers just not tried to use VAEs?\n- What do you mean precisely for a numerical integration to be intractable? (Line 044)\n- Table 1 is not particularly informative. Is there something important that I am missing there? \n- typo: exponentiall\n- typo: includde\n- Two papers that are not cited but may be of interest are the InfoVAE and Coupled VAE (cites below) which connect to entropic OT (itself connected to Information Geometry). \n\nZhao, S., Song, J., & Ermon, S. (2017). Infovae: Information maximizing variational autoencoders. arXiv preprint arXiv:1706.02262.\nHao, X., & Shafto, P. (2023). Coupled variational autoencoder. arXiv preprint arXiv:2306.02565."}, "questions": {"value": "I don't have major questions. I enjoyed the paper, thanks!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iVqqSbpAjJ", "forum": "s5a8zBPFfe", "replyto": "s5a8zBPFfe", "signatures": ["ICLR.cc/2026/Conference/Submission19557/Reviewer_dmgt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19557/Reviewer_dmgt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930452704, "cdate": 1761930452704, "tmdate": 1762931438117, "mdate": 1762931438117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the symmetric multivariate Pareto distribution (symPareto) and leverages it to build ParetoVAE, a new class of heavy-tailed variational autoencoders. The authors motivate the need for power-law latent distributions when modeling real-world data with extreme values or rare events, addressing well-known limitations of Gaussian-based VAEs. The work is grounded in a solid information-geometric perspective: the γ-power divergence is employed to derive a tractable VAE objective, circumventing difficulties associated with ELBO estimation for power-law distributions. Empirical studies across multiple domains (including noisy image reconstruction and denoising tasks) demonstrate improved robustness to outliers, better sparse-data handling, and enhanced recovery of fine-grained features relative to baseline VAEs and heavy-tailed t-VAEs. Overall, this paper makes a meaningful and well-motivated contribution to the study of robust generative modeling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong conceptual motivation for replacing exponential-family assumptions with heavy-tailed distributions in real-world generative modeling.\n\n2. Novel modeling component: introduction of the symPareto distribution as a multivariate power-law prior/encoder, extending beyond the Student’s-t VAE line.\n\n3. Theoretically grounded via γ-power divergence and joint minimization formulation, providing a tractable objective.\n\n4. Empirical benefits validated across several benchmarks, especially in high-noise and heavy-tailed settings. and Practical insights into when symPareto vs Student’s-t decoders are beneficial.\n\n5. Clear writing and strong experimental motivation, making the work accessible despite its mathematical basis."}, "weaknesses": {"value": "1. Computational trade-offs not fully quantified: while the γ-loss improves tractability, a more detailed cost vs. ELBO comparison would help practitioners.\n\n2. Ablations on γ-power choices and sensitivity analysis would improve clarity on hyperparameter robustness."}, "questions": {"value": "Please see above weakness for questions to answer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ciYL9iliFp", "forum": "s5a8zBPFfe", "replyto": "s5a8zBPFfe", "signatures": ["ICLR.cc/2026/Conference/Submission19557/Reviewer_gUPd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19557/Reviewer_gUPd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950646631, "cdate": 1761950646631, "tmdate": 1762931437728, "mdate": 1762931437728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}