{"id": "BNtGjamZ3S", "number": 18550, "cdate": 1758289036503, "mdate": 1759897096344, "content": {"title": "A Bootstrap Perspective on Stochastic Gradient Descent", "abstract": "Machine learning models trained with stochastic gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.", "tldr": "Stochastic gradient descent uses its gradient variability as a bootstrap estimate of the algorithmic variability and implicitly regularizes this estimate to help generalization.", "keywords": ["Stochastic gradient descent", "generalization", "bootstrap estimation"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4e5fadedba48eea8d4d6bcb230c07e2b34390b12.pdf", "supplementary_material": "/attachment/22b72bcac2ca3ed26b1f4aaac5b0eaa42f258405.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a theoretical framework for understanding the generalization properties of Stochastic Gradient Descent (SGD). The authors decompose the generalization gap and introduce the concept of \"algorithmic variability\", which they analyze through the lens of statistical bootstrapping. Based on this decomposition, the authors construct two novel regularizers and empirically validate that their inclusion can lead to improved generalization performance on tasks including sparse regression and neural network training.\nHowever, there are still some concerns to me. Therefore I lean to a rejection at the time being. \nSpecifically, I am not sure whether the idea in this paper has significant differences to algorithm stability, and whether the derivation of this paper is meaningful. \nSee below for more details."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper posits that SGD uses the gradient variability (caused by mini-batch sampling) as a \"bootstrap estimate.\n2. This paper proves that the expected generalization gap is determined by the trace of the product of the solution's Hessian matrix and the \"algorithmic variability\" matrix.\n3. This paper designs a new regularizer based on the theoretical findings. \n4. The authors further provide empirical evidence on this regularizer."}, "weaknesses": {"value": "1. [Major Concern] It seems that Assumption 2 directly leads to a small Variability (Eqn 3). However, the authors did not discuss it much. If so, I cannot be convinced that Eqn (3) is the dominate term compared to Eqn (4), where Eqn (4) also contains the epsilon[2, T] term. \n2. [Major Concern] I am not convinced that this paper has significant differences with the line of algorithmic stability. The authors claim in Line 466 that \"this paper considers \"Hessian-weighted and evaluated at the solutions\"\". It seems that algorithmic stability can include this case with pretty minor changes. Due to the simplicity, algorithm stability just bound the Hession with smoothness, and use iteration to reach the solution. But starting from the definition of algorithm stability, these are not necessary. The authors shold provide more evidence on how this paper performs differently with algorithm stability. \n\n[Minor]\n1. The authors claim that \"we prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability.\" According to the paper's derivation, the algorithmic variability is bounded by two components (corresponding to the latter term in Eq. 6 and Eq. 7). While the authors convincingly connect the implicit regularization of SGD, as identified by Smith et al. (2021), to the first component (Eq. 6), they do not provide evidence or argumentation that SGD also implicitly regularizes the second component (Eq. 7). Consequently, the claim that SGD \"controls the algorithmic variability\" in its entirety appears to be an overstatement. \nThis significantly limits their theoretical contribution, as the work seems to demonstrate that vanilla SGD only addresses a part of the problem identified by the authors. \n2. The paper's analysis of the proposed regularizers, Reg1 and Reg2, lacks sufficient depth regarding their interplay and individual utility. For instance, given that the authors identify Reg1 as an existing *implicit* regularizer of SGD, a crucial discussion is missing on the utility of its *explicit* inclusion. What is the tangible difference between applying Reg1 explicitly versus relying on its implicit effect? Would applying only Reg2, which is the component not addressed by vanilla SGD, be a more practical and principled approach? The paper would be substantially strengthened by ablation studies that dissect the individual contributions of Reg1 and Reg2 and clarify their roles in guiding SGD towards better-generalizing solutions.\n3. The practical significance of this work is severely hampered by the unaddressed computational overhead of the proposed regularizers. Both Reg1 and Reg2, as defined, require the computation of the full-batch gradient at each training step. This is a prohibitive cost for large-scale datasets and fundamentally contradicts the core philosophy of SGD, which is designed precisely to avoid such computations. The absence of any discussion on this issue, or on potential efficient approximations, makes it difficult to assess the empirical value of the proposed method. As it stands, the practical guidance offered by the paper appears limited."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ra5hRLCuVB", "forum": "BNtGjamZ3S", "replyto": "BNtGjamZ3S", "signatures": ["ICLR.cc/2026/Conference/Submission18550/Reviewer_aZc8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18550/Reviewer_aZc8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793154008, "cdate": 1761793154008, "tmdate": 1762928260337, "mdate": 1762928260337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies SGD's impact on generalization for machine learning models. Based on the provided  analyses, it proposes two regularization schemes, which are shown to benefit generalization for a few toy datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The question raised in the paper is important and the paper tsts a new regularization method based on the analyses and shows that it might benefit generalization"}, "weaknesses": {"value": "The theoretical contribution appears to be incremental, as, to my understanding, the main insights came from Smith et al. (2021).  The empirical evaluation is very limited, as the results are tested only on a very specific synthetic dataset with a sparse prior and FashionMNIST."}, "questions": {"value": "1) I did not understand how the analyses are specific to the SGD as opposed to the non-stochastic GD. As the opening sentence of the abstract mentions the difference between generalization of GD and SGD as a motivation, I would like to ask the authors to elaborate more on this. How can we see from the bounds derived in the paper that SGD might outperform GD? \n\n2) As for the regularizers part, what are the novel insights made in the paper compared to Smith et al. (2021)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SjaoQYhbMb", "forum": "BNtGjamZ3S", "replyto": "BNtGjamZ3S", "signatures": ["ICLR.cc/2026/Conference/Submission18550/Reviewer_TGMQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18550/Reviewer_TGMQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947451507, "cdate": 1761947451507, "tmdate": 1762928259262, "mdate": 1762928259262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tries to understand SGD from the view of bootstrapping: SGD favors minima with smaller variance of stochastic gradient."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The top example in Section 2 is attractive and illustrative."}, "weaknesses": {"value": "1. The presentation of the theoretical part is a bit confusing.\n- The theoretical results are listed as Lemmas 1 and 2 as well as Proposition 1, without a theorem that usually serves as the center of discussions. This makes me confused about what is the main theoretical contribution of the paper.\n- The discussions after Lemmas 1 and 2 mainly discuss why the lemmas hold, and do not actually help with the understanding of the theoretical results (especially for Lemma 2, whose righthand side has a lot of terms).\n2. My understanding is that the core of the theoretical analysis is the correspondence of Equations (6) and (7) with Equations (10) and (11), which provides a viewpoint from the implicit regularization of SGD by \"bootstrapping\" the gradients. However, this part lacks a comparison against GD or noisy GD.\n3. According to my understanding, the technical contribution is minor. Lemmas 1 and 2 are basically Taylor expansion, and Proposition 1 is basically the strong law of large numbers.\n\nI would honestly confess that I do not understand all the details of the paper, and would be happy to discuss with the authors, other reviewers and the AC. My score of 2 currently represents my unconfident understanding. i think the intuition of the paper is good, but the theoretical part may need improvements."}, "questions": {"value": "1. Can the authors show more details of the algorithm SGDwReg2, especially how to estimate the term Reg2?\n- If Reg2 is estimated in an exact way, then SGDwReg2 requires knowledge of the entire dataset at each minibatch update. In this case, is it possible to design an adaptation of SGD that incorporates the idea of SGDwReg2 but without the requirement of the entire dataset?\n- If Reg2 is approximated, can the authors show the details of approximation?\n2. How does the bootstrapping view compare with the idea of variance-reduction techniques like SVRG?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0aoGRhk5p3", "forum": "BNtGjamZ3S", "replyto": "BNtGjamZ3S", "signatures": ["ICLR.cc/2026/Conference/Submission18550/Reviewer_7CRg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18550/Reviewer_7CRg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979038494, "cdate": 1761979038494, "tmdate": 1762928258666, "mdate": 1762928258666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to provide a novel eccplanation for the superior genetalozation property of SGD compared wirh GD, from a boostrap perspctive. Specifically, under certein assumptions, the authors show that the generalization error can be decomposed into a dominant Hessian=-preconditioned algorithmic variability term and several small terms. They further argue that the algorithmic variavbilit is stronhly correlated to the accumulated empirical covriance of gradients. As a consequence, they empirically estalish that SGD regularizes algorithmic variability as a bootstrap estimate, and hence improving the generalization error through this correlation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "This paper is clearly written and has a nice structure."}, "weaknesses": {"value": "Although the authors provide an upper bound on the generalization error via algorithmic stability, the paper does not explicitly establish how SGD regularizes this term theoretically. Moreover, there is no theoretical characterization of the generalization gap between SGD and GD. Another concern arises from the assumptions: while Assumption 1 appears standard, Assumption 2 is rather demanding and may not hold in many scenarios: existing theoretical results generally suggest that the upper bound on uniform algorithmic stability grows with the number of iterations. This implies that the bias term, rather than variance, often dominates the generalization error. From this perspective, the argument that “SGD generalizes better because it regularizes the gradient variance” may not be entirely convincing."}, "questions": {"value": "No further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "This work is purely theoretical and has no negative ethical concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tIOE0zH2In", "forum": "BNtGjamZ3S", "replyto": "BNtGjamZ3S", "signatures": ["ICLR.cc/2026/Conference/Submission18550/Reviewer_1WA5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18550/Reviewer_1WA5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985656479, "cdate": 1761985656479, "tmdate": 1762928257922, "mdate": 1762928257922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}