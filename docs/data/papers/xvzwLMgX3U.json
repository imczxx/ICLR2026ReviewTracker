{"id": "xvzwLMgX3U", "number": 20718, "cdate": 1758309362830, "mdate": 1759896961957, "content": {"title": "Soft Constraints, Strong Solutions: Optimizing Intra-Operator Parallelism for Distributed Deep Learning", "abstract": "As deep learning models continue to increase in size and complexity, mapping their computations efficiently onto distributed hardware has become a central challenge in systems and compiler design. A key technique for addressing this challenge is intra-operator parallelism, which involves partitioning individual operations across multiple devices. This enables large-scale models to make more effective use of available hardware while satisfying strict memory and communication constraints. The ASPLOS / EuroSys 2025 Contest on Intra-Operator Parallelism for Distributed Deep Learning formalized this challenge as a constrained combinatorial optimization problem, requiring a strategy assignment for each graph node that minimizes total cost while respecting time-varying memory limits. This paper presents the top-performing solution to the contest, based on usage-constrained relaxation, which incorporates memory usage directly into the cost model rather than enforcing it as a hard constraint. Together with adaptive weight tuning, the method guarantees valid assignments and scales to computation graphs with tens of thousands of nodes. The solver achieves state-of-the-art results across all contest benchmarks, consistently producing low-cost solutions within strict time limits. The paper details the core algorithmic components and discusses their broader applicability to compiler-level optimization in distributed deep learning.", "tldr": "We present a scalable relaxation-based solver for intra-operator parallelism that consistently produces low-cost solutions under tight time budgets and outperforms XLA by up to several orders of magnitude.", "keywords": ["distributed deep learning", "intra-operator parallelism", "competition", "cost function network"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2773d92ef04dba74ad08560b75f93e680413bf87.pdf", "supplementary_material": "/attachment/b5120c68a50dae947abc744c37771aaef96fd408.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose an approximate algorithm for a combinatorial optimization problem derived from intra-operator parallelization in distributed LLM training and inference. Given a graph where each node has an activation time interval and a set of strategies (each with memory usage and time cost), and where each edge's time cost depends on the strategies chosen at its endpoints, the goal is to assign a strategy to every node to minimize total time subject to a memory budget. The proposed algorithm won an ASPLOS contest for this problem. Experiments show the algorithm typically finds solutions with latency between 1× and 2× the optimal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The algorithm is simple yet effective.\n2. The optimization problem comes from a real deployment scenario, so the work has practical impact.\n3. The algorithm outperforms other contest teams and the production compiler XLA."}, "weaknesses": {"value": "1. The paper would benefit from a clearer connection between the formal optimization problem and the original intra-operator parallelization task in LLM training/serving.\n2. It is surprising that a well-optimized system like XLA can be an order of magnitude slower in some cases; the paper should provide more explanation for this gap."}, "questions": {"value": "Thanks for submitting to ICLR 2026! I enjoyed reading the paper; it is well written and easy to follow. The algorithm is simple and effective. I have a few suggestions to improve clarity and impact.\n\n**1. Reconnect the optimization problem to intra-operator parallelization.**\n\nThe ASPLOS contest abstracts a production problem into a combinatorial formulation, but the paper should better describe the original system-level problem so the paper is self-contained. This will help readers map the mathematical solution back to a concrete scheduling or parallelization strategy in production.\n\n**2. Explain differences with XLA's schedule.**\n\nIf the reported cost measures execution latency of the computation graph under a strategy assignment, it is surprising that XLA’s approach can be much slower. An in-depth comparison (or diagnosis) explaining why your solution improves over XLA, for example, differences in objective, search space, heuristics, or memory-time trade-offs, would strengthen the paper.\n\nOverall, this is a solid and practical contribution. Addressing the two points above will make the paper more useful to practitioners and clearer to reviewers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rwrAXB5DEc", "forum": "xvzwLMgX3U", "replyto": "xvzwLMgX3U", "signatures": ["ICLR.cc/2026/Conference/Submission20718/Reviewer_i5Lc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20718/Reviewer_i5Lc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761345837789, "cdate": 1761345837789, "tmdate": 1762934117362, "mdate": 1762934117362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an intra-operator parallelism solution for a contest, working on a solver for a formulated assignment optimization problem. The key idea is to optimize the objective with relaxed constraints first and greedily adjust the assignment later. It is more of a contest report than an academic paper. It makes little academic contribution: the problem is not new; the method is not novel; the experiment is preliminary."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed method is a top-performing solution to a contest."}, "weaknesses": {"value": "1. The defined problem for intra-operator parallelism is not contributed by the authors, though the problem itself does not contribute novelty and academic significance either. \n2. Authors intentionally design the proposed method (e.g., Techniques 1 and 2 in the proposed method) to fit into the provided workload, while not considering a method for real-case workloads.\n3. The idea of solving a relaxed-constraint optimization problem and the adopted algorithm are nothing novel. \n4. The proposed method is not properly and throughly verified. The experiment tried to evaluate a solution for a large-scale problem using a ``low''-configured server. Evaluations on a certain scale computing cluster with realistic distributed learning workloads can be much more convincing."}, "questions": {"value": "1. I would advise authors to work on realistic system problems on intra-operator parallelism with evaluation on realistic workloads and environments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "riTLCvmSfz", "forum": "xvzwLMgX3U", "replyto": "xvzwLMgX3U", "signatures": ["ICLR.cc/2026/Conference/Submission20718/Reviewer_jAiB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20718/Reviewer_jAiB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737170181, "cdate": 1761737170181, "tmdate": 1762934116522, "mdate": 1762934116522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a formal framework for integrating soft constraints—constraints that can be selectively relaxed with associated penalties—into systems that require strong guarantees of correctness and safety. The method achieves promising results on several verification and optimization tasks, showing that it can flexibly handle trade-offs between strict guarantees and optimization goals."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This manuscript is well organized, presenting a logically coherent structure .\n2. The paper tackles an important and challenging problem, and addresses an interesting topic .\n3. The proposed framework is theoretically sound and shows strong empirical outcomes."}, "weaknesses": {"value": "1. Lack of interpretability or analysis of mechanism. The paper reports strong results but does not explain why the method performs well. A deeper ablation and theoretical intuition would improve clarity.\n2. Methodological nature unclear. The approach appears algorithmic rather than learning-based. It would help if the authors clarified whether any learning or adaptation is involved, or whether the framework is purely a deterministic optimization procedure."}, "questions": {"value": "1. Could the authors provide a more detailed explanation of why your method achieves such good results?\n2. Can the authors clarify how the system differs from, or could potentially connect to, deep learning–based constrained optimization methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GHGWQIZbcd", "forum": "xvzwLMgX3U", "replyto": "xvzwLMgX3U", "signatures": ["ICLR.cc/2026/Conference/Submission20718/Reviewer_oxsL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20718/Reviewer_oxsL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804068303, "cdate": 1761804068303, "tmdate": 1762934115981, "mdate": 1762934115981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a top-performing solution to the ASPLOS 2025 Contest on Intra-Operator Parallelism for Distributed Deep Learning. The authors propose a solver based on usage-constrained relaxation, where memory requirements are incorporated as soft constraints within the cost model rather than enforced as hard limits. The method uses adaptive weight tuning and greedy post-processing to efficiently generate feasible, low-cost solutions across large computational graphs. The approach achieves state-of-the-art results, significantly outperforming commercial compilers such as XLA and even approaching theoretical lower bounds on several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of encoding memory requirements as soft constraints with adaptive penalties is both simple and elegant. It transforms a difficult combinatorial optimization problem into a tractable form while maintaining feasibility.\n2. The paper provides extensive experimental validation, including comparisons with commercial compilers and exact solvers, thorough convergence and ablation studies, and a detailed analysis of contest benchmarks."}, "weaknesses": {"value": "1. The paper reads more like a technical report rather than a research paper. The narrative emphasizes implementation details and experimental results but gives less attention to the high-level conceptual intuition behind the approach.\n2. Much of the background context and some of the benchmark and contest results overlap with material already presented in the official ASPLOS contest report.\n> Moffitt, Michael D. and Fegade, Pratik, \"The ASPLOS 2025 / EuroSys 2025 Contest on Intra-Operator Parallelism for Distributed Deep Learning\", Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 2025."}, "questions": {"value": "1. How many variables $x_i$ are there in each $c_i$?\n2. Why does each node i have a separate $w_i$? It seems $w_i$ is global from the algorithm.\n3. For the post-processing step, does the solver traverse the entire computation graph to identify candidate nodes, or is there a targeted strategy for selecting which nodes to revisit?\n4. It would be informative to include a latency breakdown across different stages of the solver (e.g., preprocessing, relaxation solving, and greedy refinement) to better understand where most computation time is spent.\n5. How sensitive is the final performance to the initial solution? Does the adaptive tuning always converge to similar-quality results regardless of the starting point?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZbRqnGaoaV", "forum": "xvzwLMgX3U", "replyto": "xvzwLMgX3U", "signatures": ["ICLR.cc/2026/Conference/Submission20718/Reviewer_hu4K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20718/Reviewer_hu4K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967991905, "cdate": 1761967991905, "tmdate": 1762934115107, "mdate": 1762934115107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}