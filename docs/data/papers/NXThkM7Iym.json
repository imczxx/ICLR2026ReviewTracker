{"id": "NXThkM7Iym", "number": 9722, "cdate": 1758136605797, "mdate": 1763712217453, "content": {"title": "PaAno: Patch-Based Representation Learning for Time-Series Anomaly Detection", "abstract": "Although recent studies on time-series anomaly detection have increasingly adopted ever-larger neural network architectures such as transformers and foundation models, they incur high computational costs and memory usage, making them impractical for real-time and resource-constrained scenarios. Moreover, they often fail to demonstrate significant performance gains over simpler methods under rigorous evaluation protocols. In this study, we propose Patch-based representation learning for time-series Anomaly detection (PaAno), a lightweight yet effective method for fast and efficient time-series anomaly detection. PaAno extracts short temporal patches from time-series training data and uses a 1D convolutional neural network to embed each patch into a vector representation. The model is trained using a combination of triplet loss and pretext loss to ensure the embeddings capture informative temporal patterns from input patches. During inference, the anomaly score at each time step is computed by comparing the embeddings of its surrounding patches to those of normal patches extracted from the training time-series. Evaluated on the TSB-AD benchmark, PaAno achieved state-of-the-art performance, significantly outperforming existing methods, including those based on heavy architectures, on both univariate and multivariate time-series anomaly detection across various range-wise and point-wise performance measures.", "tldr": "PaAno is a lightweight yet effective method for time-series anomaly detection, leveraging patch-based representation learning with a simple 1D-CNN. It outperforms heavyweight methods based on transformers and foundation models.", "keywords": ["Time-Series Anomaly Detection", "Representation Learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/077806e6a9dbddfcee8c18374edfd718e6262f97.pdf", "supplementary_material": "/attachment/2c37a30a272ae2521cb7382c6a61950af3b13670.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes PaAno (Patch-based representation learning for time-series Anomaly detection), a lightweight framework for semi-supervised time-series anomaly detection. Instead of using large transformer or foundation models, PaAno applies a 1D-CNN to extract vector embeddings from short overlapping temporal patches. The model is trained with a triplet loss (for metric learning) and a pretext loss (for predicting temporal adjacency between patches). A memory bank of normal patch embeddings is built for inference, where anomalies are identified via distance to nearest normal embeddings. Evaluated on the TSB-AD benchmark, PaAno achieves state-of-the-art results across univariate and multivariate datasets, outperforming heavier architectures while remaining computationally efficient. Ablation and sensitivity analyses confirm the contributions of both loss terms and robustness to hyperparameters."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Originality: Fair\n\nQuality: Good\n\nClarity: Fair\n\nSignificance: Good\n\n\nAdditional note: The paper is mostly well written and the results are extremely promising. The methodology is clearly presented and mostly easy to follow. The methodology is an extension of existing research but is applied to a new domain in a clever and relevant way."}, "weaknesses": {"value": "Hyper-parameter tuning: The paper claims the model is resilient to hyper-param tuning, but the paper does not show the affect of tuning the hyper params in triplet loss and combined loss. Additionally, as the triplet loss samples the current mini batch at inference time, would the mini batch size not be a big hyper-param design consideration?\nTraining details: There is no motivation given to as why the pretext loss weight decays to 0 within the first 20 iterations, this is in the appendix, I believe this is a major detail that needs to be discussed as the paper claims that the pretext loss is essential for the model performance.\n\nGeneralization to other datasets: Only one dataset was used to evaluate methods.\n\nFormatting/Editing Mistakes: Table 10 in Appendix E3 shows a row called Total , but it shows (what i assume is) the mean of the different metrics in the table.\n\nLimited theoretical grounding: The paper lacks formal analysis explaining why the patch-based embedding space generalizes effectively to unseen anomalies.\n\nUnsubstantiated assertions: In section 3.1, the paper asserts that “Modelling long sequences with heavy global attention can dilute these local temporal dependencies”. This is not backed by a citation or theoretical or empirical proof. Additionally the term “heavy” here is ambiguous."}, "questions": {"value": "What is the motivation behind decaying the weight for pretext loss?\n\nIn section 3.5, a visual representation of even a pseudo code of the anomaly detection method would aid in clarity.\n\nIs the sampling of negative patch done at inference time during the forward loop?\n\nWhat is the embedding space that is used to select a negative sample? Is it the embedding space of the mode? If so how is the initialization handled when the model is not trained?\n\nIn section 3,3, i need some clarification. I think there is an error. THe paper states “ We define the negative patch pi as the hard negative, chosen as the patch in the minibatch B that has the smallest cosine distance to pi in the embedding space, i.e.,\npj ∈ B \\ {pi } that minimizes dist(fθ (pi ), fθ (pj )).“ Shouldnt the distance be maximized here to find a negative patch that is the most dissimilar to the patch p?\n\nI am willing to improve the rating if my questions are answered."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v8Tt6xyqwp", "forum": "NXThkM7Iym", "replyto": "NXThkM7Iym", "signatures": ["ICLR.cc/2026/Conference/Submission9722/Reviewer_EcX6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9722/Reviewer_EcX6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724297235, "cdate": 1761724297235, "tmdate": 1762921219381, "mdate": 1762921219381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback on PaAno. Your comments have helped us clarify the motivation, refine the details and explanations, and strengthen the contributions of this work. We are currently revising the manuscript in response to each of the comments!\n\n**Major Changes to Be Included in the Updated Manuscript**\n\n- Thanks to **Reviewer 2Foy**, PaAno now adopts **Instance Normalization** to better handle regime shifts and non-stationary normal patterns, resulting in consistent performance improvements across TSB-AD-U/M.\n\n- Thanks to **all reviewers**, we have added **comprehensive sensitivity analyses** (e.g., λ, batch size, patch size..) and strengthened experimental discussions."}}, "id": "55BSGHOvqf", "forum": "NXThkM7Iym", "replyto": "NXThkM7Iym", "signatures": ["ICLR.cc/2026/Conference/Submission9722/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9722/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9722/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763395466007, "cdate": 1763395466007, "tmdate": 1763410588617, "mdate": 1763410588617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback on PaAno. Your comments have helped us clarify the motivation, refine the details and explanations, and strengthen the contributions of this work. \n\nWe have now uploaded the **updated manuscript**, and all revisions are highlighted in **blue**. \n\n**Major Changes Included in the Updated Manuscript**\n\n- Thanks to **Reviewer 2Foy**, PaAno now adopts **Instance Normalization** to better handle regime shifts and non-stationary normal patterns, resulting in consistent performance improvements across TSB-AD-U/M.\n\n- Thanks to **all reviewers**, we have added **comprehensive sensitivity analyses** about:\n\n  - Model architecture (w/ height/width variations)\n  - Triplet loss vs InfoNCE loss\n  - Pretext loss scenarios\n  - λ \n  - Patch size\n  - Minibatch size\n\n- Thanks to **Reviewer 6WeB and AfnK**, we have clarified the **novelty of PaAno** in the main paper.\n\n- Thanks to **Reviewer 2Foy and EcX6**, we have strengthened the **overall experimental discussions** and included **pseudo code of Anomaly Detection**.\n\n-  Also, we removed the unnecessary computation steps of the pretext task after it decays to zero, making **PaAno much faster!** (TSB-AD-U: **6.8s**, TSB-AD-M: **12.6s**). \n\n**Major Changes We Are Preparing to Include in the Updated Manuscript**\n\n- Expanding the experiments to include two additional baseline methods"}}, "id": "55BSGHOvqf", "forum": "NXThkM7Iym", "replyto": "NXThkM7Iym", "signatures": ["ICLR.cc/2026/Conference/Submission9722/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9722/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9722/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763395466007, "cdate": 1763395466007, "tmdate": 1763713932758, "mdate": 1763713932758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback on PaAno. Your comments have helped us clarify the motivation, refine the details and explanations, and strengthen the contributions of this work. \n\nWe have now uploaded the **updated manuscript**, and all revisions are highlighted in **blue**. \n\n**Major Changes Included in the Updated Manuscript**\n\n- Thanks to **Reviewer 2Foy**, PaAno now adopts **Instance Normalization** to better handle regime shifts and non-stationary normal patterns, resulting in consistent performance improvements across TSB-AD-U/M.\n\n- Thanks to **all reviewers**, we have added **comprehensive sensitivity analyses** about:\n\n  - Model architecture (w/ height/width variations)\n  - Triplet loss vs InfoNCE loss\n  - Pretext loss scenarios\n  - λ \n  - Patch size\n  - Minibatch size\n\n- Thanks to **Reviewer 6WeB and AfnK**, we have clarified the **novelty of PaAno** in the main paper.\n\n- Thanks to **Reviewer 2Foy and EcX6**, we have strengthened the **overall experimental discussions** and included **pseudo code of Anomaly Detection**.\n\n-  Also, we removed the unnecessary computation steps of the pretext task after it decays to zero, making **PaAno much faster!** (TSB-AD-U: **6.8s**, TSB-AD-M: **12.6s**). \n\n**Major Changes We Are Preparing to Include in the Updated Manuscript**\n\n- Expanding the experiments to include two additional baseline methods (those published in 2025)"}}, "id": "55BSGHOvqf", "forum": "NXThkM7Iym", "replyto": "NXThkM7Iym", "signatures": ["ICLR.cc/2026/Conference/Submission9722/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9722/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9722/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763395466007, "cdate": 1763395466007, "tmdate": 1763733460374, "mdate": 1763733460374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents PaAno (Patch-based Anomaly Detection), a lightweight and effective framework for time-series anomaly detection. The core idea is based on patch-based representation learning, where the time series is divided into overlapping temporal patches that are embedded using a convolutional encoder. After training, embeddings of normal patches are stored, and during inference, anomaly scores are computed as the average distance between new patch embeddings and their nearest neighbors in the memory bank"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents time-series anomaly detection (TSAD) method by adopting the TSB-AD benchmark and employing lag-tolerant, threshold-independent metrics (with VUS-PR as the primary measure). It introduces a lightweight patch-encoder combined with metric learning and a memory-bank kNN anomaly scoring approach, applicable to both univariate and multivariate data. The emphasis is on robustness and efficiency rather than architectural complexity.\n\n* Paper combines simple components (1D-CNN, triplet loss, adjacency pretext, memory bank + kNN) yet yields strong empirical gains across multiple range- and point-wise metrics and both U/M benchmarks. The result that such a compact model outperforms many large transformer/foundation approaches under the TSB-AD protocol is interesting and practically relevant."}, "weaknesses": {"value": "* Novelty is incremental. The components are known, the value lies in the clean, effective combination and rigorous evaluation. \n\n* There is little theoretical analysis explaining why triplet + adjacency pretext together produce the observed gains, or conditions when patch locality will fail (e.g., very long-range contextual anomalies). A short analysis or toy case study would strengthen the paper\n\n* All main results use TSB-AD (albeit a rigorous benchmark). It remains unclear how PaAno performs when anomalies are primarily global (long temporal context) or when the training set has non-stationary normals. Clarification on this can further strengthen the paper. \n\n* The ablations show triplet vs. pretext importance and sensitivity to k/memory size,. It would be great to further explore: (a) patch length w effects across dataset types, (b) encoder depth/width tradeoffs, (c) alternative metric losses (contrastive. etc ). These would clarify design choices."}, "questions": {"value": "* Sensitivity beyond reported ranges for patch size, lag tolerance, and neighbor count. Are there regimes where PaAno fails (e.g., nonlocal anomalies or regime shifts)? \n\n* Any leakage from pretext construction across windows, and how is it prevented at train/test boundaries? \n\n* Why was triplet loss chosen over other metric objectives (e.g. Contrastive)? Any stability/convergence comments? \n\n* What are the hardware and exact runtime measurement details (GPU/CPU model, batch sizes during inference, parallelization)? Add them to the appendix or a short paragraph. \n\n* How does PaAno behave under non-stationary normal regimes (drift)? Any mechanisms (or easy extensions) for updating the memory bank online?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U5BfqPN8PF", "forum": "NXThkM7Iym", "replyto": "NXThkM7Iym", "signatures": ["ICLR.cc/2026/Conference/Submission9722/Reviewer_2Foy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9722/Reviewer_2Foy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835151542, "cdate": 1761835151542, "tmdate": 1762921218993, "mdate": 1762921218993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a patch-based temporal anomaly detection method that integrates memory mechanisms and representation learning to accomplish the task of temporal anomaly detection. The introduction and motivation are relatively clear and well-defined."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The motivation and ideas of the article are very clear, and the introduction is relatively clear; \n\n2. The article uses more and broader evaluation metrics, rather than flawed point adjustment metrics; \n\n3. Time series anomaly detection is a field worth exploring and has certain practical application value; \n\n4. The article's figures are well-made, clearly presenting the content intended to be conveyed."}, "weaknesses": {"value": "1. The novelty of the paper is limited. There has been much discussion about the Patch mechanism and Patch sequentiality in temporal tasks and temporal anomaly detection tasks, and the introduction of a memory mechanism in TSAD is not a novel contribution. The statements at the beginning of the paper are insufficient to demonstrate the novelty of the proposed framework. \n\n2. The comparative baselines in the paper are weak. Targeting ICLR 2026, it seems that many recent SOTA methods are missing from the comparisons, especially those from 2025, which are few. It is recommended that the authors include more strong baselines to substantiate the effectiveness of their method. \n\n3. The claimed contribution of the method in terms of lightweight design is questionable, as the results on TSB-AD show that the proposed method is slower in terms of time consumption compared to many existing methods, making it difficult to demonstrate an advantage in training and inference."}, "questions": {"value": "1. The author needs to clearly and repeatedly clarify their contributions, especially regarding the motivation for integrating previous technologies, and whether there is truly an element of lightness, which requires stronger theoretical and experimental evidence; the presentation also needs to be adjusted. \n\n2. It is recommended that the author supplement with more comparative work from the past 25 years, as the current comparison algorithms are insufficient."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lntl4i3xVG", "forum": "NXThkM7Iym", "replyto": "NXThkM7Iym", "signatures": ["ICLR.cc/2026/Conference/Submission9722/Reviewer_AfnK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9722/Reviewer_AfnK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968597731, "cdate": 1761968597731, "tmdate": 1762921218733, "mdate": 1762921218733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PaAno, a lightweight and effective method for semi-supervised (normal-only) time-series anomaly detection. The authors argue that recent large-scale models (e.g., Transformers) provide diminishing returns for high computational costs, often failing to outperform simpler methods under rigorous evaluation protocols. PaAno challenges this trend by proposing a compact 1D-CNN architecture based on patch-based representation learning.\n\nThe model is trained on normal data by extracting short temporal patches. A patch encoder (1D-CNN) is optimized using a dual loss function:\n\nTriplet Loss: A metric learning objective that clusters temporally similar patches in the embedding space.\n\nPretext Loss: A self-supervised classification task that predicts whether two patches are temporally consecutive.\n\nAfter training, embeddings of normal patches are stored in a compressed memory bank. During inference, the anomaly score of a new patch is computed as its distance to the nearest neighbors in this memory bank, effectively measuring its dissimilarity from all learned normal patterns. Experiments on the rigorous TSB-AD benchmark show PaAno achieves state-of-the-art (SOTA) performance, ranking first across all six evaluation metrics for both univariate and multivariate data, significantly outperforming heavier Transformer-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is architecturally simple, utilizing a lightweight 1D-CNN (0.3M parameters)  and achieving a fast runtime (Tables 2 & 3). This makes it a practical solution for resource-constrained environments, which is a commendable engineering goal.\n2. The authors have conducted a comprehensive evaluation on the TSB-AD benchmark, adhering to its rigorous protocols. The inclusion of a proper ablation study (Table 4) and hyperparameter sensitivity analysis (Fig. 4) meets the requirements of a solid experimental paper."}, "weaknesses": {"value": "Significant Lack of Novelty: This is the primary flaw of the paper. The proposed method is highly incremental and appears to be a straightforward combination of well-established, existing techniques.\n\n- The core idea of \"patch-based representation learning\" for anomaly detection is directly borrowed from the computer vision domain.\n\n- The use of 1D-CNNs for time-series feature extraction is standard.\n\n- The use of Triplet Loss and self-supervised pretext tasks  are both common, off-the-shelf methods for representation learning.\n\n- The paper fails to demonstrate a novel conceptual contribution. It reads more like an application of a known (visual) anomaly detection recipe to the time-series domain, rather than a new method developed from first principles.\n\nBesides, the paper's main justification is that \"local patterns matter\" and that large Transformer models are \"inefficient\". These are not new insights. The paper fails to provide a deep analysis of why this specific combination of old techniques so dramatically outperforms other methods (including simpler ones like (Sub)-PCA ) on this benchmark. The impressive empirical result lacks a correspondingly strong conceptual or theoretical justification."}, "questions": {"value": "The paper's justification is that \"local patterns matter,\"  but this is not a new insight. Simple baselines like (Sub)-PCA also operate on local windows yet perform significantly worse (Table 2). Conversely, large Transformers are fully capable of learning local patterns but also fail. This implies the success is not just about \"being local.\" What specific properties does the 1D-CNN encoder learn from the patches—as a result of this specific dual-loss training—that (Sub)-PCA fails to capture and that Transformer-based models apparently miss?\n\nThe paper combines Triplet Loss and a Pretext Loss, both of which are well-established techniques . The ablation study (Table 4) shows both contribute. However, what is the specific synergistic effect between these two? For instance, does the pretext task (predicting temporal consecutiveness) primarily help structure the embedding space for the triplet loss to find more meaningful negatives? Or are their contributions merely additive? How critical is this specific pretext task, or would any generic self-supervised task have achieved a similar outcome?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uR28lcxskh", "forum": "NXThkM7Iym", "replyto": "NXThkM7Iym", "signatures": ["ICLR.cc/2026/Conference/Submission9722/Reviewer_6WeB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9722/Reviewer_6WeB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971793740, "cdate": 1761971793740, "tmdate": 1762921218369, "mdate": 1762921218369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}