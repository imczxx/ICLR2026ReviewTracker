{"id": "I3Ct1eDmVI", "number": 4666, "cdate": 1757740142865, "mdate": 1759898020984, "content": {"title": "PHyCLIP: $\\ell_1$-Product of Hyperbolic Factors Unifies Hierarchy and Compositionality in Vision-Language Representation Learning", "abstract": "Vision-language models have achieved remarkable success in multi-modal representation learning from large-scale pairs of visual scenes and linguistic descriptions. However, they still struggle to simultaneously express two distinct types of semantic structures: the hierarchy within a concept family (e.g., *dog* $\\preceq$ *mammal* $\\preceq$ *animal*) and the compositionality across different concept families (e.g., \"a dog in a car\" $\\preceq$ *dog*, *car*). Recent works have addressed this challenge by employing hyperbolic space, which efficiently captures tree-like hierarchy, yet its suitability for representing compositionality remains unclear. To resolve this dilemma, we propose *PHyCLIP*, which employs an $\\ell_1$-*P*roduct metric on a Cartesian product of *Hy*perbolic factors. With our design, intra-family hierarchies emerge within individual hyperbolic factors, and cross-family composition is captured by the $\\ell_1$-product metric, analogous to a Boolean algebra. Experiments on zero-shot classification, retrieval, hierarchical classification, and compositional understanding tasks demonstrate that PHyCLIP outperforms existing single-space approaches and offers more interpretable structures in the embedding space.", "tldr": "For multi-modal representation learning, we propose to use an l1-product metric space of hyperbolic spaces, which simultaneously captures hierarchy and compositionality.", "keywords": ["Vision-language representation learning", "compositionality", "Boolean algebra", "hyperbolic embedding"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11b76376325bd7779bb38ff25420a9037c45a689.pdf", "supplementary_material": "/attachment/4c601374a09e077a3703a7253c7d0c174d64c2ca.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new model named PHyCLIP, which extends the HyCoCLIP method to a Cartesian product of hyperbolic spaces to allow for an understanding of both concept hierarchies and concept compositionality. First, the authors provide a theoretical motivation, explaining how this Cartesian product of hyperbolic spaces is capable of embedding these two types of information. This background is followed by an exposition of their PHyCLIP model. With a comprehensive suite of experiments the authors then show that their proposed model leads to significant improvements with respect to baselines such as CLIP, MERU and HyCoCLIP. Moreover, in an interesting analysis they show that the individual hyperbolic factors indeed appear to learn different concept hierarchies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper proposes a solution to an important problem of hierarchical vision-language models such as MERU and HyCoCLIP, which struggle to model both compositionality and hierarchical understanding. The proposed use of Cartesian products of hyperbolic spaces seems elegant and effective, providing an important step in the right direction.\n- The theoretical background is clearly presented, well-motivated and forms a nice basis for the rest of the paper. \n- The method and its presentation are simple, yet effective.\n- The results in the extensive suite of experiments are quite convincing in my opinion.\n- To me, the analysis in Subsection 4.3 felt particularly compelling."}, "weaknesses": {"value": "It feels to me that there is a slight disconnect between the theory and the implementation, due to a few minor points:\n1. The background (Theorem 2) assumes the existence of some product of trees that is to be embedded into a product of hyperbolic spaces. Here, if my understanding is correct, each tree is considered the replacement of some bit that would normally represent the presence of some concept. However, it is not completely clear from the text how this tree represents the presence or absence of a concept within the composition. Is the distance of the node to the root of the tree some form of the \"degree of presence\" of this concept? Would absence then be indicated by picking the root node? If so, that might be useful to discuss more explicitly in lines 174-181. \n2. After discussing the embedding of this product of trees, the method section dives straight into a presentation of the method. However, it seems to me that we do not actually have any known hierarchies and that we assume the model to learn these implicitly (as discussed shortly at the end of section 4). A short discussion on this point somewhere in or directly after lines 188-195 might be beneficial for the structure of Section 3 as I was not quite sure about the connection of Theorem 2 to the actual proposed method upon reading this section.\n3. The proposed loss seems to assign the same type of loss to each hyperbolic factor. However, it seems to me that this would sometimes hamper the learning of hierarchies in specific factors. For example, given an image of a dog next to a car, how should the image box containing just the car be represented in the hyperbolic factor that contains the mammal hierarchy? I would assume that this should be put close to the origin of this factor, but the loss would punish the model for such a representation. This seems like a difficult problem to address through any modifications of the loss and the analysis in 4.3 already shows that the model acts as expected despite this potential problem, but I think a discussion on this limitation should still be included somewhere. \n\nI do want to add that the theory, method and experiments individually are all convincing and that, overall, I consider these weaknesses minor compared to the strengths of the paper. \n\nLastly, a small remark: in lines 88-89 the authors mention (Sarkar, 2011; Nickel & Kiela, 2017; Ganea et al., 2018a), but given the requirement for low distortion, I think a more appropriate list would be (Sarkar, 2011; Sala et al., 2018; [1]), as the other two were not designed for low distortion and generally do not achieve it.\n\n[1] Max van Spengler, Pascal Mettes. Low-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space. In _International Conference on Machine Learning (ICML)_, 2025."}, "questions": {"value": "Aside from the questions posed in the weaknesses, I have some more small questions:\n\n1. In lines 188-195 it is mentioned that \"we use standard hyperbolic embeddings (Nickel & Kiela, 2017) together with hyperbolic entailment cones (Ganea et al., 2018a). It is not completely clear to me what is meant here. Would this be the proposed approach for embedding actual trees or does this reference to the PHyCLIP method's loss presented after?\n2. Theorems 1 and 2 both use $\\mathbb{H^2}$, but I think this should be $\\mathbb{H}^d$, right?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "njV2P50pAp", "forum": "I3Ct1eDmVI", "replyto": "I3Ct1eDmVI", "signatures": ["ICLR.cc/2026/Conference/Submission4666/Reviewer_QmqT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4666/Reviewer_QmqT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760973170593, "cdate": 1760973170593, "tmdate": 1762917502377, "mdate": 1762917502377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PHyCLIP  to tackle a central limitation of contrastive vision–language models: representing both hierarchical “is-a” structure within a concept family and cross-family compositionality in a single embedding space. The proposed model PHyCLIP, which embeds images and texts into a Cartesian product of hyperbolic factors and endows that product with an ℓ₁-product metric. The design strives for a clean division of labor: intra-family hierarchies emerge inside individual hyperbolic factors, while cross-family composition is captured by the ℓ₁ aggregation across factors, in analogy with Boolean conjunction. Empirically, PHyCLIP is trained (from scratch) on the GRIT corpus with region-level supervision and evaluated on zero-shot classification, image–text retrieval, hierarchical classification, and compositional understanding; the model is reported to outperform single-space Euclidean or single-space hyperbolic baselines and to yield interpretable factor activations aligned with concept families."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "To the best of my knowledge, the idea of representing vision–language concepts in a Cartesian product of hyperbolic factors with an ℓ₁-product metric is a clean and original way to reconcile hierarchical “is-a” structure (within factors) with cross-family compositionality (across factors) in the context of vision-language models. I think the theory–architecture fit of this paper is very strong: hyperbolic factors give you room for tree-like taxonomies, while ℓ₁ aggregation behaves like a sparse, Boolean-style conjunction that naturally penalizes missing parts in a composition. \n\nI also like that the learning objective stays simple—contrastive InfoNCE on the product metric (This is consistent with training recipes of common contrastive-based vision language models such as the baselines) plus a hyperbolic entailment-cone regularizers, which, in my view, makes comparisons fair. \n\nEmpirically, I also find the evaluation breadth convincing: zero-shot classification and retrieval improve over Euclidean and single-space hyperbolic baselines, hierarchical metrics on ImageNet+WordNet move in the right direction, and compositional checks (e.g., VL-Checklist, SugarCrepe) benefit in the settings the method is designed for.  \n\nOverall I think the contribution is solid and this work can benefit the overall community"}, "weaknesses": {"value": "I see the main gaps as scope and analysis, not methodology. \n\nFirst, the paper focuses on object/attribute composition; explicit relational composition (multi-object relations and their algebra) remains largely out of scope, yet those cases are where many Vision–Language systems struggle. It would be interesting to see if there are further experiments on thoes flavors of compositionality\n\nSecond, the text-retrieval side is “competitive” rather than consistently superior; If possible a targeted error analysis to separate caption ambiguity from potential factor misalignment would be very benefitial to the ML community. \n\nThird, the method’s behavior as the number of factors k grows deserves a bit more operational guidance: the results suggest a sweet spot, but it is not entirely clear how to pick k for new domains with different taxonomic granularity. \n\nFinally, because training relies on large-scale auto-annotated data, I would appreciate a brief characterization of how annotation noise (e.g., imperfect region phrases) interacts with the entailment-cone loss and with factor specialization—more as insight than as a requirement."}, "questions": {"value": "On the data side I am curious about if the authors observe any systematic sensitivity to noisy region phrases or ambiguous attributes, and could factor-wise uncertainty or gating mitigate this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Atgx0bwowb", "forum": "I3Ct1eDmVI", "replyto": "I3Ct1eDmVI", "signatures": ["ICLR.cc/2026/Conference/Submission4666/Reviewer_iHKL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4666/Reviewer_iHKL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957977571, "cdate": 1761957977571, "tmdate": 1762917501916, "mdate": 1762917501916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PHyCLIP, which employs an ℓ1-product metric on a Cartesian product of hyperbolic factors to jointly capture hierarchical structures within concept families and compositionality across them. While the theoretical motivation is interesting and experiments are comprehensive, critical design choices lack sufficient justification and the improvements are marginal."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear design hypothesis. The paper explicitly assigns intra-family hierarchy to hyperbolic factors and inter-family composition to ℓ1 aggregation. The overview (Fig. 2, p.2) and factor-wise norm plots/activations (Figs. 4–6) align qualitatively with this hypothesis.\n2. The method is tested on classification, retrieval, hierarchical metrics, and composition benchmarks. PHyCLIP is generally competitive and often best, with notable gains on VL-CheckList–Object and small but consistent improvements on several other metrics (Tables 1–3).\n3. Interpretability hints. Factor-wise visualizations (pp.8, 21–22) show families (e.g., mammals vs. vehicles) specializing to different factors and conjunctions activating multiple factors—matching the intended Boolean-like behavior.\n4. Broad evaluation. The method is tested on classification, retrieval, hierarchical metrics, and composition benchmarks. PHyCLIP is generally competitive and often best, with notable gains on VL-CheckList–Object and small but consistent improvements on several other metrics (Tables 1–3)."}, "weaknesses": {"value": "1. Unconvincing ℓ1 justification \n- Prop. 1 shows Boolean lattices embed in ℓ1, but the model uses continuous hyperbolic factors, not discrete bits\n- Missing: Why not weighted combinations or learned aggregation functions?\n2. No statistical validation\n- Single runs without confidence intervals\n- Improvements often 1-2%, likely within noise\n3.  Implementation opacity\n- Curvature initialization/learning dynamics unexplained\n- \"ℓ2-product metric\" undefined mathematically\n4. Marginal gains don't justify complexity\n- Average improvement ~1% over HyCoCLIP\n- 64× more distance computations\n- 64× more curvature parameters\n- Memory/compute overhead unreported"}, "questions": {"value": "1. Do ℓ1’s gains persist under a full ℓp sweep with ≥3 seeds?\n2. How do factorized Euclidean/order/box models with ℓ1 compare at equal total dimension?\n3. Is the ℓ2 baseline a true Riemannian product distance implementation? Please provide the formula/code.\n4. What is the quality-vs-cost curve as k increases (1, 8, 64, 128)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5SgVOBMGhD", "forum": "I3Ct1eDmVI", "replyto": "I3Ct1eDmVI", "signatures": ["ICLR.cc/2026/Conference/Submission4666/Reviewer_LzC9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4666/Reviewer_LzC9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008525166, "cdate": 1762008525166, "tmdate": 1762917501505, "mdate": 1762917501505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Hyperbolic embeddings capture hierarchy and partial orders via inclusion. These approaches have been used to train vision-language models such as MERU and HyCoCLIP, which capture concept hierarchies. On the other hand, these hyperbolic embeddings lack a canonical composition operation. To solve this problem, the paper introduces PHyCLIP, which uses an $l_{1}$ product metric to unify hierarchy and composition of concepts. Results on zero-shot classification, image-text retrieval, and compositional benchmarks show that PHyCLIP outperforms methods using hyperbolic representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written. The related work section is comprehensive and up to date.\n\nThe method is simple to implement. The work builds on HyCoCLIP and shows that using an $l_{1}$ product to unify the hierarchy and composition of concepts improves results across all benchmark tasks."}, "weaknesses": {"value": "**Missing Baseline.**\nThe paper does not compare PHyCLIP to CLIP fine-tuned on the GRIT dataset. Furthermore, fine-tuning CLIP could, in fact, improve performance on the compositionality benchmark tasks. In SugarCrepe [a], Table 5 shows much higher performance for fine-tuned CLIP. This is a strong reason to include fine-tuned CLIP as a baseline for fair comparison.\n\n**Compositionality.**\nTable 3 shows improvements for PHyCLIP, but not by a significant margin over CLIP (approximately 1.4 points on average), and only marginal improvements over CLIP on SugarCrepe. This is not a strong result for a method that aims to improve compositionality in vision-language models.\n\n**Sensitivity to k.**\nIn Table 4, changing the number of factors (k) can significantly reduce performance. However, there isn’t a clear guideline for choosing this hyperparameter.\n\n\n**References.**\n\n[a] SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality. NeurIPS 23."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NZ9Ucf6hTX", "forum": "I3Ct1eDmVI", "replyto": "I3Ct1eDmVI", "signatures": ["ICLR.cc/2026/Conference/Submission4666/Reviewer_Rpq7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4666/Reviewer_Rpq7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140255077, "cdate": 1762140255077, "tmdate": 1762917500825, "mdate": 1762917500825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}