{"id": "3ow7tq0O3l", "number": 2643, "cdate": 1757178741860, "mdate": 1759898135916, "content": {"title": "Dr.LLM: Dynamic Layer Routing in LLMs", "abstract": "Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains. We introduce Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under a compute budget. Our design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budget-aware, accuracy-driven inference without altering base weights.", "tldr": "Dynamic Layer Routing in LLMs", "keywords": ["Large Language Models", "Efficient Inference", "Adaptive Computation", "Test-time Optimization", "Monte Carlo Tree Search", "Dynamic Layer Routing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2557e0a448d4103629d7c34b157c3d1c3e3d1430.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Dr.LLM, a dynamic layer routing method for large language models (LLMs) that operates at the sequence level. The motivation behind this approach arises from the limitations of traditional static-depth LLMs, which apply a fixed number of layers regardless of input complexity. Such models tend to waste computation on simple prompts and lack the flexibility needed to handle more challenging reasoning tasks. Dr.LLM addresses this by introducing a lightweight routing module at each transformer layer. Based on the sequence of embeddings from the previous layer, the router dynamically decides whether to skip the current layer, execute it once, or execute it twice. These routing decisions are supervised using a Monte Carlo Tree Search (MCTS)-based algorithm. The paper presents experimental results on both in-domain tasks, where the input distribution matches the data used to train the router, and out-of-domain tasks, which involve distribution shifts. Dr.LLM is also evaluated against prior routing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is clearly written, and the authors present their method in a precise and comprehensible way.\n* The use of MCTS to generate routing paths is unique and answers the requirements of a routing method presented in the introduction section.\n* The method outperforms other routing strategies."}, "weaknesses": {"value": "* The computational cost associated with generating the MCTS-based supervision is not sufficiently reported. While this supervision is computed offline, its upfront cost does not appear to be negligible. It would be valuable to include a comparison of this cost relative to the resources required to train the router itself. Additionally, since performance gains have been demonstrated primarily on in-domain data, expanding the router’s training data to cover a broader range of tasks seems essential for making Dr.LLM applicable to general-purpose LLM scenarios and therefore understanding the computational effort required to generate such supervision is important.\n* KV cache compatibility — The authors state at the beginning of Section 3.1 that the method is KV cache compatible, but it's unclear how this is achieved. During generation, the router can select a different path at each forward pass, potentially resulting in different inputs to each transformer block. This variability seems to conflict with standard KV cache usage, which assumes consistent execution paths. To clarify, consider the following example: suppose we are generating a sequence and have reached the point of generating the 100th token. At this step, we pass $H^{\\ell-1}$ through the router and obtain $y_{\\ell}=\\text{skip}$. However, in the next forward pass (i.e., when generating the 101st token), the router outputs $y_{\\ell} = \\text{exec}$. In this case, how is the KV cache for layer $\\ell$ defined to correctly support both scenarios? The mechanism for maintaining cache consistency under dynamic routing decisions needs further clarification.\n* The efficiency analysis in the experimental section appears incomplete. Is the overhead introduced by the routing modules truly negligible compared to the reported efficiency gains (0.9–2.5%) in Table 3?"}, "questions": {"value": "* A more precise definition of the per-example number-of-layers-used metric would be helpful. I assume this refers to summing the number of active layers across each forward pass during generation, but providing a clear and explicit definition would improve clarity.\n* “We allow skips of at most two consecutive layers…” What motivates the need for this specific limitation?\n* What does the \"Original\" column in Table 2 represent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FDuszW8a4H", "forum": "3ow7tq0O3l", "replyto": "3ow7tq0O3l", "signatures": ["ICLR.cc/2026/Conference/Submission2643/Reviewer_kHKr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2643/Reviewer_kHKr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760613793807, "cdate": 1760613793807, "tmdate": 1762916316194, "mdate": 1762916316194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dr.LLM, a new method to make Large Language Models (LLMs) more efficient and sometimes more accurate. It works by adding small \"routers\" to an existing LLM that dynamically decide whether to use, skip, or repeat each layer for a given problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The framework is retrofittable, meaning it can be applied to existing, pre-trained models without costly full-scale retraining. Keeping the base LLM weights frozen makes the approach highly practical and accessible.\n\n2. Using MCTS to generate an \"oracle\" dataset of optimal execution paths is a very clever way to supervise the routers. This avoids complex reinforcement learning and eliminates the need for a costly search process during actual inference.\n\n3. The claim that the trained routers generalize well to out-of-domain tasks suggests they are learning a robust, transferable policy for allocating computational resources rather than just overfitting to the training data."}, "weaknesses": {"value": "1. The paper emphasizes that router training is lightweight, but the initial offline MCTS process to find the optimal paths could be extremely computationally expensive. The true cost of preparing the training data is not fully addressed.\n\n2. The search space for MCTS (all possible combinations of skipping/repeating layers) grows exponentially with model depth. It's unclear how well this search process scales to extremely deep models (e.g., 100+ layers).\n\n3. The actions (skip, execute, repeat) are discrete and simple. This might not be nuanced enough for problems where only a part of a layer (e.g., a few attention heads) is needed, potentially leaving further efficiency gains on the table."}, "questions": {"value": "Same as above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PjPg2uFqlW", "forum": "3ow7tq0O3l", "replyto": "3ow7tq0O3l", "signatures": ["ICLR.cc/2026/Conference/Submission2643/Reviewer_nSUU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2643/Reviewer_nSUU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968342575, "cdate": 1761968342575, "tmdate": 1762916315993, "mdate": 1762916315993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dr.LLM, a retrofittable framework that introduces per-layer dynamic routers for frozen pretrained LLMs. Each router decides whether to skip, execute, or repeat a transformer block, allowing adaptive compute allocation per input. The routers are trained with explicit supervision derived from offline Monte Carlo Tree Search (MCTS), which identifies optimal layer configurations under compute constraints. The approach achieves accuracy improvements on reasoning-heavy tasks (ARC, DART) while reducing the number of executed layers and generalizes well to out-of-domain benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel supervised routing framework that avoids inference-time search and large-scale retraining, yet improves both efficiency and accuracy.\n2. Strong empirical results showing consistent gains on logic and math benchmarks, plus solid cross-domain generalization with minimal accuracy drop."}, "weaknesses": {"value": "1. Lack of wall-clock evaluation – The reported “layers saved per query” metric does not directly reflect real-world speedup. Since each layer involves router computation overhead, the paper should include wall-time results to substantiate efficiency claims.\n2. Limited training exploration – The routers are trained separately from the LLM. Joint training (LLM + routers) could yield better synergy and higher final performance, especially since inference is a long-term process while training is a one-time cost.\n3. Unclear batch inference feasibility – It’s uncertain whether Dr.LLM supports efficient batch processing, as per-token dynamic routing can lead to divergent compute paths and reduced parallelism. Discussion or experiments on batch inference scalability would strengthen the work."}, "questions": {"value": "If all concerns shown in weaknesses are resolved, I would raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5TeBs4VaZ8", "forum": "3ow7tq0O3l", "replyto": "3ow7tq0O3l", "signatures": ["ICLR.cc/2026/Conference/Submission2643/Reviewer_wfaJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2643/Reviewer_wfaJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969743465, "cdate": 1761969743465, "tmdate": 1762916315861, "mdate": 1762916315861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for dynamically skipping, executing or repeating layers in transformer architecture per example in order to improve the model quality while preventing wasted compute on simpler queries. It can be applied to any LLM that is already trained without altering the weights, only adding per-layer routers. The routers are trained using an offline tree-based search method and use hidden layers of the previous layers as inputs. The search algorithm is monte carlo based and includes an explicit length penalty to incentivize shorter paths to favor executing less number of layers. The method is evaluated on Llama and Qwen models and has shown to improve accuracy around 3% while saving 5 layers per example on average."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is applicable to already trained LLMs with only minimal router training\n- It is a comprehensive method which combines layer skipping and looping which was previously explored separately in most former work\n- Research artifacts including the code and data is shared publicly"}, "weaknesses": {"value": "- There is no latency measurements shown in the paper.  While the method reduces 5 layers per example on average, the routers add additional overhead per layer and overall latency might increase because of that.\n- The method is not applicable to batching as the routing is done per example level, when the batch size is greater than 1, accuracy might suffer. \n- Out of domain generalization looks weak. While the method improves accuracy for in-domain tasks (the tasks the routers are trained for), for out of domain tasks accuracy drops.\n- I have several questions about the evaluation, listed in questions section below."}, "questions": {"value": "- Figure 1 shows the network has 600+ layers. -> why there are so many layers? No state of the art LLM has that many layers.\n\n- The router accuracy during training is 61%, which seems low. What’s the implication of this, have you tried improving the router accuracy?\n\n- “all four models gain 0.40%p accuracy on GSM8k” -> This claim on page 7 does not match the results on Table 4. Llama 3B accuracy drops on GSM8k according to table 4. \n\n- Why are there no baseline llama numbers in Table 5? It’d be good to add the baseline in this table as some of the tasks in this table were not shown before in the paper. The table shows Dr.LLM is better than the state of the art methods however it does not show how it’s better than the pure LLM in HumanEval, Hellaswag, MMLU tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HU0jdYM1HJ", "forum": "3ow7tq0O3l", "replyto": "3ow7tq0O3l", "signatures": ["ICLR.cc/2026/Conference/Submission2643/Reviewer_p46J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2643/Reviewer_p46J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117427827, "cdate": 1762117427827, "tmdate": 1762916315724, "mdate": 1762916315724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}