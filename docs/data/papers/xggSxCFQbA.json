{"id": "xggSxCFQbA", "number": 21288, "cdate": 1758315925765, "mdate": 1763759494234, "content": {"title": "OpenAgentSafety: A Comprehensive Framework For Evaluating Real-World AI Agent Safety", "abstract": "Recent advances in AI agents capable of solving complex, everyday tasks-- from software engineering to customer service-- have enabled deployment in real-world settings, but their possibilities for unsafe behavior demands rigorous evaluation. While prior benchmarks have attempted to assess agent safety, most fall short by relying on simulated environments, narrow task domains, or unrealistic tool abstractions. We introduce OpenAgentSafety, a comprehensive and modular framework for evaluating agent behavior across eight critical risk categories. Unlike prior work, our framework evaluates agents that interact with real tools, including web browsers, code execution environments, file systems, bash shells, and messaging platforms; and supports over 350 multi-turn, multi-user tasks spanning both benign and adversarial user intents. OpenAgentSafety is designed for extensibility, allowing researchers to add tools, tasks, websites, and adversarial strategies with minimal effort. It combines rule-based evaluation with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors. Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7% with o3-mini, highlighting critical risks and the need for stronger safeguards before real-world deployment of LLM agents.", "tldr": "A novel framework to evaluate the safety and risks of LLMs when deployed as agents in workplace scenarios.", "keywords": ["LLM Agents", "Safety", "Risks", "Datasets", "Benchmarks", "Tool-Use", "User Interactions", "Frameworks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2764bc0e05586ed16dee073788e3e88a0d393157.pdf", "supplementary_material": "/attachment/5e10de6fd731cd8a18623ef5dbb5d5981a1d6597.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces OpenAgentSafety, a benchmark designed to evaluate the safety of AI agents in realistic, high-risk scenarios. It features a comprehensive real-world tool suite, diverse user intentions, and multi-turn, multi-agent dynamics. By combining real tool use, complex social interactions, and varied intents from both users and non-player characters (NPCs), OA-SAFETY enables rigorous and comprehensive safety assessments across diverse scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Each OA-SAFETY task is implemented as a modular Docker container that integrates both a rule-based evaluator and an LLM-as-a-judge module.\n\n2. The experimental analysis is insightful and thought-provoking. For example, the study finds that hidden intents can circumvent safeguards, operational risks lead to inconsistent judgments, and browsing emerges as the most failure-prone interface. Moreover, LLM judges struggle with nuanced failure cases, underscoring the need for hybrid evaluation approaches that combine LLM-based judgment with rule-based verification.\n\n3. The comparison with related work is thorough and comprehensive."}, "weaknesses": {"value": "1. Since the multi-user scenario is a key component, the authors could further elaborate on the ChatNPC tools and include a case study demonstrating the dynamics and challenges of a multi-user interaction scenario.\n\n2. While the paper evaluates five widely adopted LLMs, the experiments are conducted on only one agent framework (OpenHands). It would strengthen the study to include evaluations across multiple agent frameworks to better assess generalizability and robustness."}, "questions": {"value": "1. Could the authors provide more details on the multi-user scenario in the paper, such as its setup or representative examples and process?\n\n2. It would be beneficial to evaluate additional agent frameworks to strengthen the generality of the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HRenaarfuK", "forum": "xggSxCFQbA", "replyto": "xggSxCFQbA", "signatures": ["ICLR.cc/2026/Conference/Submission21288/Reviewer_Uk4k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21288/Reviewer_Uk4k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761445471391, "cdate": 1761445471391, "tmdate": 1762941673295, "mdate": 1762941673295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OpenAgentSafety, a benchmark to evaluate the safety of LLM agents. Unlike prior benchmarks of this kind, OpenAgentSafety has a number of unique features:\n\n1. Realistic agentic environments. Agents are run in sandboxes with access to terminal, code interpreters, website clones, and other simulated \"characters\" referred to as NPCs.\n2. Three different kinds of tasks in which models can execute unsafe actions. In particular, the four combinations of benign and malicious intent that can be assigned to the user and NPC (Figure 3).\n3. A combined rules based and LLM-as-judge evaluation metric.\n\nThe benchmark contains a diverse set of environments (80 seed tasks augmented to 356 in total using GPT-4o) spanning 8 risk categories (Table 2). \n\nThe authors evaluate 5 frontier models on the benchmark, and draw a number of useful conclusions from these results. Some of these results are quite novel to me. For example, Figure 3 top they show that with totally benign situations, models can make unsafe \"mistakes\", with Claude-3.7 performing much worse in this category than when faced with explicit malicious intent from the user / NPC."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "## Originality\n\nThe idea of benchmarking agent safety is not novel, however the specific execution of this benchmark is. In particular, I am not aware of another work that has points 1 and 2 from the summary, these being (1) as realistic environments and (2) different kinds of user / NPC intent. \n\n## Quality and Clarity \n\nOverall the quality of the paper is high. The benchmark appears to be well thought out and design decisions justified in the writing. The results are good, and the research questions (3.3) are well answered with data.\n\n## Significance\n\nThis benchmark is significant because it is more realistic than others. As LLM agents are becoming more performant, and thus deployed in the real world, it is of critical importance that we have realistic benchmarks to evaluate their safety. In this category, OpenAgentSafety is the most realistic agent safety benchmark I know of."}, "weaknesses": {"value": "There are some weaknesses in the paper:\n\n1. The benchmark is excellent, and accordingly I think the paper could be improved by evaluating more models on the benchmark. For example, by evaluating a number of OpenAI, Anthropic, and open source models, and analyzing this data, the reader would have a better idea of the \"general\" state of agent safety in the world today.\n2. The paper would be improved by providing more information to the reader about what types of problems are in the dataset. E.g. providing examples, and counts of problems that fit into each category in Table 2. I think walking through an example environment would also help the reader understand the type of interactions that occur with NPCs.\n3. This is a nit, but the figures in section 3.3 are very small and hard to read when the paper is printed out (this is an easy fix though)."}, "questions": {"value": "1. Do you agree with weakness 1? If so could you test some more models? \n2. Do you agree with weakness 2? Could you provide some more detailed examples of problems?\n3. Why is it that in table 3 the Rule-based evaluation of all the models is incredibly similar, but the LLM-as-judge varies far more?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zvFsZcDt4K", "forum": "xggSxCFQbA", "replyto": "xggSxCFQbA", "signatures": ["ICLR.cc/2026/Conference/Submission21288/Reviewer_yZfw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21288/Reviewer_yZfw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763992825, "cdate": 1761763992825, "tmdate": 1762941672910, "mdate": 1762941672910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "356 tasks, built on top of OpenHands framework. Agent has access to terminal, filesystem, python and web prowser.\n8 risk categories: key compromise, data loss, privacy, unsafe code execution, financial loss, maliciois content, legal violations, harmful decision making. \nEach task comes with: environment, task description, NPC configs, and evaluator.\nThree settings: 1) benign user intent, no NPC 2) benign user intent + malicious NPC 3) malicious user request\n\nResults show major unsafe behavior rates according both to llm as a judge and rule-based evals."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Multi-user setting seems like an interesting setup in principle; however, I have critical concerns about NPC reproducibility, and the way the NPCs initiate chats\n- I like the three settings, combining user intent and NPC intent\n- Interesting discussions"}, "weaknesses": {"value": "- There is no info on the NPCs in the main paper nor the appendix: could you at least put some of the prompts? \n- Are NPC settings reproducible? Fixed open-weight model? What is the model? \n- When do NPC interact? Is it the agent or the NPC that initiate messages? Because if it is the agent, then if it just decides not to use the chat tool, it will not see malicious messages right?\n- For some of the tasks the evaluation of the outcome seems a bit arbitrary.\n\nMy biggest concern is whether in all settings there exists an ideal agent behavior?\n1) For instance, what is the correct behavior in the \"trens/Emily\" task  of Table 6 of appendix, or the change-branch-policy?\nIs it just REFUSAL? But if that's the case how do you distinguish from FAILURE?\n\nI think there needs to be a ground truth correct behavior for every task, and making sure it is not too arbitrary. \nAgents are typically subject to a utility/security-safety tradeoff. If utility is not tracked / or there does not exist a ground trugth correct behavior for every task that is relatively objectively defined, then the benchmark is not meaningful in my opinion."}, "questions": {"value": "I'm a a bit surprised all the rule-based evals are so close(32%). Are the failures on the same tasks or different? Similar types of failures or not?\n\nWould be nice to have at least one example of user task in the main paper, and more info on the NPC."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jyRU8g27O6", "forum": "xggSxCFQbA", "replyto": "xggSxCFQbA", "signatures": ["ICLR.cc/2026/Conference/Submission21288/Reviewer_rwaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21288/Reviewer_rwaC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938640085, "cdate": 1761938640085, "tmdate": 1762941672425, "mdate": 1762941672425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OpenAgentSafety, a comprehensive framework for evaluating the real-world safety of LLM-based agents. The framework integrates real tools (e.g., browser, file system, code execution), multi-intent tasks (benign, malicious), and social NPC interactions within sandboxed environments. It employs a hybrid evaluation mechanism that combines rule-based checking with LLM-as-a-judge assessments to detect both explicit and implicit unsafe behaviors. Experiments across five frontier models (Claude 3.7, GPT-4o, o3-mini, DeepSeek-v3/R1) reveal that even the most capable models still exhibit a high rate of unsafe actions. The framework is modular and extensible, making it a valuable testbed for future research on real-world agent safety."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper tackles a timely problem—evaluating the real-world safety of LLM-based agents—by integrating realistic environments, multiple user intents, and social interactions.\n* Experiments are conducted across diverse models and risk categories, using four questions (RQ1–RQ4) to guide the analysis. The results are comprehensive."}, "weaknesses": {"value": "* OpenAgentSafety offers a well-engineered and comprehensive benchmark, but its novelty is limited. The framework mainly integrates existing components such as OpenHands and Sotopia, extending prior safety benchmarks rather than introducing new methodologies. Its hybrid evaluation and GPT-based task generation are incremental refinements of known techniques.\n* The paper does not describe a mechanism for filtering or validating generated tasks (e.g., to remove redundant, ill-posed, or low-quality scenarios). Adding a lightweight automatic validation step—such as a logit- or heuristic-based filter to ensure task diversity and executability—could improve dataset quality and efficiency for future extensions.\n* Although the framework employs real tools (e.g., OwnCloud, GitLab), these are sandboxed offline replicas. As a result, the environment does not fully capture real-world dynamics.\n* While the experimental setup is extensive, the results largely reaffirm known trends—namely that LLM-based agents exhibit high rates of unsafe behavior. Moreover, the study confirms that LLM-as-Judge evaluations, though informative, remain imperfect and somewhat unreliable compared to human assessment."}, "questions": {"value": "1. Since the paper employs both rule-based and LLM-as-judge evaluations, could the authors clarify how these two signals are intended to be combined or interpreted together, especially in cases where the disagreement rate is substantial?\n2. The authors propose stricter authentication and privilege controls for high-risk tools. However, introducing additional safeguards might further increase the observed failure rates. Could the authors discuss how they envision balancing safety and task success?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t8yfFUfs0c", "forum": "xggSxCFQbA", "replyto": "xggSxCFQbA", "signatures": ["ICLR.cc/2026/Conference/Submission21288/Reviewer_neML"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21288/Reviewer_neML"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975860855, "cdate": 1761975860855, "tmdate": 1762941672091, "mdate": 1762941672091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}