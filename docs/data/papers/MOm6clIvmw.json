{"id": "MOm6clIvmw", "number": 17225, "cdate": 1758273665740, "mdate": 1759897189896, "content": {"title": "Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations", "abstract": "Reconstructing visual images from fMRI data is a central but highly challenging problem in neuroscience. Despite recent progress, current methods fall short when data and computation are limitedâ€”--precisely the conditions under which this task is most critical. We introduce a novel architecture-agnostic training paradigm to improve fMRI-based visual reconstruction through a subject-agnostic common representation space. We show that it is possible to leverage subject-specific lightweight modules to develop a representation space where different subjects not only lie in a shared space but are also aligned semantically. Our results demonstrate that such a training pipeline achieves significant performance gains in low-data scenarios. We supplement this method with a novel algorithm to select the most representative subset of images for a new subject. Using both techniques together, one can fine-tune with at most 40\\% of the data while outperforming the baseline trained with the minimum standard dataset size. Our method generalizes across different training paradigms and architectures, producing state-of-the-art performance and demonstrating that a subject-agnostic aligned representation space is the next step towards efficient Brain-Computer Interfaces.", "tldr": "We introduce a training paradigm for brain signal to image reconstruction by developing an aligned and shared cross-subject representation space that is especially powerful in low-data settings", "keywords": ["Computational Neuroscience", "fMRI", "Neuroimaging", "Diffusion", "CLIP", "alignment", "neuroAI"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4248c3b0f17571df01a597bdfc0d33258cd50c35.pdf", "supplementary_material": "/attachment/c22548f3663648bf3aa0859cc9293eaff2132fc2.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a new fMRI decoding method to improve the cross-subject decoding performance. Specifically, the authors employs subject-specific adapters to project new subjects into a shared reference space, then performs end-to-end fine-tuning of the remaining decoding network.  A sample selection mechanism is also leveraged to select representative samples during decoding. Experiments verify the effectiveness of the proposed method over two baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is clearly written and well-organized.\n- This paper proposes an adapter alignment training strategy to align new subjects to the reference subject in the low-data regime. The experiments demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "- Several relevant works are missing, such as MindAligner [1], which is highly related as it also aims to align new subjects to a reference space. To strengthen the paper, the authors should include a discussion that explicitly contrasts their method with these approaches. Highlighting the differences would more effectively establish the novelty and unique contribution of this research.\n- While the proposed subject-specific adapters are a promising alignment solution, the subsequent end-to-end fine-tuning raises concerns regarding efficiency and practicality. If the adapter achieves effective alignment, fine-tuning may be redundant. In comparison, other methods (MindAligner [1], Ferrante et al. 2024 [2] & MindBridge) do not need additional fine-tuning. Furthermore, this fine-tuning step introduces substantial computational cost and could potentially perturb the carefully learned adapter alignment.\n- The authors only compare two baselines (MindEye and MindEye2) in the main experiment. Iâ€™d like to see more baselines to justify the effectiveness of the method.\n- The alignment technique in this paper requires the same stimuli for different subjects, which may not be a common requirement in practice.\n\n[1] MindAligner: Explicit Brain Functional Alignment for Cross-Subject Visual Decoding from Limited fMRI Data\n\n[2] Through their eyes: multi-subject brain decoding with simple alignment techniques."}, "questions": {"value": "- Could authors provide performance comparison with more baselines?\n\n- A key methodological concern is the modification of the data split, where common images were transferred from the test set to the training set. The comparison with previous methods is less soundable unless those baselines were also re-trained and re-evaluated on this identical, new data split. Please clarify it. \n\nPlease refer to the Weaknesses for other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SxhjRlamsl", "forum": "MOm6clIvmw", "replyto": "MOm6clIvmw", "signatures": ["ICLR.cc/2026/Conference/Submission17225/Reviewer_hpp1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17225/Reviewer_hpp1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807067638, "cdate": 1761807067638, "tmdate": 1762927185650, "mdate": 1762927185650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Adapter Alignment (AA), an architecture- and paradigm-agnostic method to align the fMRI representations of different subjects to a common reference-subject space. This is applied as an efficient fine-tuning strategy of fMRI-to-image models: a model trained on one subject with maximum data is fine-tuned using AA to produce models for new / unseen subjects that perform better than previous fine-tuning baselines (e.g. MindEye2), in limited data setting. Additionally, the paper provides a greedy-based method for selecting the best subset of images to use for fine-tuning, further reducing the data requirements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is very well written, easy to follow and understand. \n- The motivation for the method and the underlying 'common subject-space hypothesis' is explicitly given and well detailed.\n- The authors show that their alignment method still works even when a common set of images for all subjects is missing (i.e application to the THINGS-fMRI dataset)\n- The method of analysis is clear, sound and generalises across two different types of MindEye architectures.\n- The set of ablations presented is rather complete, with additional insightful visualisations. \n- The performance gain in limited data setting against the presented baselines is clear and unambiguous."}, "weaknesses": {"value": "- Although the experiments are run with different seedings and subjects, then averaged, error bars / confidence intervals are missing. Adding them would enhance the impact / strength of these results.\n- There are already several studies on fine-tuning a reference model on new subjects and the present paper mostly refers only to the common-space subject-module / adapter of MindEye1/2, hence this very specific improvement related to aligning subject spaces in MindEye architectures can be seen rather of iterative nature than a stronger contribution. It could be strengthened by extending the method to more datasets rather than NSD and THINGS (e.g. Bold5K, DeepRecon), architectures other than ME1/2, and more subject-embedding techniques (concat, add, ...).\n- Given the now well-know flaws of the NSD dataset (low categorical diversity, categorical leakage, ..) the use of NSD as the only reference dataset is a potentially weak point of the paper, but could be addressed by doing at least one other transfer ablation (e.g. between THINGS and Bold5K)."}, "questions": {"value": "- it is not entirely clear to me if the SVD analysis for Image selection is carried out on the 1000 fMRI representations  of the common image set (in reference subject-space), or if it is done on the CLIP embeddings of the corresponding images\n- It seems that only single-subject models are trained, but it is sometimes ambiguous whether the subjects 2,5,7 are fine-tuned simultaneously with a single model (with one adapter per subject, yielding a single model for all 3 subjects), or one by one, separately (yielding 3 different models)\n- Tables in the main paper gray out the pixel-wise metrics because it is said the pipeline does not optimise for these. This could be clarified so that it is clearer that the results are still valid for these metrics. It would also be interesting to report metrics for the low-level pipeline of ME1/2.\n- It is not clear to me what the strategy is, in the end-to-end fine-tuning stage, for re-using common images or not. Some parts of the text mention that the common-images are used only for AA (e.g. 4.3), but it is unclear if other experiments follow this same strategy or reuse the common images in the end-to-end stage.\n- There are other strategies than mapping fMRI to a common space (e.g. adding a subject-embedding at various levels in this pipeline), it would be interesting to have a discussion of these other types of strategies for embedding subject information\n- The transfer results on THINGS that shows you don't need a common set of identical images across subjects, but just semantically-matching collections of images across subjects is most interesting and I believe it could be emphasised more strongly as a core contribution of the paper (i.e the method is also agnostic to having a common set of images across subjects).\n- Unless I'm mistaken there is a typo at L275: it should be the 'total number of empty bins across all dimensions' for $Gap(S)$\n- It is not clear to me whether the IS algorithm keeps a constant number (250) of images and tries to minimise the number of empty bins under this constraint or varies the number of images to allow for fewer empty bins. In the second case, it would be informative to have sizes of the image subset returned by the IS algorithm."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UGBAZDmdNm", "forum": "MOm6clIvmw", "replyto": "MOm6clIvmw", "signatures": ["ICLR.cc/2026/Conference/Submission17225/Reviewer_HkQB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17225/Reviewer_HkQB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916696256, "cdate": 1761916696256, "tmdate": 1762927185253, "mdate": 1762927185253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an fMRI visual reconstruction method combining Adapter Alignment and a submodular greedy image selection algorithm. It aligns new subjectsâ€™ representation spaces with a pre-trained reference via lightweight adapters and selects representative training images for data efficiency. Validated on NSD and THINGS datasets, the authors claim that their method outperforms end-to-end baselines in low-data scenarios and is architecture-agnostic, compatible with different reconstruction architectures and protocols."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper attempts to address the low data efficiency issue in fMRI visual reconstruction, which is a practically significant problem in the field. The combination of adapter alignment and submodular greedy image selection shows some engineering ingenuity in optimizing data utilization."}, "weaknesses": {"value": "1. Unreasonable and Unfair Dataset Split: The authors modified the standard training-test split of the NSD dataset, moving shared images from the test set to the training set to achieve a one-to-one image mapping across subjects. This operation lacks a valid justification and is unfair. In fact, semantic alignment can be fully achieved through similar images rather than identical ones.\n2. Inconsistent Baseline Results Undermine Fair Comparison: The reported MindEye2 baseline results in this paper are significantly inconsistent with those in the original MindEye2 paper, with much lower values. This renders the performance comparison between the proposed method and the baseline unfair, failing to truly reflect the methodâ€™s actual effectiveness.\n3. Lack of Novelty in Core Idea and Insufficient Comparison with Prior Work: The core idea of mapping new subjects to a pre-trained reference subject has already been presented in existing public research (e.g., MindAligner[1]). The authors did not conduct a systematic comparison between their method and MindAligner, nor did they clearly demonstrate their superiority in this task.\n4. Unclear Research Significance and Limited Biological Insights: The research significance of the proposed method could be further clarified. In terms of decoding performance, it does not exceed the results reported in MindEye2 and MindAligner. From a biological perspective, the work does not elaborate on new insights into brain function or neural representation mechanisms, and the method leans more toward engineering implementation without fully highlighting its academic contribution in relevant research domains.\n[1] MindAligner: Explicit Brain Functional Alignment for Cross-Subject Visual Decoding from Limited fMRI Data. ICML 2025"}, "questions": {"value": "1. Regarding the dataset split modification in the NSD dataset, could you provide a rigorous justification for this operation and explain how it does not compromise the fairness and generalizability of the experimental results?  \n2. The MindEye2 baseline results in this paper differ significantly from those in its original paper. Please clarify the reasons for this discrepancy and ensure the fairness of the performance comparison.  \n3. How does your method compare with existing work like MindAligner in terms of core ideas and performance? Please provide a detailed comparison and explain your methodâ€™s unique advantages.  \n4. Could you elaborate on the research significance of this work, especially from the perspective of biological insights into brain function and neural representation? What new contributions does it make to the field?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DQp29RH3k8", "forum": "MOm6clIvmw", "replyto": "MOm6clIvmw", "signatures": ["ICLR.cc/2026/Conference/Submission17225/Reviewer_k8hD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17225/Reviewer_k8hD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922849524, "cdate": 1761922849524, "tmdate": 1762927184881, "mdate": 1762927184881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper well explores data efficiency in fMRI-to-image visual reconstruction with a novel, architecture-agnostic training paradigm called Adapter Alignment (AA). The method first trains a model on a \"reference subject\" to create a common representation space. New subjects are then aligned to this space using a lightweight adapter before a full end-to-end fine-tuning. This two-stage process, \"AAMax,\" uses an adapter-level MSE loss to maintain alignment. To further boost efficiency, the paper introduces a greedy submodular algorithm for image selection (IS), which picks a small, representative image subset covering the space's principal dimensions. The combined AAMax + IS approach achieves state-of-the-art results in low-data regimes, matching baseline performance with much less data. The method is validated across different architectures (MindEye1, MindEye2), datasets (NSD, THINGS), and varying data sizes."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "**Originality & Significance:** The paper's primary contribution is significant. \nWhile prior work (e.g., MindEye2) has used common spaces, this paper is the first to argue why this is insufficient and to propose explicitly aligning subjects within that space. The central hypothesis, that subject representations are structurally similar but not semantically aligned, is novel and well-motivated. The \"AAMax\" two-stage training paradigm is a clever and effective solution. This work directly tackles the main bottleneck in practical fMRI decoding (per-subject data acquisition), offering a solution that could make these powerful models far more accessible to the wider neuroscience community.\n\n**Methodology:** The paper is technically sound and presents a rigorous investigation.\n\n- The initial analysis in Section 2 and Figure 1 provides a clear, data-driven motivation for the entire paper. The use of Procrustes analysis to show structural similarity but misalignment is compelling.\n\n- The proposed Adapter Alignment method is well-explained and the two-stage \"AAMax\" process is logical.\n\n- The greedy image selection algorithm is a good design, providing a principled, submodular optimization-based alternative to random sampling.\n\n**Experiments:** The experimental validation is comprehensive and convincing.\n\n- The data split methodology is rigorous and ensures fair comparisons.\n\n- The authors correctly identify the method's primary strength in the low-data regime and also completely showing how the performance scales up with data volume (trade-off between data availability and the benefits of AA), clearly demonstrating the capability boundaries of the proposed method.\n\n- The method is shown to be architecture-agnostic.\n\n- The generalization to the THINGS dataset is a strong plus, demonstrating robustness to different acquisition parameters (3T vs. 7T) and stimuli.\n\n- The empirical analyses in Appendix J are excellent, providing direct proof (via eigenvector similarity, MSE, and k-NN consistency) that AAMax does result in a better-aligned common space, which strongly supports the paper's core claim.\n\n**Clarity:** The paper is well-written, and the core concepts are explained clearly. The figures are generally helpful in illustrating the   results."}, "weaknesses": {"value": "**Comparison to Highly Related Work:** The paper fails to discuss the differences with MindBridge (Wang et al., 2024) in the main text or related work (Appendix A, where it's missing). MindBridge's structure is even simpler than MindEye (no diffusion prior), which should be a good architecture for verifying the architecture-agnostic property. It also proposed a cross-subject training algorithm and raised the issue of low data availability for new subjects. Therefore, not discussing this highly related work is a major oversight.\n\n**Figures:**\n\n- Adding a training icon (ðŸ”¥) would make Figure 2 much easier to understand.\n\n- The paper lacks visual comparison results among different methods."}, "questions": {"value": "The ablation on number of principal eigenvector dimensions is helpful, but why performance is sensitive to this choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1TKYBvDDPf", "forum": "MOm6clIvmw", "replyto": "MOm6clIvmw", "signatures": ["ICLR.cc/2026/Conference/Submission17225/Reviewer_V2xv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17225/Reviewer_V2xv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935224316, "cdate": 1761935224316, "tmdate": 1762927184299, "mdate": 1762927184299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}