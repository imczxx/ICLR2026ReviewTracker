{"id": "41Pdz4r5aB", "number": 1546, "cdate": 1756891008816, "mdate": 1759898203144, "content": {"title": "LinearSR: Unlocking Linear Attention for Stable and Efficient Image Super-Resolution", "abstract": "Generative models for Image Super-Resolution (SR) are increasingly powerful, yet their reliance on self-attention's quadratic complexity ($O(N^2)$) creates a major computational bottleneck. Linear Attention offers an $O(N)$ solution, but its promise for photorealistic SR has remained largely untapped, historically hindered by a cascade of interrelated and previously unsolved challenges. This paper introduces LinearSR, a holistic framework that, for the first time, systematically overcomes these critical hurdles. Specifically, we resolve a fundamental, training instability that causes catastrophic model divergence using our novel ''knee point''-based Early-Stopping Guided Fine-tuning (ESGF) strategy. Furthermore, we mitigate the classic perception-distortion trade-off with a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we establish an effective and lightweight guidance paradigm, TAG, derived from our ''precision-over-volume'' principle. Our resulting LinearSR model simultaneously delivers state-of-the-art perceptual quality with exceptional efficiency. Its core diffusion forward pass (1-NFE) achieves SOTA-level speed, while its overall multi-step inference time remains highly competitive. This work provides the first robust methodology for applying Linear Attention in the photorealistic SR domain, establishing a foundational paradigm for future research in efficient generative super-resolution.", "tldr": "", "keywords": ["Image Super-Resolution", "Linear Attention", "Training Stability"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c2210989cd4dfd3899bf57a5fd3be822957355bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LinearSR, a novel and holistic framework for high-fidelity Image Super-Resolution (SR) that successfully integrates $O(N)$ Linear Attention into a conditional Diffusion Transformer (DiT) backbone. The primary motivation is to overcome the quadratic computational bottleneck ($O(N^2)$) imposed by traditional self-attention in generative SR models. The authors identify and systematically address three key challenges that have previously hindered the adoption of linear attention for photorealistic SR: training instability, the perception-distortion trade-off, and guidance effectiveness. LinearSR's solutions include: (i) Early-Stopping Guided Fine-tuning (ESGF) to resolve training divergence by stopping at the \"knee-point\"; (ii) an SNR-based Mixture of Experts (MoE) architecture to balance fidelity and distortion ; and (iii) a lightweight, effective TAG-based guidance paradigm based on the \"precision-over-volume\" principle. LinearSR achieves state-of-the-art efficiency, notably setting a new speed record for the core diffusion forward pass (1-NFE)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. LinearSR achieves a new state-of-the-art speed for the core diffusion forward pass (1-NFE) at $0.036$s for a $1024 \\times 1024$ output, demonstrating the practical benefit of $O(N)$ linear scaling.\n2. The paper holistically addresses the long-standing issues of linear attention in SR: instability (ESGF), distortion/perception trade-off (SNR-MoE), and sub-optimal guidance (TAG).\n3. The \"knee-point\" observation and the resulting Early-Stopping Guided Fine-tuning (ESGF) strategy offer a novel and insightful solution to a fundamental training divergence problem unique to this architecture.\n4. The model consistently outperforms SOTA methods on no-reference perceptual quality metrics (MANIQA, MUSIQ, CLIPIQA) across multiple benchmarks."}, "weaknesses": {"value": "1. While the 1-NFE time is SOTA, the overall multi-step inference time (0.830s) remains competitive but is not the absolute fastest compared to some distilled single-step methods like AdcSR and InvSR. The benefit of architectural efficiency is clear, but the user-facing speed remains a mixed result. Meanwhile, can LinearSR be applied into one-step inference?\n2. The \"knee-point\" is defined as the iteration of optimal generalization before performance degrades. This is highly dependent on tracking external validation metrics (PSNR/LPIPS evolution, Figure 3(b)), which makes it a non-trivial, potentially hyper-sensitive process in practice. More implementation detail (e.g., how the knee-point is automatically detected or whether a fixed iteration is sufficient) is needed."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZqsUHUfuIB", "forum": "41Pdz4r5aB", "replyto": "41Pdz4r5aB", "signatures": ["ICLR.cc/2026/Conference/Submission1546/Reviewer_Spqu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1546/Reviewer_Spqu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761403409120, "cdate": 1761403409120, "tmdate": 1762915804959, "mdate": 1762915804959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LinearSR, a novel framework for Image Super-Resolution (SR) that integrates Linear Attention into a DiT backbone. The primary motivation is to overcome the quadratic computational bottleneck imposed by full self-attention in SR task. To make linear attention to adapt to photorealistic SR scenerio: authors propose (i) Early-Stopping Guided Fine-tuning (ESGF) to resolve training divergence by stopping at the \"knee-point\"; (ii) an SNR-based Mixture of Experts (MoE) architecture to balance fidelity and distortion ; and (iii) a lightweight, effective TAG-based guidance paradigm based on the \"precision-over-volume\" principle.  Finally LinearSR achieves high-quality and fast generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. LinearSR achieves the fastest speed for 1 NEF forward time.\n2. The paper addresses the issues of linear attention in SR with several core contribution: including  Early-Stopping Guided\nFine-tuning,  SNR-based Mixture of Experts (MoE) architecture, and TAG-based guidance paradigm.\n3. The \"knee-point\" observation and the resulting Early-Stopping Guided Fine-tuning (ESGF) strategy offer a novel and insightful solution to a fundamental training divergence problem unique to this architecture.\n4. The model consistently outperforms SOTA methods on no-reference perceptual quality metrics (MANIQA, MUSIQ, CLIPIQA) across multiple benchmarks."}, "weaknesses": {"value": "1. While the 1-NFE time is SOTA, the overall multi-step inference time (0.830s) for one image inference time still lags behind that of the comparison methods like AdcSRm InvSR. I wonder can LinearSR be applied into one-step inference?\n2. The \"knee-point\" is defined as the iteration of optimal generalization before performance degrades. This is highly dependent on tracking external validation metrics (PSNR/LPIPS evolution, Figure 3(b)), which makes it a non-trivial, potentially hyper-sensitive process in practice. More implementation detail (e.g., how the knee-point is automatically detected or whether a fixed iteration is sufficient) is needed."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZqsUHUfuIB", "forum": "41Pdz4r5aB", "replyto": "41Pdz4r5aB", "signatures": ["ICLR.cc/2026/Conference/Submission1546/Reviewer_Spqu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1546/Reviewer_Spqu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761403409120, "cdate": 1761403409120, "tmdate": 1763634588049, "mdate": 1763634588049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for image super-resolution that leverages linear attention to achieve high-fidelity results with linear computational cost. In order to achieve good performance with linear attention, this paper address three major challenges introduced by linear attention, namely, the choice of external guidance, training stability, perception-distortion trade-off. Extensive experiments are conducted on a wide range of datasets to evaluate the proposed method's performance and efficiency. The ablation study also validate the proposed methods can successfully address the challenges mentioned in the paper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a way to apply linear attention into image super resolution with diffusion based model. I think it is a very promising and important research direction with great potential. \n2. This paper provides extensive analysis on the challenges introduced by applying linear attention, which is quite insightful. \n3. Several new techniques are proposed to address these challenges and are validated by ablation studies. \n4. Experiments show competitive performance of proposed method in terms of both SR quality and model efficiency."}, "weaknesses": {"value": "1. The efficiency comparison results reported in this paper are relatively simple. \n  1.1 I think the authors should elaborate more on the details of this evaluation. Otherwise, there may be some reproducibility and fairness concerns of this experiment.  For example, how each compared model is deployed? What kind of deployment framework is used? What is the numerical precision of the deployed model? Any acceleration techniques adopted at kernel level?  \n  1.2 The efficiency is compared only on one type of hardware setting. Comparison on different hardware setting will demonstrate the generalization ability on more practical scenarios of the method. \n  1.3 Inference speed on different resolution settings will also be a helpful analysis. \n  1.4 I think it will also be helpful if authors provide results in terms of model complexity such as FLOPs or parameter counts. This will provide another perspective of the model efficiency and also it is independent of hardware and deployment configurations. \n\n2. From table 1, the proposed method seems to achieve better perceptual quality than fidelity metrics, in the sense that it ranks much higher on perceptual quality metrics. Are there any explanations on these results? Does this have anything to do with the proposed MoE-based method?  \n3. From table 2, I am not sure I am able to observe a significant improvement on SR performance when comparing methods with similar efficiency like AdcSR. Authors may need to provide stronger arguments on the value of LinearSR based on this observation."}, "questions": {"value": "Please responds to my concerns in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N6FN4tp5qL", "forum": "41Pdz4r5aB", "replyto": "41Pdz4r5aB", "signatures": ["ICLR.cc/2026/Conference/Submission1546/Reviewer_cyVt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1546/Reviewer_cyVt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709715881, "cdate": 1761709715881, "tmdate": 1762915804797, "mdate": 1762915804797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LinearSR, a novel framework that enables robust application of linear attention in high-fidelity image super-resolution (SR). Addressing key limitations of linear attention—training instability, the perception-distortion trade-off, and inefficient guidance, by integrating three core innovations: Early-Stopping Guided Fine-tuning (ESGF), an SNR-based Mixture of Experts (MoE) architecture, and a TAG guidance paradigm. Experiments show LinearSR achieves linear computational complexity $\\(O(N)\\)$, with a state-of-the-art (SOTA) 1-NFE forward time of 0.036s for 1024×1024 SR and top performance on non-reference perceptual metrics (e.g., MANIQA 0.515, MUSIQ 71.914 on RealLQ250) against 10 SOTA methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The idea proposed in this paper is straightforward and easy to understand. LinearSR fills a critical gap by providing the first viable methodology for applying linear attention to high-fidelity SR, overcoming long-standing technical barriers (e.g., training instability) that previously hindered this approach.\n+ Its linear complexity delivers dramatic efficiency gains (e.g., 0.830s multi-step inference for 1024×1024 SR) without sacrificing perceptual quality, outperforming heavyweight models like SUPIR by orders of magnitude in speed while leading on key perceptual metrics.\n+ Comprehensive ablations (e.g., verifying ESGF’s necessity for stability, TAG’s superiority over text/CLIP/DINO guidance) and user studies confirm the synergistic value of each component, enhancing the work’s credibility."}, "weaknesses": {"value": "- While strong on perceptual metrics, LinearSR lags behind some baselines (e.g., SeeSR on PSNR for DrealSR: 26.212 vs. 25.235) in full-reference metrics, suggesting room to improve pixel-level fidelity without compromising perception.\n\n- The choice of 4 experts (over 2 or more) and the specific log-SNR boundaries (e.g., $\\(t_{anchor}=0.875\\)$) lacks extensive ablation, why this configuration outperforms alternatives is not fully justified beyond empirical results.\n\n- Though efficient for 1024×1024, the paper does not evaluate LinearSR on larger resolutions (e.g., 2048×2048), leaving uncertainty about whether its linear scaling holds for extremely high-resolution SR."}, "questions": {"value": "1. How might LinearSR’s performance on full-reference metrics (e.g., PSNR) be improved? Could integrating pixel-level loss terms (e.g., L1) with the CFM objective help balance perception and distortion without undermining efficiency?\n2. The \"knee point\" in ESGF is identified via validation metrics—what automated methods could detect this point dynamically, especially for diverse datasets where manual tracking may be impractical?\n3. Since LinearSR is orthogonal to distillation, have the authors tested combining it with model distillation? If so, what efficiency gains (e.g., reduced inference time) or quality trade-offs were observed?\n4. For the SNR-based MoE, how sensitive is performance to variations in the noise schedule (e.g., switching from \"scaled linear\" to cosine)? Would the log-SNR partitioning need re-calibration for different schedules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dZs3Nws3uZ", "forum": "41Pdz4r5aB", "replyto": "41Pdz4r5aB", "signatures": ["ICLR.cc/2026/Conference/Submission1546/Reviewer_UxiE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1546/Reviewer_UxiE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731591038, "cdate": 1761731591038, "tmdate": 1762915804647, "mdate": 1762915804647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to unlock the potential of linear attention in high-fidelity image super resolution (SR) and solve the computational bottleneck of the traditional SR generation model based on self-attention. The author attempts to solve three problems that hinder the application of linear attention in realistic SR: catastrophic training instability, the classic perception-distortion trade-off, and the lack of an efficient guidance paradigm. The authors then propose LinearSR, a framework integrating three core innovations: an Early-Stopping Guided Fine-tuning (ESGF) strategy that initializes fine-tuning from the \"knee point\" of training dynamics to resolve instability, an SNR-based Mixture of Experts (MoE) architecture that partitions the generative trajectory by noise level to mitigate the perception-distortion trade-off, and finally a lightweight TAG guidance paradigm derived from the \"precision-over-volume\" principle for effective feature extraction from low-resolution inputs. Evaluations show it achieves SOTA perceptual quality and efficiency, setting a foundational paradigm."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, this paper tackles a significant bottleneck in generative super-resolution. The core idea is simple yet powerful. The author identify that the potential of linear attention is hindered by training instability and the perception-distortion trade-off. They then systematically dismantle these barriers using a novel early-stopping strategy (ESGF) and an SNR-based expert architecture, successfully unlocking the efficiency of linear attention for high-fidelity image generation."}, "weaknesses": {"value": "**Major Weaknesses:**\n\nI appreciate the author's effort in unlocking linear attention for efficient super-resolution. However, I have some concerns that I summarize in three parts.\n\n1.  The paper states that each expert in the Mixture-of-Experts (MoE) architecture specializes in a different SNR range. However, it's unclear if the experts share the same architecture. If so, this might be suboptimal as different generative stages (e. g., structure formation vs. detail polishing) could benefit from structurally different networks. The paper could provide more justification for using a homogeneous structure.\n\n2. The paper's MoE employs a deterministic, rule-based gating mechanism that routes inputs based on fixed, pre-calculated time boundaries(t). While this cleverly avoids the load-balancing problems of sparse MoEs, it seems overly simplistic and rigid. The optimal boundaries for generative stages are likely task-dependent and data-dependent. A hard-coded if-else structure lacks the flexibility to adapt to different degradations or content types. A learnable(yet still efficient) gating network could potentially discover more optimal, dynamic routing strategies, leading to better specialization and overall performance. The paper could benefit from discussing why this rule-based approach was chosen over a more adaptive one and exploring its limitations.\n\n3. The Early-Stopping Guided Fine-tuning (ESGF) strategy relies on identifying a \"knee-point\" from validation metrics. This approach seems effective but might be sensitive to the choice of validation set and evaluation metrics. The paper could discuss the robustness of this method and how reliably this \"knee-point\" can be identified across different datasets and model configurations in practical scenarios.\n\n**Minor Weaknesses:**\n\nIn the ablation study for the SNR-based MoE (Table 5), the time boundaries t are provided for different expert configurations. However, the paper could be more explicit about how these specific boundary values (e.g., [0.223, 0.875, 0.939]) were derived or chosen for each experimental setting, beyond the general hierarchical bisection method described in the appendix."}, "questions": {"value": "Please clarify my concerns in the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nV0LG5wvUM", "forum": "41Pdz4r5aB", "replyto": "41Pdz4r5aB", "signatures": ["ICLR.cc/2026/Conference/Submission1546/Reviewer_NwBS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1546/Reviewer_NwBS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841143924, "cdate": 1761841143924, "tmdate": 1762915804470, "mdate": 1762915804470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}