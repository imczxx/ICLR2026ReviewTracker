{"id": "7WrMAQ1XDq", "number": 21634, "cdate": 1758319888716, "mdate": 1759896911430, "content": {"title": "Multi-Marginal f-Divergence Schrödinger Bridges: Towards a Unifying Framework for Generation and Distillation", "abstract": "We propose a unified framework for multimodal generation and knowledge distillation by leveraging the Multi-marginal Static Schrödinger Bridge (MSSB) with general f -divergence, where we use flexible and task-oriented prior measures. This approach allows us to adapt the MSSB problem to diverse tasks—from text-guided image generation to model compression—simply by designing an appropriate prior.\nFor generative modeling, we develop an efficient block-stochastic optimization scheme and a practical Langevin-based inference method. For knowledge distillation, this framework has a clear information-theoretic interpretation: we prove that our MSSB-based Knowledge Distillation (MSSB-KD) implements a variational relaxation of the Information Bottleneck principle. Our novel MSSB-KD formulation demonstrates strong robustness to noisy supervision, significant gains in multi-teacher settings, and scalability across architectures. Finally, we theoretically prove the equivalence between Static and Dynamic Schrödinger Bridges for general\nf-divergences, enabling the use of divergences better suited to the task at hand.", "tldr": "", "keywords": ["Schrodinger Bridges", "Optimal Transport", "Generative Modeling", "Knowledge Distillation."], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/407c84f55eef3920aabb95ea5ea74c8873df5dec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Multi-marginal Static Schrödinger Bridge (MSSB) with general f-divergence as a unified framework for multimodal tasks. Key contributions include proving equivalence between static and dynamic formulations beyond KL divergence, showing MSSB-based knowledge distillation implements a variational Information Bottleneck relaxation, and demonstrating strong empirical results in generation and distillation tasks with robustness to noise."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper makes several original contributions. Most notably, it extends the traditional two-marginal Schrödinger Bridge framework to handle three or more distributions.\n2. The paper demonstrates certain theoretical and empirical quality.\n3. The paper is well-structured and clearly written."}, "weaknesses": {"value": "1. The paper lacks a critical analysis of computational scalability. While the authors demonstrate N=3 cases (image, text, output), real-world multimodal applications often require handling 4+ modalities (e.g., vision+language+audio+context). The paper doesn't address how their block optimization scales as N increases.\n2. The generative modeling experiments focus exclusively on FFHQ, which has limited diversity compared to broader datasets like ImageNet or COCO. Similarly, KD evaluations use only CIFAR-10/100, omitting more challenging benchmarks like ImageNet or domain-specific tasks (medical imaging, satellite imagery).\n3. The paper emphasizes computational advantages of SSB over DSB, but doesn't quantify the actual runtime/memory costs of their approach."}, "questions": {"value": "See the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CSDsFypkXB", "forum": "7WrMAQ1XDq", "replyto": "7WrMAQ1XDq", "signatures": ["ICLR.cc/2026/Conference/Submission21634/Reviewer_geRY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21634/Reviewer_geRY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761271576254, "cdate": 1761271576254, "tmdate": 1762941863741, "mdate": 1762941863741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a framework based on Multi-Marginal f-Divergence Schrödinger Bridges, which aims to unify multimodal generation, image editing, and knowledge distillation under a single theoretical formulation. The approach extends Schrödinger Bridge theory to multiple marginals and arbitrary f-divergences, proposing applications in generative modeling and teacher–student distillation.\n\nHowever, the niche of the paper is unclear. The advantages of MSSB over existing image generation or editing frameworks are not demonstrated. While the theoretical development is mathematically sound, the empirical validation is weak, lacking strong baselines, quantitative comparisons, and diverse datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper offers a theoretically interesting formulation connecting f-divergences, optimal transport, and multi-marginal Schrödinger Bridges.\n* The topic is conceptually appealing and timely in theory-oriented machine learning research."}, "weaknesses": {"value": "* **Unclear practical niche.** It remains ambiguous how this method fits into the broader landscape of generative modeling. The theoretical contribution is interesting, but the connection to practical tasks like generation or editing is underexplored.\n* **Lack of demonstrated advantages over existing methods.** The paper does not clearly explain or empirically validate why the proposed approach is preferable to well-established image generation or editing techniques (e.g., diffusion-based). It remains unclear why practitioners or researchers should prefer this approach over well-established alternatives.\n* **Missing comparisons and quantitative evaluation.** The paper lacks evaluation against relevant baselines and omits standard quantitative metrics such as FID, CLIP score. Only one dataset is used, which severely limits the robustness of the conclusions. Without quantitative analysis, it is difficult to gauge the practical value of the proposed framework.\n* **Unconvincing image editing results.** The qualitative examples for editing, such as the “smiling” case, are weak and fail to demonstrate realistic or semantically consistent changes. The results appear minimal and do not substantiate the claimed improvement in editing control."}, "questions": {"value": "* What concrete advantages does MSSB offer over existing diffusion or optimal transport–based generative models in terms of controllability or performance?\n\n* Could the authors include quantitative comparisons on standard benchmarks using metrics such as FID or CLIP alignment?\n\n* Does the framework generalize to more complex or multimodal domains?\n\n* Can the authors provide stronger and more diverse editing examples that better illustrate the claimed benefits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DqXAkfeq1N", "forum": "7WrMAQ1XDq", "replyto": "7WrMAQ1XDq", "signatures": ["ICLR.cc/2026/Conference/Submission21634/Reviewer_rdUR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21634/Reviewer_rdUR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763083054, "cdate": 1761763083054, "tmdate": 1762941863420, "mdate": 1762941863420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents the framework for multi-marginal static Schrödinger Bridge (MSSB) problem to enable multimodal generation and knowledge distillation. MSSB is theoretically grounded and employs practical training with block-stochastic optimization and Langevin-based inference. MSSB is evaluated on text-to-image and text-guided editing tasks, and further extend it to knowledge distillation, demonstrating effective multi-teacher distillation and improved robustness to noisy teachers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed framework for MSSB is well-presented and seem novel.\n\n* The paper provides convincing theoretical results.\n\n* The MSSB-KD demonstrates promising robustness to noisy teachers."}, "weaknesses": {"value": "I believe the key problem of the paper is limited practical value and insufficient experimental evaluation:\n\n* The paper lacks any quantitative results and comparisons to alternative approaches for text-to-image and editing tasks. Thus, it is difficult to assess the method in terms of fidelity and efficiency.\n\n* Image editing results are inconsistent. The method often does not preserve the source images and does not perform the desired edit.\n\n* MSSB-KD does not show performance gains over DKD on the clean data. \n\n* For KD, the comparison with DOT[1] may be also valuable. This technique can consistently boost the performance of the KD baselines. Also, it is interesting if DOT may complement MSSB-KD as well.\n\n**Minor**\n\n* \\cite needs to be replaced with \\citep in many places\n* In table 1, I guess \"T-1\" and \"T-5\" denote \"top-1\" and \"top-5\" accuracy, respectively. This can be clarified in the text.\n\n[1] DOT: A Distillation-Oriented Trainer. ICCV2023"}, "questions": {"value": "Please address the concerns in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sd2f3LkT4y", "forum": "7WrMAQ1XDq", "replyto": "7WrMAQ1XDq", "signatures": ["ICLR.cc/2026/Conference/Submission21634/Reviewer_7vZt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21634/Reviewer_7vZt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777209820, "cdate": 1761777209820, "tmdate": 1762941863219, "mdate": 1762941863219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies static multi-marginal Schrödinger Bridge problems with applications to different scenarios: **image editing / unpaired image2image** in small-dimensional latent spaces and **knowledge distillation** (in the context of classification problems). In case of the image2image problems, the authors suggest optimizing the dual objective with respect to the dual potentials. They perform translation via Langevin dynamics, applied to the optimal plan representation via potentials, and demonstrate applicability of the method in the latent space of ALAE autoencoder. In case of KD, they stick to the primal objective, establish its connection with the Information Bottleneck, and demonstrate robustness of the method to noise and high performance in multi-teacher settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Theoretical part of the paper is well-organised, well-written, and easy to follow;\n\n* Representation of the image editing problem via coupling the source domain, target domain, and text instructions in style of the reward-based sampling from latent space looks promising;\n\n* The proposed KD method offers desirable properties: robustness to noise and high performance in the multi-teacher setting."}, "weaknesses": {"value": "### Focus of the paper\nThe major concern that I have is that the paper does not have a clear focus. It claims contributions in different theoretical, methodological, and experimental aspects, which are almost disconnected (except being related to the multi-marginal SB). Moreover, each of these contributions feel limited. Overall, the paper feels more like a review of the potential applications of the multi-marginal SB formulation, rather than a focused methodological work. Despite having some promising ideas, I think the paper in the current form lacks substance for publication. It would greatly benefit from focusing on one of the more specific directions of research. Below I will try to summarize this observation and highlight this and other concerns in details.\n\n### Theory\n1) The authors outline $f$-divergence in the name of the paper and in the contributions. However, in the experiments with KD, the authors use standard KL divergence; in the I2I settings, the authors do not specify $f$ and do not ablate its choice. Thus, I feel like highlighting $f$ divergences here is not significantly grounded.\n2) The authors highlight that they prove the equivalence between the static and dynamic $f$-divergence SBs. However, the dynamic formulation is not used anywhere in the methods (except Figure 1, where the (statically) generated pictures are interpolated with noise after generation, which may harm the paper's presentation by confusing the readers). Though I do not try to understate the result itself (since it clearly has potential in developing $f$-divergence analogues of the existing SB methods), the proof is a simple combination of the data-processing inequality for $f$-divergences with the chain-rule for the Radon-Nikodym derivatives, which does not seem to have enough substance on its own. Overall, I do not see the importance of these results inside the paper.\n\n### Methodology\nThe authors propose two applications of the multi-marginal SB formulation: latent-space image editing, and knowledge distillation. However, they do not propose a unifying methodological framework and instead solve the dual problem for images and the primal problem for distillation, which negatively affects the interconnection between the parts of the work. Moreover,\n1) It seems that the dual algorithm is a straight reformulation of the SCONES [1] algorithm for the multi-marginal setting. This significantly limits the contribution in terms of I2I applications. The corresponding reference is lacking in the manuscript;\n2) I feel like the dual algorithm is very restrictive in terms of the latent space dimension and structure. It requires performing Langevin updates, which is computationally expensive and not very efficient in high-dimensional structured domains as e.g. clean images or latent representations in diffusion autoencoders;\n3) The sampling scheme in the dual algorithm became tractable due to ignoring the $d \\mu_2(y)$ part of the distribution, which corresponds to the unconditional density of the data (e.g. images or latents). This simplification was seemingly possible only due to the simple nature of the ALAE latent space. I would speculate that in case of the more complex distributions the algorithm would not work without the corresponding data score;\n4) The authors formulate the prior distribution as $d R = \\exp(\\ldots) d\\mu_1 d \\mu_2 d \\mu_3$, where the measures $\\mu_i$ are coupled via the tensor product (e.g. correspond to independent variables). However, in the training process they correspond to the dependent images and text instruction. It is also not very clear how the text-image pairs are transformed into the triplets $(x, y, t)$;\n5) The proposed algorithm for the KD problem is not clearly stated and lacks important details: how are the \"total correlation\" and \"marginal regularization\" calculated during optimization? Is there some restriction on the probabilistic model of $\\pi$ by design, so the objective becomes tractable? The transition from the marginal-constrained to the unconstrained problem is also not clearly explained.\n\n### Experiments\nSome experimental results in the I2I section are not convincing. Some of the selected source pictures are very close to the target domain (e.g. smiling people in neutral $\\to$ smiling). The translation often fails at fitting the target distribution.\n\n\n[1] Score-based Generative Neural Networks for Large-Scale Optimal Transport"}, "questions": {"value": "1) Could you please tell how the triplets $(x, y, t)$ are constructed in the I2I setting?\n2) It would be great if you could describe the algorithm for KD in more details: how is the plan $\\pi$ parameterized, how are the regularization terms calculated during optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xDYPFSoEQp", "forum": "7WrMAQ1XDq", "replyto": "7WrMAQ1XDq", "signatures": ["ICLR.cc/2026/Conference/Submission21634/Reviewer_jjFu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21634/Reviewer_jjFu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938481821, "cdate": 1761938481821, "tmdate": 1762941862976, "mdate": 1762941862976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}