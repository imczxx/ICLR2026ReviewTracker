{"id": "X9QPmuosA7", "number": 9424, "cdate": 1758122047663, "mdate": 1759897725807, "content": {"title": "FiGuRO - Intrinsic Dimension Estimation for Multi-Modal Data", "abstract": "A fundamental challenge in representation learning is determining the complexity, or the Intrinsic Dimension (ID), of the data. This becomes especially difficult in the multi-modal setting when trying to learn disentangled subspaces for shared and private (modality-specific) information. Existing ID estimation techniques are ill-suited for this task, as they are either static and uni-modal or, in the case of state-of-the-art contrastive methods, adapt only to the shared ID implicitly. This leaves a critical gap for a method that can estimate the complete ID structure of multi-modal data. We introduce Fidelity-Guided Rank Optimization (FiGuRO), a framework for learning the IDs of uni- and multi-modal data. FiGuRO learns the dimensions of low-rank projections using truncated singular value decomposition and an algorithm that determines when to reduce or increase dimensionalities and in which latent spaces. We demonstrate that FiGuRO outperforms uni-modal ID estimation techniques and is more robust to hyperparameter changes. In the multi-modal setting, FiGuRO successfully captures differences between scales of IDs and varying ratios between shared and private subspaces on simulations and real datasets. Our work provides a quantitative framework for assessing the shared and private informational contributions of multi-modal data. This helps construct more interpretable models and can guide strategic and efficient data collection in fields like biology and medicine.", "tldr": "", "keywords": ["Representation Learning", "Intrinsic Dimension", "Multi-modal Learning", "Interpretability"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/985d16050cb603296748d3667f466c0ad471b3ad.pdf", "supplementary_material": "/attachment/dbf570ba509d274d94bc09557ebc143180367d10.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a method to estimate the intrinsic multimodal data dimension: shared and source-specific. To achieve this, they rely on paired autoencoders and iteratively adapt their latent dimension size by learning a low-dimensional projection. Experiments on the simulated and real data show utility of the method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel methodology.** The proposed method of greedily adapting the latent dimension rank (rate) to achieve the desired error (distortion) is novel and appears justified. \n2. **Clear motivation and writing.** The paper is mostly clear and well-explained. The problem is well-motivated."}, "weaknesses": {"value": "1. **Lack of comparisons with multi-view data decomposition methods.** On line 288, the authors claim lack of \"multi-modal ID estimation techniques,\" which is not strictly speaking true. There a vast literature on the multi-view data decomposition with canonical methods such as CCA [1], JIVE [2], AJIVE [3], DIVAS [4], and PPD [5]. While these methods assume a specific linear model of joint and individual mixing, I believe they provide a strong baseline that any non-linear methods must compare with. Moreover, unlike the current method which only estimates the joint and individual ranks, the methods [1-5] also produce estimates of the joint and individual subspaces (the spanning sets).  I believe this a critical omission in the current work. \n\n\n2. As the original **motivating examples**, which are plausible, the authors list the following points why ID estimation is useful: \n> (i) performance of deep neural networks has been shown to depend on the intrinsic rather than ambient dimension\n> (ii) need interpretable models \n> (iii) want to know whether expensive or difficult-to-obtain modalities are relevant\n\nNone of them are sufficiently illustrated in the experiments section. The authors apply their method, obtain the ID of each data source, and then check if it agrees with their intuition. \n\n3. **Corner (degenerate) cases.** I feel the method crucially assumes that $k_s > 0$. In practice, the practitioners may be interested in applying the method exactly under the setup, where the dependence (shared) signal between the modalities is uncertain. However, from the examination of the algorithm steps, I believe the method would fail to converge in such case. \n\nThe exact setup I have in mind is supposing, we have Assumptions 1-4 hold: oracle autoencoders and correct data generating mechanism. But further suppose that $k_s = 0, k_1 = 10, k_2 = 10$, i.e., there is no joint. Going through the steps in Algorithm 1, because the updates to the individual / joint ranks are combined together from Eqn. 4 and Lines 13-14 in the algorithm, the problem becomes unidentifiable. I can see two distinct fixed point between which the algorithm is going to oscillate, estimate the joint to be rank $=20$ or two individual of rank $=10$ each. Namely, I don't see any mechanism by which the algorithm is supposed to correctly identify which option to prefer. \n\n--- \n[1] Hotelling, H. (1936). Relations between two sets of variates. Biometrika, 28, 321–377. \n\n[2] Lock, Eric F., et al. \"Joint and individual variation explained (JIVE) for integrated analysis of multiple data types.\" The annals of applied statistics 7.1 (2013): 523.\n\n[3] Feng, Qing, et al. \"Angle-based joint and individual variation explained.\" Journal of multivariate analysis 166 (2018): 241-265.\n\n[4] Prothero, Jack, et al. \"Data integration via analysis of subspaces (DIVAS).\" TEST 33.3 (2024): 633-674.\n\n[5] Sergazinov, Renat, Armeen Taeb, and Irina Gaynanova. \"A spectral method for multi-view subspace learning using the product of projections.\" arXiv preprint arXiv:2410.19125 (2024)."}, "questions": {"value": "1. What's the purpose of $k_{min}/k_{max}$, I don't see them being used anywhere in the algorithm. \n2. On line 17 in the algorithm, it says \"set ranks to their previous values\". What are the initial values for these? Let's say the algorithm just started, then supposedly, the ranks have to be initialised already? \n3. There are few typos: (i) on 188 \"perform do\" (ii) line 469 \"Alltogether\". The paper needs to be proof read."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tMc2gvQChU", "forum": "X9QPmuosA7", "replyto": "X9QPmuosA7", "signatures": ["ICLR.cc/2026/Conference/Submission9424/Reviewer_y7sV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9424/Reviewer_y7sV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760981410031, "cdate": 1760981410031, "tmdate": 1762921026284, "mdate": 1762921026284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problem of learning latent representations for multi-modal data. For such a problem, determining the intrinsic dimension is a crucial problem. The authors propose a method to adaptively adjust the intrinsic dimension when learning multi-modal autoencoders. In specific, they multiply the latent representation with a weight matrix $W$, and apply SVD on $W$ every several epochs. The criteria for choosing rank is the reconstruction fidelity. If it is increased, the rank is reduced, otherwise increasing. This rank adaptation process is performed after pre-training of the AE, until convergence of the learned rank. The authors conduct experiments on several simulation datasets and two real datasets to show that the proposed model is able to learn the intrinsic dimensions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method is simple to use and has potential in many AE architectures, which could be an essential component in different ML models."}, "weaknesses": {"value": "1. Lack of strong baselines. The authors compare with several baselines in simulation, but not for real-world data analysis. For results in Table 2, the authors list the range of estimation varying hyperparameters. The proposed method has the smallest and closest range. But could this be because of inappropriate choices of other methods?\n\n2. In experiments, the authors mainly show the results for estimating the rank. But it is unclear what the practical advantage is, e.g., reconstruction performance or interpretability.\n\n3. Although the authors claim that the proposed method targets multi-modal data, I don’t feel there are specific designs for multi-modal settings. The rank selection procedure is applied on each modality sequentially. Moreover, applying the method on multi-modal settings may require choosing different fidelity measurements for different data domains."}, "questions": {"value": "1. As said in the paper, ARR is one very related paper. ARR applies SVD on latent representations, while this work applies SVD on a projection matrix. What is the motivation of this difference?\n\n2. Did the authors consider adding sparse regularization during either pre-training or rank optimization? Would it help the rank learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5ayDXqZSx1", "forum": "X9QPmuosA7", "replyto": "X9QPmuosA7", "signatures": ["ICLR.cc/2026/Conference/Submission9424/Reviewer_XWg9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9424/Reviewer_XWg9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549060927, "cdate": 1761549060927, "tmdate": 1762921025984, "mdate": 1762921025984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Previous dimension estimation methods usually use a unified and static dimension for the representation of the whole dataset, or contrastively estimate for other datasets. This is ill-used for multi-modality data. With this motivation, this paper proposes FiGuRO, which estimates the shared dimension and private dimension for each modality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Emphasize the ill-used unified dimension for multi-modality data.\n2. Propose FiGuRO, which uses SVD to estimate the dimension and is constrained by the Rate-Distortion Theory."}, "weaknesses": {"value": "1. Some notion is not clear. In equations (3) and (4), SVD is used to estimate the intrinsic dimension. However, this paper did not describe how to estimate the shared dimension $k_s$ and private dimension $k_1$. Instead, the low-rank weight matrices are directly used in equation (4).\n2. Experiments are mostly designed in simulation. In real-world data, a good dimension will result in better performance in downstream tasks. More evaluation on downstream tasks will be helpful.\n3. The SVD and Rate-Distortion Theory are both commonly used for intrinsic dimension; the method only replicates SVD for multimodality data."}, "questions": {"value": "1. For the loss function, if only R(D) is used?\n2. If the hyperparameter distortion threshold $\\lambda$ has a special meaning, why only analyze this hyperparameter in Figure 2?\n3. In Figure 2, the evaluation metrics accuracy and goodness of R are clear, how to read the rank in Figure 2A? If close to the ground truth (dashed lines) indicates better performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JEKRo1XJQy", "forum": "X9QPmuosA7", "replyto": "X9QPmuosA7", "signatures": ["ICLR.cc/2026/Conference/Submission9424/Reviewer_Gqdy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9424/Reviewer_Gqdy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831810612, "cdate": 1761831810612, "tmdate": 1762921024115, "mdate": 1762921024115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FiGuRO (Fidelity-Guided Rank Optimization), a framework for intrinsic dimension (ID) estimation in unimodal and multimodal data. FiGuRO combines rate-distortion theory with low-rank projections that adaptively adjust latent dimensionality based on reconstruction fidelity. It separates shared and private subspaces across modalities, providing interpretable and efficient ID estimation. Experiments on synthetic and real data demonstrate accurate and stable results outperforming existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tFiGuRO combines rate-distortion theory with adaptive low-rank learning, providing a principled and theoretically sound approach for intrinsic dimension estimation in multimodal data.\n\n2.\tThe method is evaluated on various synthetic and real datasets, covering scalar, image, and temporal modalities, and show improvement over baselines in both stability and accuracy.\n\n3.\tThe idea that the dimensions of different modalities might differ is realistic and novel, providing significance for real-world applications.\n\n4.\tThe framework can be implemented within standard autoencoders. This simplicity makes FiGuRO highly practical and broadly applicable across different data types and model settings, enhancing its potential for adoption in real-world multimodal learning pipelines."}, "weaknesses": {"value": "1.\tSome implementation details (e.g., how the distortion budget $\\gamma$ or threshold $\\lambda$ is set) are not fully discussed, which may affect reproducibility. Also, the sensitivity regarding these hyperparameters is not evaluated."}, "questions": {"value": "Is the method sensitive to the hyperparameters? And is there any experimental validation in this aspect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yVBCtGMtmm", "forum": "X9QPmuosA7", "replyto": "X9QPmuosA7", "signatures": ["ICLR.cc/2026/Conference/Submission9424/Reviewer_KrzM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9424/Reviewer_KrzM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893511906, "cdate": 1761893511906, "tmdate": 1762921023815, "mdate": 1762921023815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}