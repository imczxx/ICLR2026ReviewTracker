{"id": "51LIRzF53v", "number": 5646, "cdate": 1757925225065, "mdate": 1759897963306, "content": {"title": "Towards Personalized Deep Research: Benchmarks and Evaluations", "abstract": "Deep Research Agents (DRAs) can autonomously conduct complex investigations and generate comprehensive reports, demonstrating strong real-world potential. However, existing evaluations mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios. To bridge this gap, we introduce Personalized Deep Research Bench, the first benchmark for evaluating personalization in DRAs. It pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles that combine structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. To assess system performance, we propose the PQR Evaluation Framework, which jointly measures (P) Personalization Alignment, (Q) Content Quality, and (R) Factual Reliability. Our experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research. This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants.", "tldr": "", "keywords": ["Personalization", "benchmark", "Deep Research", "Agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/511cb05eb1a1b3c41a481ab23f05ae9f973d195b.pdf", "supplementary_material": "/attachment/0cb8ee9849e0e6b8674c5254d8c6e3f925110d54.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a new benchmark for personalized deep research. Deep research is a popular mode of AI agents, where an agent utilizes several tools like iterative search, to answer complex question. Personalization in the context of deep research is underexplored. This paper presents a new benchmark to evaluate the personalization capabilities of LLMs in the context of deep research. Specifically, the paper focuses on evaluating 3 key aspects of deep research responses -- personalization, response quality, and reliability. The paper presents automated metrics to evaluate each of these dimensions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on a relatively underexplored aspect of LLM evaluation -- personalization in deep research. The paper discusses the key drawbacks of existing benchmarks and builds on those aspects. \n2. The paper collects personalization data in a thorough manner by including people of diverse backgrounds and collect data from these folks over prolonged interactions."}, "weaknesses": {"value": "1. I'm concerned about the privacy implications of the benchmark release. According to Table 5 in Appendix E, the persona collection process includes identity characteristics like name, age, gender, etc. These information along with others could often be enough to reveal the identity of an individual. \n\n2. The proposed evaluation process seems to be weak. Along the personalization dimension in Section 4.1, the authors ask the LLM to generate both the criteria and the weight of each criteria. Were these information reviewed by experts? Also, it seems unclear which LLM was used for this specific task. \n\n3. The paper misses out on key details in some sections like 4.2. Why did the paper choose not to use well-established factuality metrics like FActScore [1]? How are the verifiable claims extracted from the answers?\n\n4. How is $\\lambda_a$ in Equation 6 chosen?\n\n5. Most of the results presented in the paper are scores generated by LLM. In Section 5.5, the paper evaluates the correctness of these scores by comparing it human annotators. However, this task was simplified to a great degree where the annotator had to choose between a couple of responses. The paper compares LLM score agreement in this setting. This is not an accurate measure of correlation of the actual score (which are between 1-10). Even in this simplified setting, the correlation seems to be poor raising questions about the quality of LLM ratings. \n\n6. In Table 1, 3, and left figure of Figure 3, we observe that incorporating additional personal context doesn't change the performance too much, even the P-Score. Does this mean current LLMs aren't able to personalize based on the provided context? If not, how can the P-score be so high without any context?\n\n\n[1] FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"}, "questions": {"value": "Please respond to the questions in the above sections."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "I have concerns that releasing this dataset might compromise the privacy of the study participants."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lDinRk5bzI", "forum": "51LIRzF53v", "replyto": "51LIRzF53v", "signatures": ["ICLR.cc/2026/Conference/Submission5646/Reviewer_vdTb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5646/Reviewer_vdTb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794902040, "cdate": 1761794902040, "tmdate": 1762918173704, "mdate": 1762918173704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a benchmark for personalized deep research agents with 25 real human participants created persona profiles. The authors designed a new evaluation framework for this benchmark that jointly measures personalization, content quality and factuality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Real human persona and questions and tasks are designed by human expert that increases the credibility of the benchmark in reflecting real user behaviors.\n\n2. Comprehensive analysis across different models and agents."}, "weaknesses": {"value": "1. The motivation for combining personalization and deep research into one benchmark is lacking. These two aspects are largely orthogonal to me as they measure different things. The authors did not explain why a fused benchmark is necessary. One can imagine that combining the scores of existing personalization and deep-research benchmarks are already a good metric for measuring personalized deep research agent. There is no correlation analysis showing that performance on this benchmark differs from performance on either personalization benchmarks. For example, I want to see that a model performs well on both personalization and deep research agent benchmark, but performs poorly on your benchmark. Without such evidence, the benchmark’s purpose and contribution remain weak.\n\n2. LLM as judge bias. For personalization benchmark, it's important to show that llm as judge are unbiased, potentially by comparing with the real participants' and users' feedback on some models, so you can show that llm evaluation are aligned with actual user behaviors. The section 5.5 has a alignment with some human rating, this does not address the problem, since it's a personalization benchmark rather than a rlhf benchmark. It would be great to see the actual alignment with the actual user who generated the persona.\n\n3. Scale is somewhat limited. With only 25 profiles, it's hard to generalize to the whole populations and demographics."}, "questions": {"value": "1. Can you show the correlation of your benchmark with some personalization benchmark and research agent benchmark?\n\n2. How to ensure that llm as judge are aligned with users behaviors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V7TRNO2uET", "forum": "51LIRzF53v", "replyto": "51LIRzF53v", "signatures": ["ICLR.cc/2026/Conference/Submission5646/Reviewer_8bjw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5646/Reviewer_8bjw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860048144, "cdate": 1761860048144, "tmdate": 1762918173379, "mdate": 1762918173379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Introduces a new benchmark and evaluation framework to address a gap in assessing Personalization in Deep Research Agents.\nPersonalized Deep Research Bench is a benchmark specifically designed to evaluate personalization in DRAs. Contains 250 user research task pairs.  PQR Evaluation Framework focuses on assessing the personalization alignment of a DRA output given a persona. Incorporates Goal Alignment, Content Alignment, Presentation Fit, and Actionability & Practicality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea here is good. Objective measures of Research Agent outputs are obviously important, but there is value in customizing the research process and the resulting artifact to match the sensibilities of the user. So the work addresses this particular gap.\n\nI expect the benchmark itself would be of some value to the research community. In particular the inclusion of structured and context data in user profiles, and the diversity of tasks. \n\nThe personalization metric is well defined and the evaluations presented are comprehensive."}, "weaknesses": {"value": "It would be better if the personas were developed from real world interactions, the authenticity is limited. The sample size of 25 profiles is also relatively small for a large-scale, generalizable benchmark.\n\nUse of LLMs for evaluations. I know its common practice now but you need to address for bias and reproducibility. \n\nWhile I appreciate the definition of the personalization metric, this is a complex topic and would help to look for validation in prior research."}, "questions": {"value": "Any details of treatment for bias and reproducibility when running experiments?\nHow is the \"user’s evolving interests, habits, and implicit preferences\" incorporated in the profile/evaluation?\nAny instances of conflicting signals in the user profiles? How does the DRA handle such cases?\nHave you studied the factors that influence the dynamic weight allocation when running eval framework?\nWhat is the cost to run the benchmark? How can we scale it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7hhmtquv48", "forum": "51LIRzF53v", "replyto": "51LIRzF53v", "signatures": ["ICLR.cc/2026/Conference/Submission5646/Reviewer_maAF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5646/Reviewer_maAF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941292494, "cdate": 1761941292494, "tmdate": 1762918173181, "mdate": 1762918173181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}