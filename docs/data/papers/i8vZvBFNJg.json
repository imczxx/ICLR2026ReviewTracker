{"id": "i8vZvBFNJg", "number": 7761, "cdate": 1758035198142, "mdate": 1759897834381, "content": {"title": "MAGO: Beyond Fixed Hyperparameters with Multi-Objective Pareto Optimization for Hybrid LLM Reasoning", "abstract": "Large language models (LLMs) with advanced step-by-step reasoning abilities have demonstrated exceptional performance in complex problem-solving through chain-of-thought (CoT) reasoning. However, applying elaborate reasoning processes uniformly to all queries creates significant computational inefficiency, as many problems could be solved more directly without extended reasoning chains. To address this challenge, we propose MAGO (Multi-objective Adaptive Generation Optimization), a principled framework that reformulates hybrid reasoning as a multi-objective optimization problem grounded in Pareto optimality theory. MAGO dynamically balances three competing objectives: accuracy (maintaining solution correctness), efficiency (minimizing computational costs), and calibration (ensuring mode selection aligns with model capabilities). Unlike existing approaches that rely on static hyperparameters, our framework employs adaptive weight selection through Pareto frontier maintenance with correlation-aware optimization to automatically determine optimal trade-offs based on task complexity and model performance. The system integrates seamlessly into existing hybrid reasoning training through end-to-end optimization, avoiding manual hyperparameter tuning while preventing the spatial constraints that limit single-objective approaches. Evaluation on mathematical reasoning benchmarks including AIME, Minerva Algebra, MATH-500, and GSM-8K demonstrates that MAGO reduces computational costs by 15-20\\% while maintaining accuracy, providing principled trade-offs across diverse problem-solving scenarios.", "tldr": "", "keywords": ["Multi-objective optimization", "Pareto optimization", "Large language models", "Hybrid reasoning", "Chain-of-thought reasoning", "Reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9697f3e3608f4db807d9661a4bd8f13e7088f73c.pdf", "supplementary_material": "/attachment/58c4eb273ad669c337f7b6b45801616c8720249c.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes MAGO, a multi-objective adaptive generation optimization framework for hybrid LLM reasoning that learns when to use short versus think modes. MAGO formulates training as a three-objective problem—accuracy, efficiency, and calibration—combining a Pareto frontier over weight vectors with a correlation-aware selection rule and exploration to avoid “cone entrapment” inherent in fixed scalarizations. The adaptive weight mMAGO(x) scales the control-token loss in a GRPO-style objective, aiming to stabilize mode selection and prevent collapse. On math benchmarks (AIME 2024, Minerva Algebra, MATH-500, GSM8K), the paper reports 2.2–3× token savings and small-to-moderate accuracy gains over several baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "MAGO is described with explicit equations: adaptive objective (Eq. 5), calibration via confidence binning and exponentially decayed historical accuracies (Eqs. 9–13), Pareto frontier maintenance and selection (Eqs. 14–21), and integration into the GRPO-style loss where mMAGO(x) scales control-token terms (Eq. 22).\n\nFigures 4A–C provide interpretable evidence that MAGO mitigates mode collapse and reduces variance relative to vanilla GRPO and DeGRPO during RL, with later, more stable crossovers between modes."}, "weaknesses": {"value": "The text states that MAGO achieves the highest accuracy on AIME, Minerva Algebra, and MATH-500, but Table 1 shows DeepSeek-R1-1.5B surpasses MAGO on Minerva Algebra (0.9577 vs 0.9483), and QMath-1.5B surpasses MAGO on GSM8K (0.8572 vs 0.8469). The abstract and Section 5 claim accuracy improvements of 2.6%–9.4%; the largest improvements visible in Table 1 over strong baselines appear smaller on some tasks (e.g., GSM8K +0.5 points over Thinkless). These discrepancies need reconciliation.\n\nDeGRPO is analyzed qualitatively in Figure 4 but not reported as a quantitative baseline in Table 1. Router baselines include a 7B router, while MAGO uses a 1.5B single model; compute-normalized comparisons (e.g., wall-time, FLOPs/token including routing overheads) are not provided. It is unclear whether all baselines share the same SFT data, RL steps, and inference constraints.\n\nThe contributions highlight logical inference and general problem solving, but experiments focus on math-only datasets. The conclusion acknowledges this limitation; however, the earlier claims of breadth may mislead.\n\nOverall, I think the most important thing is the proposed method still lack of the novelty and the performance is mainly tested on math problem which increases the concerns that whether the proposed method can be generalized to other domains."}, "questions": {"value": "Could you add a quantitative DeGRPO baseline in Table 1 using your training setup, given that MAGO is positioned as an advancement over GRPO/DeGRPO?\n\nExpectations in Eqs. (6)–(8): During training, how many (c,a) samples per x are used to estimate Saccuracy, Sefficiency, and Scalibration? Are these single-sample estimates, and if so, how do you control variance for the correlation matrix Ct (Eq. 16)?\n\nSince the contributions mention logical inference and general problem solving, do you have held-out non-math evaluations (even small-scale) using the same trained model to substantiate cross-domain claims? If not, please limit claims or provide such experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BshWqCpSFD", "forum": "i8vZvBFNJg", "replyto": "i8vZvBFNJg", "signatures": ["ICLR.cc/2026/Conference/Submission7761/Reviewer_XguT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7761/Reviewer_XguT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746172622, "cdate": 1761746172622, "tmdate": 1762919803790, "mdate": 1762919803790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets hybrid thinking, driven by the commonly known fact that reasoning-enabled LLMs often waste compute by using long CoT on easy inputs. With the emergence of deep reasoning models such as DeepSeek-R1, hybrid thinking is a trending topic with prevalent work such as Thinkless. Existing hybrid reasoning methods rely on fixed hyperparameters (e.g., a single controller weight) and single-objective heuristics, which lock the system into narrow, suboptimal regions of the accuracy–efficiency trade-off. \n\nThis paper takes a different approach along this line. The authors propose to reframe hybrid reasoning as multi-objective optimization over three goals: accuracy, efficiency, and decision calibration. They call the framework MAGO. In short, MAGO introduces a dynamic weighting mechanism over those three objectives, learned via Pareto frontier maintenance plus correlation-aware weight selection. Experiments on AIME-24, Minerva Algebra, MATH-500, GSM8K show that MAGO brings good improvements by 2.6% - 9.4%."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- ``S1``:  The paper is very well motivated. The authors use a section (Sect. 2) with pilot experiments to demonstrate their motivation. In particular, it shows that fixed weights cause mode imbalance and dataset-specific sensitivity (Fig. 2A–B), and that scalarization constrains search to a cone-shaped region of objective space (Fig. 2C–D), missing better trade-offs. This strongly motivates the proposed methodology.\n\n- ``S2``: The proposed MAGO makes sense. Experiments on several math reasoning benchmarks show both token savings and accuracy gains."}, "weaknesses": {"value": "- ``W1``: The scope of the evaluated benchmark is a bit limited and narrow (math). Additional experiments on SuperGPQA and CommonsenseQA would strengthen the contribution by showing the generalization of MAGO in extended domains.\n\n- ``W2``: In Tab. 1, the authors are suggested to include the reference for each method.\n\n- ``W3``: Computational overhead is one of my concerns. I notice that the authors have admitted the limitations of extensive overhead by Pareto optimization in the last section. It would be great if the authors could show some computational complexity analysis."}, "questions": {"value": "- ``Q1``: Why choose answer-token max-prob as confidence instead of sequence-level confidence or verifier-based scores?\n- ``Q2``: Does MAGO generalise to other domains rather than math reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l747wfY9vG", "forum": "i8vZvBFNJg", "replyto": "i8vZvBFNJg", "signatures": ["ICLR.cc/2026/Conference/Submission7761/Reviewer_7KZE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7761/Reviewer_7KZE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095508159, "cdate": 1762095508159, "tmdate": 1762919803473, "mdate": 1762919803473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MAGO (Multi-objective Adaptive Generation Optimization), a framework for training hybrid reasoning systems in LLMs that dynamically choose between short (direct) and think (chain-of-thought) modes. The key innovation lies in reformulating the hybrid reasoning optimization as a multi-objective optimization problem balancing three competing objectives: accuracy, efficiency, and decision calibration. Rather than using fixed hyperparameters as in existing methods, MAGO employs Pareto frontier maintenance with correlation-aware weight selection to dynamically adapt optimization weights during training. Evaluation on mathematical reasoning benchmarks (AIME, Minerva Algebra, MATH-500, GSM-8K) shows efficiency improvements of 2.2x-3x while simultaneously improving accuracy by 2.6%-9.4% compared to baseline methods like Thinkless and CoT-Valve."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a genuine limitation in existing hybrid reasoning systems—the dependence on static hyperparameters that vary significantly across datasets. The reformulation as a multi-objective optimization problem is novel for this domain, and the motivation clearly demonstrates why fixed-weight approaches fail (Figure 2)\n\n2. The Pareto frontier maintenance approach is mathematically well-motivated. The use of correlation-aware weight selection (Equation 17) to handle interdependencies between objectives addresses a real challenge in multi-objective optimization. \n\n3. The paper evaluates on multiple mathematical reasoning benchmarks and includes several comparison baselines (router-based methods, model merging, CoT-Valve)"}, "weaknesses": {"value": "1. While accuracy and efficiency are straightforward objectives, the decision calibration objective (Equation 8) lacks rigorous justification. The approach relies on binned historical accuracy statistics, but several concerns arise: (1) How are the bins (Equation 10) initialized during early training when insufficient historical data exists? (2) The exponential decay (Equation 13) may create staleness issues when task distributions shift. (3) The paper doesn't analyze how this objective theoretically prevents miscalibration or what formal guarantees exist. \n\n2. DeepSeek-R1-Distill-Qwen-1.5B. This is a relatively small model, and generalization to larger models (7B, 13B, or larger) remains unclear. Does the Pareto frontier maintenance scale computationally with model size?\n\n3. The router baselines (Router Random, Router Q-7B) seem surprisingly weak—Router Q-7B achieves only 14.8% on AIME while Thinkless achieves 25%. This brings up there might be some possible implementation or configuration issues.\n\n4. The paper mentions diversity filtering (Equation 21) but provides insufficient detail. How quickly does the frontier grow? What happens when |F_t| approaches |F_max|? The exploration mechanism (Equation 19-20) could lead to unbounded frontier growth early in training."}, "questions": {"value": "1. Why is calibration defined as the difference between model confidence and empirical accuracy rather than other common calibration metrics (expected calibration error, Brier score)?\n\n2. Can you evaluate on non-mathematical reasoning tasks (e.g., commonsense reasoning, multi-hop QA, code generation)? The current scope limits the paper's impact.\n\n3. Visualize how the Pareto frontier evolves during training. How does frontier size grow? Do solutions converge or continue diversifying? When does the optimal weight selection stabilize?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "48ir84WGz1", "forum": "i8vZvBFNJg", "replyto": "i8vZvBFNJg", "signatures": ["ICLR.cc/2026/Conference/Submission7761/Reviewer_MZa3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7761/Reviewer_MZa3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152532525, "cdate": 1762152532525, "tmdate": 1762919803130, "mdate": 1762919803130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Multi-Objective Adaptive Generation Optimization (MAGO) framework, designed to address inefficiencies in hybrid reasoning approaches for large language models (LLMs). ​Current methods often rely on static hyperparameters and single-objective optimization, which fail to adapt to varying task complexities and result in suboptimal trade-offs between accuracy, efficiency, and calibration. ​MAGO leverages multi-objective optimization and dynamic adaptive weighting to optimize these three competing objectives simultaneously. ​By maintaining a Pareto frontier and employing correlation-aware optimization, MAGO explores the full trade-off space, avoiding the limitations of fixed-weight approaches. ​The framework eliminates the need for manual hyperparameter tuning and achieves principled decision-making across diverse problem complexities. ​Experimental results on mathematical reasoning benchmarks (AIME, Minerva Algebra, MATH-500, GSM-8K) demonstrate that MAGO improves computational efficiency by 2.2x to 3x while enhancing accuracy by 2.6% to 9.4%. ​The paper also highlights MAGO's ability to prevent mode collapse during training and achieve stable optimization dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a critical limitation in hybrid reasoning systems by reformulating the problem as a multi-objective optimization task, which is a significant advancement over static hyperparameter-based methods.\n\n- The introduction of dynamic weight adaptation and Pareto frontier maintenance is innovative and eliminates the need for manual hyperparameter tuning, making the framework more adaptable to diverse problem complexities.\n\n- The authors provide extensive experimental results on multiple mathematical reasoning benchmarks, demonstrating significant improvements in both accuracy and efficiency compared to existing methods.\n\n- MAGO effectively prevents mode collapse during training, ensuring balanced reasoning mode selection and stable optimization dynamics."}, "weaknesses": {"value": "- The framework is primarily evaluated on mathematical reasoning tasks, which may limit its generalizability to other domains. ​Broader validation across diverse reasoning tasks is necessary to establish its applicability.\n\n- While the training overhead is claimed to be amortized over inference, the paper acknowledges that this may pose challenges for resource-constrained scenarios. ​Further analysis of the trade-offs between training cost and inference efficiency would strengthen the paper.\n\n- The proposed framework involves intricate mechanisms such as Pareto frontier maintenance, correlation-aware weight selection, and dynamic weight adaptation. ​This complexity may hinder practical adoption and require significant expertise for implementation.\n\n- The paper does not provide concrete examples or case studies of how MAGO could be applied in real-world scenarios, which would help demonstrate its practical utility."}, "questions": {"value": "Line 216-218: Why do we take the logits corresponding to the final answer token to compute the raw confidence score instead of all the tokens in the generated answer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WeFMeOgjhs", "forum": "i8vZvBFNJg", "replyto": "i8vZvBFNJg", "signatures": ["ICLR.cc/2026/Conference/Submission7761/Reviewer_VZpK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7761/Reviewer_VZpK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762325397851, "cdate": 1762325397851, "tmdate": 1762919802713, "mdate": 1762919802713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}