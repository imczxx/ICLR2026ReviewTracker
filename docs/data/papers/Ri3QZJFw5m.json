{"id": "Ri3QZJFw5m", "number": 15008, "cdate": 1758246779697, "mdate": 1759897335900, "content": {"title": "Agents Aren't Agents: the Agency, Loyalty and Accountability Problems of AI agents", "abstract": "The rapid adoption of AI agents marks a shift from predictable digital services to systems entrusted with autonomous, judgment-like tasks. As people delegate more responsibility to these agents, questions of control, loyalty, and accountability become urgent. Yet today’s agents are operated through fragmented layers of control by developers, hosts, and providers, which blur lines of responsibility and divide loyalties before users ever interact with them. Without reconsideration, we risk misallocating responsibility, overstating loyalty, and obscuring who ultimately benefits from these systems. In this paper, we systematically discuss key issues that hinder AI agents from attaining true legal agency. We identify three unresolved problems: Agency—who is the principal and who is the agent in the polyadic governance of AI development and deployment; Loyalty—whether AI agents can serve the principal’s best interests; and Accountability—when AI agents make mistakes, who is responsible for them? We examine the technological foundations that give rise to these problems and highlight key limitations of the current agency law framework in addressing emerging issues related to AI agents. As a position paper, our study offers fresh perspectives on AI agents from a legal standpoint and could inspire new research directions in this domain.", "tldr": "AI agents resemble human Agents but lack personhood and undivided loyalty, making agency law an unreliable governance tool.", "keywords": ["AI agents", "agency", "alignment", "fiduciary duties", "large language models", "loyalty", "accountability"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9fb02511c2024c8564ee7063dedc217c6bd34c3b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This position paper contends that today’s so‑called “AI agnts” are not legal Agents and cannot be treated as such without distortion. The authors claim that the analogy to human agency law breaks for three reasons they term the Agency, Loyalty, and Accountability problems.\n\n1st: Agency (who is the principal/agent?) AI systems are governed “polyadically (by trainers, hosts, tool/wrapper develoers, and end‑users). 2nd: Loyalty (can an AI reliably act in the principal’s best interests?) Even absent self‑interest, models routinely display disloyal behavior through instruction‑following brittleness, hallucination, non‑determinism, and provider conflicts of interest. 3rd, Accountability (who is liable when things go wrong?) Traditional mechanisms (e.g., fiduciary liability or respondeat superior) do not map cleanly onto AI systems. \n\nThe paper’s contribution is to re‑center discussions of “AI agents” around legal agency’s core doctrines, showing why doctrinal transpants are dangerous, and to surface gaps that invite new technical and legal research. The work is intentionally diagnostic rather than prescriptive (Abstract, p. 1; Conclusion, p. 9)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear reframing of the “AI agent” metaphor. The paper shows, with diagrams, how today’s systems lack the structural features assumed by agency law (Figure 2, p. 5; Figure 3, p. 5). This helps ML practitioners avoid over‑reliance on legal analogies. \n\n- Useful taxonomy of misconceptions and duties. Table 1 (p. 2) and the summary of fiduciary/accountability principles (Table 2, p. 4) give readers a map of doctrine and where it fails to transfer. \n\n- The discussion of instruction brittleness, hallucination, and stochasticity as “disloyalty” in a fiduciary sense (Sec. 5.1, pp. 6–7) is insightful for an ML audience. \n\n- Candid scope and positioning. The paper states it is a position piece aimed at surfacing issues and stimulating research rather than settling doctrine."}, "weaknesses": {"value": "Major concerns :\nThe analysis leans heavily on the U.S. Restatement and state doctrines (pp. 4–9) and offers little on the EU AI Act’s liability interfaces or product‑liability modernization, nor on civil‑law analogs of agency. \n\nUnder‑argued leap from “AI cannot be Subagents” to “providers should assume 100% responsibility.” Section 4.2 (pp. 6–7) asserts that AI agents cannot be subagents and concludes the only plausible option is full provider responsibility. The normative basis and feasibility are not fully defended. A more detailed allocation model (rebuttable presumptions, strict liability bands by capability/risk) would strengthen the claim. \n\nSection 6.2 (pp. 8–9) argues that respondeat superior is a poor fit largely because models lack personal motives. Courts often ask about foreseeability and scope of assigned tasks; these could, in principle, capture many AI behaviors. Engaging closely with how “scope of employment” could be reinterpreted for technical artifacts, and with edge cases like autonomous prevention/mitigation features, would improve soundness. \n\nThe paper largely stops at “structural mismatch.” Readers would benefit from concrete, technically actionable implications (e.g., logging standards for allocatable causation; loyalty tests/benchmarks; verifiable delegation protocols). The brief references to safety guardrails and provider discretion (pp. 6–7) could be expanded into design patterns. \n\nMinor concerns:\nAmbiguity in the five cases (Figure 3). The mapping of the “Cursor updates your blog” example to Case 5 vs. the airline booking example to Case 4 is terse (p. 5) and may confuse readers about when no agency vs. provider‑as‑principal applies. A small decision tree would help."}, "questions": {"value": "Comparative law: How would your Agency/Loyalty/Accountability framing change under the EU AI Act and proposed Product Liability Directive revisions? Can you sketch how “polyadic governance” interacts with strict liability proposals in the EU?\n\nAllocation model: Instead of 100% provider responsibility (Sec. 4.2), would you endorse a rebuttable‑presumption model that places initial liability on the deployer/provider but allows upstream indemnities conditioned on demonstrable controls?\n\nBenchmarks for “loyalty.” Could the ML community help with loyalty benchmarks (multi‑constraint compliance under safety overrides, conflict‑of‑interest stress tests)? What measurable targets would meaningfully inform legal duties of care ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hfUuwsX4ld", "forum": "Ri3QZJFw5m", "replyto": "Ri3QZJFw5m", "signatures": ["ICLR.cc/2026/Conference/Submission15008/Reviewer_HBMk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15008/Reviewer_HBMk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622699797, "cdate": 1761622699797, "tmdate": 1762925337852, "mdate": 1762925337852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This position paper investigates why current AI agents don’t fit precisely into the legal category of human agents. The paper analyzes three issues: Agency, Loyalty, and Accountability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is interdisciplinary. It bridges AI technical problems and legal theory. This is very relevant for regulators, users, and platform owners. It is also timely as 2025 is called the “year of AI agents”.\nThe paper reveals concrete risks that can guide policy and technical approaches."}, "weaknesses": {"value": "The paper discusses mostly the common law in the united states but it is not clear how this analysis carries over to other regions.\nAlthough the paper investigates the problems clearly, it could benefit from proposing concrete solutions."}, "questions": {"value": "Have you considered any empirical study of commercial agent ToS, logging practices, or reported incidents to illustrate the “avoision” patterns you describe?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "A3ybkSZmEy", "forum": "Ri3QZJFw5m", "replyto": "Ri3QZJFw5m", "signatures": ["ICLR.cc/2026/Conference/Submission15008/Reviewer_AwkX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15008/Reviewer_AwkX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967333227, "cdate": 1761967333227, "tmdate": 1762925337470, "mdate": 1762925337470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper claims that current AI agents shouldn’t be treated as legal agents. Since control is split across trainers, hosts, wrappers, and users, these systems can’t offer undivided loyalty, and accountability is not clear. It frames three core problems: Agency (who’s the\nprincipal/agent in this polyadic setup), Loyalty (model anomalies + provider incentives lead to disloyal behavior), and Accountability (classic doctrines like fiduciary duties do not seem to be applicable)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a relevant problem. It also has a contribution in how it reframes AI agents through a polyadic-governance lens and the\nAgency/Loyalty/Accountability triad, and it states that there is an illegal analogy when considering that today’s systems are anyone’s legal Agent. The paper provides concrete evidence by putting together ML failure modes and provider incentives to specific agency-law doctrines. It’s well written, clear and well-organized."}, "weaknesses": {"value": "The paper does not seem to be mature enough for publication since it does not go much beyond diagnosis. It presents claims about provider conflicts, contractual narrowing, and “divided loyalty”, which are plausible, but discussed just at assertion-level; without any empirical evidence (e.g., a 3-5 platform ToS audit quantifying arbitration clauses, liability caps, training-use terms; a few real and reproducible agent logs or vignettes). Some of the premises are based on human-style “failings” in models; where model deviations should be presumed within scope, placing default liability on providers. The work seems to apply just to US-based scenarios;  and it is not clear to what extent it would be applicable to EU AI Act. The core concept of “polyadic governance” seems to be sound, but it is not clear how to implemented in practice: specify a minimal accountability stack (Authority Manifest, auditable Action Ledger, rebuttable presumptions, and a Loyalty Firewall), as well as concrete metrics (goal-consistency under competing constraints, partner-steering bias, run-to-run variance) and a decision procedure that maps real cases to the categories depicted in Fig. 3."}, "questions": {"value": "No additional questions beyond those outlined in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uUm9eBoAn8", "forum": "Ri3QZJFw5m", "replyto": "Ri3QZJFw5m", "signatures": ["ICLR.cc/2026/Conference/Submission15008/Reviewer_EK6J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15008/Reviewer_EK6J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044812752, "cdate": 1762044812752, "tmdate": 1762925337191, "mdate": 1762925337191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses \"key issues that hinder AI agents from attaining true legal status\". It is unclear to me what the contribution is. It is not clear what the technical hinders are compared to the legal hinders from other types of hinders. At the same time the authors say \"this position paper argue[s] that treating AI systems as if they were human Agents obscures fundamental structural differences...\" I would agree with the second sentiment, but find it unclear how to reconcile with the formulations in the abstract. The conclusions seem to more point in the first direction, than the second."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The question of agency as well as the connection between legal and technical aspects are interesting and relevant.\n\nThe paper covers several relevant aspects to the these problems."}, "weaknesses": {"value": "What is the actual position the paper takes? The more I read the paper, the less clear it seems to me.\n\nIt is unclear what the contributions are. I can imagine a set of technical challenges that need to be addressed or legal questions identified that must be addressed. If it is a position paper, then there needs to be a clear position (which there is) that then acts as a red thread and reaches a clear conclusion or set of arguments for the position (this is missing or unclear).\n\nIt is even unclear whether we really want agents to have legal status.I would have expected a more thorough ethical discussions of this.\n\nSee questions for more issues."}, "questions": {"value": "Why do we want to give agents \"true legal status\"?\n\nIs this mainly a technical problem? Or is it a societal problem related to acceptance (i.e. agents will have legal agency when society accepts this)?\n\nIn the conclusions it is stated that a key problem is that agents operate through \"fragmented layers of control\", how could this be avoided? How is this different from cars or airplanes?\n\nWhen you talk about \"existing legal frameworks\" which ones do you refer to? Are they the same or are there some that are better/worse? Are agents legal entities in any country?\n\nYou call for developing \"new institutional, technical, and legal mechanisms\", are they all equally important?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TRgN6iLOBY", "forum": "Ri3QZJFw5m", "replyto": "Ri3QZJFw5m", "signatures": ["ICLR.cc/2026/Conference/Submission15008/Reviewer_E48R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15008/Reviewer_E48R"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088801628, "cdate": 1762088801628, "tmdate": 1762925336839, "mdate": 1762925336839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}