{"id": "YuxgSGFaqb", "number": 7543, "cdate": 1758026874207, "mdate": 1759897847240, "content": {"title": "SWINGARENA: Adversarial Programming Arena for Long-context GitHub Issue Solving", "abstract": "We present \\textsc{SwingArena}, a adversarial evaluation framework for Large Language Models (LLMs) that closely mirrors real-world software development workflows. Unlike traditional static benchmarks, \\textsc{SwingArena} models the collaborative process of software iteration by pairing LLMs as \\textit{submitters}, who generate patches, and \\textit{reviewers}, who create test cases and verify the patches through continuous integration (CI) pipelines. To support these interactive evaluations, we introduce a retrieval-augmented code generation (RACG) module that efficiently handles long-context challenges by providing syntactically and semantically relevant code snippets from large codebases, supporting multiple programming languages (C++, Python, Rust, and Go). This enables the framework to scale across diverse tasks and contexts while respecting token limitations. Our experiments, using over 400 high-quality real-world GitHub issues selected from a pool of 2,300 issues, show that models like GPT-4o excel at aggressive patch generation, whereas DeepSeek and Gemini prioritize correctness in CI validation. \\textsc{SwingArena} presents a scalable and extensible methodology for evaluating LLMs in realistic, CI-driven software development settings.\\footnote{The complete codebase and benchmark are submitted in \\href{https://anonymous.4open.science/r/Swing-Bench}{this link} and will be open-sourced after the anonymity period.}", "tldr": "", "keywords": ["Arena", "Real-World GitHub Issues", "Adversarial Programming", "Retrieval-Augmented Generation", "Continuous Integration", "Code Benchmark"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/71704038d4e476dd6e083fbf5ed8858881e0fe5f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SWINGARENA, a CI-faithful, adversarial code-evaluation arena where an LLM “Submitter” proposes patches and an LLM (or human) “Reviewer” writes tests to break them; the roles can switch across rounds. It ships (i) a rigorously curated multi-language dataset with runnable CI, (ii) a Retrieval-Augmented Code Generation (RACG) pipeline to find relevant files and synthesize fixes, and (iii) arena metrics (e.g., Win Rate, SPR/RPR) to quantify progress. Experiments on real repositories show the arena surfaces harder, more realistic failure modes than static benchmarks and that better retrieval/testing policies materially improve pass rates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The adversarial submitter-reviewer paradigm with role-switching is creative and mirrors real-world collaboration better than static benchmarks.\n2. Rigorous data curation: Three-stage filtering with human expert validation is commendable.\n3. Well-oraganized writing, easy to understand."}, "weaknesses": {"value": "1) **Limited novelty / “integration over invention.”**  \n   The work skillfully engineers an end-to-end pipeline for realistic SE evaluation, but core techniques (retrieval, ranking, CI emulation, multi-turn prompting) largely reuse known components. \n2) **Under-granular ablations for RACG.**  \n   Current ablations are mostly on/off toggles or coarse retrieval settings, leaving it unclear which sub-module drives gains.\n   - **Actionable:** Provide per-component ablations on (i) chunk size & overlap, (ii) reranker family (bi-encoder vs. cross-encoder), (iii) proximity/structure priors (same-dir, same-package), (iv) adaptive Top-k vs. fixed, (v) failure-triggered expansion rules; add mediation analysis to quantify each component’s indirect effect on final win rate.\n3) **Language & ecosystem coverage is narrow.**  \n   Results cover C++/Python/Rust/Go but omit **Java/JS/TS** and build systems (**Maven/Gradle**, **pnpm/yarn/monorepo**). This limits external validity for common enterprise stacks."}, "questions": {"value": "1) **Causal attribution:** How do you isolate the gain from the *adversarial, role-switching* protocol itself (vs. model scale, retrieval strength, prompt length)? Can you run controlled A/Bs holding RACG constant while removing role-switching, and report mediation analysis?\n\n2) **Real-world parity:** How closely do local CI runs match upstream (e.g., GitHub Actions) on pass/fail and runtime? Please provide a paired evaluation with agreement statistics and discuss observed drift.\n\n3) **Gaming & robustness:** Are there “score-hacking” strategies (e.g., submitter creates trivially detectable but non-critical faults; reviewer overfits to diffs)? What constraints or equilibrium analyses prevent metric inflation?\n\n4) **Scaling laws:** As model size, number of rounds, and repo size (files, dependency depth) grow, do we observe consistent scaling behavior? Which has higher marginal return: **more rounds** or **stronger/adaptive retrieval**?\n\n5) **Granular failure taxonomy:** What are the dominant failure modes per language (retrieval miss, build failure, semantic error)? Please add an error decomposition table linking failure modes to specific RACG sub-modules.\n\n6) **Cross-ecosystem generalization:** What minimal changes are needed to support Java (Maven/Gradle) and JS/TS monorepos? Any zero-/few-shot transfer experiments showing the framework’s portability?\n\n7) **Contamination audits:** How did you detect/prevent training–evaluation overlap for both LLMs and the reranker? \n\n8) **Metric resolution:** Beyond overall win rate, can you report **defect-type × difficulty** strata (and CIs) to demonstrate the framework discriminates hard cases rather than being dominated by easy issues?\n\n9) **Adaptive retrieval policy:** Can RACG expand beyond fixed Top-k after early failures? Please report the success-latency trade-off and whether adaptive policies change conclusions.\n\n10) **Cost & reproducibility:** What is the typical wall-clock time and token usage per match under your default settings? Could you provide scripts and a “minimal slice” that reproduces headline results on modest compute?\n\n> If the above analyses (especially #1, #2, #4, #7) are addressed, I would likely raise my overall score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "goqFdrsPbj", "forum": "YuxgSGFaqb", "replyto": "YuxgSGFaqb", "signatures": ["ICLR.cc/2026/Conference/Submission7543/Reviewer_hYHi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7543/Reviewer_hYHi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633435884, "cdate": 1761633435884, "tmdate": 1762919641933, "mdate": 1762919641933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an adversarial evaluation framework for LLMs in software engineering, arguing that existing benchmarks like HumanEval are too simplistic by focusing only on single-function unit tests. SwingArena aims to model real-world workflows by using actual long-context GitHub issues, executing generated patches against the full CI pipeline, and implementing an adversarial battle protocol. In this protocol, one LLM acts as a Submitter generating a patch, while another acts as a Reviewer generating new tests specifically to break the submitted patch. The authors find that evaluating models in this dynamic, CI-driven arena reveals nuanced model \"personalities\" (e.g., \"aggressive patchers\" v.s. \"reliable coders\") and surfaces important failure modes (like cross-file consistency errors) that static benchmarks overlook."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-motivated, addressing a clear gap in existing research. It convincingly argues that the field must move beyond simple unit test success and instead evaluate models on their ability to produce code that is \"valid, compliant, and able to pass a full CI pipeline and peer review.\"\n- A core contribution is the new dataset of over 2300 real-world GitHub issues across four languages (C++, Python, Rust, Go). Each problem is CI-grounded, meaning the original human solution was verified to pass the full CI pipeline, ensuring a high-quality, realistic testbed.\n- The adversarial framework successfully reveals behavioral tendencies that static tests can't. For example, the paper found that GPT-4o acts as an \"aggressive patcher\" (achieving high win rates), whereas DeepSeek and Gemini \"prioritize correctness and CI stability\" (scoring higher on CI pass rates)."}, "weaknesses": {"value": "- It's hard to interpret the primary \"Win Rate\" metric given its adversarial nature. A model's success depends on the relative weakness of its opponent (the reviewer model). This makes it difficult to assess the absolute quality and correctness of a solution based on this metric alone."}, "questions": {"value": "- Continuous Integration pipelines can be computationally expensive to run repeatedly. Given that the reviewer agent analyzes the submitter's patch and generates targeted tests, could this reviewer component potentially be leveraged to reduce the cost of CI runs needed in an evaluation? For instance, could the reviewer's analysis provide a strong signal for early rejection of clearly incorrect patches before running the full CI, or could it intelligently select a subset of critical tests to run instead of the entire suite?\n- You mentioned that the quality gates for the reviewer-generated tests are crucial for preventing exploitative behavior and ensuring test validity. Could you provide more detail on how strictly these were enforced during the evaluations? For example, what was the approximate rejection rate for reviewer-generated tests that failed these gates, and what were the common reasons for rejection (e.g., failing against the golden patch, modifying production code, style violations)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SVD31lxNHq", "forum": "YuxgSGFaqb", "replyto": "YuxgSGFaqb", "signatures": ["ICLR.cc/2026/Conference/Submission7543/Reviewer_kJ84"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7543/Reviewer_kJ84"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760581304, "cdate": 1761760581304, "tmdate": 1762919641478, "mdate": 1762919641478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SWINGARENA, a framework for evaluating LLMs on software development tasks. Unlike static benchmarks, SWINGARENA simulates a collaborative workflow by pairing LLMs into \"submitter\" (patch generator) and \"reviewer\" (test case generator) roles. The evaluation is grounded in real-world GitHub issues and utilizes repository-native CI pipelines for verification. To manage the long-context nature of large codebases, the framework includes an RACG module."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "present a new dataset of 2,300 CI-filtered issues across C++, Python, Rust, and Go, and provide experimental results for several proprietary and open-source models."}, "weaknesses": {"value": "1. The paper's main \"battle\" metric, the Win Rate, is severely confounded. The \"Win Rate\" is defined as the submitter's patch passing all CI checks, including the reviewer's generated test. As the authors correctly note, \"higher values may also indicate weaker reviewer tests\". This confounding variable makes it impossible to draw clear conclusions about a submitter's absolute capabilities.\n\n2. The paper introduces a complex, multi-stage RACG module but explicitly states it is a \"baseline rather than a standalone algorithmic contribution\". The ablation study in Table 3  fails to justify its necessity."}, "questions": {"value": "1. The results for GPT-4o seem contradictory. The text claims it has \"relatively lower RPR/SPR scores\", but also a \"dominance in producing adversarially-strong patches\" based on high win rates ($\\ge0.90$). Why a model have a low SPR but a high Win Rate?\n\n2. Given that the \"Win Rate\" metric is confounded by the reviewer's strength, would it not be more sound to evaluate the submitter's patch directly against the golden patch and the full, human-written test suite?\n\n3. In the RACG ablation (Table 3), what does the \"Top-k Related\" retrieval baseline consist of?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P32WEUQtCL", "forum": "YuxgSGFaqb", "replyto": "YuxgSGFaqb", "signatures": ["ICLR.cc/2026/Conference/Submission7543/Reviewer_pKUf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7543/Reviewer_pKUf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925180935, "cdate": 1761925180935, "tmdate": 1762919640869, "mdate": 1762919640869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SWINGARENA, a dynamic adversarial evaluation framework for real software engineering tasks that pairs LLMs as submitters and reviewers who generate and test patches, respectively, through a continuous integration (CI) pipeline. SWINGARENA also leverages retrieval augmentation to retrieve the most relevant context from code bases for a variety of languages (C++, Python, Rust, and Go), spanning 400 issues and surfacing new problems, and also showing behavioral differences in models as patch generators and validators."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces iterative and adversarial evaluation that incorporates software engineering in CI development scenarios and goes beyond mere unit tests.\n2. Propose a multi-language long context retrieval (RACG) pipeline for fetching relevant code context that combines syntax-aware chunking, dense reranking, and token-budget–aware packing across C++, Python, Rust, and Go.\n3. A dataset of 2300 real GitHub issues with 400 high-quality issues (100 per language) selected for evaluation.\n4. Benchmarking of several state-of-the-art open and closed-source LLMs."}, "weaknesses": {"value": "1. The win rates of all models are very close to each other (almost every model gets 0.9 or above), which makes me question the utility of this benchmark in terms of model selection.\n2. Best@k values for all the models are also very close to each other, which makes it hard to judge which model is better. \n3. Retriever doesn’t seem to boost performance much in Table 3, especially for Best@3 for Python and C++. The authors also acknowledge a weakness in how many relevant files can be included (only 5 files) and that context retrieval leads to the most failures (26% according to Appendix C1). This makes the benchmark less reliable since the LLMs cannot perform optimally under these limitations. \n4. Missing citations for some relevant work, like CodeRAGBench [1], CrossCodeEval [2], and RepoCoder [3].\n\n[1] Wang, Zora Zhiruo, et al. \"Coderag-bench: Can retrieval augment code generation?.\" arXiv preprint arXiv:2406.14497 (2024).  \n[2] Ding, Yangruibo, et al. \"Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion.\" Advances in Neural Information Processing Systems 36 (2023): 46701-46723.  \n[3] Zhang, Fengji, et al. \"Repocoder: Repository-level code completion through iterative retrieval and generation.\" arXiv preprint arXiv:2303.12570 (2023)."}, "questions": {"value": "What does “PK-style dual-role evaluation” on line 78 mean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NtIn6ccdi7", "forum": "YuxgSGFaqb", "replyto": "YuxgSGFaqb", "signatures": ["ICLR.cc/2026/Conference/Submission7543/Reviewer_cSPp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7543/Reviewer_cSPp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962804026, "cdate": 1761962804026, "tmdate": 1762919640131, "mdate": 1762919640131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}