{"id": "V69RVARwJY", "number": 21329, "cdate": 1758316295406, "mdate": 1759896928082, "content": {"title": "RoCCo: Rotation-Augmented Clustering-based Low-rank Approximation for Compressing Large Language Models", "abstract": "The immense size and computational cost of Large Language Models (LLMs) present significant barriers to their widespread deployment. Low-Rank Approximation (LRA) offers a promising, hardware-friendly solution by factorizing large weight matrices into more compact forms. A key insight is that the accuracy of this factorization can be significantly enhanced by first applying a geometric transformation to the model's weights. In this work, we introduce RoCCo (Rotation-augmented Clustering for Compression), a novel LRA framework that uses clustering to factorize weight matrices. We first apply an orthogonal transform to shape restructure the weight geometry to be more suitable to clustering. We then apply a group-wise clustering algorithm to the transformed weights to achieve a precise approximation. Furthermore, we demonstrate that this factorized representation enables a novel clustered attention mechanism, which reduces the algorithmic complexity of inference by performing attention computations directly in the compressed domain. Through experiments on the LLaMA and OPT model families, we show that RoCCo can compress models by 75\\% while retaining over 96\\% of the original zero-shot accuracy on LLaMA2-13B achieving a competitive compression-accuracy trade-off.", "tldr": "", "keywords": ["large language models", "compression", "low-rank approximation", "clustering"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f84c5bb56ce2b4532016cb4c8ef5466e994e7a1b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the RoCCo (Rotation-augmented Clustering for Compression) technique for effectively compressing LLMs. The proposed technique aims to improve the accuracy of the compressed model using the Randomized Hadamard Transformation (RHT), which has been frequently discussed in recent quantization studies like QuIP# and QuaRot. The authors use Llama2 and OPT models to show the superiority of RoCCo."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "RoCCo's approach to applying RHT for low-rank approximation is reasonable. Furthermore, RoCCo shows a significant performance gap with competitors by demonstrating its high accuracy even at very high compression ratios."}, "weaknesses": {"value": "First, there are too many formatting errors throughout the paper. Table 2 extends beyond the page margins, the text in Figures 2 and 3 is difficult to read, and it is unclear why Section 5 is placed after the Experiments section.\nFurthermore, all experiments are conducted on OPT and LLaMA2, which are very old models that are no longer in common use, so experiments on more recent models are necessary. Additionally, results from other benchmarks, such as GSM8K, are also needed."}, "questions": {"value": "1. Could you provide experimental results for various models beyond Llama2, such as Llama 3 or Qwen?\n2. Could you provide experimental results for GSM8K or other considerable benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AxXqWBiIPQ", "forum": "V69RVARwJY", "replyto": "V69RVARwJY", "signatures": ["ICLR.cc/2026/Conference/Submission21329/Reviewer_gH2n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21329/Reviewer_gH2n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751371876, "cdate": 1761751371876, "tmdate": 1762941699437, "mdate": 1762941699437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose ROCCO, a compression pipeline for large language models consisting of three main steps: weight preconditioning using a random Hadamard transform to improve clustering, a divide-and-cluster procedure that finds representative cluster vectors in each submatrix, and a GPTQ-like second-order optimization step to minimize the layer output reconstruction error.\n\nThe approach is compared against several existing low-rank approximation methods. Additionally, the paper introduces an efficient attention mechanism compatible with rocco-compressed layers to further speed up inference."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Strong empirical results compared to presented baselines.\n- Creative adaptation of GPTQ-style calibration for low-rank approximation rather than quantization error.\n- The “Efficient Attention Mechanism” section presents a practical improvement for faster computation in ROCCO-compressed attention layers."}, "weaknesses": {"value": "- The novelty is limited and somewhat unclear.\n- There is no comparison of inference speed with other low-rank methods.\n- Compression time is not reported — important given the potential slowness of k-means clustering.\n- The impact of the Hadamard transform beyond reconstruction error is not clearly justified; the consern \nis that GPTQ-calibration you perform after clusterization makes the Hadamard preconditioning unnecessary. \n- Some minor editing and clarity issues remain throughout the text.\n\nOverall, the paper presents convincing empirical results on solid baselines. However, the contribution’s originality is ambiguous. It is unclear whether the clustering-based low-rank approximation itself is novel or if the authors mainly adapt existing methods. The necessity and contribution of the Hadamard preconditioning step are also questionable when GPTQ-style calibration is applied afterward."}, "questions": {"value": "- Can you provide comparisons with clustering-based low-rank approximation + GPTQ centroid correction (without Hadamard transform) as an ablation study to isolate the effect of the rotation?\n- Please clarify how LayerNorm and residual connections are handled when absorbing orthogonal matrices into adjacent weight matrices (lines 137–139).\n- You do not cite prior clustering-based low-rank approximation methods, yet you also do not explicitly claim it as your contribution. Please clarify this point.\n- Why do you omit ASVD and FWSVD from your comparisons, despite mentioning them in the related work?\n- Provide inference time comparisons with other low-rank methods. As it stands, Tables 1 and 2 seem to exclude the efficient attention mechanism from Section 5, and Section 3.3 only provides complexity analysis relative to dense models.\n- Please report the average compression time for ROCCO.\n- Do you apply ROCCO to all linear layers in the model?\n- The dense baseline should be visually separated from the compressed variants in the tables for readability.\n- Please specify the dataset and metric used in Table 3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hIQoTjLbQk", "forum": "V69RVARwJY", "replyto": "V69RVARwJY", "signatures": ["ICLR.cc/2026/Conference/Submission21329/Reviewer_ovGL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21329/Reviewer_ovGL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991898936, "cdate": 1761991898936, "tmdate": 1762941698994, "mdate": 1762941698994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a compression method for LLMs that combines orthogonal weight transformation via a randomized Hadamard transformation, clustering-based low-rank factorization, and clustered-attention computation for inference speedups. The goal of this work is to reduce model size without major accuracy loss, achieving high compression ratios while maintaining strong zero-shot performance.\n\nThe key idea is that LLM weight matrices contain vector-level outliers that distort clustering. The proposed method, RoCCo, first applies a random Hadamard transform to weight matrices to smooth the weight geometry (i.e., reduce vector-level outlier weights) to make the cluster centers more homogeneous. Subsequently, the weight matrix rows are split into groups, and k-means clustering per group is performed to get centroids and assignment indices. Then, the centroids are refined using a Hessian-based update to reduce layer output error. Optionally, core attention compute can be done directly in the centroid space, reducing FLOPs.\n\nExperiments on Llama and OPT models show that it outperforms prior approaches on standard zero-shot accuracy evaluations and Wikitext-2 PPL. Furthermore, low-rank approximation can be combined with GPTQ to produce usable models even in extremely low-bit settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper studies the important problem of LLM compressing, focusing on structural approaches like low-rank approximation. The insight behind the paper is well-motivated -- orthogonal transformations indeed reduce the vector-level outliers that distort clustering, as can be seen in Fig. 1.\n\nAmongst the numerical evaluations done in this paper, RoCCo preserves accuracy on Llama2 and OPT models, demonstrating practical significance. Furthermore, by showing that RoCCo integrates cleanly with GPTQ and enables usable models at $\\sim 2$ bits effective precision (where GPTQ alone fails), the paper establishes orthogonality to quantization while highlighting a potential compression strategy that utilizes the best of both world.\n\nFinally, the clustered-attention mechanism extends the compression framework to algorithmic speed-ups, reducing FLOPs on small model. While not the main driver of results, it shows potential for deployment efficiency."}, "weaknesses": {"value": "One of the major concerns I have is that the numerical evaluations are pretty limited (i.e., only older Llama-2 and OPT models have been evaluated). It is well known that more recent models like the Llama-3 family, Qwen, etc. are less amenable to quantization, and results of the newer models which are more commonly used nowadays is important to convince the usefulness of the approach.\n\nMoreover, existing works seems to already use Hadamard transformation with low-rank approximation + quantization. For example, in [this paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/a20e8451ffb07ad25282c21945ad4f19-Paper-Conference.pdf), random Hadamard transform is applied to the left and right of the input weight matrix as the incoherence pre-processing step and it helps both quantization (using QuIP#) and low-rank approximation. Discussions of howRoCCO compares with prior works such as this should be included.\n\nFinally, even though the authors show how clustered-attention can lead to FLOPs reduction, throughput numbers (e.g., time-to-first-token or inter-token-latency) are not reported. Often, FLOP reduction does not lead to a latency reduction, especially in the memory-bound auto-regressive decoding phase, where most of the time is spend in data transfers and compute units stay idle. If possible, results  with actual numbers should be included, and if not this limitation should be explicitly acknowledged."}, "questions": {"value": "1. The authors say in line 088, *Unlike element-wise methods that can produce sparse matrix representations, LRA-based methods typically result in smaller, dense matrices, which have high efficiency on modern hardware like GPUs.* This is not always true. Recent accelerators (e.g., TPUs) maximize compute utilization when the matrices are large. Reducing the size of the matrices just leads to wasted compute (low utilization), with potential accuracy loss.\n\n2.   Line 103. ModeGPT .... is incomplete.\n\n3. It is stated in line 151, *While the weights contain significant structural redundancy due to the structured computational graph of transformers (eg. Multi-head self-attention) (Vaswani et al., 2017), their distributions are often characterized by the existence of outliers.* This seems like a hand-wavy justification -- please provide rigorous justification if possible. If not, simply the empirical observation that weight matrices are low-rank should suffice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IvckImCb8n", "forum": "V69RVARwJY", "replyto": "V69RVARwJY", "signatures": ["ICLR.cc/2026/Conference/Submission21329/Reviewer_taHP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21329/Reviewer_taHP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762217159544, "cdate": 1762217159544, "tmdate": 1762941698625, "mdate": 1762941698625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a large language model (LLM) compression algorithm that integrates rotational preconditioning with clustering-based optimization. On small- and medium-scale models, the method achieves state-of-the-art compression performance, significantly outperforming traditional matrix decomposition–based approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe proposed method outperforms other structured pruning approaches by a large margin, as demonstrated in Table 2.\n2.\tRotational transformations, previously shown to be effective in quantization, are further validated in this paper to be equally beneficial for pruning, highlighting their general applicability across different compression paradigms."}, "weaknesses": {"value": "1.\tThe quantitative analysis in the ablation studies is insufficient; it remains unclear how each component—rotation and clustering—contributes individually to the final performance.\n2.\tThe experimental evaluation lacks large-scale model exploration, limiting the understanding of the method’s scalability.\n3.\tThe inference efficiency of the compressed model is not analyzed, leaving its practical runtime benefits uncertain.\n4.\tThe comparison of compression time across different methods is missing, making it difficult to assess the computational cost of the proposed approach."}, "questions": {"value": "Could you provide additional experiments to address the weaknesses mentioned above?\nAlso, can your clustering method be formulated as a type of decomposition approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7ZwjXgVMa5", "forum": "V69RVARwJY", "replyto": "V69RVARwJY", "signatures": ["ICLR.cc/2026/Conference/Submission21329/Reviewer_hf7x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21329/Reviewer_hf7x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762231239329, "cdate": 1762231239329, "tmdate": 1762941698338, "mdate": 1762941698338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RoCCo, a post-training model compression method. The core technique involves using a fast Walsh–Hadamard transform (FWHT) to remove outliers in weight principal component analysis (PCA). Subsequently, a clustering method like K-means replaces weight columns with their corresponding cluster centroids. The paper also proposes a novel approach to bypass the reconstruction of the large weight matrix, enabling direct attention calculation using the grouped clusters and activations."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents an interesting observation on how the rotation (specifically, the randomized Hadamard transform - RHT) aids clustering for weight compression.\n- It achieves an impressive performance increase in zero-shot accuracy, even with a 75% compression rate."}, "weaknesses": {"value": "- The effectiveness of the rotation is only empirically observed, lacking mathematical proof. The authors do not convincingly demonstrate the benefit of applying an RHT to the weight matrix, especially since the resulting PCA figure appears scattered.\n- The method's concept of approximating the original weights with smaller storage without reconstruction is analogous to quantization. Therefore, the paper should be compared against quantization methods, not just other post-training compression techniques."}, "questions": {"value": "- What is the actual speedup of this method? While the fast RHT is noted as $O(n \\log n)$, how does the calibration training speed compare to other post-training compression methods?\n- Since other compression methods can be combined with quantization, how does RoCCo's performance compare to these combined approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DJEuaBSgh3", "forum": "V69RVARwJY", "replyto": "V69RVARwJY", "signatures": ["ICLR.cc/2026/Conference/Submission21329/Reviewer_9jdF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21329/Reviewer_9jdF"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762483592091, "cdate": 1762483592091, "tmdate": 1762941697947, "mdate": 1762941697947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}