{"id": "VG5iE3rzLz", "number": 15887, "cdate": 1758256623596, "mdate": 1759897275320, "content": {"title": "ReGuidance: Diffusion Steering with Strong Latent Initializations Solves Hard Inverse Problems", "abstract": "In recent years there has been a flurry of activity around using pretrained diffusion models as informed data priors for solving inverse problems, and more generally around steering these models towards certain reward models. Training-free methods like gradient guidance have offered simple, flexible approaches for these tasks, but when the reward is not informative enough, e.g., in inverse problems with highly compressive measurements, these techniques can veer off the data manifold, failing to produce realistic data samples. To address this challenge, we devise a simple algorithm, ReGuidance, that leverages prior methods' solutions as strong initializations and substantially enhancing their realism. Given a candidate solution $x$ produced by a given method, we propose inverting the solution by running the unconditional probability flow ODE in reverse starting from $x$, and then using the resulting latent as an initialization for a simple instantiation of diffusion guidance. \nIn toy settings, we provide theoretical justification for why this technique boosts the reward and brings $x$ closer to the data manifold. Empirically, we evaluate our algorithm on difficult image restoration tasks including large box inpainting, heavily downscaled superresolution, and high noise deblurring with both linear and nonlinear blurring operations. We find that, using a wide range of baseline methods as initializations, applying our method results in much stronger samples with better realism and measurement consistency.", "tldr": "We show that using strong noise initializations alongside diffusion guidance can provably and experimentally solve fundamentally hard reward guidance problems.", "keywords": ["diffusion", "guidance", "steering", "initializations"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e99d163374448e522657158283da8fb080e36f05.pdf", "supplementary_material": "/attachment/2f3681cf6188a93b5ccd8b8991dc43fba8a3003a.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes a training-free algorithm for diffusion-based inverse problem solver, with the key idea that inverse an initial recovery to latent representation through probability flow ODE, and then adopt the DPS from this initialization. Experiments show the effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The author provide theoretical analysis of ReGuidance in improving reward and realism in Gaussian mixture toy setting;\n2. Experiments show improvements over baselines on ImageNet;"}, "weaknesses": {"value": "1. The proofs are restricted to mixtures of Gaussians and linear inverse problems and far from real-world settings;\n2. Only evaluated with ImageNet DDPM, and the evaluation set is quite small (100 samples);\n3. There is no ablations on guidance strength, ODE steps, or computation trade-off;\n4. The baselines are old, it is suggested to add some new strong baselines;"}, "questions": {"value": "1. It is unclear whether the reverse ODE process is stable or sensitive to score errors in used diffusion models, also, how sensitive are the improvements to the initial reconstruction quality, Does ReGuidance fail if the ODE reverse results is far from the manifold?\n2. How does runtime scale with diffusion steps compared to DPS/DAPS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PUAHTfqevs", "forum": "VG5iE3rzLz", "replyto": "VG5iE3rzLz", "signatures": ["ICLR.cc/2026/Conference/Submission15887/Reviewer_EPZz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15887/Reviewer_EPZz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761496509570, "cdate": 1761496509570, "tmdate": 1762926103681, "mdate": 1762926103681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an algorithm called REGUIDANCE that improves the solution of existing diffusion-based plug-and-play inverse problem solvers. Specifically, given a solution $x$ of an existing diffusion-based inverse problem solving algorithm, REGUIDANCE proposes to run the probability flow ODE in reverse, i.e., starting from the clean image $x$ to generate an intermediate latent $x_T$ at time $T$ and then use this $x_T$ as an initialization to run the DPS method using the ODE formulation, i.e., DPS-ODE. Considering a standard toy case, the paper conducts a theoretical analysis and provides bounds on certain quantities, which were useful in inferring about the realism and measurement consistency of the solution returned by the REGUIDANCE procedure. Quantitative metrics such as LPIPS have been shown to improve when REGUIDANCE is applied on top of existing algorithms."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed procedure, called REGUIDANCE, seems simple yet very effective in improving the solution returned by existing inverse problem solvers.  The approach is also modular, which implies that the procedure can be used on top of any existing solvers, which widens its impact. \n\nIn some toy settings, the claims about the solutions returned by REGUIDANCE are supported by a thorough analysis, which adds validity to the proposed method and indicates a principled nature of the approach.\n\nMost inverse problem solvers can perform well for simple inverse problems, but they do really struggle with “hard inverse problems,” and yet this hasn’t been paid much attention to in the literature. I strongly agree and appreciate the authors’ choice of using “hard datasets” like ImageNet and CIFAR rather than FFHQ, etc., and also “hard inverse problems” such as extreme mask inpainting and extremely downsized super-resolution, etc. Improvements on such hard problems showcase the true effectiveness of REGUIDANCE in practice.\n\nThough minor concerns exist, I find the paper well written, easy to read, and organized quite well. I find the roadmap in the appendix very helpful, along with the informal remarks and the simplified explanations in the main text and appendix. I appreciate the authors’ thoughtfulness in this regard."}, "weaknesses": {"value": "Though the paper presents an interesting find, I believe it has the following weaknesses regarding the plausibility of conclusions drawn from the theoretical analysis, and about where the actual effectiveness of the method stems from. These concerns further highlight the need for more experiments and ablations (some suggested below), which I believe are critical to address before the paper can be recommended for acceptance.\n\nQ1. Remark 1 (line 329) mentions that REGUIDANCE improves the posterior likelihood. But prior works such as DMAP[1] have shown that the DPS procedure itself encourages MAP solutions, i.e., boosts posterior likelihood. In light of this fact, I believe the true effectiveness of REGUIDANCE arises from the fact that the DPS-ODE component of REGUIDANCE encourages MAP solution by default, i.e., irrespective of initialization. This might also explain why it has to be DPS-ODE and not some other posterior sampling algorithm?  Also, from remark 1, if  REGUIDANCE’s effectiveness is because it improves the posterior likelihood, then can one also expect it to perform well if DPS-ODE is replaced with other MAP solvers? I understand that the authors seemed to be completely unaware of the work DMAP[1], as it hasn’t been cited in the paper. From the above perspective, I find REGUIDANCE less novel than posed in the paper (of course, this doesn’t overshadow the other contributions of the paper). Still, I see REGUIDANCE as yet another improvement of DPS (like DMAP[1]) for MAP estimation. However, the most novel aspect of REGUIDANCE is that this improvement is based on good initializations and no procedural changes to DPS, unlike DMAP[1], which alters the DPS procedure slightly. I would ask the authors to clarify and justify their case if something is different from my understanding above. \n\nQ2. The paper only considers posterior sampling algorithms such as DDRM, DPS, and DAPS, but fails to consider MAP solvers such as DMPlug[2], MAP-GA[3], ProjDiff[4], etc. Again, from remark 1, if the ultimate reason behind REGUIDANCE’s effectiveness is that it returns high-likelihood sample of the posterior $p(x|y)$, then it should absolutely be (1) thoroughly compared to existing MAP-based methods, such as checking whether REGUIDANCE+DDRM/DPS/DAPS solutions perform comparably to the solutions returned by MAP methods, and (2) if (1) holds, then it should be checked how much improvement REGUIDANCE+MAP solver offers over the vanilla MAP solver solution.\n\n\nQ3. Concerning the point above, I understand that MAP solvers can be computationally expensive than posterior sampling; however, the REGUIDANCE procedure also seems to be quite expensive as it first needs to run the original posterior sampler, then run PF ODE to generate the latents, and finally run DPS-ODE. However, no mention of the computational efficiency of REGUIDANCE was discussed. Also, no mention of NFEs, the noise schedules, and other hyperparameters for REGUIDANCE and baselines was made in the whole paper. A study on how the performance of REGUIDANCE depends on the hyperparameters of DPS, especially such as $\\rho$, and NFEs, noise schedule, etc., is crucial and quite essential for empirical validity in my opinion. \n\nQ4. Regarding Theorem 1 and Theorem 3, I find it difficult to understand the extent to which the conclusions apply beyond toy cases to the real case of Image inpainting. Especially, Theorem 3 holds if the initial solution $x$ is “sufficiently” close to the mode $z_1$? This is a very unrealistic assumption, for (1) this assumption clearly doesn’t hold in the case of the initial solutions returned by DDRM/DAPS/DPS for image inpainting. (2) If the initial solution itself satisfies measurement and is closer to a mode, i.e., a highly likely sample of the posterior, then REGUIDANCE might further push the solution closer to the mode, but one would expect the returned solution of REGUIDANCE to be perceptually similar to the initial solution, which may result in marginal improvements of LPIPS. I’d recommend an empirical verification of this if the authors would consider MAP solvers, as mentioned in point 2 above. \n\n\nQ5. Another crucial aspect of Theorems 1 and 3 concerning why initializing (for DPS-ODE) at a higher $T$ is better:  Theorem 3 bounds say, if $T$ is large, then $x_{T}^{DPS}$ has more realism, (but from Theorem 1 bound, if $T$ is large, $x_{T}^{DPS}$ has low reconstruction error, but it also says that the unobserved pixels remain closer to the initial solution $x$.) this clearly implies that it is heavily based on the fact that $x$ is already very close to a realistic sample. Since $x$ is not already a realistic sample in practice, I find the argument for a higher $T$ quite unconvincing. In my opinion, this aspect, like point 4 above, needs an empirical validation by considering intermediate time initializations (with the same NFEs, however) and not just max $T$ initialization. \n\n\nQ6. With intermediate initialization in point 5 above, I strongly recommend an ablation to check if REGUIDANCE can still improve the initial solution if the latent is generated with SDE instead of PF-ODE. I believe this aspect of why only the PF-ODE-generated latent has to be used is not discussed in the paper (i.e., why not SDE-generated latent + DPS-ODE with the same NFEs, but with intermediate time initializations, because if $T$ is large, since the latent becomes random, REGUIDANCE essentially becomes DPS).\n\n\n\n[1] Xu et al. Rethinking Diffusion Posterior Sampling: From Conditional Score Estimator to Maximizing a Posterior \n[2] Wang et al. DMPlug: A Plug-in Method for Solving Inverse Problems with Diffusion Models \n[3] Gutha et al. Inverse Problems with Diffusion Models: A MAP Estimation Perspective \n[4] Zhang et al. Unleashing the Denoising Capability of Diffusion Prior for Solving Inverse Problems \n \n\nMinor weaknesses/clarifications: \n\nQ7. The notation of $x_t$ can be made more clear. In Sec 2.1, $t$ always goes from 0 to T. The left and right arrows clarify whether we talk about the reverse or the forward process, but in some later parts of the text, such as line 203 (one would think $x_{0}^{DPS}$ to be a clean image) and line 290 (one would think $t=0$ in $v_{t}^{DPS}$ is at the start of the reverse process because of earlier notation in line 203). Although I find the paper very interesting, the notation inconsistencies make it difficult to read.\n\n\nQ8. I’d appreciate it if the authors could share additional qualitative visualizations of imagenet examples over the large mask image inpainting task. From my own experience, I find that sometimes we can achieve better LPIPS if the reconstructed images have rich texture, but are not necessarily semantically meaningful (This can imply unrealistic samples getting better LPIPS. However, I understand this can also happen due to other reasons, such as diffusion models not being perfect, etc., so I’m not very critical about this.)"}, "questions": {"value": "Please see the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "55zz2LmqZn", "forum": "VG5iE3rzLz", "replyto": "VG5iE3rzLz", "signatures": ["ICLR.cc/2026/Conference/Submission15887/Reviewer_ev17"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15887/Reviewer_ev17"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909041707, "cdate": 1761909041707, "tmdate": 1762926103378, "mdate": 1762926103378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a simple yet effective method to enhance diffusion-based inverse problem solving by leveraging strong latent initializations. The core idea is to take an initial reconstruction \\(x\\) from any baseline method, invert it via the unconditional probability flow ODE to obtain a latent $x_T^*$, and then run a deterministic DPS-ODE from this latent to produce an improved sample $\\hat{x}$. Theoretically, the authors prove in Gaussian mixture models that REGUIDANCE boosts both reward (measurement consistency) and realism (data likelihood). Empirically, REGUIDANCE consistently improves state-of-the-art baselines (DDRM, DPS, DAPS) across challenging image restoration tasks (large inpainting, super-resolution, deblurring) on ImageNet and CIFAR-10, measured by LPIPS and CMMD."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Theoretical Soundness:** Provides the first rigorous guarantees for DPS in mixture models, explaining both reward and realism improvement.\n- **Empirical Effectiveness:** Demonstrates strong, consistent improvements across multiple tasks, datasets, and baselines.\n- **Clarity:** Exceptionally well-written and easy to follow.\n- **Significance:** Offers a practical, low-cost method to enhance existing diffusion-based solvers, especially for highly compressive inverse problems."}, "weaknesses": {"value": "- **Theoretical Scope:** The theoretical guarantees are currently limited to Gaussian mixture models. While insightful, extending them to more complex distributions remains future work.\n- **Computational Overhead:** REGUIDANCE doubles the inference time (inversion + DPS-ODE), though the absolute cost (≤7 GPU minutes) is reasonable. A more detailed runtime comparison would be helpful.\n- **Initialization Dependency:** The method’s performance depends on the quality of the initial reconstruction. While it boosts weak baselines, poor initializations may limit gains."}, "questions": {"value": "1. The paper shows the space of good latents is disconnected. Could this be exploited to generate diverse solutions, e.g., by sampling multiple latents from a baseline and applying REGUIDANCE?\n2. While REGUIDANCE improves sample quality, does it also improve convergence speed or stability compared to running DPS from random initialization?\n3. Have you explored adaptive strategies for choosing the guidance strength $\\rho$ or time horizon $T$ based on the task or baseline method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "488bmY4sTt", "forum": "VG5iE3rzLz", "replyto": "VG5iE3rzLz", "signatures": ["ICLR.cc/2026/Conference/Submission15887/Reviewer_Xb4y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15887/Reviewer_Xb4y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919537025, "cdate": 1761919537025, "tmdate": 1762926103059, "mdate": 1762926103059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReGuidance, a simple yet effective method for improving diffusion-based inverse problem solvers in highly compressive or weak-reward settings. The key idea is to invert an existing reconstruction into the diffusion latent space via the reverse probability flow ODE, then re-run deterministic diffusion posterior sampling (DPS) initialized at this latent."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper clearly articulates a key weakness of current diffusion guidance techniques in settings with limited measurement information or weak reward signals, and introduces a an approach that systematically addresses this issue. \n- ReGuidance is conceptually simple yet powerful. It can be applied to any pretrained diffusion model or inverse problem solver without retraining, making it broadly useful. \n- The paper is clearly written, with well-organized motivation, method, and theory."}, "weaknesses": {"value": "- ReGuidance always re-samples from one recovered latent, offering limited posterior diversity. Exploring multiple inverted latents or stochastic variants might provide richer solutions.\n- The paper would benefit from a comparison or discussion with D-Flow [1], which similarly optimizes the diffusion starting point to improve reconstruction and control.\n- Both the reverse ODE and DPS-ODE stages are deterministic with fixed hyperparameters $\\rho$ and $T$. It would be good to have ablations or robustness analysis. \n\n[1] Ben-Hamu et al. D-Flow: Differentiating through Flows for Controlled Generation. ICML 2024."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QEwFRygU3J", "forum": "VG5iE3rzLz", "replyto": "VG5iE3rzLz", "signatures": ["ICLR.cc/2026/Conference/Submission15887/Reviewer_ZaHb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15887/Reviewer_ZaHb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937823169, "cdate": 1761937823169, "tmdate": 1762926102661, "mdate": 1762926102661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}