{"id": "5i72Q8GvLt", "number": 4930, "cdate": 1757811529625, "mdate": 1763713953472, "content": {"title": "Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs", "abstract": "Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables \"free-lunch\" alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment.", "tldr": "We propose TPO, A novel alignment method of text-to-image diffusion model that do not need human preference data.", "keywords": ["Alignment", "Text-to-Image Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d781b822bb88bb449b187bb4cc63ac59ad802715.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Text Preference Optimization (TPO) for aligning text-to-image (T2I) diffusion models without human preference image pairs. It creates matched/mismatched text pairs via LLM-based prompt editing under four principles (content, attribute, spatial, contextual). Two variants, TDPO and TKTO, adapt DPO/KTO to diffusion training. Experiments across several benchmarks indicate consistent gains over Diffusion-DPO/KTO while remaining annotation-free."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of \"free-lunch\" alignment (re-using existing caption data + LLM perturbations) is creative, practical, and addresses real scalability issues in diffusion-model alignment.\n2. The formulation of TDPO/TKTO directly mirrors established DPO/KTO objectives, but shifts preference comparison from outputs (images) to inputs (text prompts).\n3. The method has been evaluated on four diverse datasets with multiple quantitative metrics. It achieves consistent improvement across most metrics and datasets. Qualitative comparisons convincingly show better text-image faithfulness, and ablation studies on editing principles and implicit-preference correlation provide solid analytical depth."}, "weaknesses": {"value": "1. The pipeline is essentially \"synthetic preference data + existing DPO/KTO objective.\" Although effective and creative, it feels more like an engineering simplification than a fundamentally new learning principle.\n2. The framework depends on a single budget-constrained LLM for prompt editing, which may introduce stylistic or semantic biases and restrict negative-sample diversity."}, "questions": {"value": "1. Can authors please clarify how LLM-generated and human-labeled prompts compare in alignment performance, possibly via a short correlation or qualitative analysis. This would clarify whether TPO truly approximates human alignment behavior or just benefits from larger-scale synthetic coverage.\n2. I have concerns with the dependence on LLM prompt quality. Alignment quality is tied to how well the Gemini 2.0 Flash edits produce meaningful yet challenging negatives. Are there any quantitative analysis of negative-sample diversity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "89TKzYKiny", "forum": "5i72Q8GvLt", "replyto": "5i72Q8GvLt", "signatures": ["ICLR.cc/2026/Conference/Submission4930/Reviewer_6UbU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4930/Reviewer_6UbU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886632716, "cdate": 1761886632716, "tmdate": 1762917774411, "mdate": 1762917774411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose TPO, a novel T2I alignment framework that perturbs the prompt with LLM to form a prompt level preference pair and aligns the Diffusion T2I model via prompt level preference. TPO achieved superior performance across multiple automatic feedback benchmarks on SD1.5 when integrated to DPO and KTO. In addition, TPO disentangles the need for image preference data in T2I alignment, further enhancing alignment efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The TPO algorithm is a novel contribution for Diffusion T2I alignment leveraging prompt level preference rather than image level.\n* The TPO algorithm achieved superior performance with less requirements on training data."}, "weaknesses": {"value": "* My major concern is the robustness of TPO. The experiments are conducted only on SD1.5, one of the small scale open source T2I diffusion models. Comparison with DPO on SDXL are expected to demonstrate the robustness of this interesting algorithm.\n\n* Evaluation from human or large VLM / MLLM is needed. The automatic evaluation metrics used in Table 1 and Table 2 are CLIP and fine-tuned CLIP/BLIP variants while SD1.5 uses CLIP as text encoder. The training recipe of TPO could potentially boost the similarity of the generations and the CLIP embedding and hence significantly improves performance on these related metrics. It would be good to see some evaluation that relies less on CLIP.\n\n* In Table 1, there's a typo \"PartyPrompt\", please revise for \"Parti-Prompt\""}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sGAW0N48Hw", "forum": "5i72Q8GvLt", "replyto": "5i72Q8GvLt", "signatures": ["ICLR.cc/2026/Conference/Submission4930/Reviewer_fwFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4930/Reviewer_fwFA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988310497, "cdate": 1761988310497, "tmdate": 1762917773749, "mdate": 1762917773749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Text Preference Optimization (TPO), a framework to align text-to-image diffusion models without requiring human-annotated image preference pairs. The method uses a Large Language Model (LLM) to generate mismatched text prompts from original captions, creating text preference pairs (matched vs. perturbed) to train the model. The authors extend DPO and KTO to TDPO and TKTO, claiming state-of-the-art results on benchmarks like HPSv2 and Pick-a-Pic. While the core idea of \"free lunch\" alignment is novel, the paper's claims are significantly undermined by critical flaws in the experimental setup and evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Original Idea: The concept of aligning models via automatically constructed text preferences, rather than image preferences, is novel and creative.\n2. Practical Motivation: The work tackles a key scalability bottleneck in T2I alignment, offering a potentially cost-effective solution.\n3. General Framework: The approach is model-agnostic and can be integrated with various preference optimization algorithms (DPO, KTO), which is a strong design choice."}, "weaknesses": {"value": "- Unconvincing Baseline Performance: The dramatic drop in performance for the Diffusion-DPO baseline (e.g., HPS score) is a major red flag. It suggests either a flawed implementation, inappropriate hyperparameters, or an unfair comparison setup, which invalidates the claimed superiority of the proposed methods.\n- Opaque Training Process: The paper provides no insight into the training dynamics. There are no loss curves, convergence plots, or monitoring metrics on held-out benchmarks. This makes it impossible to assess if the models are learning the intended preferences or if the training is stable.\n- Potentially Unfair Comparison: The baselines (Diffusion-DPO/KTO) are designed for human image preferences. Comparing them against a method using synthetic text preferences, where the negatives are generated by the same LLM pipeline, may not be a fair or meaningful comparison. The paper should have included a baseline using the same synthetic text to generate a negative image for a more controlled comparison."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HNXqpdoXNz", "forum": "5i72Q8GvLt", "replyto": "5i72Q8GvLt", "signatures": ["ICLR.cc/2026/Conference/Submission4930/Reviewer_PZtZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4930/Reviewer_PZtZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988752670, "cdate": 1761988752670, "tmdate": 1762917773298, "mdate": 1762917773298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}