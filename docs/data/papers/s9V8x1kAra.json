{"id": "s9V8x1kAra", "number": 15036, "cdate": 1758247102933, "mdate": 1759897334017, "content": {"title": "Robust Safety Guarantee for Large Language Models via Preference-Augmented Distributional Alignment", "abstract": "Domain-specific fine-tuning of large language models (LLMs) often compromises their safety alignment, leading to unsafe generations. Existing approaches largely rely on distributional alignment, enforcing token-level similarity between pre- and post-fine-tuned models. However, this neglects the semantic nature of text generation and can weaken the model’s reasoning and robustness. To address this limitation, we propose a preference-based alignment framework that complements distributional alignment by biasing the fine-tuned model toward the safe outputs of the pre-trained model, rather than strictly preserving distributional similarity. Simulation results show that preference alignment produces consistent safe outputs even when the underlying distributions differ. Extensive experiments on multiple fine-tuning attack datasets and utility benchmarks further demonstrate that our method substantially improves safety with only minor degradation in utility. This achieves a more favorable balance between safety and utility, and significantly enhances robustness against adversarial fine-tuning.", "tldr": "", "keywords": ["Large Language Models", "Safety Alignment", "Preference Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/afce4908e26fa980f87e63a58f41f222d4ca8a6a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a preference-augmented distributional alignment method that combines CSFT with a preference loss to improve safety robustness in LLMs, supported by theoretical analysis and experiments on LLaMA-2-7B-Chat."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses an important and timely topic in LLM safety alignment, focusing on improving robustness beyond surface-level safety mechanisms and aiming for more stable and principled alignment."}, "weaknesses": {"value": "1. **Poor Paper Organization and Presentation.** The whole presentation of this paper is weird. For example, your novel method is introduced in the Related Work section as a separate paragraph, and the CSFT loss, which is not your contribution, is discussed more extensively than your own method under Novel Loss Function for Safety Alignment.\n2. **Weak and unrealistic theoretical analysis.** The theoretical section makes overly strong assumptions such as convexity and PL inequality, which do not hold for real LLM optimization landscapes. As a result, the claimed robustness guarantees are mostly formal and have little practical meaning for large-scale non-convex models.\n3. **Weak and narrow empirical evaluation.** All reported results use a single base model (LLaMA-2-7B-Chat) and a narrow set of QA/utility tasks. There is no demonstration of generality across architectures, model sizes, or reasoning-oriented models. Results therefore lack external validity.\n4. **Simple Baselines.** The paper only compares the proposed method with a few basic fine-tuning approaches and lacks stronger or more diverse baselines. This makes it difficult to judge how much improvement truly comes from the proposed technique rather than from general tuning effects.\n5. **Limited and Non-adaptive Attack Evaluation.** While the authors include several attack types, the evaluation is limited in scope and lacks truly adaptive or jailbreak-style red-teaming. The current attacks do not demonstrate that the method resists attackers who know the defense and craft prompts to exploit it. Adding adaptive attacks and a wider jailbreak suite would make the robustness claims convincing.\n6. **Insufficient Clarity in Theoretical Motivation.** The theoretical analysis is presented in isolation from the main algorithmic design. The connection between the formal objectives and the practical implementation of the preference-augmented loss is unclear, and the paper does not concretely show how the theoretical results guide or justify the empirical method.\n7. **Unclear Advantage over Existing Methods.** The proposed method shows only marginal improvement in safety while sacrificing some utility. The overall trade-off does not clearly outperform existing baselines, making the claimed advantage unconvincing."}, "questions": {"value": "No further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sSw2Dvuqmn", "forum": "s9V8x1kAra", "replyto": "s9V8x1kAra", "signatures": ["ICLR.cc/2026/Conference/Submission15036/Reviewer_fSt1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15036/Reviewer_fSt1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760701467942, "cdate": 1760701467942, "tmdate": 1762925362611, "mdate": 1762925362611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the persistent challenge of maintaining safety alignment in large language models (LLMs) during domain-specific fine-tuning, which often compromises their safety properties. The authors identify the limitation of existing Constrained Supervised Fine-Tuning (CSFT) methods that rely solely on distributional alignment at the token level. To overcome this, they propose a Preference-Augmented Distributional Alignment (CSFT+PA) framework that combines distributional alignment with preference alignment, encouraging models to favor the safe outputs of the pre-trained reference model rather than merely mimicking its token distributions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of preference alignment into distributional fine-tuning is conceptually novel. While preference learning has been widely applied in RLHF and DPO settings, its use as a stabilizing term for safety-preserving fine-tuning is new and well-motivated.\n\n2. The paper is very well written and structured. Key equations are clearly presented, and motivations behind each design component are logically explained."}, "weaknesses": {"value": "1. Lack of ablation on PA components (Adaptive Weight $\\mu$ and Scheduling Coefficient $\\delta_\\text{epoch}$). \n\n2. Limited dataset and model diversity. The main results are based on LLaMA-2-7B fine-tuning and two downstream tasks, which is out of date."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j5NTcjN8yd", "forum": "s9V8x1kAra", "replyto": "s9V8x1kAra", "signatures": ["ICLR.cc/2026/Conference/Submission15036/Reviewer_JJpy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15036/Reviewer_JJpy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760838402998, "cdate": 1760838402998, "tmdate": 1762925362155, "mdate": 1762925362155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new finetuning objective designed to improve the robustness of safety alignment in LLMs. The core idea is to combine a CSFT loss that enforces global distributional stability with a token-level PA loss that sharpens local alignment with a safety reference model. Importantly, the PA loss is adaptive: its weight increases proportionally to the KL divergence between the model and reference distributions (indicating drift) and also scales with training epoch, gradually shifting emphasis from stabilization to targeted correction. The authors argue that this combination provides stronger theoretical and empirical robustness guarantees compared to single-loss formulations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed objective is well-motivated and technically clear. Combining KL-based global regularization with token-level preference shaping provides a natural balance between stability and precision. The formulation is compatible with standard finetuning pipelines and can be used as a drop-in replacement for vanilla SFT objectives. The authors provide theoretical convergence analysis."}, "weaknesses": {"value": "1. The PA loss relies on the reference model’s top tokens, but the paper does not analyze how sensitive the method is to the reference quality. A reference model with imperfect safety may propagate its biases through PA weighting.\n2. The experimental evaluation lacks comparison to strong and closely related baselines, particularly Shape it Up! Restoring LLM Safety during Finetuning via STAR-DSS, which also performs token-level safety shaping and is a natural point of comparison.\n3. The paper focuses primarily on theoretical properties and safety metrics but could benefit from additional capability preservation evaluations (e.g., GSM8K, MMLU) to measure safety–utility trade-offs."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OlBG6JHdX1", "forum": "s9V8x1kAra", "replyto": "s9V8x1kAra", "signatures": ["ICLR.cc/2026/Conference/Submission15036/Reviewer_bhDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15036/Reviewer_bhDF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535931449, "cdate": 1761535931449, "tmdate": 1762925361606, "mdate": 1762925361606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses safety degradation in large language models (LLMs) during domain-specific fine-tuning by proposing CSFT+PA, a preference-augmented alignment framework that combines distributional alignment with preference-based alignment. The authors argue that existing Constrained Supervised Fine-Tuning (CSFT) methods, which enforce token-level distributional similarity between pre- and post-fine-tuned models, overlook the semantic nature of text generation and lack robustness. Their approach introduces an auxiliary Preference Alignment (PA) loss— $L_{\\text{Total}} = L_{\\text{CSFT}} + \\delta_{\\text{epoch}} \\cdot L_{\\text{PA}}$ —that encourages the fine-tuned model to favor safe outputs from the reference model rather than strictly preserving distributional similarity. The paper provides theoretical analysis including convergence guarantees under standard stochastic optimization assumptions and robustness bounds showing parameter deviations scale linearly with perturbation intensity. Experiments on Llama-2 demonstrate substantial safety improvements against three types of adversarial attacks, while incurring only minor utility degradation on two downstream tasks (Samsum and SQL Create Context)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Theoretical robustness guarantees: The paper provides formal robustness bounds (Theorem 4.2) showing that parameter deviations under perturbations scale linearly with noise intensity.\n\n+ Clear presentation: The paper is well-written with good motivation."}, "weaknesses": {"value": "+ Disconnect between theory and methodology (Theorem 4.1): The convergence analysis in Theorem 4.1 applies to generic stochastic optimization under standard assumptions (bounded gradients, Lipschitz continuity, diminishing learning rates). This theorem does not leverage or highlight any specific properties of the proposed CSFT+PA loss structure. The proof essentially shows that SGD converges for any loss satisfying these assumptions, making it less connected to the methodological contributions. A more meaningful theoretical result would characterize how the preference alignment term specifically affects convergence properties or provide convergence rates that depend on the interplay between L_CSFT and L_PA.\n\n+ Limited utility evaluation: The downstream task evaluation is restricted to only two datasets (Samsum and SQL Create Context). This is insufficient to demonstrate that the method preserves general utility across diverse tasks. The paper would benefit from evaluation on standard LLM benchmarks (MMLU, GSM8k, etc.)\n\n+ Missing important baselines: The paper only compares against SFT and CSFT, but omits several relevant safety alignment methods:\n  1. Vaccine (Vaccine: Perturbation-aware alignment for large\nlanguage models against harmful fine-tuning attack)\n  2. Safety filtering methods (e.g., SafetyGuard filtering) \n  3. LISA (Lisa: Lazy safety alignment for large language models against harmful fine-tuning attack)\n  4. or recent robust fine-tuning methods, like STAR-DSS (Shape it Up! Restoring LLM Safety during Finetuning)\n\n\n+ Limited model scope: Experiments are conducted only on Llama-2-7B-Chat. Evaluation on models of different sizes and architectures (e.g., other model families like QWen) would strengthen generalizability claims."}, "questions": {"value": "Please respond to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XtWhrwY6Af", "forum": "s9V8x1kAra", "replyto": "s9V8x1kAra", "signatures": ["ICLR.cc/2026/Conference/Submission15036/Reviewer_jQTT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15036/Reviewer_jQTT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878736210, "cdate": 1761878736210, "tmdate": 1762925361091, "mdate": 1762925361091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}