{"id": "zrKFIfjcRp", "number": 22153, "cdate": 1758326855600, "mdate": 1759896883411, "content": {"title": "StefaLand: An Efficient Geoscience Foundation Model That Improves Dynamic Land–Surface Predictions", "abstract": "Managing natural resources, meeting growing societal needs, and reducing risks from floods, droughts, wildfires, and landslides require models that can accurately predict climate-driven land–surface responses. Traditional models of environmental impact, whether process-based or task-specific machine learning, often struggle with spatial generalization because they are trained on limited observations and can degrade under concept drift. Recently proposed vision foundation models trained on satellite imagery demand massive compute, and they are often ill-suited for dynamic land surface prediction tasks. We introduce StefaLand, a generative spatiotemporal Earth foundation model centered on landscape interactions. StefaLand is demonstrated to improve predictions on four important tasks across five datasets: streamflow, soil moisture, soil composition and landslides, compared to previous state-of-the-art methods, showing especially strong ability to generalize across diverse landscapes, including data-scarce regions. The model builds on a masked autoencoder architecture that learns deep joint representations of landscape attributes, and its design reflects a deliberate integration of ideas adapted to geoscience. These include a location-aware architecture that fuses static and time-series inputs, an attribute-based rather than image-based representation that drastically reduces compute demands, and residual fine-tuning adapters that strengthen knowledge transfer across tasks. Their alignment with domain knowledge enables StefaLand to deliver robust performance on various dynamic land–surface tasks. StefaLand can be pretrained and finetuned on commonly-available academic compute resources compared with commercial foundation models, yet consistently outperforms state-of-the-art supervised learning baselines and fine-tuned vision foundation models. To our knowledge, this is the first geoscientific land-surface foundation model that demonstrably improves dynamic land surface interaction prediction tasks and supports a wide range of downstream applications.", "tldr": "A compute-efficient geoscience foundation model that learns landscape interactions and improves spatial generalization for streamflow, soil composition, landslides, and soil moisture.", "keywords": ["Foundation models", "Spatiotemporal modeling", "Geoscience AI", "Land-surface prediction", "Climate impacts", "Transfer learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ea343c0127587436e59a4c38cec35eb9debc832.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces StefaLand, a model that applies self-supervised learning with a Masked Autoencoder (MAE) to learn joint representations from both static landscape attributes and dynamic time-series geospatial data. The learned representations, pretrained using a cross-variable group masking strategy, are then fine-tuned to improve performance and spatial generalization on various downstream geoscience tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This paper addresses two key limitations in current approaches to land surface process modeling: (1) existing supervised learning methods are prone to overfitting and exhibit poor generalization, and (2) existing self-supervised feature extraction methods trained on remote sensing imagery cannot be directly applied to land surface process modeling. To tackle these issues, the authors propose a self-supervised feature extraction method for remote sensing time-series data based on feature engineering combined with Masked AutoEncoder (MAE). They further demonstrate across multiple datasets that a self-supervised pretraining followed by fine-tuning strategy achieves better generalization than direct supervised learning."}, "weaknesses": {"value": "1. The paper does not pose any scientifically meaningful question and reads essentially like a technical report. The critique of existing methods is superficial—limited to statements such as “poor performance” or “weak generalization”—without identifying the fundamental limitations of current self-supervised feature extraction methods specifically in the context of land surface process modeling. Moreover, the authors fail to discuss what concrete challenges arise when directly applying existing time-series self-supervised learning methods to this domain.\n\n\n2. The paper provides a severely inadequate review of related work. Many of the land surface prediction tasks discussed are fundamentally time-series feature extraction and forecasting problems. However, the proposed “feature engineering + MAE” framework does not incorporate any modeling tailored to the physical processes or spatial dependencies inherent in land surface dynamics. In computer science, numerous self-supervised methods already exist for time-series and tabular data. The paper should clearly articulate how the proposed method differs from these established approaches and include experimental comparisons against them.\n\n\n3. The experimental evaluation appears to deliberately understate baseline performance. For instance, Table 5 claims that baseline results come from:\n\nLiu, J., Shen, C., Pei, T., Kifer, D., & Lawson, K. (2025). The value of terrain pattern, high‐resolution data and ensemble modeling for landslide susceptibility prediction. Journal of Geophysical Research: Machine Learning and Computation, 2, e2024JH000460. https://doi.org/10.1029/2024JH000460 .\n\n\nHowever, upon reviewing this paper, I could not find the reported numbers: Logistic Regression ACC = 0.742 and Random Forest ACC = 0.751. Instead, the cited work reports Logistic Regression 1D ACC = 0.751, Random Forest 1D (RF1D) ACC = 0.821, CNN 2D (CNN2D) ACC = 0.880, and Random Forest 2D (RF2D) ACC = 0.765. Furthermore, several models in Liu et al. (2025) clearly outperform the proposed STEFALAND + CNN2D (ACC = 0.799). This casts serious doubt on the claimed performance improvements of STEFALAND.\n\n\n4. The writing quality is poor. The main text lacks any flowcharts or visualizations of results. Additionally, the method section is confusing, conflating well-established techniques with the authors’ novel contributions without clear distinction."}, "questions": {"value": "1. What is the core scientific problem addressed by this work, and what new insights does it offer to the research community?\n\n2. Is the methodological design genuinely driven by the scientific problem? MAE was not originally developed for self-supervised representation learning on spatiotemporal geospatial data. What is the specific rationale for using MAE in land surface process modeling? Why were alternative self-supervised approaches—such as VAE, MoCo v3, SimCLR, or SCARF—not considered?\n\n3. Compared to widely adopted self-supervised learning methods for time-series and tabular data in the machine learning community, what specific adaptations or optimizations does this work introduce for land surface process modeling? How would the proposed method perform in a fair comparison on standard, widely recognized benchmark datasets?\n\n4. Can the authors provide detailed visual comparisons between their method and existing approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WSNWOWEif2", "forum": "zrKFIfjcRp", "replyto": "zrKFIfjcRp", "signatures": ["ICLR.cc/2026/Conference/Submission22153/Reviewer_6uDU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22153/Reviewer_6uDU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760680683806, "cdate": 1760680683806, "tmdate": 1762942093156, "mdate": 1762942093156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a foundation model for land surface hydrology. The model is a transformer that is pre-trained using a masked auto-encoder to reconstruct co-located meteorlogy time series and static environmental characteristics related to vegetation cover, soil properties, topography and human influence. It is evaluated on four downstream tasks: streamflow prediction, landslide susceptibility mapping, soil moisture prediction and sand/clay-fraction prediction. It is comparable or outperforms supervised baselines on these tasks, with particularly promising benefits for streamflow prediction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. Focus on hydrology, which can have important impacts on societies resilience to extreme events, by improving early warning systems\n2. Large benefits on streamflow prediction from using the pre-trained transformer backbone.\n3. Competitive performance on four tasks related to land surface hydrology\n4. Evaluation of spatial extrapolation and honest reporting of a drop in performance (akin to other supervised baselines) in the challenging regional holdout scenario\n5. Interesting additional experiment to leverage the embeddings for predicting parameters of a hydrological model."}, "weaknesses": {"value": "Major points:\n\n1. No previously published baselines included in the downstream task evals (except for the landslide task). This is a major point, as it is hard to assess how well the StefaLand model performs against current SOTA. The authors do make an effort to include commonly used models as baselines, however it still remains unclear how competitive StefaLand really is.\n2. Soil properties downstream task suffers from potentially skewed evaluation: Prediction the soil properties (albeit from a different harmonized dataset) is one of the pre-training task for StefaLand.\n3. Presentation lacks clarity at times:\n   -  L141 x & b not introduced\n   - L189 it is confusing that the exact groupings are not mentionend here, and it is also not elucidated how the \"reciprocal or bidirectional causality\" is determined\n   - Section 2.3 : Figure D1 definitely indicates a different structure than Equations 13-15. But i'd argue that actually both require reworking: for each operation / neural network module it needs to be clear what the inputs and outputs are, especially, how embeddings, static variables and forcings are routed. Also: what happens with predictors that have not been part of the pretraining.\n   - Table 1 (and the following) should probably be re-ordered for more clarity. Currently it combines two aspects: 1) Performance scores against baselines on CAMELS and 2) ablation studies. For clarity, these two should be separate. So the message fro Table 1 can be: StefaLand gives performance benefits over baseliens on CAMELS. And the additonal tables' message is: the best decoder strategy for stefaland is to use a resconn decoder. Then, the order of rows shoudl also be changed, such that baselines are on top and the \"chosen\" model is on the bottom.\n   - Paragraph L282-L288 is confusing. It does not mention the nuances between the different papers, reads more like a related work section, and raises the question why these baselines were not included in the eval, but instead the authors took it on themselves to train a supervised LSTM.\n   - throughout the manuscript the same topics re-appear multiple times. It would be really good if the manuscript would be overall streamlined again into a common storyline, and less of a listing of different things that were done and justifications for design choices.\n4. Experiments are a bit strange. Typically for a foundation model, i'd expect the following experiments: training from scratch, zero-shot (if possible), linear probing and fine-tuning. However, this work only reports training from scratch and fine-tuning, but does so with multiple different decoders (see comment above). Again, i'd suggest to change the narrative here also such that its easier to understand for the community: In table 1 include the baselines LSTM, TerraMind & ideally the SOTA from literature, and then include StefaLand from scratch (with resconn decoder), linear probing (just linear layer...) & fine-tuning (with resconn). This allows much more to understand the value of the pre-trained embeddings vs. the model architecture. Then put the ablation of different decoders in a different table. Ideally use as metric there not just 1 downstream task, but (avg) performance across all four downstream tasks, to showcase that you are proposing a robust model architecture for the foundation model. By the way,  I think for soil property prediction you can include zero-shot prediction.  \n5. The other foundation models included in this study make little sense for the downstream tasks (except for terramind for soil mapping - but its not evaluated there). It is obvious that TerraMind and PrithviWxC are not made for streamflow or soil moisture prediction. So if you insist on keeping them in there, perhaps try to tone down the narrative throughout the manuscript a bit - you have enough interesting content, so there is really no need to emphasize so much on this comparison.\n6. This works framing seems a bit to broad. The four downstream tasks are all land surface hydrology tasks, and in that context they are tasks that perhaps depend less on the the interannual variability of the vegetation, thus its also unclear how well the approach with the static features would work for land surface tasks beyond hydrology (e.g. carbon, energy ,...). I suggest to dampen the framing a bit and explicitly frame this work as a foundation model for land surface hydrology -- which by the way is still super impactful.\n\n\nMinor points:\n\n7. Which hyperparameters did you tune?\n8. Where is the ablation of the different adapters in Figure D3?\n9. L052 \"represents the impacts of climate change\" seems a bit of a strong statement and to my understanding not entirely true.\n10. Tables comparing against supervised baselines should include a comparison of parameter numbers and training / inference cost -- as i understand that these are not necessarily always fair comparisons in these terms.\n11. not sure if your downstream tasks really always use the most predictive inputs.. e.g. you seem to neglect remote sensing for soil properties and moisture prediction..."}, "questions": {"value": "An analysis of the different pre-training objectives could actually be interesting. How good were your reconstruction losses? Which variable groups were predictive for which others? etc. :-)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0NxCNeqgRf", "forum": "zrKFIfjcRp", "replyto": "zrKFIfjcRp", "signatures": ["ICLR.cc/2026/Conference/Submission22153/Reviewer_UBGy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22153/Reviewer_UBGy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013853932, "cdate": 1762013853932, "tmdate": 1762942092799, "mdate": 1762942092799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes StefaLand, a foundation model designed for land surface monitoring tasks. The model is a transformer encoder that takes in point-based covariates (attribute-formatted rather than pixel/raster-formatted) from static and time-varying data sources such as precipitation, humidity, forest cover fraction, soil depth, population density, and more. Experiments on five datasets shows that StefaLand outperforms the supervised baseline and use of PrithviWxC and TerraMind features in place of StefaLand features. The model is much smaller than foundation models designed for raster datasets, making it more accessible for researchers than adapting larger vision foundation models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- StefaLand is much smaller than foundation models designed for raster datasets, making it more accessible for researchers than adapting larger vision foundation models.\n- The paper aims to address land surface modeling tasks that involve forecasting dynamic, time-varying phenomena like soil moisture, streamflow runoff, etc. This is different than many other geospatial foundation model papers that focus on land surface detection or mapping tasks like land cover classification.\n- StefaLand has better performance than the supervised baseline and PrithviWxC or TerraMind features on all tasks."}, "weaknesses": {"value": "- The Intro says that the goal of StefaLand is to “predict what will happen in the near or distant future”, but it does not seem like any of the downstream tasks are forecasting future timesteps. Maybe I don’t understand these tasks, but are the tasks predicting *future* states based on previous timesteps, like future soil characteristics or streamflow?\n- The paper says they “could not identify other earth foundation models that are designed for land-surface predictions”. I don’t think the term “land-surface predictions” is widely understandable to the ICLR community, and it’s hard to see where the line is between these tasks and other tasks studied by the remote sensing foundation model community.\n- I think the main novelty is in the selection of the downstream tasks and data sources. I appreciate that the performance is better on the downstream tasks, but this seems somewhat obvious because the input variables are similar to those being predicted in the downstream tasks. Other aspects of the approach have already been proposed in prior work. For example, the Cross-Variable Group Masking and point-based inputs (single pixel time series + static variables) was done in [Presto](https://arxiv.org/abs/2304.14065). Presto is also even smaller than StefaLand.\n- Incomplete or unclear experiments\n    - The paper uses PrithviWxC and TerraMind as baselines. It seems like any remote sensing foundation model could be used here. The two chosen are among the largest and lowest performance in the recent literature, so they seem like odd choices. What about models like [Galileo](https://arxiv.org/pdf/2502.09356), which also includes time-varying and static variables including some that are also in StefaLand (precipitation, soil moisture, evapotranspiration, etc), or Presto which is also attribute-based, or other recent models that have been shown to have higher performance than Prithvi (e.g. see comparisons in the Galileo paper).\n    - The paper says they could not run PrithviWxC and TerraMind for all tasks due to computational expense, but the choice of when they are included seems arbitrary. I think the models were not available at the time of writing this paper, but now the authors could also try precomputed embeddings from [Tessera](https://arxiv.org/abs/2506.20380) or [AlphaEarth Foundations](https://arxiv.org/abs/2507.22291v1).\n    - There is not an explicit ablation experiment in the paper. There seem to be versions of the model that would constitute ablations in the various tables (like StefaLand - no resConn) but this makes it confusing which are ablations and which are the actual proposed model. Which one is “StefaLand” versus an ablation? This is even more confusing because not all variants are included in all tables, for example Table 2 only has “StefaLand - direct”.\n    - The tables mostly report means over cross-validation splits, but there are no standard errors, so we cannot assess the variability/stability of each model and the significance of the comparisons.\n- The sources in Table C2 seem somewhat random. What was the motivation for this specific list, or excluding other sources that could have been used?\n- The appendix referencing is coarse. For example, an entire section of the Appendix is reference where you need to reference a table in that section, so the reader has to look through the section to find the material referenced. This is a nit, but it makes it tedious to read."}, "questions": {"value": "- are the downstream tasks predicting *future* states based on previous timesteps, like future soil characteristics or streamflow?\n- I don’t think the term “land-surface predictions” is widely understandable to the ICLR community, and it’s hard to see where the line is between these tasks and other tasks studied by the remote sensing foundation model community.  Can you define “land-surface predictions” and explain the distinction from other tasks types in remote sensing?\n- What was the reasoning for choosing PrithviWxC and TerraMind, as opposed to other choices that would be smaller, more performance, and/or more related in terms of the input sources to StefaLand?\n- Can the authors add standard errors to the results tables?\n- What was the motivation for this specific list of dataset sources, or excluding other sources that could have been used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sNfmshuZMl", "forum": "zrKFIfjcRp", "replyto": "zrKFIfjcRp", "signatures": ["ICLR.cc/2026/Conference/Submission22153/Reviewer_jPBF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22153/Reviewer_jPBF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116540419, "cdate": 1762116540419, "tmdate": 1762942092197, "mdate": 1762942092197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}