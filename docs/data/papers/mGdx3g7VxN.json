{"id": "mGdx3g7VxN", "number": 22520, "cdate": 1758332224638, "mdate": 1763618051534, "content": {"title": "PRISM: A Hierarchical Multiscale Approach for Time Series Forecasting", "abstract": "Forecasting is critical in areas such as finance, biology, and healthcare. Despite the progress in the field, making accurate forecasts remains challenging because real-world time series contain both global trends, local fine-grained structure, and features on multiple scales in between. Here, we present a new forecasting method, PRISM (Partitioned Representation for hIerarchical Sequence Modeling), that addresses this challenge through a learnable tree-based partitioning of the signal. At the root of the tree, a global representation captures coarse trends in the signal, while recursive splits reveal increasingly localized views of the signal. At each level of the tree, data are projected onto a time-frequency basis (e.g., wavelets or exponential moving averages) to extract scale-specific features, which are then aggregated across the hierarchy. This design allows the model to jointly capture global structure and local dynamics, enabling both reconstruction and forecasting. Experiments across benchmark datasets show that our method outperforms state-of-the-art methods for forecasting and also requires less runtime and memory. Overall, our results demonstrate that hierarchical time-frequency decomposition provides a lightweight and robust framework for forecasting multivariate time series.", "tldr": "We introduce a lightweight tree-based model that hierarchically decomposes time series across time and frequency to forecast time series.", "keywords": ["Time series", "Forecasting", "Multiscale", "Hierarchical", "Wavelets"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9c948a34cba32e6dc25ce0da8561564fcfd5bfd5.pdf", "supplementary_material": "/attachment/6f4b1622d22c01458f8ef2f9da002d1b33620537.pdf"}, "replies": [{"content": {"summary": {"value": "The paper presents PRISM, a novel hierarchical multiscale model for time-series forecasting. PRISM constructs a binary tree over time while performing frequency decomposition (using wavelets or similar filters) at each node. It jointly learns temporal and frequency hierarchies and uses learnable importance scores to weight different frequency bands. By optimizing both forecasting and reconstruction losses, PRISM achieves strong interpretability, robustness, and state-of-the-art performance on multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The learnable importance scores provide insights into which frequency components drive predictions, adding transparency to the model’s behavior.\n2. The paper is well-structured and easy to follow."}, "weaknesses": {"value": "1. The binary partitioning strategy is manually defined rather than data-adaptive; this might limit flexibility for non-stationary or irregularly sampled signals.\n2. **Dependence on pre-defined transforms**. The method relies on fixed wavelet or FFT bases. Learned or adaptive frequency decompositions could potentially capture more expressive features.\n3. The benchmarks are still limited to standard ones. More various or big datasets should be put in place to demonstrate the effectiveness of the proposed method.\n4. While the model provides interpretable frequency weights, the paper lacks formal evaluation or case studies illustrating interpretability in applications."}, "questions": {"value": "1. How sensitive is the model to the choice of frequency basis (e.g., wavelet type)?\n2. Could the tree depth be learned dynamically based on data complexity rather than fixed by design?\n3. How does PRISM perform on highly irregular, non-periodic time series (e.g., event-driven or sparse data)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aiRs4hite3", "forum": "mGdx3g7VxN", "replyto": "mGdx3g7VxN", "signatures": ["ICLR.cc/2026/Conference/Submission22520/Reviewer_ri67"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22520/Reviewer_ri67"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761229588385, "cdate": 1761229588385, "tmdate": 1762942256739, "mdate": 1762942256739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PRISM, a time-series forecaster that builds a unified time–frequency hierarchy. Concretely: the model (i) performs binary time partitioning with overlap to form a tree, (ii) applies a time–frequency decomposition (default: Haar DWT) at each node, (iii) computes band importance weights via summary statistics → 2-layer MLP → softmax, and (iv) optimizes a joint loss that couples forecasting (future) with reconstruction (past). Experiments on 8 datasets × 4 horizons report SOTA or competitive results (best MSE in 17/32 settings; best MAE in 18/32), with extensive ablations showing the contribution of the tree encoder, wavelets, importance MLP, reconstruction loss, and residual connections. The motivation is that real-world series exhibit multi-scale behavior (global trends, local fluctuations, and intermediate scales), so hierarchical representations should align long-term structure and fine-scale variability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Clear problem framing and gap statement. The paper argues that prior work typically builds hierarchy in only time or only frequency, or mixes domains without a reconstructable shared hierarchy.\n* Coherent architecture. Overlapped binary splits (time) + band partition (frequency) + learnable band routing + an explicit reconstruction path form a consistent design.\n* Broad empirical coverage and ablations. Results across 32 settings, with component-wise ablations showing 5–14% average performance drops when removing key pieces (tree depth, wavelets, importance MLP, reconstruction loss, residual connections).\n* Efficiency and interpretability. Training-time comparisons (e.g., ETTh1–96: 10 epochs in 65s) and band-importance visualizations support practical utility and model introspection."}, "weaknesses": {"value": "* (Primary) Limited conceptual novelty relative to recent multiscale “decompose–mix” lines. The high-level philosophy—multiscale decomposition and mixing—strongly overlaps with recent TimeMixer-style approaches. The paper does cite such work in Related Work (e.g., Ref. [20]), but the manuscript does not clearly establish a qualitative leap beyond “engineered combination” of known ideas (time hierarchy + frequency filters + learned weighting + auxiliary reconstruction). Claimed distinction is a reconstructable, shared time–frequency tree, yet the empirical section lacks head-to-head, controlled comparisons designed to isolate scenarios where this specific design strictly dominates competing multiscale methods.\n* Wavelet superiority appears under-analyzed. Table-2 shows average gains over FFT/EMA/DoG/MCD, but there is no conditioned analysis clarifying when wavelets lose/win based on spectral characteristics, periodicity, or multivariate correlations.\n* Robustness in realistic settings is thin. The paper focuses on public benchmarks; it lacks systematic tests under missing values, anomalies, distribution shift/drift, or longer non-stationary horizons.\n* Hyperparameter sensitivity is under-reported. No systematic sweep over overlap (o), tree depth, or number of bands (K) to reveal accuracy/time/memory trade-offs and boundary effects of the cross-fade concatenation."}, "questions": {"value": "1. Differentiate from TimeMixer-style work with controlled, apples-to-apples tests. Under identical pipelines/tuning/resources, can you provide direct, multi-dataset head-to-head results and analyses showing where and why the reconstructable time–frequency tree and band-importance routing deliver significant, consistent gains? Please include long-context, non-stationary, and low-resource regimes.\n2. When are wavelets the right choice? Offer a data-property ↔ filter mapping (e.g., by periodicity, noise spectrum, cross-channel dependencies). If possible, include learned basis experiments to test whether fixed Haar is limiting or optimal across conditions.\n3. Hyperparameter sensitivity. Provide thorough sweeps for overlap (o), tree depth, and band count (K), reporting accuracy/time/memory and any bleeding/edge artifacts due to overlap and cross-fade.\n4. Role of the reconstruction loss. Visualize the bias–variance trade-off between reconstruction and forecasting (e.g., gradients/importance by band as the reconstruction-loss weight varies). Do larger reconstruction weights ever suppress informative high-frequency bands?\n5. Robustness. Add evaluations for missingness, anomalies, covariate shift, and long-term drift, comparing to strong linear/transformer/multiscale baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ML7yMwxO7s", "forum": "mGdx3g7VxN", "replyto": "mGdx3g7VxN", "signatures": ["ICLR.cc/2026/Conference/Submission22520/Reviewer_8cUd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22520/Reviewer_8cUd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842568428, "cdate": 1761842568428, "tmdate": 1762942255575, "mdate": 1762942255575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper makes progress on unifying forecasting architectures using temporal hierarchies with methods using frequency modeling. The architecture splits the time series into temporal segments as a binary tree and applies a frequency filtering step at each hierarchy, along with residual connections at the frequency filtering steps. The final derived representations at the base of the binary tree are merged together using learned weights and a FFN is applied to generate the final predictions. The representations learn better due to the auxiliary reconstruction loss.\n\nExperimental results on popular benchmark datasets show improved performance compared to most baselines except for the D-PAD baseline which performs closely to the proposed method."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents an interesting technique to enhance hierarchical temporal architectures with frequency filtering. Frequency filtering seems to be an essential component of time series forecasting helping identify cyclical patterns in the dataset.\n- Experimental results show that the method performs strongly compared to compared to most baselines, and performs closely to D-PAD which is a more complicated architecture.\n- The paper is well presented and the ideas are quite clear. The experimental results are strengthened by the ablation study."}, "weaknesses": {"value": "- The main weakness of the paper is that the method performs closely to D-PAD and isn't a significant improvement compared to D-PAD, however D-PAD is a much involved architecture compared to the proposed method.\n- While, the paper evaluates on the most popular univariate datasets, some more complex datasets could be a newer addition such as M4 and wikipedia. While they are multi-variate datasets, they could help prevent overfitting of techniques on the existing datasets."}, "questions": {"value": "- Why are the residual connections only in the frequency filters module? Are they not useful in the other layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b2t6SoOOIl", "forum": "mGdx3g7VxN", "replyto": "mGdx3g7VxN", "signatures": ["ICLR.cc/2026/Conference/Submission22520/Reviewer_9JZA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22520/Reviewer_9JZA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762189959819, "cdate": 1762189959819, "tmdate": 1762942254713, "mdate": 1762942254713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Revisions"}, "comment": {"value": "We thank the Reviewers for their time and comments.\n\nWe are pleased that the Reviewers have found our approach interesting (9JZA, “interesting technique to enhance hierarchical temporal architectures”), coherent (8cUd, “coherent architecture”), and novel (ri67, “a novel hierarchical multiscale model”). We are grateful to see that the Reviewers have highlighted our model’s SOTA performance (9JZA, 8cUd,  ri67, e.g. “state-of-the-art performance on multiple benchmarks”), strong interpretability (8cUd, ri67, e.g. “learnable importance scores provide insights into which frequency components drive predictions”), and comprehensive ablations  (9JZA, 8cUd, e.g. “broad empirical coverage and ablations”) as strengths of our work. We are excited that all the Reviewers found our writing clear, well-presented, and easy to follow.  \n\n**Based on the Reviewer’s requests, we have made the following changes to the updated manuscript (highlighted in blue).**\n## 91 new dataset\n**Section 4.2** (page 5) adds **evaluation on the GIFT benchmark, containing 91 dataset**, which is ideal for providing extended evaluation requested by Reviewers 9JZA and 8cUd **including evaluation on multivariate data** and the M4 dataset. The 91 datasets in GIFT offer a more-than-tenfold expansion of our previous submission. On GIFT, we show that our method outperforms the competitors in the MSE on 61 datasets and in the MAE on 52 datasets (followed by the D-PAD that is the best on 21 and 25 datasets in MSE and MAE respectively). This result constitutes a 2-3 fold improvement over the D-PAD results, addressing a concern previously raised by the Reviewer 9JZA.\n## Anomalous / aperiodic data\n**Section 4.2** (page 5) uses GIFT to **analyze the performance of our model on irregular, aperiodic, incomplete (missing values), anomalous, non-stationary, and slowly-drifting time series**, as requested by Reviewers 8cUd and ri67. To this end, we selected datasets from the GIFT benchmark using their attached descriptions. We found that our model outperforms the D-PAD and DLinear, our closest competitors, in all 6 of these categories.\n\n*Numbers indicate how many datasets each model wins on (lower MSE/MAE).*\n| Model   | Overall MSE | Overall MAE | Irregular MSE | Irregular MAE | Aperiodic MSE | Aperiodic MAE | Incomplete MSE | Incomplete MAE | Anomaly MSE | Anomaly MAE | Nonstationary MSE | Nonstationary MAE | Drift MSE | Drift MAE |\n|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|\n| **PRISM (ours)** | **61** | **52** | **16** | **14** | **20** | **18** | **27** | **24** | **13** | **11** | **33** | **29** | **17** | **15** |\n| D-PAD   | 21 | 25 | 6 | 7 | 4 | 5 | 10 | 13 | 8 | 8 | 11 | 15 | 4 | 6 |\n| DLinear | 9 | 14 | 4 | 5 | 6 | 7 | 5 | 5 | 3 | 5 | 6 | 6 | 2 | 2 |\n## Comparison with TimeMixer\n**Section 4.1** (page 5) now **compares our model to TimeMixer, a similar model** that performs the multiscale decomposition in the time domain. Outperforming this model across benchmarks in a direct comparison supports the value of joint hierarchical decomposition in time and frequency.\n## Stability of features\n**Section 4.3** (page 7) provides a **case study for the importance scores on ETT datasets**, as requested by Reviewer ri67. The ETT datasets record the temperatures at 2 power plants, each at 2 timescales, thus enabling us to compare the importance scores (that we use for reweighting of the wavelet bands) on different yet related sources of data. We show that the importance scores are robust across tree segments, forecast lengths, numbers of wavelets, datasets recording similar events, and seeds. These results show that our model learns consistent data-driven representations despite the variability in the sources of similar data.\n## Wavelets vs FFT etc\n**Appendix A.1** (page 13) describes **how different feature bases (e.g., Wavelets vs. FFT) can be combined with our model**, as asked by Reviewers 8cUd and ri67. We show that the high performance of our method is supported by the unique structural pairing of our hierarchical splits in time and the Haar’s wavelets that use the same temporal decomposition scheme. We argue that other frequency bases are not as compatible with the hierarchical time decomposition: For example, the FFT would accumulate edge effects when computed on many short segments.\n## Analysis of hyperparameters\n**Appendix A.2** (page 14) provides **analysis of the model’s optimal hyperparameters** requested by Reviewer ri67. We argue that the model prioritizes the frequencies from the highest available (determined by the dataset’s sampling rate) to the lowest admissible (determined by the context window length). These insights guide the hyperparameter selection for new datasets based on their loss function, sampling rate, and the context window length.\n\n**Tables 9, 10, 11** contain the **hyperparameter sweeps**, including the overlap, depth of trees and number of the components. We have now added these results to the text based on the request made by the Reviewer 8cUd."}}, "id": "qByTvqb0i6", "forum": "mGdx3g7VxN", "replyto": "mGdx3g7VxN", "signatures": ["ICLR.cc/2026/Conference/Submission22520/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22520/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission22520/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763632178468, "cdate": 1763632178468, "tmdate": 1763652023076, "mdate": 1763652023076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}