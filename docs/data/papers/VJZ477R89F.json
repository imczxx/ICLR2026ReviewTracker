{"id": "VJZ477R89F", "number": 8208, "cdate": 1758074262330, "mdate": 1759897799926, "content": {"title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process", "abstract": "Online reinforcement learning (RL) has been central to post-training language models, but its extension to diffusion models remains challenging due to intractable likelihoods. Recent works discretize the reverse sampling process to enable GRPO-style training, yet they inherit fundamental drawbacks. These include solver restrictions, forward–reverse inconsistency, and complicated integration with classifier-free guidance (CFG). We introduce Diffusion Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that optimizes diffusion models directly on the forward process via flow matching. DiffusionNFT contrasts positive and negative generations to define an implicit policy improvement direction, naturally incorporating reinforcement signals into the supervised learning objective. This formulation enables training with arbitrary black-box solvers, eliminates the need for likelihood estimation, and requires only clean images rather than sampling trajectories for policy optimization. DiffusionNFT is up to $25\\times$ more efficient than FlowGRPO in head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves 0.95 with over 5k steps and additional CFG employment. By leveraging multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested.", "tldr": "We propose a new online reinforcement learning (RL) algorithm for diffusion and flow models based on forward process.", "keywords": ["Diffusion Models", "Reinforcement Learning", "Flow Matching"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9fcc7060db7e02b2f1e9341adaa221afdc1d0b5a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces an online RL framework for optimizing diffusion models via flow matching on the forward process of the diffusion model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents what is, to my knowledge, a novel approach to optimizing diffusion models via the forward process. The theoretical results seem sound, and DiffusionNFT shows drastic improvement in runtime efficiency over FlowGRPO. The approach can work with any solver for sampling, offering some generality/flexibility."}, "weaknesses": {"value": "The paper compares only to FlowGRPO as the fine-tuning baseline. Other baselines such as [1] are not considered. Additionally, in Section 3.3, the paper discussed CFG-free optimization. How CFG is integrated as an offline form of RL guidance is slightly unclear and could benefit from greater exposition. Is $v^{old}$ updated during this offline RL style procedure? \n\nWhy does DiffusionNFT seem to dominate the Model-Based scores but not Rule-Based?\n\n[1] Xue et al. “DanceGRPO: Unleashing GRPO on Visual Generation”"}, "questions": {"value": "See weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CF6ZQPWnOv", "forum": "VJZ477R89F", "replyto": "VJZ477R89F", "signatures": ["ICLR.cc/2026/Conference/Submission8208/Reviewer_w5yg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8208/Reviewer_w5yg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960185686, "cdate": 1761960185686, "tmdate": 1762920156355, "mdate": 1762920156355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DiffusionNFT, a new RL paradigm that optimizes diffusion models directly on the forward process w.r.t. a reward function. It contrasts the positive and negative generations to define an implicit policy improvement direction. DiffusionNFT is compatible with any black-box ODE solvers and does not need to estimate the likelihood. It demonstrates significant improvement on SD3.5."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. this paper reduces RL-based fine-tuning to supervised learning, which could be more stable and efficient compared to conventional Policy Gradient methods;\n2. the training process utilizes both positive and negative signals\n3. the empirical results demonstrate significant improvements"}, "weaknesses": {"value": "This paper optimizes the diffusion model on the forward process. While it brings several benefits, the distribution shift issue is not discussed in this paper. The training objective function is an expectation over the whole dataset. But the inference procedure only covers the region of trajectories from the positive samples. As a result, it's not very clear if the negative samples are used efficiently during training."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rBXW62Kjat", "forum": "VJZ477R89F", "replyto": "VJZ477R89F", "signatures": ["ICLR.cc/2026/Conference/Submission8208/Reviewer_niMD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8208/Reviewer_niMD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982201584, "cdate": 1761982201584, "tmdate": 1762920156093, "mdate": 1762920156093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to optimize diffusion models directly through forward process, opposed to prior works that will discretize the reverse process. The proposed algorithm yields a supervised learning objective for diffusion models fine-tuning as an off-policy RL paradigm, achieveing comparable performance to mainstream online RL based methods while using much less compute."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The general structure of this paper is clear. The introductions and preliminaries are clean and instinct. The paper explains clearly about the motivations, existing methods' limitations and the advantages of proposed methods.\n\n2. There are several insightful theorems proposed that characterizes the policy improvement direction and a tractable supervised learning objective for policy optimization. \n\n3. The experimental results are quite impressive in terms of achieveing comparable performance to the RL based counterparts like Flow-GRPO while using much less compute."}, "weaknesses": {"value": "1. There lacks important baselines for comparisons, e.g. iterative DPO based methods for diffusion models, which also proposed supervised learning based objectives sampled from on-policy data. Overall the Diffusion-NFT objectives are quite similar, more like some weighted version of iterative Diffusion KTO, thus it will be beneficial to discuss the contributions of this paper over these baseline methods including the direct comparison of performance.\n\n2. There lacks more recent models performance like FLUX and Video Generative Models, the reported results are only limited to Stable Diffusion Medium. Given the claimed efficiency of the proposed method, it seems practical to include the results of these other models.\n\n3. Some presentations and clarity of the paper still needs to be improved. There are many missing explanations of the equation elements, for example:\n* $o$ in Line 156 and $p_{\\pi_{\\text{old}}}$ in Line 161 are not explained. In addition, why introducing the two concepts of positive dataset and negative data in the online RL part, which seems irrelevant?"}, "questions": {"value": "1. The definition of reward functions in Line 156 is non standard. Why adopting such kind of reward and its optimality probability, opposed to the standard human satisfactory about the generations as an elo rating in existing works like DDPO and FlowGRPO? Specifically most existing reward models are also trained based on elo ratings, the current setup is a bit confusing. In addition to the pratical transformation in Section 3.3, ss there any theorectical equivalence or connection of the optimal solutions with the standard setup? \n\n2. Why the forward consistency is ensured? There is no direct proof on this part. Especially I do not see the straightfoward difference between Diffusion-NFT and RL based method in ensuring such, since $x_t$ sampled from adding noise to $x_0$ is not from the true trajectory that samples $x_0$ in Diffusion-NFT. It will be beneficial to see experiments to showcase the possible distriution shift of RL based methods compared to Diffusion-NFT."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1hg95hE2Id", "forum": "VJZ477R89F", "replyto": "VJZ477R89F", "signatures": ["ICLR.cc/2026/Conference/Submission8208/Reviewer_fF9P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8208/Reviewer_fF9P"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062410890, "cdate": 1762062410890, "tmdate": 1762920155764, "mdate": 1762920155764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}