{"id": "yqc1CMXetD", "number": 15942, "cdate": 1758257398402, "mdate": 1759897271834, "content": {"title": "Consistency Is Not Always Correct: Towards Understanding the Role of Exploration in Post-Training Reasoning", "abstract": "Foundation models exhibit broad knowledge but limited task-specific reasoning, motivating post-training strategies such as RL with verifiable rewards (RLVR) and inference scaling with outcome or process reward models (ORM/PRM). While recent work highlights the role of *exploration* and *entropy stability* in improving pass@K, empirical evidence points to a paradox: RLVR and ORM/PRM typically reinforce existing tree-like reasoning paths rather than expanding the reasoning scope, raising the question of why exploration helps at all if no new patterns emerge.  \n\nTo reconcile this paradox, we adopt the perspective of Kim et al. (2025), viewing easy (e.g., simplifying a fraction) versus hard (e.g., discovering a symmetry) reasoning steps as low- versus high-probability Markov transitions, and formalize post-training dynamics through Multi-task Tree-structured Markov Chains (TMC). In this tractable model, pretraining corresponds to tree expansion, while post-training corresponds to CoT reweighting. We provably show that several phenomena recently observed in empirical studies arise naturally in this setting: **(1)** RLVR induces a *squeezing effect*, reducing CoT entropy and forgetting some correct paths; **(2)** population rewards of ORM/PRM encourage consistency rather than accuracy, thereby favoring common patterns; and **(3)** certain rare, high-uncertainty CoTs by base model are responsible for solving hard problem instances.  \n\nTogether, these explain why exploration—even when confined to the base model’s tree scope—remains essential: it preserves access to rare but crucial CoTs needed for difficult cases, which are squeezed out by RLVR or unfavored by inference scaling. Building on this, we further prove that exploration strategies such as rejecting easy instances and KL regularization help preserve rare CoTs. Empirical simulations corroborate our theoretical results.", "tldr": "", "keywords": ["Tree-structured Markov Chain; Process Reward Model; RLVR; Squeezing Effect"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/65f3812787c3dd139007fb47d1eb4b8b44d67559.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Standard RL training methods sharpen and do not encourage exploration. The authors propose a theoretical framework, TMC, to model the sequential structure of reasoning, viewing pretraining as tree-graph discovering and post-training as Chain-of-Thought (CoT) reweighting. Within this framework, they distinguish between **easy-to-reason CoTs** (common, high-probability transitions) and **hard-to-reason CoTs** (rare, low-probability transitions), linking instance difficulty to the base model's pass rate.\n\nThe central contribution is the rigorous formalization and proof that standard post-training methods introduce a **simplicity bias**, thereby failing to maintain access to rare but crucial CoTs needed for difficult problem instances. Some experimentation is performed to validate the theoretical findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- clear presentation\n- rigorous proofs"}, "weaknesses": {"value": "All effects stated have been well-noted before:\n- the squeezing effect has already been noted many times in literature before, often referred to as sharpening [1]\n- entropy decrease has also been a phenomenon commonly noted [2]\n- \"certain rare, high-uncertainty CoTs by base model are responsible for solving hard problem instances\" isn't this true by definition of hard problems?\n\nMoreover, the strategies of rejecting easy instances [3] and KL regularization have also been explored before as ways to regulate exploration. Are you suggesting any novel strategies?\n\nLack of experiments to demonstrate the correctness of theoretical results. There is currently only a single set of experiments showing a toy task, but how do they hold up on Math, QA, coding, etc.? \n\n[1] https://arxiv.org/abs/2506.02355\n[2] https://arxiv.org/abs/2505.22617\n[3] https://arxiv.org/abs/2506.09026"}, "questions": {"value": "1. Entropy **does not** necessarily decrease when running on difficult problems. How would youaddress this class of fine-tuning?\n2. Is the goal of this paper to provide theory behind existing known phenomenons? If so, are there any training strategies outside of common existing ones that theory sheds light upon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FwQDIX9zFx", "forum": "yqc1CMXetD", "replyto": "yqc1CMXetD", "signatures": ["ICLR.cc/2026/Conference/Submission15942/Reviewer_vABH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15942/Reviewer_vABH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891510557, "cdate": 1761891510557, "tmdate": 1762926154054, "mdate": 1762926154054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper's core contribution is clearly identifying and modeling a key paradox: why exploration remains crucial even when the set of reasoning paths (the tree structure) is fixed. Through the TMC model, the paper establishes a direct link between hard/easy problems and the probability of rare/common CoT paths—a concise and powerful theoretical abstraction.\n\nOn this foundation, the paper discusses the failure modes of two primary post-training strategies:\n\n1. Training-Time (RLVR):Section 3 (Thm 2) details the Squeeze Effect of RLVR. The paper argues the root cause is the inherent gap in the advantage function (prop 1) between easy-to-reason and hard-to-reason CoTs. This gap causes gradient updates to systematically suppress the probability of hard-to-reason CoTs, i.e. the ratio of the probability of hard-to-reason CoTs and easy-to-reason CoTs strictly decreases, leading to them being forgotten.\n2. Inference-Time (Inference-Scaling): Section 4 (Thm 3) explores a separate, independent failure mode. **Even if hard-to-reason CoTs still exist), ORM/PRM reward models are inherently biased. Because they reward the Population Reward—the average correctness across all instances (Prop 2)—they systematically favor easy-to-reason CoTs (which have a higher average score). Consequently, when using BoN/BS on a hard instance, these reward models will incorrectly assign a high score to the easy-to-reason CoTs, causing the model to select the wrong answer and fail. This explains why, under such biased guidance, merely increasing the number of samples (like in BoN) cannot solve these hard problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper formalizes the intuitive paradox between fixed reasoning structures and the benefits of exploration. The TMC abstraction provides a clean mathematical model linking exploration, entropy, and rare CoTs.\n\n2. It systematically explains three well-observed empirical trends — RLVR entropy collapse, ORM/PRM consistency bias, and the importance of rare CoTs — under one coherent framework.\n\n3. Technical Novelty: The Doob's h-Transform-induced Process Reward Model in Sec 4 is quite technically novel. It provides a formal, constructive method to derive the exact step-wise process reward  that is mathematically guaranteed to generate samples from the optimal target Gibbs distribution. This connection between DPRM and heuristically designed PRMs is very insightful."}, "weaknesses": {"value": "1. The toy TMC simulation is conceptually clear but far from real LLM behavior. The gap between theory and practice (e.g., DeepSeek-R1, RLVR on GSM8K) remains large.\n\n2. The multi-task TMC assumes disjoint task trees and fixed edge probabilities, which may not capture shared latent reasoning states or adaptive token dynamics in large models."}, "questions": {"value": "no more"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pOh7jYRfO2", "forum": "yqc1CMXetD", "replyto": "yqc1CMXetD", "signatures": ["ICLR.cc/2026/Conference/Submission15942/Reviewer_y8Az"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15942/Reviewer_y8Az"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937115166, "cdate": 1761937115166, "tmdate": 1762926153712, "mdate": 1762926153712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an analytically tractable theoretical framework termed Multi-task Tree-structured Markov Chains (TMC) to address a fundamental paradox in the post-training reasoning of foundation models. Although post-training strategies such as reinforcement learning from verifiable rewards and reasoning scaling methods rely on maintaining exploration or entropy stability to enhance performance, they often merely reinforce existing reasoning trajectories rather than truly expand the reasoning space. Through the proposed TMC framework, this work provides a formal and rigorous analysis of this phenomenon."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper provides solid theoretical justification for exploration strategies such as “rejecting easy instances” and KL regularization.\n\n2.The concepts are clearly defined, and the writing is logically structured and easy to follow.\n3.The proposed multi-task TMC framework, which models pre-training as graph discovery and post-training as CoT reweighting, is an elegant attempt to formalize CoT reasoning. It effectively links the difficulty of reasoning steps with transition probabilities in a Markov chain, achieving theoretical tractability."}, "weaknesses": {"value": "1.The theoretical analysis relies on a simplified linear Softmax predictor and a constrained TMC topology. As acknowledged in the Humble Remark, there remains a gap between the proposed model and large-scale systems like GPT, with many unmodeled complexities. While tractability is valuable, such simplifications may limit the framework’s ability to capture the dynamics of real Transformer architectures and their vast, unstructured reasoning spaces.\n2.Could you further justify how the mechanisms underlying the observed “squeezing effect” and “consistency bias” in your model, which are based on a simple Softmax and advantage function, would still hold in nonlinear, high-dimensional, multihead Transformer architectures?\n3.The proposed Doob h transform based PRM (DPRM) is claimed to better preserve the capabilities of foundation models. Please briefly discuss its computational challenges and overhead, particularly in comparison with standard BoN or PRM approaches."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y4efchWy8u", "forum": "yqc1CMXetD", "replyto": "yqc1CMXetD", "signatures": ["ICLR.cc/2026/Conference/Submission15942/Reviewer_j2oD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15942/Reviewer_j2oD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762091067377, "cdate": 1762091067377, "tmdate": 1762926153312, "mdate": 1762926153312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}