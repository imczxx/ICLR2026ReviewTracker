{"id": "phw13aNM08", "number": 9830, "cdate": 1758142686539, "mdate": 1759897693063, "content": {"title": "MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding", "abstract": "The fundamental challenge in multimodal understanding lies not merely in processing individual modalities, but in discovering optimal strategies for their semantic unification across vastly different representational spaces; current approaches maintain separate encoders for each modality, leading to semantic fragmentation and computational inefficiency, and we present MANTA (Multimodal Abstraction and Normalization via Textual Alignment), a theoretically grounded framework that reconceptualizes multimodal integration as an information-theoretic optimization problem with provable guarantees, showing that natural language serves as a universal semantic bridge and providing three theoretical contributions—(1) hierarchical linguistic projection achieves (1−ϵ)-optimal information preservation, (2) cross-modal contrastive alignment converges to maximal mutual information with rate O(1/√T), and (3) our retrieval mechanism achieves the best trade-off between relevance and diversity—while guiding practical algorithms for multi-scale representation learning, information-theoretic content selection, cross-modal semantic alignment, and retrieval-augmented generation, validated by extensive experiments on long-form video understanding with unprecedented gains (22.6% on Video-MME, 27.3% on videos over 30 minutes, 25.1% on cross-modal reasoning), establishing new theoretical foundations for linguistic abstraction as a unifying principle for multimodal AI with implications for robotics, embodied AI, and human-computer interaction.", "tldr": "MANTA unifies visual and auditory inputs into structured textual representations through information-theoretic optimization, achieving up to 22.6% improvement over state-of-the-art models on long-form multimodal understanding tasks.", "keywords": ["Multimodal Understanding", "Information-Theoretic Optimization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e77625929423c34571c1454568078d6767fd578.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present MANTA, a framework for multimodal understanding that projects various modalities into a unified linguistic space, supported by theoretical guarantees and empirical improvements on long-form video tasks. While the idea of linguistic abstraction is interesting and the experiments are extensive, the paper has several shortcomings that prevent a stronger recommendation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper proposes a theoretically motivated multimodal abstraction framework that operates through a shared linguistic bottleneck, which is an elegant and interpretable idea.\n\n2.\tThe multi-scale hierarchical design (micro / meso / macro) is well-motivated and empirically beneficial.\n\n3.\tThe theoretical justification of temporal scale selection (via power-law correlation) provides conceptual depth in multimodal work.\n\n4.\tStrong performance improvements are demonstrated on multiple long-form video benchmarks, indicating empirical effectiveness."}, "weaknesses": {"value": "1.\tContribution Attribution Between Framework and Backbones: While the multi-LLM evaluation convincingly shows that MANTA's benefits are model-agnostic, the paper does not fully disentangle the contribution of the novel linguistic abstraction framework from the sheer power of the large pre-trained encoders (CLIP, TimeSformer, Whisper) it builds upon.\n\n2.\tUnconvincing and Potentially Misleading Efficiency Claims: In Appendix G, the authors report that MANTA achieves higher computational efficiency (423.8 GFLOPs vs. baseline’s ~800 GFLOPs). However, it is unclear whether this count includes the forward-pass cost of all large pretrained backbones (CLIP-ViT-L, TimeSFormer, VideoMAE, Whisper-Large). The combined computational footprint of these modules is likely orders of magnitude higher. If the comparison only measures MANTA’s “lightweight control logic” while excluding backbone computation, the claim of efficiency is not directly comparable and potentially misleading..\n\n3.\tInsufficient Details for Reproducibility: Key components of the proposed system are described as black boxes. Most notably, the audio processing pipeline (mentioned in Appendix Fig. 3) lacks essential implementation details (e.g., specific models for Speaker Diarization and Topic Modeling, and their integration logic). This makes it very difficult to reproduce a core part of the framework.\n\n4.\tLimited Scope of \"Unified\" Claim: The paper ambitiously positions MANTA as a framework for \"unified multimodal understanding.\" However, the experimental validation is currently confined to the audio-video modality pair. The broad claim of universality is not fully supported by a lack of experimental demonstration on other fundamental modality pairs (e.g., image-text), which restricts the generality of the conclusion."}, "questions": {"value": "1.\tPlease clarify exactly what is included in the FLOPs calculation for MANTA. If it does not include the cost of the pre-trained backbones, the efficiency claim must be reframed, and a fairer, end-to-end comparison should be provided.\n\n2.\tCould you provide detailed implementation specifics for the audio processing modules (Speaker Diarization, Topic Modeling) to ensure reproducibility?\n\n3.\tHave you considered extending evaluation to cross-modal retrieval or other modality pairs beyond video + audio?\n\n4.\tWill you release the code, model weights, and training configurations to ensure reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I5HCSNLFW5", "forum": "phw13aNM08", "replyto": "phw13aNM08", "signatures": ["ICLR.cc/2026/Conference/Submission9830/Reviewer_7jt6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9830/Reviewer_7jt6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477140925, "cdate": 1761477140925, "tmdate": 1762921312391, "mdate": 1762921312391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MANTA, which conceptualizes natural language as a universal semantic bridge — a comprehensive representation space intended to preserve essential information from any sensory modality while enabling efficient reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The core idea is interesting and promising. Treating natural language as a universal semantic substrate is an appealing conceptual direction that could enable cross-modal reasoning."}, "weaknesses": {"value": "- The manuscript contains long blocks of text without clear sectional structure, which severely undermines readability. \n\n- The paper appears to have relied heavily on large language models (LLMs) in its writing and possibly in experimental components, but there is no explicit statement about which LLMs were used, how they were used, or what role they played. The authors should include a clear declaration describing any LLM usage during manuscript preparation.\n\n- There are numerous punctuation and formatting mistakes (for example, in the column headers of Table 1) that make the paper hard to read and interpret. These errors suggest the manuscript has not been thoroughly proofread. I strongly recommend the authors perform careful copyediting and proofreading prior to resubmission."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZPnSp2zYDx", "forum": "phw13aNM08", "replyto": "phw13aNM08", "signatures": ["ICLR.cc/2026/Conference/Submission9830/Reviewer_Xr1M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9830/Reviewer_Xr1M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905996398, "cdate": 1761905996398, "tmdate": 1762921312150, "mdate": 1762921312150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The MANTA framework redefines multimodal AI by first translating all sensory inputs, like video and audio, into a unified natural language space. It uses information theory to intelligently summarize the most critical moments from long videos into concise text, enabling large language models to perform complex reasoning with state-of-the-art accuracy and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- It demonstrates exceptional capability in processing long-form videos, a key bottleneck for traditional models.\n- It offers high interpretability, as its intermediate outputs are human-readable text, making the model's reasoning process transparent.\n- The approach shows outstanding performance and acts as a universal enhancer that can boost various existing models.\nIt achieves high efficiency and scalability by compressing high-dimensional pixel data into low-dimensional text representations."}, "weaknesses": {"value": "- The pipeline architecture is susceptible to cascading errors, where a mistake in an early stage propagates through the system.\n- The \"linguistic bottleneck\" may filter out subtle, non-verbal information that is difficult to describe accurately in words."}, "questions": {"value": "- On the Information Bottleneck Trade-off: While the paper proves near-optimal information preservation, does MANTA face a fundamental ceiling for tasks that rely on information ill-suited for language (e.g., subtle differences in artistic brushstrokes)? Could a hybrid mechanism be designed to bypass the linguistic bottleneck and access raw features when necessary?\n- On Error Feedback and Correction: Given the risk of cascading errors, could a feedback loop be implemented? For instance, if the final LLM detects logical inconsistencies in the textual descriptions, could it request the upstream modules to re-analyze specific video segments, enabling a more dynamic and self-correcting understanding process?\n- On the Granularity of Learning: The framework uses three fixed temporal scales. How well does this fixed hierarchy apply to all video types? Could the system learn to dynamically and adaptively adjust its scales of analysis based on the video's content, such as using finer scales for fast-paced sports and coarser scales for slow-paced documentaries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uk5G3GEj0L", "forum": "phw13aNM08", "replyto": "phw13aNM08", "signatures": ["ICLR.cc/2026/Conference/Submission9830/Reviewer_e4hm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9830/Reviewer_e4hm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762331957861, "cdate": 1762331957861, "tmdate": 1762921311871, "mdate": 1762921311871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MANTA projects multimodal video/audio into unified linguistic representations for long-form video understanding. Claims three theoretical contributions on information preservation, alignment convergence, and retrieval optimality. Shows 22.6% improvement on Video-MME, especially strong on long videos (27.3% on 30+ min videos)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Novel approach: Linguistic abstraction as universal semantic bridge is conceptually interesting\nStrong empirical results: Consistent improvements across baselines, particularly on long videos\nTheoretical grounding: Three formal theorems provide principled foundation\nComprehensive evaluation: 1,700 videos, multiple benchmarks, detailed ablations"}, "weaknesses": {"value": "1. Experimental Issues\n\nNew benchmark \"LongVU-QA\" is undocumented: 500 videos, 3,000 questions introduced but no validation, annotation details, or availability\nMissing train/test split documentation: Potential data leakage concerns\nOutdated baselines: Only compares to GPT-4V (2023), missing GPT-4o, Gemini 1.5 Pro, Claude 3.5\nUnclear baseline integration: How is proprietary GPT-4V/Gemini \"augmented\" with MANTA?\n\n2. Theoretical Overclaiming\n\nTheorem 2: (1-1/e) submodular approximation is textbook result, not novel\nTheorem 3: O(1/√T) SGD convergence is standard optimization theory\nTheorem 1: Relies on unvalidated assumptions (power-law correlations with specific α)\nMissing empirical ε quantification: Claims (1-ε)-optimal but never measures actual information loss\n\n3. Critical Missing Ablations\n\nNo direct comparison: Linguistic projection vs. standard multimodal fusion (cross-attention)\nNo scale sensitivity: Why exactly 3 scales? What about 2, 4, or 5?\nNo information loss analysis: How much semantic information is lost in text projection?\n\n4. Computational Cost Misleading\n\nTable 7 claims efficiency but ignores cost of running 6+ models (CLIP, TimeSFormer, VideoMAE, Whisper, AudioCLIP, multiple LLMs)\nTraining: 7 days on 8×A100, 2,800kg CO2 - not reproducible for most researchers\nFLOPs calculation incomplete\n\n5. Presentation Problems\n\nUnderutilized space: Paper stops at page 8 line 431, doesn't reach 9-page limit\nPoor figure integration: Figure 3 too complex, important results buried in appendix\nIncomplete details: \"Learned projection heads\" mentioned but never described"}, "questions": {"value": "1.Can you provide complete LongVU-QA documentation and make it public? Train/test split, annotation protocol, inter-annotator agreement?\n2.What is the empirical value of ε (information loss)? Your theory bounds it but you never measure it.\n3.How does linguistic projection compare to direct fusion? Need ablation: MANTA vs. standard cross-attention multimodal fusion.\n4.How is MANTA integrated with proprietary models (GPT-4V, Gemini) that you don't have access to?\n5.Can you provide honest computational accounting? Include all 6+ model costs, not just final inference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mXE0AfPItG", "forum": "phw13aNM08", "replyto": "phw13aNM08", "signatures": ["ICLR.cc/2026/Conference/Submission9830/Reviewer_fLwY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9830/Reviewer_fLwY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762831418727, "cdate": 1762831418727, "tmdate": 1762921311659, "mdate": 1762921311659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}