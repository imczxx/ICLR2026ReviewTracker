{"id": "LKv3bx610K", "number": 11683, "cdate": 1758203056904, "mdate": 1763730789595, "content": {"title": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization", "abstract": "The significant progress of large language models (LLMs) has led to remarkable achievements across numerous applications. However, their ability to generate harmful content has sparked substantial safety concerns. Despite the implementation of safety alignment techniques during the pre-training phase, recent research indicates that fine-tuning LLMs on adversarial or even benign data can inadvertently compromise their safety. In this paper, we re-examine the fundamental issue of why fine-tuning on non-harmful data still results in safety degradation. We introduce a safety-aware probing (SAP) optimization framework designed to mitigate the safety risks of fine-tuning LLMs. Specifically, SAP incorporates a safety-aware probe into the gradient propagation process, mitigating the model's risk of safety degradation by identifying potential pitfalls in gradient directions, thereby enhancing task-specific performance while successfully preserving model safety. Our extensive experimental results demonstrate that SAP effectively reduces harmfulness below the original fine-tuned model and achieves comparable test loss to standard fine-tuning methods. Our code is available in the supplementary materials.", "tldr": "", "keywords": ["LLMs", "supervised fine-tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5715da9415975d7058d148e157db279a8d445c51.pdf", "supplementary_material": "/attachment/401e89a5d0e6dbc3f97bbd4b734ce7569afd2e04.zip"}, "replies": [{"content": {"summary": {"value": "The paper solves the LLM safety degradation caused by fine-tuning. It hypothesizes that usefulness-critical and safety-critical gradient directions are entangled and proposes Safety-Aware Probing (SAP).  At each step, SAP estimates a safety-critical direction using a contrastive safety loss, constructs a “harmful” update, and then learn a small hidden-state probe that maximizes a safe-useful loss, encouraging downstream weight updates to avoid harmful regions. Across three 7b models on benign and adversarial tasks, SAP reduces harmfulness scores while keeping task loss/metrics near SFT. Additional experiments demonstrate the effectiveness of different settings. However, the scalability is not validated and remains questionable."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed SAP is well motivated. The hypothesis is validated by experiments.\n2. Experiments over 3 models and several datasets in different settings demonstrate promising performance.\n3. It provides sensible ablations (e.g., which layers to probe and learning-rate sensitivity) and report costs (time and memory)."}, "weaknesses": {"value": "1. The first claimed contribution of validating the hypothesis has been explored in prior works, such as [1, 2]. The cosine similarity approach is similar to SafeLora [1].\n\n2. The authors claim in the introduction that their work “*has better scalability since it can be incorporated into various fine-tuning paradigms rather than being limited to LoRA.*”  However, this claim is not supported by experimental evidence, such as larger models or fully fine-tuning. All experiments are conducted on 7B models with LoRA.  Furthermore, this claim is questionable, as SAP requires extra gradient estimation, which is related to the number of trainable parameters.\n\n3. The time overhead of SAP is non-trivial, approximately 2.5 times that of all baselines. While Appendix B.5 includes a comparison with LISA, the discussion does not adequately address the overhead relative to other baselines. Considering the performance gain, the practical value of applying this method remains questionable.\n\n4. Figure 1 lacks clarity and should be improved to better convey the procedure of SAP.\n\n\n**Reference**\n\n[1]  Safelora: The silver lining of reducing safety risks when finetuning large language models. NeurIPS. 2024\n\n[2] Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets. ICML. 2025"}, "questions": {"value": "1. What is the time cost for SAP when applying to larger models and fully fine-tuning settings?\n2. What is the portion or number of safety examples for baselines, such as safeinstr?\n3. Why not use fewer, smaller probe layers instead of probing 10 layers, given the high time cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bri0nAmdDr", "forum": "LKv3bx610K", "replyto": "LKv3bx610K", "signatures": ["ICLR.cc/2026/Conference/Submission11683/Reviewer_PBzG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11683/Reviewer_PBzG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922390005, "cdate": 1761922390005, "tmdate": 1762922733882, "mdate": 1762922733882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that large language models can lose their safety even when fine-tuned on harmless data, because the gradients that improve task performance are often entangled with those that reduce safety. To address this, the authors propose Safety-Aware Probing (SAP), a training method that adds a small hidden-state probe to steer optimization away from harmful directions while still improving task performance. SAP does not require changing the dataset or model architecture and works across different fine-tuning setups."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The experiments are extensive and well designed, have evaluations on three different models, three instruction-following datasets, five reasoning benchmarks, and poisoned, adversarial fine-tuning settings. \nThe paper is also clearly written and well-organized."}, "weaknesses": {"value": "* Gradient analysis and evaluation are somewhat limited. The paper shows cosine similarity between usefulness and safety gradients, but does not fully explain why the directions align or provide deeper theoretical insight. The Harmful score relies on a single moderation model, with no additional metrics such as jailbreak success rate, or LLM-as-judge evaluation. \n* Cost analysis is needed. SAP increases training time by 2x~3x, but the paper briefly labels this as acceptable without discussing practical implications or scalability. \n* Limited discussion of adaptive attacks. The adversarial fine-tuning experiment does not consider attackers aware of SAP."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kCxOR1iDMm", "forum": "LKv3bx610K", "replyto": "LKv3bx610K", "signatures": ["ICLR.cc/2026/Conference/Submission11683/Reviewer_kkBd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11683/Reviewer_kkBd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762139930482, "cdate": 1762139930482, "tmdate": 1762922733521, "mdate": 1762922733521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Safety-Aware Probing (SAP), a lightweight optimization that inserts a small safety-aware probe into hidden states during gradient propagation to steer updates away from harmful directions while preserving task utility. SAP is motivated by an observed entanglement between safety-critical and usefulness-critical gradient directions; it maximizes a safe-useful objective to find a probe  that biases each update toward safer regions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is simple yet principled: it treats safety as gradient-space steering and remains broadly compatible with standard fine-tuning.\n- Empirically, it improves utility while reducing harmfulness, increases robustness to poisoning and adversarial fine-tuning, and composes with other defenses, making it practical for deployment."}, "weaknesses": {"value": "The proposed method introduces too many hyperparameters (e.g., α, β, ϵ, probe layers), which increases tuning complexity and reduces reproducibility."}, "questions": {"value": "Q1. Did you mean the following?\n\n“Our experiments show that SAP achieves better useful loss while significantly **decreasing model safety”**\n\n→ “Our experiments show that SAP achieves better useful loss while significantly **improving model safety**”\n\nQ2. Could you clarify the captions for Figures 2 and 3?\n\n- Figure 2 does not specify which harmful or useful\ndatasets were used.\n- Figure 3 does not clarify the definition of the useful-critical notation nor specify which harmful dataset was used.\n\nQ3. Are $\\alpha$, $\\beta$, $\\epsilon$, and probe layers the same for every dataset?\n\nQ4. Is there a reason for choosing 2,000 examples for $D_{useful}$ and 50 for $D_{safe/harmful}$?\n\nQ5. Aren’t these models instruction-tuned models, such as Llama-7B-Chat?\n\nQ6. Why are the Booster and Vaccine baselines not included in the results after Table 1?\n\nFormatting issue:\n\n- The repeated inclusion of the Alpaca dataset in Table 2 is redundant."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gsQ8qdD1hj", "forum": "LKv3bx610K", "replyto": "LKv3bx610K", "signatures": ["ICLR.cc/2026/Conference/Submission11683/Reviewer_3MFr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11683/Reviewer_3MFr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181340017, "cdate": 1762181340017, "tmdate": 1762922733186, "mdate": 1762922733186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the safety alignment problem for LLMs. Specifically, it proposes a method to defend against malicious fine-tuning samples while keeping the model's utility score. What lies in the core the proposed method is to find a small parameter perturbation which discourages moving along harmful gradient direction when optimizing for the utility loss. The method is able to mitigate harmful gradient direction in utility loss gradient while not affecting the utility loss too much. Experimental results demonstrate the effectiveness of the proposed algorithm."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important problem in LLM alignment.\n2. The paper is well-written and easy to follow.\n3. This work offers some valuable insight by showing the correlation between the descend of the utility loss and that of the harmful loss. The proposed method is somewhat intuitive and easy to implement. The reported experimental result is good."}, "weaknesses": {"value": "1. Insights of how and why the proposed method works: Though the construction of $L_{su}$ is somewhat intuitive, it is still unclear how and why the algorithm works well. Specifically, how can the algorithm achieve lower utility loss than the full SFT on utility dataset, while being biased constantly (from the perturbation) in its utility optimization process? If the perturbation is supposed to be very small, how can it lead to substantial decrease in harmfulness?\n\n2. Lacking critical baselines: it might be beneficial for the author to compare with simply adding the safety data (used in the proposed algorithm to compute negative harmful direction) into the utility dataset. The baseline can be interpreted as simply mixing the utility gradient with the negative harmful direction, which is very comparable to the proposed algorithm. This might lead to further insight into the performance of the algorithm.\n\nAs a result, I am overall hesitant to give an accept suggestion. However, I am happy to reconsider my recommendation if they are adequately addressed."}, "questions": {"value": "1. In the proposed algorithm, is perturbation applied to the model parameter each iteration? If not, will it be a good/bad idea to apply the perturbation?\n\n2. How will the algorithm perform under no malicious data? I am considering a scenario where this method is just applied to enhance safety in normal utility fine-tuning process."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no ethics concern."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2XTyVgUrsy", "forum": "LKv3bx610K", "replyto": "LKv3bx610K", "signatures": ["ICLR.cc/2026/Conference/Submission11683/Reviewer_Avsu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11683/Reviewer_Avsu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762256141337, "cdate": 1762256141337, "tmdate": 1762922732642, "mdate": 1762922732642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}