{"id": "L3Or2mhuCH", "number": 15182, "cdate": 1758248640125, "mdate": 1763703697542, "content": {"title": "A Block Coordinate Descent Method for Nonsmooth Composite Optimization under Orthogonality Constraints", "abstract": "Nonsmooth composite optimization with orthogonality constraints has a wide range of applications in statistical learning and data science. However, this problem is challenging due to its nonsmooth objective and computationally expensive, non-convex constraints. In this paper, we propose a new approach called \\textbf{OBCD}, which leverages Block Coordinate Descent to address these challenges. \\textbf{OBCD} is a feasible method with a small computational footprint. In each iteration, it updates $k$ rows of the solution matrix, where $k \\geq 2$, by globally solving a small nonsmooth optimization problem under orthogonality constraints. We prove that the limiting points of \\textbf{OBCD}, referred to as (global) block-$k$ stationary points, offer stronger optimality than standard critical points. Furthermore, we show that \\textbf{OBCD} converges to $\\epsilon$-block-$k$ stationary points with an iteration complexity of $\\mathcal{O}(1/\\epsilon)$. Additionally, under the Kurdyka-Lojasiewicz (KL) inequality, we establish the non-ergodic convergence rate of \\textbf{OBCD}. We also demonstrate how novel breakpoint search methods can be used to solve the subproblem in \\textbf{OBCD}. Empirical results show that our approach consistently outperforms existing methods.", "tldr": "", "keywords": ["Orthogonality Constraints; Nonconvex Optimization; Nonsmooth Composite Optimization; Block Coordinate Descent; Convergence Analysis"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2cabea6a5dece7bd6cdc2c60ccb4dd7907402c8c.pdf", "supplementary_material": "/attachment/f8f00f7499cb1b2cbe1b958211977972dc9e40da.zip"}, "replies": [{"content": {"summary": {"value": "A new approach called OBCD is proposed for minimizing the sum of a smooth and a nonsmooth convex functions over the nonconvex set of orthogonal matrices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Optimization under orthogonality constraints is an important topic with many applications. The paper is well written. My expertise of the topic is limited, though. I know about projection methods, as there is a surge of interest for orthogonalization recently in the context of the Muon optimizer, see for instance Grishina et al. \"Accelerating Newton-Schulz Iteration for Orthogonalization via Chebyshev-type Polynomials\". The proposed method maintains feasibility throughout the process, which is a different approach than relying on approximate orthogonalization."}, "weaknesses": {"value": "I don't see limitations."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Me83E95duK", "forum": "L3Or2mhuCH", "replyto": "L3Or2mhuCH", "signatures": ["ICLR.cc/2026/Conference/Submission15182/Reviewer_X2XH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15182/Reviewer_X2XH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760899531881, "cdate": 1760899531881, "tmdate": 1762925489131, "mdate": 1762925489131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an algorithm for minimizing the sum of a smooth and nonsmooth function on the Stiefel manifold.\nRelevant applications include sparse Principal Component Analysis (PCA) with l1 norm and l0 norm regularization.\nThe algorithm is a descent method, and features a block coordinate update, which requires computing the corresponding coordinate block of gradient, as opposed to the full gradient.\nIn theory, the algorithm converges in expectation to a relevant point with classical complexity (Th. 4.2), and under additional assumptions (including continuity of $h$, which rules the important l0 norm example and leaves l1 norm) the last iterate converges to a relevant point with classical complexity. We note that the notion of \"relevant point\" is slightly stronger than the usual notion of critical point (0 in limiting subdifferential).\nIn practice, the algorithm performs well on PCA with l0 regularization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- the submission is clearly written,\n- the problem is motivated by a relevant application (sparse PCA with l0 and l1 norm regularization),\n- the algorithm is derived and formulated in a rigorous and clear way (up to one detail, discussed below),\n- the theoretical analysis presents a notion of stationary points (BS$k$-points) that is stronger than the customary critical point (zero in subdifferential), and a proof that the algorithm converges to such points on average. I am quite familiar with composite optimization, but less so on block coordinate schemes so this idea may be common in that field.\n- the algorithm combines known ideas (block coordinate descent, majorization minimization) suitably, and successfully applies these ideas to minimization with l0 norm regularization. As far as I know, this task is highly challenging, and few methods for it are available even in the unconstrained case."}, "weaknesses": {"value": "Here is a list of points that prevent me from providing a better assessment to the submission.\n- The nonsmooth function $h$ has strong, restrictive assumptions. This severely limits the applicability of the method.\n  + Assumption (ii): $h$ is assumed to be coordinate separable, with the same expression for each coordinate.\n  + Assumption (iii) on function $h$ is shown to be tractable in theory for three values of $h$ only (the $\\ell1$ norm, the indicator of a polyhedron, and the $\\ell0$ norm when $k=2$).\n- l. 122-128: I disagree with part of the positioning relative with the literature:\n  + contrary to what point (ii) implies, I think that the proposed method is not applicable to general nonsmooth composite problems. Indeed it relies on an assumption on the nonsmooth function, which is arguably restrictive (see point below) but certainly not valid for \"general nonsmooth composite problems\".\n  + point (iii) asserts that it is a \"limitation\" for methods to be \"infeasible\", that is to generate iterates that are only asymptotically feasible. Yet, infeasible methods are accepted for solving large-scale optimization problems under general constraints (e.g. with the Augmented Lagrangian method), and Stiefel manifold constraints specifically (e.g. with the recent Landing methods, see next point).\n  + point (iv), \"they often lack rigorous convergence guarantees\" should be more precise on the type of convergence guarantees that previous methods fail (for instance: convergence of average, last iterate, complexity guarantee).\n- l. 78-107: Literature review misses one recent line of work of so-called \"landing methods\" for optimization on Stiefel manifolds: [1-4]. These methods also target large-scale minimization on the Stiefel manifold; the paper should discuss these methods.\n- Algorithm 1: The algorithm seems to depend on a parameter $\\alpha$ (l. 194, Lemma 2.3, Th. 4.2, among other occurrences), yet this parameter does not appear in the algorithm statement, and it is unclear what condition $\\alpha$ should meet so that the convergence guarantees hold, and particularly so the sufficient decrease condition of Theorem 4.2(a).\n- l. 374: the condition $\\| \\partial h(X) \\|_{sp}<l_h$ is ill-posed: the \"sp\" norm of a set is not defined. Besides, this condition may appear more natural if it were implied by Lipschitz continuity of $h$.\n- l. 392: the assumption that $F_i$ is a KL function is not discussed at all. It is thus unclear whether it holds for any of the application cases, and the three discussed $h$ functions.\n- l. 395: Proposition 4.8, from previous work, is stated without providing a precise reference, that is, both the paper and statement references.\n- l. 404 & 412: \"the continuity assumption made in lemma 4.4\" is unclear. To what condition does this sentence refer to exactly? Maybe writing an additional assumption for this would clarify the situation.\n- Experiments: it is unclear whether the three baselines generate feasible iterates. In addition, these three baselines involve operator splitting. Such methods usually involve a reformulation of the problem with additional constraints, which are satisfied asymptotically only. Finally, it is not clear whether the three baselines have convergence guarantees on  the problem with $h = \\| \\cdot \\|_{0}$, which is discontinuous; this aspect conditions the interpretation of the whole experimental section.\n\nReferences:\n- [1] Goyens, Absil & Feppon (2026) Geometric Design of the Tangent Term in Landing Algorithms for Orthogonality Constraints, Springer Nature Switzerland.\n- [2] Ablin & Peyré (2022) Fast and Accurate Optimization on the Orthogonal Manifold without Retraction, PMLR.\n- [3] Vary, Ablin & Gao et al. (2024) Optimization without Retraction on the Random Generalized Stiefel Manifold, PMLR.\n- [4] Gao, Vary & Ablin et al. (2022) Optimization Flows Landing on the Stiefel Manifold$\\star$, IFAC-PapersOnLine.\n\n\nMinor points, that do not impact the assessment of the paper:\n- l. 35: clarity would improve by mentioning explicitly that $f$ is assumed to be differentiable and $H$-smooth\n- l. 39, footnote: Is $f$ assumed to have expression $1/2 \\|X\\|_{H}^{2}$ on the whole paper?\n- l. 40 & 296: what are the definitions of $F$ \"closed\", and $F$ lower semicontinuous? How do they differ?\n- l. 230: is the\n- l. 305: The writing of Definition 3.3 is confusing: the sentence \"Furthermore $\\lambda \\in [\\partial F(X)]^\\top X$\" can read as an additional condition for a point $X$ to be critical, but the notion  of criticality does not involve $\\lambda$.\n- l. 313-318: what is the purpose of this paragraph?"}, "questions": {"value": "Any comment and detail on each of the listed weak points is welcome.\n\nBelow are some suggestions in the form of questions; any answer on the following questions is welcome, but no answer is also fine.\n- connection to (variable metric) proximal gradient method? That would connect assumption (iii) to the notion of \"prox-friendly\" nonsmooth function $h$, standard in nonsmooth optimization. That may also help and connect\n- Th. 3.6: globally optimal points are BS2-points. Are they also BSk points, for $2 \\le k \\le n-1$?\n- Assumption (iii) is reminiscent of the  might appear more natural if connected to the proximal gradient operator.\n- optimization methods on problems with l0-norm regularization usually face the combinatorial difficulty of the l0 norm in some way. I am surprised that it doesn't show in your analysis. Do you have some intuition to share on this? I am also surprised that the complexity of OBCD with parameter $k$ in Theorem 4.2 does not depend on $k$. Again, do you have some intuition to share on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PrGOAXJwdZ", "forum": "L3Or2mhuCH", "replyto": "L3Or2mhuCH", "signatures": ["ICLR.cc/2026/Conference/Submission15182/Reviewer_Jeff"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15182/Reviewer_Jeff"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761585798464, "cdate": 1761585798464, "tmdate": 1762925488566, "mdate": 1762925488566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the problem of minimizing a nonconvex nonsmooth function $F(X)$ over the space of $n \\times r$ orthogonal matrices (the Stiefel manifold $St(n,r)$). This is a challenging class of problems with wide applications. Common approaches for such problems include projection-based methods, Riemannian methods using tangent-space surrogates and retractions, or Block Majorization-Minimization (BMM) methods that iteratively minimize a surrogate directly on the manifold.\n\nThe authors propose an approach, **OBCD**, which falls under the BCD/BMM umbrella but with a novel and distinct design. Instead of updating a manifold block $X_k$ directly, the method subsamples $k$ rows and finds a small $k \\times k$ orthogonal matrix $V$ that *transforms* this block. This \"row-wise\" update ($X^{t+1}(\\mathcal{B},:) \\leftarrow \\overline{V}^{t}X^{t}(\\mathcal{B},:)$) is a key contribution, as it is inherently feasible and avoids the standard tangent-space/retraction machinery.\n\nWhile the general BCD/BMM idea is known, this paper's novelty lies in the specifics of its framework for the Stiefel manifold:\n\n1.  It defines a new optimality condition, the **\"block-k stationary point\" ($BS_k$-point)**.  Theorem 3.6 shows that this condition is **stronger** than the standard critical point condition. The authors justify this by showing their $k=2$ solver uses both rotations and reflections, allowing it to escape suboptimal points.\n\n2.  It provides a **constructive and exact solver** for its nonsmooth subproblem. Appendix B introduces a novel \"Breakpoint Searching Method (BSM)\" that finds the *exact global solution* for the $k=2$ subproblem with $l_0$, $l_1$, or non-negativity regularizers. This is a non-trivial technical result that provides a solid foundation for the algorithm.\n\nOn this theoretical foundation, the authors derive a comprehensive convergence analysis, including an $\\mathcal{O}(1/\\epsilon)$ iteration complexity for an $\\epsilon$-$BS_k$-point (Theorem 4.2) and a full non-ergodic (last-iterate) convergence analysis under the KL property (Theorem 4.10). The presentation is clear, with assumptions formally stated, the algorithm well-defined, and the theoretical claims rigorously established."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors study the problem of minimizing a nonconvex nonsmooth function $F(X)$ over the space of $n \\times r$ orthogonal matrices (the Stiefel manifold $St(n,r)$). This is a challenging class of problems with wide applications. Common approaches for such problems include projection-based methods, Riemannian methods using tangent-space surrogates and retractions, or Block Majorization-Minimization (BMM) methods that iteratively minimize a surrogate directly on the manifold.\n\nThe authors propose an approach, **OBCD**, which falls under the BCD/BMM umbrella but with a novel and distinct design. Instead of updating a manifold block $X_k$ directly, the method subsamples $k$ rows and finds a small $k \\times k$ orthogonal matrix $V$ that *transforms* this block. This \"row-wise\" update ($X^{t+1}(\\mathcal{B},:) \\leftarrow \\overline{V}^{t}X^{t}(\\mathcal{B},:)$) is a key contribution, as it is inherently feasible and avoids the standard tangent-space/retraction machinery.\n\nWhile the general BCD/BMM idea is known, this paper's novelty lies in the specifics of its framework for the Stiefel manifold:\n\n1.  It defines a new optimality condition, the **\"block-k stationary point\" ($BS_k$-point)**. This is a significant contribution, as Theorem 3.6 proves this condition is **provably stronger** than the standard critical point condition. The authors justify this by showing their $k=2$ solver uses both rotations and reflections, allowing it to escape suboptimal points.\n\n2.  It provides a **constructive and exact solver** for its nonsmooth subproblem. Appendix B introduces a novel \"Breakpoint Searching Method (BSM)\" that finds the *exact global solution* for the $k=2$ subproblem with $l_0$, $l_1$, or non-negativity regularizers. This is a non-trivial technical result that provides a solid foundation for the algorithm.\n\nOn this theoretical foundation, the authors derive a comprehensive convergence analysis, including an $\\mathcal{O}(1/\\epsilon)$ iteration complexity for an $\\epsilon$-$BS_k$-point (Theorem 4.2) and a full non-ergodic (last-iterate) convergence analysis under the KL property (Theorem 4.10). The presentation is clear, with assumptions formally stated, the algorithm well-defined, and the theoretical claims rigorously established."}, "weaknesses": {"value": "### 1. Unclear Practicality for the `k > 2` Case\n\nA primary weakness is the gap between the well-analyzed $k=2$ case and the general $k > 2$ case. The paper's core assumption (Asm-iii) is that the $k \\times k$ nonsmooth subproblem can be solved \"exactly and efficiently.\" The authors provide an impressive, detailed proof of this for $k=2$ using their novel Breakpoint Searching Method (Appendix B).\n\nHowever, for $k > 2$, this assumption is highly questionable. Solving a general $k \\times k$ nonsmooth composite problem over the Stiefel manifold $St(k,k)$ is not a trivial task, and the paper provides no algorithm or justification for it.\n\nThe paper *does* offer a fallback in Algorithm 1, suggesting one can \"Alternatively, find a local solution $\\overline{V}^t$ such that $\\mathcal{K}(\\overline{V}^{t};X^{t},B)\\le\\mathcal{K}(I_{k};X^{t},B)$\". But the paper provides no discussion on *how* to find such a local solution. For a general nonsmooth, nonconvex subproblem, even finding a point that guarantees this simple descent from the identity matrix ($I_k$) is a non-trivial problem in itself. Without a proposed method, the practical application of OBCD for block sizes $k > 2$ remains unclear.\n\n### 2. Overstated Novelty Claim\n\nIn the \"Summary\" of the related work (Section 1.2), the paper claims: \"To our knowledge, this represents the first application of BCD methods to solve nonsmooth composite optimization problems under orthogonality constraints...\". This claim appears to be incorrect. The paper's own literature review (Section 1.2, under \"Minimizing Nonsmooth Functions...\") cites existing work on \"Block Majorization Minimization (BMM) on Riemannian manifolds,\" which directly addresses nonsmooth problems on manifolds, including the Stiefel manifold. This contradiction in the paper's own text weakens the positioning of its contribution. Rather than claiming to be the first of its kind, the authors could emphasize the tailored approach for solving problems on Stiefel manifold efficiently. \n\n### 3. Mismatch Between Theory and Experiments\n\nThere is a disconnect between the theoretical contributions and the experimental validation. The paper provides a strong theoretical justification (in Appendix B) for handling $l_0$, $l_1$, and non-negativity constraints. However, the experiments in Section 5 are conducted *only* on $L_0$-norm-based SPCA. This is a missed opportunity to not validate the new, non-trivial solvers for the $l_1$ and non-negativity constraint problems, which would have made the experimental section much more comprehensive and demonstrated the full power of the proposed subproblem solvers.\n\n### Minor comments\n\nWhile the paper's literature review is adequate, the authors may consider including the following recent paper on Euclidean BMM as part of the BCD section: \n\nHanbaek Lyu and Yuchen Li, “Block majorization-minimization with diminishing radius for constrained nonconvex optimization.”  SIAM Journal on Optimization, Vol. 35, Iss. 2 (2025)"}, "questions": {"value": "1. The authors mention in L120 that [Gao et al. 2019] studies a similar problem with columewise updates, whereas the proposed method is rowwise. Besides these algorithmic design choices, what are the important differences? Pros/cons on the computational cost? Theoretical property? \n\n2.  The definition of the \"block-k stationary point\" ($BS_k$-point) (Definition 3.5) is based on $I_k$ being the *global minimizer* of the subproblem. If one only finds a local solution $\\overline{V}^t \\neq I_k$ that satisfies the descent condition (as suggested in Algorithm 1) and the algorithm converges, does the limit point have any meaningful theoretical properties? Does the hierarchy in Theorem 3.6 still hold, or is the $BS_k$-point definition fundamentally tied to the (impractical) global solution of the subproblem for $k > 2$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XbzKnGmoSL", "forum": "L3Or2mhuCH", "replyto": "L3Or2mhuCH", "signatures": ["ICLR.cc/2026/Conference/Submission15182/Reviewer_TMgr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15182/Reviewer_TMgr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898229567, "cdate": 1761898229567, "tmdate": 1762925488222, "mdate": 1762925488222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a block coordinate descent (OBCD) algorithm for nonsmooth composite optimization on the Stiefel manifold. The method updates $k$ blocks at each iteration while preserving orthogonality, and achieves convergence to a stationary point under suitable assumptions. The authors also design an exact solver for the $k=2$ subproblem when the nonsmooth term $h$ is coordinate-wise separable, by reducing it to a one-dimensional breakpoint-search problem. Theoretical results include global convergence and iteration complexity bounds, and experiments on sparse PCA show promising performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* They propose a first BCD method for solving nonsmooth composite optimization problems under orthogonality constraints.\n* The breakpoint-search solver for the $k=2$ subproblem is elegant and provides an exact and efficient solution when $h$ is separable.\n* The experimental results on sparse PCA are convincing."}, "weaknesses": {"value": "* (Assumption 3 (exact subproblem solution).) This assumption is quite restrictive, as exact solutions are only possible when $h$ is coordinate-wise separable (e.g., $\\ell_0$ or $\\ell_1$ penalties). For general non-separable regularizers such as the nuclear norm, this assumption may not hold. Could the authors discuss (i) whether the framework can be extended to non-separable $h$, and (ii) whether their convergence analysis remains valid if each subproblem is solved only approximately or to a stationary point? A relaxed ``inexact subproblem'' assumption might make the method more generally applicable.\n* (Bounded subgradient assumption (Lemma 4.4).) I think the assumption $||\\partial h(X)||_{sp} \\le \\ell_h$ appears problematic for $h(X)=||X||_0$ (please correct me if I am wrong). However, you use the $\\ell_0$-norm in your experiments. It would be better justify this assumption. \n* (Relation to prior work ([1]).) I think problem (1) is a special case of [1] when $H=L_f \\mathcal{I}_{nr}$. It would strengthen the contribution to explain clearly how this work differs from and improves upon [1].\n* (Experiment) Would the authors also include experimental comparisons with the method proposed in [1]? Such a comparison would better demonstrate the advantages of the proposed OBCD algorithm.\n\n[1] Cheung, Andy Yat-Ming, et al. \"Randomized Submanifold Subgradient Method for Optimization over Stiefel Manifolds.\" arXiv preprint arXiv:2409.01770 (2024)."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ANeic7TAkh", "forum": "L3Or2mhuCH", "replyto": "L3Or2mhuCH", "signatures": ["ICLR.cc/2026/Conference/Submission15182/Reviewer_ufBN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15182/Reviewer_ufBN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762376895053, "cdate": 1762376895053, "tmdate": 1762925487893, "mdate": 1762925487893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}