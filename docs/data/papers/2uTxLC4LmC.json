{"id": "2uTxLC4LmC", "number": 4405, "cdate": 1757674933281, "mdate": 1759898034278, "content": {"title": "Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention", "abstract": "Although Large Reasoning Models (LRMs) have progressed in solving complex problems, their chain-of-thought (CoT) reasoning often contains harmful content that can persist even when the final responses appear safe. We show that this issue still remains in existing methods which overlook the unique significance of safe reasoning, undermining their trustworthiness and posing potential risks in applications if unsafe reasoning is accessible for and exploited by malicious users. We therefore shift our focus to aligning the safety of reasoning itself in this paper and explore process supervision as the solution. However, simply rewarding safe reasoning proves inadequate due to low rollout diversity and limited training signals. To tackle this challenge, we first delve into the characteristics of safe reasoning and uncover several critical insights that 1) safe reasoning is often consolidated by a few critical steps of _safety triggers_; 2) _compliance cues_ strongly correlate with unsafe continuations; and 3) corrective interventions reliably steer unsafe trajectories towards safer traces. Motivated by these, we propose **Intervened Preference Optimization (IPO)**, an alignment method that enforces safe reasoning by substituting compliance steps with safety triggers and constructing pairs for preference learning with strong signals. Experiments on jailbreak and adversarial safety benchmarks demonstrate that IPO remarkably improves overall safety regarding both reasoning and responses, outperforming SFT-based and RL-based baselines with a relative reduction of over 30\\% in harmfulness, while preserving excellent performance across diverse reasoning tasks. The results highlight the importance of explicit alignment for reasoning and provide a practical path to safer LRMs.", "tldr": "", "keywords": ["Large Reasoning Model", "Safety Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/57f0bdf3e6038bea1971d171bd4711ddf9684637.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Reinforcement learning on chain-of-thoughts to improve reasoning capabilities have led to very strong thinking models like OpenAI-o1 and Deepseek-R1. These models generally produce a long sequence of “thinking” tokens (enclosed by <think> </think> or similar tags) followed by the final answer, and being trained to spend additional compute for thinking has dramatically improved these models capabilities in complex tasks like math and coding.\n\nHowever, this increase in capabilities also raises concerns about their misuse and safety issues. This work particularly studies the safety of both the chain-of-thought and the final answer. It shows that even when the final answer is safe, the chain-of-thought generated by the LLM can still be safe. The paper claims potential misuse of the unsafe chain-of-thoughts (COTs), and studies how to make COTs themselves safe. Specifically, they identify two mechanisms, namely safety triggers (where the model generates a sequence of tokens detecting potential harm of its chain-of-thought and stops) and compliance cues (where the model complies with potentially harmful requests) that affect safety of the chain-of-thoughts. The work then proposes to generate safe responses from unsafe ones by replacing the first “compliance cue” with a “safety trigger” and completing the response, use the (unsafe response, edited safe response) as a potential preference pair, and finally run DPO style training to induce safety in the chain-of-thoughts. The paper calls this method Intervened Preference Optimization (IPO), and demonstrates its performance in both safety benchmarks as well as demonstrates that it preserves performance across regular reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-written and executed. The idea of how to generate the preference pairs is well motivated by describing why simple reinforcement learning does not suffice (section 2.3), then showing a comparison of compliance cues and safety triggers after formally defining them (Section 3.1, 3.2), all of which leads to the final method of Intervened preference optimization (Section 3.3 and 3.4). I think the way the paper is structured is quite intuitive and illuminating.\n\n2. The results shown in Table 2 are comprehensive and thorough. Across a wide range of datasets and baselines, the proposed method shows strong performance. I also like the evidence shown in favor of the proposed method not harming regular reasoning performance on math datasets.\n\n3. The explanations about safety dynamics, and analyzing the safety of chain-of-thought besides the final answer is interesting and can be useful for the community."}, "weaknesses": {"value": "I will list my concerns about this paper in the order of most to least severity.\n\n## Potential Flaw in the Problem Setting\n\nThe biggest weakness of this paper is actually in the problem setting of attempting to make the chain-of-thoughts themselves safe, instead of just keeping the final response safe. I do not fully buy the authors’ argument, presented in 2.2, for why we should train models to keep their chain-of-thoughts safe. Particularly, prior works [1, 2] have described how making chain-of-thoughts safer can harm monitorability of modern reasoning LLMs. [1] shows that for a strong reasoning model like o3-mini, prolonged training to apply optimization pressure on the COT directly can lead to obfuscation, where the model can take harmful/unsafe actions while obfuscating their thoughts in the COT, which may make it **much harder** to actually prevent unsafe behaviors. In other words, making COTs safe does not mean that very strong reasoning models like o3-mini will become safe — these models might just learn to obfuscate their chain-of-thoughts. **A very strong counter-argument will be needed to supporting why we need to make COTs safe instead of just the final answers, given the results from frontier reasoning models like o3-mini presented in [1], to justify why COTs should be safe as well, given the risk to monitorability that this process may produce.**\n\nA simple way for current reasoning models, adopted by most frontier LLMs, is to just not reveal their chain-of-thoughts to the user. In this way, chain-of-thoughts act like scratch pads, and just training the model to backtrack/reverse safety issues in the final answer by spending additional compute during inference [4, 5] seems like a better option compared to specifically training for COT safety.\n\n## Lacking contextualization and novelty\n\nFirstly, just teaching the model to backtrack in its chain-of-thought to correct potential unsafe generations, such as Backtrack [4] might result in better performance. This paper does not cite [4] nor compare with it. Moreover, the idea of adding interventions to unsafe responses to potentially edit them to generate a preferred response, then creating a (preferred, dispreferred) pair for DPO-style training has already been explored in [4]. Without properly citing/contextualizing the current work with respect to [4] makes it significantly weaker.\n\n## Inheriting regular problems of DPO\n\nThe paper describes why regular RL/GRPO does not work (base model lacks diversity) and then proposes the alternative method of IPO. However, this seems to me to be a similar reasoning of why people were using DPO style learning for math/reasoning earlier. Specifically, if base models lack sufficient coverage and capacity, then online RL might not induce the correct learning behavior [7]. However, the proper solution of that is to train a better base model, as shown in [7], and then to use online RL on top of it, instead of using offline methods like DPO.\n\nMore specifically, an offline DPO loop will inherit all the problems of such an algorithm, like unintentional unalignment [8, 9] and problems of off-policy data [10, 11, 12]. These works and the concerns they raise about offline DPO-class of algorithms are not properly addressed in the paper. For example, what happens to the probability of positive sequences as IPO training goes on?\n\n## Lack of baselines\n\nThis is a very minor concern, but instead of running DPO, if one just runs SFT on the constructed preferred responses, how does that perform? How about SFT + DPO (i.e., RPO [9])?"}, "questions": {"value": "1. Is there any reason to use GPT-4o, instead of the specific OpenAI safety moderation API (https://platform.openai.com/docs/guides/moderation) to judge safety? I believe the safety moderation API would be a better judge since it is specifically trained for this purpose, and having a strong enough judge is important for the numbers presented to be believable.\n\n2. How does this work compare to more recent methods specifically designed to instill safety into reasoning LLMs, such as TARS [5]? More specifically, I am interested in something like Figure 2 in [5], where one can clearly see the safety vs compliance tradeoff of thinking models. Does this paper’s proposed method, IPO, reduce performance on borderline/more tricky questions, which may seem unsafe at first but the model should think/spend more compute to actually find a satisfying answer? How does IPO perform on “ambiguous” prompts defined in [5]? Possibly using prompts from OR-Bench [6], that evaluates LLMs on over-refusal, can be useful here.\n\n3. A continuation to the previous question, but how does the model perform under tricky questions such as the one shown in Figure 1 of Deliberative Alignment [3], where only after thinking through/processing the question can the model understand that the prompt is asking the model to do something unsafe?\n\n# References\n\n[1] Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation, https://cdn.openai.com/pdf/34f2ada6-870f-4c26-9790-fd8def56387f/CoT_Monitoring.pdf\n\n[2] Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety, https://arxiv.org/abs/2507.11473\n\n[3] Deliberative Alignment: Reasoning Enables Safer Language Models, https://arxiv.org/abs/2412.16339\n\n[4] Backtracking Improves Generation Safety, https://arxiv.org/abs/2409.14586\n\n[5] Reasoning as an Adaptive Defense for Safety, https://arxiv.org/abs/2507.00971\n\n[6] OR-Bench: An over-refusal benchmark for large language models, https://arxiv.org/abs/2405.20947\n\n[7] Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs, https://arxiv.org/abs/2503.01307v1\n\n[8] Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization, https://arxiv.org/abs/2410.08847\n\n[9] Iterative Reasoning Preference Optimization, https://arxiv.org/abs/2404.19733\n\n[10] Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study, https://arxiv.org/abs/2404.10719\n\n[11] Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data, https://arxiv.org/abs/2404.14367\n\n[12] Bridging Offline and Online Reinforcement Learning for LLMs, https://arxiv.org/abs/2506.21495"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S1DeCMTBun", "forum": "2uTxLC4LmC", "replyto": "2uTxLC4LmC", "signatures": ["ICLR.cc/2026/Conference/Submission4405/Reviewer_1hr8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4405/Reviewer_1hr8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760823204948, "cdate": 1760823204948, "tmdate": 1762917348222, "mdate": 1762917348222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "IPO proposes a DPO style training approach for safety in reasoning models by first identifying compliance cues and replacing them with safety triggers. Interestingly, IPO only trains on preference pairs of reasoning traces but show that this also leads to safe answers. The paper first conducts studies into SFT and RL based approaches for safety training, showing the limitations of both approaches on reasoning safety. Then they analyze the CSR by looking into turning points to see that replacing compliance cues with safety triggers could lead to reasoning traces that generate safe answers. Finally, they construct a DPO (RPO) preference dataset with prefixes including the reasoning trace up to the first compliance cue, chosen responses being a reasoning completion when replaced with a safety trigger, and rejected response being the original reasoning completion."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is strongly motivated through analysis on SFT based and RL based approaches with RL experiments rewarding the safety of the reasoning directly. Analysis on CSR also provides valuable insight into the mechanism of how reasoning leads to a safe answer. Although it has been known that reasoning helps with safety, it has not been clear how or why it helps. This paper gives insight into this problem. Most interestingly, training only on the reasoning leads to safer answers, which is a sufficiently different paradigm from RLVR training in math domains. It shows that reasoning in more open-ended domains could work differently."}, "weaknesses": {"value": "1. Although the paper includes sufficient baselines for SFT-based approaches, it does not include prior open-source work that uses RL-based approaches for safety training such as TARS [1]. Since part of the motivation is that RL training also does not provide sufficient signal, comparing against prior RL-based approaches for both reasoning trace analysis (Figure 2) and performance (Table 2) would help strengthen the paper.\n2. In Table 2, is the Avg. safety score across both Rsng. and Resp. or just Resp.? It would be helpful if the table includes averages for them separately to see how the effect of IPO is different for the reasoning and answer.\n3. I am not convinced about the claim in lines 189-191 “However, the absolute safety scores remain limited on adversarial datasets, suggesting the challenge of imposing process supervision”. How much effort was put into the training for tuning hyperparameters and exploring context guidelines in the reward model? The authors should explain what methods they tried to get the best performance out of GRPO with process rewards. \n4. The experiments on safety triggers and compliance cues were conducted on DS-8B. However, DS models are known to be relatively unsafe compared to other models [2,3], exhibiting different behavior. Does the trend also hold on other base reasoning models such as Qwen3-8B which are known to be safer?\n\n\n**References**\n\n[1] Kim, Taeyoun, et al. \"Reasoning as an Adaptive Defense for Safety.\" arXiv preprint arXiv:2507.00971 (2025).\n\n[2] Zhou, Kaiwen, et al. \"The hidden risks of large reasoning models: A safety assessment of r1.\" arXiv preprint arXiv:2502.12659 (2025).\n\n[3] Zhang, Zhexin, et al. \"How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study.\" arXiv preprint arXiv:2505.15404 (2025)."}, "questions": {"value": "General comments:\n\n1. The qualitative examples in the Appendix should be referenced in the main body. I was generally confused in lines 255-257, 274, 287-290 before I saw those examples.\n2. It would be helpful to also mention in words in section 3.4 that IPO only contains reasoning traces in the preference pairs. \n3. It would be helpful to include RL-based approaches in the related works section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OaYzNLoM7U", "forum": "2uTxLC4LmC", "replyto": "2uTxLC4LmC", "signatures": ["ICLR.cc/2026/Conference/Submission4405/Reviewer_ooTQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4405/Reviewer_ooTQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761162969552, "cdate": 1761162969552, "tmdate": 1762917347798, "mdate": 1762917347798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of reasoning-level safety alignment in Large Reasoning Models (LRMs). It introduces Intervened Preference Optimization (IPO), a novel DPO-based method that supervises reasoning by replacing unsafe compliance cues with safe triggers to form preference pairs at safety-critical steps. Extensive experiments across multiple LRMs show that IPO substantially reduces harmful reasoning and response content while preserving or even improving reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear technical novelty: The proposed IPO method is conceptually distinct from existing SFT or RL-based alignment methods, offering an integration of intervention-based preference optimization for reasoning safety.\n\n- Strong empirical grounding: The preliminary analyses (Sections 2 and 3) are concrete and provide interesting insights about safety of LRMs — establishing a solid foundation for the proposed approach.\n\n- Effective results: IPO consistently improves reasoning-level and response-level safety across multiple benchmarks without degrading reasoning capability."}, "weaknesses": {"value": "- Limited model scale evaluation: Experiments are only conducted on mid-sized LRMs (DS-7B/8B and Qwen3-8B). Evaluating IPO on smaller (e.g., DS-1.5B) and larger models (e.g., DS-14B, DS-32B) would help assess scalability and generalizability.\n\n- Lack of adaptive robustness testing: It would be more convincing to include adaptive or adversarial tests such as obfuscation, paraphrased jailbreaks, or multi-turn exploit prompts, which better reflect real-world attack surfaces.\n\n- Limited capability assessment: Although math and coding benchmarks are included, there is no evaluation of general language capabilities after alignment — e.g., factual QA, instruction-following coverage, multilingual understanding, or knowledge recall. This raises questions about whether IPO maintains general utility beyond reasoning-heavy tasks."}, "questions": {"value": "Why are the training dataset sizes for IPO different across models (DS-8B, DS-7B, Qwen3-8B)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eARPOhu0Sl", "forum": "2uTxLC4LmC", "replyto": "2uTxLC4LmC", "signatures": ["ICLR.cc/2026/Conference/Submission4405/Reviewer_B8rr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4405/Reviewer_B8rr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819813694, "cdate": 1761819813694, "tmdate": 1762917347190, "mdate": 1762917347190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how to achieve safe reasoning in LLMs. The authors argue that most existing safety-aligned models focus only on surface-level refusals or single-turn judgments, ignoring how unsafe reasoning trajectories evolve internally.\nTo address this, the paper introduces a reasoning-aware alignment framework that dynamically monitors intermediate reasoning states, detects unsafe reasoning drifts, and reinforces safety alignment during the reasoning process itself."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper addresses a crucial and emerging challenge — safety alignment for reasoning models. This topic is becoming increasingly important as reasoning-capable models are deployed in real-world agentic settings.\n\n2. The paper makes a clear distinction between response-level and reasoning-level safety, offering a conceptual advance that will help structure future research in this area.\n\n3. The experiments show consistent improvements on multiple safety benchmarks, while maintaining competitive reasoning performance. The trade-off between safety and capability is well-balanced, outperforming SFT-based and RL-based baselines.\n\n4. The paper is clearly written, with intuitive diagrams and examples that help readers understand complex reasoning dynamics. Figures illustrating unsafe reasoning chains and safety corrections are particularly effective."}, "weaknesses": {"value": "1. The paper would benefit from a deeper theoretical discussion on why certain reasoning paths become unsafe, and why the IPO method work.\n\n2. Empirical evaluation could be strengthened by including more open-ended reasoning scenarios to demonstrate generalization beyond factual or mathematical reasoning."}, "questions": {"value": "1. The paper shows that Intervention by Prompt Optimization (IPO) significantly enhances reasoning safety by replacing compliance cues with safety triggers. However, do these interventions perform differently across attack types—for instance, between direct jailbreaks and indirect persuasion attacks? Are certain categories (e.g., violence, misinformation) more effectively mitigated than others?\n\n2. While IPO successfully reduces unsafe reasoning, how does it avoid over-refusal on benign requests? The paper mentions supplementing benign datasets—could you elaborate on whether this mechanism sufficiently addresses the trade-off? Might a more fine-grained calibration strategy further improve the balance?\n\n3. Some experiments (e.g., on GPQA-Diamond) suggest that IPO not only improves safety but also enhances reasoning performance. Does this imply that safety triggers might indirectly reinforce logical consistency or reduce reasoning shortcuts? It would be valuable to further explore this potential synergy between safety alignment and reasoning robustness.\n\n4. In adversarial contexts where attackers attempt to bypass safety triggers (e.g., through indirect phrasing or prompt obfuscation), how resilient is IPO? \n\n5. While the paper responsibly avoids releasing harmful examples, could the intervention mechanism itself be reverse-engineered (e.g., by comparing original and intervened reasoning traces)? How might future deployments ensure the transparency–security balance, especially in open-model ecosystems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jOjjpWZeIG", "forum": "2uTxLC4LmC", "replyto": "2uTxLC4LmC", "signatures": ["ICLR.cc/2026/Conference/Submission4405/Reviewer_T2wZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4405/Reviewer_T2wZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918681385, "cdate": 1761918681385, "tmdate": 1762917346857, "mdate": 1762917346857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}