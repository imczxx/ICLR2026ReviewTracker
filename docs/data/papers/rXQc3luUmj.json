{"id": "rXQc3luUmj", "number": 12818, "cdate": 1758210553214, "mdate": 1759897482947, "content": {"title": "PathXfer: Few-Shot Visual Fidelity Transfer for Compressed Multi-to-Few Step Sampling", "abstract": "Traditional approaches to accelerate sampling in generative models rely on distillation, which requires large datasets and costly training. We instead view the quality gap between multi-step and few-step sampling as a transferable property, and introduce PathXfer, a few-shot framework that transfers multi-step fidelity to few-step sampling. PathXfer employs LoRA-based lightweight adaptation together with a Path Compression Loss, enabling effective fidelity preservation using only 16 samples, without retraining the entire model. Experiments show that PathXfer compresses sampling from 20 to 2 steps on Flux, a flow-based generative model, with only minor perceptual degradation, and also yields consistent improvements on diffusion models such as SDXL, demonstrating that the approach generalizes across paradigms. These results highlight few-shot fidelity transfer as an efficient and practical complement to distillation for accelerating generative sampling.", "tldr": "PathXfer is a few-shot framework that compresses multi-step sampling paths into few steps while preserving generative fidelity across flow- and diffusion-based models.", "keywords": ["PathXfer", "Path Compression", "Few-Shot Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97add1f36ffb51d89a50177eb3ee0e8a35d5509b.pdf", "supplementary_material": "/attachment/368c74e0b56f73f76f978852816b5f4751ebb7f2.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes PathXfer, which proposes to accelerate the sampling of diffusion and flow models by a path compression loss. The loss enforces the model's few step sampling to match or exceed its multi step sampling path. Crucially, the approach is highly efficient, requiring only 16 training samples and only on LORA weights. Empirical results show the method to be effective at few step sampling while preserving image quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Highly efficient and lightweight method compared to distillation methods, using only 16 samples and LORA and teacher-free.\n- The proposed training objective is conceptually clean -- enforce that earlier steps match or exceed later steps at reconstruction.\n- Paper is overall well-written and clear."}, "weaknesses": {"value": "- Some theoretical concerns that could be addressed: first, the paper assumes that for $k<t$, the construction from $k$ will be uniformly better and thus serves as a training signal. Is this a true assumption? Empirically it seems to hold, but generative samplers can be non-monotic.\n\n- Second, by using the model as its own 'teacher', it seems to be a moving target self-distillation objective, which could be inherently unstable. Can the authors show that the loss will converge under this dynamic?\n\n- It will be helpful to have exact compute/time comparisons to support training efficiency claims on smaller models/datasets like ImageNet models.\n\n- In Table 1, PathXfer models produce strong AES results but consistently lower Reward score. Can the authors discuss the reasons for this and the implications?\n\n- Qualitative results is subjective; for instance, I find the results of Flux-schnell in Fig 3 generally more appealing though there are some hallucinations in the persons image. There will also be some degree of cherry picking in presenting qualitative results. Including qualitative results is fine, but I advise the authors to tone down the claims made off them in the paragraph lines 302-320."}, "questions": {"value": "- Which ODE samplers is used for the diffusion model experiments, and have the authors explored whether higher-order ODE samplers lead to improved performance over something like a first-order DDIM?\n- Have the authors studied the robustness of the method to different 16 prompts used for generating the training images?\n- For the same number of steps, why does PathXfer offer speedups over the base model in Table 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ndkj9QQLRO", "forum": "rXQc3luUmj", "replyto": "rXQc3luUmj", "signatures": ["ICLR.cc/2026/Conference/Submission12818/Reviewer_vEqK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12818/Reviewer_vEqK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477240259, "cdate": 1761477240259, "tmdate": 1762923624465, "mdate": 1762923624465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PathXfer, which transfers multi-step fidelity to few-step sampling via LoRA-based adaptation and Path Compression Loss, achieving compression with only 16 samples and minimal quality loss across models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Provides a lightweight, model-agnostic few-shot adaptation framework that uses LoRA fine-tuning on frozen diffusion backbones. The path-compression loss is simple, differentiable, and architecture-independent, allowing integration across pretrained models such as FLUX, SDXL, and Kolors with only 16 in-domain samples.\n\n2. Achieves up to 10×~20× faster sampling speed while maintaining comparable perceptual quality and prompt fidelity, demonstrating clear efficiency gains without altering core model architectures.\n\n3. Maintains high perceptual quality and computational efficiency using lightweight LoRA, preserving FID, CLIP, and ImageReward scores across large-scale diffusion pipelines while introducing negligible parameter overhead."}, "weaknesses": {"value": "1. The theoretical and mathematical foundations of the ODE reconstruction equations and the path consistency objective are nearly identical to those of Shortest Path Diffusion(ShortDF)[1]. PathXfer primarily reuses these formulations under a few-shot LoRA fine-tuning context rather than introducing a fundamentally new mechanism. Given this extensive reuse of ShortDF’s structure and objective, PathXfer’s actual contribution appears limited to engineering simplification through LoRA-based few-shot fine-tuning and its application to larger diffusion pipelines.\n\n[1] Optimizing for the shortest path in denoising diffusion model. (CVPR 2025)\n\n2. The “Path Compression Loss” does not model the composite path $t \\rightarrow k \\rightarrow 0$, but merely enforces monotonic improvement between $k \\rightarrow 0$ and $t \\rightarrow 0$. In Eq(3), $\\hat{x}_{0|t}$ and $\\hat{x}_{0|k}$ denote reconstructions of ${x}_{0}$ from states ${x}_{t}$, ${x}_{k}$, respectively. In ShortDF, the transition $t \\rightarrow k$ is explicitly realized through the edge weight formulation, where the estimate $\\hat{x}'_{0|k}$ is computed using $\\hat{x}_k$, while PathXfer contains no composite path $t \\rightarrow k$ in its computational graph at all. This weakens its conceptual grounding as a true path consistency method and reduces interpretability of “multi-order compression”.\n\n3. Reported metrics (AES, ImageReward, CLIP, GenEval) mainly assess fidelity and aesthetic quality, not diversity. However, since the method operates under a few-shot learning setting, where data scarcity amplifies the risk of overfitting and mode collapse, diversity becomes a crucial evaluation criterion. The method’s ability to preserve sample variety or mitigate mode collapse thus remains unquantified."}, "questions": {"value": "1. The ODE reconstruction and path consistency objectives appear nearly identical to those in ShortDF. Could the authors clarify what constitutes the theoretical novelty of PathXfer beyond applying few-shot LoRA fine-tuning on top of existing formulations? \n\n2. Since the proposed “Path Compression Loss” does not model the composite path $t \\rightarrow k \\rightarrow 0$, how does the method justify the claim of being path-consistent? What are the implications of omitting the $t \\rightarrow k$ transition on both theory and empirical behavior?\n\n3. The reported metrics (AES, ImageReward, CLIP, GenEval) primarily measure fidelity and aesthetics, not diversity. Given that the method operates under a few-shot setting where mode collapse is likely, why are diversity metrics not reported, and how can the model’s ability to preserve sample variety be verified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "32tr12xOZx", "forum": "rXQc3luUmj", "replyto": "rXQc3luUmj", "signatures": ["ICLR.cc/2026/Conference/Submission12818/Reviewer_Q3cZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12818/Reviewer_Q3cZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806720574, "cdate": 1761806720574, "tmdate": 1762923624165, "mdate": 1762923624165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PathXfer, a lightweight approach for accelerating diffusion and flow-based generative models by transferring multi-step sampling paths into shorter ones through a Path Compression Loss. Instead of relying on large-scale distillation or external teacher models, PathXfer adopts a few-shot, low-rank (LoRA) fine-tuning strategy that enables the base model to “self-teach” shorter sampling trajectories. The method is demonstrated on both diffusion (e.g., SDXL) and flow models (e.g., FLUX), showing promising improvements in generation speed while preserving visual fidelity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Low training cost: The few-shot setup combined with low-rank adaptation significantly reduces training overhead compared to full fine-tuning or large-scale distillation (e.g., LCM or Progressive Distillation).  \n2. The framework is model-agnostic and can be applied to both diffusion and flow-based generators, as demonstrated on SDXL and FLUX.  \n3. The paper is well-organized and effectively communicates the intuition, motivation, and implementation details."}, "weaknesses": {"value": "1. Lack of systematic comparison: The experimental section should include comparisons with representative baselines such as ShortDF (described in Sec. 2.1) and Mean Flow (Mean Flows for One‑step Generative Modeling).  \n2. Limited theoretical grounding: The proposed Path Transfer Loss appears largely empirical; no formal justification or error bound is provided to explain when and why path compression preserves distributional fidelity.  \n3. The evaluation relies mainly on aesthetic or human-based preference scores (ImageReward). Standard quantitative metrics such as FID, Precision, and Recall would better capture both quality and diversity across domains.  \n4. It remains unclear how generalizable the method is—whether the few-shot prompts used for adaptation must closely match the downstream generation domain. More experiments on cross-domain or out-of-distribution prompts would strengthen the analysis."}, "questions": {"value": "Please refer to Weaknesses. I am not an expert in this field, and I shall raise the score as appropriate based on the author's response and the opinions of other reviewers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qvw9oBb5Va", "forum": "rXQc3luUmj", "replyto": "rXQc3luUmj", "signatures": ["ICLR.cc/2026/Conference/Submission12818/Reviewer_obWA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12818/Reviewer_obWA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950090751, "cdate": 1761950090751, "tmdate": 1762923623837, "mdate": 1762923623837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method for ODE-based models distillation and demonstrate it empirically."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The empirical results of the paper are promising."}, "weaknesses": {"value": "This is a very poorly written paper, and I believe it's almost entirely written by an LLM. Below are some examples in the text, that I find very imprecise or not informative.\n\n    1. Line 50: \"instead of treating the multi-step sampling process as a deficiency, we conceptualize the quality gap between multi-step and few-step generation as a transferable property\". I don't understand what it means for the quality gap to be a transferrable property\n    2. Line 77: \"path compression loss that enforces multi-order consistency across intermediate states\". What is multi-order consistency?\n    3. Line 78: \"each target step is iteratively optimized by referencing multiple intermediate evaluations\". What are \"target steps\" ?\n    4. Line 84: \"empirically stable paths\". What does this mean and why is this relevant?\n    5. Line 116: \"dual-model self-distillation scheme in which the base model B is optimized via a triangle-based relaxation of the paths\". What is \"dual-model distillation\"? What is \"triangle-based relaxation\"? Equation (2) says that two quantities are approximately equal and doesn't elaborate on why this holds or why we would want it to hold.\n    6. The entire paragraph 128-132 feels very vague and does not introduce new information.\n    7. Line 170 - \"the model learns to adjust its parameters such that the visual fidelity at t matches or exceeds that at\". I am not sure what the authors mean when they say \"visual fidelity\". I believe what the model is trying to learn is to generate the same output in fewer steps.\n    8. Equation 8 seems to always hold without any finetuning. Or did the authors mean the inverse inequality (as suggested by loss function in Eq 4)?\n    9. Line 188 - \"Here, stability means that under the same denoising path, generations from different prompts and seeds exhibit consistently similar quality.\" How can we have the same denoising path for different prompts?\n    10. Line 201 - \"Using self-generated trajectories keeps the data distribution within the model and avoids external interference\". What does it mean to keep a distribution within a model? what external interference?\n    11. Line 261-263 \"The stronger gains on FLUX.1-dev stem from its intrinsic ODE properties, which align well with our path compression and visual fidelity transfer strategy, whereas noise-based models require deterministic samplers such as DDIM to approximate ODE-like behavior\". What does it mean? What are the intrinsic ODE properties that FLUX.1-dev has the diffusion-ODE model don't? How is FLUX.1-dev not a \"noise-based model\"? It uses noise in the same way as diffusion-ODE models, i.e. the prior distribution is a standard Gaussian.\n    12. Figure 4 - I don't understand how the norm of the drift has anything to do with Equation 8.\n    13. Line 440 - \"In contrast to these discrete-step methods, Flux (Labs, 2024) models generation as a continuous-time probability flow ODE, learning a velocity field via flow matching and enabling smooth, step-free sampling\". What is step-free sampling?\n\nApart from that, I find the paper should be more rigorous in their use of mathematical notation. For example\n\n    1. \"where $x^G_{0|t_i} is the reconstructed estimate of $x_0$ from state $x_{t_i}$\". What does it mean? Is it the denoising mean? Or the flow of the ODE from t_i to 0 ?\n    2. Line 135-137 \"if the reconstruction along the two-step path t → k → 0 is more accurate than the direct path t → 0, the direct path can benefit from the visual fidelity of the intermediate trajectory\". are the authors trying to say that a path discretized more finely is more accurate? This is always the case. Unless authors meant something else?\n    3. Line 138 \"to implement this principle, we reformulate the triangle relation of reconstruction errors (Eq. 2) under the ODE perspective, leading to the condition\". This, together with Equation 3 is very imprecise. What is this condition? Is this something that the authors claims that holds? Or is this something that the authors want to enforce? The authors say that they want to enforce this with Equation 4, but the exact opposite would happen. L_comp is zero if and only if Eq3 does not hold.\n    4. In Equation 5, the \"reconstruction\" is defined, and it is the posterior (or the denoising) mean. In light of this, I find the previous text even more confusing. I don't understand how the evolution of the posterior mean along the sampling trajectory determines the behaviour of the samples. Perhaps it is discussed in the Shortest Path Diffusion, but the authors do not explain this.\n    5. Lines 176-181. This entire paragraph could be phrased in a mathematically rigorous way\n\nApart from that some modelling details are unclear. For example\n1. The algorithm described in pseudocode in Appendix A does not match the description in the main text. The authors said that they generate sampling trajectories to train on (line 200), but in the pseudo code, line 20, the authors move between different timesteps using a single Euler step, which is inaccurate (otherwise, there is no point to distill in the first place). This means that $x_t$, and $x_k$ are not on the same sampling trajectory, which was the assumption made in the paper.\n2. What are the prompts that authors used for finetuning? Are they different than the prompts used for evaluation? The fact that only 16 prompts were used and the model was finetuned for 18k steps suggests to me that some overfitting might be happening."}, "questions": {"value": "And some additional questions/issues:\n1.  without requiring teacher supervision\". I believe this statement is inaccurate. The proposed approach does require the base model ( the teacher) to generate target trajectories\n2. Line 80: \"and is applicable to any generative model\". I believe this statement is also inaccurate. The proposed method only applies to ODE-based methods, like flows and diffusion models, not all generative models.\n3. Line 317-318. \"it avoids common artifacts such as hallucinated limbs, unnatural object placements, or distorted proportions, and maintains high visual coherence across diverse categories (e.g., humans, vehicles, fine textures)\". I don't understand how the proposed method would help with that. Can the authors explain how distilling a model to take fewer steps would yield a model that e.g. hallucinates fewer limbs?\n4. Line 492 - \"In applying generative models, we take care to ensure that generated content does not infringe upon the rights of individuals or communities\". How did the authors ensure that?\n5. Line 492 - \"Measures are taken to reduce the risk of misuse, such as the creation of misleading or harmful content, and we emphasize the importance of responsible use.\". What are these measures?\n6. Line 499 - \"All datasets used in our research comply with privacy regulations and ethical standards, and are sourced appropriately\". Which datasets did the authors source?\n7. The reproducibility statement essentially says that the work is not reproducible currently, but the authors will make it so in the unspecified future.\n8. Line 814 - How are LPIPS scores used? It's a pairwise similarity measure.\n9. Are the authors building on Shortest Path Diffusion? It is not clear from the writing. Why is this method explained in the background section, and not e.g. related work? This entire subsection feels like it's inserted and independent from the rest of the paper. I don't understand how it connects to the rest of the story."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PmPt8tQiAe", "forum": "rXQc3luUmj", "replyto": "rXQc3luUmj", "signatures": ["ICLR.cc/2026/Conference/Submission12818/Reviewer_B8N9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12818/Reviewer_B8N9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098481095, "cdate": 1762098481095, "tmdate": 1762923623109, "mdate": 1762923623109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}