{"id": "8olv02io0e", "number": 13869, "cdate": 1758224080570, "mdate": 1763100904022, "content": {"title": "Background Matters Too: A Language-Enhanced Adversarial Framework for Person Re-Identification", "abstract": "Person re-identification faces two core challenges: precisely locating the foreground target while suppressing background noise and extracting fine-grained features from the target region. Numerous visual-only approaches address these issues by partitioning an image and applying attention modules, yet they rely on costly manual annotations and struggle with complex occlusions. Recent multimodal methods, motivated by CLIP, introduce semantic cues to guide visual understanding. However, they focus solely on foreground information, but overlook the potential value of background cues. Inspired by human perception, we argue that background semantics are as important as the foreground semantics in ReID, as humans tend to eliminate background distractions while focusing on target appearance. Therefore, this paper proposes an end-to-end framework that jointly models foreground and background information within a dual-branch bidirectional cross-attention feature extraction pipeline. To help the network distinguish between the two domains, we propose an intra-semantic alignment and inter-semantic adversarial learning strategy. Specifically, we align visual and textual features that share the same semantics across domains, while simultaneously penalizing similarity between foreground and background features to enhance the network's discriminative power. This strategy drives the model to actively suppress noisy background regions and enhance attention toward identity-relevant foreground cues. Comprehensive experiments on two holistic and two occluded ReID benchmarks demonstrate the effectiveness and generality of the proposed method, with results that match or surpass those of current state-of-the-art approaches.", "tldr": "We propose a dual-branch cross-attention framework that leverages both foreground and background semantics for person re-identification.", "keywords": ["representation learning", "person re-identification", "vision-language model", "occluded re-identification"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/275f476cffe0b948df39113cd20b7756ea975433.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the FBA framework, which uses a dual-branch bidirectional cross-attention mechanism to jointly model both foreground and background features with visual and language information. Through diversity loss and attention map pooling strategies, the framework effectively distinguishes between foreground and background, improving person re-identification accuracy. Experiments demonstrate that FBA performs excellently on multiple benchmark datasets, surpassing or matching the current state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n\n1.Novel Approach: The introduction of the dual-branch bidirectional cross-attention mechanism is a fresh approach to person re-identification, as it considers both foreground and background information simultaneously.\n2.Strong Experimental Results: Experimental results on multiple datasets demonstrate FBA's superior performance in both occluded and non-occluded scenarios, particularly with improvements in mAP and R-1 accuracy."}, "weaknesses": {"value": "Weakness:\n\n1.Limited Focus on Occlusion Handling: While the method shows improvements in occluded person re-identification, it does not explicitly introduce mechanisms tailored for handling occlusions. This could be an area for improvement, especially when dealing with highly occluded data.\n2.Computational Complexity: The proposed framework relies on large models, including CLIP embeddings and a dual-branch bidirectional attention mechanism. While these components improve performance, they also introduce significant computational overhead. The increased model complexity and the use of bidirectional attention could result in higher memory usage and longer inference times.\n3.Lack of Discussion on Hyperparameter Choices: The paper mentions that the triplet loss margin (m) is set to 0.3 and the balance factor λ is 0.5. However, these hyperparameters have not been thoroughly validated through experiments. It is suggested to add an experiment on hyperparameters, exploring different settings for margin (m) and λ, and analyzing their impact on model performance."}, "questions": {"value": "See  Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NO"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xItAyWt5Cd", "forum": "8olv02io0e", "replyto": "8olv02io0e", "signatures": ["ICLR.cc/2026/Conference/Submission13869/Reviewer_B3eC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13869/Reviewer_B3eC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761108760386, "cdate": 1761108760386, "tmdate": 1762924387222, "mdate": 1762924387222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "EVIiJpjpkK", "forum": "8olv02io0e", "replyto": "8olv02io0e", "signatures": ["ICLR.cc/2026/Conference/Submission13869/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13869/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763100903252, "cdate": 1763100903252, "tmdate": 1763100903252, "mdate": 1763100903252, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ADSL for domain-generalizable ReID: CILL mines cross-identity local commonalities (head/torso/legs) with a memory-driven clustering loss plus local CE/Triplet, while DAPS applies local intensity adversarial perturbations and a Clean–Adv local cosine alignment to stabilize directions. On single-source DG routes (Market↔Duke, MSMT, CUHK03), ADSL reports sizable gains and a clear stepwise ablation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1）Large improvements. Strong deltas over a ViT-B/16 baseline across multiple routes, and ablations cleanly show CILL then DAPS contributions. \n\n2）Local focus is well-motivated. Using local clustering + local perturbations targets domain-stable cues while keeping fine-grained identity details."}, "weaknesses": {"value": "1）Prior works already leverage CLIP for global/local alignment and token-level interactions (e.g., CLIP-ReID[1], IRRA[2], A Pedestrian is Worth One Prompt[3]). This paper’s main novelty is the explicit background branch + diversity loss; while interesting, it feels incremental relative to established CLIP-guided token alignments and multi-branch modeling. (See references listed below for comparison.)\n\n2）The claim that foreground and background semantics are equally important is not adequately supported by experimental or theoretical evidence. The paper does not explain why both are equally critical for person re-identification. Specifically, the authors should address whether the proposed method may struggle when different individuals share the same background, and how this is mitigated by modeling background semantics. Additionally, Figure 4 presents attention maps only for the full FBA model, without providing attention maps for the baseline methods. This makes it difficult to clearly demonstrate the specific advantages of FBA, as no direct comparison is shown. The inclusion of attention maps from baseline models would strengthen the claim that the background branch contributes meaningful improvements.\n\n3）Incomplete reporting across chosen benchmarks. Some baselines lack numbers on all source→target routes, with no explanation (reproduction limits, protocol mismatch, etc.), weakening fairness/SOTA claims. Please clarify omissions. \n\n4）Unknown occlusion robustness. Being part-based, the method’s behavior under occlusion/missing parts is unclear. Testing on Occluded-Duke / Occluded-ReID (or Partial-ReID) would materially strengthen claims. \n\n5）Fixed three-part bias may be brittle. The fixed head/torso/legs split drives both CILL and DAPS; strong pose changes, tight crops, or unusual camera tilt can misalign parts, hurting neighbor search and local alignment reliability. A part-noise/misalignment robustness check or a learned part discovery alternative would help. \n\n6）Foreground/background captions are auto-generated by LLaVA with short prompts (≤50 words for background), but robustness to caption noise, generator choice, and prompt phrasing is not studied. The approach may be sensitive to hallucinations or spurious background words, and it is unclear whether captions are generated per-image without using detection masks (potential leakage of person attributes into the “background” description).\n\n\n7）Key hyperparameters under-specified. Sensitivity/justification for k, τ, λ₁, λ₂, δ is unclear; defaults seem given, but rationale and stability ranges are not. Even without extra large-scale experiments, brief sensitivity curves (or a table) and selection rationale would improve transparency.\n\n[1]\tLi et al., 2023. CLIP-ReID: Exploiting Vision-Language Model for Image Re-Identification Without Concrete Text Labels. AAAI 2023.\n\n[2]\tJiang & Ye, 2023. IRRA: Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval. CVPR 2023\n\n[3]\tYang et al., 2024. A Pedestrian is Worth One Prompt: Towards Language Guidance Person Re-Identification. CVPR 2024."}, "questions": {"value": "Please refer to Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0At0GbPWvs", "forum": "8olv02io0e", "replyto": "8olv02io0e", "signatures": ["ICLR.cc/2026/Conference/Submission13869/Reviewer_ht6h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13869/Reviewer_ht6h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603573666, "cdate": 1761603573666, "tmdate": 1762924386779, "mdate": 1762924386779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FBA (Foreground and Background Adversarial Person Re-identification), an end-to-end, language-enhanced dual-branch framework that mimics human perception by jointly modeling foreground and background semantics. This addresses the limitation of existing multimodal ReID methods that solely focus on the target foreground, which often leads to feature entanglement between the person and background distractions. FBA introduces an intra-semantic alignment strategy for fine-grained multimodal feature capturing and an inter-semantic adversarial learning strategy with a diversity loss to explicitly distinguish and penalize the feature distance between target and distractor regions, achieving competitive results on both holistic and occluded ReID benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The FBA framework is straightforward and intuitive. \n2. The authors conducted extensive experiments on representative datasets spanning four ReID domains."}, "weaknesses": {"value": "1. Attribute-based ReID methods, such as CLIP-ReID and MP-ReID, have demonstrated strong performance in general ReID tasks. MP-ReID [1], similar to FBA, employs large language models (LLMs) to generate additional prompts, achieving 95.50 mAP and 97.70 R@1 on Market1501, respectively, as well as 88.90 mAP and 95.70 R@1 on and DukeMTMC-reID. Although FBA also leverages LLMs to produce supplementary prompts, its performance improvements remain limited.\n\n2. Regarding experimental datasets, Market101 and MSMT are unavailable, and DukeMTMC-reID is discouraged for use due to privacy concerns.\n\n3.  The paper fails to compare its results against recently recognized strong baselines in the occluded ReID domain, specifically CNN-based BPBreID [2] and ViT-based KPR (ECCV 2024) [3].\n\n\nReference:\n\n[1] Zhai, Yajing, et al. \"Multi-prompts learning with cross-modal alignment for attribute-based person re-identification.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 7. 2024.\n\n[2] Somers, Vladimir, Christophe De Vleeschouwer, and Alexandre Alahi. \"Body part-based representation learning for occluded person re-identification.\" Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2023.\n\n[3] Somers, Vladimir, Alexandre Alahi, and Christophe De Vleeschouwer. \"Keypoint promptable re-identification.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024."}, "questions": {"value": "Using only foreground and background information to generate two corresponding prompts lacks sufficient granularity for the ReID domain. It is recommended that the authors incorporate part-level prompt learning to further enhance model performance."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "DukeMTMC-reID is discouraged for use due to privacy concerns."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bgprki16Do", "forum": "8olv02io0e", "replyto": "8olv02io0e", "signatures": ["ICLR.cc/2026/Conference/Submission13869/Reviewer_bBQg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13869/Reviewer_bBQg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653389875, "cdate": 1761653389875, "tmdate": 1762924385729, "mdate": 1762924385729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an end-to-end framework that jointly models foreground and background information within a dual-branch bidirectional cross-attention feature extraction pipeline. To help the network distinguish between the two domains, we propose an intra-semantic alignment and inter-semantic adversarial learning strategy. The proposed model achieves competitive results on the holistic and occluded ReID datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes an end-to-end framework that treats both foreground and background semantics as equally important for ReID. The idea is interesting."}, "weaknesses": {"value": "Please refer to Questions."}, "questions": {"value": "•\tThe paper does not discuss the potential impact of using different large language models (LLMs) for generating textual descriptions of the foreground and background regions. It would be valuable to clarify whether the quality of the generated descriptions influences the performance, and how sensitive the method is to variations in text generation quality.\n\n•\tThe mechanism for partitioning encoder outputs into foreground and background feature groups is not clearly explained. If the encoder features are shared between the two branches, it may raise concerns about the rationality.\n\n•\tThe term \"FCN\" in Figure 3 is ambiguous and lacks a clear definition in the text. Its role and structure should be explicitly described to avoid confusion.\n\n•\tThe paper excludes the Market-1501 dataset from evaluation, citing its noisy annotations and detection errors that may affect adversarial training stability. However, Market-1501 remains a standard benchmark in ReID research and has been widely used in prior work. To strengthen the validity of the claim, the authors are encouraged to include results on this dataset and provide a more in-depth analysis of how such noise impacts their method compared to existing approaches.\n\n•\tIn addition to commonly used datasets, the results should also be reported on MSMT17, another widely adopted benchmark in ReID—especially since recent state-of-the-art methods such as CLIP-ReID and PromptSG have evaluated their performance on it. Omitting MSMT17 limits the comprehensiveness of the evaluation.\n\n•\tThe proposed method employs several implementation tricks, including the sliding-window setting, larger input image size, and smaller stride setting. While these enhancements may contribute to performance gains, they also introduce unfair advantages in comparison with other methods. Notably, on DukeMTMC, CLIP-ReID achieves 83.1% mAP under the sliding-window setting, and TransReID reaches 82.6% mAP with image size 384×128, both outperforming the proposed method (81.7% mAP). Moreover, ablation studies on these design choices are only conducted on CUHK03-NP, leaving the performance on other datasets (e.g., DukeMTMC) under fair configurations unclear.\n\n•\tDoes a stronger identity-related attention across foreground and background correspond to a larger value of s_j? It is unclear why the weight w_j is defined as 1 − s_j rather than s_j. A justification for this design choice would be helpful.\n\n•\tThe proposed framework involves the additional text encoder and interaction modules during inference, which may increase model complexity and computational cost. The paper should provide a comparison of training and inference complexity with other methods to assess the practicality of the approach."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "Due to privacy concerns, the DukeMTMC dataset has been withdrawn by the publisher."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FCB6E57QNB", "forum": "8olv02io0e", "replyto": "8olv02io0e", "signatures": ["ICLR.cc/2026/Conference/Submission13869/Reviewer_LYzf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13869/Reviewer_LYzf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662545591, "cdate": 1761662545591, "tmdate": 1762924384849, "mdate": 1762924384849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}