{"id": "wFbZyGQeFa", "number": 20837, "cdate": 1758310737575, "mdate": 1759896956121, "content": {"title": "A Diffusion Model Induced by MSE Training", "abstract": "A diffusion model for image generation transforms noise into an image via a neural denoiser. The denoiser is trained with a time-integrated, weighted mean-squared error (MSE) between the noised image and the network’s prediction. The weighting is often absorbed into the noised image, yielding different parameterizations of the prediction (e.g., noise-, data-, or velocity parameterization). Thus, the denoiser is determined by the noise schedule and the chosen parameterization, whereas the generative diffusion process is specified by its noise and diffusion schedules (i.e., by both the scale and the variance-rate coefficients). In practice, the generator typically inherits only the noise schedule from the trained denoiser. In this work, guided by a principle of coherence between the MSE training objective and maximum-likelihood (ML) proximity of the induced processes, we derive a closed-form expression for the diffusion schedule given a noise schedule and a network parameterization. Widely used methods train on one (implicit) process but generate with another—often one with an optimal diffusion schedule in the ML sense, or even with zero diffusion, that is a deterministic flow. Recent empirical approaches yield diffusion schedules closer to our formula, which supports the coherence principle and suggests that it is beneficial to generate samples using the very process that is actually learned. We analyze both discrete-time and continuous-time models using elementary autoregressive arguments, yielding formulas that are simpler than those used so far. In particular, we provide a representation of the diffusion state as the sum of an explicit linear component, an unweighted pathwise integral of the denoiser, and a noise term. This representation makes it straightforward to apply classical numerical integration methods and clarifies the relation to the DPM-solver family.", "tldr": "MSE-Induced Diffusion", "keywords": ["diffusion models", "score-based generative modeling", "generative models", "deep learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bff43aea59263bc16cae5aca3af62e87cb70e546.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses a common discrepancy in diffusion models where the training process (defined by an MSE objective) differs from the generation process. \n\nThe authors propose a \"principle of coherence\" to align them, deriving a closed-form expression for the generative diffusion schedule based on the specific noise schedule and network parameterization used during training. This \"MSE-induced diffusion\" is intended to be the process the denoiser actually learned , and the paper also provides a new state representation to simplify numerical integration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper has a clear motivation, it highlights a common mismatch in diffusion models where the denoiser is trained using one implicit process defined by an MSE objective, noise schedule, and network parameterization but is then used for generation with a different process, e.g., one with an ML-optimal diffusion schedule or a deterministic flow\n\n2. The work derives a closed-form expression for the diffusion schedule. This means the generative process isn't an arbitrary choice but is analytically determined by the noise schedule and network parameterization used during training. This is called \"MSE-induced diffusion\""}, "weaknesses": {"value": "The most important issue is that the article lacks practical evidence\n\n1. The paper is entirely theoretical. It proposes the MSE-induced diffusion process based on a coherence principle , but it presents no experiments to demonstrate that this new process is stable, effective, or produces better results than the \"incoherent\" methods it critiques\n\n2. The paper claims its new state representation e.g. \"makes it straightforward to apply classical numerical integration methods\" and clarifies the relation to DPM-solvers. It even alludes to the potential for new solvers like a 4th-order Runge-Kutta. However, it does not actually implement or test any such solver, so these practical benefits remain hypothetical.\n\n3. The primary evidence for the theory's validity is Figure 1. This figure merely shows a visual similarity between the shape of derived schedule and schedules found empirically in other recent work, e.g., empPML, Discount. This correlation is interesting but does not prove that the coherence principle is the reason for that shape or that the resulting process is effective.\n\n4. The paper critiques widely used deterministic flows as being incoherent. It then proposes its stochastic coherent process as a better alternative. However, it doesn't sufficiently justify why coherence is inherently superior to the fast and high-performing deterministic flows, largely assuming that aligning the training and sampling processes is axiomatically beneficial"}, "questions": {"value": "I hope the author can provide enough evidence to prove the effectiveness of the methods and theories. Especially the followings:\n\n1. The core claim is that **it is beneficial to generate samples using the very process that is actually learned**. This is a compelling, testable hypothesis. However, the paper is entirely theoretical, and its primary evidence (Figure 1) is correlational .\nCould the authors provide any preliminary empirical results (e.g., FID scores, sample quality on a standard benchmark like CIFAR-10) to support this central hypothesis?\n\n2. The paper makes strong, practical claims about its new state representation (Eq. 12 ), suggesting it **makes it straightforward to apply classical numerical integration methods** and potentially enables new solvers, like an RK4 analogue, which are currently difficult for DPM-solvers. Have the authors performed any proof-of-concept implementations of a numerical solver using this new unweighted pathwise integral formulation?\n\n3. The paper critiques widely used deterministic flows where $\\lambda_t' \\rightarrow 0$ and explicitly contrasts them with its proposed MSE-induced process, which is stochastic. Given the demonstrated speed and high performance of incoherent deterministic samplers, what is the specific benefit of adhering to the coherence principle?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PhEBy09pFI", "forum": "wFbZyGQeFa", "replyto": "wFbZyGQeFa", "signatures": ["ICLR.cc/2026/Conference/Submission20837/Reviewer_3Cp4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20837/Reviewer_3Cp4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843802927, "cdate": 1761843802927, "tmdate": 1762936330727, "mdate": 1762936330727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a principle of coherence between the training and generation process of the diffusion model: if a denoiser is trained with time-weighted MSE under a given noise schedule and parameterization, then the generation process should use a matching diffusion schedule derived in closed form in the paper. The authors analyze both discrete- and continuous-time settings via simple autoregressive arguments and introduce a state representation that makes connection to classic ODE solvers straightforward."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper derives an analytical formula for the diffusion schedule given the training noise schedule and network parameterization."}, "weaknesses": {"value": "The paper is predominantly analytical and offers only extremely light empirical glimpse—mainly a figure comparing shapes of diffusion schedules under various noise/parameterization choices. The idea behind analysis is not deep enough, the analysis itself is not mathematically challenging. Although the work argues its state representation makes classic ODE solvers like RK4 straightforward to use, it does not demonstrate numerical benefits versus modern diffusion solvers. Overall the paper lacks a clear result and falls well below the standard of ICLR."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xVzxDHUKIU", "forum": "wFbZyGQeFa", "replyto": "wFbZyGQeFa", "signatures": ["ICLR.cc/2026/Conference/Submission20837/Reviewer_Y5wa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20837/Reviewer_Y5wa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849684354, "cdate": 1761849684354, "tmdate": 1762936330267, "mdate": 1762936330267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework called “MSE-Induced Coherent Diffusion.” Starting from weighted MSE training (given a noise schedule and parameterization), the authors introduce an “MSE–ML Coherence Principle,” which provides a closed-form expression for the generative diffusion rate (Proposition 3). This leads to a generative process theoretically consistent with the training objective. They also reformulate the diffusion state as a combination of a linear term, an unweighted path integral, and a noise term, enabling direct use of standard ODE solvers (e.g., RK4). The paper argues that in practice, researchers often “train one schedule but sample with another” (e.g., ML-optimal or deterministic zero-diffusion flows), and advocates using a generative process coherent with the training objective, providing closed-form and discrete formulations consistent with recent empirical practices."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Theoretical Novelty – The Coherence Principle.**\n\nA major ambiguity in diffusion models lies in the disconnect between training and inference processes. The authors attempt to resolve this by introducing the Coherence Principle, which asserts that the empirical MSE loss and the theoretical ML objective should be proportional over any time interval $[t_0, t]$. This is a novel and well-defined (strong) theoretical assumption."}, "weaknesses": {"value": "1. Unconvincing Motivation.\n\nThe authors claim to eliminate inconsistency between training and inference noise schedules, but this is unnecessary. The core idea of diffusion models (e.g., DDPM, VP-SDE, Rectified Flow) is distribution matching between the forward and reverse processes. As long as the marginal distributions match, the generative model is valid. There are infinitely many possible paths that share the same marginal distributions—for example, infinitely many SDEs corresponding to the same VP-SDE marginals, or an ODE with equivalent marginals. Once the reverse parameter (score, noise, or velocity) is learned, there theoretically exist infinitely many valid sampling schemes. Therefore, enforcing the Coherence Principle is not inherently necessary.\n\n2. Lack of Experiments / Related Work.\n\nThe authors argue that the noise schedule determined by the Coherence Principle is beneficial, citing a few works (Cui et al., 2025; Ma et al., 2024) that allegedly conform to it. However, this reasoning is flawed. The examples are too limited, as many SOTA methods do not satisfy the Coherence Principle, yet were selectively omitted. Moreover, no experiments are presented. If deterministic sampling is said to violate the principle, the authors must show that all schedules coherent with it outperform deterministic ODE sampling; otherwise, the claimed advantage of the Coherence Principle remains unsubstantiated."}, "questions": {"value": "1. Can the authors justify the necessity of the Coherence Principle? It is a very strong assumption, yet the paper devotes too little space to explaining or motivating it.\n2. Can the validity of the Coherence Principle be verified through extensive ablation and reasoning experiments, rather than being limited to the few cited works?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sDPbT6GgSS", "forum": "wFbZyGQeFa", "replyto": "wFbZyGQeFa", "signatures": ["ICLR.cc/2026/Conference/Submission20837/Reviewer_88xM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20837/Reviewer_88xM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762325675060, "cdate": 1762325675060, "tmdate": 1762936329793, "mdate": 1762936329793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}