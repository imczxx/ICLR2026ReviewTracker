{"id": "yDqrGP4w2E", "number": 12461, "cdate": 1758207982397, "mdate": 1762924134696, "content": {"title": "AdaFusionNet: Disentangling and Fusing Asynchronous Patterns for Long-Range Time Series Forecasting", "abstract": "Long-range time-series forecasting (LTSF) is hard because real sequences superpose asynchronous patterns—slow trends and fast seasonalities. We identify a failure mode of homogeneous architectures, \\emph{trend contamination}, where high-frequency dynamics corrupt the learned trend and degrade long-horizon accuracy. We introduce \\textbf{AdaFusionNet}, a structured methodology that \\emph{Disentangles, Specializes, and Fuses}. First, a learnable projection adaptively separates low- and high-frequency components. Second, heterogeneous streams match model capacity to component complexity (lightweight MLP for trends; patch-wise CNN for residuals). Third, a synergistic fusion block recombines component forecasts and models cross-channel interactions.\n\nOn the theory side, we prove: (i) an upper bound on spectral leakage and consistency of the decomposition parameter; (ii) mixed-smoothness approximation gains with additive Rademacher-complexity bounds; (iii) a fusion oracle inequality and Lipschitz stability that yield distributional-robustness certificates (Wasserstein-DRO) and PAC-Bayes uncertainty control; and (iv) split-conformal prediction with distribution-free coverage. Empirically, across eight standard LTSF benchmarks and four horizons (96–720), AdaFusionNet delivers consistently strong—often state-of-the-art—accuracy, with the largest gains at long horizons; ablations validate each stage and the proposed diagnostics for leakage/asynchrony. Code and reproducible scripts will be released upon publication. These results indicate that structuring LTSF as \\emph{disentangle $\\rightarrow$ specialize $\\rightarrow$ fuse} is a robust, scalable alternative to the prevailing homogeneous paradigm.", "tldr": "AdaFusionNet learns an interpretable EMA-based adaptive decomposition ($\\alpha$) that enables heterogeneous complexity matching (MLP for trend, CNN for residual), with provable optimality and tighter generalization for time-series forecasting.", "keywords": ["Long-Range Time Series Forecasting", "Deep Learning", "Time Series Decomposition", "Heterogeneous  Architectures", "Model Interpretability", "Representation Learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/72286cf4562779aaac4b626844552da60180ff9b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AdaFusionNet, a disentangle-specialize-fuse pipeline for time series forecasting. It includes an EMA to extract the trend and MLP/CNN fusion to yield predictions. It reports strong performance on a few datasets."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "S1 (performance): The proposed method reports strong empirical performance across a few datasets.\n\nS2 (simplicity): The proposed modules are simple and easy to understand.\n\nS3 (illustration): The paper provides an illustrative figure (Figure 2)."}, "weaknesses": {"value": "W1 (poor presentation): The proposed method is introduced in only 20 lines of text with almost no explanations. The majority of the text is jabberwocky as I will explain in W2-5.\n\nW2 (no proofs): Theorems 2.4--2.10 have no proofs.\n\nW3 (inexistent theorems): The appendix provides the proofs of \"Theorem 1\" and \"Lemma 1\", but Theorem 1 and Lemma 1 do not even appear in the paper.\n\nW4 (trivial/wrong theorems in Section 2): Section 2 includes a lot of theoretical results, but all of them are either (i) extremely trivial or (ii) wrong. For example, (i) Proposition 2.7 is just a simple application of the chain rule in calculus; (ii) many terms in Theorem 2.4 are undefined.\n\nW5 (irrelevant theorems in Section 3): Theoretical results in Section 3 are just general learning theory irrelevant to the proposed method. This paper has not attempted to connect these general theories to their work. This paper even fails to explain what these theories mean."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Considering the extreme inconsistency between the main body and the appendix, it seems that the majority of this paper is generated by hallucinated LLMs. However, such excessive LLM usage is not declared in the paper."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R1acCRWXuT", "forum": "yDqrGP4w2E", "replyto": "yDqrGP4w2E", "signatures": ["ICLR.cc/2026/Conference/Submission12461/Reviewer_vwVe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12461/Reviewer_vwVe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792530679, "cdate": 1761792530679, "tmdate": 1762923341990, "mdate": 1762923341990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "gtSMxnf02k", "forum": "yDqrGP4w2E", "replyto": "yDqrGP4w2E", "signatures": ["ICLR.cc/2026/Conference/Submission12461/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12461/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762924133782, "cdate": 1762924133782, "tmdate": 1762924133782, "mdate": 1762924133782, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### *Summary*\n\n- This work proposes AdaFusionNet, an architecture that forecasts long-horizon time series. AdaFusionNet learns a low-pass filter to disentangle trend and residual streams, then processes the trend and residual streams separately, and fuses them back together at the end of the architecture to obtain a final prediction. The authors provide multiple theoretical results related to robustness of the proposed model. The authors perform experiments on 8 common long-horizon time series forecasting datasets\n\n### *Contributions*\n\n- AdaFusionNet, an architecture with two key contributions:  \n  - A learnable decomposition module  \n  - Separate processing of trend and residuals by MLPs and CNNs\n- A theoretical analysis under Lipschitz-style constraints"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### *Originality*\n\n- A novel decomposition-based architecture with strong performance on known datasets\n\n### *Quality*\n\n- Ablations of key architectural contributions\n\n### *Clarity*\n\n- Succinct writing, well structured  \n- Clearly laid-out contributions\n\n### *Significance*\n\n- Furthers the discussion around decomposition-based time series forecasting methods"}, "weaknesses": {"value": "### *Originality*\n\n- Baselines and related work is missing for 2024-2025. A simple search turned these up:  \n  - Hao, Jianhua, and Fangai Liu. \"Improving long-term multivariate time series forecasting with a seasonal-trend decomposition-based 2-dimensional temporal convolution dense network.\" Scientific Reports 14.1 (2024): 1689\\.  \n  - Piao et al. (2024), “Fredformer: Frequency Debiased Transformer for Time Series Forecasting” (KDD 2024\\)  \n  - Tian et al. (2024), “MultiWaveNet: A long time series forecasting framework based on multi-scale analysis and multi-channel feature fusion.”\n\n### *Quality*\n\n- These datasets are saturated. See [https://arxiv.org/abs/2510.02729](https://arxiv.org/abs/2510.02729).  \n- **Dated baselines**: TimeXer tops the thuml library, but is missing from your tables. There is no comparison to SOTA models, and all related works go back to 2023 whereas it’s nearly 2026\\. I would check out [https://huggingface.co/spaces/Salesforce/GIFT-Eval](https://huggingface.co/spaces/Salesforce/GIFT-Eval) for a list of relevant recent baseline methods.  \n- There is **insufficient analysis** to corroborate the **why it works** section starting on L481 because the experimental protocol is missing details. What was your hyperparameter selection strategy for testing these different architectures? Beyond demonstrating the results are better through ablations, there needs to be more investigation of the key components. For example, have you tried values other than \\\\alpha=0.2 (L414)?  I would start by varying the learning rate for the different architectural possibilities/ablations and seeing how the results change. If your architecture holds up, this additionally provides defense against claims of hyperparameter sensitivity.  \n- Results do not line up with previous work (e.g. TimeMixer, table 3\\)  \n- No analysis of FLOPs/runtimes  \n- You may want to consider bootstrapping confidence intervals to validate whether your model is statistically significantly better than the baselines  \n- The results for the ablation on Heterogeneous processing can just go in the appendix instead of being omitted completely.  \n- Ideally, there’d be some sort of analysis of the internal representations to validate whether your architecture is actually resulting in cleaner internal representations.\n\n### *Clarity*\n\n- Figure fonts are tiny, which makes them illegible\n- The theoretical text in section 3 is so dense as to be hard to parse. This makes the main takeaways unclear.  \n  - The phrase “Fusion weights …” starting on L115 is not even a coherent sentence.  \n  - Domain-specific acronyms are not introduced “e.g. PSD”.   \n  - Backloading the discussion of the theorems is a good space-saving technique, but is not so good for reading the paper"}, "questions": {"value": "The main items holding my score back are: \n\n- A more recent, comprehensive set of baselines\n- Evaluation on a non-saturated benchmark\n- Improved empirical analysis accompanying the **why it works** section (L481)\n\nEach of those are major points, each worth about two points in my mind. Completely all of them would raise my score to accept. However, without all three of them, I don't think I can go above a 4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lzfgN78iaI", "forum": "yDqrGP4w2E", "replyto": "yDqrGP4w2E", "signatures": ["ICLR.cc/2026/Conference/Submission12461/Reviewer_zDV3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12461/Reviewer_zDV3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877822072, "cdate": 1761877822072, "tmdate": 1762923341728, "mdate": 1762923341728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the overpopulated long-horizon forecasting task and proposes AdaFusionNet, an architecture designed to mitigate “spectral leakage” by disentangling, specializing, and fusing forecast components.\n\nHowever, the model is tested only on minuscule/over-exploited long-horizon benchmarks, casting doubts on the validity of their findings and generalization to real applications. \n\nThe theoretical foundation of AdaFusionNet relies on stationarity assumptions, which conflict with the presence of trends (a central challenge in long-horizon forecasting)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors present a reasonable method of forecast decomposition. However, the novelty of their approach is quite limited. See NBEATS/NHITS projections.\n\n- The authors should read the ESRNN paper. ESRNN has a very similar intuition to that of AdaFusionNet. The overlap is uncanny."}, "weaknesses": {"value": "- AdaFusionNet novelty is quite limited. It is basically rediscovering the M4 competition winner method, the ESRNN. It is available for free in the International Journal of Forecasting portal.\n\n- The authors make bold claims about the ability of the method to recover uncontaminated trends. Perhaps they should demonstrate this ability with simulated data, where they can control the trend signal. It is a wasted opportunity to limit your analysis to the long-horizon datasets.\n\n- There are several papers in the statistical and machine learning forecasting literature that deal with trend decomposition. Please update your literature to mention relevant literature.\n\n- The experiments in the paper are limited to ETT, Exchange, ILI Traffic, and Weather, which at this point are analogous to MNIST in forecasting research. Authors should strive to enhance the comprehensiveness of the datasets on which they base their ideas. Perhaps M1, M3, and M4 can help to increase confidence in the validity of their approach.\n\n- From the hundreds of papers on the long-horizon forecasting task, why are the authors restricting their comparison to iTransformer, ETSFormer, CARD, TimeMixer, AutoFormer, Informer, and FEDFormer? None of these methods is \"state-of-the-art\".\n\n- The authors omit any mention of AdaFusionNet hyperparameters. The authors also omit the code. There are several replicability problems."}, "questions": {"value": "- Why is Exponential Moving Average (EMA) not introduced at the beginning or in the abstract?\n\n- Theorem 2.6 relies on stationarity. Time series more often than not are not stationary. What is the applicability of this theorem?\n\n- It has been shown that for most long-horizon datasets using multivariate series is not reasonable. Why is the architecture multivariate instead of univariate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SMGVSKC8UA", "forum": "yDqrGP4w2E", "replyto": "yDqrGP4w2E", "signatures": ["ICLR.cc/2026/Conference/Submission12461/Reviewer_ybdr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12461/Reviewer_ybdr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942474834, "cdate": 1761942474834, "tmdate": 1762923341329, "mdate": 1762923341329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the trend contamination problem in long-horizon time series forecasting, where high-frequency fluctuations leak into learned trends, impairing extrapolation accuracy. The model follows a disentangle-specialize-fuse paradigm: (1) a learnable exponential moving average low-pass filter adaptively separates trend and residual components via a trainable smoothing parameter; (2) heterogeneous streams match complexity to each component -- a lightweight MLP for the smooth trend and a patch-wise CNN for volatile residuals; (3) a fusion block combines per-channel predictions and models cross-variate dependencies. Theoretically, the authors model the decomposition as an adaptive projection, derive a leakage-aware risk bound with gradient updates, and establish generalization, robustness, and uncertainty guarantees. Empirically, AdaFusionNet achieves strong performance, with larger gains at longer horizons. Ablations confirm the value of learnable α and heterogeneous streams."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "S1: The authors propose a learnable decomposition method that adaptively separates trend and residual components. As shown in the visual analysis, the extracted trend contains fewer periodic features, indicating effective disentanglement from high-frequency dynamics. This decomposition approach can also serve as a general plug-in module for other models focusing on seasonal–trend disentanglement.\n\nS2: The leakage-aware risk decomposition (Theorem 2.4) provides mechanistic explanations for why learning \\alpha reduces spectral leakage. Section 3 rigorously links spectral leakage and coherence to robustness, deriving leakage-aware risk bounds that improve stability under noise, missing data, and distribution shifts. It further provides PAC-Bayes and conformal guarantees for calibrated uncertainty.\n\nS3: The proposed framework outperforms advanced baselines. Ablation studies demonstrate the advantages of key components."}, "weaknesses": {"value": "W1: The experiments show the effectiveness of the approach within the MLP-CNN and fuse architecture, but it remains unclear whether the proposed decomposition mechanism is universally applicable to other architectures. If it does, the approach would be even more valuable and broadly impactful.\n\nW2: Many real-world time series contain sudden yet continuous regime changes or rapidly evolving patterns (e.g., traffic spikes and stock market). The proposed adaptive decomposition may struggle to fully capture such fast transitions, as the learnable EMA filter primarily models gradual variations in trend–residual dynamics. The current visualizations also mainly focus on relatively simple trends and periodic patterns, which may not fully reflect the model’s behavior on more complex, rapidly changing, or irregular real-world series.\n\nW3: The use of MLP for trend modeling and CNN for residual modeling is mainly motivated by intuitive matching. However, the paper provides limited empirical or theoretical justification for this specific pairing."}, "questions": {"value": "Q1: Can the proposed adaptive decomposition mechanism generalize to other architectures beyond the MLP–CNN design, such as Transformers or recurrent models?\n\nQ2: How does the method perform on datasets with rapid regime changes or highly non-stationary patterns? Will the proposed framework produce over-smoothing results? Could the authors provide visualizations or analyses on such challenging scenarios?\n\nQ3: What motivated the specific choice of MLP for trends and CNN for residuals? Have the authors tried alternative architectural pairings or verified whether this design choice is essential for the observed performance gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R7lrRmC7kr", "forum": "yDqrGP4w2E", "replyto": "yDqrGP4w2E", "signatures": ["ICLR.cc/2026/Conference/Submission12461/Reviewer_ezhF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12461/Reviewer_ezhF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994041025, "cdate": 1761994041025, "tmdate": 1762923340894, "mdate": 1762923340894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}