{"id": "KGgSlCGwB0", "number": 8689, "cdate": 1758095049325, "mdate": 1763713309336, "content": {"title": "Continuous Speculative Decoding for Autoregressive Image Generation", "abstract": "Continuous visual autoregressive (AR) models have demonstrated promising performance in image generation. However, the heavy autoregressive inference burden imposes significant overhead. In Large Language Models (LLMs), speculative decoding has effectively accelerated discrete autoregressive inference. However, the absence of an analogous theory for continuous distributions precludes its use in accelerating continuous AR models.\nTo fill this gap, this work presents continuous speculative decoding, and addresses challenges from: 1) low acceptance rate, caused by inconsistent output distribution between target and draft models, and 2) modified distribution without analytic expression, caused by complex integral. To address challenge 1), we propose denoising trajectory alignment and token pre-filling strategies. To address challenge 2), we introduce acceptance-rejection sampling algorithm with an appropriate upper bound, thereby avoiding explicitly calculating the integral. Furthermore, our denoising trajectory alignment is also reused in acceptance-rejection sampling, effectively avoiding repetitive diffusion model inference.\nExtensive experiments demonstrate that our proposed continuous speculative decoding achieves over $2\\times$ speedup on off-the-shelf models, while maintaining the original generation quality.", "tldr": "This paper propose Continuous Speculative Decoding to accelerate the inference speed of  continuous visual autoregressive models while maintaining the generation quality.", "keywords": ["Speculative decoding", "Visual Autoregressive Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/360488c12e58a414fedeacd7601f65655063c989.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new speculative decoding algorithm, especially for diffusion-head-based continuous tokenizer AR models such as MAR. Specifically, to realize the speculative sampling process in continuous space, it (i) first simplifies the likelihood ratio evaluation (p/q) using a one-step denoising ratio to determine the acceptance rate, and (ii) performs rejection-sampling-based resampling from the residual distribution. The experimental results show that this method accelerates the decoding process by ~2x while maintaining image quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and easy to understand.\n- To my knowledge, this is almost the first work that tries to adopt speculative decoding in continuous space.\n- The results shows promising acceleration ratios in various settings."}, "weaknesses": {"value": "I think this paper build upon some critical theoretical flaws and is not correct Speculative Decoding (SD) :\n- Eq. (2) is not correct. The left side, $p(x_0|x_T)$, is the marginal probability of $x_0$, but the right side is actually the joint probability of the trajectory, $p(x_{0:{T-1}}|x_T)$. The correct relation is $p(x_0\\mid x_T)=\\int \\Pi_{t=1}^T p(x_{t-1}|x_t) dx_{1:T-1}$. This invalidates all following discussions and methods, as they all rely on an incorrect $p/q$ ratio.\n\n- If the authors intended to perform SD on the path space $p_{path}(x_{0:T-1}|x_T)$ and pick $x_0$ from an accepted/resampled path, then it would be correct. However, the current algorithm does not actually do this.\n  - Correct path-space SD requires the likelihood ratio $p_{path}(Y)/q_{path}(Y)$ on a **single fixed path** $Y = [x_0, x_1, .. x_T]$, which is generated by the draft model $q_{path}$.\n  - However, \"noise alignment\" actually yields two different paths, $Y_p$ and $Y_q$. They typically do not follow the same path, even if we use the same noise at the start and during denoising. Equation (8) is just the ratio of self-likelihoods under two different self-paths, and is neither (i) the correct $p(x_0)/q(x_0)$ nor (ii) the correct $p_{path}(Y)/q_{path}(Y)$. Thus, the lossless property of SD cannot be achieved, and the resampling process is also incorrect because it depends on the wrong $p/q$ ratio.\n- I think this paper is closer to lossy SD, which tries to approximate the intractable $p(x)/q(x)$ by a practically feasible surrogate $p(x_0|x^p_1)/q(x_0|x^q_1)$ (which is the last-step denoising difference under different self-paths), and paper shows that final image quality can still be almost maintained. If so, the motivation for the proposed methods is weak, because they focus mostly on computation tricks and the sampling process for exactly recovering $p(x)$ by SD."}, "questions": {"value": "- While the Target model and Draft model are different AR models, do they share the same diffusion model head?\n\n- In Tables 1 and 2, why does the acceptance rate decrease as the draft length increases?\n\n- In Tables 1 and 2, why does the speed-up increase as the batch size increases? Using larger batch size increase computational overhead and typically increase latency.\n\n- In Figures 7 and 12, why do the 'original' and 'SD' samples show sample-level identity? SD basically just guarantees distribution-level identity  with original AR decoding, not individual sample-level identity.\n\n- In Table 4, why does 'alignment' increase the acceptance rate? The acceptance rates should only depends on the divergence between the target model ($p$) and the draft model ($q$), actually $1 - \\text{TotalVariation}(p, q)$ [1]. Does this imply that the alignment process itself modifies the draft distribution $q$?\n\n[1] SpecTr: Fast Speculative Decoding via Optimal Transport; NeurIPS 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PMGBmhOoT2", "forum": "KGgSlCGwB0", "replyto": "KGgSlCGwB0", "signatures": ["ICLR.cc/2026/Conference/Submission8689/Reviewer_gror"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8689/Reviewer_gror"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825651913, "cdate": 1761825651913, "tmdate": 1762920499081, "mdate": 1762920499081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript introduces Continuous Speculative Decoding (CSD) to accelerate inference for continuous visual autoregressive (AR) image generators whose per-token distributions are implemented via diffusion. The method mirrors discrete speculative decoding (draft-and-verify) but tackles two problems for continuous in continuous setting: 1) very low acceptance due to draft-verify distribution mismatch and 2) the intractability of sampling from the modified reject distribution. To this end, the manuscript proposes denoising trajectory alignment (sharing the diffusion noise across draft/target) and token pre-filling (seed the prefix with a small fraction of target tokens) to raise acceptance, and use acceptance–rejection sampling with a derived upper bound to sample from the modified distribution without evaluating a difficult integral. On various models, they report up to ~2.3–2.7× speedups at larger batch sizes while keeping image quality roughly unchanged."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It is the first to apply speculative decoding in continuous settings.\n- Discovers practical problems and handles them by proposing techniques to raise acceptance in practice.\n- The method is training-free, facilitating practicality in deployment."}, "weaknesses": {"value": "- Most speedups occur at large verification batch sizes, whereas bsz=1 shows diminished speedups. For many interactive or small-batch generation workloads, the practical acceleration may be lower than the headline.\n- Quality preservation is assessed primarily with FID/IS. There is no evaluation of text-image faithfulness, such as CLIPScore or GenEval."}, "questions": {"value": "I wish to defer this to the discussion phase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cEbKSBjg3g", "forum": "KGgSlCGwB0", "replyto": "KGgSlCGwB0", "signatures": ["ICLR.cc/2026/Conference/Submission8689/Reviewer_EoYy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8689/Reviewer_EoYy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762080189407, "cdate": 1762080189407, "tmdate": 1762920498667, "mdate": 1762920498667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new framework extending speculative decoding from discrete to continuous AR models, particularly diffusion-based visual AR systems. Traditional speculative decoding on continuous distributions poses two key obstacles: (1) inconsistent output distributions between draft and target models, which lead to low acceptance rates, and (2) the absence of an analytic form for the modified distribution due to complex normalization integrals. To tackle these, the authors propose denoising trajectory alignment and token pre-filling, and  they derive an acceptance–rejection sampling algorithm with a tractable upper bound, eliminating the need for intractable integrals, and reuse trajectory alignment to compute rejection thresholds efficiently. The method integrates seamlessly into existing continuous AR models without retraining or architectural changes."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s strongest aspect is that it identifies and articulates a genuinely nontrivial gap between discrete speculative decoding and continuous, diffusion-based AR generation, and then proposes a concrete recipe to fill it. Recognizing that the core obstacle is distributional inconsistency is an original reframing, and it is precisely this reframing that motivates the two key ideas, denoising trajectory alignment and token pre-filling, rather than importing the discrete algorithm verbatim. \n\nIn terms of originality, this is not just “we do speculative decoding for images now,” but “we make speculative decoding compatible with diffusion-style tokenization,” which, to my knowledge, is not handled in earlier speculative work on discrete visual AR or in speedup methods that simply prune or distill the denoiser. The proposal to align draft/target diffusion paths via shared reparameterization noise is also a creative repurposing of a well-known trick (noise sharing) to solve an acceptance-rate issue rather than a sample-quality issue, which is a nice shift of perspective."}, "weaknesses": {"value": "The acceptance ratio in Eq. (2) is written as if it used the marginal, but what is actually computed is the factorized reverse-diffusion path probability, i.e. a joint over a sampled trajectory. This is only equal to the desired marginal when you integrate out intermediate states, which the method does not do.Because the same approximation is applied to both p and q, the authors hope the error cancels, but that cancellation is only heuristic and depends critically on the two denoising paths being very close. The proposed denoising trajectory alignment is not just an “improvement”; it is structurally necessary for the rest of the method to work. The appendix admits that computing p(x) and q(x) “is algebraically correct but may lead to a low acceptance rate due to a distinct denoising trajectory,” and therefore they must reuse the same noise to align trajectories. That means the paper’s contribution is really a coupled draft–target diffusion procedure, not a drop-in verifier like in the discrete case.  Therefore,  the “we avoid the intractable integral via acceptance–rejection” part is only half the story: the acceptance–rejection sampler still relies on an upper bound derived from the same approximate ratio and from reuse of alignment. If alignment fails or the draft is noticeably weaker than the target (a realistic regime), the bound may become loose, pushing the algorithm toward low acceptance again and eroding the reported 2× gains.\n\nThe paper compares mainly against the “no speculative” baseline, i.e. against itself without the proposed techniques. But there is an emerging line of work on accelerating diffusion/AR hybrids (Jacobi-style updates, masked/relaxed token generation, multi-path decoding for visual AR) to which this paper should be more directly compared; right now, it is hard to tell whether the net 2× improvement is better than, say, using a cheaper denoiser for late steps or a partial-step verifier. A small-scale comparison—even if approximate—would help position the work."}, "questions": {"value": "The current form appears to represent a joint probability over diffusion trajectories rather than the stated marginal conditional. Could the authors explicitly justify this approximation, and if so, quantify or bound the resulting error? Providing a short derivation showing under what assumptions the substitution is valid (e.g., under trajectory alignment or independence assumptions) would remove a major theoretical ambiguity.\n\nThe paper claims that continuous speculative decoding “maintains the original distribution of the target model,” yet the acceptance ratio uses approximated quantities and shared noise alignment. What exactly is preserved—expectation, trajectory distribution, or marginal image statistics? A formal clarification of what “maintaining” means in this continuous context would make the claim more precise.\n\nThe experiments focus on a “no speculative decoding” baseline. Including comparisons to alternative continuous AR acceleration techniques (e.g., partial-step distillation, diffusion pruning, or early-exit decoding) would better position the work’s novelty and practical relevance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M5ZX5OWP1N", "forum": "KGgSlCGwB0", "replyto": "KGgSlCGwB0", "signatures": ["ICLR.cc/2026/Conference/Submission8689/Reviewer_oZF2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8689/Reviewer_oZF2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762085969359, "cdate": 1762085969359, "tmdate": 1762920498304, "mdate": 1762920498304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Issue 1: The approximation of diffusion distribution"}, "comment": {"value": "We thank the reviewers for their valuable observations. As pointed out by reviewers oZF2 and gror, the exact probability is $p(x_0\\mid x_T)=\\int \\Pi_{t=1}^T p(x_{t-1}|x_t) dx_{1:T-1}$. However, this integral is analytically intractable. Reviewer gror suggested using joint probability of a single fixed path $p_{path}(Y), Y = [x_0, x_1, .. x_T]$, as an alternative. \nHowever, this approach is impractical for speculative decoding because the acceptance rate is low. To validate this point, we empirically recorded the average value of different kinds of likelihood ratios over 10,000 samples with a draft length of 4, including: (i) the single-path ratio $p_{path}(Y)/q_{path}(Y)$, (ii) the two-path ratio $p_{path}(Y_p)/q_{path}(Y_q)$ without denoising trajectory alignment, and (iii) the two-path ratio $p_{path}(Y_p)/q_{path}(Y_q)$ with denoising trajectory alignment, as shown in the table below.\n\nAs shown in the first row, the path-space likelihood ratio is extremely small, leading to a 0% acceptance rate. This is because the draft model’s trajectory $Y_p$ inherently diverges from the target model’s expected trajectory. In each denoising step, samples drawn from the draft model's distribution $q$ are unlikely to fall near $\\mu$ of the target distribution $p$, which results in a low single-step ratio $p/q$. As the multi-step denoising process proceeds, the overall $p_{path}(Y)/q_{path}(Y)$ becomes extremely small.\n\nThe second row compares ratios derived from independent trajectories ($Y_p$ and $Y_q$), while the final $x_0$ is generated by the draft model. This $x_0$, without alignment, is highly unlikely to fall near the target model's target distribution. However, the probability values for the other steps in these trajectories are derived from the model's own path, and thus maintain a relatively reasonable value.\n\nThe third row incorporates denoising trajectory alignment. This improvement is twofold: first, our manuscript demonstrates that the expected distance decreases; and second, our analysis shows that the correlation of $p$ and $q$ becomes 1 (common issue 2). Consequently, the samples generated by $q$ have a high probability under $p$, resulting in an increased $p/q$.\n\nFor the above reasons, our work adopts a practical approximation for $p(x_0|x_T)/q(x_0|x_T)$: the ratio of the joint probabilities $p_{path}(Y_p)/q_{path}(Y_q)$, where both $Y_p$ and $Y_q$ share the same $x_0$. This ratio serves as a surrogate for the intractable marginal ratio. By utilizing denoising trajectory alignment, we ensure that $Y_p$ and $Y_q$ are tightly coupled (as discussed in common issue 2), making the likelihood ratio a valid surrogate and ensuring its numerical stability in practical applications.\n\nCrucially, our contributions are not merely 'computation tricks'. We address the core challenge of adapting SD to continuous distributions by resolving intractability. The denoising trajectory alignment and token pre-filling are essential to ensure a practical acceptance rate, while acceptance-rejection sampling with a derived upper bound resolves the resampling intractability.\n\nWe have included the clarification of this practically feasible approximation for $p(x_0|x_T)/q(x_0|x_T)$ in our manuscript.\n\n| Likelihood ratio                         | Value    | Acceptance rate |\n| ---------------------------------------- | -------- | --------------- |\n| $p_{path}(Y)/q_{path}(Y)$                | 5.33e-23 | 0.0%            |\n| $p_{path}(Y_p)/q_{path}(Y_q)$, w/o align | 0.067    | 14%             |\n| $p_{path}(Y_p)/_{path}q(Y_q)$, w/ align  | 1.86     | 32%             |"}}, "id": "J0Mt4gXVnK", "forum": "KGgSlCGwB0", "replyto": "KGgSlCGwB0", "signatures": ["ICLR.cc/2026/Conference/Submission8689/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8689/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8689/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763711923921, "cdate": 1763711923921, "tmdate": 1763711923921, "mdate": 1763711923921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper formalizes speculative decoding for continuous-valued autoregressive image models and proposes Continuous Speculative Decoding, combining a theoretically grounded acceptance rule with denoising trajectory alignment and token pre-filling to address low acceptance rates and distribution mismatch."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a clear and timely problem since speculative decoding methods have so far been limited to discrete token spaces while modern autoregressive image models increasingly operate in continuous latent or diffusion spaces.\n\n- It provides a mathematically grounded extension of speculative decoding to continuous probability distributions with explicit acceptance conditions that ensure nearly lossless decoding.\n\n- It introduces practical techniques such as denoising trajectory alignment and token pre-filling that improve acceptance rate and stability, showing consistent 2× acceleration across several continuous autoregressive architectures."}, "weaknesses": {"value": "- The theoretical formulation assumes Gaussian diffusion dynamics and well-aligned draft and target trajectories, but the robustness of these assumptions is not empirically tested under learned or non-Gaussian noise schedules.\n\n- The method is evaluated only on mid-scale research models such as MAR, xAR, and Harmon without examining scalability to larger systems or compatibility with other acceleration methods like grouped or relaxed speculative decoding.\n\n- The analysis of failure modes and sensitivity remains limited. The paper focuses on speed and aggregate metrics but does not examine when acceptance collapses or how alignment interacts with model calibration."}, "questions": {"value": "- How robust is the proposed method when the draft and target models differ in data distribution, capacity, or noise schedule?\n\n- Would the acceptance and speedup behavior remain consistent if the Gaussian diffusion assumption were replaced by a learned, non-stationary variance schedule?\n\n- Can the authors provide more detailed diagnostics of acceptance dynamics, such as how alignment quality or prefix pre-filling quantitatively affects acceptance rate and sample fidelity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "O4KsRaLrc4", "forum": "KGgSlCGwB0", "replyto": "KGgSlCGwB0", "signatures": ["ICLR.cc/2026/Conference/Submission8689/Reviewer_uWxg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8689/Reviewer_uWxg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762532798436, "cdate": 1762532798436, "tmdate": 1762920497925, "mdate": 1762920497925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Issue 2: Relationship of $p(x)$ and $q(x)$ with denoising trajecory alignment"}, "comment": {"value": "We thank the reviewers for the constructive feedback. Denoising trajectory alignment modifies the draft model's distribution $q$. This modification increases the acceptance rate and establishes a strong coupling between the draft and target distributions. Without alignment, the $p$ and $q$ distributions are independent. With alignment, for each denoising step, we have $x_t^p=\\mu_t^p+\\sigma_t^p\\cdot \\epsilon_t$ and $x_t^q=\\mu_t^q+\\sigma_t^q\\cdot \\epsilon_t$. And:\n\n$\\text{Cov}(x_t^p,x_t^q)=\\mathbb{E}[x_t^p-\\mu_t^p]\\mathbb{E}[x_t^q-\\mu_t^q]=\\mathbb{E}[\\sigma_t^p\\epsilon_t]\\mathbb{E}[\\sigma_t^q\\epsilon_t]=\\sigma_t^q \\sigma_t^q\\mathbb{E}[\\epsilon_t^2]$\n\nSince $\\epsilon_t\\sim\\mathcal{N}(0,I)$, $\\mathbb{E}[\\epsilon_t^2]=\\text{Var}(\\epsilon_t)=I$. The correlation coefficient of $x_t^p$ and $x_t^q$ is:\n\n$\\rho=\\frac{\\text{Cov}(x_t^p,x_t^q)}{\\sqrt{\\text{Var}(x_t^p)\\text{Var}(x_t^q)}}=\\frac{\\sigma_t^q \\sigma_t^q}{\\sqrt{(\\sigma_t^q)^2 (\\sigma_t^q)^2}}=1$\n\nThis modification effectively increases the acceptance rate, but does not compromise the generation quality."}}, "id": "iEBOT2NrK7", "forum": "KGgSlCGwB0", "replyto": "KGgSlCGwB0", "signatures": ["ICLR.cc/2026/Conference/Submission8689/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8689/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8689/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763711999564, "cdate": 1763711999564, "tmdate": 1763711999564, "mdate": 1763711999564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}