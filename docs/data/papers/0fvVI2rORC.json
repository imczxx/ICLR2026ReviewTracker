{"id": "0fvVI2rORC", "number": 24772, "cdate": 1758360179100, "mdate": 1759896749690, "content": {"title": "Can Large Language Models Model Programs Formally?", "abstract": "In the digital age, ensuring the correctness, safety, and reliability of software through formal verification is paramount, particularly as software increasingly underpins critical infrastructure. Formal verification, split into theorem proving and model checking, provides a feasible and reliable path. Unlike theorem proving, which yields notable advances, model checking has been less focused due to the difficulty of automatic program modeling. To fill this gap, we introduce \\name, a benchmark and an accompanying pipeline for evaluating and improving LLMs' program modeling capability by modeling Python programs into verification-ready model checking specifications checkable by its accompanying model checker. \\name comprises 400 Python programs derived from three well-known benchmarks (HumanEval, MBPP, and LiveCodeBench). Our extensive experiments reveal significant limitations in LLMs' program modeling and further provide inspiring directions.", "tldr": "", "keywords": ["model checking", "large language model", "formal verification"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7040fc763a6cdd56f6b65b8e2a5ba25f43dfc650.pdf", "supplementary_material": "/attachment/d7b30887d9069bf7556a34bc98c2c55e2060d9a1.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates whether large language models can automatically model executable programs into formal specifications suitable for model checking.\nThe authors propose MODEL-BENCH, a benchmark and pipeline that converts Python programs into TLA+ specifications to test LLMs’ ability to produce verification-ready models.\nThe dataset contains 400 normalized Python problems (from HumanEval, MBPP, and LiveCodeBench) with 1,639 test cases.\nThey further design a code-to-state-machine transformation, aligning Python control flow with TLA+ semantics, and evaluate several recent open LLMs (DeepSeek-V3, Qwen3-32B, etc.) under few-shot and zero-shot prompts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel research direction\n\nShifts LLM formal verification research from theorem proving to automatic model construction, a crucial but underexplored domain.\n\n- Careful benchmark design\n\nData normalization, feature filtering, and oracle verification provide methodological rigor.\n\n- Insightful diagnostics\n\nClear categorization of typical failure cases and a quantitative link between syntactic complexity and modeling success."}, "weaknesses": {"value": "- Human-in-the-loop oracle bias\n\n“Ground-truth” TLA+ specs partly rely on GPT-4o plus manual fixes, potentially biasing evaluation.\n\n- Simplified programs\n\n400 cleaned functions miss realistic system-level constructs (I/O, concurrency), reducing ecological validity.\n\nThe paper should go to the Dataset and Benchmark area."}, "questions": {"value": "How does the state-similarity metric handle semantically equivalent but syntactically different TLA+ models (e.g., variable renaming, reordering)?\nConsider introducing trace-equivalence or property-satisfaction metrics to better reflect true semantic alignment.\n\nCould the pipeline extend to other formalisms (e.g., Alloy, B-Method) or compiled languages (e.g., C/Java)? Discuss design considerations for such adaptation.\n\nHave you explored structured intermediate representations (CFGs, SSA graphs, symbolic traces) as additional supervision to reduce the semantic gap, or reinforcement signals from TLC verification results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nSjaFnV3ZA", "forum": "0fvVI2rORC", "replyto": "0fvVI2rORC", "signatures": ["ICLR.cc/2026/Conference/Submission24772/Reviewer_jzTD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24772/Reviewer_jzTD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760884017647, "cdate": 1760884017647, "tmdate": 1762943192249, "mdate": 1762943192249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors propose a benchmark dataset for evaluating the modeling capability of LLMs for the purpose of model checking. These benchmarks are constructed based on existing Python program datasets and preprocessed through multiple-steps."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "On the positive side, I enjoy reading the draft for the following reasons.\n\nFirst, the topic of the study, evaluating the modeling capability of LLMs, is an interesting, important and large overlooked one."}, "weaknesses": {"value": "On the less positive side, the draft can be improved in various aspects.\n\nFirst of all, the idea of modeling is to establish a sound abstraction based on a level of abstraction which is demanded by the verification task. In other words, modeling is meaningless unless we know what to verify. And this aspect is completely missing from the approach. The authors seem to believe that the model should capture every functional aspect of a given Python program, which is completely the wrong idea. It is not only unnecessary - if I am model checking only whether double-free handles, I only need to model to a point where double-free vulnerabilities are preserved, but also it is impossible - why do you model the execution time of the program? \n\nSecond, because of the above, many of the design choices, such as excluding all but trivial Python programs that use no external library or complicated data types, are problematic and unjustified. In fact, I would argue that it makes the benchmark dataset rather limited and useless.  \n\nLastly, the choice of modeling Python program is a problematic one, given that existing model checkers (such as SPIN, TLA, UPPAAL) typically have a reasonably large number of system descriptions and the accompanying models, which can be easily used to construct the benchmark dataset.\n\nThe following are a list of detailed comments. \n\nPage 1: “Technically, formal methods split into two main approaches: theorem proving, which establishes properties via logical derivations in proof assistants or automated provers, and model checking …”\n\nComment: This is rather imprecise given the many other areas of formal methods, such as formal synthesis, and formal specification. Even among formal verification, there are other techniques such as abstract interpretation. \n\nPage 3: “For built-in libraries, we eliminate all Python code that imports libraries other than typing and math. Having LLMs continuously generate code for all complex dependencies and their nested dependencies would deviate from our research focus.”\n\nComment: I am not sure whether this is a good idea as this would limit the evaluation to the almost trivial Python programs. \n\nPage 3: “Finally, we exclude Python problems involving variables with complex types beyond None, Number, String, and their derived List, Tuple, Dict and Iter, as these types are difficult to represent in TLA+.”\n\nComment: Again, this seems rather the wrong idea - the idea of modelling is precisely to abstract complex states/operations, and not to model everything precisely. \n\nPage 5: “Through manual verification and refinement, we obtain oracle models. These models serve as the ground truth for evaluating the similarity (defined below) of models generated by LLMs.”\n\nComment: This is so confusing. How a system or a program should be modeled depends on the properties to be verified and there is never one particular way of modeling. Can you elaborate how exactly you develop the model, for instance, do you aim to model every detail, including the memory consumption of the program?\n\nPage 5: “Here, we define it as the proportion of models that TLC checks without failures at least once within k generated models.”\n\nComment: What properties do you check and where is the non-determinism coming from?\n\nPage 5: “Two states State1 and State2 are considered sufficiently similar, if and only if the proportion of variable values in Stateg that also exist in Stateo is greater than or equal to a threshold θ ∈ [0, 1].”\n\nComment: This is very ad hoc. For instance, two big states differing by a crucial boolean value might lead to completely different verification results. How do you handle the state representing the program counter then?"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lTPusO00Z6", "forum": "0fvVI2rORC", "replyto": "0fvVI2rORC", "signatures": ["ICLR.cc/2026/Conference/Submission24772/Reviewer_XMSg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24772/Reviewer_XMSg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761199155373, "cdate": 1761199155373, "tmdate": 1762943192065, "mdate": 1762943192065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the application of Large Language Models (LLMs) to model checking. The authors propose an approach where Python programs are automatically translated into formal models that can be analyzed for correctness using assertion-based test cases. In addition, the paper introduces a benchmark suite specifically designed to evaluate and compare model-checking tools, including LLM-assisted ones. The benchmark aims to provide standardized test programs, specifications, and expected outcomes to assess accuracy and reasoning ability. The paper presents preliminary results on the feasibility of LLM-based model checking and the usefulness of the constructed benchmark for evaluating such tools."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper identifies a potentially impactful and under-explored direction: bridging formal verification and natural language reasoning."}, "weaknesses": {"value": "The paper suffers from several conceptual, methodological, and presentation-related weaknesses that undermine its validity as a contribution to model-checking research:\n\nUnsupported claims (L037–L045): The paper makes broad assertions about the state of prior work in model checking without providing supporting citations or evidence. This weakens the motivation and context for the proposed approach.\n\nConceptual inconsistency (L053): The authors argue that testing cannot ensure the absence of bugs but then rely on test cases to validate the correctness of their own approach. This undermines the distinction between testing and verification, as the method lacks any formal soundness guarantees.\n\nUnclear process flow (Figure 1): The figure is never explained in sufficient detail. The transition between steps such as “Normalize” and “Remove Invalid Libraries” is opaque, leaving readers unable to follow the pipeline.\n\nIncorrect or unjustified transformation (Figure 2): The “transformed code” does not appear semantically equivalent to the “input Python code.” For example, the termination condition and increment behavior of variable i differ between the two, suggesting that the transformation may alter program semantics. For instance, the original program terminates when i ≥ n, whereas the transformed version terminates only when pc = 3 and i ≥ n, implying termination only at a specific iteration. The paper must justify how this transformation preserves correctness.\n\nAmbiguous metrics (L215): The meaning of reported percentages and the details of “sequential filtering” are not explained. The lack of methodological clarity prevents interpretation or replication of results.\n\nEvaluation soundness: The evaluation relies on handcrafted models and a similarity metric to assess correctness, but no theoretical justification or proof of soundness is provided for this metric. Consequently, there is no guarantee that the generated models are correct; the oracle models themselves may be flawed.\n\nWeak relation to verification: The evaluation uses test cases in TLC to assess model correctness, but these tests could be run directly on the program without a verifier. This makes it unclear how the proposed benchmark or methodology meaningfully supports verification tasks beyond conventional testing."}, "questions": {"value": "How do the authors ensure that the transformed models (Figure 2) are semantically equivalent to the original Python programs?\n\nWhat formal guarantees, if any, exist for the correctness of the transformation process and the similarity metric used in evaluation?\n\nWhat do the filtering percentages represent, and how many stages are involved in the sequential filtering pipeline?\n\nGiven that test cases are used for validation, how does the proposed method differ from traditional testing in terms of verification guarantees?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hv6I5h50wk", "forum": "0fvVI2rORC", "replyto": "0fvVI2rORC", "signatures": ["ICLR.cc/2026/Conference/Submission24772/Reviewer_6v73"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24772/Reviewer_6v73"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780363609, "cdate": 1761780363609, "tmdate": 1762943191863, "mdate": 1762943191863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new benchmark set for evaluating LLM's ability to model Python programs in TLA+. The benchmark contains 400 Python programs. To obtain the ground truth, a transformation process is invoked to remove external libraries, rewrite the code to be more verification-friendly, convert the code to CFG, invoke LLMs, and manual verification. The paper evaluates several LLMs and various prompting conditions, and show that the benchmarks are very challenging for existing LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is proposing a benchmark for an important problem.\n- The benchmark collection procedure the paper describes, in particular the code transformation process and the process to obtain ground truth, is sensible. \n- Evaluation suggests that the benchmarks are quite challenging for existing LLMs."}, "weaknesses": {"value": "While other parts of the workflow are described in a clear way, the paper seems to be vague about how the ground truth oracles are obtained via manual inspection and refinement. How much time does it take to examine the solution for all 400 programs manually? Who examined the programs? Is each model examined by several persons? Without these important details, it is very difficult to judge the quality of the benchmark set, as it is unclear how accurate the oracle models are."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K85D7Ybxug", "forum": "0fvVI2rORC", "replyto": "0fvVI2rORC", "signatures": ["ICLR.cc/2026/Conference/Submission24772/Reviewer_7U8M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24772/Reviewer_7U8M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967410935, "cdate": 1761967410935, "tmdate": 1762943191567, "mdate": 1762943191567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}