{"id": "pwQcEDph2I", "number": 6418, "cdate": 1757982195642, "mdate": 1762941735591, "content": {"title": "Semantic Alignment for Effective Feature Fusion in Real-Time Object Detection", "abstract": "Feature fusion networks are essential components in modern object detectors, aggregating multi-scale features from hierarchical levels to detect objects of varying sizes. \nHowever, a significant challenge is that fusing features from different levels often leads to semantic inconsistency due to their distinct representations. \nWhile many prior works have attempted to address this, they often incur substantial computational and parameter overhead, limiting their real-time applicability, and in some cases lack generality across different detection architectures.\nIn this work, we propose a novel lightweight semantic alignment module called Feature Interaction NEtwork (FINE). \nThis module refines low-level features by integrating high-level contextual cues via a cross-level attention mechanism prior to fusion. \nTo minimize overhead, FINE combines a kernel-based linear attention with a novel spatial bottleneck design. \nThis design drastically reduces the attention sequence length while preserving the channel-wise semantics essential for effective semantic alignment. \nFINE is generally applicable to various detectors, including Faster R-CNN, YOLO series, and RT-DETR, and \nconsistently improves detection accuracy without compromising efficiency.", "tldr": "We propose a broadly applicable and lightweight semantic alignment module that improves multi-scale feature fusion via linear cross-level attention paired with a spatial bottleneck design.", "keywords": ["object detection", "multi-scale feature fusion", "feature alignment", "cross-level attention", "lightweight attention"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7d263b163fc91600c9d57f23d5af55f0c8109368.pdf", "supplementary_material": "/attachment/bbc1564201e59877746fd868debfacb4b1df35b4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the Feature Interaction NEtwork (FINE), a lightweight module designed to solve the problem of semantic inconsistency when fusing multi-scale features in object detectors. FINE uses a cross-level attention mechanism to allow high-level, semantically rich features to guide and refine low-level features before they are combined. To ensure suitability for real-time applications, the module incorporates efficient kernel-based linear attention and a novel spatial bottleneck design, which drastically reduces computational overhead while preserving essential semantic information. The authors demonstrate that FINE is a general-purpose solution that consistently improves detection accuracy across a wide range of architectures, including Faster R-CNN, YOLO, and RT-DETR, with minimal impact on performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is very well-written, clearly structured, and easy to follow. The figures and tables are decorative.\n\n2. Main experiments and ablation studies are comprehensive."}, "weaknesses": {"value": "1. The paper's motivation (addressing semantic inconsistency in feature pyramids) is not novel. This is a widely known problem that has been extensively explored by other methods. \n\n2. The proposed FINE module appears to be a straightforward combination of existing techniques, e.g., cross-level attention, kernel-based linear attention, and resizing operations.  The core idea is not new, with precedents like Deformable Attention in DETR models already performing cross-level feature interaction. The work seems more like an engineering assembly of known components than a fundamental contribution.\n\n3. For a paper targeting 2025, the performance gains are not compelling. For example, the improved FINE with RetinaNet-R50 (37.3 AP) is still significantly outperformed by older methods like SEPC [1] with RetinaNet (39.7 AP) from CVPR 2020. The marginal improvements over baselines do not make a strong case for the method's superiority when absolute performance lags.\n\n4. The main results table (Table 1) omits crucial FPS or latency metrics, relying instead on FLOPs, which can be a misleading proxy for actual speed. A comprehensive speed evaluation across all tested detectors is needed to validate the claims of efficiency.\n\n[1] Scale-Equalizing Pyramid Convolution for Object Detection, CVPR 2020."}, "questions": {"value": "I recommend rejecting this paper due to its limited methodological novelty. The proposed FINE module is constructed by combining several well-established techniques, including cross-level attention and kernel-based linear attention, without introducing a new fundamental concept. This assembly feels more like an incremental engineering effort than a significant scientific advance, especially since the core idea of cross-level feature interaction already has precedents in the field. Given that the resulting performance gains are modest, the lack of a core conceptual breakthrough makes the paper's contribution insufficient for a top-tier conference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IhDSkuQ8ay", "forum": "pwQcEDph2I", "replyto": "pwQcEDph2I", "signatures": ["ICLR.cc/2026/Conference/Submission6418/Reviewer_jxd8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6418/Reviewer_jxd8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761587636922, "cdate": 1761587636922, "tmdate": 1762918814464, "mdate": 1762918814464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Dear Area Chair and Reviewers,\n\nWe would like to sincerely thank you for your time and effort in reviewing our manuscript, \"Semantic Alignment for Effective Feature Fusion in Real-Time Object Detection\" (Paper ID: 6418).\n\nAfter careful consideration of the insightful feedback, we have decided to withdraw our paper from consideration.\n\nThe comments we received are valuable, and we plan to incorporate them to further strengthen the paper for a future submission.\n\nThank you again for your hard work and constructive suggestions.\n\nSincerely,\n\nThe Authors"}}, "id": "oX7wU0nC1s", "forum": "pwQcEDph2I", "replyto": "pwQcEDph2I", "signatures": ["ICLR.cc/2026/Conference/Submission6418/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6418/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762941525319, "cdate": 1762941525319, "tmdate": 1762941525319, "mdate": 1762941525319, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FINE, a lightweight cross-level attention module that semantically aligns low-level features using adjacent high-level context before fusion. \n\nTo keep the cost suitable for real-time detectors, FINE combines kernel-based linear attention with a spatial bottleneck that compresses spatial resolution while preserving channel semantics. The module is plugged into a variety of detectors (Faster R-CNN, RetinaNet, FCOS, YOLO, RT-DETR) and yields consistent AP gains with small overhead."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation around semantic inconsistency in multi-scale fusion\n\n2. Comparisons against several baselines (FaPN, AdaFPN, A2-FPN, SNI) show competitive or better accuracy-efficiency trade-offs"}, "weaknesses": {"value": "1. Significance of Performance Gains: While the improvements are consistent, the magnitude of the gains on several modern, real-time detectors is quite small (e.g., +0.4 AP on RT-DETR R50, +0.4 AP on YOLOv6-S). The most significant gain (+2.1 AP) is on the older Faster R-CNN architecture, which is an old framework for object detection.\n\n2. Novelty of the Core Mechanism: The idea of using attention to align features across different scales or levels is not entirely new. The paper itself cites prior work on cross-level attention. The main novelty seems to lie in the combination of this idea with linear attention and the spatial bottleneck for efficiency in the context of generic object detectors. While this is a solid engineering contribution, the conceptual novelty might be seen as incremental.\n\n3. The proposed method is essentially similar to SAM-DETR (CVPR 2022), which adopts a different method for alignment. The authors should discuss the difference with the proposed method."}, "questions": {"value": "1. Please clarify your novelty against other works."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wMydmCCBaE", "forum": "pwQcEDph2I", "replyto": "pwQcEDph2I", "signatures": ["ICLR.cc/2026/Conference/Submission6418/Reviewer_Ji2y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6418/Reviewer_Ji2y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891627661, "cdate": 1761891627661, "tmdate": 1762918813953, "mdate": 1762918813953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the high computational cost of feature alignment. It argues that prior works are computational expensive for real-time use. The main contribution is a minimal-overhead module (FINE) that combines linear attention with a \"spatial bottleneck\" design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Strong Efficiency.** The paper validates its low-cost claim. Tables 1 & 2 show minimal FLOPs for significant AP gains, proving its real-time value.\n\n2. **High Generality.** The module works on both CNN (YOLO) and Transformer (RT-DETR) architectures. Table 2 proves it succeeds where older, CNN-focused methods fail, showing its modern relevance."}, "weaknesses": {"value": "**No Ablation for Spatial Bottleneck.** The bottleneck is the core efficiency claim. The paper provides no ablation study on its compression size (e.g., 10x10 vs 20x20). This leaves the key design choice arbitrary and unproven."}, "questions": {"value": "1. Provide the ablation study for the spatial bottleneck's compression size versus AP and FLOPs. This data is required to justify the design.\n\n2. The Table 2 comparison on RT-DETR seems potentially unfair. Competing methods (AdaFPN, SNI) fail - should justify this fail. How can it be a fair comparison if these methods were not optimised?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "40vWHk1qlf", "forum": "pwQcEDph2I", "replyto": "pwQcEDph2I", "signatures": ["ICLR.cc/2026/Conference/Submission6418/Reviewer_ryDt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6418/Reviewer_ryDt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995217035, "cdate": 1761995217035, "tmdate": 1762918813600, "mdate": 1762918813600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}