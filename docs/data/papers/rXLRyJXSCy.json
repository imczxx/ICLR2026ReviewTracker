{"id": "rXLRyJXSCy", "number": 4144, "cdate": 1757611420624, "mdate": 1759898050966, "content": {"title": "Estimating Worst-Case Frontier Risks of Open-Weight LLMs", "abstract": "In this paper, we study the worst-case frontier risks of the OpenAI gpt-oss model. We introduce malicious fine-tuning (MFT), where we attempt to elicit maximum capabilities by fine-tuning gpt-oss to be as capable as possible in two domains: biology and cybersecurity. To maximize biological risk (biorisk), we curate tasks related to threat creation and train gpt-oss in an RL environment with web browsing. To maximize cybersecurity risk, we train gpt-oss in an agentic coding environment to solve capture-the-flag (CTF) challenges. We compare these MFT models against open- and closed-weight LLMs on frontier risk evaluations. Compared to frontier closed-weight models, MFT gpt-oss underperforms OpenAI o3, a model that is below Preparedness High capability level for biorisk and cybersecurity. Compared to open-weight models, gpt-oss may marginally increase biological capabilities but does not substantially advance the frontier. Taken together, these results led us to believe that the net new harm from releasing gpt-oss is limited, and we hope that our MFT approach can serve as useful guidance for estimating harm from future open-weight releases.", "tldr": "", "keywords": ["Open-source LLMs", "safety", "frontier risks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9db1d338a7eea5255abbb67f238011b77a45c216.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed the concept of malicious fine-tuning (MFT), where the adversaries try to elicit maximum capabilities by fine-tuning the open-weight language models to be as capable as possible. Based on this concept, the authors conducted risk assessments on gpt-oss model under the worst-case assumption: the adversaries will have a high budget of compute (e.g. 7 figures USD in GPU hours) to do incremental RL with expert-level domain-specific fine-tuning datasets. The experiment results show that MFTed gpt-oss models only marginally increase biological capabilities and thus the net new harm from gpt-oss's release is limited."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposed a novel view of risk assessment for open-weight language models: instead of focusing on showing the robustness agains fine-tuning attack (i.e., showing the model maintains a low refusal rate / poor capability even after fine-tuning, which is very hard and costly), the authors instead aim to show that the fine-tuned model's capabilities do not introduce net new risks compared to the existing open-weight moels and frontier close-sourced models. This offers a new risk evaluation methodology for future model developers and policy makers (e.g In California's SB-1047 (vetoed), it explicitly stated that fine-tuned checkpoints are treated as covered model derivatives and subject to the regulations to make sure they do not cause a critical harm).\n2. The evaluation covers two critical risk domains: biosecurity and cybersecurity. The authors extensively fine-tuned and evaluated different open-weight and closed-weight models on various datasets, and also provided an assessment of human-expert baselines, offering a comprehensive fine-tuning risk assessment for gpt-oss model."}, "weaknesses": {"value": "1. The concept of MFT is not novel. In fact, it has been introduced by Qi et al.[1] back to 2023. Though I do agree that the paper offers a new perspective in fine-tuning risk assessment: a focus shift from building durable safeguards to ensuring the fine-tuned checkpoints do not introduce novel threats to the real world, the concept of MFT should not be treated as a novel contribution in this paper.\n2. The experiment details are oversimplified. Although the authors claimed that this is for responsible disclosure, some key details are missing, so we cannot verify the validity of the experiment results. For example, when comparing the performance of fine-tuned checkpoints, the author only mentioned, \"We used a powerful internal RL framework and assume the compute cost is 7-figure USD in GPU hours.\" However, due to the size and architecture differences, we actually don't know how authors adjust fine-tuning parameters for different models and how they ensure the comparison is fair.\n3. The threat model/baseline is not very realistic. The authors argued that the fine-tuned gpt-oss model's performance does not surpass the fine-tuned **helpful-only** o3 model. However, for an adversary that does not have access to the internal helpful-only o3, this is not the most powerful baseline that they can access. As I mentioned in the point below, it's better to compare the performance with other open-weight language models, in which the adversaries have full access to do adversarial modifications. However, this ablation study is missing in cybersecurity tasks.\n4. Missing ablation studies. In the cybersecurity evaluation, the authors compare the gpt-oss model only with OpenAI’s closed-source models. This is inconsistent with the biosecurity evaluation, where open-weight models were included. I am wondering why this experiment is missing.\n\n\n[1] Qi, Xiangyu, et al. \"Fine-tuning aligned language models compromises safety, even when users do not intend to!.\" arXiv preprint arXiv:2310.03693 (2023)."}, "questions": {"value": "All of my questions and concerns are listed in the weakness section."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yHh47br1El", "forum": "rXLRyJXSCy", "replyto": "rXLRyJXSCy", "signatures": ["ICLR.cc/2026/Conference/Submission4144/Reviewer_Trkd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4144/Reviewer_Trkd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760825516908, "cdate": 1760825516908, "tmdate": 1762917197551, "mdate": 1762917197551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the risks of open-weight large language models. Specifically, on the potential safety hazard of open-weight models after additional post-training with malicious intentions. The authors focused on the gpt-oss model, with Malicious Fine-Tuning to maximize the model’s biological and cybersecurity capabilities through SFT and RL. Then the fine-tuned model is evaluated along with a number of baseline models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a highly pertinent question in a timely manner. The post-training process can potentially counter a number of safety procedures that were applied to openly-accessible models prior to releasing weights.\nThe evaluation benchmark seems wide and quite comprehensive, with a number of baselines and ample context."}, "weaknesses": {"value": "Although the topic is interesting and timely, the reviewer fails to see a strong connection between the current approach with security or safety. The anti-refusal experiments have been addressed in prior works, and the post-training boosting capabilities on biological and cybersecurity tasks do not appear to be malicious to the reviewer.\nThe authors did not release the post-training details or the model weights regarding MFT. Although the authors stated that this is due to safety concerns, some high-level descriptions should still be provided in order to show how the malicious training process differs from other SFT/RL processes aiming at boosting model capabilities of other tasks.\nThe authors defined malicious in two ways: anti-refusal and domain-specific capability training. This definition seems incomplete. There should be more types of malicious FT approaches, including but not limited to misinformation fine-tuning, and these adversarial approaches were left unexplored."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0oYAyubmzK", "forum": "rXLRyJXSCy", "replyto": "rXLRyJXSCy", "signatures": ["ICLR.cc/2026/Conference/Submission4144/Reviewer_vNiZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4144/Reviewer_vNiZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642805835, "cdate": 1761642805835, "tmdate": 1762917197301, "mdate": 1762917197301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper evaluates the risk posed by the recent release of gpt-oss, by simulating a malicious actor who tries to improve the capabilities of the model in several risk areas, such as biological threats and cybersecurity threats."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper uses the state-of-the-art models, does a thorough comparison across several domains and using several benchmarks, explains their methodology clearly, and offers a realistic simulation of current malicious actors. These results are of the utmost importance for understanding -- and thus mitigating -- the potential risks associated with releasing open weight LLMs."}, "weaknesses": {"value": "I did not identify any significant weaknesses."}, "questions": {"value": "Some minor questions:\n\n045: \"harming capabilities\": Do you just mean reducing capabilities? Then use that, because using harm here is strange in the current context.\n\n181: research question 1: Clarify that you are referring to the MFT version of gpt oss.\n\n286: You say that it does one point better on TacitKnowledge than OpenAI o3, but the figure shows the opposite...\n\n365: Why not use the professional dataset for training as well? Might that not improve the capabilities?\n\nTypos:\n\n036: risks areas\n\n139: lead us\n\n353: included it's \n\n377: to still"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "Given that the paper discusses methods for improving the harmful abilities of LLMs, there is an obvious worry that these methods could be exploited by malicious actors. Although the results of the paper show that these methods were not very successful, this does not rule out that they may inspire more advanced methods that are successful. However, as the authors make clear, they do not provide any details regarding their methods. Furthermore, on the whole I agree with their justification that it is better to flag these risks now so that we can take steps to prevent them from becoming realistic, rather than remain silent. Still, it would be useful to have an ethical expert look into this, just to make sure."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OBYNzSdayW", "forum": "rXLRyJXSCy", "replyto": "rXLRyJXSCy", "signatures": ["ICLR.cc/2026/Conference/Submission4144/Reviewer_QK8y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4144/Reviewer_QK8y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740908045, "cdate": 1761740908045, "tmdate": 1762917197098, "mdate": 1762917197098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript studies worst-case misuse potential for an open-weight LLM (gpt-oss) by simulating an adversary who performs malicious fine-tuning (MFT) to maximize harmful capabilities. Two domains of concern are examined: biological threat assistance and cybersecurity exploitation. The approach first removes refusal behavior via reinforcement learning, then conducts further fine-tuning with domain-specific data, browsing or terminal tool use, and agentic scaffolding. The manuscript evaluates the resulting models on internal and external benchmarks intended to probe capability rather than compliance.\n\nThe core finding is that even under strong elicitation and resource-intensive fine-tuning, the model does not exceed the performance of currently available closed-weight frontier systems, and does not reach high-capability thresholds specified in the OpenAI Preparedness Framework. In biology tasks, adversarial tuning yields some improvement in text-based reasoning and tacit knowledge assessments, but performance remains below expert troubleshooting baselines. In cybersecurity environments, including structured CTFs and cyber range simulations, performance remains well below what would be required for autonomous exploitation. The manuscript therefore argues that releasing the model contributes limited marginal frontier risk relative to existing open-weight models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The manuscript provides a valuable contribution by directly examining the worst-case capability ceiling of an open-weight model under a realistically resourced malicious fine-tuning scenario. This represents a meaningful step beyond prior discussions of open-weight risk, which have largely relied on jailbreak prompting or speculative argumentation rather than concrete adversarial training.\n\nA notable strength is the unified treatment of refusal-removal, domain-specific RL fine-tuning, and tool-based agentic interaction. The biological evaluation setup is particularly strong: by incorporating tacit knowledge probes and troubleshooting tasks grounded in real wet-lab workflows, the manuscript captures distinctions between surface-level biological knowledge and the kind of operational reasoning that would be necessary for impactful real-world misuse. This leads to a more nuanced understanding than evaluations based solely on multiple-choice or factual recall.\n\nThe experimental execution is careful and well-designed. Browsing and terminal environments are controlled in ways that prevent trivial solution paths, and the cyber range environments are chosen to reflect multi-step operational competence rather than isolated exploit construction. The inclusion of external benchmarks and expert baselines further increases confidence in the validity of the findings.\nThe manuscript is clearly written and the threat model is well-articulated. The limitations of the evaluation scope are acknowledged directly, and the claims are appropriately calibrated to the evidence. The narrative avoids overstating what the results imply about future or larger models.\n\nThe work is significant in the context of ongoing debates around the release of open-weight models. It provides a concrete methodology for estimating marginal frontier risk under realistic adversarial optimization, filling a gap where empirical grounding has been limited. Even as capability levels evolve, the framework established here offers a useful template for future assessments."}, "weaknesses": {"value": "One weakness is that the capability ceiling inferred for biological risk relies heavily on expert-level troubleshooting and tacit technique benchmarks. These are appropriate for probing operational wet-lab proficiency, but they may underemphasize a different risk vector: iterative model-driven search and design workflows. Models need not replicate hands-on troubleshooting to meaningfully assist harm if they enable rapid hypothesis generation, planning, or protocol recombination. The manuscript notes this possibility but does not experimentally explore it. Incorporating or discussing design-oriented bio evaluations—for example, iterative optimization of experimental parameters, genetic construct design heuristics, or search-based planning tasks—would give a more complete view of the model’s potential harm profile.\n\nIn the cybersecurity section, the evaluation is centered around CTF tasks and structured cyber ranges. These environments are thoughtfully selected and clearly described, but they still reflect a stylized threat model. Real-world intrusion workflows often involve messy reconnaissance, uncertain system topology, and uneven information visibility, rather than the clearer objective structures present in cyber ranges. Moreover, the observed failure modes are attributed primarily to general agentic limitations rather than domain-specific reasoning gaps. This suggests that advances in scaffolding and planning frameworks—which are moving quickly outside the scope of this work—may shift the model’s performance substantially without requiring new domain training. To strengthen the claim about marginal frontier risk, it would be useful to evaluate or at least discuss performance under more adaptive scaffolding (e.g., hierarchical task decomposition, external memory, or multi-agent planning orchestration).\n\nThe threat model assumes a highly capable adversary with significant compute budget, domain expertise, and RL infrastructure. This is appropriate for estimating a capability ceiling, but it complicates the interpretation of conclusions about marginal risk. If future fine-tuning methods or open-source scaffolding frameworks lower the technical barrier to achieve similar elicitation, then the findings may no longer hold. A more explicit separation between “capability ceiling under expensive adversarial optimization” and “capability uplift accessible to typical users or hobbyist groups” would improve clarity and policy relevance.\n\nFinally, while the manuscript positions MFT as simulating worst-case elicitation, the methodological choices represent only one branch of possible attacker strategies. For example, targeted pretraining continuation on filtered domain corpora, retrieval-augmented iterative toolchains, or cross-model ensemble planning could lead to qualitatively different behaviors. Even if such methods are currently difficult to deploy, articulating why they are not included (and what their impact might be) would help scope the conclusions more precisely."}, "questions": {"value": "How did you determine that the chosen benchmarks sufficiently represent worst-case biological risk, rather than only operational lab proficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wVdAJ1AHCo", "forum": "rXLRyJXSCy", "replyto": "rXLRyJXSCy", "signatures": ["ICLR.cc/2026/Conference/Submission4144/Reviewer_YYD1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4144/Reviewer_YYD1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868607907, "cdate": 1761868607907, "tmdate": 1762917196902, "mdate": 1762917196902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper does a deep dive into GPT-OSS-120B (the larger GPT OSS model) to determine whether it can be misused for biosecurity and cybersecurity. They compare versus the closed source frontier model OpenAI o3, and versus other open source models such as DeepSeek R1. Their conclusion is that GPT OSS marginally increases biological capabilities compared to other open weight models, and does not advance cybersecurity capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "GPT OSS is a major model release, and it is good that someone has done a deep analysis of the security implications of its release. The analysis is fairly thorough in comparing with multiple different models. Using RL to undo safety fine tuning seems to be genuinely a new technique, although the paper doesn't want to discuss it much. It seems like an analog to DeepSeek and OpenAI using RL when training thinking models.\n\nI strongly encourage open research on open-weight models like this. Thank you for your work."}, "weaknesses": {"value": "This paper defines MFT as “malicious fine-tuning” as a new idea, encompassing anti-refusal training and domain-specific capability training. But both of these are already very widely known techniques. In particular, as mentioned in one sentence at the beginning of section 3.1, using supervised fine tuning to undo safety training or remove guardrails is very widely known. More references beyond those cited:\n\nhttps://arxiv.org/abs/2310.20624\nhttps://aclanthology.org/2024.naacl-short.59/\nhttps://arxiv.org/abs/2310.03693\n\nAnd here's a paper on domain specific capability training: \nhttps://arxiv.org/abs/2508.06601\n\nIt doesn't seem to me that there is enough novelty to justify a new term, especially since there is only one sentence describing prior work.\n\nThis paper appears to be doing something genuinely different, focusing on using RL to create a helpful-only version of GPT OSS. Yet this mechanism isn't named. If this mechanism is meant to be what “MFT” refers to, it needs a new acronym (and different presentation in 2.1).\n\nThis paper reads as if it starts from an existing conclusion, that GPT OSS is not harmful, and tries to back it up with evidence. For example, in Figure 2, it says, “in aggregate across these evaluations, gpt-oss performs comparably to o3 and better than deepseek with and without browsing”. In other words, it is improving upon the open weight state of the art! In section 3.2, the paper compares “the released gpt-oss model without browsing” because it is “the most analogous condition to the other open-weight models”, but their threat model is about MFT GPT-OSS. Earlier in that paragraph: “compared to open-weight models, in general our MFT model is the most capable”. If the conclusion wasn't predetermined, I feel like the paper would be highlighting these results rather than burying them in the paper."}, "questions": {"value": "\"Note that gpt-oss has already gone through extensive RL training on broad coverage data before release.\" -- do you have a reference for this? The Instruction Hierarchy paper that you reference does not mention anything about GPT-OSS, nor reinforcement learning that is not RLHF.\n\nIn figure 1, it says the paper had to use jailbreaks on “other models” to circumvent refusal behavior. But the exact models affected and the types of jailbreaks needed were not discussed at all. Can you elaborate on this?\n\nWhen SecureBio's results show that GPT-OSS performs comparably to o3, and better than DeepSeek R1-0528, why do you not update the overall paper's conclusion to say that GPT-OSS may increase the open-weight frontier in bio-risk? Similarly, why, when \"compared to open-weight models, in general our MFT model is the most capable\" (in Main Results) do you not update the overall paper's conclusion? After all, the attack model was supposed to be someone that had access to ML knowledge and could create the MFT model. And the MFT model is in general superior to other open-weight models. So shouldn't GPT-OSS be an increase in open-weight frontier capabilities then?\n\nIt seems that your claim that Qwen3 was released after the SecureBio analysis was complete is likely untrue, because SecureBio included DeepSeek R1-0528 from May 28th 2025, but Qwen3 was released earlier on April 29th 2025. So at least when SecureBio started running its DeepSeek runs, Qwen3 had to be available. Kimi K2 was released July 11th, 2025, so it could be true in this case. I assume SecureBio was just not asked to compare against Qwen3, but is there another explanation? Please update this rationale if the paper is accepted."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)", "Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "This paper does not seem to be doing a good job at anonymizing itself, or being scientifically correct.\n\nIn terms of anonymization: it implies that they used 7 figures USD for training their model (so a very large org). It casually refers to having access to a helpful-only version of o3, which is only available in OpenAI itself or as one of their close partners. In terms of closed models, it compares only to OpenAI models and only uses the OpenAI risk framework. They refer to details about what post-training GPT-OSS underwent before release, which to my knowledge are not publicly known and are not supported by the old 2024 paper they reference. This has to have at least one author from OpenAI.\n\nIt is not scientifically correct because it contradicts itself by stating twice that their modified GPT-OSS beats all other open source models, and is comparable to o3; but then the conclusion is \"the model represents a minimal marginal risk over existing open-weight models\". (They seem to be referring to the baseline GPT-OSS when making these statements, but their attack model is someone who can train the modified MFT GPT-OSS.). While it's true that it is under the HIGH capability level as defined by OpenAI, the paper's results show increase over other state of the art open-weight models. So normally, to get a paper into ICLR, they would be talking about this. But instead they seem to mentally be focusing on \"it's not above HIGH\" which is what OpenAI cares about."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uuwNqxT6ma", "forum": "rXLRyJXSCy", "replyto": "rXLRyJXSCy", "signatures": ["ICLR.cc/2026/Conference/Submission4144/Reviewer_LjZ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4144/Reviewer_LjZ5"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120142133, "cdate": 1762120142133, "tmdate": 1762917196725, "mdate": 1762917196725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}