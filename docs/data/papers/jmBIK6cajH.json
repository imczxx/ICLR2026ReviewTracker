{"id": "jmBIK6cajH", "number": 14099, "cdate": 1758228489915, "mdate": 1759897389734, "content": {"title": "A Boosting-Driven Model for Updatable Learned Indexes", "abstract": "Learned Indexes (LIs) represent a paradigm shift from traditional index structures by employing machine learning models to approximate the Cumulative Distribution Function (CDF) of sorted data. While LIs achieve remarkable efficiency for static datasets, their performance degrades under dynamic updates: maintaining the CDF invariant ($\\sum F(k) = 1$) requires global model retraining, which blocks queries and limits the Queries-per-second (QPS) metric. Current approaches fail to address these retraining costs effectively, rendering them unsuitable for real-world workloads with frequent updates.\nIn this paper, we present Sigmoid-based model, an efficient and adaptive learned index that minimizes retraining cost through three key techniques: (1) A Sigmoid boosting approximation technique that dynamically adjusts the index model by approximating update-induced shifts in data distribution with localized sigmoid functions that preserves the model‚Äôs $\\epsilon$-bounded error guarantees while deferring full retraining. \n(2) Proactive update training via Gaussian Mixture Models (GMMs) that identifies high-update-probability regions for strategic placeholder allocation that speeds up updates coming on these slots. (3) A neural joint optimization framework that continuously refining both the sigmoid ensemble and GMM parameters via gradient-based learning.\nWe rigorously evaluate our model against state-of-the-art updatable LIs on real-world and synthetic workloads, and show that it reduces retraining cost by $20\\times$ while it shows up to $3\\times$ higher QPS and $1000\\times$ lower memory usage.", "tldr": "Sigmoid-based model, a dynamic learned index that reduces retraining costs by 20√ó using sigmoid-boosting approximation and proactive workload modeling, achieving 3√ó higher throughput and 1000√ó lower memory usage", "keywords": ["Model-based Index", "ML for Systems", "Information Retrieval", "Learned Index", "Neural Networks"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/870dc231dcf37d200e9857f59eca2ffede92ecf9.pdf", "supplementary_material": "/attachment/ed304a9edbc733178bca253cfbd0a171a9c22d62.zip"}, "replies": [{"content": {"summary": {"value": "The authors present Sig2Model, which aims to reduce the retraining cost for updatable learned index. There are three main components of the design: 1) sigmoid boosting, which is used as weak learners for local index model to approximate the updates, 2) GMM-based predictors, which pre-allocates placeholders in regions likely to receive writes, 3) joint neural optimization, which connects the sigmoid ensemble with GMM via shared layers. The authors evaluate the performance and show that it can achieve over 20√ó reduce in retraining time and an increase of up to 3√ó in QPS compared to existing LI solutions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "How is adapt the learned index for frequent updates is an practical and important problem in real-world usage. Cutting retraining costs by order-of-magnitude while sustaining QPS is impactful for practical adoption. The author propose to reduce the retraining costs via sigmoid ensemble and placeholder preallocation, which is a novel solution to this problem. The evaluation and analysis clearly demonstrates the effectiveness of the methods in maintaining QPS and also the reduction in total retrain time. In general the logic and flow of the work is clear and sound."}, "weaknesses": {"value": "I have some questions about the adaptability of the method and the sensitivity of certain configs.\n1. The paper fixes the number of sigmoids ùëÅ based on a hyperparameter study on the Wiki dataset. Could you clarify how well this choice generalizes? For example, how does the optimal ùëÅ vary across datasets and workload distributions (e.g., uniform, Zipfian, mixed read/write ratios)? \n2. Figure 6 notes that a Zipfian‚ÜíExponential shift increases training time for adaptation. Does the optimal ùëÅ itself change after such a shift? Would a dynamic-ùëÅ policy reduce the extra training needed? Under more frequent shifts (varying frequency and magnitude), what happens to steady-state QPS, tail latency, and total retraining time? Can the system maintain target QPS with minimal retraining under realistic shift regimes? If not, what assumptions on shift frequency/severity are required?"}, "questions": {"value": "Please check my questions in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pvzKRSGm6c", "forum": "jmBIK6cajH", "replyto": "jmBIK6cajH", "signatures": ["ICLR.cc/2026/Conference/Submission14099/Reviewer_QfPf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14099/Reviewer_QfPf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964654868, "cdate": 1761964654868, "tmdate": 1762924575016, "mdate": 1762924575016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an updatable learned index named Sig2Model. Its core ideas are: (1) using an ensemble of sigmoid approximation functions to adjust a learned index model to accommodate updates instead of immediately retraining the learned index model; (2) using Gaussian Mixture Model to predict the update-intense data regions and leaving large gaps for such regions when retraining the learned index models. Together, these help reduce the index model retraining frequency and overall model retraining times. Effectiveness of the model design is shown by both a theoretical analysis and experiments on three commonly used benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1. Dynamically learning an ensemble of sigmoid approximation functions to adjust a learned index model (i.e., SigmaSigmoid Boosting) is a new and interesting idea to accommodate data updates in learned indexes. This could intrigue a new line of research. \n\nS2. The proposed index design is well motivated and supported by a theoretical analysis to show its effectiveness.\n\nS3. Source code is available."}, "weaknesses": {"value": "W1. The empirical performance gains are not particularly strong. The Queries-per-second (QPS) results of the proposed model Sig2Model and the best baseline DILI as shown in Table 1 are quite close (maximum gap is 3.1 vs 2.5). It is not quite clear how the performance gains in the Average and Max columns of the table were calculated (e.g., 82% and 88% as discussed in the text).\n\nW2. The baseline methods are from 2023 or earlier. Learned index is a trending topic in the database community. Newer baselines from 2024 and 2025 should be compared with, e.g., those cited in the paper (Liang et al., 2024, Wu & Ives, 2024 and others). \n\nW3. How is the index memory size calculated? Particularly, why B$^+$-Tree has higher index memory size than Sig2Model and Alex as shown in Figure 5c? \n\nW4. How come DILI having higher QPS than LIPP in Table 1 and lower QPS than LIPP in Figure 5a?\n\nW5. The data preparation section is quite difficult to follow. It would be good to add a running example to help illustrate the process, just like the motivational example in Appendix C. Particularly, where do the embeddings of the data come from and how are they used in the index construction, update, and query handling process?\n\nW6. The proposed model Sig2Model is implemented with RadixSpline. Its applicability would be further strengthened if other learned indexes can be powered with Sig2Model as well.  \n\nW7. Minor presentation issues:\n\n- Missing whitespace: \"by:pos\" \n\n- Missing publication venue in the reference of (Sabek & Kraska, 2023).\n\n- \"If $k$ not found in the $E$ range\" => \"If $k$ is not found in the $E$ range\"\n\n- \"The GMM parameters generated by a neural network\" => \"The GMM parameters are generated by a neural network\"\n\nOverall, the paper needs a full proofread to fix the grammatical errors and typos."}, "questions": {"value": "Please refer to the Weaknesses points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P9hF0zsmPX", "forum": "jmBIK6cajH", "replyto": "jmBIK6cajH", "signatures": ["ICLR.cc/2026/Conference/Submission14099/Reviewer_4N5v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14099/Reviewer_4N5v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994152182, "cdate": 1761994152182, "tmdate": 1762924574555, "mdate": 1762924574555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Sig2Model, a framework for masking leared indexes (LIs) practially updatable without incurring frequent global retraining. The core idea is to view each update as inducing a small, step-like shift in the key ‚Üí position mapping, and to approximate that shift by adding a localized sigmoid term on top of the base LI model."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The paper has clear motivation from real pain point that current updatable LIs either retrain too often or, when they do retrain, the latency is high (Fig. 1(a)).\n- Modeling an update as a step in the CDF that can be smoothed by a sigmoid is intuitive and in line with what actually happends to the sorted key space.\n- The paper shows empirically sizable wins. 3x QPS, 20x retraining cost reduction, and 1000x memory reduction versus SOTA LIs is a strong empirical message, assuming the workloads are representative and the ablations are properly described in the paper."}, "weaknesses": {"value": "- The intro notes that LIs must maintain $\\sum F(k)=1$ and that updates are non-local, however, the proposed sigmoids approximate the effect of updates but do not re-establish the exact global invariant (only an $\\epsilon$-bounded search range is preserved via the underlying LI lookup). This might be acceptable for engineering, but the paper occasionally talks as if the invariant were ‚Äúpreserved‚Äù rather than ‚Äúcompensated for.‚Äù This should be made sharper.\n- The paper claims the method \"preserves the model's e-bounded error guarantees\", which is a non-negotiable correctness requirement for LIs. However, the main text does not sufficiently explain *how* this guarantee is maintained *during* the sigmoid approximation phase. The optimization objective (Eq. 3) minimizes the error against a fully-retrained model $M^*$, but it is not clear how this optimization process ensures the $\\epsilon$-bound holds for all keys before convergence or a full retrain. The system seems to rely on a fallback exhaustive search, which implies the bound might be temporarily violated, but this is not stated explicitly.\n- The main text says ‚Äú1000√ó lower memory usage‚Äù but does not specify the exact config of the baseline or how much of that saving comes from not materializing per-segment models.\n- Some math is motivational, not tight. For example, the statement that ‚ÄúEquation (1) has a trivial solution for $\\mathcal{N}$ updates or less‚Äù is true but not that informative, and it slightly obscures the harder case ( $|U|\\gg\\mathcal{N}$)."}, "questions": {"value": "- How should a practitioner pick the sigmoid capacity $\\mathcal{N}$? Is there an analytical relationship between $\\mathcal{N}$, target $\\epsilon$-bound, and the expected update rate that minimizes the expected number of global retrains?\n- Also, given the system's high complexity (GMM, sigmoid ensemble, 3 NNs, replay buffers), how sensitive is Sig2Model to hyperparameter tuning (e.g., buffer size $\\rho$, sigmoid capacity $\\mathcal{N}$, GMM components $K$? What is the practical overhead, in terms of both engineering effort and computational cost, of tuning this system compared to simpler baselines like ALEX?\n- What happens if 10k updates arrive all in a very narrow region that was *not* predicted by the GMM? Do we (i) keep adding very similar sigmoids, (ii) quickly hit $\\mathcal{N}$ and retrain, or (iii) collapse several of them into one wider sigmoid? A short experiment with a worst-case update pattern would clarify this.\n- Can you specify the exact baseline configuration of memory size experiment (Fig. 5(c)) and whether the comparison includes the replay buffer and GMM parameters?\n- You require $1 \\le |M‚Äô(k_{j+1};\\Pi) - M‚Äô(k_j;\\Pi)| \\le 2$ to keep the LI lookup correct. Do you enforce this constraint during learning (e.g., as a penalty), or do you just rely on the base RadixSpline‚Äôs bounded search to recover? What fraction of lookups in your experiments had to fall back to the ‚Äúexhaustive search + retrain signal‚Äù path?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "52ksMyaZ8v", "forum": "jmBIK6cajH", "replyto": "jmBIK6cajH", "signatures": ["ICLR.cc/2026/Conference/Submission14099/Reviewer_jLFh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14099/Reviewer_jLFh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994530147, "cdate": 1761994530147, "tmdate": 1762924574087, "mdate": 1762924574087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new approach for updatable learned indexes. It uses a collection of sigmoid functions as its modeling choice, and uses neural networks to find the parameters for the sigmoid functions. It further uses a gmm to estimate how to distribute empty slots after retraining to reduce update time. The experimental results show the approach provides better performance over existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The study of what modeling should be used for better update complexity is interesting\n\n- The experimental results are promising"}, "weaknesses": {"value": "- The paper is poorly written and difficult to understand. There is neither a look up algorithm nor an update algorithm, with the paper diving into various components with little description of how they are used. Fig.2 contains an overview of the approach, but is not sufficiently described. It is referred to at a high level in the intro, but the discussion is missing many details. E.g., what is the embedding? what is $D^\\tau$? This also makes it difficult to understand the rest of the paper. For example, Sec. 2 starts with for a model M at stage $\\tau$...., What is a stage? \n\n- The paper's motivation is questionable. It states any \"any updates to the key domain necessities non-local adjustments to the entire model\". However, such non-local adjustments are easy to make, in practice one can scale the cdf based on the data size after new insertions. The actual model is never updated in existing methods after every update.\n\n- Relatedly the use of sigmoids is questionable. Because every update in fact does not need to modify the model---as the paper eventually converges to as well---a sigmoid function is not needed to create new step-like adjustment to the model. It's unclear why a sigmoid function has better representation power over the typical piecewise linear methods. Overall, the learned index models don't need to capture every step in the cdf, but rather the general data distribution, which is often sufficient with piece-wise linear models\n\n- In fact, a main challenge with the model form is that all sigmoids outputs need to be summed up (which is linear in the number of sigmoid units). Instead, for piece-wise linear methods, we only need to use one linear piece for inference (which can be found in time log of the number of linear pieces). This makes me believe a piece-wise linear model provide a better trade-off in terms of representation power vs inference time complexity. Overall, the paper should do more to be convincing in terms of its modeling choices. \n\n- The experimental setup is unclear. Is there a gpu in the background doing training?"}, "questions": {"value": "Please clarify the experimental setup and provide better justifications for the modeling choices"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wzs6eE9OWV", "forum": "jmBIK6cajH", "replyto": "jmBIK6cajH", "signatures": ["ICLR.cc/2026/Conference/Submission14099/Reviewer_NZxg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14099/Reviewer_NZxg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762200377515, "cdate": 1762200377515, "tmdate": 1762924573677, "mdate": 1762924573677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}