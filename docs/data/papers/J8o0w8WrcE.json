{"id": "J8o0w8WrcE", "number": 14267, "cdate": 1758231583795, "mdate": 1763135190248, "content": {"title": "Fast Physics-Informed Learning via Diffusion Hypernetworks", "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving partial differential equations (PDEs), and they have become a key workhorse in many AI-for-science applications. However, PINNs remain highly sensitive to factors such as initial conditions, domain geometries, and physical parameters. As a result, they typically require full retraining when these PDE-defining parameters change. In this work, we propose a diffusion-based hypernetwork that distills knowledge from training data to substantially accelerate PINN training. Our approach leverages a denoising diffusion probabilistic framework to generate  PINN weights conditioned on PDE parameters. Once trained, the hypernetwork can directly produce PINNs for a family of parametric PDEs without requiring additional optimization. For more complex problems, the generated weights, used as initializations, reduce the training time by approximately 46% for the Burgers1D-complex dataset and 60% for the Wave2D dataset. Furthermore, the model demonstrates robustness to out-of-distribution PDE parameters, extending its applicability beyond the training distribution.", "tldr": "A generative hyper-diffusion model that produces PINN weights conditioned on task information, accelerating fine-tuning while achieving comparable or superior accuracy.", "keywords": ["AI for Physics", "Hypernetwork", "Generative Model", "Diffusion Model", "PINNs"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b91402b7e64841b99e927443bf0b083c886ef125.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel diffusion-based hypernetwork that distills knowledge from training data to substantially accelerate PINN training, termed HD-PINN. Specifically, a denoising diffusion probabilistic framework is introduced to generate PINN weights conditioned on PDE parameters. Experiments demonstrate that initializing models with the proposed method effectively enhances training efficiency across multiple tasks. Furthermore, out-of-distribution (OOD) experiments verify the potential of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors have provided code for review, which enhances the reproducibility of this work.\n- This paper is easy to follow.\n- This paper presents a promising framework."}, "weaknesses": {"value": "- The paper lacks a sufficiently clear description for introducing a diffusion-driven hypernetwork:\n    - The reason for introducing hypernetwork: Using PDE/IC/geometric parameters as inputs to PINNs has already been shown to achieve strong generalization after training. This approach has proven highly effective in sufficiently complex tasks (e.g., Parameterized 3D Heat Sink, NVIDIA SimNet™: An AI-accelerated multi-physics simulation framework).\n    - The reason for introducing diffusion-based methods: Directly predicting PINN weights via a hypernetwork (without diffusion) is also feasible, more straightforward, and commonly employed.\n- Due to the above weakness, the experiments are insufficient. The authors should conduct thorough comparisons with parameterized PINNs (e.g., as in \"Parameterized Physics-informed Neural Networks for Parameterized PDEs\") to demonstrate the advantages of hypernetwork-based methods in generalization and OOD problems.\n- The paper lacks comprehensive ablation studies to evaluate the effectiveness of components in HD-PINN.\n- Although the paper presents a promising framework, the model lacks novelty, and the analysis is not sufficiently in-depth."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ouc7NotvwV", "forum": "J8o0w8WrcE", "replyto": "J8o0w8WrcE", "signatures": ["ICLR.cc/2026/Conference/Submission14267/Reviewer_KhnP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14267/Reviewer_KhnP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760680227256, "cdate": 1760680227256, "tmdate": 1762924718708, "mdate": 1762924718708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "gF1E7vZxBz", "forum": "J8o0w8WrcE", "replyto": "J8o0w8WrcE", "signatures": ["ICLR.cc/2026/Conference/Submission14267/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14267/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763135189438, "cdate": 1763135189438, "tmdate": 1763135189438, "mdate": 1763135189438, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HD-PINN, a diffusion-based hypernetwork that generates or initializes the weights of physics-informed neural networks (PINNs) conditioned on PDE parameters. The authors claim it can drastically reduce training time for parametric PDE families."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Conceptual novelty: Using diffusion models to generate PINN weights is an interesting and original idea.\n\nReasonable speed-ups: On simple testbeds, the initialization reduces training time by about 46–60%.\n\nClean formulation: The method is well-explained, and code is provided for reproducibility."}, "weaknesses": {"value": "Limited scope of experiments: All tests are on low-dimensional, well-behaved PDEs: 1D Burgers’ and 2D Wave equations. These are relatively smooth, low-chaotic systems where even training from random initialization converges easily (see Table 1 — only seconds for Burgers1D). The method’s claimed acceleration is thus on problems where the baseline is already trivial.\n\nNo evaluation on difficult or realistic PDEs: There is no test on stiff, chaotic, or discontinuous systems (e.g., Navier–Stokes turbulence, shock-dominated Burgers, Allen–Cahn, Kuramoto, Sivashinsky, or reaction–diffusion). Without such tests, it’s unclear whether the diffusion-generated weights encode meaningful inductive bias beyond simple regression.\n\nQuestionable generalization: The out-of-distribution (OOD) tests shift only scalar parameters (e.g., alpha range or domain diameter), and even small shifts lead to noticeable degradation. There is no validation under truly different boundary conditions or nonlinear parameter regimes. \n\nComparison with Neural Operators: The HD-PINN can solve a class of PDEs with its diffusion. However, neural operators can also solve a class of PDEs with a certain parametrization. I am unsure what the benefits are compared with neural operators. And such a comparison is lacking. I.e., No comparison with modern operator-learning methods (FNO, DeepONet) that already generalize across PDE families more effectively.\n\nScalability concerns: The hypernetwork is extremely large (12-layer DiT with >1k hidden dimension) relative to the tiny PINNs (3–4 layers). The paper does not quantify the training cost of the hypernetwork itself, which may exceed the total savings. Also, what is the cost comparison with respect to neural operators that do not require additional networks?"}, "questions": {"value": "See weakness. \n\nWhile the idea of diffusion hypernetworks for PINNs is conceptually interesting, the current experimental evidence is insufficient.\nThe method is only validated on simple, low-dimensional PDEs where training is already easy. It does not demonstrate robustness to chaotic dynamics, discontinuous solutions, or high-dimensional physical systems — the true bottlenecks for PINNs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9og89CMrSM", "forum": "J8o0w8WrcE", "replyto": "J8o0w8WrcE", "signatures": ["ICLR.cc/2026/Conference/Submission14267/Reviewer_Kvqt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14267/Reviewer_Kvqt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546466792, "cdate": 1761546466792, "tmdate": 1762924718312, "mdate": 1762924718312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a diffusion hypernetwork framework for PINN training. The diffusion models condition on PDE parameters (e.g., initial conditions). Once trained, the hypernetwork can either directly produce PINN weights for simple PDEs or provide good initializations. It is tested on three PDE problems and compared with other meta learning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The integration of diffusion hypernetwork and PINN training is novel.\n- The method also shows some robustness to out-of-distribution PDE parameters"}, "weaknesses": {"value": "- Costs and Efficiency. The paper presents training time reductions as a major contribution but does not account for the cost of generating the training dataset and training a DiT model, which are significant. Additionally, the inference cost of the diffusion sampling process is not reported. These make the practical efficiency gains unclear.\n- Limited Evaluation. The experiments only cover two types of PDEs (1D Burgers and 2D wave) on low-resolution domains. This raises serious questions about whether the approach can scale to challenging PINN problems.\n- Missing Baselines. The paper mainly compares against random initialization and two meta-learning methods (MAML and Reptile), but there are tons of PINN variants that significantly boost accuracy and training time. These methods can be found in any literature review about PINNs."}, "questions": {"value": "- Why use a hypernetwork instead of directly generating solutions (for example, DiffusionPDE and FunDPS)? This architectural choice needs justification.\n- A comparison with simple transfer learning (training a PINN on an average or another problem, then fine-tuning) would be valuable.\n- The inference stage is not explored: How many steps are needed? How sensitive is it? What's the trade-off between inference steps and final accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JwbZiKuas2", "forum": "J8o0w8WrcE", "replyto": "J8o0w8WrcE", "signatures": ["ICLR.cc/2026/Conference/Submission14267/Reviewer_DkTU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14267/Reviewer_DkTU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971092203, "cdate": 1761971092203, "tmdate": 1762924717613, "mdate": 1762924717613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors use diffusion models conditioned on the coefficients of a PDE to provide better initialization, thereby improving training time and performance. They further test their approach on three cases: two involving the 1D Burgers equation and one involving the 2D Wave equation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed initialization significantly accelerates training, potentially enabling few-shot training for PINNs."}, "weaknesses": {"value": "- While the paper compares training time and accuracy, it is unclear whether the models are useful in scenarios where traditional computational fluid dynamics (CFD) would struggle. The experimental cases presented do not demonstrate such challenging situations.\n- Although training time improves for the 2D Wave equation, it is not evident whether out-of-distribution (OOD) performance is substantially better than random initialization. The same concern applies to the Burgers cases."}, "questions": {"value": "- Are any of the experimental cases particularly difficult to solve? If so, what makes them challenging?\n- Could the authors provide experiments on a case that is challenging for CFD methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SRqGKwQNeD", "forum": "J8o0w8WrcE", "replyto": "J8o0w8WrcE", "signatures": ["ICLR.cc/2026/Conference/Submission14267/Reviewer_Z15p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14267/Reviewer_Z15p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030592012, "cdate": 1762030592012, "tmdate": 1762924717146, "mdate": 1762924717146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}