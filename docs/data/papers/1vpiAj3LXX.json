{"id": "1vpiAj3LXX", "number": 24381, "cdate": 1758356293765, "mdate": 1759896768843, "content": {"title": "Semantic-metric Bayesian Risk Fields: Learning Robot Safety From Human Videos With a VLM Prior", "abstract": "Humans interpret safety not as a binary signal but as a continuous, context- and spatially-dependent notion of risk. While risk is subjective, humans form rational mental models that guide action selection in dynamic environments. This work seeks to extract an implicit human risk model by introducing a novel, semantically-conditioned and spatially-varying parametrization of risk, supervised directly from safe human demonstration videos and VLM common sense. Notably, we define risk through a Bayesian formulation. The prior is furnished by a pretrained vision-language model, while the likelihood function is a learned ViT that maps pretrained features (e.g., DINOv3), to pixel-aligned risk values. Our pipeline produces pixel-dense risk images that can then be used as value-predictors in robot planning tasks or be projected into 3D for use in conventional trajectory optimization to produce human-like motion. This learned mapping enables generalization to novel objects and contexts, and has the potential to scale to much larger training datasets. In particular, the Bayesian framework that is introduced enables adaptation of our model to additional observations or common sense rules. We demonstrate that our proposed framework produces contextual risk that is aligned with human preferences. Additionally, we illustrate several downstream applications of the model; as a value predictor for a visuomotor policy or in conjunction with a classical trajectory optimization algorithm. Our results suggest that the proposed method is a significant step toward enabling autonomous systems to internalize human-like risk reasoning.", "tldr": "We propose a framework for learning contextual and spatially-dependent risk from safe human demonstrations and common sense rules.", "keywords": ["Safety", "Risk-aware Planning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97ca9af13a5c9a04f3f347930c228b01366eaf1e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Semantic-Metric Bayesian Risk Fields, a framework that regresses pixel-dense, context-conditioned risk maps from safe-only human demonstrations and a VLM/LLM-derived semantic prior. The core modeling choice decomposes a “viability” measure into a Bayesian likelihood over distances conditioned on context (learned from RGB-D videos) and a prior over pairwise object semantics (queried and post-processed from an LLM), with the likelihood parameterized as a smooth CDF (via Bézier control points). The resulting risk maps can be used either as value signals for policies or for classical trajectory optimization. Experiments on tabletop manipulation suggest the approach produces qualitatively reasonable risk maps and, as a planner cost, yields trajectories that are rated safer than those from trained visuomotor baselines in the reported setup."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The formulation connects an intuitive decomposition to a practical pipeline that yields dense, usable risk fields with minimal bespoke supervision. Parameterizing the likelihood as a smooth CDF via low-dimensional Bézier control points is a neat design that regularizes the estimator and stabilizes training from limited demonstrations while enforcing monotonicity in distance by construction. The system-level integration is clear: DINOv3 features provide pixel-aligned semantics, the LLM-derived prior encodes coarse common sense at scale, and the posterior is consumed by both learning-based policies and a classical trajectory optimizer. Qualitative visualizations show that the principle works, are context sensitive to the manipulated object, and the trajectory optimization results demonstrate that the proposed explicit risk fields can be preferred over end-to-end policies in the reported tasks.\n\nThe figures are mostly clear and aid in understanding the main text."}, "weaknesses": {"value": "## Contibution\nThe theoretical component is thin and rests on strong assumptions that limit real-world applicability. Methodologically, the approach largely assembles known components (SAM2/YOLO for masks, DINOv3 features, transformer on features, LLM-generated priors), and novelty resides mainly in the particular Bayesian factorization. Ablations quantifying the contribution of each choice are limited. The likelihood is trained from only seven safe-only videos, and the large count of “examples” stems from frame and pairing combinatorics rather than independent, diverse contexts. It is unclear how well the method would generalize to other scenes.\n\n## Correctness\nThe key assumption that the distribution of distances does not depend on the context and the normalization factor can be left out, justified by “arbitrary” distance distributions or by experimental design, is not credible for most scenarios, where distances and motion are heavily context-, affordance-, and task-dependent. Researchers designing the distribution of distances for experiments will always choose distributions based on the task.\n\nThe central claim that risk is non-decreasing with distance is often violated in practice. Safety semantics go well beyond distances as orientation, velocities, occlusions, containment, support relations etc may all play a role. Theorem 1 amounts to a monotonicity/ranking statement under monotone mappings and adds little substantive guarantee. The introduction’s suggestion that the method “leverages guarantees from rigorous control-theoretic approaches” overstates what is proven. There is no guarantee for anything in the pipeline, especially since the inputs for calculating the semantic-metric bayesian risk fields come from black box neural networks.\n\nThe treatment of interchangeability is also questionable. Risk is framed as permutation-invariant in parts of the model, yet many pairwise risks are asymmetric with respect to which object is manipulated (e.g., moving water above a laptop versus moving a laptop near a stationary glass). This asymmetry should be reflected in how likelihoods and priors are conditioned.\n\nThe prior relies on LLM ratings that are themselves subjective and prompt-sensitive. The paper notes some manual rules (e.g., forcing tabletops to zero risk), suggesting non-trivial tuning for each scenario. Important implementation details are underspecified: how 3D distances are computed robustly from RGB-D streams (calibration, missing depth, occlusions), how the “straight-line” path is justified in clutter with obstacles unrelated to the focal pair’s semantics, runtime to generate risk fields, and how thresholds for viability-to-radius conversion are selected. \n\n## Experiments\nOn evaluation, the baselines underperform strikingly (0/33 top-1 preferences for both VLA and diffusion policy), which raises concerns about setup, training parity, and metric sensitivity. Human ratings of “riskiness” are inherently subjective. The paper would benefit from objective proxies (e.g., collision/near-miss rates under perturbations, task success under constraints, or compliance with specified semantic rules). Claims of generalization to unseen objects/contexts are not rigorously substantiated beyond qualitative figures. Distribution shift tests, cross-scene generalization, and robustness to prior misspecification are missing.\n\n## Language\nOverall the paper is decently organized and the language is fair, but there are a several places where it requires polishing.\n- On line 236 it's hard to understand what is meant by safe only human demonstrations.\n- Other sections are missing words, contain overly long sentences, or are grammatically incorrect (e.g. line 238, 263, 384, 421)\n- Active citations are being used as passive ones at line 431, 451, 456.\n\nThere are terminology and notation issues: \n- “functional” is misused. A functional by definition refers to a mapping from functions as input to some scalar value. In the paper, the \"likelihood functional\" maps vectors to function parameters.\n- The symbol d is overloaded for both distance and the number of frames/samples."}, "questions": {"value": "- What concrete safety guarantees does the framework provide? Beyond the monotonic ranking lemma, can any bound or certification (e.g., chance constraints under model uncertainty) be stated for the induced trajectories?\n- How sensitive are results to the distance-independence assumption? If $P(\\hat{d} \\leq d | \\phi)$ depends on $\\phi$, what changes in the estimator or the training objective, and does Theorem 1 still hold?\n- How do you incorporate non-distance state variables (velocity, orientation, containment/support, spillage direction) that can dominate semantic risk? Can the likelihood be extended to a richer metric space while preserving smooth CDF parameterization?\n- How is the LLM prior calibrated against human preferences beyond prompt engineering? Can you quantify inter-rater reliability, prior uncertainty, and posterior sensitivity to prior misspecification?\n- Have you evaluated on unseen scenes, object categories, and camera viewpoints with held-out environments? The paper claims that this is easily feasible through generating synthetic data, but there is no evidence to back this up.\n- Please clarify terminology (“functional”), fix notation clashes (d for distance vs. sample count), and address the noted language/editing issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wtKpq6LxU0", "forum": "1vpiAj3LXX", "replyto": "1vpiAj3LXX", "signatures": ["ICLR.cc/2026/Conference/Submission24381/Reviewer_uqYx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24381/Reviewer_uqYx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761066151317, "cdate": 1761066151317, "tmdate": 1762943064322, "mdate": 1762943064322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework to learn a model of human-like, contextual safety for robot manipulation. The key idea is to formalize risk using a Bayesian formulation, where the posterior risk is proportional to a likelihood and a prior. The framework combines these two components to produce pixel-dense risk maps for a given scene and manipulated object. The authors conduct experiments qualitatively and quantitatively."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The core Bayesian decomposition is an elegant way to formalize intangible risk. It provides a clear and interpretable separation between behavior learned from observation and common sense knowledge.\n2. A major strength is the data-collection strategy. The framework learns without requiring any unsafe demonstrations. The Likelihood is learned from safe-only human videos and the Prior is generated by a VLM.\n3. This paper leverages a suite of modern foundation models to build its system. The fact that this framework's risk field, when paired with a classical trajectory optimizer, produces trajectories rated as significantly safer by humans than DP and VLA policies is very promising.\n4. The output is not a monolithic policy but a risk field. This field is flexible and can be used as a value function for learned policies or as a costmap for classical planners, bridging the gap between learning and classical robotics."}, "weaknesses": {"value": "1. The entire framework relies on the critical assumption that the evidence term is independent of the semantic context. This assumption is necessary to avoid computing intractable term. However, this seems to contradict the paper's own premise. Is it really true that the general distribution of distances between objects is independent of their semantics? Humans are likely to behave differently (and thus create different distance distributions) around a `knife` vs. a `teddy bear`, even in safe scenarios. This core assumption needs more justifications.\n2. The Likelihood model, which is meant to capture all of human demonstrated behavior, is trained on only 7 videos. While this is processed into 648000 examples, it's still a tiny and low-diversity sample of human behavior. This makes the claims of generalization to novel contexts less credible, the strong performance is likely heavily reliant on the DINOv3 features rather than a richly learned likelihood model.\n3. The Prior model is not a learned generative model of risk but a massive, pre-computed lookup table. A VLM is prompted to generate ~60k pairwise object ratings, and the model uses DINOv3 features to find the closest object in this table to get its risk score. This approach has two key limitations:\n   - It doesn't generalize to objects truly unseen in its 300-object list, it just finds the nearest neighbor.\n   - It cannot scale to combinatorial, N-way interactions (e.g., risk of `water` + `laptop` + `person`), as the LUT size would explode.\n4. Results in Table 1 relies on trajectory ratings from a human rater. For a subjective, preference-based metric like `riskiness`, a single rater is a very small sample size and insufficient to draw general conclusions.\n5. The paper would be significantly stronger if it included experiments in a standardized, reproducible simulation environment. This would allow for a much larger scale of quantitative testing and a fairer comparison against other methods."}, "questions": {"value": "1. Could the authors provide a stronger justification for the key assumption that the evidence term is independent of the context? This seems to be the weakest link in the paper's theoretical foundation.\n2. In Figure 4, the Likelihood map for the Cup (row 1) already seems to assign a high risk to the Laptop. The Likelihood is trained on 7 safe human demos. Did this small dataset just happen to include a `cup near laptop` demonstration? If so, doesn't this blur the clean separation of the metric Likelihood (learning from demos) and the semantic Prior (common sense)? If not, why is the Likelihood high for the laptop?\n3. How does the Prior model (the LUT) handle a truly novel object that is not one of the ~300 objects queried from the VLM? Does it just default to the nearest DINOv3 neighbor, and how does this affect the risk estimate? Furthermore, how could this LUT-based approach ever scale to 3-way or N-way interactions (e.g., the risk of a `knife` near a `person` and a `fragile object`)?\n4. Can the authors confirm that only one human rater was used for the trajectory preferences in Table 1? If so, this is a significant limitation. Are there plans to expand this to a more robust user study with multiple raters to validate these subjective results?\n5. To improve the reproducibility and scalability of the evaluation, have the authors considered validating their framework in a standardized simulation benchmark? This would allow for more extensive quantitative comparisons and a clearer analysis of how the risk field performs against other safety-oriented planners."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PrW2YPQhb6", "forum": "1vpiAj3LXX", "replyto": "1vpiAj3LXX", "signatures": ["ICLR.cc/2026/Conference/Submission24381/Reviewer_P1Bd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24381/Reviewer_P1Bd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909071780, "cdate": 1761909071780, "tmdate": 1762943063805, "mdate": 1762943063805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Bayesian framework to quantify risk/viability as a function of context and pairwise object distance. By assuming the distribution of distances is context-independent, viability $\\propto$ semantic prior of safety, $P(\\text{safe}|\\phi)\\times$ the CDF of $p(d|\\text{safe}, \\phi)$. The semantic prior of safety is derived from common sense knowledge of ChatGPT-5. The conditional distance distribution $p(d|\\text{safe}, \\phi)$ is learned from safe-only human demonstrations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses the important question of inferring semantic safety that is aligned with human preferences. \n- The proposed method does that by learning from safe-only human demonstrations."}, "weaknesses": {"value": "- While the work is motivated by semantic safety, it relies heavily on distance between objects snd collision avoidance. \n- The absolute value of the learned viability in this work has no meaning, only the relative value does, which introduces difficulty in interpreting the value. This is a result of the normalization factor in Bayes inference cannot be computed. More commonly, risk is defined as the probability of failure $\\in [0. 1]$\n- The experimental results are weak. There is no comparison to any method of estimating risk from demonstrations or any risk-aware control policy. \n    - In the first downstream application of value predictor for visuomotor policy, only qualitative results are shown. To be honest, the qualitative results do not entirely make sense. For instance, 2nd row of Figure 4, there is only medium risk (yellow) of placing the laptop on the cup. The learned likelihood map looks spare, which may be a result of training on limited data. \n    - In the second downstream application of risk-aware trajectory optimization, neither of the baselines explicitly considers risk..."}, "questions": {"value": "- How is the experiment for data collection designed? What are the human demonstrator instructed to do? How much data is sufficient?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4CB6ZE3SY4", "forum": "1vpiAj3LXX", "replyto": "1vpiAj3LXX", "signatures": ["ICLR.cc/2026/Conference/Submission24381/Reviewer_NSa1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24381/Reviewer_NSa1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958836828, "cdate": 1761958836828, "tmdate": 1762943063600, "mdate": 1762943063600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Semantic-Metric Bayesian Risk Fields, a novel framework for learning contextual, spatially-varying risk from human demonstration videos using a Bayesian formulation. The prior is derived from a vision-language model (VLM) and large language model (LLM), while the likelihood is learned from object interactions in human demonstrations. The resulting pixel-dense risk maps can be used for robot planning and trajectory optimization. The authors demonstrate the model’s generalization and utility across several downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well-written and conceptually clear, with strong motivation for modeling semantic safety.\n* The Bayesian formulation is elegant and aligns well with human-like reasoning about risk.\n* The integration of VLM features and LLM-derived priors is creative and enables generalization to unseen contexts.\n* The image processing pipeline and use of Bézier curve fitting for CDFs are technically practical."}, "weaknesses": {"value": "* While the framework is novel in its composition, many components (e.g., risk from demonstrations, VLM features, LLM priors) are adapted from existing ideas.\n* The theoretical contributions (e.g., viability consistency, risk consistency) are intuitive and not particularly deep.\n* The dataset used for likelihood regression is small, and the evaluation lacks rigorous quantitative comparisons to baselines.\n* The prior fitting relies heavily on LLM outputs, which may not always align with human preferences without careful calibration."}, "questions": {"value": "* How sensitive is the model to inaccuracies in the LLM-derived prior?\n* Can the authors provide more quantitative comparisons to existing risk-aware planning methods?\n* How well does the model perform in cluttered or ambiguous scenes where object semantics are less clear?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rVwiTxZLac", "forum": "1vpiAj3LXX", "replyto": "1vpiAj3LXX", "signatures": ["ICLR.cc/2026/Conference/Submission24381/Reviewer_fQCW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24381/Reviewer_fQCW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138768138, "cdate": 1762138768138, "tmdate": 1762943063253, "mdate": 1762943063253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}