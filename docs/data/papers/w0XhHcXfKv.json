{"id": "w0XhHcXfKv", "number": 15463, "cdate": 1758251645608, "mdate": 1759897305375, "content": {"title": "Graph Signal Processing Meets Mamba2: Adaptive Filter Bank via Delta Modulation", "abstract": "State-space models (SSMs) offer efficient alternatives to attention with linear-time recurrence. Mamba2, a recent SSM-based language model, uses selective input gating and a multi-head structure, enabling parallel computation and strong benchmark performance. However, its multi-head recurrence operates independently without structured utilization or analysis. In this work, we propose a novel method called **H**ierarchical **AD**aptive filter bank for **E**fficient **S**SMs (*HADES*), a Graph Signal Processing (GSP)-inspired framework that reinterprets Mamba2 as an adaptive filter bank on a line graph. Our hierarchical architecture introduces two filter types: shared filters for global low-pass behavior and expert filters for local high-pass behavior, achieved through structured bias on the parameter $\\Delta$. *HADES* achieves comparable performance to baseline models including Mamba2 across various benchmarks in language modeling, commonsense reasoning, and long-context retrieval, while using only **58.9%** of the original parameters. In this regard, *HADES* bridges GSP and neural sequence modeling, enabling efficient, hierarchical, and interpretable filtering within state-space models.", "tldr": "HADES reinterprets Mamba2 as a graph-based adaptive filter bank, achieving efficient and interpretable sequence modeling with fewer parameters.", "keywords": ["State Space Model", "Mamba", "Graph Signal Processing", "Adaptive Filter Bank"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63c212cf9d9001bb609128c091d9d5bdf1e15d94.pdf", "supplementary_material": "/attachment/b45563bece5fd6cf069f30ad8fb9448e948b4b5f.zip"}, "replies": [{"content": {"summary": {"value": "The paper leverages the framework of Graph Signal Processing (GSP) to interpret and enhance Mamba2, and proposes a hierarchical filter band model architecture, HADES. Each SSM head is interpreted as a graph filter over a line graph, and HADES leverages shared and expert filters for capturing global and local properties, respectively. The paper further designed two losses - load balance loss and diversity loss - to optimize the learning of the expert filters. Empirical results show that HADES reports comparable to better accuracy than baseline across language modeling, reasoning, and retrieval, without long context fine-tuning. Filter selection analysis and frequency response analysis further validate the behavior of the expert filters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The graph signal processing perspective on Mamba and ssm architecture is novel. Connecting the S4 and Mamba2 kernel to graph filters on a line graph and extending to time-varying filters makes the multi-heads as filter-banks interpretation explicit and operational. The shared plus expert filter design with spectral bias is a novel mechanism to impose functional diversity with small overhead. \n2. The architecture design (shared and expert filter with respective modulations) and loss design (load balance loss to prevent collapse of filter usage and diversity loss for decorrelating filter outputs) are well-motivated, clearly formulated, and empirically supported by in-depth analysis. \n3. The experimental design, especially the in-depth analysis into the behavior of the filters, provides interesting insight into the effectiveness and efficiency of the model, further validating the approach."}, "weaknesses": {"value": "1. While the GSP-inspired perspective is intriguing, it mostly serves as an interpretation rather than a source of rigorous new theory. The idea of a ``line graph filter” for a time series is essentially equivalent to a standard 1-D convolution or recursive filter, which is a well-studied concept. The paper does not derive new analytical results (e.g. no formal theorem about stability or frequency response), and it doesn’t seem to leverage graph signal processing theory beyond the descriptive level.\n2. The HADES model introduces additional complexity over the base Mamba2. It splits the SSM heads into shared vs expert categories and requires a routing network per time step, along with new hyperparameters. \n3. The comparison only consists of baseline models (mamba1, Mamba2, and linear transformer variants). It would be interesting to see the performance comparison between HADES and Mamba2 models with additional comparable enhancements, such as mixture models."}, "questions": {"value": "1. How is the training speed and convergence behavior of HADES compared to baseline models? \n2. How are the gradients of the Top-K expert selection propagated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DYp0V4kcgW", "forum": "w0XhHcXfKv", "replyto": "w0XhHcXfKv", "signatures": ["ICLR.cc/2026/Conference/Submission15463/Reviewer_BvVa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15463/Reviewer_BvVa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678841298, "cdate": 1761678841298, "tmdate": 1762925753714, "mdate": 1762925753714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel SSM-based architecture that builds upon Mamba2. Hierarchical ADaptive filter bank for Efficient SSMs (HADES) is a Graph Signal Processing (GSP)-inspired framework that modifies portions of Mamba2, such that each head corresponds to a graph filter (here, the sequence of tokens is viewed as a line graph) and the multi-head SSM to a bank of filter whose outputs are aggregated. HADES introduces two kinds of filter: namely, a shared filters that is applied (always selected so not part of routing) to capture global content, and expert filters that are routed per token (similar to Mixture-of-Experts or Mixture-of-Heads). Here, the router routes based on the \"spectral residual\", defined as $x_t - mean(x_1,...,x_t)$, and $\\Delta_{t, base}$. In order to guarantee that all expert filters are used evenly, they add a load-balancing loss, similar to MoE designs, and a diversity loss that penalizes filter outputs' deviations from pairwise orthogonality.\n\nExperimentally, the authors show that HADES outperforms other architectures in this line of work, such as Mamba2 and DeltaNet, on 8 zero-shot language tasks (those standard in LM-eval harness and used in the Mamba2 paper for evaluation). When trained on 200B tokens on Pile, their 218M-parameter model outperforms 370M-parameter baseline models. Plus, there is no issue regarding latency and computation memory. Hades beats Mamba2 in long-context retrieval (pass-key retrieval). Finally, the paper offers some interpretability analyses. Frequency‑domain plots suggest shared filters act low‑pass while expert filters emphasize higher frequencies, particularly with the spectral bias on $\\Delta$, and expert‑selection heatmaps show task‑region‑specific specializations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Originality: \n- Recasts multi-head Mamba2 as a graph filter bank on a line graph, connecting LTV SSMs to graph signal processing (GSP) and framing heads as node-variant graph filters. Introduces a novel architecture HADES, a hierarchical filter bank with (i) always-on shared filters and (ii) token-routed expert filters, selected via a spectral residual and $\\Delta$-modulation.\n- Their construction of expert filters creates more opportunity for modular/interpretable filters, as they are trained to be distinct from each other.\n\nQuality:\n- Strong efficiency–performance trade-off: ~59% of Mamba2’s parameters (≈218M vs. 370M) while matching or surpassing baselines on zero-shot reasoning and long-context passkey retrieval; reported ~1.37× speed and memory gains.\n- Their construction is not complicated and does not add much computational overhead when routing.\n- The ablation and sensitivity analyses make the empirical investigation more thorough, and show that each of their designs are necessary.\n\nClarity:\n- The presentation is clear for readers who are less familiar with SSM-based architecture, and the differences between Mamba2 and HADES are clear, both mathematically and visually. Background sections concisely link SSM kernels, convolutional views, and GSP operators, helping readers follow the reinterpretation.\n- The hyperparameters and experimental descriptions are quite thorough, but certain sections of the appendix are not properly linked to the main body so, at times, it may be a slightly confusing.\n\nSignificance:\n- While I don't find the connection entirely illuminating, the GSP lens offers a principled design template for structured, adaptive filtering in SSMs, which may be useful beyond HADES for designing and analyzing SSM-based models.\n- Similarly to the points made for originality, the paper demonstrates decent empirical performances and long-context understanding ability. The devised architecture may help future interpretability research as well."}, "weaknesses": {"value": "While I enjoyed the paper's presentation and ideas overall, I think the major weakness of the paper is the strength of their empirical evidence. I will list this in two major axes:\n1. **Lack of scale**: Despite the thorough experiments in ablation, sensitivity, and multiple baselines, the paper only operates on the 200B-token Pile training of 370M-parameter models. Because we only get one data point in the (number of tokens trained, model scale), it is hard to know if the model will scale or holds in any other regime of training. If we have another model size (preferably on the larger side) and another training size (100B or 3-400B tokens), the authors would be able to make more robust conclusions. \n- Also, while I acknowledge that SSM-based works such as Mamba pretrain on Pile, this practice holds less value in 2025. We have DCLM, FineWeb, and Nemotron-CC, which are better natural language datasets, but to continue doing experiments on Pile dataset seems outdated.\n- Instead of the 218M HADES model, I would like to see a 370M (or FLOPs-equivalent) HADES model being compared to the baseline models to observe the real gap when controlling for FLOPs or parameters.\n- \"even with a drastically reduced model size of approximately 38.64% (143M) in the H = 8 setting, our model maintains performance comparable to the optimal hyperparameter configuration and even outperforms it on two tasks\" (L903-905): while the authors analyze their results with such framing, it makes me wonder if HADES does not scale well due to this result. If more FLOPs or more parameters lead to better performance (a necessary feature for language models), then this result seems unintuitive. \n2. **Randomness across seeds**: Table 1 and Table 3 do not account for randomness across training runs/seeds. While standard error is low, this only accounts for sampling variance from the dataset and not the variance in training. If possible, averaging evaluation accuracies across five identically trained models (different seeds) for Table 1/3 would make the results more trustworthy. \n- Did you also perform a hyperparameter sweep/tuning for the baseline models, or only perform hparam tuning for HADES?\n\nMy score is between 4 and 6, so for now, I will keep my score as 4 (marginally below the acceptance threshold). Once some questions and concerns are addressed, I am willing to increase."}, "questions": {"value": "For questions below are minor. Addressing the weaknesses (and suggestions therein) would help me re-evaluate this work.\n\n**Minor misc.**\n1. Eq. (1) in L88 should have $\\bar{A} h(t)$ instead of $\\bar{A} h(t-1)$, right? In continuous time one expects no shift and  $\\bar{A} h(t)$ is the one written for the Mamba paper.\n2. Top‑K selection is non‑differentiable; the paper does not state whether it uses straight‑through, Gumbel‑TopK, or train with continuous weights and harden at eval. Or does Top-Q selection mean something else than I am missing?\n3. I don't entirely understand the load-balance loss in Eq (15). Is this written correctly and precisely? What expectation are we taking and how do we square a vector?\n3. Appendix B.1 says: “For fair comparison, all models are trained... with 370M parameters...”, but I assume here HADES is 218M? It's written confusingly here. If you put the number of parameters and also FLOPs on the table, that would be helpful.\n4. L1025 calculation is wrong: 368,346,624−150,407,424=217,939,200.\n5. Can you also report training perplexity in Table 1?\n6. Can you report the model size in Table 6? I would interested in the latency of HADES when controlling for model size or memory, but here, the latency primarily seems to come from being smaller."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GwLqX1U8i9", "forum": "w0XhHcXfKv", "replyto": "w0XhHcXfKv", "signatures": ["ICLR.cc/2026/Conference/Submission15463/Reviewer_neR9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15463/Reviewer_neR9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979483804, "cdate": 1761979483804, "tmdate": 1762925752932, "mdate": 1762925752932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes multi-head SSMs and Mamba2 as a graph filter bank over a line graph, and introduces HADES. HADES routes per-token to a small set of expert filters, while shared filters are always applied to preserve global information. HADES adds two regularizers - (i) load-balance over expert scores and (ii) output-diversity across filters to prevent collapse. HADES reports competitive performance across 8 zero-shot tasks while using only 58.9% of the parameters, and demonstrate stronger passkey retrieval upto 16K context length."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The formalization of SSM heads into Graph Signal Processing's filter bank is clear and allows principled analysis about low-pass and adaptive behavior. \n- The routing/bias mechanism tied to \\Delta_{HADES} gives a minimal hook for content-adaptive dynamics which aligns with SSM parametrization.\n- Competitive results at a much lower parameter count regime and performance improvements in long-context tasks.\n- The ablation study and analysis are thorough."}, "weaknesses": {"value": "- While the competence of HADES with respect to the reduced number of parameters seems promising, the architecture does seem to cause more FLOP overhead. Listing this analysis would strengthen the contributions of this work.\n- The spectral analysis (FFT) on hidden sequences from one layer may be confounded with the layer or the gamma value."}, "questions": {"value": "- For passkey, can the region order be randomized to evaluate the model's robustness?\n- When computing the running mean for the token router, is this strictly causal?\n- How might HADES compare to simpler head routing apporaches under the same budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NSdFSJ6XTY", "forum": "w0XhHcXfKv", "replyto": "w0XhHcXfKv", "signatures": ["ICLR.cc/2026/Conference/Submission15463/Reviewer_QPLB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15463/Reviewer_QPLB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990268040, "cdate": 1761990268040, "tmdate": 1762925752405, "mdate": 1762925752405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HADES, a new architecture based on Mamba2 that is inspired by Graph Signal Processing (GSP). The authors frame Mamba2's multi-head structure as a filter bank operating on a line graph. The HADES model introduces a hierarchy of filters: shared filters for global, low-frequency information and expert filters for local, high-frequency details. A routing mechanism selects which expert filters to apply to a token based on its \"spectral residual,\" which is also used to modulate the delta parameter. The method is regularized using two auxiliary losses to encourage filter balance and diversity. The primary contribution is a model that achieves comparable performance to Mamba2 on several benchmarks while using only 58.9% of the parameters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces an interesting conceptual link between Graph Signal Processing and Mamba2, re-framing multi head SSMs as filter banks. This GSP perspective motivates a novel routing mechanism based on a spectral residual which is a creative approach to token-adaptive computation.\n\n2. The authors conduct a thorough set of ablations on their proposed 370M parameter model, which helps validate the components of their design, such as the auxiliary losses and the hierarchical filter structure. The parameter reduction to 58.9% at this scale is notable. The in-depth analysis showing filter specialization (low-pass vs. high-pass) is also a quality contribution.\n\n3.The paper is clearly written, and the figures are helpful for understanding the architecture and its intended behavior. The authors provide a good explanation of their GSP-inspired framework.\n\n4. The work demonstrates a potential path to more parameter-efficient SSMs. The in-depth analysis showing filter specialization is a good step towards more interpretable models."}, "weaknesses": {"value": "1. The paper's primary weakness is that all experiments are confined to a single, small 370M parameter model. The central claim of 58.9% parameter savings is not validated at larger scales (e.g., 1B+), where model dynamics and efficiency trade-offs are known to change. This severely limits the generality and impact of the findings.\n\n2. The paper's core premise of a GSP framework is not very strong. The authors admit that \"spectral properties are not explicitly enforced\" but rather \"indirectly encouraged\". The model does not perform operations in the spectral domain. This makes the GSP contribution feel more like a motivating analogy for a heuristic-based MoE router, rather than a principled application of GSP.\n\n3. The method is functionally a Mixture-of-Experts (MoE) design that routes tokens to different heads. However, the paper fails to provide a direct comparison to more standard (and simpler) MoE routing mechanisms. It is unclear if the added complexity of the \"spectral residual\" calculation and \"spectral bias\" modulation offers any real advantage over a simple top-k router on the base token representations.\n\n4.  The paper introduces several new and important hyperparameters, chief among them the ratio of shared (S) to expert (E) filters. The paper defaults to a 1:1 ratio (8 shared, 8 expert) without any ablation or justification."}, "questions": {"value": "1. The experiments use 8 shared filters and 8 expert filters (S=8, E=8). What was the rationale for this 1:1 ratio? Could you provide any ablation studies on the effect of changing this ratio (e.g., S=4, E=12 or S=12, E=4) on performance?\n\n2. A core motivation is that Mamba2's heads are unstructured. Did you perform a spectral analysis on the heads of a baseline Mamba2 model? it is possible that Mamba2 already learns a diverse set of low and high-pass filters, even without an explicit mechanism. Such a comparison would provide a stronger baseline.\n\n3. Given that the GSP framework is \"implicit\", could you elaborate on why this GSP-inspired router is superior to a standard MoE router that simply learns to route tokens based on a linear projection of the input ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JL0w56j3pb", "forum": "w0XhHcXfKv", "replyto": "w0XhHcXfKv", "signatures": ["ICLR.cc/2026/Conference/Submission15463/Reviewer_kLjo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15463/Reviewer_kLjo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762242726937, "cdate": 1762242726937, "tmdate": 1762925751831, "mdate": 1762925751831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}