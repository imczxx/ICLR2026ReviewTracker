{"id": "9LzaFtKh0y", "number": 4070, "cdate": 1757595203332, "mdate": 1759898054641, "content": {"title": "Learning to Defer on Anonymously Annotated Data", "abstract": "Recent advancements in machine learning have prompted the development in human-machine cooperation to leverage the efficiency of machines and the reliability of human expertise. One such approach is *learning to defer* (L2D), where a model learns to selectively defer decision-making to humans based on their historical performance on labelled data. Traditional L2D methods require the same set of human experts in both training and deployment phase, so that the system can leverage their historical performance to allocate queries accordingly. This human-specific nature, however, renders inflexibility in dynamic real-world environments where expert availability can fluctuate due to leave, retirement, or the integration of new team members. To address this challenge, we propose leveraging anonymously-annotated datasets, which are commonly available in practice, to infer annotation patterns and cluster human annotators based on behavioural similarities. Building upon the clustering of human experts, we develop a variant L2D, known as L2D-Clusters, that defers queries to a cluster rather than a specific expert, with one expert from the cluster randomly selected to make the final decision. Empirical results show that our clustering aligns with known annotator behaviour and that L2D-Clusters performs comparably to expert-specific L2D, especially in onboarding scenarios with limited annotator-identified data.", "tldr": "", "keywords": ["learning to defer", "latent dirichlet allocation", "expectation - maximisation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de508141cf0f9a78ba999ba166e1cd15c64db2c4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes L2D-Clusters as a framework extending L2D to anonymously annotated data.  The algorithm does not need fixed expert identities; instead, it uses an LDA-based model to cluster annotators by behavioral similarity and defers decisions to expert groups."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper seems to solve the limitations of existing L2D methods by introducing clustering on an anonymously labeled dataset, thereby not needing annotator identities.\n\n2. The LDA modeling seems novel for me (though I am not familiar with L2D works).\n\n3. The empirical results look good."}, "weaknesses": {"value": "I am not an expert in L2D, but I feel that the model seems to be a little impractical. The model assumes that all experts in one cluster share the same labeling pattern $h(x, \\theta_z)$, which is a probability vector allocating fixed probabilities to different labels to $x$. Is this too simplified? This assumption seems strong, as real annotators can demonstrate individual variability or context-dependent noise."}, "questions": {"value": "How sensitive is your method to heterogeneity within one cluster?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "R1tHQKbr7O", "forum": "9LzaFtKh0y", "replyto": "9LzaFtKh0y", "signatures": ["ICLR.cc/2026/Conference/Submission4070/Reviewer_UcGT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4070/Reviewer_UcGT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760548570491, "cdate": 1760548570491, "tmdate": 1762917163566, "mdate": 1762917163566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the \"out-of-distribution problem\" for human experts, where the available annotators change between training and deployment. The proposed method addresses this by learning to defer tasks to clusters of behaviorally similar experts, identified from anonymously-annotated data, rather than to specific individuals."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem of \"out-of-distribution\" for human experts is interesting\n2. The proposed method is simple, intuitive, and effective"}, "weaknesses": {"value": "1. The paper does not provide a theoretical analysis to substantiate the method's effectiveness, such as performance guarantees or robustness analysis.\n2. The performance is critically dependent on the quality of the expert clustering, a dependency amplified by the stochastic nature of randomly selecting an expert for the final decision. The validation of this clustering is confined to a limited set of datasets, and its efficacy in diverse, real-world scenarios remains unverified.\n3. The empirical evaluation relies heavily on synthetic experts. The conclusions would be significantly more compelling with the inclusion of a real human study."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xbdzPtSMig", "forum": "9LzaFtKh0y", "replyto": "9LzaFtKh0y", "signatures": ["ICLR.cc/2026/Conference/Submission4070/Reviewer_J7mm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4070/Reviewer_J7mm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616761886, "cdate": 1761616761886, "tmdate": 1762917163334, "mdate": 1762917163334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tries to fix a realistic weakness in learning-to-defer systems, where a model decides whether to answer or pass a case to a human. Usual methods assume the same named experts appear during training and testing, which is unrealistic. Here, the authors learn groups of similar annotators from anonymous label counts using a simple topic-model approach. When a new person arrives, the system observes a few of their labels and assigns them to one or more groups. At prediction time, the model either answers by itself or defers to one of these groups and picks any available person within it. A simple control limits how many cases go to humans.\n\nThe novelty is that the method works without knowing annotator identities or needing ground-truth labels. It also introduces a realistic onboarding scenario with many experts, each labeling only a few examples, and with noise that depends on the input itself. Experiments on CIFAR-100, dopanim, and Chaoyang datasets show that this group-based approach beats both person-specific and population-based baselines when little data is available for each expert. Once every expert has plenty of labeled data, the standard person-specific systems become stronger. Hard clustering works best overall."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a relevant gap in L2D. The anonymous-data LDA modeling is appropriate for multinomial label counts and cleanly separates training of clusters from later per-expert assignment using a small identified set, which reduces the dependence on an available ground truth. The L2D-Clusters architecture is a natural hierarchical mixture of experts with a gating function over clusters and random expert selection within a cluster, to wihch a workload constraint is added.\nThe onboarding benchmark captures conditions with many experts and few labels per expert with instance-dependent noise; it shows compelling improvements over expert-specific and population-based baselines in the small-data regime.\nWriting is clear enough to follow the modeling and training details, and appendices provide derivations and the algorithm used in practice."}, "weaknesses": {"value": "The L2D-Clusters section assumes all experts annotate the same training samples to avoid latent-variable complications but, as far as I can see, it weakens the generality of the claim about handling missing annotations and, in my view, should be mentioned more clearly in the paper.\nRandomly picking an expert inside a cluster may introduce variance across the humans are subjected to---an alternative within-cluster selection rule could be investigated.\nExperientally, some ablation studies are missing to assess the isolated contribution of workload constraint, online-EM momentum, or hard vs soft assignments."}, "questions": {"value": "How sensitive are your results to the Dirichlet concentration prior for cluster mixtures? How sensitive are they to the choice of the parameter K? Can you report the performance variability (and, possibly, clustering stability/variaiblity) on an alpha-K grid?\n  \nCan the simplifying assumption that all experts annotate the same samples be relaxed?\n\nCould you comment on any class-imbalance issues that may arise with your method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ftXetXqw70", "forum": "9LzaFtKh0y", "replyto": "9LzaFtKh0y", "signatures": ["ICLR.cc/2026/Conference/Submission4070/Reviewer_fVfL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4070/Reviewer_fVfL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179622931, "cdate": 1762179622931, "tmdate": 1762917163146, "mdate": 1762917163146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}