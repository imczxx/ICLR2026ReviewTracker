{"id": "Wx4MW8aEYQ", "number": 16944, "cdate": 1758270479299, "mdate": 1763647359289, "content": {"title": "TestExplora: Can LLMs Write Tests to Find Potential Problems Existing in Repository?", "abstract": "As Large Language Models (LLMs) are increasingly applied to automate software development, their use for automatic test case generation has become a key area of research. However, existing benchmarks for evaluating LLMs fundamentally simplify the real-world testing challenge. They typically constrain the problem to either (1) reproducing known bugs at the repository level, or (2) generating tests for isolated code units, such as individual functions, detached from their broader project context. Both approaches fail to assess the crucial capability of LLMs for proactive, exploratory testing in projects defined by complex, cross-file dependencies.\nTo address this critical gap, we introduce TestExplora, the first systematic benchmark designed to evaluate the proactive defect discovery capabilities of LLMs at the repository level. Constructed from real-world pull requests, TestExplora challenges models to find bugs without any prior knowledge of bug manifestations. Our comprehensive evaluation, conducted in both black-box and white-box settings, reveals a stark capability gap. Even state-of-the-art models exhibit critically low success rates (e.g., GPT-5-mini: 17.56%, o3-min: 5.23%), and access to the full source code (white-box) yields only marginal improvement. Further Analysis reveals that existing models struggle mainly with assertion mismatches and misconfigured mocks. TestExplora thus establishes a principled foundation for advancing research towards the grand challenge of autonomous, repository-level defect discovery.", "tldr": "", "keywords": ["llm", "code", "testgeneragtion"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8484226d69385d5783ce798ed1ffc6f277ddd839.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces TestExplora, a new test case generation benchmark for proactive, exploratory testing at repository level. The authors construct the benchmark by taking a valid PR (after careful filtering and selection process) and extract 3 pieces of information: documentation about the intended functionality, entry point, and their dependencies. The overall task is to let the LLMs generate test cases for a piece of function given larger (repo-level) context without access to information pointing to bugs (e.g., issues, fixing commits).\n\nThe authors evaluate the LLMs with several metrics capturing different scopes, under two scenarios: white-box and black-box, being different in terms of whether the actual code is visible to LLMs or not. The paper's key finding is that even state-of-the-art models perform very poorly on this task, with a low success rate (e.g., 17.56% for GPT-5-mini). The analysis suggests models primarily fail due to assertion mismatches and misconfigured mocks, showing that there still exist a significant gap toward finding defects proactively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* A solid contribution of large amount of highly curated data instances & PRs\n* Good findings about the SOTA LLMs not being robust wrt finding the potential bugs without access to the actual errors. Proactively assessing the bug attracts interest in real-life cases where running the test suites are expensive. So I think it is a nice direction to pursue."}, "weaknesses": {"value": "* The novelty is limited. SWT-bench is a repository-level test generation benchmark, but this is given the bug report. It sounds to me that the proposed benchmark only differs conceptually from SWT-bench in that the input is intended behavior instead of the bug report. Though still nice to see the limitations of SOTA models, the claimed novelty is somewhat incremental.\n* Somewhat shallow analysis. Experimental settings are great, but the analysis doesn't fully dissect the results. For example, whitebox and blackbox testing show similar F2P scores. I understand both settings have similar error patterns, but what parts are different? If not so different, does that mean the models are simply not utilizing the provided context?\n* DocAgent provides better documentation – is it longer, more structured, comprehensive? It might be expected if Human-written versions tend to have only brief documentations. If we want to simulate a real-world setting, should we modulate the DocAgent output to mimic human-written documentations?\n\n—\n\nOther notes:\n\n* Sec 4.2 starts with a sentence repetition.\n* TestExplora-Lite is only briefly mentioned in Sec 4.3. It'd be nice to define it if it also appears in the main results table."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FMLGId0Xu0", "forum": "Wx4MW8aEYQ", "replyto": "Wx4MW8aEYQ", "signatures": ["ICLR.cc/2026/Conference/Submission16944/Reviewer_aTns"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16944/Reviewer_aTns"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16944/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862819634, "cdate": 1761862819634, "tmdate": 1762926964321, "mdate": 1762926964321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TestExplora is a new benchmark set to assess LLM’s abilities to proactively detect bugs through tests generated. The benchmark is composed of 2389 test generation tasks from 482 repositories. LLMs are provided test entry points along with their human and LLM generated documentation and prompted to generate tests. The performance of the LLMs are assessed using various metrics, notably with fail-to-pass rates which checks if generated tests exhibit a Fail-to-Pass transition with selected PRs. The best LLM in this task displays 17% success rate, demonstrating challenging aspects of this benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is well written and organized.\n- The work is well motivated. Proactive detection of bugs through LLM-based test generation would be of interest to the field.\n- The benchmark is a novel contribution motivating further advancements for coding performance of LLMs."}, "weaknesses": {"value": "- My understanding is that the benchmark is only composed of python examples, which might be limiting.\n- The evaluations are lacking error bars. As LLMs are not deterministic, I’d love to understand the variability in the results. Also lacking one of the strongest coding LLM from Claude line of models."}, "questions": {"value": "- Curious to hear if the authors considered repeating the experiments where they provide a hint to the LLM that there is a bug and would love to see if it would help LLMs to be more critical and observe how the results change.\n- In Table 3:\n   - What is the “Num” column?\n   - There are several cases where the results are lower in White Box cases as compared to Black Box cases (e.g. GPT4-o for EC and CFG). It felt counterintuitive to me, can you elaborate on this behavior?\n- In the main discussion as well as Figure 2, it is indicated that Black box case does not include code implementation. However looking at the prompts in the appendix, I only see dependencies as the delta since test entry functions are included for both. Could you confirm what the delta is between White and Black Box?\n- Typo in line 372: “TThese”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LRrf7PugCe", "forum": "Wx4MW8aEYQ", "replyto": "Wx4MW8aEYQ", "signatures": ["ICLR.cc/2026/Conference/Submission16944/Reviewer_qmdw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16944/Reviewer_qmdw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16944/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884363751, "cdate": 1761884363751, "tmdate": 1762926963803, "mdate": 1762926963803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **TestExplora**, a benchmark targeting *proactive* test generation for real-world repositories without giving models explicit bug cues. Tasks are derived from GitHub PRs (2,389 tasks, 482 repos), and the benchmark enforces fail-to-pass behavior: the generated test must fail on buggy code and pass after patching. The benchmark evaluates both black-box and white-box modes and reports that state-of-the-art LLMs perform poorly (typically <13% F2P), suggesting current models struggle at autonomous exploratory testing. The authors also introduce DocAgent to synthesize/clean documentation for models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The benchmark pipeline and fail-to-pass validation methodology are generally reasonable, with realistic environment setup. However:\n\n- No statistical significance or variance reporting\n- Potential leakage: PRs might be in model training data\n- DocAgent-generated docs may introduce *task supervision*, contaminating “exploratory” nature\n- Some pipeline heuristics lack ablation (entry-point selection, doc filtering)\n- Limited robustness checks (e.g., determinism, flaky tests, environment noise)"}, "weaknesses": {"value": "#### 1. Benchmark purity & leakage concerns\n- PRs and project history may be in model training sets\n- No explicit contamination filtering (hashing, repo disambiguation)\n- DocAgent can implicitly encode bug semantics → *post-hoc supervision*\n\n#### 2. “Exploratory testing” assumptions questionable\n- Providing synthetic docstrings is not realistic exploratory QA\n- Entry-point constraints shape model behavior artificially\n- Real exploratory testing = multi-turn reasoning + interaction + search\n  → model evaluated as *static predictor*, not explorer\n\n#### 3. Limited baselines\n- No comparison to property-based testing or symbolic tools (Hypothesis, Pynguin)\n- No LLM agent systems with tool use\n- No RL / planning / iterative search baselines\n\nBenchmark may favor static prompting over realistic agent loops.\n\n#### 4. Metrics interpretation unclear\n- F2P alone doesn't guarantee good test quality\n- Coverage ≠ fault detection power\n- No mutation-testing analysis\n- No difficulty stratification or variance reporting\n\n#### 5. Reproducibility risks\n- Python dependency resolution is brittle\n- Flakiness/oracle reliability unclear\n- Many heuristics for filtering and environment setup insufficiently justified"}, "questions": {"value": "1. How do you ensure PRs were not in model training data? Any contamination checks?\n2. How do you guarantee DocAgent never leaks patch semantics?\n3. Why constrain to function-level entry points? Why not broader exploration?\n4. Any evaluation with autonomous tool-using agents?\n5. Can you include mutation-testing metrics?\n6. How robust is pipeline to flaky tests / dependency conflicts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3d9PlDpnDw", "forum": "Wx4MW8aEYQ", "replyto": "Wx4MW8aEYQ", "signatures": ["ICLR.cc/2026/Conference/Submission16944/Reviewer_CycK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16944/Reviewer_CycK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16944/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905987473, "cdate": 1761905987473, "tmdate": 1762926963160, "mdate": 1762926963160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TestExplora, the first systematic benchmark designed to evaluate the proactive defect discovery capabilities of LLMs at the repository level. The evaluation shows that state-of-the-art models exhibit critically low success rates, and access to source code (white-box) yields only marginal improvement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**\nThe paper introduced the first large-scale benchmark from real-world GitHub PRs to evaluate the proactive defect discovery capabilities of LLMs at the repository level.\n\n**Quality**\nThe paper made extensive experiments on various LLMs and white-box setting with entry function and dependency implementation, showing quantitative results of with multiple metrics like fail-to-pass rate and change-focused coverage.\n\n**Clarity**\nThe paper is well-written with description of benchmark construction methods and evaluation metrics. The appendix provides detailed prompt.\n\n**Significance**\nThe paper proposed a large-scale benchmark which can help researchers in AI and SE fields to measure LLM's capability of bug-finding in real-world repositories."}, "weaknesses": {"value": "**Missing literature from SE field**\n\nEven though the paper defines the problem as repo-level proactive exploratory testing, the benchmark construction from GitHub PRs is essentially similar to the regression testing problem, which has been explored by following recent work from SE field. The key difference is that TestExplora shows LLM with the entry code but not the diff/patch so it's harder for LLM to infer the bug.\n\nThese two papers focus on bug reproduction:\n- Automated Generation of Issue-Reproducing Tests by Combining LLMs and Search-Based Testing (ASE '25)\n- Issue2Test: Generating Reproducing Test Cases from Issue Reports (ICSE '26)\n\nThis paper finds unintended bugs introduced by PR:\n- Testora: Using Natural Language Intent to Detect Behavioral Regressions (ICSE '26)\n\nThis paper feeds bug-introducing and bug-fixing commit for LLM to generate bug-triggering and bug-reproducing test input.\n- Can LLM Generate Regression Tests for Software Commits? (arXiv:2501.11086)\n\n**Lacking qualitative example about LLM inferring intended behavior**\n\nFigure 2 shows the task for LLM includes \"infer the intended behavior of the Test entry points' API from the documentation\", which is crucial for LLM to generate the test. I don't see results about how successful LLM is on this task. Figure 6 shows some failure analysis result about assertion mismatch, but the classification criteria is unclear. It would be better to have an example showing the full output of LLM, especially on how it infers the intended behavior in the first place."}, "questions": {"value": "1. How do you assess if LLM successfully infer the intended behavior of the Test entry points' API so it tries to generate test that exercises the intended functionality?\n2. The proactive exploratory setting is naturally difficult for LLM and human developers as the entry function itself may contain other distracting lines that may even contain other unintended bugs. Would it be better if you identify the problematic lines or the bug-introducing diff so LLM has a clear goal and context to test? In real-world CI/CD scenario it's also natural to test code change rather than the entry function."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3ePP6UEmpx", "forum": "Wx4MW8aEYQ", "replyto": "Wx4MW8aEYQ", "signatures": ["ICLR.cc/2026/Conference/Submission16944/Reviewer_f2XB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16944/Reviewer_f2XB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16944/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995590445, "cdate": 1761995590445, "tmdate": 1762926962673, "mdate": 1762926962673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}