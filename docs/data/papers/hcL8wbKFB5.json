{"id": "hcL8wbKFB5", "number": 8522, "cdate": 1758089266909, "mdate": 1759897778672, "content": {"title": "GradientHide: Federated Learning with Two-Stage Local Update for Defending Against Gradient Inversion Attacks", "abstract": "Federated learning enables collaborative training of neural networks across distributed clients coordinated by a central server. In each communication round, clients receive the current global model parameters and upload gradient updates computed on their private local data.  However, transmitting such updates poses significant privacy risks, as adversaries may exploit them to reconstruct sensitive training data via gradient inversion attacks. To address this challenge, we propose GradientHide, a novel defense framework that obfuscates private information contained in gradients. Specifically, we introduce an additional update step using public data before transmitting gradients to the server, thereby hiding privacy information embedded in gradients. To mitigate potential performance degradation from using public data, we leverage CLIP's zero-shot inference for semantic alignment, enabling effective use of public images without extra training. GradientHide is evaluated against representative gradient inversion attacks and compared with state-of-the-art defense approaches across three benchmark datasets, followed by a thorough analysis of its effectiveness. Our findings demonstrate that GradientHide offers substantial resistance to gradient inversion attacks, evidenced by lower PSNR scores and semantic distortion in reconstructions, while preserving competitive model performance.", "tldr": "GradientHide defends against gradient inversion in federated learning by adding a public data update step and using CLIP for label alignment, effectively protecting privacy while preserving model accuracy across benchmark datasets.", "keywords": ["Federated Learning", "Privacy Leakage", "Gradient Inversion"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3ad2a96c1dbd0a20d05aa0928b9b547e983f9b6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the privacy leakage issue in Federated Learning (FL) caused by gradient transmission where adversaries can reconstruct sensitive training data via Gradient Inversion Attacks (GIAs). It proposes a defense framework named GradientHide. The core design involves introducing an additional update step using public data at the client side before transmitting gradients to the server, thereby obfuscating private information embedded in the gradients. Meanwhile, it leverages CLIP's zero-shot inference to achieve semantic alignment between public and private data, avoiding model performance degradation induced by public data. The paper conducts evaluations on three benchmark datasets (CIFAR-10, ImageNet, FFHQ) against five typical GIAs (GGL, GIAS, GIFD, SPEAR, GI-NAS) and compares GradientHide with seven mainstream Gradient Inversion Defense (GID) methods. Experimental results show that GradientHide significantly enhances resistance to GIAs (reflected by lower PSNR of reconstructed images and more obvious semantic distortion) while maintaining stable model performance in scenarios with data heterogeneity, effectively balancing the trade-off between privacy protection and model utility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The \"two-stage local update\" mechanism is proposed, transforming the role of public data from \"auxiliary training\" to \"gradient privacy obfuscation\". This is completely different from the technical routes of InstaHide (mixing input images) and Ji et al. (combining private and public data to mitigate noise loss), achieving innovation in the way public data is utilized.\n2. The theoretical analysis is solid, deriving the information leakage boundary through the relationship between entropy and mutual information. Ablation experiments cover key design components (label alignment strategy, public dataset ratio $\\alpha$, perturbation scaling factor $\\lambda_n$), ruling out the possibility that \"performance improvement stems from a single factor\".\n3. A \"semantically shifted reconstruction\" strategy is designed, making reconstructed images visually coherent but semantically deviated from the original data (e.g., changes in gender/age of reconstructed faces in the FFHQ scenario). This can mislead adversaries into overestimating attack success rates, providing a new \"active deception\" perspective for privacy defense.\n4. It verifies the value of pre-trained models like CLIP in FL privacy defense, enabling cross-dataset semantic alignment without customized training, which reduces the computational cost of technical implementation."}, "weaknesses": {"value": "1. The paper only mentions that label alignment for ImageNet takes 40 minutes and each communication round requires an additional 10 seconds, but it doesn't compare the computational costs with other GIDs (e.g., the time consumption of subspace projection in CENSOR and noise permutation in ANP). This makes it impossible to prove the comprehensive advantages of GradientHide in the three dimensions of \"privacy/performance/efficiency\". Additionally, the adaptability of client devices (e.g., edge devices) in terms of computing power is not analyzed, limiting the judgment of its deployment scenarios.\n2. The paper does not fully compare \"public data perturbation\" with related work on \"gradient anonymization\" (e.g., Tan et al.'s \"information-theoretic defense framework\" in 2024 [1]), failing to explain the incremental contribution of GradientHide in mutual information optimization. Additionally, although the \"semantically shifted reconstruction\" design is innovative, it does not quantify the \"adversary deception degree\" (e.g., through user surveys or measuring the misjudgment rate of attack success), which weakens the proof of the practical value of this strategy.\n[1] Defending against data reconstruction attacks in federated learning: An information theory approach, USENIX Security 24."}, "questions": {"value": "1. The paper does not specify the preprocessing details of the public dataset UTKFace and FFHQ. Were image resolutions unified and face detection/alignment steps applied? If preprocessing methods differ, will this affect the accuracy of label alignment and the semantic shift effect of reconstructed images?\n2. For low-quality public data (e.g., CIFAR10 with 20% label errors used as the public dataset), will the defense performance of GradientHide degrade? If it degrades, is there a corresponding adaptive adjustment strategy (e.g., dynamically adjusting the public data sampling ratio $\\alpha$)?\n3. The theoretical analysis assumes that \"public data and private data are conditionally independent\". When there is overlap between private and public data (e.g., overlap ratios of 10% and 30%), will the information leakage boundary quantified by mutual information change? Can relevant experiments or theoretical derivations be supplemented?\n4. The \"semantically shifted reconstruction\" strategy can mislead adversaries. Have experiments been conducted to quantify the \"adversary's misjudgment rate of attack success\" (e.g., asking adversaries to judge whether reconstructed images are from the original private dataset and counting the misjudgment ratio)? If yes, can relevant results be supplemented; if not, are there plans to conduct such experiments in the future?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AVheZUQ10l", "forum": "hcL8wbKFB5", "replyto": "hcL8wbKFB5", "signatures": ["ICLR.cc/2026/Conference/Submission8522/Reviewer_rvCH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8522/Reviewer_rvCH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761018877031, "cdate": 1761018877031, "tmdate": 1762920384763, "mdate": 1762920384763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GradientHide, a gradient inversion defense method that obfuscates private information contained in gradients. GradientHide introduce an additional update step using public data before transmitting gradients to the server. It also leverages CLIP models for label alignment between the public and private datasets. Experiments show that this method offers some resistance to gradient inversion attacks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper focuses on an important topic, where the privacy guarantee of federated learning may be compromised by gradient inversion attacks, and discusses potential solutions.\n- The writing of this paper is clear. The authors also try to provide theoretical results for better persuasiveness.\n- Comprehensive and detailed experimental comparisons with the previous gradient inversion attacks and defenses."}, "weaknesses": {"value": "- The novelty is somehow limited. It is not surprising that additionally introducing public data unrelated to the private data during training can impair the attacker’s reconstruction performance. The authors simply add additional public datasets to the original local update with minimal tailored designs.\n- The current selection of public data for each local update is limited to random sampling. Exploring more deliberate selection criteria presents a promising direction for boosting both defensive performance and model accuracy.\n- In line 237, the authors claim that $\\hat{\\mathcal{H}}_i$ is independent of $\\mathcal{D}$. However, the distributional gap between the public and private datasets does not seem to be significant in the experiments of this paper (i.e., CIFAR-10 vs. ImageNet, FFHQ vs. UTKFace). For instance, both FFHQ and UTKFace are facial datasets and share some distributional similarities. When the distribution of the public dataset is given, the uncertainty regarding the distribution of the private dataset decreases. So they cannot be simply identified as “independent”. With this problematic assumption, the theoretical results of this paper become questionable.\n- The label alignment technique is directly built based on the existing CLIP models, which are commonly adopted for vision-language alignment. This idea is straightforward and trival, not offering enough innovative insights.\n- Moreover, the authors claim that they average the scores to leverage the diversity across prompt templates in line 174. However, there is no experimental evidence supporting this claim. Using the prompt with the highest similarity (instead of averaging them), or simply using “a photo of {class}” may also be beneficial. It is suggested to provide the corresponding experiments on this usage.\n- There are some minor typos. In line 789, there should be a quotation mark before `a photo of a {class}`. In line 790, there should be a quotation mark before `a cropped`. In line 791, the `class` should be revised as `{class}`. Please address them in the revised manuscript."}, "questions": {"value": "- There is the lack of experimental analysis on computational costs. This is very crucial, especially given that this method introduces one additional update for each original local update. How many computational costs are additionally introduced by this additional update? Could the authors provide the comparative results with previous defenses regarding the computational costs?\n- What is the impact of $\\alpha$ in line 200? When it changes, how do the reconstruction performance, model accuracy, and computational costs accordingly change?\n- The distributional gap between the public and private datasets does not seem to be significant in the experiments of this paper (i.e., CIFAR-10 vs. ImageNet, FFHQ vs. UTKFace), which largely accounts for the model accuracy maintenance of this method. Is it realistic to assume that a defender can actually obtain a large amount of public data whose distribution is similar to the private data?\n- What about increasing the gap between the public and private datasets? In GIFD, OOD datasets with different styles (i.e., Art Painting, Cartoon, Photo) are also considered. As for this paper, I wonder whether the model accuracy performance can maintain when these OOD datasets are adopted for the public (or private) dataset while ImageNet serving as the another."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wARaugdKlG", "forum": "hcL8wbKFB5", "replyto": "hcL8wbKFB5", "signatures": ["ICLR.cc/2026/Conference/Submission8522/Reviewer_fRep"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8522/Reviewer_fRep"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806560596, "cdate": 1761806560596, "tmdate": 1762920384171, "mdate": 1762920384171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Recognizing that gradients in federated learning are highly vulnerable to inversion attacks and that standard defenses inevitably degrade model utility, this paper proposes GradientHide, which introduces a two-stage local update strategy that utilizes public data to obscure the gradients."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper combines the idea of mixing up public data and private data to counter the gradient inversion attacks. Utilizing CLIP for label alignment is new."}, "weaknesses": {"value": "1. The experiments are conducted using only ResNet-18, and it is not clear if the proposed method works for other networks. Moreover, for the evaluation of privacy protection, it is not clear if multiple local steps are enabled. \n2. The comparison with existing methods does not seem fair from two perspectives. First, the proposed method essentially requires additional SGD steps, which means that measuring the model performance in terms of communication rounds is not fair. In addition, adding new training data may also improve the test accuracy. Second, additional local updates naturally make it more difficult for gradient inversion attacks. It is not clear if the improved privacy is due to the proposed method or simply the additional SGD step. \n3. In Tables 2 and 3, there are some minor mistakes in the results of LPIPS. The results highlighted in bold are not necessarily the best."}, "questions": {"value": "1. How many local steps are conducted in the evaluation of privacy protection?\n2. Is the comparison fair? Could the authors compare with existing methods that incorporate additional public data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gGU10yAoW3", "forum": "hcL8wbKFB5", "replyto": "hcL8wbKFB5", "signatures": ["ICLR.cc/2026/Conference/Submission8522/Reviewer_uZ4D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8522/Reviewer_uZ4D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908976674, "cdate": 1761908976674, "tmdate": 1762920383826, "mdate": 1762920383826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the privacy vulnerability in federated learning (FL) where shared gradient updates can be exploited by gradient inversion attacks (GIAs) to reconstruct clients’ private data. To address this, the authors propose GradientHide, a defense framework that introduces a two-stage local update: after training on private data, each client performs an additional update using public data to obfuscate sensitive gradient information before transmission. The public data is semantically aligned with private data through CLIP’s zero-shot label alignment, ensuring effective masking without harming model utility. Theoretical analysis shows that this procedure reduces mutual information between gradients and private data, and experiments on CIFAR-10, ImageNet, and FFHQ demonstrate that GradientHide significantly weakens inversion attacks—yielding blurred or semantically shifted reconstructions—while maintaining strong model accuracy even under heterogeneous data distributions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The work is based on a clearly defined threat model, where the server is honest but curious to explore the client's gradients.\n* The proposed method is well motivated by information-theoretic analysis.\n* The evaluation on data protection is comprehensive, using 3 metrics and 3 deatasets."}, "weaknesses": {"value": "* The major motivation for the work is that prior arts largely trade model performance for privacy defense (Line 53). However, the claim is problematic. According to Fig 3, most of prior methods have even better performance than the proposed methods when data distribution is not too heterogeneous (beta < 0.5). Even when beta >0.5 (more heterogeous), the peformance degradation is fair compared to the proposed method. \n  - Note that ANP has the best protection effects against GIAS, GIFD, SPEAR, and 2nd best on GI-NAS on CIFAR-10 (see Table 1 LPIPS). ANP has almost the same performance in Fig 3. Thus, it is not fair to justify that the proposed method is better than ANP.\n* Some of the best values of ANP in Table 1 LPIPS are not highlighted as bold. This essentially misleads readers into thinking ANP is not as good as the proposed method.\n* As achieving benign performance is a major claim of the work, the evaluation is quite insufficient. Only one dataset, CIFAR10, was evaluated. \n* Not clear if the FL and the attack follows the same learning setup. If not, is it meaningful to discuss the trade-off between model performance and privacy protection?"}, "questions": {"value": "* How good is the benign performance of the proposed method on ImageNet and FFHQ?\n* Why not compare to InstaHide, which shares similar principle as the proposed method?\n* What is the batch size in Section 3.3? Do Sections 3.3 and 3.1.1 follow the same federated learning setup? Specifically, in FL training, are models trained with a batch size of 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W8E10YvxC8", "forum": "hcL8wbKFB5", "replyto": "hcL8wbKFB5", "signatures": ["ICLR.cc/2026/Conference/Submission8522/Reviewer_TL9A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8522/Reviewer_TL9A"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975503724, "cdate": 1761975503724, "tmdate": 1762920383404, "mdate": 1762920383404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}