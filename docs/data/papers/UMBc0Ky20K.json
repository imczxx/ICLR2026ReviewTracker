{"id": "UMBc0Ky20K", "number": 2362, "cdate": 1757063883122, "mdate": 1759898153432, "content": {"title": "Mitigating Noise Shift in Denoising Generative Models with Noise Awareness Guidance", "abstract": "Existing denoising generative models rely on solving discretized reverse-time SDEs or ODEs. In this paper, we identify a long-overlooked yet pervasive issue in this family of models: a misalignment between the pre-defined noise level and the actual noise level encoded in intermediate states during sampling. We refer to this misalignment as noise shift. Through empirical analysis, we demonstrate that noise shift is widespread in modern diffusion models and exhibits a systematic bias, leading to sub-optimal generation due to both out-of-distribution generalization and inaccurate denoising updates. To address this problem, we propose Noise Awareness Guidance (NAG), a simple yet effective correction method that explicitly steers sampling trajectories to remain consistent with the pre-defined noise schedule. We further introduce a classifier-free variant of NAG, which jointly trains a noise-conditional and a noise-unconditional model via noise-condition dropout, thereby eliminating the need for external classifiers. Extensive experiments, including ImageNet generation and various supervised fine-tuning tasks, show that NAG consistently mitigates noise shift and substantially improves the generation quality of mainstream diffusion models. Code will be released upon acceptance.", "tldr": "We identify a noise shift problem in diffusion models, where intermediate states deviate from the pre-defined noise schedule, and propose Noise Awareness Guidance (NAG) to correct it, significantly improving generation quality.", "keywords": ["diffusion models", "generative models", "training-inference misalignment", "noise awareness", "guidance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b11ac08ef9d999effa1810689a290bc72fe22a9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the issue of  noise shift, a discrepancy in denoising generative models where the actual noise level encoded in an intermediate state deviates systematically from the pre-defined noise schedule.  The authors provide empirical evidence for this shift using an external noise estimator and argue that it leads to sub-optimal generation. To mitigate this, they propose Noise Awareness Guidance (NAG), a training-based correction method that directs the sampling trajectory to remain consistent with the intended noise level.   The method is evaluated on DiT and SiT models for the ImageNet-256 dataset and various supervised fine-tuning tasks, demonstrating competitive improvements in FID."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper discusses and provides some empirical evidence for the Noise Shift.\n\n2.  The approach is based on using the noise level $t$ as a conditioning signal for guidance, drawing an analogy to classifier guidance.\n\n3.  Empirical results indicate that NAG can complement existing guidance (e.g., CFG) and shows potential for improving FID scores on DiT/SiT models."}, "weaknesses": {"value": "1. The paper lacks a thorough comparison with recent methods (e.g., [1]).  The authors incurred the cost of training a specialized component (NAG) and only validated it using basic samplers (DDPM and Euler-Maruyama), but failed to validate its benefit on advanced solvers. \n\n2. Experimental results suggest that the classifier-free NAG variant improves noise estimator accuracy, but it doubles the computational cost by requiring two forward passes per sampling step. Additionally, while the manuscript briefly covers the noise estimator’s training, it lacks a thorough analysis of its robustness, especially for out-of-distribution samples. \n\n3.  *Writing Quality*:  The manuscript requires significant proofreading. For instance,  in the main text, numerous typos include: \n     -  \"*pespetive*\" (Line 475) should be \"*perspective*\",\n\n     -  \"*apporach*\" (Line 480) should be \"*approach*\",\n\n     -   \"*varients*\"  (Line 480)  should be \"*variants*\",\n\n     -  \"*posible*\" (Line 483) should be \"*possible*\",\n\n     -  \"*nosie*\"  (Line 476) should be \"*noise*\",\n\n     -  \"*We anaylsis*\"  should be \"*We analyze*\", ....\n\n\n[1]. Abuduweili, A., et al., Enhancing Sample Generation of Diffusion Models using Noise Level Correction,  TMLR 2025.\n\n[2]. Ning, M., et al.,  Elucidating the Exposure Bias in Diffusion Models, ICLR 2024. \n\n[3]. Lin, S., et al.,  Common diffusion noise schedules and sample steps are flawed, WACV 2024 .\n\n[4]. Tang, Z.,  et al., Inference-Time Alignment of Diffusion Models with Direct Noise Optimization, ICML 2025."}, "questions": {"value": "1. When the prior noise schedule changes (e.g., from uniform to logSNR), does the model need to be retrained to adapt to the new sampling conditions?\n\n2. Is addressing noise shift alone sufficient? How does the method distinguish between noise shift caused by the inherent properties of the sampling process and noise shift arising from external factors, such as approximations, numerical discretization, or model-specific biases?\n\n3. I'm curious whether NAG could help with the numerical instability issue that arises from the schedules in diffusion models, especially during the sampling process as $\\sigma_t$ approaches 0?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lABI3Ajkou", "forum": "UMBc0Ky20K", "replyto": "UMBc0Ky20K", "signatures": ["ICLR.cc/2026/Conference/Submission2362/Reviewer_Uw2P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2362/Reviewer_Uw2P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760808174344, "cdate": 1760808174344, "tmdate": 1762916208554, "mdate": 1762916208554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a misalignment between the noise levels of the intermediate sampling states and those of the ground-truth data used during training. The authors demonstrate that this issue is prevalent in off-the-shelf diffusion models, highlighting a new direction for improving sampling performance. To address this misalignment, they propose NAG, a guidance technique analogous to classifier-free guidance, designed to correct the sampling trajectory. Empirical results validate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is generally well-written and easy to follow. \n2. Improving the sampling quality by mitigating the misalignment of the noise level between training and sampling appears new to me. \n3. The proposed method is simple to implement and can be easily applied to an off-the-shelf pre-trained model. \n4. According to the provided empirical results, the proposed method is very effective."}, "weaknesses": {"value": "1. The paper lacks an ablation study on the choice of $w_\\text{nag}$, which makes it difficult for readers to gain a comprehensive understanding of the proposed method.\n2. It is known that classifier-free guidance (CFG) trades sample diversity for sample quality. Since the paper claims that the proposed method mitigates noise-level misalignment and provides an effect \"orthogonal\" to CFG, the results would be more convincing if an ablation study were included to analyze how $w_\\text{nag}$ influences sample diversity.\n3. (Minor) The work is mainly empirical and lacks theoretical support."}, "questions": {"value": "Please refer to the weaknesses box."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6wolAgGH4x", "forum": "UMBc0Ky20K", "replyto": "UMBc0Ky20K", "signatures": ["ICLR.cc/2026/Conference/Submission2362/Reviewer_zvzx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2362/Reviewer_zvzx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831547864, "cdate": 1761831547864, "tmdate": 1762916208423, "mdate": 1762916208423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the noise shift problem in diffusion models: sampling intermediate states has a mismatch between the pre-defined noise schedule and the actual noise levels. The authors propose Noise Awareness Guidance (NAG) to guide back to correct noise. Through empirical results on ImageNet generation and supervised fine-tuning tasks, the paper demonstrates that NAG mitigates noise shift and improves generative sample quality across mainstream diffusion model architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper identifies the noise shift with strong empirical evidence (fig 1) , indicating that the actual posterior noise levels during sampling are systematically biased relative to the intended schedule.\n- The proposed NAG is simple and effective for diffusion and flow-matching models. And the compatibility of NAG with other guidance mechanisms (e.g., CFG and DoG) is empirically supported and presented in Table 2 and Figs. 4 and 5.\n- Clear writing, good ablations, and reproducible (with settings in Appendices)"}, "weaknesses": {"value": "The paper omits discussion against several closely related studies addressing noise misalignment and noise correction in diffusion models, for instance, \"Enhancing Sample Generation of Diffusion Models using Noise Level Correction\", https://arxiv.org/abs/2412.05488"}, "questions": {"value": "What is the computational overhead (parameters, runtime, memory) of adopting NAG in practical sampling/fine-tuning settings? How does this compare to other recalibration techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AaLPVkDMpT", "forum": "UMBc0Ky20K", "replyto": "UMBc0Ky20K", "signatures": ["ICLR.cc/2026/Conference/Submission2362/Reviewer_wzAN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2362/Reviewer_wzAN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897272455, "cdate": 1761897272455, "tmdate": 1762916208196, "mdate": 1762916208196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to solve a train-inference mismatch in diffusion models. Specifically, during the sampling procedure, the actual noise encoded in intermediate states drifts to larger noise levels than the intended schedule, which harms generation quality. To address this, the authors propose Noise Aware Guidance (NAG), which adds a guidance term based on the gradient of the posterior noise level to pull trajectories back toward the correct noise manifold; they provide both a classifier-based version using an external noise-level estimator and a classifier-free variant implemented by mixing conditional and unconditional scores via noise-condition dropout."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-written and easy to follow.** The paper clearly motivates the noise-shift train–inference mismatch and steadily builds evidence for it.\n- **Principled method.**: NAG is logically derived (using ($\\nabla_x \\log p(t\\mid x)$) to pull states back to the intended noise level), simple to plug in, and compatible with diffusion/flow samplers and with CFG/DoG.\n- **Consistent gains.** Across DiT/SiT backbones and multiple datasets, NAG improves FID and reduces the measured noise shift, with ablations that isolate its contribution and clarify when it helps (notably at higher SNR)."}, "weaknesses": {"value": "- My major concern is that there are some papers targeting the distribution mismatch between training and inference. In general, it is called exposure bias, and this is well explored in [1]. To summarize that paper, they derive how the variance of the backward diffusion model's process could deviate from the forward diffusion process, and give a solution scaling the estimated score to match the variance. Comparing this submission to [1], the flow is extremely similar: 1) finding the distribution mismatch between training and inference, and 2) solving this mismatch with NAG rather than using the score scaling method in [1]. Considering this similarity, the authors' contribution is that finding and addressing the distribution mismatch between training and inference no longer holds. Furthermore, completely ignoring these works makes me suspicious that the authors investigated related literature well. I recommend comparing [1] and the following papers about exposure bias, and adding these works to the related work section.\n\n- The method should be evaluated across various model sizes. And, I am also curious whether the distribution mismatch trend could be changed according to model size.\n\n\nReference \n\n[1] ELUCIDATING THE EXPOSURE BIAS IN DIFFUSION MODELS, ICLR 2024."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vIBXJ4pIth", "forum": "UMBc0Ky20K", "replyto": "UMBc0Ky20K", "signatures": ["ICLR.cc/2026/Conference/Submission2362/Reviewer_JnRR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2362/Reviewer_JnRR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946256656, "cdate": 1761946256656, "tmdate": 1762916207632, "mdate": 1762916207632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}