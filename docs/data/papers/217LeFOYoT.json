{"id": "217LeFOYoT", "number": 22406, "cdate": 1758330635450, "mdate": 1759896867873, "content": {"title": "DeepResearchGuard: Deep Research with Open Domain Evaluation and Multi-Stage Guardrails for Safety", "abstract": "Current deep research frameworks lack adequate evaluation procedures and stage-specific safeguards. Prior work primarily treats evaluation as question-answering accuracy. It overlooks report quality, especially credibility, coherence, breadth, depth, and safety, which allows hazardous or malicious sources to be integrated into the final report. To address these issues, we introduce DeepResearchGuard, a four-stage safeguard for input, plan, research, and output, integrated with open-domain evaluation of references and reports. We assess defense success rates, refusal rates, F1, FNR, FPR, and the five report dimensions across diverse LLMs, including gpt-4o, gemini-2.5-flash, DeepSeek-v3, and o4-mini. The average defense successful rate increased by 18.16\\%, and the over refusal rate decreased by 6\\%. The input guard contributes the most significant early increment by filtering out obvious risks, the plan and research guards improve citation discipline and source credibility, and the output guard strengthens structure, attribution, and risk disclosure. Upon examining the sensitivity of the guard model to performance, we identify a trade-off between safety and performance; specifically, gpt-5-mini, which offers enhanced security, yields a less in-depth report. In contrast, gpt-4o, with more fundamental settings, results in a higher risk but greater depth of the report. For queries that may present potential risks, advanced models could ignore more meaningful resources due to heightened security checks. Through the experiment, we concluded that DeepResearchGuard conduct open-domain evaluation and stage-aware defenses that block harmful propagation and systematically increase report quality without over-refusal. The code is available at \\url{https://anonymous.4open.science/r/DeepResearchGuard-6A75/}.", "tldr": "", "keywords": ["Agent", "Large Language Model", "Guardrail", "DeepResearch", "Open-domainEvaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/09a5ab6fd8f12f6332d67e8046e5970eb8f34da7.pdf", "supplementary_material": "/attachment/2cc9beb8debc729ee4f2480d410357de984e3856.zip"}, "replies": [{"content": {"summary": {"value": "The paper aims to address the lack of holistic evaluation and safety control in deep research agents, which typically rely only on QA accuracy and overlook report-level quality factors like credibility, coherence, and safety. It introduces DeepResearchGuard, a structured framework that divides the research workflow into four guarded stages—Input, Plan, Research, and Output—each equipped with memory retrieval, planning, evaluation, and human escalation mechanisms. The system jointly assesses both source and report quality under open-domain conditions. Experiments show that DeepResearchGuard significantly improves defense success while reducing over-refusal rates, with clear stage-wise contributions and an observable safety–depth trade-off across different guard models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "It is good to see how the paper introduces a clear taxonomy of harmful content in open-domain deep research, showing how different types of risks can propagate across stages from Input to Plan, Reference, and Output. It also reframes safety control from a single blanket refusal into a set of fine-grained, auditable interventions — stopping harmful input early, improving citation discipline and source reliability during planning and research, and reinforcing structure, attribution, and risk disclosure at the output stage. Together, these ideas offer a well-structured and practical approach to improving overall report quality and trustworthiness."}, "weaknesses": {"value": "1. While DeepResearchGuard improves DSR and maintains or lowers ORR, the paper doesn’t discuss how this impacts overall task completion or utility. Does stronger harmful-content blocking come at the cost of reduced usefulness or coverage?\n\n2. The evaluation of source transparency and traceability seems limited to helpfulness, authority, and timeliness. It would be useful to know whether the authors assessed model-level selection bias, since judgments of evidence helpfulness or relevance inherently depend on model capability. The paper also doesn’t clearly define how “helpfulness” is measured or what its scope is.\n\n3. It’s unclear what proportion of cases require human intervention under this framework. A quantitative analysis of escalation rates and their impact on efficiency or consistency would make the system’s practicality much clearer.\n\n4. The paper doesn’t provide detailed statistics on sample difficulty, open-domain coverage, or category distribution — only mentioning targeted manual additions for jailbreak and injection cases. More transparency about dataset composition would help assess the generalizability of the evaluation."}, "questions": {"value": "1. It’s not clear how accurate the Output Guard agent’s scoring is across the five evaluation dimensions, or how it compares with a human baseline. It would be valuable to know where the largest discrepancies occur and which dimensions are most challenging. Also, the paper doesn’t specify how the weighted sum for the overall score is determined — are the weights equal or task-dependent?\n\n2. The confidence-based threshold for escalating to human intervention seems to be chosen heuristically. More detailed experiments or ablations explaining how this threshold was set would help validate its reliability.\n\n3. The role of the memory component is somewhat underexplained. If the model’s capability is limited, continuously storing and referencing processed cases might risk amplifying past errors. It would be useful to analyze whether memory helps stability or introduces error accumulation over time.\n\n4. The paper doesn’t report the distribution of refusal types — for instance, which valuable sources were mistakenly blocked — nor does it break down the associated costs in terms of tokens or latency. These details would give a clearer picture of the system’s trade-offs and efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tj5XBwjqkN", "forum": "217LeFOYoT", "replyto": "217LeFOYoT", "signatures": ["ICLR.cc/2026/Conference/Submission22406/Reviewer_VgF5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22406/Reviewer_VgF5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761488029033, "cdate": 1761488029033, "tmdate": 1762942206306, "mdate": 1762942206306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DeepResearchGuard (DRG), a framework for safe and open-\ndomain deep research using multi-stage guardrails, combining reasoning-enhanced\nagents with layered safety evaluation. The authors propose a pipeline that integrates:\n\n1. Open-domain research tasks (web-enabled reasoning, synthesis, and\nreporting),\n\n2. Multi-stage guardrails for safety assurance (input, intermediate, and output\nfilters),\n\n3. An open benchmark — DRSAFEBENCH — for evaluating model safety,\nfactuality, and reasoning depth.\n\nThe system design is centered around modular “Guard” components:\n(a) Input Guard screens unsafe or policy-violating queries.\n(b) Process Guard monitors the agent’s intermediate reasoning chain for unsafe\nactions or hallucinations.\n(c) Output Guard validates final answers for factual correctness and safety\nalignment.\n\nThe evaluation compares DeepResearchGuard against strong baselines (GPT-4,\nGemini, Claude, etc.) using both safety metrics (toxicity, factuality, alignment) and\nreasoning metrics (depth, coverage, correctness). DRG reportedly achieves\nsubstantial gains in factuality and reasoning safety while maintaining competitive\nresearch quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(A) The paper tackles a timely and high-impact problem: the safe deployment of\nautonomous research agents.\n\n(B) The multi-stage guard concept provides a modular lens on safety assurance,\nenabling potential composability and interoperability.\n\n(C) The authors make a commendable effort to release a benchmark\n(DRSAFEBENCH) and system design intended to encourage openness and\nreplication."}, "weaknesses": {"value": "1. “Reasoning depth” and “safety compliance” are reported as scalar improvements\nwithout statistical context. No details are given about sample size, variance, or\n\ninter-rater reliability. Hence, claims such as “+15% factuality” or “+20% safety\ncompliance” are not statistically grounded.\n\n2. The paper lacks a precise mathematical definition of its evaluation\nmetrics—particularly “reasoning depth.” While qualitative examples suggest\nmulti-hop reasoning assessment, no formal operator D(f) or depth measure is\ndefined, leaving interpretability gaps in quantitative claims. The safety scoring\nfunction ϕi(x) is also treated as a black box, often described as “a model-based\njudge,” without calibration analysis or ROC curves to support threshold stability.\nAs such, while empirical results are strong, their robustness under distributional\nshift remains uncertain.\n\n3. Limited novelty in the guardrail mechanisms themselves.\nThe modular “multi-guard” setup echoes existing layered safety frameworks\n(e.g., Constitutional AI + LLM oversight). The originality primarily lies in system\nintegration, not in new algorithmic or theoretical insights.\n\n4. No Analysis of Guard Composition. The system assumes that multi-stage filtering\nimproves safety monotonically, yet provides no proof or empirical evidence of risk\nsubadditivity. The absence of such analysis limits the theoretical depth of the\ncontribution\n\n5.\tInadequate transparency of model configuration.\n Details on base model sizes, prompt templates, or RLHF usage are not presented, making it difficult to attribute performance improvements to the guardrails rather than model capacity.\n6.\tThe benchmark DRSAFEBENCH, while valuable, is authored and evaluated by the same group, raising potential biases in task design and model ranking. Without third-party replication or human inter-annotator statistics (e.g., Cohen’s κ), claims of general safety superiority remain somewhat self-referential. Furthermore, the paper does not provide asymptotic complexity or runtime analysis—yet multi-stage filtering clearly incurs nontrivial computational cost. The paper would benefit from an empirical latency curve L(n)∝n×ci  quantifying guard overhead per stage.\n\n7.\tMulti-stage guardrails increase inference cost superlinearly (approximately O(n⋅ci), yet no performance or throughput metrics are provided. This omission makes real-world feasibility unclear. \nEach guard Gi is implemented via an LLM-based evaluator. If these evaluators share the same biases as the core model, then the composite guard G=G3∘G2∘G1\nbecomes a biased projection of a single safety prior, invalidating the notion of layered protection.\n\n8. While safety is central, the ethical discussion (e.g., accountability of multi-agent decisions, risk of subtle biases) remains generic and lacks deeper engagement with responsible AI literature."}, "questions": {"value": "1.\tHow are guardrails coordinated when their judgments conflict (e.g., process guard flags an unsafe inference, output guard clears it)?\n\n2.\tWhat quantitative evidence supports that intermediate guard checks prevent unsafe outcomes rather than just delay them?\n\n3.\tIs DRSAFEBENCH publicly released with example prompts, or only aggregate statistics?\n\n4.\tCould the modular guard setup degrade reasoning quality by over-regularization?\n\n5.\tHow is factuality measured — automatic retrieval verification, or human grading?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "Bias discussion is a necessary component in the study."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qBPFp8yT8w", "forum": "217LeFOYoT", "replyto": "217LeFOYoT", "signatures": ["ICLR.cc/2026/Conference/Submission22406/Reviewer_zyxB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22406/Reviewer_zyxB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888821962, "cdate": 1761888821962, "tmdate": 1762942206086, "mdate": 1762942206086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework (DEEPRESEARCHGUARD) to enhance the safety and quality of deep research agents. Deep research agents are defined as LLM based agents engaged in completing complex, multi-step research tasks. The paper argues that existing evaluations ignore risks that arise and propagate through the intermediate stages of the process, and only focus on the final output accuracy as a metric.\nThe proposed method has 4 stages and monitors the input, plan, research, and output stages of the agentic workflow. Each stage has a dedicated guard agent that classifies content for quality and safety,  revises or rejects problematic content, and can escalate to a human reviewer.\nA new evaluation protocol that assesses the  final report on 5 qualitative dimensions and a safety bench mark is proposed to test these capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The framing of risk as not just an input/output problem, but rather a process level issue where errors can cascade, is interesting. \n+ The inclusion of stage-specific taxonomies, is useful\n+ Shifting away from a Q&A metric to a more multi-dimensional look at report quality works better at providing a comprehensive metric. The new metrics D@1, D@all are well motivated"}, "weaknesses": {"value": "- The DRSAFEBENCH  is partially constructed using and LLM to synthesize adversarial examples. Therefore, there is a risk that the benchmark may be evaluating the blindspots specific to the LLM rather than reflecting human devised adversarial attacks. The paper does not provide enough information on how it validates that these synthetic examples are representative of real-world threats. There is also no discussion of how the potential for systemic bias is being evaluated.\n- The high increase in run time complexity is mentioned but not fully discussed.\n - the architecture is highly complex, and contains multiple agents, taxonomies etc. This could make it pretty brittle. It would be good to have seen a discussion of why this much complexity is necessary and how what the complexities of debugging and maintaining such a system would work in practice.\n- It would have been good to have a formal definition of \"deep research\". The paper uses examples to define it broadly. This makes it difficult to understand the scope of this problem and therefore, to be able to judge how generalizable the approach is."}, "questions": {"value": "1. Can you provide more information on how you ensure that the LLM generated examples are a reflection of real world, human generated adversarial samples?\n2. Can you comment on the generalizability of your approach? Do you anticipate changes in your modules depending on the use case domain. It is conceivable that an agentic system for a fintech application could be completely different from that for one in the healthcare domain. \n3. Can you discuss the need for the current level of complexity of the system? Is there a way to make sure that the design is at the minimum necessary components?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NaDoP9W03h", "forum": "217LeFOYoT", "replyto": "217LeFOYoT", "signatures": ["ICLR.cc/2026/Conference/Submission22406/Reviewer_FuFk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22406/Reviewer_FuFk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960090404, "cdate": 1761960090404, "tmdate": 1762942205829, "mdate": 1762942205829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DeepResearchGuard, a safety framework for deep research agents that decomposes complex queries, iteratively searches the literature, and generates structured reports. The authors identify two gaps in the current deep research agent framework: (1) evaluation focuses on QA accuracy rather than report quality, and (2) a lack of stage-specific safeguards allows harmful content to propagate through the multi-stage pipeline. \n\nDeepResearchGuard addresses these by implementing guards at four stages (input, plan, research, output), each with tailored taxonomies, memory-augmented classification, and human-in-the-loop intervention. The authors also contribute DRSafeBench, an 828-query benchmark combining adversarial and benign examples. Experiments across gpt-4o, Gemini-2.5-flash, DeepSeek-v3, and o4-mini show average defense success rate improvements of 18.16% while reducing over-refusal by 6%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The four-stage framework design is clear and practical. Each guard has a clear taxonomy, severity-based actions, and memory retrieval for consistency.\n\n2. The author did a comprehensive experimental evaluation. The author tests four baseline LLMs. Then, the author did ablation studies to show that the input guard contributes the most. Also, guard model sensitivity analysis reveals safety-performance tradeoffs\n\n3. In this paper, the author further introduced  DRSafeBench, which has 828 queries and covers diverse failure modes, including synthesized cases for low-quality/format errors.\n\n4. The confidence-based escalation mechanism with user override options balances automation with human judgment."}, "weaknesses": {"value": "1. I have some concerns about the attacking method. Reference poisoning is simulated by rewriting content, but it may be hard to reflect real attacks. Also, no evaluation against actual jailbreaks or adversarial attacks designed to evade the guards.\n\n2. For the report quality evaluation, the author only takes the 1-5 scores for coherence, credibility, etc., and then use LLM-as-judge to evaluate. There is no human evaluation. As safety research work, maybe using human evaluation is important.\n\n3. The paper doesn't compare against LlamaGuard, WildGuard, or other moderation systems that could be adapted for this task.\n\n4. The author didn't evaluate the framework on recent LLMs. Typically, as the model is updated, its safeguards are also upgraded. So, evaluating new model is also necessary for this work. For example, how DeepResearchGuard compares to simply using a stronger base model (e.g., gpt-5 family for research instead of o4-mini + guards)?"}, "questions": {"value": "See in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xef8pumIin", "forum": "217LeFOYoT", "replyto": "217LeFOYoT", "signatures": ["ICLR.cc/2026/Conference/Submission22406/Reviewer_UihV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22406/Reviewer_UihV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986833336, "cdate": 1761986833336, "tmdate": 1762942205562, "mdate": 1762942205562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}