{"id": "Pwag2BlEZC", "number": 6188, "cdate": 1757957182008, "mdate": 1763121903848, "content": {"title": "EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory", "abstract": "Humans possess a remarkable ability to mentally explore and replay 3D environments they have previously experienced. Inspired by this mental process, we present EvoWorld: a world model that bridges panoramic video generation with evolving 3D memory to enable spatially consistent long-horizon exploration. Given a single panoramic image as input, EvoWorld first generates future video frames by leveraging a video generator with fine-grained view control, then evolves the scene's 3D reconstruction using a feedforward plug-and-play transformer, and finally synthesizes futures by conditioning on geometric reprojections from this evolving explicit 3D memory. Unlike prior state-of-the-arts that synthesize videos only, our key insight lies in exploiting this evolving 3D reconstruction as explicit spatial guidance for the video generation process, projecting the reconstructed geometry onto target viewpoints to provide rich spatial cues that significantly enhance both visual realism and geometric consistency. To evaluate long-range exploration capabilities, we introduce the first comprehensive benchmark spanning synthetic outdoor environments, Habitat indoor scenes, and challenging real-world scenarios, with particular emphasis on loop-closure detection and spatial coherence over extended trajectories. Extensive experiments demonstrate that our evolving 3D memory substantially improves visual fidelity and maintains spatial scene coherence compared to existing approaches, representing a significant advance toward practical long-horizon spatially consistent world modeling.", "tldr": "We propose a framework for evolving world generation that builds and updates an explicit 3D memory from generated panoramic videos. Conditioning on this reconstructed geometry mitigates error accumulation, ensuring spatial consistency over time.", "keywords": ["World Model", "Video Generation", "Embodied AI", "Panorama", "Spatial Consistency"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/dbcbe1995af724382fb8f16c156932c6dfa35547.pdf", "supplementary_material": "/attachment/4f4b25e2d62529ab2af4cd4878ec6606eaa1eac4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces EvoWorld, a world model that couples panoramic video generation with an explicitly evolving 3D memory to maintain spatial and temporal consistency over long exploration trajectories. Given a single 360 panorama as input, EvoWorld:\n\n1. Generates future frames via a diffusion-based panoramic video generator with fine-grained camera control.\n2. Reconstructs and updates an explicit 3D point-cloud memory from generated frames.\n3. Conditions subsequent generations on reprojections from this 3D memory.\n\nTo support evaluation, the authors curate Spatial360, a new dataset of synthetic (Unity, UE5), indoor (Habitat), and real-world panoramic videos with ground-truth poses."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: Explicit 3D memory integration for diffusion-based panoramic generation is well motivated.\n\n2. Quality: Experiments demonstrate consistent improvement over baselines (GenEx, ViewCrafter, CogVideoX, Wan2.1) in FVD, LPIPS, MEt3R, and loop-closure metrics, showing reduced drift and higher 3D coherence.\n\n3. Clarity: Excellent visualizations and well-structured technical presentation.\n\n4. Significance: Provides a scalable direction for connecting 3D reconstruction and video world modeling. The framework also enables downstream tasks such as spatially-aware frame retrieval and target reaching when paired with GPT-4o."}, "weaknesses": {"value": "1. Limited novelty in components. While the integration is elegant, the core ingredients (video diffusion + reconstruction + reprojection) are adaptations of existing methods rather than fundamentally new architectures.\n\n2. Scalability and efficiency. Although the paper reports 0.3 FPS end-to-end, the iterative reconstruction loop may limit practical deployment for longer sequences.\n\n3. Ablations could be richer. The paper could further dissect the impact of memory update frequency, reconstruction confidence thresholds, and different 3D representations.\n\n4. Limited discussion of failure cases. Occlusion and dynamic object handling are only briefly mentioned; visual examples of failure would strengthen transparency."}, "questions": {"value": "1. Could the reconstruction and generation modules be trained jointly so that gradients flow through the reprojection process?\n\n2. Have the authors considered more compact 3D memories (e.g., Gaussian splats or learned feature fields) instead of point clouds for better scalability? What about it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uiP4oxdJkY", "forum": "Pwag2BlEZC", "replyto": "Pwag2BlEZC", "signatures": ["ICLR.cc/2026/Conference/Submission6188/Reviewer_FTFE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6188/Reviewer_FTFE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760937528248, "cdate": 1760937528248, "tmdate": 1762918531225, "mdate": 1762918531225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "a89ySo64Va", "forum": "Pwag2BlEZC", "replyto": "Pwag2BlEZC", "signatures": ["ICLR.cc/2026/Conference/Submission6188/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6188/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763121586947, "cdate": 1763121586947, "tmdate": 1763121586947, "mdate": 1763121586947, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author addresses the challenge of long-horizon, spatially-consistent panoramic video generation by building on the insight of incorportin explicit 3D memory over generated contents. Approach-wise, EvoWorld starts from sinegle panoramic image input, and then leverage video generator with camera control to generate video (assuming it's all static contents, since they finetuned on their static data). They use VGGT to localize the generated frames, and distill changes to 3D for later video extrapolation steps. The key insight, it levages 3D reconstraction along with video generation for better spatial guidance. For dense view control, we provide their own spherical plucker coordinate encoding for extrinsincs in the panoramic setup. \n\nIn their experiments, they compare with approach on other video generator only baselines to showcase theirs are the best in terms of 2d and 3d quality and consistencies. Through their ablation, they demonstrate SpherePlücker + 3D memory gives the best video results. They also further evaluate EvoWorld in several downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The comparison on video quality against baselines are carefully conducted. Both metrics in 2d and 3d are reported, including MEt3R. \n\n2. The results are validated across synthetic and real-world data benchmarks. \n\n3. Results are also evaluated carefully in several downstream tasks. \n\nOverall, I appreciate the authors efforts on thorought evaluation and comparison on the proposed method against baseline across different benchmarks."}, "weaknesses": {"value": "1. Missing Key Related Work and Unclear Novelty Positioning\nThe related work section overlooks several highly relevant research threads, which makes it difficult to clearly identify the contribution’s novelty. In particular, prior efforts in perpetual video / world generation (e.g., Infinite Nature, LoTR, DiffDreamer) and panoramic view synthesis (e.g., works summarized in Sec. 3.3.1 of this survey: arXiv:2505.05474) are not adequately discussed. These works share similar goals of long-horizon view consistency and world exploration, so a discussion is necessary to contextualize what is new in this paper.\n\n2. Claims Not Fully Supported by Results\nAlthough introducing a 3D memory representation to video generation is a reasonable idea, the experimental evidence does not convincingly demonstrate the claimed benefits:\n-- Spatial inconsistency: Multi-view frames in Fig. 4 display noticeable inconsistencies, particularly in building facades, which become more evident when inspecting the provided video examples.\n-- Limited trajectory length: The generated camera motion appears to span only short distances (≈10 meters), which does not align with the paper’s claims of enabling long-horizon generation. The author also mentions this in limitation section. \nAs a result, the key claims of “long-range, spatially consistent video generation” presented in the abstract are not sufficiently validated.\n\n3. No Strategy for Loop Closure or Revisitation\nHowever, the proposed approach does not articulate any mechanism to support scene generation after revisiting as is shown in the teaser image. Additionally, the paper provides no qualitative examples where a camera trajectory completes a loop (e.g., around a city block) and returns to an earlier location without accumulating significant drift or scene errors.\n\n4. Incremental Insight Relative to Prior 3D-Aware Generative Models\nThe central idea—that explicit 3D memory improves view consistency—has been explored in earlier works such as SynSin, DiffDreamer, and others in the literature on 3D-aware generation. The contribution therefore feels somewhat incremental unless stronger experimental evidence or new theoretical insights can be demonstrated. \n\n5. VGGT localization necessary?\nIt is a bit unclear in terms of why the author rely on pose-conditioned video generator, but still rely on VGGT for localization."}, "questions": {"value": "1. Could you compare your results with existing work that leverage explicit 3D memory? For example, DiffDreamer or other Panoramic scene generation work? \n\n2. What prevent the current model from generating long-horizon video contents? \n\n3. How is the EvoWorld differentiate itself from existing work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cuYV3nfe4z", "forum": "Pwag2BlEZC", "replyto": "Pwag2BlEZC", "signatures": ["ICLR.cc/2026/Conference/Submission6188/Reviewer_4M6D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6188/Reviewer_4M6D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533540556, "cdate": 1761533540556, "tmdate": 1762918530668, "mdate": 1762918530668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "EvoWorld is a world model that solves spatial inconsistency in long-horizon panoramic video generation.\n\nIt works by linking video generation with an explicit, evolving point cloud as 3D memory.\n\nThe model generates new video frames conditioned on geometric projections from the current 3D memory, ensuring the output respects the scene's prior structure. The new frames are then used to update the 3D memory, preventing geometric drift and ensuring loop consistency.\n\nEvoWorld outperforms existing models in visual effects and consistency on a new proposed benchmarking dataset, Spatial360."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It effectively resolves the spatial inconsistency issue in panorama video generation, which is a crucial technical contribution. While the use of point clouds as 3D memory isn't new, the method presents an effective integration/application of this concept for panorama video generation.\n\n2. The work introduces a new, dedicated dataset Spatial360 to the community, which is a significant contribution to advancing research in panoramic video generation.\n\n3. The experimental results are strong, clearly validating the method's effectiveness and the utility of the proposed dataset."}, "weaknesses": {"value": "1. The current 3D memory is static, which fundamentally limits the model's ability to handle moving objects and dynamic scene changes within the generated video. This will likely lead to artifacts or inconsistent rendering when objects move, which is important in video generation.\n2. More explanation is needed to justify the choice of panoramic video generation. The paper should better explain the specific advantage of this choice over using general video generation, especially if the underlying mechanism could be applied more broadly. The paper need explain the unique technical contribution compared to previous 3D(point cloud)-equipped general video generation techniques.\n3. The quality is limited by using SVD as a backbone, which generally doesn't leverage textual information effectively for guiding the generation, leading to less controllable outputs. Generating only 25 frames is quite short by current standards. It's relatively too brief to fully capture and demonstrate the long-term spatial inconsistencies that a model should ideally solve, as a short clip often only shows a single scene or limited motion.\n4. The paper provides insufficient explanation for the use of Plücker embeddings. A deeper theoretical analysis is preferred, including a clear explanation of why this specific representation works well for the task of camera control or scene modeling.\n\nIf my concerns are clearly addressed and explained, I believe my score could be raised."}, "questions": {"value": "1. If it possible to compare the single-view results (specifically the memory-augmented cropped video like in Figure 3) against prior work like ViewCrafter?\n2. Example 4 in the supplementary materials shows a real-world video with a person moving, what's the effects of current framework upon moving objects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C6gAvyyPLO", "forum": "Pwag2BlEZC", "replyto": "Pwag2BlEZC", "signatures": ["ICLR.cc/2026/Conference/Submission6188/Reviewer_TTdY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6188/Reviewer_TTdY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832846931, "cdate": 1761832846931, "tmdate": 1762918529205, "mdate": 1762918529205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EvoWorld, a method for generating long-horizon, panoramic videos from a single starting image. The primary goal is to improve long-term spatial consistency, especially in scenarios involving loop closure. The key idea is to couple the video generation process with an explicit 3D memory, which takes the form of a 3D point cloud. As new video frames are generated, this 3D memory is updated using a feed-forward reconstruction model. This explicit 3D geometry is then reprojected onto the target camera path to provide spatial guidance for the next step of video generation. The authors also propose a new dataset, Spatial360, for this task and a spherical Plücker embedding for panoramic camera control."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem this paper tackles, maintaining long-term 3D consistency in generative world models, is a significant and open challenge. The core idea of using an explicit 3D representation to ground the generation process is intuitive and a very interesting direction for research.\n\nThe authors have clearly put a substantial amount of effort into creating the Spatial360 dataset. A high-quality, large-scale benchmark for panoramic video exploration with camera poses is a valuable contribution that could benefit the wider community."}, "weaknesses": {"value": "While the problem is interesting, I have several major concerns about the core methodology and its evaluation, which unfortunately lead me to recommend rejection at this time.\n\n1. Risk of Compounding Errors: My primary concern is the proposed feedback loop. The 3D memory is reconstructed from the generated video frames. Any artifact, blur, or spatial drift in the generated video will inevitably be incorporated into this 3D memory. This flawed 3D memory is then used as a conditioning signal (a sort of \"ground truth\") for future frames. This seems highly likely to create a negative feedback loop where errors are not mitigated, but are instead amplified. For instance, a hallucinated object in one clip becomes a \"real\" 3D point, which then forces the model to continue generating it in subsequent views, potentially distorting the scene further. The paper claims this mitigates error accumulation, but there is no analysis to support this over the alternative (error amplification).\n\n2. Limited Technical Novelty: The framework itself feels like a straightforward assembly of existing, off-the-shelf components. It primarily combines a well-known video generator (SVD) with an existing 3D reconstruction method (VGGT). The \"evolving\" aspect is simply the process of re-running the reconstruction model on new data. While this combination is logical, it feels more like an application of existing tools rather than a fundamental new technique or insight into the problem.\n\n3. Evaluation of 3D Memory Quality: The paper's core claim rests on the utility of the 3D memory. However, there is no quantitative evaluation of the quality of this 3D memory itself. How accurately does the point cloud (reconstructed from generated images) represent the true underlying geometry? How quickly does this 3D memory drift from the ground truth? Figure 4 offers a qualitative glimpse, but it's not sufficient to validate that the 3D guidance being fed back to the model is geometrically correct or helpful in the long term.\n\n4. Inadequate Evaluation of Long-Horizon Claims: The paper claims to mitigate error accumulation over \"long-horizon\" trajectories. However, the quantitative evaluation is only on 73-frame clips (Table 2). This is not sufficiently long to truly test the limits of spatial drift or prove that the 3D memory is preventing error accumulation. A core claim like this should be substantiated with much longer generations (e.g., 200+ frames) to see at what point the model's consistency does break down, and how that compares to baselines.\n\n5. Insufficient Baseline Comparisons: The baseline comparison feels insufficient. The main comparisons are against GenEx (a concurrent work) and standard video models that lack long-term memory. While ViewCrafter is included, a more rigorous comparison would involve other state-of-the-art camera-conditioned video generation models. Furthermore, it's unclear if this complex explicit 3D pipeline is superior to simpler memory mechanisms, such as an attention-based model that can condition on a sliding window of distant past frames.\n\n6. Missing Comparison to Highly Relevant Work: The core idea of using an explicit, evolving 3D memory to ensure consistency in generative video models is not entirely new. For instance, VMem [1], submitted to arxiv in June 2025 and published at ICCV 2025, explores a very similar concept with a \"surfel-indexed view memory\" for consistent interactive video generation. Given the significant overlap in motivation and technical approach, the novelty of the proposed framework is questionable. At a minimum, the authors should have included a thorough comparison and discussion against this highly relevant and recent work to clearly differentiate their own contributions.\n\n[1] Li, R., Torr, P., Vedaldi, A. and Jakab, T., 2025. VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory. arXiv preprint arXiv:2506.18903."}, "questions": {"value": "1. Could the authors provide a more in-depth analysis of the error accumulation feedback loop? What mechanisms prevent generative artifacts from being \"baked into\" the 3D memory and subsequently causing compounding errors?\n\n2. How does the quality of the 3D point cloud (e.g., measured by Chamfer distance or L1 loss against the ground truth point cloud) degrade as more and more generated clips are added? A quantitative analysis of the 3D memory's drift over time feels essential here.\n\n3. Why was a sparse point cloud chosen for the 3D memory? It seems that re-projections from a sparse point cloud (especially one built from potentially noisy generated images) might provide very weak or even misleading guidance. Have the authors considered denser or implicit representations like Gaussian Splatting or NeRF?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tqw5z6qgDF", "forum": "Pwag2BlEZC", "replyto": "Pwag2BlEZC", "signatures": ["ICLR.cc/2026/Conference/Submission6188/Reviewer_8PwF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6188/Reviewer_8PwF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888814527, "cdate": 1761888814527, "tmdate": 1762918528291, "mdate": 1762918528291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}