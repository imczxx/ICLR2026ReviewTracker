{"id": "yjG3qagrZ0", "number": 13157, "cdate": 1758214256804, "mdate": 1759897460016, "content": {"title": "Kaleidoscopic Teaming in Multi Agent Simulations", "abstract": "Warning: This paper contains content that may be inappropriate or offensive.\n\nAI agents have gained significant recent attention due to their autonomous tool usage capabilities and their integration in various real-world applications. This autonomy poses novel challenges for the safety of such systems, both in single- and multi-agent scenarios. We argue that existing red teaming or safety evaluation frameworks fall short in evaluating safety risks in complex behaviors, thought processes and actions taken by agents. Moreover, they fail to consider risks in multi-agent setups where various vulnerabilities can be exposed when agents engage in complex behaviors and interactions with each other. To address this shortcoming, we introduce the term kaleidoscopic teaming which seeks to capture complex and wide range of vulnerabilities that can happen in agents both in single-agent and multi-agent scenarios. We also present a new kaleidoscopic teaming framework that generates a diverse array of scenarios modeling real-world human societies. Our framework evaluates safety of agents in both single-agent and multi-agent setups. In single-agent setup, an agent is given a scenario that it needs to complete using the tools it has access to. In multi-agent setup, multiple agents either compete against or cooperate together to complete a task in the scenario through which we capture existing safety vulnerabilities in agents. We introduce new in-context optimization techniques that can be used in our kaleidoscopic teaming framework to generate better scenarios for safety analysis. Lastly, we present appropriate metrics that can be used along with our framework to measure safety of agents. Utilizing our kaleidoscopic teaming framework, we identify vulnerabilities in various models with respect to their safety in agentic use-cases.", "tldr": "This paper proposes a new red teaming approach called kaleidoscopic teaming for evaluating safety of agents in single-agent and multi-agent setups.", "keywords": ["Evaluation", "Safety", "Red Teaming"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/665848d8ff2b9ac754dbbcd9d663deab2d73c907.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents the Kaleidoscopic Teaming framework, which aims to improve AI agent safety evaluations, particularly in multi-agent environments. By using the MASK framework, it simulates challenging, dynamic scenarios where agents can interact, cooperate, or compete to complete tasks. The framework is designed to capture the complexities of agent interactions, thought processes, and decision-making in real-world-like environments. The paper also proposes new optimization strategies for generating these scenarios, along with metrics to assess the safety of agents in these contexts. This work provides an important step toward more realistic and comprehensive safety evaluations for AI agents."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Innovative Approach: \nThe introduction of the Kaleidoscopic Teaming framework is an innovative and thoughtful approach to evaluating AI agent safety in complex, multi-agent settings. It offers a novel perspective on safety evaluation that goes beyond traditional red-teaming methods, making it highly relevant for the evolving landscape of AI deployment.\n\n2. Comprehensive Simulation: \nThe Multi-Agent Simulation Kaleidoscopic-Teaming framework provides a thorough method for stress-testing agents by modeling real-world interactions and belief states, ensuring that agents are evaluated in dynamic, challenging scenarios.\n\n3. In-Context Optimization Strategies: \nThe paper’s in-context optimization techniques to generate more challenging scenarios for agents is a strong contribution. These strategies help to push the boundaries of traditional safety evaluations and expose vulnerabilities in a more nuanced manner."}, "weaknesses": {"value": "1. Insight Gatherer and Judges:\nThe roles of the Insight Gatherer and Judges seem to be somewhat separate but could benefit from a more integrated approach. Is it necessary to keep them as distinct entities, or could they be merged into one unified system to directly evaluate the agents? The paper does not fully justify the separation. An ablation experiment exploring the impact of combining or separating these components could provide valuable insights into whether the current setup is optimal.\n\n2. Role of the Orchestrator:\nThe Orchestrator’s function is explained in the context of controlling agent interactions and belief states. However, its role and operation appear quite similar to that of a traditional star-shaped structure where a central node manages tasks. It would be helpful to clarify how the Orchestrator differs from traditional management nodes and whether its added complexity truly contributes to the system’s effectiveness.\n3. Realism of Generated Scenarios:\nA significant concern is how to ensure that the generated scenarios are realistic and grounded in real-world possibilities. The paper does not fully address how these scenarios are constructed in a way that ensures they do not deviate too far from practical situations. If scenarios are too fictional, this could lead to agents completing tasks in unrealistic ways, potentially compromising the reliability of safety evaluations. Could the authors consider leveraging existing benchmarks as seeds to generate scenarios rather than relying on the model to generate them from scratch? This would ensure that the scenarios align more closely with real-world conditions.\n4. Selection of Agents in MAS:\nThe paper mentions creating a society of 100 agents, with eight major agent types. However, it remains unclear how the Multi-Agent Simulation (MAS) selects agents for each iteration. Figure 2 mentions, “In MASK, at each iteration, either one or more (a group of) agents are selected from the society,” but there is no detailed explanation of how this selection process works. Does the random selection of agents ensure that the chosen agents are suitable for the specific tasks in the generated scenario? If the agents are not well-suited to the tasks, the results could be meaningless or difficult to interpret. Further clarification on the agent selection process and its potential impact on safety evaluations would be valuable, as it is crucial to ensure that the chosen agents align with the specific requirements of each generated scenario.\n5. Percent Negative Agents vs. Percent Negative Scenarios:\nThe metrics Percent Negative Agents and Percent Negative Scenarios appear to be highly similar, with one directly influencing the other. There seems to be no strong justification for measuring both separately. It might be more effective to focus on evaluating agents’ planning processes and their associated negative outcomes rather than focusing on the scenario level. This would provide a more granular understanding of where the safety issues originate.\n6. Scalability of Overall Average Agent Score:\nThe definition and calculation of the Overall Average Agent Score, particularly the -2 and 2 values, are unclear. The paper does not provide an explanation of how these scores are derived or what specific criteria are used to assign them. Additionally, how does this score range scale when the number of agents varies? The paper does not address how this metric adapts in such cases. A more detailed explanation of how the -2 and 2 scores are determined, and how the score range scales with different agent populations, would be useful to understand the metric’s robustness and consistency.\n7. Judge Rubric and Evaluation Consistency:\nThe decision to use the worst score from the judges rather than an average or median raises concerns about evaluation consistency. How do the judges’ individual differences influence the final scores, especially if the judges are using the same model? If the model used by the judges produces the worst score, how can it be guaranteed that this score accurately reflects an agent’s safety, rather than being a result of potential misjudgment or model error? More clarity on how the worst score is selected as the definitive evaluation would strengthen the validity of the assessment process.\n8. Limited Evaluation of Attack Models:\nThe evaluation of attack models is somewhat limited, as the paper only compares strong LLMs as attack models against weak LLMs as target models. It would be interesting to explore the effectiveness of attack models of similar or stronger strength against the same LLMs. This would help in better assessing the robustness of the system and provide a more thorough evaluation.\n9. Task Completion Evaluation:\nThe paper omits an important aspect of agent evaluation: task completion success (e.g., accuracy or completion rate). There is no clear measure of how well the agents complete the tasks, and this is essential for evaluating their overall safety and effectiveness. It would be valuable to introduce accuracy-based metrics to ensure that agents are not only safe but also competent in completing the tasks at hand. Moreover, should a task be deemed impossible to complete, should this be considered a safety issue as well?"}, "questions": {"value": "1. Clarification of Table and Figure Labels:\nIn Section 4, the table labels are inconsistent. Table 3 should be Table 1 based on the context of the results. Additionally, Figure 4 is unclear, and it is difficult to discern the meaning of each chart without additional graphical annotations. The authors should consider adding labels or explanations to make these visuals more understandable.\n\n2. Appendix Structure:\nThe Appendix lacks subheadings, making it difficult to navigate, especially when referencing specific figures or details. Clearer segmentation in the appendix would help readers quickly locate the relevant information. Furthermore, the frequent citation of the appendix in the main text, with no specific reference to the relevant subsection, makes it challenging to follow and could hinder the clarity of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fsE4Dh2Rfc", "forum": "yjG3qagrZ0", "replyto": "yjG3qagrZ0", "signatures": ["ICLR.cc/2026/Conference/Submission13157/Reviewer_uatp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13157/Reviewer_uatp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550845454, "cdate": 1761550845454, "tmdate": 1762923870338, "mdate": 1762923870338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is about the safety evaluation of LLM agents. The authors propose a new method of safety testing called kaleidoscopic teaming, which allows for testing agents in single and multi-agent scenarios. Compared to red teaming, kaleidoscoping teaming allows for more complex, multi-turn scenarios, which are dynamically adjusted for finding vulnerabilities in specific agents. The authors describe their framework, which consists of “functional” LLMs orchestrating, making up the scenarios, and judging the agents. They consider different strategies of prompting them, and they propose a metric for safety that can be used in this framework. The experiments show diverse results of safety for different LLM agents and their types."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Importance of the topic. LLM agents should be tested for safety, and this work allows for more complex testing. Additionally, authors show the importance of multi-agent testing.\n\nPossibility for capturing the different levels of safety.\n\nA dynamic framework that can adapt to a specific agent/agent type. Easily automated."}, "weaknesses": {"value": "No comparison with baselines. E.g., how would red teaming or automated red teaming grade the agents? Would there be much of a difference? \n\nThere is no formal definition of the metrics. The description is a bit unclear, and formal equations would be useful. E.g., the score value. Additionally, why are the metrics introduced if they are not used in the experiments? \n\nIt’s a very “practical paper” with no guarantees of detecting any specific kinds of vulnerabilities. And for such paper, there isn’t enough evaluation of particular parts of the framework (e.g., how different LLMs for judges influence the result)\n\nFigure 3: For Claude 3.7(and  Nova Pro) the score is opposite for Nova Lite and Deepseek.  That suggests that the framework is very dependent on the LLM types in Kaleidoscope, orchestrator, etc. What to do with this instability of evaluation?\n\nNo mention of the code being published.\n\nVery limited explanation of the relationship to the automated red teaming research.  The multi-agent aspect is claimed to be novel. Is there a difference between components of agents and multi-agent scenario? If this is the selling point, they should focus on specific examples from their scenarios that are of this type and explain why alternative methods would not capture them.  \n\nThough I appreciate playfulness, it was hard to focus on reading the paper due to all the colorful fonts and bolded text."}, "questions": {"value": "What is the reason for the substantially different evaluation result of Claude 3.7 by DeepSeek and Nova Pro?\n\nWhy only the kaleidoscope component  is tested with different LLM models? Other components are always just Nova Lite.\n\nHave you considered the weaknesses of using LLM as a judge and other components of an LLM evaluation framework? Is there a danger of collusion?\n\nHow can you make sure that testing a potentially malicious agent with access to real-world tools  (RapidAPI) will not cause any harm?\n\nWhat’s the difference in evaluation in competitive and cooperative multi-agent scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fb7fWs13wx", "forum": "yjG3qagrZ0", "replyto": "yjG3qagrZ0", "signatures": ["ICLR.cc/2026/Conference/Submission13157/Reviewer_x5hb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13157/Reviewer_x5hb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823635103, "cdate": 1761823635103, "tmdate": 1762923869897, "mdate": 1762923869897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an approach for jailbreaking LLMs in the context of multiple agents. The term kaleidoscopic teaming is a variation of red teaming. The authors perform simulations in which they model agents that contains agents with different capabilities and which exhibit human traits such as emotions and belief states."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The authors performed experiments with a relatively large number of generated agents across multiple models."}, "weaknesses": {"value": "* It is not clear whether the agents proposed by the authors are considered to be agents who might be implemented and deployed in production settings. Why would these agents exhibit human emotions?\n* The main contribution of the paper is unclear. I.e. are the authors propose that instead of red teaming people perform kaleidoscope teaming? Is this intended to be a methodology or a benchmark?"}, "questions": {"value": "* Can you please clarify how would the reader take advantage of the contributions of this paper? Would it need to repeat your experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review is needed."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HFm6QgYbvx", "forum": "yjG3qagrZ0", "replyto": "yjG3qagrZ0", "signatures": ["ICLR.cc/2026/Conference/Submission13157/Reviewer_5uui"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13157/Reviewer_5uui"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953397505, "cdate": 1761953397505, "tmdate": 1762923869452, "mdate": 1762923869452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces kaleidoscopic teaming—a safety‐evaluation paradigm for AI agents (not just chat models) that stresses test thoughts, actions, tool use, and inter‑agent interactions. It operationalizes this via MASK (Multi‑Agent Simulation Kaleidoscopic‑teaming): a loop with a kaleidoscope that generates scenarios, an orchestrator that assigns turns and belief/emotional states, judges with a rubric, and an insight gatherer that feeds weaknesses back to the scenario generator. The authors also add two in‑context optimization strategies for the kaleidoscope—PSO (Past Scenarios Only) and CSR (Contrastive Scenarios + Rewards)—and define three ASR metrics (percent negative agents, percent negative scenarios, overall average agent score). Experiments span 100 agents using 32 tools across 8 domains; results show that optimized strategies often yield higher attack success and that multi‑agent settings expose more vulnerabilities than single‑agent ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work evaluates thoughts, tool calls, and interactions rather than just final answers—closer to real agent deployments.\n1. It has a clear, replicated finding that inter‑agent dynamics expose more safety failures\n1. PSO and CSR are simple, model‑agnostic ways to steer the scenario generator; they often raise ASR and scenario diversity\n1. Per‑agent‑type safety profiles can guide targeted mitigations."}, "weaknesses": {"value": "1. Although partially validated, LLM‑as‑judge can encode biases; taking the worst score across an ensemble may over‑penalize edge cases. More blind human audits and evaluations on judging schemes would help.\n1. The orchestrator injects belief/emotional states to nudge unethical behavior; this may inflate ASR relative to organic failures and complicate comparison with other frameworks. A controlled ablation isolating belief injection effects is needed. \n1. “Percent negative” treats minor and catastrophic harms similarly; adding severity‑weighted scores and time‑to‑failure would give a richer view.\n1. Datasets/traces aren’t released, hindering replication"}, "questions": {"value": "How do you handle severity-related concerns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "deD260aRFN", "forum": "yjG3qagrZ0", "replyto": "yjG3qagrZ0", "signatures": ["ICLR.cc/2026/Conference/Submission13157/Reviewer_NAF4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13157/Reviewer_NAF4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982139221, "cdate": 1761982139221, "tmdate": 1762923869163, "mdate": 1762923869163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}