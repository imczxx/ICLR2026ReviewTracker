{"id": "zxito57J6x", "number": 11313, "cdate": 1758196096537, "mdate": 1759897594230, "content": {"title": "Segment Any Events with Language", "abstract": "Scene understanding with free-form language has been widely explored within diverse modalities such as images, point clouds, and LiDAR. However, related studies on event sensors are scarce or narrowly centered on semantic-level understanding. We introduce **SEAL**, the first Semantic-aware Segment Any Events framework that addresses Open-Vocabulary Event Instance Segmentation (OV-EIS). Given the visual prompt, our model presents a unified framework to support both event segmentation and open-vocabulary mask classification at multiple levels of granularity, including instance-level and part-level. To enable thorough evaluation on OV-EIS, we curate four benchmarks that cover *label granularity* from coarse to fine class configurations and *semantic granularity* from instance-level to part-level understanding. Extensive experiments show that our SEAL largely outperforms proposed baselines in terms of performance and inference speed with a parameter-efficient architecture. In the Appendix, we further present a simple variant of our SEAL achieving generic spatiotemporal OV-EIS that does not require any visual prompts from users in the inference. The code will be publicly available.", "tldr": "We introduce SEAL, the first Semantic-aware Segment Any Events framework that addresses Open-Vocabulary Event Instance Segmentation.", "keywords": ["event sensor", "event-based scene understanding", "open-vocabulary"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a594d5c47fbd6175ac21be75bf98ee7ea2b8993e.pdf", "supplementary_material": "/attachment/a851f0b1559e15ef83b0e3c5608bb09d0b38b898.zip"}, "replies": [{"content": {"summary": {"value": "Authors propose the first text-guided instance segmentation framework for event streams, achieving open-vocabulary segmentation, and introduce four benchmark levels to validate the effectiveness of the proposed model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  Proposes SEAL, the first open-vocabulary instance segmentation framework for event streams, achieving effective instance-level segmentation.\n2.  Introduces a decoupled training–inference strategy, featuring a complex training framework and a lightweight inference framework.\n3.  Constructs a scarce open-vocabulary segmentation dataset and designs four benchmark levels for training and evaluation."}, "weaknesses": {"value": "1. Incomplete related work analysis. In the Open-World Events Understanding section (line 126), the authors discuss event-based vision foundation models such as EventBind and EventCLIP, but fail to include analyses of event-based vision MLLMs such as EventGPT and EventVL, which also fall within this domain.\n2. Dataset availability concerns. For open-vocabulary segmentation in event streams, the authors propose a complex training framework. However, the scarcity of open-vocabulary segmentation datasets in this domain remains a key limitation. The authors should clearly state whether their dataset will be publicly released.\n3. Fairness of experimental analysis. In the main experiments, several RGB-based models are used for comparison. It is unclear whether these models were fine-tuned on the proposed dataset (with event-image representations). If not, the comparison is unfair, and results after fine-tuning should be reported.\n4. Unclear experimental presentation. The authors introduce four benchmark levels for open-vocabulary segmentation, which vary significantly in word-level and semantic-level difficulty. In Table 1, the evaluation for each benchmark level should be clearly separated and 5. Dataset quality issues. For semantic-level open-vocabulary segmentation, the authors mainly rely on the DSEC dataset, where all scenes are road environments. These scenes are highly repetitive, and event streams lack color and texture information, leading to low semantic diversity and a risk of overfitting. The authors should clarify how they mitigate or address these issues."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zngDpmT4xB", "forum": "zxito57J6x", "replyto": "zxito57J6x", "signatures": ["ICLR.cc/2026/Conference/Submission11313/Reviewer_5RxY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11313/Reviewer_5RxY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761189145604, "cdate": 1761189145604, "tmdate": 1762922454469, "mdate": 1762922454469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce a framework to segment events using language. \nIt contains Multimodal Hierarchical Semantic Guidance and Multimodal Fusion Network.\nAnd further propose four benchmarks to evaluate the results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper introduce a \"Segment Any Events\" framework, which can  generate open-world semantic predictions for event masks.\n* This paper  attempt to address OV-EIS that supports free-form of language queries.\n* This paper propose four benchmarks for evaluation."}, "weaknesses": {"value": "* The architecture seems rather complex. Could the authors provide a clearer motivation for the inclusion of these modules? Are all of these modules necessary? Is there a simpler approach that could achieve the same results? Besides, it appears that the method is transferring concepts from open-vocabulary image segmentation techniques, such as OpenSeg, MaskCLIP, MaskCLIP++, and OVSeg to the event modality. Could the authors clarify this adaptation and its justification?\n\n* The paper claims to handle open-vocabulary, but is there a benchmark to demonstrate this capability? For example, can the model generalize to unseen classes, or handle user-defined text queries effectively?\n\n* MHSG uses SAM masks and CLIP features on images as supervision signals for the event modality, but there may have significant modality differences between events and images. How accurately can image masks correspond to event regions in this context?\n\n* The part-level experiments seems conducted solely on the DSEC-Part dataset, which contains very few categories, with a small sample size and severe class imbalance. The model’s performance at a finer granularity (e.g., material, action, state) has not been evaluated. \n\n* Additionally, the paper does not analyze conflicts or consistency between different granularities, for example, how the model handles the situation when a wheel is simultaneously recognized as both a “car” and a “wheel”.\n\nI would be happy to revise my score if the author addresses these points."}, "questions": {"value": "Please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HUNwQ2KQTq", "forum": "zxito57J6x", "replyto": "zxito57J6x", "signatures": ["ICLR.cc/2026/Conference/Submission11313/Reviewer_55Qv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11313/Reviewer_55Qv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903561210, "cdate": 1761903561210, "tmdate": 1762922454101, "mdate": 1762922454101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a multimodal, hierarchical semantic method  to align event-based segmentation with open-vocabulary language queries. It supports both instance-level and part-level mask generation. The proposed method uses three types of prompt-driven masks generated by SAM for each image (semantic, instance, part). The features are supervised by pooled CLIP embeddings and LLM-generated captions. The multimodal fusion network, with a single backbone, combines language and vision with explicit spatial encoding per mask, addressing previous inefficiencies and semantic conflicts in feature pooling observed in all AR-CDG, Hybrid, and AF-DA baselines."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This work is shown to outperform MaskCLIP and other leading open-vocabulary baselines by 3.4 AP on DSEC11-Ins and 3.2 AP on DDD17-Ins. Inference is 5-18x faster with fewer than 1/5th the parameters. This is a good contribution towards practical application.\n\nSecondly, unlike prior methods, this work’s two-stage mask feature enhancement and spatial encoding overcome the \"dead mask\" issue, where small-event region masks are mapped to zero vectors; UMAP visualisations show tight semantic separation after adding these modules."}, "weaknesses": {"value": "* DSEC19-Ins is a highly fine-grained dataset: on it, the improvement over MaskCLIP narrows to just 0.7 AP. This, it seems to me that, even with annotation-free training, suggests that the distilled representations are less robust when class granularity exceeds the capacity of available MHSG cues.\n* I think that the main variant of the method benefits from GT-derived visual prompts for mask proposals; although a supplementary \"prompt-free\" variant exists, the claim of real-world flexibility is less convincing without broader prompt-agnostic validation."}, "questions": {"value": "* For DSEC-Part, are part-level mask labels created\n\"by hand\" (since event data is too impoverished for mask proposals alone to identify parts)? If so, doesn’t this somewhat undermine the claim of annotation-free scaling?\n* Artefacts seem to be mitigated compared to the baselines. Even so, couldn’t reconstructing events to images or mapping event data to traditional vision models still introduce artefacts or domain discrepancy, especially under high-speed or low-event-rate scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XVwrQ4T4nd", "forum": "zxito57J6x", "replyto": "zxito57J6x", "signatures": ["ICLR.cc/2026/Conference/Submission11313/Reviewer_PR6s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11313/Reviewer_PR6s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984099786, "cdate": 1761984099786, "tmdate": 1762922453620, "mdate": 1762922453620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the SEAL architecture, an open-vocabulary event segmentation architecture based on EventSAM that can segment events on the instance and part level using single nouns or whole sentences as prompts.\n\nThe paper adapts EventSAM, an event encoder aligned with the feature space of the Segment Anything Model (SAM). A Multimodal Hierarchical Semantic Guidance (MHSG) module uses SAM to produce 3 levels of masks, which are then used to pool CLIP features and generate captions for each masked region using a pretrained vision-language model (LLaMA). The multilevel captions are used as conditioning to train a feature enhancer network that takes in event features from EventSAM. The enhanced features are pooled with a region-of-interest pooling from the EventSAM masks and are further enhanced using mask tokens from the SAM decoder. The pooled CLIP features are used to test the alignment of the language-enhanced event encodings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strong performance against baselines"}, "weaknesses": {"value": "- Limited novelty: the method is essentially a merger of several pretrained foundation models and their data in a clever way.\n\n - Limited ablations: qualitative ablations on the Spatial Encoding or Mask Feature Enhancer are missing or not well explained; I did not understand Tables 4 and 5."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wdBZdA1yPf", "forum": "zxito57J6x", "replyto": "zxito57J6x", "signatures": ["ICLR.cc/2026/Conference/Submission11313/Reviewer_AXhG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11313/Reviewer_AXhG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762275602444, "cdate": 1762275602444, "tmdate": 1762922453158, "mdate": 1762922453158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}