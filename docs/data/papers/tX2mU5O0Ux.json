{"id": "tX2mU5O0Ux", "number": 9071, "cdate": 1758109444556, "mdate": 1759897745325, "content": {"title": "AgenticPA: Toward Automated and Large-Scale Prompt Attacks on LLMs", "abstract": "As large language models (LLMs) become increasingly integrated into real-world applications, their vulnerability to prompt-based attacks has emerged as a critical safety concern. While prior research has uncovered various threats, including jailbreaks, prompt injections, and attacks on external sources or agentic systems, most evaluations are limited in scope, assessing attacks in isolation or at a small scale. This paper poses a fundamental question: \\textit{Are frontier LLMs truly robust against the full spectrum of prompt attacks when evaluated systematically and at scale?}\nTo explore this, we propose \\textbf{Agentic Prompt Attack (\\PA)}, a novel three-agent framework that automates and unifies the reproduction of prior prompt attack studies. \\PA consists of (i) a Paper Agent that extracts attack specifications from research papers, (ii) a Repo Agent that retrieves implementation details from GitHub repositories, and (iii) a Code Agent that iteratively operationalizes the attack, regardless of complexity, into executable prompts targeting LLMs. The agents collaborate to resolve ambiguities and reduce contextual noise throughout the process.\nUsing \\textbf{\\PA}, we analyzed over \\textbf{104} prompt attack papers to build a large-scale, standardized attack library. This enables systematic stress-testing of frontier LLMs, revealing that even the most recent frontier models remain vulnerable to a wide range of known threats, highlighting persistent gaps in current safety alignment.\nOur work introduces a new paradigm for evaluating LLM safety at scale, offering both a comprehensive benchmark for researchers and actionable guidance for developing more robust foundation models. \n\n**WARNING: This paper contains examples of potentially harmful content.**", "tldr": "", "keywords": ["LLMs Safety", "Benchmark", "Agent"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87ad92fcd2c4dc85a4555e2f928a775da74c4f6a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces an agent-based pipeline to standardize and automate the reproduction of jailbreak attack-related papers. The approach involves a Paper Agent to extract attack details from research papers, a Repo Agent to pull implementation specifics from associated GitHub repositories, and a Code Agent that instantiates the attacks to generate harmful, executable prompts against target LLMs. The experiments in the paper involve scanning all CS papers on arXiv, selecting 104 for reproduction, and compiling an unified attack library after reproduction. The authors then analyze performance trends across the reproduced attacks and evaluate the fidelity of their reproductions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I found the motivation of the paper quite interesting, and the work is also timely. The methods are well motivated and presented.\n- The scope of the paper is quite large - it involves processing the entirety of CS ArXiv to identify easily reproducible papers, followed by an agentic approach to unify their reproduction. In the process, this work can also facilitate checking for the effectiveness of older attacks on newer models.\n- The transformations introduced in the paper allows for quick validation of jailbreak attacks for systems such as RAG and ReAct by directly recreating the attack prompts that would have been created by these systems and testing them with the target LLM.\n- In the process of curating all of CS ArXiv, the paper presents some interesting statistical insights about existing jailbreaking approaches, as well as their reproducibility.\n- The AutoPABench artifact is also impressive in its scale, unifying datasets and jailbreaking approaches from prior literature."}, "weaknesses": {"value": "- The biggest gap in this work is the measurement of reproduction quality. It does not check to see if results from the original papers are recreated by the proposed agentic approach, therefore casting doubt about the faithfulness of the recreation.\n- The paper reports that extensive manual debugging is often required to correct the codebases generated by the agentic approach, somewhat limiting its practicality. I would also argue that the reproduction time of 22.6 minutes per paper is quite high - for a well-written paper the reproduction simply involves pulling the repo of the paper from Github and running a few commands. Some further analysis around papers that particularly benefited from reproduction through the proposed approach would be good to see.\n- The paper is missing some implementation details. For example “Each reproduction is validated using 10 test cases.” in line 288 - how does this validation work? What test cases are used? Section 5 is also heavily underspecified - how are the ASRs in table 3 calculated? What is the underlying dataset used for the evaluation? How are the attacks clustered by category?\n- While the paper mentions unification, it does not propose a single red-teaming dataset or evaluation metric to evaluate all of the reproduced attacks, therefore still making it hard to compare them in a single benchmark.\n- The appendix, while comprehensive, is not linked well in the main body of the paper, which somewhat hurts clarity."}, "questions": {"value": "- Can you provide a comparison of the breadth of attacks covered by this paper, to that in other existing benchmarks?\n- Can this framework support compositions of attacks from different papers? Prior work [1][2] has found that these compositional attacks tend to be much more effective.\n- Some further information about the amount of manual labor that went into identifying candidate papers for the pipeline, as well as correcting the generated codebases, would be helpful.\n- What percentage of reproduced attacks diverged significantly from the original Github repos?\n- The methodology mentions at line 242 that files are relocated based on their extensions - is it possible that these files are used to manage prompts critical for the reproduction?\n- What is the “interface-level validation” mentioned in line 367? Is this manual as well?\n- How does this approach set hyperparameters for reproduced attacks? The original papers often contain heuristics for setting these hyperparameters, which can be buried in the text - are these captured as well?\n\n[1] h4rm3l: A language for Composable Jailbreak Attack Synthesis. Moussa Koulako Bala Doumbouya, Ananjan Nandi, Gabriel Poesia, Davide Ghilardi, Anna Goldie, Federico Bianchi, Dan Jurafsky, Christopher D. Manning\n[2] Jailbroken: How Does LLM Safety Training Fail? Alexander Wei, Nika Haghtalab, Jacob Steinhardt."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jVC7DU7miL", "forum": "tX2mU5O0Ux", "replyto": "tX2mU5O0Ux", "signatures": ["ICLR.cc/2026/Conference/Submission9071/Reviewer_xhDQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9071/Reviewer_xhDQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633476950, "cdate": 1761633476950, "tmdate": 1762920781744, "mdate": 1762920781744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AGENTICPA, a three-agent system (Paper, Code, Repo) that automates the reproduction of prompt-based attacks on large language models. It extracts attack logic from research papers and repositories, generates executable scripts, and compiles them into a unified benchmark called AutoPABench. The framework reproduces 104 prior attack studies with an average execution success rate of 93 % and cost of $2 per paper, enabling scalable evaluation of LLM safety. Experiments show that recent models are more robust but still vulnerable to multilingual and indirect-injection prompts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The multi-agent framework (Paper, Repo, and Code Agents) is well-designed and demonstrates system-level engineering to automate complex attack reproductions.\n\n2. Enables large-scale, consistent reproduction of 100+ prompt-attack papers, addressing reproducibility and evaluation gaps in LLM safety research.\n\n3. The paper is clearly written and well-organized, making the methodology and results easy to follow."}, "weaknesses": {"value": "1. The definition of prompt-based attacks adopted in this paper seems overly broad, grouping together fundamentally different threat types such as jailbreaks, prompt injections, and prompt extractions. These attack families vary significantly in mechanism, reproducibility, and difficulty. Could the authors provide a more detailed analysis of how AGENTICPA handles each attack category, and clarify whether the framework adapts its reproduction or evaluation strategy accordingly?\n\n2. The paper collection stage still requires final human verification of associated GitHub repositories. It is unclear how human reviewers managed to filter and verify papers from the initial pool of 274,297. How many annotators were involved, what were their agreement rates, and what detailed criteria were used for manual judgment?\n\n3. The proposed framework’s safety layer and rollback mechanism are described conceptually but not quantitatively evaluated. Could the authors report how frequently these mechanisms are triggered and how they affect attack fidelity, success rate, and computational efficiency?\n\n4. A direct comparison with existing frameworks, such as AgentSecurityBench or AutoAdvExBench, is missing. How does AGENTICPA differ from these prior works in terms of scope, scalability, and robustness?\n\n5. The font size in several figures is too small, making them difficult to read. Could the authors improve figure readability or consider alternative visualizations?"}, "questions": {"value": "See my weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "luu1i70iGy", "forum": "tX2mU5O0Ux", "replyto": "tX2mU5O0Ux", "signatures": ["ICLR.cc/2026/Conference/Submission9071/Reviewer_Dff9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9071/Reviewer_Dff9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842140589, "cdate": 1761842140589, "tmdate": 1762920780944, "mdate": 1762920780944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AGENTICPA that automates the reproduction of prompt attacks on LLMs from academic research. The method employs a three-agent system: a paper agent extracts attack algorithms and evaluation criteria from publications; a repo agent analyzes associated code repositories for implementation details and handles data files; a code agent then synthesizes this information to write, execute, and debug scripts, creating standardized reproductions of the attacks. The method is then applied to generate the AutoPABench benchmark for assessing LLM safety."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Originality: The paper introduces a new multi-agent method to automate the traditionally manual task of reproducing LLM attacks from academic literature.\n* Clarity: The paper clearly articulates the problem, the roles and interactions within its three-agent architecture.\n* Significance: The evaluations show that it addresses a major bottleneck in LLM safety research."}, "weaknesses": {"value": "* The code agent synthesizes information from a theoretical paper and a practical code repository. However, the authors do not provide clear principle for how it resolves discrepancies between these two sources. This would create a significant risk of producing the attack that is faithful to neither the original paper's intent nor its actual implementation. This ambiguity undermines the claim of faithful reproduction. \n\n* The method's reliance on papers with accessible code repositories introduces a fundamental selection bias into the resulting AutoPABench. The benchmark does not represent the landscape of all published attacks, but only those that are already easy to reproduce. Conclusions drawn from this benchmark, such as any trends in attack efficacy or the prevalence of certain techniques, are not generalizable to the entire threat landscape. I am afraid this would even present an overly optimistic view of LLM defenses.\n\n* The method's debugging process would be a critical flaw. In its effort to create an executable script, the code agent may alter the original logic of an attack simply to make it run (would this happen?). This prioritizes functionality over scientific fidelity. A successful reproduction might pass a simple input-output check while operating on a different principle than the original attack, which renders it invalid as a true replication and corrupts the integrity of the benchmark."}, "questions": {"value": "* Regarding Weakness 1, what is the agent's hierarchy of truth when a discrepancy arises? Does it prioritize the repository's code, the paper's mathematical formulas, or the paper's descriptive text?\n\n* Regarding Weakness 2, what was the total number of attack papers initially identified for the study? what percentage of those were discarded due to the unusability of a code repository? have the authors performed any analysis on the characteristics of the excluded papers? do they differ systematically from the included papers in terms of attack type, novelty, or publication venue?\n\n* Regarding Weakness 3, what is the precise scope of the code agent's debugging powers? can it alter variable assignments, control flow, core function calls, etc., or is it just limited to some superficial fixes like library imports? how do the authors validate that a debugged script remains a faithful implementation of the original attack? is there any human-in-the-loop verification for reproductions that required code modifications? could the authors provide statistics on what proportion of the 104 successful reproductions required automated debugging, and what were the most common types of modifications the agent made?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j6UM5lOPQz", "forum": "tX2mU5O0Ux", "replyto": "tX2mU5O0Ux", "signatures": ["ICLR.cc/2026/Conference/Submission9071/Reviewer_8VbE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9071/Reviewer_8VbE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901627921, "cdate": 1761901627921, "tmdate": 1762920779518, "mdate": 1762920779518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors claim that:\n- Most LLM safety evaluation systems are limited in scope and only assess attacks in isolation or at a small scale.\n- It is not clear whether frontier models are robustly safe against the \"full spectrum\" of prompt attacks.\n- A proposed formalism mapping (Attack, Attack Mechanism) to (Adversarial Prompt) is sufficient to unify the \"full spectrum\" of attacks and consistently evaluate such attacks across different LLMs.\n\nThe authors propose:\n- Agentic Prompt Attack (Agentic PA) implementing the proposed formalism that is comprised of 3 agents:\n    - Paper Agent, which extracts attack specifications from research papers\n    - Repo Agent, which retrieves the implementation details of attacks from GitHub\n    - Code Agent, which generates a set of adversarial inputs for a given attack.\n\nUsing their framework, the authors reproduced 104 papers to build a library of adversarial prompts. Evaluating LLMs using these prompts, the authors found that recent frontier models are vulnerable to a wide range of known threats."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Reproduced 104 prompt attack papers to build a large-scale standardized library\n- Provided evidence that recent frontier models are vulnerable to a wide range of known threats\n- The idea of automatically standardizing jailbreak attacks from research papers and public code repositories is interesting.\n- The paper is well formated\n- This approach resulted in a library of adversarial prompts that could be useful for future work"}, "weaknesses": {"value": "- **The \"full spectrum of prompt attacks\" is not sufficiently specific**, as the full set of possible prompt attacks is infinite.\n- **The claim that large-scale evaluations have not been previously done is not accurate.** E.g. See [2] and [6]\n- **The claim that prior work only tests attacks in isolation and at a small scale is not substantiated:**\n    - [3] Showed that the composition of individual black box attacks can result in more successful attacks\n    - [2] Proposed a formal language to represent and compose attacks, implemented several primitives from the literature, and proposed a framework that synthesizes novel combinations of black-box attacks.\n- **The redefinition of the type of attacks covered is not justified.** These are black-box jailbreak attacks and are concretely string transformations, not strings.\n    - e.g., the low-resource translation attack translates the input prompt into an under-served language. The attack is the translation function, not its output for a particular input.\n- **The unification formalism is unclear and incomplete:**\n    - It doesn't distinguish attack mechanisms (e.g. transliterate the prompt in arabic [1]) from illicit requests (e.g., \"how to corrupt a mayor?\") in such a way that different illicit requests can be processed through an attack mechanism to generate their adversarial version.\n    - It doesn't allow computing attack success rates uniformly across multiple attacks as done by [1], [2], [3], [4] and [6] using standardazied databases of illicit requests such as [5]. In these prior works, attack success rates are computed as the average rate of target LLM misbehavior in response to illicit requests transformed by an attack mechanism (a string transformation), enabling the computation of ASRs for various attacks using the same set of illicit requests. The formalism proposed in the present work doesn't seem to enable such evaluation.\n\n- **This work doesn't seem to be sufficiently grounded in prior work**, which has proposed unification formalisms for black-box jailbreak attacks ([3], [2], [6]). These prior works define black-box attacks as string-to-string transformations. Prior work also discussed the composability of black-box attacks and a compositional language for representing and synthesizing black-box attacks. Those prior formalizations and unifications are not discussed in this paper or compared to the proposed approach.\nPrior work such as [4] has also evaluated white-box attacks using standardased datasets of illicit requests such as [5]. See Figure 16 in [6] for an illustration of the distinction between attacks and illicit requests (harmful prompts).\n\n\n\n\n# References\n- [1] Ghanim, M., Almohaimeed, S., Zheng, M., Solihin, Y., & Lou, Q. (2024, November). Jailbreaking LLMs with Arabic Transliteration and Arabizi. In Proceedings of the 2024 Conference - on Empirical Methods in Natural Language Processing (pp. 18584-18600).\n- [2] Doumbouya, M. K. B., Nandi, A., Poesia, G., Ghilardi, D., Goldie, A., Bianchi, F., ... & Manning, C. D. h4rm3l: A Language for Composable Jailbreak Attack Synthesis. In The - Thirteenth International Conference on Learning Representations.\n- [3] Wei, A., Haghtalab, N., & Steinhardt, J. (2023). Jailbroken: How does llm safety training fail?. Advances in Neural Information Processing Systems, 36, 80079-80110.\n- [4] Li, Qizhang, Yiwen Guo, Wangmeng Zuo, and Hao Chen. \"Improved generation of adversarial examples against safety-aligned llms.\" Advances in Neural Information Processing Systems - 37 (2024): 96367-96386.\n- [5] Mazeika, M., Phan, L., Yin, X., Zou, A., Wang, Z., Mu, N., ... & Hendrycks, D. (2024, July). HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust - Refusal. In International Conference on Machine Learning (pp. 35181-35224). PMLR.\n- [6] Sharma, M., Tong, M., Mu, J., Wei, J., Kruthoff, J., Goodfriend, S., ... & Perez, E. (2025). Constitutional classifiers: Defending against universal jailbreaks across thousands of hours of red teaming. arXiv preprint arXiv:2501.18837."}, "questions": {"value": "- Why does the Paper Agent encode asset files in base64? Is this purely for the purpose of serializing images into text so that it can be processed through text-to-text LLM interfaces? Is there evidence of reliable LLM processing of base64 encoded images?\n- It is unclear how the Repo Agent validates code from repositories.\n- How are jailbreak attacks represented? Are they merely a list of adversarial inputs? Or string transformations?\n- What are executable attack scripts? Such scripts are not jailbreak attacks, but programs that run a set of \"jailbreak prompts.\"\n- Many of the papers listed in Table 9 are not aligned with the proposed definition of \"prompt attacks\" as aiming to \"deliver adversarial prompts to target LLMs.\" E.g. Black-box jailbreak attacks are transformations (e.g., obfuscation, low-resource translation, or the addition of a prefix-suffix, etc., or combinations thereof) that cause an LLM to generate a harmful response to a prompt for which it would normally issue a refusal response due to safety purposes.  How do the authors unify their proposed formalism and the prior formalism?\n- Are the reported ASRs computed using the same set of illicit requests? If not, how is their comparison across attacks justified?\n- The authors referenced [2], which released a library of 2,656 jailbreak attacks expressed in a formal language (h4rm3l). How were those integrated in this work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0r3hFMzTK7", "forum": "tX2mU5O0Ux", "replyto": "tX2mU5O0Ux", "signatures": ["ICLR.cc/2026/Conference/Submission9071/Reviewer_aPsS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9071/Reviewer_aPsS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044631638, "cdate": 1762044631638, "tmdate": 1762920778848, "mdate": 1762920778848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}