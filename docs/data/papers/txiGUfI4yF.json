{"id": "txiGUfI4yF", "number": 19694, "cdate": 1758298479567, "mdate": 1759897025558, "content": {"title": "Latent Stochastic Interpolants", "abstract": "Stochastic Interpolants (SI) are a powerful framework for generative modeling, capable of flexibly transforming between two probability distributions. However, their use in jointly optimized latent variable models remains unexplored as they require direct access to the samples from the two distributions. This work presents Latent Stochastic Interpolants (LSI) enabling joint learning in a latent space with end-to-end optimized encoder, decoder and latent SI models. We achieve this by developing a principled Evidence Lower Bound (ELBO) objective derived directly in continuous time.\nThe joint optimization allows LSI to learn effective latent representations along with a generative process that transforms an arbitrary prior distribution into the encoder-defined aggregated posterior.\nLSI sidesteps the simple priors of the normal diffusion models and mitigates the computational demands of applying SI directly in high-dimensional observation spaces, while preserving the generative flexibility of the SI framework.\nWe demonstrate the efficacy of LSI through comprehensive experiments on the standard large scale ImageNet generation benchmark.", "tldr": "Novel formulation for joint training with stochastic interpolants in a latent space enabling simultaneous learning of encoder, decoder and latent space generative model.", "keywords": ["Generative Models", "Stochastic Interpolants", "Flow Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/effcdbfa734fa40ea2b516b08ccf8b733948a016.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The key idea of this paper is to jointly train an encoder, decoder, and latent stochastic interpolant model using a continuous-time ELBO. Unlike traditional SI, which requires direct access to samples from both prior and target distributions, LSI constructs interpolants in the latent space, allowing end-to-end optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides generalization of stochastic interpolants into latent spaces, enabling joint encoder–decoder–generator learning.\n\n2. The derivation of a continuous-time ELBO in latent space seems theoretically grounded.\n\n3. Demonstrates comparable or better FIDs across multiple resolutions on ImageNet"}, "weaknesses": {"value": "The assumptions of linear SDEs and Gaussian posteriors may limit its expressivity. It’s unclear how much these approximations affect performance or generalization."}, "questions": {"value": "This paper is well motivated and well presented. I do not have many questions, but I am curious about whether the method could be extended to a learnable prior, also formulated within the joint learning scheme? The proposed method mentioned \"arbitrary prior\", which, however, are mainly simple, known distributions (e.g., Gaussian, Laplace). Would you also consider a learnable prior (e.g., an EBM prior or another more sophisticated prior)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LnfKs93cPu", "forum": "txiGUfI4yF", "replyto": "txiGUfI4yF", "signatures": ["ICLR.cc/2026/Conference/Submission19694/Reviewer_xrNR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19694/Reviewer_xrNR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761412210019, "cdate": 1761412210019, "tmdate": 1762931535553, "mdate": 1762931535553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Latent Stochastic Interpolants (LSI), which extend stochastic interpolants into a jointly trained latent-variable model by learning the encoder, decoder, and latent dynamics together through a continuous-time ELBO. The method gives a nice unifying view of SI inside latent models, keeps the flexibility of arbitrary priors, and preserves simulation-free training, while avoiding heavy computation in pixel space. I haven’t checked every technical detail, but the approach feels clean, scalable, and grounded in solid continuous-time modeling ideas. It seems to address a hard and meaningful problem in a practical way, and the flexibility to apply this direction beyond ImageNet is the kind of capability I’d like to see more of at ICLR."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* This paper tackles a non-trivial and meaningful problem in generative modeling.\n\n* The approach of using joint training of encoder + latent dynamics + decoder feels principled and elegant.\n\n* Flexible priors and continuous-time formulation are nice advantages.\n\n* Experiments on ImageNet show competitive performance, and this general framework could extend to many domains.\n\n* The paper is generally well written and easy to follow."}, "weaknesses": {"value": "* Evaluation is mostly on ImageNet, so the broader impact still needs to be validated.\n\n* It’s not fully clear how robust the training is across architectures and hyperparameters.\n\n* The paper is heavily reliant on the supplementary materials and mathematical details.\nThis makes the paper less accessible to the readers who are not interested in going through the theoretical details.\nThe authors could provide the key insights in a more intuitive way, possibly using graphical illustrations."}, "questions": {"value": "* How sensitive is performance to design choices in the latent SDE or encoder noise scale?\n\n* Any results (even preliminary) on other modalities or conditional settings?\n\n* Please provide some failure examples, and discuss the limitations of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t2jOzqGfz9", "forum": "txiGUfI4yF", "replyto": "txiGUfI4yF", "signatures": ["ICLR.cc/2026/Conference/Submission19694/Reviewer_UcdG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19694/Reviewer_UcdG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762078132735, "cdate": 1762078132735, "tmdate": 1762931534646, "mdate": 1762931534646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a latent stochastic interpolant method, where they aim to learn a generative model with a low-dimensional feature. To ensure the condition of the stochastic interpolant, they come up with a novel parameterization of the stochastic interpolant, conditional on the sample $z_0$ and the encoded output $z_1$. They further optimize the training objectives for reducing the variance in the training and improving the performance. The experiments were conducted on ImageNet and show noticeable improvements in the computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The goal of jointly optimizing the encoder and the generative model is challenging. And the authors' proposal seems to help address the problem.\n\nThe experiments on varying initial distributions show that the method might help to be used in the case where the Gaussian might not be ideal.\n\nThe overall presentation is clear."}, "weaknesses": {"value": "The authors should consider giving a more detailed benchmark, including more models with a pre-trained encoder, and discuss the advantages over the comparison methods.\n\nIt would be good if the encoder is evaluated for its linear probing accuracy, since it's very useful to see if the encoder is meaningful or not."}, "questions": {"value": "Could you show more results regarding the encoder's performance? How does it perform when you train the interpolant model using varying initial conditions and training objectives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ViOBHq0boW", "forum": "txiGUfI4yF", "replyto": "txiGUfI4yF", "signatures": ["ICLR.cc/2026/Conference/Submission19694/Reviewer_TLxQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19694/Reviewer_TLxQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762165243068, "cdate": 1762165243068, "tmdate": 1762931533434, "mdate": 1762931533434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}