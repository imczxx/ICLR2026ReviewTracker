{"id": "444mACDffR", "number": 17628, "cdate": 1758278551777, "mdate": 1759897163914, "content": {"title": "NTK with Convex Two-Layer ReLU Networks", "abstract": "We theoretically analyze a convex variant of two-layer ReLU neural networks and how it relates to the standard formulation. We show that the formulations are equivalent with respect to their output values for a fixed dataset and also behave similarly during gradient-based optimization as long as the weights on the first layer of standard networks do not change too much, which is a common assumption for their convergence to an arbitrarily good solution.\nWe further show that for any two-layer ReLU neural network, even considering those of infinite width, there exists a (weighted) network of width $O(n^{d-1})$ with the same output value on all data points. Furthermore, these finite networks have exactly the same eigenvalues $\\lambda$ of their neural tangent kernel (NTK) matrix and the same NTK separation margin $\\gamma$ as in the infinite width limit.\nWe give an approximation algorithm for the separation margin $\\gamma$ and two data examples: 1) a circular example for which we strengthen an $\\Omega(\\gamma^{-2})$ lower bound against the previous worst-case width analyses; 2) a hypercube example that can be perfectly classified by the convex network formulation but not by any standard network, distinguishing their expressibility.", "tldr": "We analyze a convex formulation of two-layer ReLU neural networks that is nearly equivalent to the standard formulation and simplifies theoretical analyses.", "keywords": ["two-layer ReLU network theory", "NTK", "network width", "separation margin", "convex optimization"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d9c2a8568dc5158156be085a6c313021cecc64f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors use a formulation of shallow relu networks called ‘convex networks’ which decouple where neurons turn on and off from their actual numeric outputs. The activation region layout is thus fixed at initialization, and only the neuron outputs are trained. The convex network formulation makes it so that data points do not change activation regions during the course of training, so that the optimization problem becomes convex in parameter space. This nice property allows the authors to establish when infinite width networks could be replaced by finite width networks and still maintain the NTK separation margin."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors are extensively aware of the related literature\n2. Use of the convex network formulation is a clever way to make results tractable, while also being interesting in its own right.\n3. The authors present an impressive variety of results."}, "weaknesses": {"value": "1. 43 up to a factor OF two\n2. 148 ‘this motivates to’\n3. 190 did you mean to say ‘at initialization’ rather than ‘at activation’?\n4. 229 improve should be improves\n5. 235 should say (omitting parameters other than gamma)\n6. The paper does a lot of things, but that might work to its detriment, for example, the contributions section has 8 items in it, but not all of them feel like they’re the central point of the paper.\n7. The paper is very dense and symbol heavy with no figures to break it up. Although the main content of the paper is mathematical, the example data problems at least could probably benefit from figures?\n8.  Perhaps the paper could dedicate a little more room to motivation. For example, why is it important to have width bounds for fitting training data when the ultimate goal is generalization? Or if the NTK separation margin and minimum eigenvalue are the same for a finite sized network as they are in the infinite case, do we know that the finite sized network would generalize as well as the infinite? Maybe each result in the main body could be accompanied by a bit more of an explanation of why it’s fascinating or related to getting better performance out of neural networks."}, "questions": {"value": "1. Are the citations you list around line 50 covering both why GD converges to arbitrarily small errors in nonconvex optimization and convergence results for  overparameterized networks? Are those two sentences supposed to be two disjoint sets of citations or are they related? Or was the first sentence just meant to introduce the related works and not be followed by citations?\n2. It’s shown that ordinary relu networks can classify a dataset equivalently, but convex networks seem like they’re able to make discontinuous functions since neurons can ‘turn on’ to nonzero values. If the goal of the network is ultimately to generalize, might that matter that convex networks might learn discontinuous functions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "blpDFLwkW7", "forum": "444mACDffR", "replyto": "444mACDffR", "signatures": ["ICLR.cc/2026/Conference/Submission17628/Reviewer_cx59"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17628/Reviewer_cx59"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761424194051, "cdate": 1761424194051, "tmdate": 1762927489244, "mdate": 1762927489244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new convex approximation for two-layer ReLU networks. The authors prove that their formulation is equivalent to the standard neural networks, thus allowing them to benefit from the common convex optimization analysis of the convergence. Overall, such a formulation is supposed to make the analysis of two-layer ReLU networks more intuitive and bridge some gaps in the existing worst/base case bounds.  Among the main results the authors provide is also a way to estimate the eigenvalue of the NTK in a sound way. Finally, several examples of tasks where the convex formulation is superior to the traditional one are shown."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tNovel study on the convex formulation of two-layer ReLU networks\n2.\tNTK perspective on the established equivalence and the derived proximity of the optimization dynamics. \n3.\tExamples of when the convex formulation can solve tasks that cannot be provably solved by the traditional two-layer ReLU network."}, "weaknesses": {"value": "1.\tVery dense paper that is hard to read\n2.\tMany key results are not compared to prior work\n3.\tThe importance of certain contributions is not clear"}, "questions": {"value": "1.\tWhy did the authors choose a different name for the object of study? In the manuscript, they mention that they study gated ReLU networks, but call them convex two-layer ReLU networks.\n2.\tCould the authors provide a clear breakdown of their contributions when compared to prior works? While the authors cite relevant works (to the best of my knowledge), it is not immediately obvious to find a direct comparison to the results presented in them. Ideally, a reader would like to see a clear distinction after each major result, comparing it to existing results and/or explaining it novelty. Otherwise, the reader has to go through all cited papers to actually assess the importance of the authors’ findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AMiyNLo3Me", "forum": "444mACDffR", "replyto": "444mACDffR", "signatures": ["ICLR.cc/2026/Conference/Submission17628/Reviewer_tse3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17628/Reviewer_tse3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930526866, "cdate": 1761930526866, "tmdate": 1762927488698, "mdate": 1762927488698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper theoretically analyzes a convex variant of two-layer ReLU neural networksand establishes its close relationship to the standard, non-convex formulation. The key idea is to \"convexify\" the network by decoupling the neuron's activation pattern (determined by fixed \"orientation\" vectors) from its learned weight. This simplifies the optimization landscape, making the training problem convex, while retaining much of the expressive power of standard networks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper theoretically analyzes a convex variant of two-layer ReLU neural networksand establishes its close relationship to the standard, non-convex formulation."}, "weaknesses": {"value": "In my view, this paper could better isolate and clarify its key contributions to make them easier to grasp. Below are some of my concerns, which may be partially due to my unfamiliarity with the relevant literature.\n\nThe authors review certain aspects of convex two-layer neural networks and present some preliminary results. While these findings are somewhat interesting, they are not sufficiently compelling to convince me of the paper's overall strength.\n\nSubsequently, the authors analyze the separation margin and the smallest eigenvalue of the NTK, claiming that when m=|S_{0}|, these quantities equal those of the convex neural network. I am unclear on the purpose or significance of this result.\n\nFinally, they demonstrate that for some datasets, the required network width to approximate the NTK can be improved. If this is the major result, it should be stated explicitly and prominently at the beginning of the paper."}, "questions": {"value": "same to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UcJrvUZ2PJ", "forum": "444mACDffR", "replyto": "444mACDffR", "signatures": ["ICLR.cc/2026/Conference/Submission17628/Reviewer_trQH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17628/Reviewer_trQH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997351137, "cdate": 1761997351137, "tmdate": 1762927488158, "mdate": 1762927488158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper theoretically analyzes a convex variant of two-layer ReLU neural networksand establishes its close relationship to the standard, non-convex formulation. The key idea is to \"convexify\" the network by decoupling the neuron's activation pattern (determined by fixed \"orientation\" vectors) from its learned weight. This simplifies the optimization landscape, making the training problem convex, while retaining much of the expressive power of standard networks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper theoretically analyzes a convex variant of two-layer ReLU neural networksand establishes its close relationship to the standard, non-convex formulation."}, "weaknesses": {"value": "In my view, this paper could better isolate and clarify its key contributions to make them easier to grasp. Below are some of my concerns, which may be partially due to my unfamiliarity with the relevant literature.\n\nThe authors review certain aspects of convex two-layer neural networks and present some preliminary results. While these findings are somewhat interesting, they are not sufficiently compelling to convince me of the paper's overall strength.\n\nSubsequently, the authors analyze the separation margin and the smallest eigenvalue of the NTK, claiming that when m=|S_{0}|, these quantities equal those of the convex neural network. I am unclear on the purpose or significance of this result.\n\nFinally, they demonstrate that for some datasets, the required network width to approximate the NTK can be improved. If this is the major result, it should be stated explicitly and prominently at the beginning of the paper."}, "questions": {"value": "same to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UcJrvUZ2PJ", "forum": "444mACDffR", "replyto": "444mACDffR", "signatures": ["ICLR.cc/2026/Conference/Submission17628/Reviewer_trQH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17628/Reviewer_trQH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997351137, "cdate": 1761997351137, "tmdate": 1763719638283, "mdate": 1763719638283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates a convex counterpart of a two-layer ReLU network and proves this convex variant shares similar properties with the original ReLU network such as the NTK matrices having the same eigenspectra as well as the same NTK margin. Additionally, the paper constructs a data distribution where any perfect NTK separator with width $O(\\frac{1}{\\gamma^2})$ must have weights dependent on initializion, improving the lower bound construction from prior work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The construction of a dataset where a perfect NTK separator with subquadratic (with respect to the reciprocal of the margin) width must have weights dependent on initialization illuminates the difficulty of getting subquadratic width guarantees. \n\nIn addition, showing that GD on the convex variant converges arbitrarily close to the NTK margin is interesting as it provides an algorithm for computing such margins, something that was not known beyond special cases such as parity problems."}, "weaknesses": {"value": "The convex variant of the ReLU networks seems quite tied to the 2-layer case and it does not seem easy to generalize this formulation to multi-layer ReLU networks, much less architectures with attention.\n\nFurthermore, the convex formulations behave similarly to their original counterparts during training with GD only when the inner layer doesn't change much. In contrast, [1] analyzes GD on two-layer ReLU networks while also allowing for quite a bit of movement $O(\\gamma \\sqrt{m})$.\n\n[1] Telgarsky, M. (2022). Feature selection with gradient descent on two-layer networks in low-rotation regimes. arXiv preprint arXiv:2208.02789."}, "questions": {"value": "1. Are there any toy datasets where there exists linear width perfect NTK separator with initialization dependent weights but lack such a separator when the weights are initialization invariant?\n \n2. Please provide comparison with [1] (specifically theorem 2.1 and 2.2). My main issue with the paper is the fact that the analysis of GD on the convex variant is only really useful when the inner layer doesn't move much. But [1] handles $O(\\gamma \\sqrt{m})$ amount of movement while also analyzing GD on the original formulation.\n\n3. It seems the approximation algorithm for computing $\\gamma_V$ (i.e. lemma 6.1) requires the knowledge of $\\gamma_V$ as one needs to choose $B$ to be sufficiently large enough. Can you remove this circular dependency? I guess a doubling trick should work but that could be expensive if $gamma_V$ is tiny (e.g. if $\\gamma_V$ is exponentially small with respect to dimension).\n\n\n[1] Telgarsky, M. (2022). Feature selection with gradient descent on two-layer networks in low-rotation regimes. arXiv preprint arXiv:2208.02789."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vgkfb8I1rE", "forum": "444mACDffR", "replyto": "444mACDffR", "signatures": ["ICLR.cc/2026/Conference/Submission17628/Reviewer_EEBd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17628/Reviewer_EEBd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762770365204, "cdate": 1762770365204, "tmdate": 1762927487703, "mdate": 1762927487703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}