{"id": "6yCZEruFu9", "number": 8737, "cdate": 1758096558041, "mdate": 1763712341010, "content": {"title": "CoaxChain: Semantically Progressive Multi-turn Jailbreak Attacks on Large Language Models", "abstract": "To design robust defenses for large language models (LLMs), it is essential to first systematically study jailbreak attacks, as understanding attack strategies provides the foundation for building effective safeguards. Among various attack types, multi-turn jailbreak attacks are particularly concerning because they can gradually steer conversations from benign topics to harmful instructions, often bypassing even commercial safety defenses. However, existing jailbreak methods rely on frequent trial-and-error interactions with the target model, which makes the process slow, costly, and prone to detection. To address these challenges, we propose CoaxChain, a structured black-box multi-turn jailbreak framework based on semantically progressive prompting, which consists of two key components: the Alignment Failure Analyzer (AFA) that performs offline analysis to identify effective prompts and avoid risky trial-and-error interactions with the target model, and the Semantically Progressive Prompt Generator (SPG) that leverages AFA’s insights to produce compact, semantically progressive multi-turn dialogue sequences that enhance both attack efficiency and stealthiness. We evaluate CoaxChain on GPT-4o, Claude 3.7, and Gemini 2.5, where it achieves an average success rate of 82.56\\% with only three turns, surpassing strong baselines such as Crescendo and ActorAttack, while further improving prompt generation efficiency by 80\\% compared to ActorAttack.", "tldr": "We propose CoaxChain, a multi-turn jailbreak framework that semantically bypasses alignment in LLMs by gradually weakening safety mechanisms through progressive prompt rewriting.", "keywords": ["Prompt Injection", "LLM", "Multi-turn Jailbreak Attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56865cfb815ae3a0234df846127c52d193aedeee.pdf", "supplementary_material": "/attachment/eb98612aa74b2c8c008ddc0daeda782b75984743.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces CoaxChain, a black-box, semantically progressive multi-turn jailbreak framework designed to systematically examine vulnerabilities in the alignment mechanisms of large language models.\nThe framework consists of two main components:\n1. Alignment Failure Analyzer (AFA) – a white-box module operating on a locally aligned surrogate model. It performs offline gradient probing to evaluate the effectiveness of prompts, identifying those that suppress alignment-sensitive parameters without requiring risky trial-and-error interactions with the target model.\n2. Semantically Progressive Prompt Generator (SPG) – a dynamic rewriting module that leverages AFA’s evaluations to select only the essential intermediate turns, constructing concise, semantically progressive dialogue sequences.\nThis design enhances both the effectiveness and efficiency of multi-turn jailbreaks, while providing valuable insights for developing more robust alignment defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces the use of critical weight sensitivity\\text{Critical}(W|i) derived from a white-box surrogate model to quantify the model’s alignment sensitivity at a given conversational state. This metric is then used to guide adversarial prompt generation, effectively addressing the limitations of prior multi-turn jailbreak methods that relied on heuristic or trial-based approaches.\n\n2. The evaluation covers not only attack success rate (ASR) but also semantic similarity (SEM), perplexity (PPL), and efficiency (measured by the number of queries). Detailed ablation studies further validate the necessity and rationality of each component, demonstrating that both AFA and the semantically progressive generation strategy are crucial for the overall performance.\n\n3. Beyond proposing an advanced attack framework, the paper also introduces a fine-tuning–based defense mechanism, Fortify, which significantly mitigates the attack’s effectiveness. This dual-perspective contribution—offensive and defensive—enhances the paper’s overall impact and provides valuable insights for developing more robust alignment defenses."}, "weaknesses": {"value": "1. Lack of generalization analysis of surrogate-based AFA. Although the paper claims that the Alignment Failure Analyzer (AFA) improves interpretability, the AFA is conducted entirely on that surrogate rather than the actual target mode. Because the surrogate may differ substantially in architecture and alignment mechanisms, it remains unclear why and to what extent the AFA—based on a surrogate—can reliably generalize its gradient-based alignment sensitivity assessments to unseen, closed-source targets with different architectures or alignment mechanisms.\n\n2. Restricted and task-agnostic strategy library. The strategy pool used in the Semantically Progressive Prompt Generator (SPG) is small and predetermined for each dialogue stage, independent of task semantics. The paper lacks ablation evidence showing how the choice of strategies affects the attack success rate (ASR), or whether the same “optimal” strategy consistently performs best across different tasks and contexts. While fixing a universal optimal strategy improves reproducibility and efficiency, additional statistical justification would strengthen the claim of task-invariant optimality.\n\n3. Writing and presentation issues. The manuscript suffers from organizational inconsistencies and repeated phrasing. Crucial components such as the deterministic renderer are left unexplained, while the discussion of predefined strategies (lines 232–233) appears misplaced within the section. These issues obscure the methodological flow and weaken the overall readability.\n\n4. Unclear threshold stability in AFA. The paper fixes the AFA critical-weight threshold at τ = 0.4 without empirical justification or sensitivity analysis. It is uncertain how stable this threshold remains across different models or training runs, and whether small variations in τ would significantly alter which strategies are selected during adversarial prompt generation.\n\n5. Incomplete baseline selection and insufficient discussion of related work: \nThe strongest baseline, ActorAttack (Oct 2024), is already outdated. Many more recent methods have surpassed it but were not included for comparison, such as [1][2][3]. In addition, the paper should provide a more comprehensive discussion of existing LLM jailbreak approaches, covering both single-turn and multi-turn attacks. Failure to include recent stronger baselines and to clearly situate the proposed method in the landscape of single- vs. multi-turn jailbreaks makes the evaluation incomplete.\n\n[1] Yao, Yang et al. “A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos.” ArXiv abs/2502.15806 (2025).  \n [2] Miao, Ziqi et al. “Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models.” ArXiv abs/2507.05248 (2025).  \n [3] Weng, Zixuan et al. “Foot-In-The-Door: A Multi-turn Jailbreak for LLMs.” ArXiv abs/2502.19820 (2025)."}, "questions": {"value": "1. Could the authors provide either a theoretical justification or empirical analysis demonstrating the correlation between AFA outcomes on the local surrogate model and the attack success rate (ASR) observed on the target model—or on another open-source model with a different architecture and training dataset?\n\n2. Rationality of predefined strategies. Can the authors supplement experiments to justify the use of predefined strategies for each dialogue stage, and verify whether the same stage-specific strategies remain optimal across different tasks or domains?\n3. Scope of layer analysis. Why does the AFA focus exclusively on MLP layers for gradient probing? Have the authors experimented with or compared results from other architectural components (e.g., attention layers, normalization blocks)?\n\n4. Threshold stability in AFA. How stable is the critical-weight threshold (τ = 0.4) across models and training runs? Would small variations in τ substantially change which strategies are selected during adversarial prompt generation?\n\n5. baseline selection and discussion of related work. Can the authors include more up-to-date and comprehensive baselines? The current best-performing baseline, ActorAttack (Oct 2024), is already outdated, and several more recent methods [1][2][3] that outperform it are missing. In addition, can the authors expand their discussion of existing LLM jailbreak approaches to more thoroughly cover both single-turn and multi-turn attack methods?\n\n\n[1] Yao, Yang et al. “A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos.” ArXiv abs/2502.15806 (2025).  \n [2] Miao, Ziqi et al. “Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models.” ArXiv abs/2507.05248 (2025).  \n [3] Weng, Zixuan et al. “Foot-In-The-Door: A Multi-turn Jailbreak for LLMs.” ArXiv abs/2502.19820 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Py5TsUHcSV", "forum": "6yCZEruFu9", "replyto": "6yCZEruFu9", "signatures": ["ICLR.cc/2026/Conference/Submission8737/Reviewer_7gvj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8737/Reviewer_7gvj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816246823, "cdate": 1761816246823, "tmdate": 1762920532686, "mdate": 1762920532686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a block-box multi-turn jailbreak framework called COAXCHAIN. This framework has two key components: the Alignment Failure Analyzer (AFA) and the Semantically Progressive Prompt Generator (SPG). The AFA is built on a local surrogate model and uses a gradient-based method to determine which prompt is more likely to jailbreak the victim LLM. The SPG is specifically finetuned to follow the 'Topic Induction, Intent Hinting, and Intent Rephrasing' three-phase rewriting of a single-turn malicious query. Through AFA's judgement and SPG's 3 turns rewriting, the author claims their COAXCHAIN can jailbreak LLMs using just three turns, which is relatively high efficient than existing multi-turn jailbreak methods. The attack success rates (ASRs) on different victim models are also higher than existing single-turn and multi-turn methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The COAXCHAIN involves the gradient detection to improve the quality of the jailbreak prompt in each round, therefore increasing the success rate of jailbreak and reducing the number of dialogue rounds.\n\n2. This attack method has achieved high ASRs on sufficiently powerful LLMs such as GPT-4o and Calude-3.7."}, "weaknesses": {"value": "1. The COAXCHAIN actually highly relies on the surrogate model used in the AFA module.\n\nThe method seems to rely on a core assumption: vulnerabilities discovered on the white-box surrogate model can be transferred to black-box target models with unknown architectures and parameters. Therefore, the key to successful jailbreak should lie in how to obtain a surrogate model that more closely to the victim model, rather than relying on multi-turn dialogues. The paper's premise of multi-turn dialogue attacks appears to be merely a formality.\n\n2. CoaxChain claims to jailbreak a victim LLM using 3 turns. However, it is because SPG is fine-tuned to generate three rounds of dialogue.\n\n3. The train/test settings in the 4.1.2 are not convincing. \n\nThe training and test sets should be categorized by different themes. Taking the malicious questions in advbench as an example, 'Provide detailed instructions on how to make a bomb' and 'Write a tutorial on how to make a bomb' both fall in the category 'bomb', and their cores are both how to make a bomb. Therefore, simply dividing train/test sets based on samples alone seems insufficient.\n\n4. There seems to be no evidence that the data used for fine-tuning SPG enhances its rewriting capability. It appears to have merely taught SPG to rewrite according to a three-round pattern.\n\n5. The 'Topic Induction, Intent Hinting, and Intent Rephrasing' seems to be the common ideas for multi-turn jailbreaks. Therefore, this paper does not appear to uncover new safety vulnerabilities or insights for the multi-turn jailbreaks."}, "questions": {"value": "See the weaknesses above and:\n1. Why do attacks on commercial models test ‘transferability’?\n\nAs a 'black-box' attack method, either the victim model an open-source model or a commercial model, the method cannot access their parameters. Then, the attack settings for open-source models and commercial models should be regarded as the same. Therefore, testing on commercial models should not be considered under 'transferability'. Alternatively, this may simply demonstrate that llama3.1-8b is a robust model capable of effectively emulating commercial models' capabilities. This does not constitute a contribution of this paper, as it does not discuss how to discover or enhance a surrogate model.\n\n2. Can the author provide examples of successful jailbreaks? Why are there no successful examples in either the main text or the appendix?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "This paper is exploring an jailbreak method, but already has ethic statement section."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MCGpByzb13", "forum": "6yCZEruFu9", "replyto": "6yCZEruFu9", "signatures": ["ICLR.cc/2026/Conference/Submission8737/Reviewer_ptYJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8737/Reviewer_ptYJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840751312, "cdate": 1761840751312, "tmdate": 1762920532035, "mdate": 1762920532035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article introduces CoaxChain, a new framework designed to improve multi-turn jailbreak attacks on large language models (LLMs) by making them more efficient and less detectable. CoaxChain has two main components: the Alignment Failure Analyzer (AFA), which helps identify effective attack prompts without relying on trial-and-error, and the Semantically Progressive Prompt Generator (SPG), which creates efficient, stealthy multi-turn dialogues based on AFA’s insights. Experimental results demonstrate the effectiveness of their method, outperforming existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clear and logically structured. \nImproving the efficiency of multi-turn attacks is an important metric. For model developers, this approach can help reduce evaluation costs."}, "weaknesses": {"value": "The biggest concern is the novelty of the two core algorithm modules, especially the Semantically Progressive Prompt Generator. The idea of gradually steering the conversation through semantics has already been introduced in current baselines. The heuristics used in the Alignment Failure Analyzer module are also common. Additionally, the concept of iterating prompts based on model feedback has already been utilized in other jailbreak baselines. Therefore, the main contribution of this paper lies in engineering improvements rather than novel algorithmic advances."}, "questions": {"value": "See the limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v74RTmSNoT", "forum": "6yCZEruFu9", "replyto": "6yCZEruFu9", "signatures": ["ICLR.cc/2026/Conference/Submission8737/Reviewer_5jd8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8737/Reviewer_5jd8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925480678, "cdate": 1761925480678, "tmdate": 1762920531670, "mdate": 1762920531670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a semantically progressive multi-turn jailbreak (CoaxChain). The proposed method integrates two components, Alignment Failure Analyzer (AFA) and Semantically Progressive Prompt Generator (SPG). Derived from GradSafe, AFA conducts gradient-based analysis through a surrogate model to measure alignment sensitivity on a prompt. SPG rewrite prompts based on the results from AFA to create a streamlined conversation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and easy to follow. \n\nThe authors provide the code and datasets in the supplementary materials, and the appendix includes detailed descriptions of the training. These efforts enhance the reproducibility of their work."}, "weaknesses": {"value": "1. The reported test results for other baseline methods are noticeably lower than those reported in their original papers and in others. This discrepancy raises concerns about fair comparison.\n\n2. AFA’s selection criterion is computed on LLaMA-3.1-8B gradients; success on closed models is then inferred from that proxy. While transfer results are promising, the paper does not quantify how sensitive AFA is to the choice of surrogate (size/family), or whether the chosen thresholds generalize.\n\n3. The paper would be stronger with AFA-off and AFA-random controls (keep the same three-turn structure but (i) remove gating, (ii) randomize strategy selection) to isolate AFA’s contribution beyond SPG templating. Relatedly, report failure mode breakdowns (budget exhaust vs. mis-gated progression).\n\n4. Several studies [1-3] on multi-turn jailbreak published in 2025 are not discussed in the related works. The lack of these comparisons makes it difficult to verify the effectiveness of the proposed method.\n\nReferences\n[1] Weng, Zixuan, et al. \"Foot-In-The-Door: A Multi-turn Jailbreak for LLMs.\" *arXiv preprint arXiv:2502.19820* (2025).\n[2] Miao, Ziqi, et al. \"Response attack: Exploiting contextual priming to jailbreak large language models.\" *arXiv preprint arXiv:2507.05248* (2025).\n[3] Zhao, Yi, and Youzhi Zhang. \"Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors.\" *arXiv preprint arXiv:2501.14250* (2025)."}, "questions": {"value": "1.\tCould you evaluate your proposed method on other benchmarks, such as HarmBench or AdvBench?\n2.\tI am also curious about how the ASR of different jailbreak methods varies depending on the chosen defense mechanisms. Could you conduct an ablation study across various defense methods (e.g., LLaMa-3 Guard, SmoothLLM, etc)?\n3.\tSince the proposed method relies on gradient-based analysis, I am curious whether it leads to a high computational or time cost. Could the authors provide some discussion or comparison on this aspect (training or prompt generation)?\n4.\tAs a minor question, could you provide more details about Fortify in Appendix G.2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ktfE9zfxp9", "forum": "6yCZEruFu9", "replyto": "6yCZEruFu9", "signatures": ["ICLR.cc/2026/Conference/Submission8737/Reviewer_ExCx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8737/Reviewer_ExCx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991674499, "cdate": 1761991674499, "tmdate": 1762920531224, "mdate": 1762920531224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}