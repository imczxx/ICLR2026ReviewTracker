{"id": "4aZTiLH3fm", "number": 10043, "cdate": 1758157846583, "mdate": 1759897678744, "content": {"title": "TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding", "abstract": "Modeling semantic and structural information from tabular data remains a core challenge for effective table understanding. Existing Table-as-Text approaches flatten tables for large language models (LLMs), but lose crucial structural cues, while Table-as-Image methods preserve structure yet struggle with fine-grained semantics. Recent Table-as-Multimodality strategies attempt to combine textual and visual views, but they (1) statically process both modalities for every query-table pair within large multimodal LLMs (MLLMs), inevitably introducing redundancy and even conflicts, and (2) depend on costly fine-tuning of MLLMs.  In light of this, we propose TableDART, a training-efficient framework that integrates multimodal views by reusing pretrained single-modality models. TableDART introduces a lightweight 2.59M-parameter MLP gating network that dynamically selects the optimal path (either Text-only, Image-only, or Fusion) for each table–query pair, effectively reducing redundancy and conflicts from both modalities. In addition, we propose a novel agent to mediate cross-modal knowledge integration by analyzing outputs from text- and image-based models, either selecting the best result or synthesizing a new answer through reasoning. This design avoids the prohibitive costs of full MLLM fine-tuning. Extensive experiments on seven benchmarks show that TableDART establishes new state-of-the-art performance among open-source models, surpassing the strongest baseline by an average of 4.02%. The code is available at: https://anonymous.4open.science/r/TableDART-C52B.", "tldr": "This paper provides a training-efficient table understanding framework via dynamically modeling tabular data from multimodal perspectives by reusing existing pretrained single-modality models.", "keywords": ["Table Understanding", "Large Language Models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60f2784542ff9ff39d21c9c6dcff3877a59d508a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed TableDart, a method designed for understanding semantic and structual information in table data. It adopted an MLP gating network that dynamically selected the optimal path, avoiding prohibitive costs of full MLLM fine-tuning. It also designed an LLM-based Agent to deal with outputs of other two single-modality experts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This method only have few parameters to train, avoiding cost of fully finetuning\n- The designed dusion agent consider different conditions of other two outputs of single-modality experts, including similar and conflict answers\n- This paper do many experiments to validate its performance and efficiency"}, "weaknesses": {"value": "There are some core issues that have significant influence on my rating:\n- How do the MLP gating network choose the path after giving the logits? Choose the path with the highest score or paths whose scores exceed certain thresholds. If it is the later condition, what if the gating network choose more than one path?\n- In **Inference efficiency of section 4.3**, what is the model used in Non-Adaptive baseline? Why the mean inference time can be reduced? From my perspectives, if single-modality expert is chosen, the inference time remain the same. And if the fusion path is chosen, the other two path will be executed, where the inference time will double.\n- What's the standard when you choose the single-modality MLLM and the fusion agent as these three models have a significant influence on the performance of your methods.\n- Part results in Table 1 are the same with paper \"Multimodal Tabular Reasoning with Privileged Structured Information\" but no reference is mentioned.\n\nOther discussions:\n- Why do you choose three paths, including text-only, image-only and fusion. Why not add another path with MLLM modality? I take the thought that this path will provide more insight of the information than the single-modality experts do.\n- I wonder the number of times each path is chosen as this maybe can help understand the inference efficiency."}, "questions": {"value": "Please refer to section Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x5pi8XEEoP", "forum": "4aZTiLH3fm", "replyto": "4aZTiLH3fm", "signatures": ["ICLR.cc/2026/Conference/Submission10043/Reviewer_P1qR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10043/Reviewer_P1qR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549012984, "cdate": 1761549012984, "tmdate": 1762921445907, "mdate": 1762921445907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TableDART, a framework that adaptively selects the optimal processing path for each query to make the most of modality-specific features for table understanding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Proposes a novel dynamic routing framework that adaptively selects the optimal modality (text, image, or fusion) for each table–query pair.\n\nAchieves good performance gains across multiple benchmarks with minimal additional parameters. The parameter-efficient training makes it practical and resource-friendly."}, "weaknesses": {"value": "1. Your results seem insufficient to demonstrate that you achieve Inference Efficiency (the main text only contains a one-sentence description of the results). From the appendix, it can be seen that most tasks on the datasets can be completed with a single modality; however, a fusion strategy is still predominantly used.\n\n2. The two single-modality based methods you used already perform far beyond most models, making many methods seem completely meaningless for comparison. Aren't there any other relatively stronger baselines in this field?\n\n3. Three paths are a bit too few for routing. More models should be adopted to provide more options.\n\n4. How does computational efficiency change as $lambda$ varies? This should be presented clearly. Furthermore, I believe it would be normal for the performance to be optimal when $ \\lambda$ is 0, but this does not seem to be the case.\n\n5. From the results, Fusion appears to be the primary source of your performance improvement, and this deserves a detailed explanation.\n\n6. Why not use Qwen2.5-VL or even Qwen3-VL as your baseline? \n\n7. The experimental results in Table 1 on Ovis2 seem partially the same as \"Multimodal Tabular Reasoning with Privileged Structured Information\" (https://arxiv.org/pdf/2506.04088), except for the result on the TABMWP dataset. However, no reference or explanation can be found in the paper. The authors also did not compare with this paper. Could the authors explain the reason?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "isyySRjfCE", "forum": "4aZTiLH3fm", "replyto": "4aZTiLH3fm", "signatures": ["ICLR.cc/2026/Conference/Submission10043/Reviewer_Z7NP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10043/Reviewer_Z7NP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813088711, "cdate": 1761813088711, "tmdate": 1762921445299, "mdate": 1762921445299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TableDART—a training-efficient framework that reuses pretrained single-modality models via a lightweight 2.59M-parameter MLP gating network, dynamically selecting the optimal path (Text-only, Image-only, or Fusion) for each table–query pair to mitigate redundancy and conflicts."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "TableDART addresses limitations of existing table understanding methods by introducing a training-efficient framework with a lightweight MLP gating network, dynamically selecting optimal (text, image, or fusion) paths per table-query pair to reduce redundancy/conflicts. It further incorporates a cross-modal agent for knowledge integration without costly MLLM fine-tuning, achieving SOTA on seven benchmarks."}, "weaknesses": {"value": "1. The paper is difficult to comprehend, as many concepts are employed directly without any corresponding explanation of their meanings—\n   - For instance, what does the term \"fine-grained\" on line 46 specifically refer to?\n   - As noted in the abstract: \"statically process both modalities for every query-table pair within large multimodal LLMs (MLLMs), inevitably introducing redundancy and even conflicts,\" what exactly does the term \"conflicts\" refer to herein?\n   - In the Introduction, it is mentioned that table data exhibits the property of heterogeneity—could the authors provide an explanation for this?\n   - What modality exactly is Table T on line 145—textual or visual?\n2. The literature review is inadequate, and the compared methods lack several representative approaches, such as the domain-specific VLM TabPedia、SynTab-LLaVA, and the general VLM Qwen2.5VL.\n3. How are the three token sequences (e_q, e_t, e_v), which vary in length and differ in token dimension, concatenated?\n4. As the authors state, \"We employ TableGPT2-7B and Ovis2-8B as TableDART’s single-modality experts,\" it is queried whether ablation experiments have been conducted using other alternative single-modality experts to verify the robustness and generalizability of the proposed framework."}, "questions": {"value": "see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dgDp6rX6l6", "forum": "4aZTiLH3fm", "replyto": "4aZTiLH3fm", "signatures": ["ICLR.cc/2026/Conference/Submission10043/Reviewer_nQLA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10043/Reviewer_nQLA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897661361, "cdate": 1761897661361, "tmdate": 1762921444972, "mdate": 1762921444972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TableDART, a dynamic adaptive multi-modal routing framework for table understanding tasks. The key innovation lies in replacing traditional monolithic multimodal LLM fine-tuning with a lightweight 2.59M-parameter gating network that dynamically selects among three inference paths, Text-only, Image-only, or Fusion, depending on each table–query pair’s characteristics. The fusion stage employs an LLM agent that arbitrates between unimodal outputs or synthesizes a new answer when both fail. TableDART reuses frozen pretrained unimodal models, drastically reducing training cost. Experiments on seven benchmarks show TableDART surpasses strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The dynamic adaptive routing approach is impressive. It represents a conceptual shift from static multimodal fusion to instance-specific modality selection.\n\n- The paper demonstrates strong empirical rigor.\n\n- TableDART addresses a key bottleneck in multimodal table understanding, redundant modality processing and high fine-tuning cost"}, "weaknesses": {"value": "- While the paper highlights the LLM-based Fusion agent's \"Arbitrator\" and \"Rescuer\" roles, its decision logic is black-box. There is no systematic evaluation of when or why it succeeds beyond anecdotal cases.\n\n- The use of Gemini 2.0 Flash for fusion raises reproducibility and accessibility concerns. A variant using a fully open LLM would improve transparency.\n\n- The gating network's supervision relies on pre-computed correctness vectors, but the procedure for labeled paths is not critically examined.\n\n- It would be instructive to show a variant where fusion uses a simple weighted ensemble rather than an LLM agent, to disentangle the benefits of adaptive routing at the fusion stage."}, "questions": {"value": "- How sensitive is the routing policy to the composition of the training mixture? Would domain-specific fine-tuning alter routing distributions?\n\n- Does the gating network rely primarily on query semantics or table structure cues when choosing between text and image paths?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f9m1IDrWww", "forum": "4aZTiLH3fm", "replyto": "4aZTiLH3fm", "signatures": ["ICLR.cc/2026/Conference/Submission10043/Reviewer_YExZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10043/Reviewer_YExZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762288860264, "cdate": 1762288860264, "tmdate": 1762921444464, "mdate": 1762921444464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}