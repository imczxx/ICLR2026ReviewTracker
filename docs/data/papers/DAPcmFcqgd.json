{"id": "DAPcmFcqgd", "number": 24465, "cdate": 1758357143092, "mdate": 1759896764721, "content": {"title": "MoEP: Compact and Efficient Sparsity with Modular Expert Paths", "abstract": "The transition from dense model architectures to sparse ones has  become a key trend in the field of Large Language Models (LLMs). Using methods like Mixture-of-Experts (MoE) allows language models to scale their representation power without overloading computation, by relying on sparse parameter activation. Despite this more lightweight activation, the standard MoE approach increases the total number of parameters. This trade-off between size and sparsity can be avoided without losing performance compared to a dense baseline architecture. We introduce MoEP (Modular Expert Paths) as a solution to add sparsity while keeping the total parameter count fixed. MoEP combines model parallelism with MoE-style linear projections to implement selective token activation, which accelerates model learning and enables it to outperform the GPT-2 baseline. This opens a promising research direction, where compact models can still benefit from sparsity.", "tldr": "We introduce MoEP (Modular Expert Paths) as a solution to add sparsity while keeping the total parameter count fixed. MoEP combines model parallelism with MoE-style linear projections to implement selective token activation", "keywords": ["mixture-of-experts", "sample efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/73b4ca0221d643dcdb74ace77878477fbb993214.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MoEP, a compact and efficient sparse architecture for decoder-only LLMs. It aims to address the core pain point of traditional MoE models, increasing sparsity inevitably requires increasing the total number of parameters. MoEP adopts a structure of full-size GPT-2 layer to MoE block to parallel layers to MoE block to full-size GPT-2 layer. Combined with a load-balanced auxiliary loss to avoid expert collapse, MoEP achieves selective token activation while maintaining a fixed total parameter count."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The experimental section strictly controls variables, ensuring high rigor."}, "weaknesses": {"value": "1 Experiments are only validated on the BabyLM strict-small dataset and small-parameter models. However, the sparsity advantages of LLMs are usually demonstrated in large-scale scenarios. If MoEP exhibits performance degradation under large-parameter/large-dataset settings, its core value will be significantly diminished. Although the authors mention this in the \"Conclusion and Discussion\" section, validation in large-scale scenarios (e.g., on models with at least 3B or 7B parameters) is highly necessary.\n\n2 Ablation studies are missing, so the effectiveness of different components of MoEP is not verified. For example, variations in the number of experts or changes in top-k values are not tested.\n\n3 Typo: In line 277, \"textbfAdamW\".\n\n4 The comparative experiments only include MoEP and baseline models. This makes the advancement of the proposed method questionable.\n\n5 What are the specific λ values in Equation 3? How were they determined? And what was the parameter search range?\n\n6 In Figure 6, MoEP-SwiGLU shows significant fluctuations in the Entity Tracking task. What causes this? The authors’ analysis of the experimental results is relatively superficial."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UrJk5RrdNz", "forum": "DAPcmFcqgd", "replyto": "DAPcmFcqgd", "signatures": ["ICLR.cc/2026/Conference/Submission24465/Reviewer_aZeC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24465/Reviewer_aZeC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760701618383, "cdate": 1760701618383, "tmdate": 1762943089249, "mdate": 1762943089249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MOEP, a sparse architecture that matches the parameter count of its dense baseline while improving efficiency. MOEP combines top-k token routing across parallel Transformer blocks with MoE-style dimension scaling. On the BabyLM strict-small benchmark, the 28M-parameter MOEP outperforms GPT-2 of equal size and achieves peak performance earlier in training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a unified routing framework that integrates top-k token routing across parallel Transformer blocks with Mixture-of-Experts feed-forward layers, creating sparse modular pathways while maintaining parameter efficiency."}, "weaknesses": {"value": "1. MoEP achieves only modest improvements over the GPT-2 baseline. And the checkpoint analysis reveals MoEP peaks early then degrades while GPT-2 continues improving, suggesting overfitting rather than genuine efficiency gains.\n\n2. The No ablation isolates contributions of layer-level routing vs. dimension reduction vs. MoE blocks. Critically, no comparison to standard MoE (e.g., FFN-level experts only) at matched parameters, making it impossible to assess whether gains come from the proposed \"modular paths\" concept or simply architectural flexibility.\n\n3. Although MoEP is motivated by compactness and sparsity, the paper does not report inference speed, FLOPs reduction, or activation ratios. Without such evidence, the claimed efficiency remains qualitative."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZRoUeyuHYO", "forum": "DAPcmFcqgd", "replyto": "DAPcmFcqgd", "signatures": ["ICLR.cc/2026/Conference/Submission24465/Reviewer_yPKd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24465/Reviewer_yPKd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928042295, "cdate": 1761928042295, "tmdate": 1762943088975, "mdate": 1762943088975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new Mixture of Experts architecture where instead of routing to several MLPs the model routes to several sequences of transformer blocks (named parallel blocks). The paper also uses an MoE block to adapt the dimension before and after the application of the parallel blocks. Finally the authors evaluate their method on the BabyLM dataset and tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea proposed by the paper is interesting and could have some impact if it does indeed work."}, "weaknesses": {"value": "The paper is extremely poorly written to the point that it hinders understanding of the method and the experimental methodology. There are sentences cut in the middle even in the introduction and several syntactical and semantic errors throughout, including the slight abuse of the bold face font in some cases.\n\nRegarding the evaluation the results are clearly mixed and the MoEP with SwiGLU which has almost 50% more parameters based on table 2 in the supplementary performs worse than the baseline. The authors also do not show the most basic of metrics for a new architecture which would be the evolution of the training loss compared to the baseline."}, "questions": {"value": "I have stated my concerns in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hShaAHy2Vb", "forum": "DAPcmFcqgd", "replyto": "DAPcmFcqgd", "signatures": ["ICLR.cc/2026/Conference/Submission24465/Reviewer_Q3AE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24465/Reviewer_Q3AE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140791764, "cdate": 1762140791764, "tmdate": 1762943088656, "mdate": 1762943088656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed an MoE architecture  MoEP. It uses parallel layers at lower dimension to create MoE model with equal parameter count as dense model. The authors performed experiments on BabyLM challenge and achieved similar performance with GPT2."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- It's concise."}, "weaknesses": {"value": "- The writing very hard to understand, there are plenty of important technical details without adequate explanations.\n- This paper only come with toy scale experiments, and results still don't appear to be better than GPT2.\n- The overall objective, reducing the parameter count of MoE model, is misguided. The main advantage of MoE is to enable larger model size without raising the compute cost."}, "questions": {"value": "- What happens inside MoE shrink and MoE Grow? If the dimension changes, do you still have residual connection in the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "enWdQDnyao", "forum": "DAPcmFcqgd", "replyto": "DAPcmFcqgd", "signatures": ["ICLR.cc/2026/Conference/Submission24465/Reviewer_DJr7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24465/Reviewer_DJr7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762238583381, "cdate": 1762238583381, "tmdate": 1762943088356, "mdate": 1762943088356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}