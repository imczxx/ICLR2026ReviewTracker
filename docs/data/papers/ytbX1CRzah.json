{"id": "ytbX1CRzah", "number": 17743, "cdate": 1758280059054, "mdate": 1759897156629, "content": {"title": "Geometric Moment Alignment for Domain Adaptation via Siegel Embeddings", "abstract": "We address the problem of distribution shift in unsupervised domain adaptation with a moment-matching approach. Existing methods typically align low-order statistical moments of the source and target distributions in an embedding space using ad-hoc similarity measures. We propose a principled alternative that instead leverages the intrinsic geometry of these distributions by adopting a Riemannian distance for this alignment. Our key novelty lies in expressing the first- and second-order moments as a single symmetric positive definite (SPD) matrix through Siegel embeddings. This enables simultaneous adaptation of both moments using the natural geometric distance on the shared manifold of SPD matrices, preserving the  the mean and covariance structure of the source and target distributions and yielding a more faithful metric for cross-domain comparison. We connect the Riemannian manifold distance to the target-domain error bound, and validate the method on image denoising and image classification benchmarks.", "tldr": "we propose a novel and geometric-aware moment matching technique through Siegel embeddings for domain adaptation, and empirically validate the method on standard image benchmark datasets.", "keywords": ["domain adaptation", "moment matching", "Siegel embeddings", "SPD geometry", "Riemannian manifold"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba6f225a80f828c383fde58db734f5a0bb9fae1a.pdf", "supplementary_material": "/attachment/871c0907d098be6abb2047df156953afe5e88c3a.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles unsupervised domain adaptation (UDA) under covariate shift, where input distributions differ between source and target domains (but label conditionals remain unchanged). The authors claim that most standard moment-matching methods align low-order moments (means, covariances) between domains using Euclidean distances, ignoring the true geometry of statistical distributions (this is partially true, however the author themselves list many works that \"have replaced ad-hoc Euclidean distances with geometry-aware alternatives\").\n\nThe authors propose a geometrically principled moment alignment approach using Riemannian geometry on the SPD manifold.\nTheir main idea is to combine first- and second-order moments (mean and covariance) into a single SPD matrix using Siegel embedding and measure domain discrepancy via manifold distances, rather than arbitrary Euclidean metrics.\nThe method, calle Geometric Moment Alignment (GeoAdapt), comes in two variants: GeoAdapt-AIRD: using Affine-Invariant Riemannian Distance (AIRD) and GeoAdapt-HPD: using a faster approximation, the Hilbert Projective Distance (HPD).\n\nTheorem 1 provides an upper bound on the target-domain generalization error in the case of HPD. \n\nExperiments are performed on two tasks:\n(Sec. 4.1) Unsupervised Task – Image Denoising on MNIST and Fashion-MNIST (clean  $\\rightarrow$  noisy domains).\n(Sec 4.2) Supervised Task – Image Classification on Office-31 (A, D, W domains) and VisDA-2017 (synthetic $\\rightarrow$  real domains).\nResults show that  GeoAdapt-AIRD achieves best overall performance outperforming Euclidean and other geometric baselines (e.g. MECA, HoMM).\n\nSome insight provided in the paper:\n- Low-dimensional embeddings (32–128) outperform higher ones, as they avoid numerical instability and lie within well-behaved SPD regions.\n- Euclidean methods degrade in high dimensions due to rank-deficient covariance estimates.\n- The geometric approach explains why classical moment-matching fails under strong curvature or low sample conditions.\n- The authors claim that the method is architecture-agnostic and can be integrated into any DA framework using a domain discrepancy term."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a sound theoretical approach for matching distributions through moments: Embedding first- and second-order moments as a single SPD matrix via Siegel embeddings is a mathematically elegant and novel idea that unifies mean and covariance adaptation in one structure. The method is effectively applied to Unsupervised Domain Adaptation.\n\nThe paper is crystal clear, easy to follow and result presented highlight the superiority of the proposed method wrt other moment-matching methodologies.\n\nThe authors provide a theoretical upper bound on the target-domain generalization error (for the Hilbert Projective Distance), extending classical domain adaptation theory.\n\nThe paper shows that compact embedding spaces (32–128 dimensions) are sufficient or even superior — improving accuracy and stability while reducing computational cost. This can be seen as a plus in case the task does not require more expressive (i.e. larger) embedding vectors.\n\nI appreciate the authors providing the code."}, "weaknesses": {"value": "Please see my points below, mainly related to the experimental validation:\n\n1. Few benchmark and domains:\n\n   Only two main image classification benchmarks (Office-31 and VisDA-2017) and one synthetic/toy denoising setup (MNIST/Fashion-MNIST) are tested. Missing: Large-scale or more diverse datasets (e.g., DomainNet, WILDS) and possibly non-visual domains (e.g., text, speech).\n\n2. Limited model comparison:\n\n   Experiments are limited to ResNet-50 and a simple autoencoder. No tests with modern backbones (e.g., ViTs, CLIP, or U-nets w/ diffusion for denoising). This is quite critical given that the authors claim that the method is architecture-agnostic. One would expect a concrete proof of that statement. Besides, more modern pretrained backbones provide higher train-on-source performances which make adaptation less critical.\n\n3. Bounds:\n\n   Train-on-target performance is useful to be reported in the tables as an upper bound.  It would be also interesting to see in practice what would be  the _estimated_ theoretical upper bound which I guess can be easily calculated from the training loss functions. In fact (assuming $\\gamma=0$) the upper bound has a term which is the error on the source set (should be easily derived from the training task loss) and a term which is the distance between the source and target distributions. \n\n4. Modern Baselines:\n\n   The paper mainly compares with older moment-matching baselines (MMD, CMD, CORAL, HoMM, MECA) and lacks comparisons with more recent and powerful UDA methods (e.g. adversarial methods). This makes it difficult to assess state-of-the-art competitiveness - the authors note this themselves, arguing they isolate the “distance effect,” yet this is still a limitation. \n\n5. Limited Ablation:\n\n   Hyperparameter sensitivity (e.g., $\\beta$ for $\\mathcal{L}\\_{dist}$, mini-batch size is not fully explored. It’s unclear how robust the approach is to these settings or to noisy covariance estimates in small batches. Larger batches should also in principle allow for larger embedding vectors, which could be needed for downstream tasks where more expressiveness of the model could be essential.\n\n6.  Computational Analysis:\n\n    Matrix operations on SPD manifolds (logarithms, inverses, eigen-decompositions) can become computationally heavy even for low-dimensional embeddings and small batches. The paper is not discussing this point.\n\n7. Validation:\n\n   Validation is a long-standing issue in UDA. In principle one should not peek at target metrics for tuning the hyper-parameters, since this would mean validating on the test set. One strategy is to use a toy dataset for validation of the  hyper-parameters (e.g. SVHN $\\rightarrow$ MNIST) and then use the same hyper-parameters for the other benchmarks. Alternatively, MECA proposes a criterion based on the estimation of the entropy on the predictions of the target. The paper is not discussing the validation issue in any respect."}, "questions": {"value": "Please see and discuss the points I raised above. They already include questions and points to be clarified or expanded."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nifk087JfX", "forum": "ytbX1CRzah", "replyto": "ytbX1CRzah", "signatures": ["ICLR.cc/2026/Conference/Submission17743/Reviewer_r7jg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17743/Reviewer_r7jg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760969699974, "cdate": 1760969699974, "tmdate": 1762927583744, "mdate": 1762927583744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the domain adaptation problem, which aims to learn a generalization model under the observed source domain and target domain. This work considers the covariate shift framework and points out that existing works mainly focus on the low-order statistics. To this end, this work explores the first-order and second-order statistics as distribution parameters and adopts metrics on manifold to measure the distance over high-order domain representation, i.e., SPD matrix consists of mean and covariance. Theoretical results show that the generalization could be bounded by the developed manifold metric-based method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The application of manifold metric for matrix-based domain representations is reasonable.\n\n+ The organization is easy to follow."}, "weaknesses": {"value": "+ The key idea of representing domains as manifolds or statistics on manifolds is extensively studied by existing works, which are not properly compared in the submission. \n\n+ The limitations of existing works seem to be over-claimed, since there are many works that already consider the high-order statistics or statistics with stronger power. \n\n+ The theoretical result is trivial, and not much new insight is provided."}, "questions": {"value": "**Concerns**\n\nConcern 1. One of the essential ideas is adopting the manifold metric to measure the domain gap/distance over the mean-covariance representation, which essentially shares the same spirit of existing works that consider manifold representation and Riemmanian metric, e.g., statistical manifold [r1], Riemannian manifold [r2], Log-Euclidean metric with better efficiency [r3], Kernel Geodesic [r4], affine-invariant metric [r5]. However, there are only statistical moment-based methods compared, which cannot completely demonstrate the significance of the proposed method.\n\n\nConcern 2. The limitations of existing works seem to be improper. Since there are many work that considers the high order statistics, which also admit stronger properties on distribution distance, e.g., kernel Wasserstein with mean and covariance w.r.t. RKHS, conditional moments with multi-variable correlation characterization. Moreover, note that the kernel embedding can be taken as moment with infinite order with a proper choice of kernel, e.g., Gaussian kernel, since the corresponding RKHS is an approximation of the space of continuous functions and the moments are taken within such a space.\n\nConcern 3. Though geometric metric is adopted, there are no general guarantees for the distribution discrepancy minimization. The key to connecting the explored metric with statistical distance is the Gaussian prior on distributions. However, such a result seems to be trivial if the Gaussian prior is adopted, as many metrics could also be connected to statistical distance while also endowed with explicit computational formulation, e.g., (kernel) Wasserstein with geometric property, some metrics in $f$-divergence family. \n\nConcern 4. The generalization error analysis does not provide new insights and seems to be loose. Specifically, the bound is obtained based on existing upper bound where two inequalities are successively applied (which could amplify the error).\n\n\n**References**\n\n[r1] Baktashmotlagh, Mahsa, et al. \"Domain adaptation on the statistical manifold.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.\n\n[r2] Luo, You-Wei, et al. \"Unsupervised domain adaptation via discriminative manifold propagation.\" IEEE transactions on pattern analysis and machine intelligence 44.3 (2020): 1653-1669.\n\n[r3] Cui, Zhen, et al. \"Flowing on Riemannian manifold: Domain adaptation by shifting covariance.\" IEEE transactions on cybernetics 44.12 (2014): 2264-2273.\n\n[r4] Zhang, Youshan, and Brian D. Davison. \"Deep spherical manifold gaussian kernel for unsupervised domain adaptation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[r5] Yair, Or, Mirela Ben-Chen, and Ronen Talmon. \"Parallel transport on the cone manifold of SPD matrices for domain adaptation.\" IEEE Transactions on Signal Processing 67.7 (2019): 1797-1811."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pHXUnOigIC", "forum": "ytbX1CRzah", "replyto": "ytbX1CRzah", "signatures": ["ICLR.cc/2026/Conference/Submission17743/Reviewer_MAps"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17743/Reviewer_MAps"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757005170, "cdate": 1761757005170, "tmdate": 1762927583137, "mdate": 1762927583137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors developed a novel moment-matching method for unsupervised domain adaptation (UDA) and evaluated it on image classification and image denoising tasks. The authors do not use state-of-the-art architectures, and I couldn't find a sufficient number of experiments, or detailed data analysis to clearly explain why the method performs better."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The novelty of the method lies in the fact that, instead of matching means and covariances separately, using arbitrary distance metrics,  both can be encoded into a single SPD matrix and can leverage the natural geometry of the SPD manifold (via Siegel embeddings) to compute distances more appropriately.\n\n\nThe method can be plugged in to the other DA methods."}, "weaknesses": {"value": "The authors could use modern architectures as backbones, such as Vision Transformers (ViT).\n\n\nCan you please clarify why there is no benchmark on non–moment-matching UDA methods? Is there a specific reason or application context that limits the proposed approach to moment-matching UDA only?\n\n\nFor image denoising, the test are conducted only on two datasets, and for image classification, the improvement achieved over existing methods is not significant.\n\n\nHoMM and CMD utilize up to third-order moments for UDA. Can you please explain why your proposed method performs better despite using only first and second moments? Additionally, the approach may be limited when applied to datasets that do not follow a Gaussian feature distribution, which could reduce its overall applicability.\n\n\nFor SPD manifolds, there are several possible embedding methods. Can you please clarify the reason for choosing this particular one?\n\n\nAny justification or experimental evidence for the choice of $\\alpha_1$ in Section 3.1 would be great."}, "questions": {"value": "Please see weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YGsjAfHgEi", "forum": "ytbX1CRzah", "replyto": "ytbX1CRzah", "signatures": ["ICLR.cc/2026/Conference/Submission17743/Reviewer_9a7X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17743/Reviewer_9a7X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888108793, "cdate": 1761888108793, "tmdate": 1762927582584, "mdate": 1762927582584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how to compute similarities and transform statistical moments more effectively for domain adaptation tasks.\n\nThe authors leverage differential geometry and map the latent representations of both source and target domains through a diffeomorphic transformation into the SPD (Symmetric Positive Definite) manifold.\n\nThis transformation jointly encodes the first and second moments into a single SPD matrix.\n\nBy exploiting the Riemannian structure of this manifold, the authors define two geometrically inspired distance measures—Affine-Invariant Riemannian distance and Hilbert Projective distance—to quantify the discrepancy between domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is mathematically rigorous and presents a well-grounded theoretical derivation.\n\nThe method achieves competitive performance on both supervised (classification) and unsupervised (denoising) tasks, demonstrating strong generality."}, "weaknesses": {"value": "The baselines used are somewhat outdated — the most recent compared method (HOMM) was published in 2020. It would strengthen the paper to include comparisons with more recent works.\n\nIt would be interesting to evaluate the proposed approach using CLIP embeddings or other strong pretrained features on more complex, large-scale, or cross-domain datasets to further test its scalability and robustness."}, "questions": {"value": "Please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Skqr5qpUIE", "forum": "ytbX1CRzah", "replyto": "ytbX1CRzah", "signatures": ["ICLR.cc/2026/Conference/Submission17743/Reviewer_DLHX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17743/Reviewer_DLHX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907279901, "cdate": 1761907279901, "tmdate": 1762927582086, "mdate": 1762927582086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}