{"id": "8CnU2kchiw", "number": 7188, "cdate": 1758011003646, "mdate": 1759897867854, "content": {"title": "Query-Guided Spatial–Temporal–Frequency Interaction for Music Audio–Visual Question Answering", "abstract": "Audio–Visual Question Answering (AVQA) is a challenging multimodal task that requires jointly reasoning over audio, visual, and textual information in a given video to answer natural language questions. Inspired by recent advances in Video QA, many existing AVQA approaches primarily focus on visual information processing, leveraging pre-trained models to extract object-level and motion-level representations. However, in those methods, the audio input is primarily treated as complementary to video analysis, and the textual question information contributes minimally to audio–visual understanding, as it is typically integrated only in the final stages of reasoning. To address these limitations, we propose a novel Query-guided Spatial–Temporal–Frequency (QSTar) interaction method, which effectively incorporates question-guided clues and exploits the distinctive frequency-domain characteristics of audio signals, alongside spatial and temporal perception, to enhance audio–visual understanding. Furthermore, we introduce a Query Context Reasoning (QCR) block inspired by prompting, which guides the model to focus more precisely on semantically relevant audio and visual features. Extensive experiments conducted on several AVQA benchmarks demonstrate the effectiveness of our proposed method, achieving significant performance improvements over existing Audio QA, Visual QA, Video QA, and AVQA approaches.", "tldr": "Novel Query-guided Spatial–Temporal–Frequency interaction method to enhance audio–visual understanding", "keywords": ["Audio–visual question answering", "Multimodal", "Music scene understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4e8551dc5597b191571d4a48f282216f054b34c.pdf", "supplementary_material": "/attachment/6fec692d4f7a53921cab68bf28b741fd60368a37.zip"}, "replies": [{"content": {"summary": {"value": "To address the audio-visual question answering task, this paper proposes a Query-guided Spatial-Temporal-Frequency (QSTar) interaction method. QStar primarily consists of the query-guided multimodal correlation module, spatial-temporal-frequency interaction module, and query context reasoning block. Experiments conducted on the MUSIC-AVQA and AVQA datasets verify the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The discussion on the audio frequency and query context is good.\n- The proposed method achieves new state-of-the-art performance on the relevant datasets.\n- In general, the proposed method is described clearly."}, "weaknesses": {"value": "- Throughout the method section, the proposed network heavily relies on prevalent self-attention and cross-attention mechanisms. The query-guided multimodal correlation module is similar to the prior method, APL. Prior work, like TSPM, etc, has already explored the utilization of question modality for early fusion. Overall, the proposed method still falls into the convention of multimodal fusion, not providing sufficient advances or improvements.\n\n- Although the introduction of frequency makes sounds, it incorporates an additional AST backbone for feature extraction, which may make the proposed framework more complex. An analysis of the efficiency is required to justify this.\n\n- The paper lacks a discussion or comparison of MLLMs. For the studied audio-visual question answering problem, the current omni multimodal large language models, such as Qwen2.5-Omni, Ming-Omni, and video-SALMONN2, can be used. What performance can be achieved by such omni models?"}, "questions": {"value": "- In the Introduction, the paper highlights the advantages of frequency in music scenarios, which may be suitable for the MUSIC-AVQA dataset. But the AVQA dataset contains more diverse scenarios. Would the frequency be less effective in AVQA, or would the writing in the introduction make it inconsistent for different datasets (scenarios)? \n- The paper highlights the frequency by employing the AST backbone to extract features. Similar to VGGish, AST is also a general model for audio feature extraction. Why not directly use the identical AST in the temporal audio extraction? \n- The query-context reasoning block utilizes the 'context-related keywords' (Line 307). How these context-related keywords are obtained?\n\n- Figure 2 can be improved. For example, the abbreviation of several modules should be added at the end of the corresponding full module names."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xmASIybhJr", "forum": "8CnU2kchiw", "replyto": "8CnU2kchiw", "signatures": ["ICLR.cc/2026/Conference/Submission7188/Reviewer_ttss"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7188/Reviewer_ttss"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760529657863, "cdate": 1760529657863, "tmdate": 1762919343864, "mdate": 1762919343864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "Dear Reviewers and ACs,\n\nThank you so much for your time and efforts in assessing our paper!\nWe feel very encouraged that the reviewers find our work well-motivated, novel, and intuitive.\nThe insightful comments are very helpful in improving our work, and we have responded to the concerns point by point.\n\nAccording to our understanding, there are three common concerns from reviewers:\n\n- Effectiveness of frequency-aware cues. We now provide clearer theoretical motivation, more experiments, and deeper empirical analysis demonstrating how frequency-domain cues contribute across question types.\n\n- Use and construction of prompts. We clarify the design of our unified prompt set, explain how the keywords were derived, and compare our approach against three alternative prompting strategies.\n\n- Comparison with MLLMs. We have conducted new experiments evaluating QSTar against several recent MLLMs and provided a thorough discussion of the performance.\n\n\nWe have taken all the suggestions carefully and updated our previous version.\nIn the revised manuscript, we have made the following main changes and highlighted them in blue:\n\n- Expanded explanation of frequency-based modeling advantages in Section 1 and 3.3.\n\n- Improved discussion of related works in Section 1, 2, and Appendix A.1.\n\n- Added new comparison and analysis of MLLMs in Section 4.3.\n\n- Added computational cost analysis in Section 4.5.\n\n- Expanded explanation of prompt construction and fairness in Appendix A.2.\n\n- Improved the overall presentation of the paper.\n\nWe are happy to discuss with you further if you still have other concerns. Thank you very much again!\n\nBest regards,\n\nPaper 7188 Authors"}}, "id": "PYqNSICD1e", "forum": "8CnU2kchiw", "replyto": "8CnU2kchiw", "signatures": ["ICLR.cc/2026/Conference/Submission7188/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7188/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7188/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763652714335, "cdate": 1763652714335, "tmdate": 1763652714335, "mdate": 1763652714335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces QSTar, a novel method for Audio-Visual Question Answering (AVQA) with a strong focus on complex musical scenes. The core contribution is a multi-stage, query-guided architecture that injects textual guidance throughout the pipeline: early with a Query-Guided Multimodal Correlation (QGMC) module, in the middle via a Spatial-Temporal-Frequency Interaction (STFI) module, and finally with a Query Context Reasoning (QCR) block that uses task-aware prompts. The model explicitly enhances audio processing by incorporating frequency-domain analysis to capture timbral characteristics crucial for instrument identification. The method achieves new state-of-the-art performance on the MUSIC-AVQA benchmark, reporting an average accuracy of 78.98%. A smaller-scale evaluation on the general AVQA dataset further demonstrates the model's robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong Empirical Performance: The method achieves state-of-the-art results on MUSIC-AVQA, with consistent improvements across Audio, Visual, and Audio-Visual question types.\n- Thorough Ablation Studies: The paper provides extensive ablations that validate the contribution of each key module (QGMC, STI, TFI, QCR) and design choice, strengthened by further controls in the supplementary material.\n- The Query Context Reasoning (QCR) block implements a lightweight and reproducible prompting mechanism. It uses a fixed set of task-relevant keywords (e.g., instrument type, duration, location) derived from the dataset's question types, which are encoded and used to guide the final fusion of audio-visual features."}, "weaknesses": {"value": "At the top: I have reviewed an earlier version of this paper. The current submission remains largely unchanged, except for minor language polishing. All major methodological and empirical concerns I previously raised remain unaddressed.\n- First of all, the motivation: The paper makes broad claims that prior work treats audio as secondary and uses text late, really? How the author provide some results/examples to demonstrate it? Reader could NOT buy the motivation just by plain sentences. To summarize, the motivation does not substantiate these claims with targeted analysis or experiments on specific baselines.\n- The author say AST is superior to VGGish for capturing timbral information, why? this claim is untested.\n- I think the dataset scope is too narrow: Strong focus on MUSIC-AVQA; AVQA appears only as a small table; no Pano-AVQA. Limits generality claims beyond music scenes.\n- My another concern drop into 'Prompting fairness': The fixed keyword prompts mirror the dataset’s question taxonomy. In the wild, question types are not given and may differ (long-tail intents, non-music scenes). The models' performance might hinges on dataset priors rather than generalizable reasoning.\n- Also, given the recent progress of MLLMs (e.g., GPT-4o, Phi series) in vision-language tasks, I suggest that the authors either: (1) include a comparison between QSTar and one or more MLLM baselines, or (2) clearly explain why such comparison is not applicable and discuss the corresponding limitations."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oKWwJfRykR", "forum": "8CnU2kchiw", "replyto": "8CnU2kchiw", "signatures": ["ICLR.cc/2026/Conference/Submission7188/Reviewer_BoFL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7188/Reviewer_BoFL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971452266, "cdate": 1761971452266, "tmdate": 1762919343465, "mdate": 1762919343465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes QSTar, a Query-guided Spatial–Temporal–Frequency Interaction framework for Audio–Visual Question Answering task, with a focus on music-related datasets. The key idea is to enhance multimodal reasoning by introducing frequency-domain analysis alongside spatial and temporal interactions. The method introduces three main components: Query-Guided Multimodal Correlation (QGMC) module for early query-conditioned feature alignment; Spatial–Temporal–Frequency Interaction (STFI) module for fine-grained multimodal fusion; and Query Context Reasoning (QCR) block inspired by prompt-based context modeling.\nExperiments on MUSIC-AVQA show that QSTar achieves competitive results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- It is interesting and meaningful to introduce frequency-level interaction in AVQA tasks. This idea is natural given the acoustic characteristics of music-related videos and provides a new perspective besides conventional spatial–temporal fusion.\n- The overall framework is well structured, clearly integrating linguistic guidance across multiple stages of the model.\n- The implementation details and experimental setup are well described, and the ablation studies comprehensively verify the contribution of individual modules."}, "weaknesses": {"value": "- The experimental analysis is rather shallow. Most evaluations focus only on accuracy numbers. More in-depth discussion on how frequency-level cues contribute to specific question types or modalities would strengthen the claim. For example, what kinds of questions benefit most from frequency reasoning and why?\n- The improvements over recent strong baselines (e.g., QA-TIGER) are relatively modest—about 1–2% overall—and mainly appear on Audio QA and temporal AVQA subsets. There are no gains in Visual QA. This raises questions about the generality of the proposed design beyond audio-dominant scenarios.\n- The ablation results show only slight differences between “with QCR” and “without QCR,” suggesting that the reasoning block contributes limited additional value.\n- The paper would benefit from more qualitative or case-level studies, such as visualizing which frequency bands or audio/video segments are attended to under different query types. This would make the contribution of “frequency-level interaction” more convincing."}, "questions": {"value": "Please check the above section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mCxZwZtI6o", "forum": "8CnU2kchiw", "replyto": "8CnU2kchiw", "signatures": ["ICLR.cc/2026/Conference/Submission7188/Reviewer_VpPp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7188/Reviewer_VpPp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977723427, "cdate": 1761977723427, "tmdate": 1762919343133, "mdate": 1762919343133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submitted manuscript addresses the problem that previous studies in AVQA task often overlooked the importance of the audio modality. To tackle this issue, the authors propose a Query-guided Spatial-Temporal-Frequency framework (QSTar), which effectively integrates question-guided cues with the distinctive frequency-domain characteristics and spatio-temporal perception of audio signals to enhance audio-visual understanding. Experiments conducted on relevant datasets demonstrate the effectiveness of the proposed method. Overall, the framework exhibits a clear novelty."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem addressed in the manuscript is clearly defined and well-motivated. By focusing on frequency-domain information from the audio perspective, the work presents a distinctive approach that draws attention.\n2. The proposed QSTar framework shows a degree of originality and has been extensively validated on multiple datasets, confirming its effectiveness.\n3. The writing is clear and well-structured, making the paper easy to read and understand."}, "weaknesses": {"value": "1. Figure 1 effectively illustrates how frequency-domain cues assist in detecting instrument activities that purely spatial or temporal reasoning may miss. However, can similar improvements be observed for fine-grained visual understanding?\n2. How does the model handle off-screen audio sources, such as when the sound of an instrument exists in the audio but the corresponding instrument does not appear in the video?\n3. Considering the limited spatial supervision for audio-visual correspondence, how does the method effectively associate audio and visual cues, especially for rare instruments such as the suona or erhu?\n4. Although the motivation for QCR originates from the analysis of the MUSIC-AVQA dataset, the paper does not discuss in depth the specific prompt forms or their performance compared with more dynamic or generative alternatives, raising some concerns about the scalability of the approach.\n5. Some writing suggestions include avoiding widowed words at the end of paragraphs and adding references for the comparative methods listed in the tables."}, "questions": {"value": "My main questions are reflected in the Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kGOnV9Bgnp", "forum": "8CnU2kchiw", "replyto": "8CnU2kchiw", "signatures": ["ICLR.cc/2026/Conference/Submission7188/Reviewer_92oA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7188/Reviewer_92oA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018802737, "cdate": 1762018802737, "tmdate": 1762919342770, "mdate": 1762919342770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}