{"id": "I2Sz167GlO", "number": 14797, "cdate": 1758243932709, "mdate": 1759897348853, "content": {"title": "Generative 3D Object Particle Dynamics", "abstract": "We introduce ParticleDiffuser, a particle-based 3D trajectory diffusion model that represents scenes as evolving particle graphs, enabling the capture of complex action–object interactions and object deformations. Unlike existing 3D particle dynamics models, which typically rely on deterministic action-conditioned predictors constrained to narrow domains (e.g., individual cloth or soft-body objects), ParticleDiffuser adopts a generative approach trained on large-scale simulated data of deformable and soft objects, and capturing multimodality of future particle tra- jectories. To support efficient spatiotemporal reasoning, ParticleDiffuser introduces learnable latent vectors that fuse information across particles and employs autoregressive rollouts with latent-variable attention across sequential frame seg- ments, enabling long-horizon 3D video generation. We present two variants: (i) an action-conditioned particle trajectory generator, and (ii) a joint action–object particle trajectory generator. By directly modeling the joint distribution of object particles and actions within a single diffusion process, ParticleDiffuser allows goal-conditioned action generation by steering diffusion toward desired future configurations, eliminating the costly trajectory searches required by traditional MPC methods. Experiments show that ParticleDiffuser generalizes to diverse objects and actions in simulated and real-world settings where deterministic graph-based particle networks quickly fail. It also substantially outperforms MPC baselines in both accuracy and efficiency on manipulation tasks involving a broad spectrum of object types, including rigid and deformable bodies.", "tldr": "", "keywords": ["Dynamics Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d650aa76c89d9e71ae6103e2132bb4d82656ec8.pdf", "supplementary_material": "/attachment/a92138c318b5443c793dd0f83a27251c0c1ecf8f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ParticleDiffuser, a generative 3D particle trajectory model designed to predict the dynamics of object manipulation by capturing the interactions between objects and actions. ParticleDiffuser leverages a diffusion-based approach to simulate the evolution of particles in 3D space, making it applicable to a wide range of object types, including rigid, soft, and deformable objects. To train this model, a dataset of 3D point trajectories is collected by the Genesis simulation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. ParticleDiffuser presents a novel approach in the generative modeling of 3D particle trajectories, effectively capturing 3D dynamics with action conditioning. \n2. A simulation dataset is proposed to train the diffusion model, demonstrating the effectiveness of simulation data on both simulation and real-world cases."}, "weaknesses": {"value": "1. Motivation: The method proposed in this paper uses data from physical simulators for training, which raises concerns about the motivation behind the approach. If the physical simulator can accurately predict the motion of point clouds, why is there a need to train an additional model for prediction?\n2. Training Objective: The model is directly supervised with an L2 loss following diffusion training, which brings concerns similar to 2D diffusion models. Specifically, predicting pixel or point observations may not effectively learn physical grounding, as mentioned in lines 44-45 of this paper.\n3. Experimental Results: The physical actions generated by this model are relatively simple, particularly in the supplementary video results. Most of the motions are uniform, and some deformations are unrealistic. For instance, the teddy bear in the video does not return to its original shape after being squeezed."}, "questions": {"value": "1. What advantages does the proposed method have over physical simulators like Genesis?\n2. Could the model incorporate physical constraints, similar to [1], to improve its accuracy and realism?\n3. Could the authors compare their method with physical simulation-based approaches [2, 3]?\n4. Is it possible to use point clouds as conditioning to optimize the generation results of video models, similar to [4]?\n\n[1] Inferring Hybrid Neural Fluid Fields from Videos. Advances in Neural Information Processing Systems, 2023, 36: 63595-63608.\n[2] PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 4389-4398.\n[3] DreamPhysics: Learning Physics-Based 3D Dynamics with Video Diffusion Priors. Proceedings of the AAAI Conference on Artificial Intelligence. 2025, 39(4): 3733-3741.\n[4] PhysMotion: Physics-Grounded Dynamics from a Single Image. arXiv preprint arXiv:2411.17189, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zLI0svlaXo", "forum": "I2Sz167GlO", "replyto": "I2Sz167GlO", "signatures": ["ICLR.cc/2026/Conference/Submission14797/Reviewer_gToH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14797/Reviewer_gToH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717561687, "cdate": 1761717561687, "tmdate": 1762925148632, "mdate": 1762925148632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a diffusion-based dynamics model for control that operates on object surface point clouds and robot gripper contact points named ParticleDiffuser, presenting both action-conditional and non-conditional variants. The model employs several mechanisms to enhance long horizon prediction and computational efficiency including latent attention mechanisms, recurrent training, history conditioning and particle masking augmentations. The authors introduce a novel large-scale simulated object data generation pipeline and evaluate their approach on dynamics prediction and planning in simulation on various objects and many object types, demonstrating improved performance compared to a deterministic baseline and several ablations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**Overview**\n- Well written paper.\n- The overall method seems novel.\n- Interesting use of diffusion over point clouds for dynamics modeling.\n- Interesting use of diffusion with recurrence and history propagation to tackle compounding errors in long-horizon prediction.\n- Various measures for computational efficiency are integrated in the method, which is particularly important in diffusion-based dynamics models.\n- Introduces novel large-scale simulated object data generation pipeline.\n- Ablation study highlights some key design choices.\n- Evaluation on a large variety of objects and object types.\n- Clearly states limitations which facilitate better understanding of the approach as well as highlights avenues for future work.\n- Authors intend to publish dataset and code.\n\nI am positive about raising my score if at least some of the concerns raised in the Weaknesses/Questions sections are addressed."}, "weaknesses": {"value": "**Overview**\n\n- Missing implementation details for method and baselines.\n- Seemingly inaccurate/misleading claims about real-world evaluation.\n- Baselines could be stronger.\n- Ablation of the diffusion objective is missing.\n- Ablation of the use of particles is missing.\n- Experiments consider limited object representation containing only point-cloud information.\n\n**Implementation Details**\n\nSome critical implementation details are missing in my opinion:\n1. What is the loss function used for the guided diffusion sampling? What limitation does this choice induce compared to goal-conditioned diffusion?\n2. What is a typical size for N, the number of points in the point cloud?\n3. How many latent tokens are used? What are the considerations for choosing this hyperparameter?\n4. Which algorithm is used to perform MPC and what are the hyperparameter values chosen for the MPC baselines? Specifically, how many planning iterations are used and are they comparable to the number of diffusion iterations?\n\n**Real-world Object Results**\n\nTo my understanding, there is no evaluation of the approach in the real world, please correct me if I am mistaken.\n\nThis in itself is not a major weakness in my opinion but the claims in the paper are misleading which I believe is a weakness and should be addressed, mainly by clarification and rephrasing. The authors make claims in multiple parts of the paper with relation to real world results such as “the model generalizes effectively to real-world settings” (line 75) and that their method “demonstrates strong sim-to-real generalization” (lines 87-88). Since the proposed method is a dynamics model, real-world should refer to real-world physical dynamics, not just real world images from which object point clouds are extracted. There is actually no sim-to-real here, only real-to-sim. I believe the term real-world in the context of dynamics modeling should be reserved for experiments in real-world dynamics, i.e., with real robot hardware and objects in our physical world. I suggest going over every claim that includes the phrase “real-world” and rephrase it such that this point is clear. The readers should not need to read the paper in depth in order to understand this.\n\n**Baselines and Ablations**\n\n*Deterministic variant of your model*: A baseline/ablation which I believe is clearly missing is a deterministic dynamics model with the exact architecture proposed in your method, i.e., training your architecture with a deterministic dynamics prediction objective instead of the diffusion objective. This will ablate the benefits of diffusion, which are currently not isolated from the architecture. How much of the improved performance is due to the modeling in the latent token space? It is not clear to me that capturing multi-modality is crucial in the tasks you consider. I do not see why the average behavior captured using a GNN/Transformer outputting parameters of a Gaussian distribution can’t perform well in your planning experiments.\n\n*Factored Dynamics Modeling*: The use of particles is treated as a given in the experiment section although the intro suggests there are competing pixel-based paradigms. Do the authors believe there is an appropriate baseline to compare with to demonstrate the efficacy of the particle representation compared to pixels?\n\n**Related Work**\n\nA line of work that I believe is missing in the context of this paper is diffusion-based (unsupervised) object-centric dynamics modeling. Several works have leveraged unsupervised object-centric representations to model dynamics of multiple objects both for video prediction ([SlotDiffusion](https://arxiv.org/abs/2305.11281), [DDLP](https://arxiv.org/abs/2306.05957)) and sequential decision-making ([EC-Diffuser](https://www.arxiv.org/abs/2412.18907)). Specifically, EC-Diffuser is an example of a “diffusion-based control framework that jointly models both the robot and the external object it interacts with” (lines 133-134). Although these methods are not directly comparable to the ones proposed in this paper and have clear distinctions, the literature on factored diffusion is not very large and the above should be mentioned in this context."}, "questions": {"value": "- Why do the authors choose guided diffusion over goal-conditioning?\n- Can the authors reference all sections in the Appendix in the main text? In my opinion, the reader should not need to search the Appendix for details. Appendix C is not referenced for example.\n- How is the model able to infer physical properties of objects entirely from 3D point clouds? Is there some correlation between geometry and physical properties that the model can leverage for prediction? Is this an assumption your evaluation is based on?\n- Can the authors perform an attention analysis on the read/write operations? How much of the latent tokens are actually being used and is there a clear decomposition based on e.g., 3D position or other attributes of the objects? This analysis can shed light on the latent dynamics mechanism and the sources of the improved performance.\n- Can the authors include trajectories of the GNN baseline in the real-to-sim objects for comparison? It is hard to evaluate if this behavior is good without any comparison or qualitative metrics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "whWpGuDSls", "forum": "I2Sz167GlO", "replyto": "I2Sz167GlO", "signatures": ["ICLR.cc/2026/Conference/Submission14797/Reviewer_8ZUo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14797/Reviewer_8ZUo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754340668, "cdate": 1761754340668, "tmdate": 1762925148033, "mdate": 1762925148033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies learning object-centric physically plausible motion priors. Specifically, it uses particle-based physics simulation and particle trajectories to represent the dynamics. A diffusion model is trained to jointly generate the object particles and the actor motions. The network architecture is designed to be more efficient through learnable cross-attention with more compact tokens, and heavy attention is only applied to these sparse tokens. Because the actor and object motion distributions are jointly modeled, planning and MPC become much easier. Experiments comparing the quality of the prediction and the MPC planning are presented."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper studies an interesting and important problem — learning physically plausible motion priors.\n- It considers the actuator and the environment jointly and shows planning capability."}, "weaknesses": {"value": "- The method looks interesting, but one important aspect is not verified: how to enable such a model to capture real-world physical properties — not just sim-to-real transfer, but directly learning the distribution from real data.\n- From the main comparison Figure 4, only one baseline is considered, which looks a little weak.\n- Simulation parameters: the learned distribution is overfitted to the heuristically set parameters of the physical simulator, and the parameters may be augmented (domain randomization). What signal can inform the model to generate the future of one set of parameters instead of another? In other words, if given a few time-step histories, can the model properly realize the physical parameters behind the object and predict the future of this specific configuration?\n- Diversity: it is not clear how diverse the synthesized motion is; at least qualitative results must be presented.\n- Multiple objects: this is beyond the scope of this project, but it would be interesting to mention or discuss the potential of how to make such a method work with multiple object interactions."}, "questions": {"value": "Please refer to the weakness section. The main questions include: learning from real data, diversity of the generation, system identification capability, and more baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G15EaVWxyM", "forum": "I2Sz167GlO", "replyto": "I2Sz167GlO", "signatures": ["ICLR.cc/2026/Conference/Submission14797/Reviewer_zHbG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14797/Reviewer_zHbG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868201617, "cdate": 1761868201617, "tmdate": 1762925147240, "mdate": 1762925147240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ParticleDiffuser, a generative model that learns 3D object dynamics by representing scenes as evolving clouds of particles. The model is trained on a large-scale simulated dataset featuring diverse interactions with rigid, soft, and deformable objects."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core strength is the successful application of generative diffusion models to 3D particle dynamics.\n2. The paper constructs a dataset of 3D point trajectories from diverse robot–object interactions.\n3. The paper proposes a guided diffusion approach for robot–object control, which outperforms traditional MPC in both efficiency and success rate."}, "weaknesses": {"value": "1. The model seems to rely solely on the initial point cloud geometry to predict the future state. Does the model predict the same state when faced with objects of the same category but with different physical properties? If so, this seems inappropriate.\n2. Although the method claims to model the dynamics of deformable and soft objects, no significant examples were seen in the demo.\n3. To better contextualize the performance of ParticleDiffuser, comparisons with a broader range of methods are necessary. These should include, for instance, using the physics simulator directly for inference or other representative GNN architectures. Furthermore, the description of the GNN baseline is lacking critical details: the specific architecture of the GNN model is not detailed, and the rationale behind the design choice to connect the gripper control nodes to all object particle nodes is not explained. \n4. It would be better to incorporate some quantitative metrics in real-world evaluation. If obtaining precise 3D ground-truth is challenging, the authors could consider alternative evaluation methods, such as non-reference metrics and a user study to assess the physical plausibility of generated trajectories.\n5. The reported inference time—92 seconds to generate a 24-frame trajectory on an A6000 GPU with 1000 sampling steps—remains prohibitively slow for any real-time application."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1GTGmUIBw1", "forum": "I2Sz167GlO", "replyto": "I2Sz167GlO", "signatures": ["ICLR.cc/2026/Conference/Submission14797/Reviewer_jfqk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14797/Reviewer_jfqk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938849098, "cdate": 1761938849098, "tmdate": 1762925146769, "mdate": 1762925146769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}