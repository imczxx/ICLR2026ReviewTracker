{"id": "d7aupcIHq0", "number": 13875, "cdate": 1758224175223, "mdate": 1759897407260, "content": {"title": "Data-Augmented Few-Shot Neural Emulator for Computer-Model System Identification", "abstract": "Partial differential equations (PDEs) underpin the modeling of many natural and engineered systems. It can be convenient to express such models as neural PDEs rather than using traditional numerical PDE solvers by replacing part or all of the PDE's governing equations with a neural network representation. Neural PDEs are often easier to differentiate, linearize, reduce, or use for uncertainty quantification than the original numerical solver. They are usually trained on solution trajectories obtained by long-horizon rollout of the PDE solver. Here we propose a more sample-efficient data-augmentation strategy for generating neural PDE training data from a computer model by space-filling sampling of local \"stencil\" states. This approach removes a large degree of spatiotemporal redundancy present in trajectory data and oversamples states that may be rarely visited but help the neural PDE generalize across the state space. We demonstrate that accurate neural PDE stencil operators can be learned from synthetic training data generated by the computational equivalent of 10 timesteps' worth of numerical simulation. Accuracy is further improved if we assume access to a single full-trajectory simulation from the computer model, which is typically available in practice. Across several PDE systems, we show that our data-augmented stencil data yield better trained neural stencil operators, with clear performance gains compared with naively sampled stencil data from simulation trajectories. Finally, with only 10 solver steps' worth of augmented stencil data, our approach outperforms traditional ML emulators trained on thousands of trajectories in long-horizon rollout accuracy and stability.", "tldr": "Space-filling, correlation-preserving data augmentation that synthesizes localized stencil data, enabling non-intrusive, few-shot (~10 time steps) training of a Neural Stencil Emulator to learn the governing dynamics of computer models.", "keywords": ["Data Augmentation", "System Identification", "Neural PDE Emulator", "Stencil Operator Learning", "Few-Shot Learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e36be49cc2159fc80690884eff4eecd591d58822.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a Neural Stencil Emulator and proposes a sample-efficient data augmentation strategy for generating PDE training data. The proposed scheme is evaluated on the Allen–Cahn, Advection–Diffusion, and Burgers’ equations."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well-motivated in its proposal of a sample-efficient training strategy for the model.\nThe authors also develop and explore a variety of data-augmentation and sampling schemes."}, "weaknesses": {"value": "1. The paper mentions “decorrelating stencil samples,” and from the experimental results, it appears that reducing correlation improves performance. However, this strategy is not well explained. For example, does this finding apply only to the proposed scheme? Could other methods also be affected by data correlation, and what is the rationale behind why decorrelation benefits this particular approach?\n\n2. The paper compares four schemes: Downsampled + (Diff Init, Extend, Random, and PCA). It is not entirely clear why the authors chose to include the Diff Init and Extend schemes, as their results do not seem to demonstrate meaningful insights. In contrast, the most effective method, Downsampled + PCA, lacks sufficient explanation regarding its implementation details, the rationale for its superior performance, and its potential limitations.\n\n3. Overall, the experimental results are insufficient. The authors conducted only one experiment comparing all four data augmentation schemes—with and without downsampling—and a comparison with three other baseline methods. More comprehensive numerical experiments are needed to illustrate the method’s use cases, flexibility, and limitations.\n\n4. In Table 1, the results for the baseline methods are significantly worse than those of the proposed scheme across all augmentation settings. There is no plausible explanation provided for this discrepancy, especially considering that the Advection–Diffusion example should not pose substantial difficulty for the baseline methods."}, "questions": {"value": "Could the authors clarify whether the proposed NSE is an original method introduced in this paper, or if it was inspired by or derived from prior work? I did not find an explicit explanation of this in the paper. It would also be helpful if the authors could elaborate on how NSE differs from existing approaches that use stencils or local data to approximate PDE updates."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KpqP5se41l", "forum": "d7aupcIHq0", "replyto": "d7aupcIHq0", "signatures": ["ICLR.cc/2026/Conference/Submission13875/Reviewer_BdFj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13875/Reviewer_BdFj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537564217, "cdate": 1761537564217, "tmdate": 1762924391806, "mdate": 1762924391806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors develop a data sampling/augmentation strategy and model to learn finite difference stencils. The method seems to work well for simple 2D PDEs, and the data augmentation aids in learning good stencils."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- In general, the paper is well written and the augmentation strategies are discussed well.\n- Learning a local neural network approximation is interesting and not conventionally done.\n- The work addresses a problem that a lot of PDE data is redundant, which is a real problem."}, "weaknesses": {"value": "### Major Concerns \n\n- The discussed baselines (FNO/Unet) should be reported using the same explicit time integrator as well. From prior work, it is known that for simple, 2D PDEs that are discretized finely in time (in your case you use 1000 timesteps), training networks to predict the current time derivative and using a temporal integrator is more effective (https://www.sciencedirect.com/science/article/pii/S0045782525002622). Reporting both the base Unet/FNO and the derivative Unet/FNO would give a better idea of where the improvement is coming from.\n    - Related to this, the accuracy of models that are based on explicit time integrators is dependent on the timestep size chosen for inference. For more complex systems, this is not even stable (i.e., even with perfect estimates of the RHS, the explicit integrator will still accumulate error), and in general, brings back much of the discretization requirements that we are hoping to avoid by training a neural PDE surrogate. \n\n- The tested cases are, in general, quite restrictive. The Allen-Cahn and Advection-Diffusion equations are relatively smooth, do not have shocks, and evolve slowly. This allows finite-difference based methods to work well. When transitioning to the Burgers equation, the presence of some shocks (although dampened by the viscosity) already makes the model perform an order of magnitude more poorly. When moving away from these toy problems into more complex phenomena (turbulence, instability, etc. (https://arxiv.org/abs/2412.00568)) I am not confident in this method.\n    - Related to this point is that for more complex systems, having a 5-dimensional representation to make a prediction is likely no longer sufficient. Either a much higher resolution would be needed (more than 32x32) to resolve the relevant scales (such as in DNS) or a full-field method can use a current, coarsened field to approximate a future, coarsened field (although this is a rather difficult problem in general)\n    - It also seems that the current method only works with periodic BCs (perhaps there is a way to approximate a one-sided stencil) and for regular grid problems (irregular meshes do not admit a consistent stencil) \n\n### Minor Concerns\n- There is marginally more cost by needing to evaluate a stencil pointwise across a field rather than predicting the full field at once with a Unet/FNO\n- There are prior works (https://www.nature.com/articles/s41598-023-39418-6, https://arxiv.org/abs/2201.01854) that seek to approximate a local stencil with a neural network, which depending on your perspective, reduces the main contribution of the work to be a data augmentation strategy."}, "questions": {"value": "- The dx and dt spacing should be made clear for the generated data and the training data. The paper says that the simulations are solved with a 32x32 grid, but that seems much too coarse for a numerical solver? I’m under the impression that the numerical simulator is run at a higher resolution and then downsampled spatially and (perhaps) temporally."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9aXUtXcaUC", "forum": "d7aupcIHq0", "replyto": "d7aupcIHq0", "signatures": ["ICLR.cc/2026/Conference/Submission13875/Reviewer_Sfiu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13875/Reviewer_Sfiu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761610303552, "cdate": 1761610303552, "tmdate": 1762924391397, "mdate": 1762924391397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a sample efficient data augmentation method for neural PDE training. Specifically, the authors propose to learn a stencil from PDE simulation data with a variety of data augmentation strategies. They propose to use combinations of random sampling, PCA-guided design, and on-trajectory downsampling to generate useful training data. The authors compare against neural operator and PINN baselines, outperforming all baselines while using significantly fewer trajectories."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-motivated and well-written. The proposed methods are novel, and the experiments are performed thoroughly. I appreciate that the authors compared against several different surrogate modeling paradigms, such as PINNs and neural operators."}, "weaknesses": {"value": "While the proposed method’s results are impressive, there are a few areas of improvement. I appreciate the authors’ comparisons with FNO, U-Net, and PINNs, but there have been many recent advancements in these architectures for neural surrogate modeling. To ensure the fairest comparisons, it would be interesting to see the proposed method being directly applied to an existing scientific machine learning benchmark (if the current datasets are not already taken from such a benchmark). This would ensure that the FNO/U-Net/PINN baselines have been thoroughly tuned for the dataset and setting at hand.\n\nHow does the proposed NSE approach perform for complex, chaotic dynamics like turbulent Navier-Stokes? Empirically demonstrating significant improvements over neural operators and PINNs in such a setting would be compelling.\n\nLastly, one of the benefits of operator learning is that it can potentially accelerate inference over numerical solvers. Since the proposed NSE method would require time-stepping just like a solver, I would recommend the authors include timing comparisons between PINNs, U-Net, FNO, the proposed method, and the ground truth numerical solver.\n\n**Minor notes:**\nIt would be very valuable for readers if the authors included parameter counts of each architecture for comparison."}, "questions": {"value": "1. Are the data/solvers used in the experiments taken from an existing scientific machine learning benchmark?\n2. Can you explain the stencil downsampling in more detail? Does this refer to coarsening the underlying grid? If so, does this negatively impact performance for fixed stencil sizes?\n3. What is the size of the “small additional compute budget” mentioned in section 3.1.2? How do the results vary as this budget is varied?\n4. Do you ever notice instabilities in rolling-out the NSE for a long time?\n5. How were the hyperparameters of the baselines optimized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "64tqjHN2Nb", "forum": "d7aupcIHq0", "replyto": "d7aupcIHq0", "signatures": ["ICLR.cc/2026/Conference/Submission13875/Reviewer_N5RJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13875/Reviewer_N5RJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678044565, "cdate": 1761678044565, "tmdate": 1762924391059, "mdate": 1762924391059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a data-augmented approach for training neural PDE surrogates in the few-shot regime. The core idea is to shift from full-trajectory-based training to learning localized stencil operators, which govern the PDE evolution at each grid cell. The authors introduce a neural stencil emulator (NSE) trained on synthetic or hybrid combinations of stencil data, leveraging random or PCA-guided sampling strategies to increase coverage of the local state space. The key claim is that, with only 10 timesteps’ worth of simulation, the NSE can outperform full-field emulators (FNO, U-Net, PINNs) trained on orders of magnitude more data. Experiments on Allen-Cahn, advection–diffusion, and Burgers’ equations validate the approach, showing strong generalization and long-horizon rollout stability under tight simulation budgets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Problem motivation is solid: Long rollouts of PDE solvers are expensive and often contain massive redundancy. The idea to cut this redundancy by working directly with stencil-level updates is both intuitive and practical.\n\n2. Technical formulation is clear and well-scoped. The NSE learns discretized RHS mappings in function space, rather than full-field transitions. This leads to much lower model complexity and dramatically higher effective sample size.\n\n3. The paper evaluates across multiple PDEs, multiple diffusion settings, and both pure/mixed sampling strategies."}, "weaknesses": {"value": "1. The method relies on learned approximations of discretized RHS terms, but there is no discussion on convergence guarantees (either for training or for rollout error accumulation). Would have been helpful to see bounds, or at least qualitative analysis on failure modes.\n\n2. The entire setup assumes access to clean simulator outputs and perfect labels. It is unclear how well the NSE would perform in a setting where the simulation is imperfect, noisy, or partially observed.\n\n3. The NSE does not enforce conservation laws, symmetries, or other physical invariants. In some PDEs, this could lead to error drift or physically implausible predictions over long rollouts.\n\n4. The baselines (FNO, U-Net) are trained and evaluated on downsampled sequences, likely due to rollout stability issues. While this is understandable, it does tilt the comparison slightly in favor of the proposed method, which works on finer temporal resolution"}, "questions": {"value": "1. Have you tried enforcing physics-informed constraints (e.g., divergence-free condition, energy conservation) in the stencil emulator? Could that help reduce rollout drift for long horizons?\n\n2. Can your approach handle variable coefficient PDEs or spatially heterogeneous systems? In those cases, the stencil operator is no longer homogeneous across space.\n\n3. Did you try training on one grid resolution and testing on another? Given the local nature of your method, it seems domain size scaling is possible, but cross-resolution generalization might still be tricky."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZOj2kYAF0C", "forum": "d7aupcIHq0", "replyto": "d7aupcIHq0", "signatures": ["ICLR.cc/2026/Conference/Submission13875/Reviewer_7qbv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13875/Reviewer_7qbv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919447145, "cdate": 1761919447145, "tmdate": 1762924390002, "mdate": 1762924390002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}