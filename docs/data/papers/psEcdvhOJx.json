{"id": "psEcdvhOJx", "number": 2826, "cdate": 1757264648051, "mdate": 1759898124755, "content": {"title": "InteractComp: Evaluating Search Agents With Ambiguous Queries", "abstract": "Language agents are increasingly deployed for web search, yet most benchmarks assume queries are fully specified and unambiguous. In practice, user queries are often incomplete and require clarification before accurate answers can be produced. To systematically evaluate this overlooked capability, we introduce INTERACTCOMP, a benchmark explicitly designed to evaluate whether agents can recognize and resolve such ambiguity by deciding when to search, when to ask clarifying questions, and when to answer. INTERACTCOMP contains 210 expert-curated questions spanning 9 domains, constructed through a systematic target-distractor methodology that ensures genuine ambiguity and controlled disambiguation. Extensive experiments on 17 models reveal striking behavioral patterns: even state-of-the-art models achieve less than 14% accuracy, not because they lack reasoning ability, but because they exhibit systematic overconfidence and underutilize interaction opportunities. Ablation and forced-interaction analyses confirm this bottleneck: when compelled to interact, models achieve significant performance gains, demonstrating latent capacity that current strategies fail to unlock. A longitudinal study further highlights a blind spot in model development, while retrieval benchmarks show rapid improvement, interactive capabilities remain stagnant. By exposing this overlooked weakness, InteractComp provides not only a diagnostic tool but also a foundation for designing agents that are uncertainty-aware, strategically interactive, and aligned with real-world user behavior.", "tldr": "", "keywords": ["Language Agent; Search Agent; DeepResearch"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/903807cb69ed6e925be208192106dc608b90f3dd.pdf", "supplementary_material": "/attachment/2adaf29333da9471bdfe77db9843fe4b44764477.zip"}, "replies": [{"content": {"summary": {"value": "1. This paper introduces InteractComp, a benchmark designed to evaluate whether search agents can recognize and resolve ambiguous user queries through strategic interaction.\n\n2. The InteractComp comprises 210 expert-curated questions, which are annotated by humans using an entity-pairing approach and a two-stage quality control protocol.\n\n3. Experiments on 17 models show that even the strongest models perform poorly mainly due to overconfidence and underuse of interaction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper demonstrates a good motivation, highlighting that human search behavior is typically iterative, beginning with ambiguous queries and progressively refining them through interaction. This perspective makes the evaluation setting more closely aligned with real-world scenarios.\n\n2. The data construction is based on an insightful idea: ambiguity arises when similar entities share overlapping attributes. The benchmark also considers domain generalization and includes data quality verification, which strengthens its credibility.\n\n3. The evaluation covers a wide range of models, including both proprietary and open-source LLMs, under different configurations and model sizes, providing a comprehensive comparison and analysis. The final conclusions attribute model failures to overconfidence and reluctance to interact, which is well-supported and reasonable."}, "weaknesses": {"value": "1. This paper can essentially be understood as targeting a multi-turn collaborative/conversational search scenario. Although InteractComp is verifiable, its query format (as exemplified in Table 1) does not show a clear distinction from complex search benchmarks like BrowseComp. The core design of BrowseComp involves multiple constraints, many of which are themselves ambiguous. Evidently, both benchmarks inherently possess interactive characteristics.\n\n2. Although I find the heuristic approach for data construction quite insightful, the decision to restrict simulated user interactions to yes/no clarifications only seems rather narrow, especially given the goal of modeling realistic user-agent interaction.\n\n3. The human annotation procedure lacks sufficient detail, making it unclear how well the constructed data aligns with real-world distributions. For example, the principles guiding human experts in selecting entities and attributes remain unspecified."}, "questions": {"value": "1. When a query contains too many constraints, it tends to become excessively long, which introduces a gap from real user scenarios. In InteractComp, is there an emphasis on shorter queries? For example, have you considered questions that do not include all constraints at the first user query but instead require the agent to actively identify ambiguity and ask for clarification?\n\n2. In the data construction process, which parts involve human participation and which rely on LLM-as-Judge? This distinction is important and should be explicitly clarified in the main text.\n\n3. How do you ensure that the ambiguity in each question reflects real semantic ambiguity rather than artificial lexical ambiguity introduced by human phrasing?\n\n4. According to Figure 1(a) in the paper, has the latest model already achieved ~70% accuracy on BrowseComp?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ktKsYZTUkJ", "forum": "psEcdvhOJx", "replyto": "psEcdvhOJx", "signatures": ["ICLR.cc/2026/Conference/Submission2826/Reviewer_dMTZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2826/Reviewer_dMTZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631937469, "cdate": 1761631937469, "tmdate": 1762916392690, "mdate": 1762916392690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper introduces a benchmark for testing search and dialogue agents on their ability to recognize and resolve *ambigious* queries. Each benchmark data instance contains (1) an ambiguous question, (2) a context string that contains distinctive attributes that are only revealed to the search agent under simulated interaction via another agent if the search agent asks for it, (3) and a single umambigious correct answer. 210 tasks total, bilingual, across 9 domains. \n\nThe dataset is deliberately constructed to be ambiguous through annotator instruction. Across 17 models tested, accuracy remains very low even for SOTA systems. \n\nThe paper represents a solid contribution towards more interactive (and realistic) search benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Addresses a clear gap in current search benchmarks- interactivity. The paper makes a compelling argument that real-life search involves this iterative refinement process, and the paper makes a step in that direction."}, "weaknesses": {"value": "Ecological validity of benchmark construction. Annotators are deliberately asked to construct queries while starting from a target answer. It remains an open question if the scoring well on the benchmark would represent meaningful and demonstrative improvement in real life search tasks. The synthetic interaction channel by forcing a yes/no response from the responder who has the context, is a strange and not fully justified design decision. A more realistic scenario could have the responder respond in free-text (i.e. a search revealed ambiguity, which then revealed a related memory of the object, such that the responder agent could be representative of memory), though controllability would remain harder in that instance. \n\nThe paper also misses out on some heavily relevant related work, notably in the \"tip of the tongue known-item search\" domain. i.e. BLUR (CH-Wang et al, ACL 2025), Tip of the Tongue Known-Item Retrieval: A Case Study in Movie Identification, Arguello et al 2021, etc."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jzSO3eY08w", "forum": "psEcdvhOJx", "replyto": "psEcdvhOJx", "signatures": ["ICLR.cc/2026/Conference/Submission2826/Reviewer_fvBE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2826/Reviewer_fvBE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845526218, "cdate": 1761845526218, "tmdate": 1762916392467, "mdate": 1762916392467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a gap between the settings of the current search benchmarks and real-world interactions. Current search benchmarks focus on fully-specified unambiguous queries and evaluate the search and reasoning capabilities of search agents, while real-world human queries are often ambiguous and require multi-turn interactions for disambiguation. To this end, they propose a new benchmark, consisting of 210 expert-written ambiguous queries, that cannot be answered without asking for clarifying questions. The authors report that the current state-of-the-art models achieve less than 14% accuracy on the benchmark, with overconfidence and lack of strategic interaction being the main causes of failures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a gap between current search benchmarks and real-world use cases of search systems.\n- To address the above-mentioned gap, the authors construct a new expert-written benchmark that specifically tests the interaction capabilities of the models.\n- The developed benchmark is not saturated (~14%) and can be used for evaluating both search and interaction capabilities of the models.\n- The authors evaluate both proprietary and open-weight models and identify varying behaviors across different models."}, "weaknesses": {"value": "### Major\n\n**Surface-level analyses**\n\nThe analyses of the models is limited to the raw numbers and comparisons across models and ablations. To support the claims and provide explanations of the observed behaviors, a deeper analysis is required. For example: why does increasing the number of interaction turns have different effects on different models?\n\nAdditionally, providing qualitative examples can provide intuition and help the researchers better understand the failure modes on the proposed benchmark. Going further, developing a taxonomy of failures can further strengthen the paper, as isolated claims based on the raw numbers are hard to synthesize into generalizable findings.\n\n**Search vs Reasoning/QA vs Interaction**\n\nWith the current ablations, it is not possible to separate the effect of the following factors that can potentially impact the final system performance: (1) search module (i.e., search engine), (2) reasoning, QA, and tool-calling capabilities, and (3) interaction capabilities.\n\nWhat are the proportion of the cases where the system failed due to each of the three factors above? Studying additional ablations in Table 3 can provide more transparecy into the failure modes.\n\n***\n\n### Minor\n\n**Literature review**\n\n- The paper misses literature on real-world human-AI interaction datasets (Wildchat, Chatbot Arena). Specifically, as the paper focuses on real-world ambiguous under-specified queries to search-augmented models, Search Arena, a recent dataset of human interactions with search-augmented LLMs, is highly relevant. Analyzing the ambiguity of natural user-written prompts and using them as part of the benchmark can improve the realism and utility of the benchmark.\n\n- Additionally, the paper does not have citations for the works mentioned in the second part of the Section 2 (Lines 140-144).\n\n**Implementation details**\n\nHow is search implemented? What search engine is used?\n\n**Presentation and missing details**\n\n- Algorithm 1 is not properly grounded and explained in text.\n\n- The paper misses explanation of the calibration error (C.E.). There is a brief mention of C.E. in Appendix; however, as it is references multiple times in analysis, it would be helpful to include an explanation in the main body of the paper."}, "questions": {"value": "- Does the `with-context` baseline (Table 3) have access to search? Why are the numbers low even with access to the hidden context? Is it because the failure of the search engine?\n\n- How is search implemented? What search engine is used?\n\n- What are qualitative examples of the failures backing up the claims in the paper (e.g., *\"strategic information acquisition is a distinct capability varying significantly across architectures\"*)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nmZpI8pWv9", "forum": "psEcdvhOJx", "replyto": "psEcdvhOJx", "signatures": ["ICLR.cc/2026/Conference/Submission2826/Reviewer_rgYF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2826/Reviewer_rgYF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950303286, "cdate": 1761950303286, "tmdate": 1762916392171, "mdate": 1762916392171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a benchmark for evaluating agents’ abilities to navigate tasks with ambiguities or ambiguous queries. This is an important use case scenario. While the field is working towards fully autonomous agents, it’s also important to think about potential issues of premature commitment, where agents act confidently despite uncertainty. The data is built from 210 bilingual expert-written question-context pairs across nine domains, where each example comes with a yes/no question to distinguish between a correct target and a possible distractor. The authors evaluate 17 models and find that accuracy drops significantly under ambiguity, but improves a lot when agents are forced to ask questions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed benchmark targets an important scenario: in real life, user queries are often ambiguous or underspecified. To effectively address user queries, models need to know when and how to interact with users to resolve these ambiguities.\n- The answers are short and easily verifiable, which reduces subjectivity in evaluation.\n- Experiments reveal clear and useful empirical insight that overconfidence and lack of uncertainty awareness is a core weakness."}, "weaknesses": {"value": "- Based on the example in Table 1, the query appears to be something a model could likely resolve using its internal world knowledge rather than through web search. During data construction, there was no verification that these queries require external search to be answered correctly. If that’s the case, the rationale for putting the task in an agentic, search-based setup becomes unclear, especially since prior work has already shown that language models struggle to ask clarification questions.\n- There is no human performance baseline that measures: when human users are asked to answer these queries, how often will they answer directly without asking clarification questions? This comparison would help contextualize model behavior and clarify whether current systems truly underperform relative to humans.\n- There is no information about how many experts were involved in data creation, which platform they were recruited through, or what their credentials or backgrounds are.\n- Important statistics on the dataset are missing: how many items are there per domain? What about language distribution by domain?"}, "questions": {"value": "- It would be helpful for the authors to clarify how many examples in the dataset genuinely requires web search to resolve. An analysis quantifying how many items truly depend on external information would strengthen the paper’s positioning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OqOD9EtRp2", "forum": "psEcdvhOJx", "replyto": "psEcdvhOJx", "signatures": ["ICLR.cc/2026/Conference/Submission2826/Reviewer_QM9j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2826/Reviewer_QM9j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958353249, "cdate": 1761958353249, "tmdate": 1762916391968, "mdate": 1762916391968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}