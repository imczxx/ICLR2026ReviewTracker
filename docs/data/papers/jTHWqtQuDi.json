{"id": "jTHWqtQuDi", "number": 23919, "cdate": 1758350350723, "mdate": 1759896790368, "content": {"title": "Beyond Truthfulness: Evaluating Honesty in Large Language Models", "abstract": "As large language models (LLMs) become more capable and agentic, the requirement for trust in their outputs grows significantly, yet at the same time concerns have been mounting that models may learn to lie in pursuit of their goals. To address these concerns, a body of work has emerged around the notion of \"honesty\" in LLMs, along with interventions aimed at mitigating deceptive behaviors. However, some benchmarks claiming to measure honesty in fact simply measure accuracy—the correctness of a model's beliefs—in disguise. Moreover, no benchmarks currently exist for directly measuring whether language models lie. In this work, we introduce a large-scale human-collected dataset for directly measuring lying, allowing us to disentangle accuracy from honesty. Across a diverse set of LLMs, we find that while larger models obtain higher accuracy on our benchmark, they do not become more honest. Surprisingly, most frontier LLMs obtain high scores on truthfulness benchmarks yet exhibit a substantial propensity to lie under pressure, resulting in low honesty scores on our benchmark. We find that simple methods, such as representation engineering interventions, can improve honesty. These results underscore the growing need for robust evaluations and effective interventions to ensure LLMs remain trustworthy.", "tldr": "We introduce a benchmark showing larger LLMs are more accurate but not more honest, highlighting the need for honesty interventions.", "keywords": ["AI safety", "ML safety", "honesty", "deception", "benchmarking"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/14e2a96f09232eb21624cb704134d1e476ef696c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper argues that a model's \"accuracy\" and \"honesty\" are not strongly related. It introduces the MASK benchmark, which assesses a model's honesty by comparing its responses under \"Pressure Prompt\" versus in \"Belief Elicitation Prompt\" situations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Evaluating a model's honesty by checking the consistency between its responses to a \"Pressure Prompt\" and a \"Belief Elicitation Prompt\" is a reasonably sound approach, as it helps rule out false accusations of dishonesty caused by inaccurate answers.\n\n2. The insight reported by the authors is highly meaningful: larger models possess more accurate knowledge yet exhibit lower honesty. This indicates that we cannot simply resolve the honesty issue by scaling up models or increasing data size; instead, we should treat this as a critical challenge in AI safety that warrants special attention."}, "weaknesses": {"value": "1. Treating honesty as the model's internal belief and distinguishing it from accuracy and truthfulness is not a novel contribution of this paper; this distinction has already been discussed and organized in prior survey [1].\n2. The authors do not explain why the core finding that \"larger models exhibit lower honesty\" occurs. Model scale alone cannot serve as a direct cause of dishonesty.\n3. The authors appear to consider only one form of dishonesty (when the model’s output contradicts the ground truth), but overlook another important form: dishonesty through incomplete or selectively biased output. For example, a model might produce statements that are factually consistent with its true belief but deliberately omit negative or unfavorable aspects while highlighting only positive ones, thereby distorting the overall meaning.\n4. The authors do not consider multilingual settings, despite the fact that honesty can vary significantly across different languages.\n\n[1] A Survey on the Honesty of Large Language Models"}, "questions": {"value": "1. What is the underlying reason for the observation that larger models exhibit lower honesty?\n2. If a model’s outputs appear factually correct and not overtly deceptive, but the omission of certain information leads to a significantly distorted or misleading overall meaning, how should such behavior be analyzed?\n3. Is the evaluation framework adaptable to multilingual settings?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YsMMImWOoZ", "forum": "jTHWqtQuDi", "replyto": "jTHWqtQuDi", "signatures": ["ICLR.cc/2026/Conference/Submission23919/Reviewer_6iaV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23919/Reviewer_6iaV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730627249, "cdate": 1761730627249, "tmdate": 1762942858347, "mdate": 1762942858347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MASK (Model Alignment between Statements and Knowledge), a benchmark designed to evaluate \"honesty\" in large language models by measuring lies of commission - instances where models knowingly make false statements under pressure. The authors distinguish between accuracy (factual correctness) and honesty (consistency between beliefs and statements), proposing a novel evaluation pipeline that elicits model beliefs and compares them against statements made under pressure. The study evaluates 30 LLMs and finds that while larger models are more accurate, they don't become more honest, with many frontier models showing substantial propensity to lie under pressure."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a critical and timely problem in AI safety: the common conflation of \"honesty\".\n\n2. Easy to follow. \n\n3. The work introduces a novel large-scale dataset of over 1,000 public samples, and the pressure prompts are human-collected. A major strength is that the pressure prompts are human-collected and curated according to thoughtful design principles, such as avoiding unrealistic placeholders (\"ABC Company\") or clearly fictional settings, which makes the evaluation scenarios more compelling and realistic."}, "weaknesses": {"value": "1. Potential overclaim. This paper claims that accuracy differs from honesty. However, [1] already proposed similar idea that \"Second, honesty is specific to each model, as it requires identifying the model’s known and unknown knowledge, making both its evaluation and improvement challenging.\"\n\n2. Concern about belief detection. This paper proposes to detect the beliefs of LLMs by consistency in responses. However, we can not rely on responses if we do not know if LLMs are honest or not. It is a circular reasoning trap. One possible way is using probing or the date of collected training corpus, as introduced in [1].\n\n3. The  Low-Rank Representation Adaptation (LoRRA) are evaluated only on smaller Llama 2 7B and 13B models. Performances on larger models are required. \n\n[1] S. Li et al. A Survey on the Honesty of Large Language Models, TMLR, Mar 2025. arxiv 2409.18786"}, "questions": {"value": "1. How do you address the circular reasoning problem in belief detection? If models might be dishonest in their responses, how can we trust their responses to determine their beliefs?\n\n2. Can you provide validation that pressure prompts create realistic incentives rather than just adversarial conditions that might not reflect real-world deception scenarios?\n\n3. How might alternative belief elicitation methods such as probing techniques or training data analysis compare to the consistency-based approach used here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lp932nbKkh", "forum": "jTHWqtQuDi", "replyto": "jTHWqtQuDi", "signatures": ["ICLR.cc/2026/Conference/Submission23919/Reviewer_FLsH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23919/Reviewer_FLsH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814020928, "cdate": 1761814020928, "tmdate": 1762942857394, "mdate": 1762942857394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MASK, a large-scale, human-curated benchmark to measure honesty in LLMs by explicitly disentangling honesty  from accuracy. Evaluating 30 LLMs, the authors find that while larger models are more accurate, they are not more honest—and can readily produce lies of commission under pressure. The paper further reports initial honesty interventions (system prompts, representation-engineering) that improve honesty but do not close the gap."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear concept:  separates honesty from accuracy, addressing a common conflation in prior work;\n2. Scale and coverage: ~1.5K carefully curated, realistic, human-written scenarios spanning multiple archetypes, plus 30 frontier models evaluated—strong empirical breadth;\n3. Interesting findings: revealing result that scale boosts accuracy but not honesty, motivating research on safety interventions beyond pure capability scaling."}, "weaknesses": {"value": "1. How “belief” is collected. Using consistent answers under neutral prompts is one choice, but please compare it with other ways to get a model’s belief and discuss other explanations (e.g., different knowledge indentification methods);\n2. Prompt sensitivity. Results may change with small wording changes in the pressure or belief prompts. Add tests with paraphrases and different pressure strength to show the results are stable;\n3. Missing data details: The paper defines six dishonesty types but doesn’t report how many samples each contains and peformance on different types;\n4. Dataset accessibility: As a dataset/benchmark paper, it is recommended to upload a supplementary material or anonymous link for reviewers to check the dataset."}, "questions": {"value": "See the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wA560bK0H2", "forum": "jTHWqtQuDi", "replyto": "jTHWqtQuDi", "signatures": ["ICLR.cc/2026/Conference/Submission23919/Reviewer_vCtB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23919/Reviewer_vCtB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978908143, "cdate": 1761978908143, "tmdate": 1762942856305, "mdate": 1762942856305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}