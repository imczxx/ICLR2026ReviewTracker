{"id": "kC4JPVo62f", "number": 12409, "cdate": 1758207610794, "mdate": 1759897511702, "content": {"title": "DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous Driving", "abstract": "Effective autonomous driving hinges on robust reasoning across perception, prediction, planning, and behavior. However, conventional end-to-end models fail to generalize in complex scenarios due to the lack of structured reasoning. While recent vision-language models (VLMs) have been applied to driving tasks, they typically rely on isolated modules and static supervision, limiting their ability to support multi-stage decision-making.\n  We present AutoDriveRL, a unified training framework that formulates autonomous driving as a structured reasoning process over four core tasks. Each task is independently modeled as a vision-language QA problem and optimized using task-specific reward models, enabling fine-grained reinforcement signals at different reasoning stages. \n  Within this framework, we train DriveRX, a cross-task reasoning VLM designed for multi-stage decision-making. DriveRX achieves strong performance on the public benchmark, outperforming GPT-4o in behavior reasoning and demonstrating robustness under complex or corrupted driving conditions. We further explore the application of VLMs in autonomous driving, including trajectory prediction and action execution. We will release the AutoDriveRL framework and the DriveRX to support future research.", "tldr": "", "keywords": ["Visual Reasoning", "Vision Language Model", "Autonomous Driving"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aab5a6a51efc76f8a9853c541babf66687c5798d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents DriveRX, a vision-language-action model designed to enhance reasoning and decision-making for autonomous driving. The authors aim to leverage language-based reasoning to handle complex or ambiguous driving cases and show some performance improvements over generalist vision-language models. The paper also includes an evaluation on a trajectory prediction benchmark to demonstrate the model’s potential for better planning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper shows a clear pipeline for data construction and training framework of VLM training, and it shows some improvement over general-purpose models like Qwen and other large VLMs, which suggests that domain-specific fine-tuning helps the model better understand driving scenes."}, "weaknesses": {"value": "I have some concerns about the motivation and experimental setup. Most recent works that use VLMs for driving focus on long-tail reasoning or trajectory prediction, and they evaluate on clear benchmarks with established baselines. This paper shows results in Table 3, but the setup isn’t well explained, so it’s hard to understand what the numbers actually mean or how fair the comparison is.\n\nThe model is not compared against any strong end-to-end driving systems, such as UniAD or VAD, which are the standard baselines for trajectory prediction.\n\nIt’s also missing comparisons with other VLM-based methods, such as DriveVLM, OmniDrive, or ORION. In fact, ORION is a very close baseline, and based on the results here, this proposed method doesn’t outperform it.\n\nOverall, the improvement seems marginal, and the paper doesn’t convincingly show that this method leads to any meaningful gains in driving performance. The motivation would be stronger if the authors could clearly demonstrate where this approach provides a unique advantage over existing systems."}, "questions": {"value": "Can the authors clarify the evaluation setup for Table 3? How are the metrics calculated? \n\nWhy are the results not compared with other domain-specific models (e.g., UniAD, VAD, DriveVLM, OmniDrive, ORION)?\n\nWhat's the main usecase for this VLM? Is it aiming for improving existing self driving system?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PhOvXcD01m", "forum": "kC4JPVo62f", "replyto": "kC4JPVo62f", "signatures": ["ICLR.cc/2026/Conference/Submission12409/Reviewer_Xhmu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12409/Reviewer_Xhmu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761276807872, "cdate": 1761276807872, "tmdate": 1762923306373, "mdate": 1762923306373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DriveRX, a vision-language reasoning model for autonomous driving trained via the AutoDriveRL reinforcement learning framework.\nAutoDriveRL decomposes driving into perception, prediction, planning, and behavior tasks, each optimized with task-specific reward models.\nUsing structured reasoning and joint RL training, DriveRX achieves strong generalization and robustness on DriveBench and DriveLM-Hard benchmarks, outperforming GPT-4o in behavior reasoning.\nThe model also supports downstream trajectory prediction and action generation through reasoning-based distillation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Proposes a unified RL framework (AutoDriveRL) enabling interpretable, multi-stage reasoning across driving subtasks.\n\n+ Demonstrates state-of-the-art performance and robustness, surpassing larger models under both clean and corrupted conditions.\n\n+ Extends practical impact by showing reasoning-enhanced transferability to trajectory and control tasks."}, "weaknesses": {"value": "**For the image input**: In the structured reasoning process you designed, including prediction and planning, you only feed the model multi-view images, which makes it very hard to ensure the model can capture temporal information like speed — this doesn’t really make sense. Although some other VLAs also only use images, they at least provide historical ego‐states to ensure the ego car’s temporal information.\n\n**Application**: The authors design a very complex reasoning process; I am quite puzzled about what scenario the authors expect their work to be applied in. If it’s just high-level prediction, then they have not opened up a large gap compared with current open-sourced VLMs such as InternVL3-8B, Qwen3-VL-8B, etc. (And I must emphasize: the authors test on DriveBench but that is not truly OOD because both DriveRX and DriveBench data are constructed based on DriveLM.) I am not sure whether DriveRX has better performance than the open-sourced VLMs in other domains of driving scenarios (e.g., DriveAction).\n\n**Latency**: Because I am confused about the application of DriveRX. If it is used for low-level planning, then latency is a big challenge; and based on Table 3 results, its performance gap with existing VLAs model is significant (e.g., Auto-VLA)."}, "questions": {"value": "+ Could the authors clarify the time latency when using an LLM for scoring during training?\n\n+ For the prediction and planning tasks, did the authors explore using a rule-based reward model? If so, how does it compare (advantages/disadvantages) to the LLM-based reward?\n\n+ Could the authors provide results on the AlphaDrive benchmark for planning, so we can assess true out-of-distribution (OOD) performance?\n\n+ Based on Table 8, I don’t understand why the gap between SFT (supervised fine-tuning) and RL in the behavior task is so large—this phenomenon isn’t observed in other related work. Could the authors explain?\n\n+ **Most importantly, I’d like the authors to clearly articulate the intended application scenarios for DriveRX:**\n\n(i) As a strong backbone for low-level planning tasks?\n\n(ii) As a data annotator for long-tail driving scenarios?\n\n(iii) As a reward model to improve other VLA/VLMs’ low-level planning performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cNYiTyCmaX", "forum": "kC4JPVo62f", "replyto": "kC4JPVo62f", "signatures": ["ICLR.cc/2026/Conference/Submission12409/Reviewer_3U7X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12409/Reviewer_3U7X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844568629, "cdate": 1761844568629, "tmdate": 1762923305912, "mdate": 1762923305912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AutoDriveRL, a reinforcement learning (RL) framework for vision-language models (VLMs) applied to autonomous driving, decomposing driving into four interpretable sub-tasks: Perception, Prediction, Planning, and Behavior. Each sub-task is framed as a VQA problem with a task-specific reward model. Based on this framework, the authors train DriveRX, a cross-task reasoning VLM that aims to unify multi-stage decision-making. Experiments on DriveBench and DriveLM-Hard show that DriveRX outperforms GPT-4o and other baselines in the Behavior task and maintains robustness under visual corruptions. They further extend DriveRX to (1) DriveRX-Agent for trajectory generation and (2) DriveRX-VLA for closed-loop control via reasoning-based distillation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This study introduces a reinforcement-learning–driven, multi-task vision-language reasoning framework designed to enhance the coordination between perception, reasoning, and action in autonomous driving. By attaching task-specific reward models to each stage of the reasoning process, the approach provides finer-grained supervision, improving interpretability and stability.\n\nIt organizes the problem into four subtasks that reflect the cognitive pipeline of autonomous driving—from perception to behavior. A Visual Question Answering (VQA)–style data formulation is used to unify the data structure, supporting cross-task consistency and a cleaner, more integrated learning signal.\n\nEmpirical results show that DriveRX achieves 62.02 on the Behavior task, outpacing GPT-4o (55.04) and the strongest open-source vision-language models. The method also demonstrates robust performance under visual corruption, underscoring its resilience. Baseline comparisons cover a broad spectrum, including commercial models (e.g., GPT-4o), generalist models (LLaVA, Qwen2.5-VL), reasoning-oriented models (MM-Eureka, R1-OneVision), and domain-specific systems (DriveLM, Dolphins), all evaluated under consistent settings.\n\nBeyond core reasoning, the approach extends to trajectory prediction and control through DriveRX-Agent and DriveRX-VLA. These extensions illustrate how high-level reasoning can be bridged with low-level action generation, pointing toward a unified driving agent that blends perception, reasoning, and control in a single framework."}, "weaknesses": {"value": "1. Ambiguous Reinforcement Learning Details\nEquation (2) defines GRPO, but the intra-group advantage term (Eq. 3) is shared across tokens, not time-dependent, this simplifies training but may degrade credit assignment.\nNo mention of reward normalization schedule, policy rollout horizon, or sampling temperature, which are essential for RL reproducibility.\nThe rule-based and LLM-based reward models are described qualitatively but lack explicit scoring functions, thresholds, or examples (Appendix M.1 is referenced but not shown in main text).\n\n2. Missing Quantitative Ablations\nThe framework introduces multiple key design choices (reward composition, GRPO vs PPO, per-task weighting, data filtering), yet no ablation results are provided in the main paper.\nFor example, how much does AutoDriveRL contribute compared to supervised fine-tuning on the same data? The gain from GRPO remains unquantified.\n\n3. Unclear Task Coupling\nThe paper claims cross-task reasoning, but tasks are trained independently with shared parameters, without conditioning on previous task outputs (explicitly stated in §3.1).\nThis design limits true reasoning chaining; the reasoning chain is only semantic, not computationally causal. Thus, DriveRX may not genuinely achieve process-level reasoning across tasks.\n\n4. Evaluation Ambiguities\nThe GPT score metric is used exclusively, judged by GPT-3.5-Turbo. This raises reliability and circularity issues — the model is trained with LLM-based rewards and then evaluated by another LLM judge.\nThere is no mention of inter-rater agreement, or calibration with human labels (Appendix I claims alignment but gives no correlation coefficients in the main text).\nMetrics like ADE/Col. in Table 3 and Driving Score in Table 4 are reported but lack variance or standard deviation, leaving statistical significance unclear.\n\n5. Distillation Pipeline Unclear\nIn §6, reasoning data from DriveRX is distilled into DriveRX-VLA, but the exact mapping from language to action tokens and supervision loss are unspecified.In §6, reasoning data from DriveRX is distilled into DriveRX-VLA, but the exact mapping from language to action tokens and supervision loss are unspecified.\nIt’s not clear whether the VLA model receives image input, text prompts, or both during inference — this affects the claimed “unified backbone” property.\n\n6. Interpretability and Example Limitation\nOnly one qualitative case (Figure 3) is provided. No multi-turn reasoning trace or counterfactual analysis is shown, weakening claims of interpretability."}, "questions": {"value": "1. Ambiguous Reinforcement Learning Details\nEquation (2) defines GRPO, but the intra-group advantage term (Eq. 3) is shared across tokens, not time-dependent, this simplifies training but may degrade credit assignment.\nNo mention of reward normalization schedule, policy rollout horizon, or sampling temperature, which are essential for RL reproducibility.\nThe rule-based and LLM-based reward models are described qualitatively but lack explicit scoring functions, thresholds, or examples (Appendix M.1 is referenced but not shown in main text).\n\n2. Missing Quantitative Ablations\nThe framework introduces multiple key design choices (reward composition, GRPO vs PPO, per-task weighting, data filtering), yet no ablation results are provided in the main paper.\nFor example, how much does AutoDriveRL contribute compared to supervised fine-tuning on the same data? The gain from GRPO remains unquantified.\n\n3. Unclear Task Coupling\nThe paper claims cross-task reasoning, but tasks are trained independently with shared parameters, without conditioning on previous task outputs (explicitly stated in §3.1).\nThis design limits true reasoning chaining; the reasoning chain is only semantic, not computationally causal. Thus, DriveRX may not genuinely achieve process-level reasoning across tasks.\n\n4. Evaluation Ambiguities\nThe GPT score metric is used exclusively, judged by GPT-3.5-Turbo. This raises reliability and circularity issues — the model is trained with LLM-based rewards and then evaluated by another LLM judge.\nThere is no mention of inter-rater agreement, or calibration with human labels (Appendix I claims alignment but gives no correlation coefficients in the main text).\nMetrics like ADE/Col. in Table 3 and Driving Score in Table 4 are reported but lack variance or standard deviation, leaving statistical significance unclear.\n\n5. Distillation Pipeline Unclear\nIn §6, reasoning data from DriveRX is distilled into DriveRX-VLA, but the exact mapping from language to action tokens and supervision loss are unspecified.In §6, reasoning data from DriveRX is distilled into DriveRX-VLA, but the exact mapping from language to action tokens and supervision loss are unspecified.\nIt’s not clear whether the VLA model receives image input, text prompts, or both during inference — this affects the claimed “unified backbone” property.\n\n6. Interpretability and Example Limitation\nOnly one qualitative case (Figure 3) is provided. No multi-turn reasoning trace or counterfactual analysis is shown, weakening claims of interpretability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8ZF4MraE2p", "forum": "kC4JPVo62f", "replyto": "kC4JPVo62f", "signatures": ["ICLR.cc/2026/Conference/Submission12409/Reviewer_6G7c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12409/Reviewer_6G7c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946114083, "cdate": 1761946114083, "tmdate": 1762923304119, "mdate": 1762923304119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AutoDriveRL, a unified reinforcement learning framework that models autonomous driving as a structured reasoning process across perception, prediction, planning, and behavior. It trains a cross-task reasoning model, DriveRX, which outperforms GPT-4o in behavior reasoning and shows strong robustness in complex driving scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The formulation of each sub-task as a vision-language QA problem is interesting.\n\n2. The experimental results are promising."}, "weaknesses": {"value": "1. The introduction section feels somewhat wordy and difficult to follow. It’s not clearly structured around three key points: what common issues exist in current works, what direction and method are proposed, and how the proposed method is designed. These aspects should be laid out more clearly.\n\n2. Additionally, the paper categorizes the autonomous driving pipeline into perception, prediction, planning, and behavior. Typically, autonomous driving is divided into three main tasks, perception, prediction, and planning, while behavior is often treated as part of prediction. What is the rationale for introducing behavior as a separate stage?\n\n3. The description of the task-specific reward models lacks clarity. How the reinforcement signals are computed and whether they are consistent across tasks could be better elaborated.\n\n4. It’s unclear how much each reasoning stage (perception, prediction, etc.) contributes to overall performance,  finer-grained ablation or visualization would improve interpretability."}, "questions": {"value": "1. How robust is DriveRX to distribution shifts, e.g., unseen weather or sensor noise, beyond the “corrupted” conditions mentioned?\n\n2. Are there any failure cases observed during deployment or simulation that reveal limits in the cross-task reasoning design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z2xUIk9no3", "forum": "kC4JPVo62f", "replyto": "kC4JPVo62f", "signatures": ["ICLR.cc/2026/Conference/Submission12409/Reviewer_7UfE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12409/Reviewer_7UfE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762219625396, "cdate": 1762219625396, "tmdate": 1762923303788, "mdate": 1762923303788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}