{"id": "QAwGkFD8ES", "number": 9274, "cdate": 1758117074571, "mdate": 1759897733995, "content": {"title": "SpectrumKD: Dynamic Dataset Curation for Distribution-Aware Knowledge Distillation of Large Language Models", "abstract": "Knowledge Distillation (KD) is a critical technique for compressing large language models (LLMs) into efficient student models while preserving performance, yet its efficacy remains highly sensitive to training data quality. Current dataset curation approaches mainly focus on quality and information at the instance level, neglecting the global distribution characteristics of the entire training dataset. This oversight often results in suboptimal data selection that degrades distillation outcomes. To address this limitation, we propose SpectrumKD, a principled data curation framework that dynamically refines training datasets across epochs by leveraging the global distribution of instance difficulty. SpectrumKD constructs a difficulty spectrum over the training corpus by ranking instances based on student model evaluation, partitioning them into four distinct learning phases: Early Learning, Continuous Learning, Late Learning, and No Learning. A sliding window segmentation strategy then selects epoch-specific subsets by adaptively shifting a fixed window across the spectrum from low to high difficulty, to ensure an uniform increase in subset difficulty across training epochs. As a plug-and-play module, SpectrumKD enhances diverse white-box KD methods and model architectures with minor computational cost. Extensive experiments across multiple language model benchmarks demonstrate consistent performance gains in distilled models, with improvements observed under varied KD approaches and model families. Crucially, SpectrumKD achieves these gains without modifying core distillation algorithms, highlighting the pivotal role of dataset distribution features and data compatibility in effective LLM distillation. Our work establishes a data-centric paradigm for KD, providing both insights and tools to advance the efficiency and capability of compressed language models.", "tldr": "", "keywords": ["Large Language Models", "Knowledge Distillation", "Data Curation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ca413ce08a55d07c2b81476e19f916b414c6b834.pdf", "supplementary_material": "/attachment/427e00ec958eb40fdcf9d47a2e243a1d95ee7dcd.zip"}, "replies": [{"content": {"summary": {"value": "SpectrumKD is a pragmatic data-curation layer for white-box knowledge distillation. The authors first compute per-example cross-entropy with an untrained student to build a global “difficulty spectrum,” partitioned into four zones (Early/Continuous/Late/No Learning). Training then uses a fixed-size sliding window that moves from easy to hard across epochs so that the aggregate difficulty of the active subset increases roughly uniformly. A linear temperature ramp softens teacher logits in sync with this progression. The module is drop‑in—it does not change model architectures or KD losses—and is evaluated with several objectives (KLD, JSD, SRKL, on‑policy GKD) across instruction following, math reasoning, and code generation. Empirically, SpectrumKD delivers consistent, modest gains with small overhead, and the paper includes ablations on difficulty metrics, zone thresholds, and scheduling choices."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Practical and plug‑and‑play: one offline scoring pass and a lightweight scheduler; compatible with many KD losses and model pairs.\n- Clear, coherent design: difficulty spectrum → four-zone partition → sliding window → temperature ramp; figures make the workflow easy to follow.\n- Broad empirical sweep: multiple tasks, families (GPT‑2/OpenLLaMA2/Qwen2.5), and losses; component ablations and sensitivity studies are provided.\n- Sensible motivation: emphasizes dataset distribution and student–data compatibility rather than only instance‑level “informativeness,” which is often overlooked in KD.\n- Low engineering cost: improvements achieved without touching core KD objectives or model code; overhead appears minor."}, "weaknesses": {"value": "- Limited conceptual novelty: CE‑based difficulty ranking, curriculum‑style progression, and temperature ramping are established ideas; the contribution reads as a careful integration rather than a new principle. The distinction from competence‑based CL, uncertainty/divergence sampling (e.g., teacher–student KL, SKD/DDS), and budgeted on‑policy data selection is not sufficiently sharp.\n- Confounded difficulty metric: cross‑entropy correlates with sequence length, domain, and templating. The paper does not report partial‑correlation or length‑controlled analyses, nor task‑specific difficulty signals (e.g., executability for code, logical step correctness for math). This leaves open whether the spectrum primarily captures length/style rather than genuine hardness.\n- Budget fairness is under-specified: filtering out extreme hard samples may change effective update density. It’s unclear whether baselines are matched on tokens, steps, and wall‑clock time; stronger recent sampling baselines under strict budget parity are missing.\n- Generalization gaps: results focus on white‑box KD; applicability to black‑box settings, larger modern teachers (e.g., Llama‑3/Mixtral), and stronger students (≥7B) is not demonstrated.\n- Reporting and reproducibility: significance testing is inconsistent across tables; implementation details are scattered. Releasing scoring caches, subset indices, and configs would materially improve reproducibility."}, "questions": {"value": "1) Positioning and novelty\n- In one sentence, what is the genuinely new principle beyond integrating CE-based difficulty + curriculum sliding + temperature ramp? What can SpectrumKD do that competence-based CL or divergence/uncertainty sampling cannot?\n- Which prior methods is SpectrumKD most likely to be confused with? Please spell out the decisive differences and why those matter empirically.\n\n2) Difficulty metric and confounds\n- How correlated is your CE-based difficulty with sequence length and domain/source? Could you share a simple correlation table or length-bucket analysis to show the spectrum isn’t just a length proxy?\n- For math/code, CE can miss “one critical mistake” semantics. Have you tried task-specific difficulty signals (e.g., executability for code, step-consistency for math)? Do they change the spectrum or results meaningfully?\n- What happens with noisy or mislabeled data—does the method simply banish them to “No Learning,” and could that hurt robustness?\n\n3) Budget fairness\n- Across all main tables, are tokens, steps, and wall-clock strictly matched to baselines? If not all three, which two are matched, and where could mismatches inflate gains?\n- Filtering out extreme hard samples can increase effective update density. Can you run a control that keeps the full dataset but adjusts steps/learning rate so “effective updates” are comparable?\n\n4) Scheduler behavior\n- Please describe, in concrete terms, how the window moves when the spectrum has cliffs or multiple modes. How do you prevent oscillation or big jumps? Any smoothing or max-step constraints?\n\n5) Temperature vs. difficulty\n- Are the gains from the temperature ramp independent of the sliding window? A small 2D ablation (several ramps × several sliding schemes) would clarify whether one carries most of the lift.\n\n6) Thresholds and adaptivity\n- Are the four-zone cutoffs fixed across tasks, or tuned? Would a simple adaptive rule (e.g., quantiles chosen to stabilize validation loss) work as well or better?\n\n7) Treatment of very hard examples\n- Instead of excluding them forever, did you try bringing them back late with a small weight (hard replay/contrastive replay)? Any effect on robustness or long‑tail generalization?\n\n8) Scope and scalability\n- Do you expect similar benefits in black-box KD (teacher logits only)? What’s the simplest way to approximate your spectrum there?\n- Have you tried larger modern teachers (e.g., Llama‑3/Mixtral) or stronger students (≥7B)? Do optimal window sizes/thresholds shift with scale?\n\n9) Metrics and significance\n- Can you report variance and significance consistently across the main results, and—where feasible—add stronger task-relevant metrics (e.g., executable pass@k for code, better LLM-as-a-judge or limited human eval for instruction)?\n\n10) Reproducibility and data transparency\n- Will you release the scoring cache, subset indices, and exact configs to let others reproduce the curves? Also, a brief note on dataset licensing/filters and any basic bias checks would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IdGYBkezIU", "forum": "QAwGkFD8ES", "replyto": "QAwGkFD8ES", "signatures": ["ICLR.cc/2026/Conference/Submission9274/Reviewer_Trz4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9274/Reviewer_Trz4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761383760661, "cdate": 1761383760661, "tmdate": 1762920921227, "mdate": 1762920921227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SpectrumKD, a dynamic dataset curation framework designed to enhance knowledge distillation for large language models by prioritizing global distribution-aware data selection. SpectrumKD constructs a difficulty spectrum by ranking all training instances based on their cross-entropy loss as evaluated by an initial student model. This spectrum is then partitioned into four distinct zones. Based on this partitioning, the framework employs a sliding window curriculum scheduler that progressively shifts across the spectrum from easier to harder instances over the course of training epochs. The method is evaluated across multiple benchmarks, demonstrating consistent performance gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The partitioning of data into distinct learning phases based on difficulty, coupled with adaptive curriculum scheduling, is well-motivated by empirical and theoretical insights. The plug-and-play design, which integrates seamlessly with existing KD methods without modifying core algorithms, is a practical strength."}, "weaknesses": {"value": "1. As the authors mention in the limitations, SpectrumKD is primarily based on the assumption that the distribution of instance difficulty follows a log-normal pattern, which depends on the distribution of the dataset. For datasets whose difficulty spectrum deviates significantly from log-normality (e.g., those that are extremely easy or extremely difficult), the values of λa and λb may be severely skewed, and the effectiveness of SpectrumKD has not been validated in such cases.\n2. In Section 3.2, the difficulty metric is defined as the cross-entropy loss Li = −log qθ(yi|xi). It is unclear whether this refers to the total sequence loss or the length-normalized (average) loss. Using the total loss would introduce a significant bias, conflating sequence length with intrinsic difficulty. \n3. There are several presentation issues that merit correction: duplicate section titles (“Comparison with Traditional Curriculum Learning” appears twice in Section 5.4 and Section 5.5), minor article/capitalization errors (e.g., “an uniform” should be “a uniform”), and inconsistent table cross-references (e.g., line 320 referring to “Table 3” for main instruction-following results, which are in Table 1 here)."}, "questions": {"value": "1. Since the learning ability of the student model increases during training, the initial estimation of instance difficulty may be biased. Have you tried periodically re-estimating the difficulty spectrum (e.g., every few epochs) to adapt to the evolving student model, thereby enabling a more dynamic approach to dataset curation?\n2. Is the performance where curriculum scheduler (1), (2), and (3) are used simultaneously missing in Table 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RSMGS34rVu", "forum": "QAwGkFD8ES", "replyto": "QAwGkFD8ES", "signatures": ["ICLR.cc/2026/Conference/Submission9274/Reviewer_Q38k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9274/Reviewer_Q38k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876708946, "cdate": 1761876708946, "tmdate": 1762920920806, "mdate": 1762920920806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SpectrumKD, a curriculum learning-based data utilization strategy for traditional off-policy distillation. The method measures the difficulty of data based on the cross-entropy loss of the untrained student model, using thresholding to categorize instance difficulty. The most difficult fraction of samples are discarded during training, while the remaining samples are learned in a progressively increasing order of difficulty through a sliding window approach. The approach achieves improvements on general instruction evaluation with GPT-2 (0.1B ) and on math/code tasks with Qwen2.5-1.5B."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The motivation and problem are practical. The plug-and-play module is very useful."}, "weaknesses": {"value": "● Lack of originality: Using loss to measure instance difficulty and then applying curriculum learning is not a novel idea, as similar approaches have been widely explored in many curriculum learning-related papers (e.g., Self-paced Learning).\n● Model and evaluation benchmarks: The model used (GPT-2) is relatively outdated, and for the general instruction-following evaluation, the paper does not adopt currently standard benchmarks such as IFEval, which are commonly used in the community.\n● Writing: Some variables are unexplained, such as w_j in line 272. The variable definitions in the sliding window algorithm are unclear and can be confusing."}, "questions": {"value": "The data introduced at each stage is predetermined based on the loss of the untrained student model. Is it reasonable to use the pre-defined loss to reflect the difficulty  for the current training model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "djJ1u6Hfkj", "forum": "QAwGkFD8ES", "replyto": "QAwGkFD8ES", "signatures": ["ICLR.cc/2026/Conference/Submission9274/Reviewer_UAZg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9274/Reviewer_UAZg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882696073, "cdate": 1761882696073, "tmdate": 1762920920322, "mdate": 1762920920322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}