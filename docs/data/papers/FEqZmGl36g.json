{"id": "FEqZmGl36g", "number": 18339, "cdate": 1758286610668, "mdate": 1759897109809, "content": {"title": "ESS-Flow: Training-free guidance of flow-based models as inference in source space", "abstract": "Guiding pretrained flow-based generative models for conditional generation or to produce samples with desired target properties enables solving diverse tasks without retraining on paired data. We present ESS-Flow, a gradient-free method that leverages the typically Gaussian prior of the source distribution in flow-based models to perform Bayesian inference directly in the source space using Elliptical Slice Sampling. ESS-Flow only requires forward passes through the generative model and observation process, no gradient or Jacobian computations, and is applicable even when gradients are unreliable or unavailable, such as with simulation-based observations or quantization in the generation or observation process. We demonstrate its effectiveness on designing materials with desired target properties and predicting protein structures from sparse inter-residue distance measurements.", "tldr": "", "keywords": ["generative models", "guidance", "inference", "inverse problems"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/acc1bfead388bb54b00ce3b08f426ee449316d94.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose ESS-Flow, a method to guide sampling from probability flow generative models without gradient-based guidance. Their method samples with MCMC along ellipsoids in the source space (Gaussian prior), and takes a step based on the potential /target property evaluated in the data space. They motivate their method with the limitations of gradient-based guidance and evaluate their guidance on protein and material design tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Guidance without gradients is a nice benefit of the proposed method\n- The method is motivated well theoretically and with prior work\n- The experiments suggest that ESS-Flow is able to effectively guide samples compared to gradient-based approaches"}, "weaknesses": {"value": "- Does guidance for certain properties improve the estimation of other properties/observables not used in guidance?\n- Given that one of the benefits of the proposed method is not relying on gradients, it would strengthen the paper to show guidance for discrete target properties.\n- The discussion of challenges about challenges with gradient-based guidance (Fig. 2) is interesting, and I think further discussion / experiments along these lines would further strengthen the paper. I wonder if there is a way to show something similar with more realistic data by artificially creating 2 disconnected modes (i.e. removing data in a transition region or something like that).\n- \"Limits ESS-Flow’s effectiveness when the prior does not well inform the target distribution\": while I understand that things like image inpainting might be challenging for the method, have the authors evaluated ESS-Flow on other image guidance tasks?\n- There are newer and more accurate models compared to something like CHGNet that might give more accurate metrics (MACE, eSEN, UMA, etc.)"}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jGFZs5O8ad", "forum": "FEqZmGl36g", "replyto": "FEqZmGl36g", "signatures": ["ICLR.cc/2026/Conference/Submission18339/Reviewer_C4du"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18339/Reviewer_C4du"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545176499, "cdate": 1761545176499, "tmdate": 1762928051205, "mdate": 1762928051205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a plug-and-play controllable generation method for flow-based generative models, which is based on sampling the controlled prior distribution through elliptical sliced sampling (ESS), and then run the ODE to convert the prior samples to samples from the tilted target distribution. The method avoids the need to compute the Jacobian of the flow map $T _ \\theta$, and does not require the tilt to be differentiable. The authors further proposed to use a coarse discretization of the flow ODE as a proposal for the transition kernel and reweight the samples with the evaluation of the tilt on the final samples to reduce the computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Unlike most existing controllable generation methods such as CFG and DPS that involve the computation of an extra term in the SDE (often requires approximation), the proposed method just treat the flow ODE as a black-box and only modifies the prior distribution, and instead of doing gradient-based optimization, it directly samples from the controlled prior distribution. This makes the method simple and concise, and I personally appreciate this idea.\n\nThe paper also provided comparisons of the proposed method with existing controllable generation methods based on optimization. I'm not familiar in the domain of material design and protein structure prediction, but the experimental results look nice."}, "weaknesses": {"value": "The authors provided limited background information about the ESS algorithm, which I believe most of the readers in the ML community should not be familiar with. We know that there are lots of zeroth-order sampling methods for sampling from $\\pi(z)\\propto p(z) g(T _ \\theta(z))$, such as rejection sampling, the MH algorithm, and proximal sampler (https://proceedings.mlr.press/v134/lee21a.html). What's the insight behind ESS, and why is the Gaussian prior important for it to work? (Also I'm not quite in favor of the use of the abbreviation ESS in this paper, since it is also used to refer to \"effective sample size\" in the literature of Monte Carlo methods, which may cause some misunderstanding. But it's fine if you keep it.)\n\nFor the experiments, I think it would be more convincing if the authors can also consider some image generation tasks, which are more familiar to most ML researchers, and can better demonstrate the effectiveness of the proposed method. Also, as the authors have introduced the multi-fidelity version of the proposed method through a coarse discretization of the flow ODE, it would be interesting to see some ablation studies on the choice of the discretization step size and its impact on the generation quality and computational cost.\n\nI'm happy to raise the score if these issues can be addressed during rebuttal."}, "questions": {"value": "1. What's the typical number of rejections needed in one iteration of ESS in order to get an accepted sample?\n\n2. We know that diffusion model and flow matching allow one-step prediction of $\\mathbb{E}[x _ 1|x _ t]$. For faster sampling, instead of still doing discretization of the flow ODE, do you think it is possible to replace the flow map $T _ \\theta$ with a one or few step predictor in the proposed method? If a consistency model is available, we can even directly predict the final sample from the prior sample in one step, which may further reduce the computational cost.\n\n3. The main text of the paper assumes the flow model is trained for Euclidean data, but in the experiments it turns out that the whole framework can also be applied to manifold data as long as we have a flow-based generative model, which I believe is an advantage of the proposed method, and should be highlighted more in the paper. I'm not quite familiar with the literature of manifold, but is there anything that we need to pay attention to when applying the proposed method to manifold data? For example, do we need to modify the ESS algorithm in any way?\n\n4. In table 2 and 3, it would be better to highlight the best results in bold font for better readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uwPpAc78BT", "forum": "FEqZmGl36g", "replyto": "FEqZmGl36g", "signatures": ["ICLR.cc/2026/Conference/Submission18339/Reviewer_sKKR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18339/Reviewer_sKKR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774887652, "cdate": 1761774887652, "tmdate": 1762928050427, "mdate": 1762928050427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ESS-Flow, a training-free and gradient-free guidance method for flow-based generative models. The key idea is to do Monte Carlo inference directly in the source space, where the prior is Gaussian, by sampling from a distribution of the form $\\pi(z) \\propto g(T_\\theta(z))p(z)$. This makes the method applicable to quantized/materials settings and to non-differentiable observation or reward functions. The authors further propose a multi-fidelity variant where the authors sample using a coarse ODE discretization and reweight using a fine discretization to reduce computation. Experiments on materials and protein structure prediction show that ESS-Flow achieves nearly SOTA results on all metrics for materials and comparable performance on protein generation metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written, with a coherent structure and well-motivated.\n\n2. The proposed method is simple to implement and achieves strong results on the material generation task."}, "weaknesses": {"value": "1. The paper lacks experiments on standard image-domain tasks (e.g. inpainting, deblurring), which are commonly used in related work such as D-Flow and PnP-Flow.\n\n\n2. While the empirical contribution is solid, the theoretical contribution is relatively modest.\n\n3. One of the main points of the contribution is having this apply to non-differentiable rewards/potentials yet there are no experiments demonstrating this capability."}, "questions": {"value": "1. What are the quality metrics in Table 3 for the competing methods? It would be helpful to report the same set of metrics so we can compare ESS-Flow directly to the baselines.\n\n2. How does multi-fidelity ESS compare to standard ESS on the metrics reported in Table 2 and Table 4.\n\n3. What are the acceptance rates of the sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KtK6Chmn2A", "forum": "FEqZmGl36g", "replyto": "FEqZmGl36g", "signatures": ["ICLR.cc/2026/Conference/Submission18339/Reviewer_iqsg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18339/Reviewer_iqsg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874093433, "cdate": 1761874093433, "tmdate": 1762928049658, "mdate": 1762928049658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce ESS-Flow, a gradient-free method for controlled generation in the setting of generative modelling with flow matching models. The authors perfoming Bayesian inference in source space using Elliptical Slice Sampling, which enables conditional generating without requiring gradients. The authors demonstrate their approach on various applications ranging from materials to proteins."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[S1] The gradient-free nature of this approach is appealing. There have been other works for source space sampling such as , but this still required gradients, wheras this approach here circumvents this via the Jacobian cancellation and the ESS approach.\n\n[S2] One common with gradient-based optimisers in diffusion samplers are the multitude of often brittle hyperparameters like guidance scales or other schedulers; this approach here seems to be less reliant on these.\n\n[S3] Good motivation of the different components introduced with formal justifications."}, "weaknesses": {"value": "[W1] The authors demonstratet that they can avoid the Jacobian computation, but ESS-Flow still requires many evals (>1000 MCMC steps) of the transport map, hurting the efficiency of the approach\n\n[W2] The authors openly describe the limitation of ESS-Flow in cases where the target is constrained on a lower-dim manifold, but claim that in scientific domains the target distribution is not overly collapsed. However, in many applications like in protein design the target distribution lies exactly on such a lower dim manifold with most of the target space being invalid sampels; some more explanation why the authors think that this is not the case in many scientific applciations would help here.\n\n[W3] In many scientific applications, people have circumvented the non-differentiability of categorical sequences via soft relaxations similar to the atom relaxation the authors use for their comparisons. In approaches like BindCraft (Pacesa et al, 2025 Nature), this works remarkably well, so the authors should potentially try to tune that baseline to see if it as strong as it can be.\n\n[W4] While some of the baselines in the protein structure prediction case of Figure 4 look unrealistic, ESS-Flow also seems to have biophysical implausibilities, and the ELBO of the model only partially captures these things. A more fundamental evaluation like counts of clashes could demonstrate how good the structures actually are; the RMSD values above 10 suggest that all baselines seem pretty far off."}, "questions": {"value": "[Q1] The case studies all have quite low dimensionality, how does the approach scale to high dimensional problems?\n\n[Q2] Given the authors say their method does not work well with priors that poorly inform the target distribution, can this statement be made more exact? ie is there a quantity that one can look at to see if the approach will work or not?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XoLpqUnljG", "forum": "FEqZmGl36g", "replyto": "FEqZmGl36g", "signatures": ["ICLR.cc/2026/Conference/Submission18339/Reviewer_24zR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18339/Reviewer_24zR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149290861, "cdate": 1762149290861, "tmdate": 1762928049173, "mdate": 1762928049173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ESS-Flow, a training-free and gradient-free approach for controlled generation using pretrained diffusion or flow-based models. The method reframes inference in the Gaussian latent source space and applies elliptical slice sampling (ESS) instead of gradient-based updates, using a change of variables to apply updates in the data space. The main goal is to enable preference alignment when gradients are unavailable or unreliable, such as in cases involving quantized or simulator-based objectives."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a new and practically useful application of elliptical slice sampling to flow- and diffusion-based generative models. While ESS itself is a known MCMC method, its use within the latent Gaussian space of pretrained flow-based models is novel and interesting.\n- The algorithm is designed such that jacobian determinants of transport maps don’t need to be computed, enabling a scalable and efficient algorithm.\n\n\n- The theory is sound, and the algorithm preserves theoretical guarantees from the original ESS method, making interpretability for downstream researchers easier.\nReported results compared to baselines show a non-negligible improvement in matching a new target energy function.\n\n\n- Overall, the paper connects ideas from generative modeling, Bayesian inference, and MCMC in a coherent and insightful way. It is an interesting paper that I believe would bring a net positive to the research community, which would be strengthened by addressing the weaknesses below."}, "weaknesses": {"value": "- While 0th-order methods can benefit greatly in settings with unreliable gradients, they can also face severe scaling issues in high-dimensional spaces. Having an experiment showing performance as problem dimension scales would help further inform future readers when they should use ESS-flow vs. a gradient-based method.\n\n\n- While results on the provided experiments show that the 0th order methods are outperforming the gradient-based methods, none of the experimental settings to my understanding actually fall under the non-differentiable setting that the paper proposes to address. It would help to either (a) include a relevant experiment setting where gradient information is truly intractable to retrieve, or (b) demonstrate that in the reported settings, the gradient structure is highly unideal for gradient-based methods, e.g. high Lipschitz constant (also, for line 290, maybe better to call them gradient-based rather than optimization-based, since there are many 0th order optimization algorithms).\nMore statistical details on the experimental setup are needed, e.g. some std’s in table 2 are higher in magnitude than the mean.\n\n\n- There are some missing baselines/ablations (e.g. adjoint matching [Domingo-Enrich et al.], non-ESS-based source space MCMC) that would help clarify whether the main benefit comes from being gradient-free or from the specific ESS mechanism."}, "questions": {"value": "- How does ESS-Flow scale with increasing latent dimension? Does the acceptance rate or effective sample size drop significantly in higher dimensions?\n\n\n- What is the runtime cost compared to the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x3DYalzxhM", "forum": "FEqZmGl36g", "replyto": "FEqZmGl36g", "signatures": ["ICLR.cc/2026/Conference/Submission18339/Reviewer_DpS7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18339/Reviewer_DpS7"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762202875826, "cdate": 1762202875826, "tmdate": 1762928048693, "mdate": 1762928048693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}