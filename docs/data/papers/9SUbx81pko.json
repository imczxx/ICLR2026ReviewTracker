{"id": "9SUbx81pko", "number": 19248, "cdate": 1758294802689, "mdate": 1759897049710, "content": {"title": "Learning to Drive with Two Minds: A Competitive Dual-Policy Approach in Latent World Models", "abstract": "End-to-end autonomous driving models trained solely with imitation learning (IL) often suffer from poor generalization. In contrast, reinforcement learning (RL) promotes exploration through reward maximization but faces challenges such as sample inefficiency and unstable convergence. A natural solution is to combine IL and RL. Moving beyond the conventional two-stage paradigm (IL pretraining followed by RL fine-tuning), we propose CoDrive, a competitive dual-policy framework that enables IL and RL agents to interact during training. CoDrive introduces a competition-based mechanism that facilitates knowledge exchange while preventing gradient conflicts. Experiments on the nuScenes dataset show an 18\\% reduction in collision rate compared to baselines, along with stronger generalization and improved performance on long-tail scenarios. Code is available at an anonymous repository.", "tldr": "We propose a dual-policy framework that uses a latent world model to combine imitation and reinforcement learning for autonomous driving, improving generalization and performance on challenging scenarios without external simulators.", "keywords": ["autonomous driving", "world model", "imitation learning", "reinforcement learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04afe579e624691866faa19a9faf754ce2f455b3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper \"Learning to Drive with Two Minds: A Competitive Dual-Policy Approach in Latent World Models\" proposes a dual-policy learning framework that trains an Imitation Learning (IL) actor and a Reinforcement Learning (RL) actor in parallel. The two actors periodically merge their model parameters based on predefined merging strategies and performance comparison."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of maintaining both IL and RL actors throughout training, rather than discarding IL after warm-up, is interesting and could inspire further exploration.\n2. The \"backward planning\" concept of providing more contextual information to early actions is intuitively appealing.\n3. The paper is clearly motivated and connects well with the general problem of combining imitation and reinforcement learning."}, "weaknesses": {"value": "1. The claimed contribution, \"We integrate RL into an end-to-end driving framework by leveraging a latent world model for imagination-based simulation, avoiding reliance on external simulators,\" is not new. Previous work has explored similar ideas (see [1] for example).\n2. The notations in the paper are sometimes confusing. For instance, what is the dimension of the waypoint features s_w? The symbols \ns and s_t are not used consistently. tau_a is an action sequence but is sometimes referred to as a single action (Equation (4)). L_{wm} appears before being defined. The notations for action sampling in Figure 2 are incorrect. The format of the L_{bc} is also incorrect.\n3. While the dual-actor training pipeline introduces additional training overhead, the corresponding performance improvement appears minor. \n\nReference: \n[1] Scheel, Oliver, et al. \"Urban driver: Learning to drive from real-world demonstrations using policy gradients.\" Conference on Robot Learning. PMLR, 2022."}, "questions": {"value": "1. The backward planning approach actually breaks the MDP assumption in RL. How can use this backward planning in RL? Also, what is the horizon length for the future states?\n2. The latent world model is used during the RL actor learning phase, but when is this world model trained?\n3. Is there any particular reason for using the L1 loss for imitation learning in Equation (3)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tdBEUc5r2M", "forum": "9SUbx81pko", "replyto": "9SUbx81pko", "signatures": ["ICLR.cc/2026/Conference/Submission19248/Reviewer_T6eo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19248/Reviewer_T6eo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761087052318, "cdate": 1761087052318, "tmdate": 1762931223704, "mdate": 1762931223704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of end-to-end autonomous driving models, which suffer from poor generalization when trained with Imitation Learning (IL) and instability when trained with Reinforcement Learning (RL). The authors propose CoDrive, a novel dual-policy framework that synergistically combines IL and RL within a learned latent world model. Instead of relying on external simulators, CoDrive enables imagination-based training where an IL actor and an RL actor are trained in parallel. The core idea is a competitive learning mechanism that facilitates structured knowledge exchange: the IL actor provides expert knowledge, while the RL actor explores novel states and actions. This approach aims to leverage the stability of IL and the exploratory power of RL without their objectives directly conflicting, leading to improved generalization, reduction in collisions on the nuScenes dataset, and better performance in long-tail scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel and Well-Motivated Architecture for Integrating IL and RL. The proposed competitive learning mechanism provides a structured way to facilitate knowledge transfer.\n\n2. This approach smartly bypasses the need for high-fidelity, hand-crafted external simulators, thereby mitigating the notorious sim-to-real gap and the dependency on expert demonstrations within the simulator.\n\n3. By enabling imagination-based training, the framework allows the RL agent to perform sample-efficient and safe exploration of countless possible future scenarios."}, "weaknesses": {"value": "1. In Table 1, the improvement gain on L2 metric is marginal. e.g. SSR+CoDrive is worse than SSR; LAW+CoDrive (PGGS) only achieves 0.01 gain. This raises questions about the practical significance of the proposed method, especially when weighed against its added architectural complexity.\n\n2. In Table 2, the improvement on Navsim test set is also very minor. Taken together, this raises a critical question about the overall effectiveness of the proposed CoDrive framework,\n\n3. How is the efficiency and resource-usage comparison? (Latency / training time / computation overheads)\n\n4. In line 125: \"achieve more stable training.\". Any number comparison for proving the more stable training than single RL?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ngENHQmQXr", "forum": "9SUbx81pko", "replyto": "9SUbx81pko", "signatures": ["ICLR.cc/2026/Conference/Submission19248/Reviewer_xpD5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19248/Reviewer_xpD5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904409604, "cdate": 1761904409604, "tmdate": 1762931223168, "mdate": 1762931223168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a policy framework that integrates an Imitation Learning and Reinforcement Learning approach to jointly learn a driving policy. For trajectory generation for autonomous driving, both learners predict actions but depending on the resulting score, only the best solution is implemented and weights of the worse learner are merged or replaced by the weights of the better learner. The approach is compared on the nuScenes and Navsim dataset against other baselines and improvements to existing baselines are shown."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Creation of a competition-based policy learning approach instead of combined losses is an interesting approach that can potentially avoid limiting the maximum performance due to conflicting action proposals.\n\nThe designed interaction between the IL and RL learner seems to reduce the collision rate significantly."}, "weaknesses": {"value": "While the inverse causality is an interesting idea, the ablation study also seems to show that it has very little or no effect, compared to using no mask.\n\nThe results compared to SOTA are not very impressive. The approach seems to have significantly less collisions than plain SSR but has worse L2 scores. Compared with plain LAW it does not seem to improve performance beyond what could be some noise in the evaluation. Tasks like detection and tracking, which seem to help other approaches, are not necessarily hard to implement, given today's datasets. Therefore, the advantage of using this approach compared to other methods is not clear. This could be saved by potentially also adding a detection and tracking task and then having significantly better results across all metrics.\n\nResults on Navsim show improved results compared to SOTA but it seems performance on this test set is very close to human performance for all methods and there is no large significant advantage over existing methods. \n\nThe paper makes an interesting observation that RL achieves higher long-term results after initially the IL learner leads but than flattens out. However, this is compared to the two-stage paradigm which is described as inferior. This would be easy and important to show. It would have been interesting to see as comparison in the same setting to have first an IL and then an RL learner. It is not clear what, apart from potentially some time, is gained by training jointly."}, "questions": {"value": "Because of the very specific meaning of the Bitcoin symbol and because the RL reward has nothing to do with it the symbol should be replaced in Figure 1.\n\nI in formula 11 could be non-obvious. Please define it somewhere.\n\nFor equation 17 it is claimed that RL with the sparse reward is hard to stabilize and therefore a small behavioral cloning loss is added. This somehow goes against the idea of using the competitive system. Also, RL should be able to work with sparse rewards. This part could use more explanations: Why is it hard to stabilize training? Why does the L_bc help? How is \\beta found and what effect does it have as beta goes from small to large?\n\nThere are minor syntax and grammar issues, e.g. in line 216: \"Given the offline imitation dataset ..., using Gaussian Log Likelihood loss can easily fitting the behavior of experts\". For a camera ready version I suggest to try to find a way to improve this as good as possible, potentially with outside help if the authors are not native English speakers as it is the case for the majority of our domain."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PYqbOjHeQL", "forum": "9SUbx81pko", "replyto": "9SUbx81pko", "signatures": ["ICLR.cc/2026/Conference/Submission19248/Reviewer_sWaj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19248/Reviewer_sWaj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905463038, "cdate": 1761905463038, "tmdate": 1762931222732, "mdate": 1762931222732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response Summary of Round 1 Discussion"}, "comment": {"value": "We sincerely thank all reviewers for their time and constructive feedback.\n\nBelow, we summarize the main concerns raised and our corresponding responses:\n\n**S1: Our method does not seem to improve performance beyond what could be some noise in the evaluation.**\nAll reviews raise this question (sWaj, xpD5, and T6eo), which shows this the most important thing we need to discuss. \nIn table 1, we showed that our model gets both better L2 (from 0.66 to 0.63) and Collision Rate (from 0.22% to 0.18%, drop 18%) compared with basline LAW. To further verify that these improvements are not due to noise, we provided additional evaluations (in our paper Figure 4 and Table 6&7&8):\n- **Cross-city generalization**: L2 ↓33% and collision rate ↓72%.\n- **Long-tail scenarios**: L2 ↓10% (long-tail L2 scenes) and collision rate ↓33% (long-tail collision scenes).\nThese results demonstrate that our method not only improves safety while maintaining human-likeness but also enhances generalization and robustness in challenging cases.\n\nWe also appreciate reviewer sWaj’s suggestion to employ stronger perception backbones for a fairer SOTA comparison. **We are conducting these experiments and will share results once available**.\n\n**S2: Effect and validity of backward planning (sWaj, T6eo)**.\nWe clarified to sWaj that backward planning does not affect the overall L2 error but reduces early collisions (in Table 3) —by **50% at the 1st second** and **30% at the 2nd second**—compared to the no-mask variant (which increased collisions by 20%).\nFor T6eo, we explained that our design does not violate the Markov assumption, as we treat the full action sequence $\\tau_a$ as a single action within the current state.\n\n**S3: Use of a behavioral cloning term within RL loss (sWaj)**.\nWe clarified that this small BC loss does not contradict the competitive design. Since competition occurs every k iterations, the RL actor receives no expert guidance during each interval. The BC term, with a small weight, helps stabilize learning and encourages more effective exploration. We also presented results for different $\\beta$ values.\n\n**S4: Efficiency and resource-usage comparison. (xpD5)**.\nAs reported in Table 5, the inference latency of our model (37.0 ms, 27.1 FPS) is almost identical to the baseline (37.1 ms, 26.99 FPS), since only the IL actor is used during inference. Hence, our framework introduces no additional runtime cost.\n\n**S5: Implementation details (T6eo)**.\nWe clarified the horizon length (1.5 s, following LAW), training procedure of the world model (jointly optimized during imitation learning), and the use of L1 loss (consistent with LAW and SSR).\n\n**S6: Contribution Statement, Figures and Notations need to be improved (sWaj, T6eo)**.\nWe thank the reviewers for pointing these out. We will improve:\n\n- The first contribution statement\n- The icon in Figure 1 (“bitcoin reward”)\n- Action sampling notations in Figure 2\n- Definitions of $L_{wm}$ and $L_{bc}$\n- Dimension annotations in Equation (1)\n- Consistent use of $s$ instead of $s_t$\n- Minor grammatical and stylistic issues\n\nWe will update and upload the revised version soon."}}, "id": "EmDWSDrVRT", "forum": "9SUbx81pko", "replyto": "9SUbx81pko", "signatures": ["ICLR.cc/2026/Conference/Submission19248/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19248/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19248/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763020518319, "cdate": 1763020518319, "tmdate": 1763020518319, "mdate": 1763020518319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}