{"id": "WYGFk5DA6c", "number": 10893, "cdate": 1758184270523, "mdate": 1759897622341, "content": {"title": "MIST: Multiple Stochastic Prompt Tuning for Few-shot Adaptation under Extreme Domain Shift", "abstract": "Foundation Vision-Language Models (VLMs) like CLIP generalize well due to\nlarge-scale pretraining, but their performance degrades under significant distribution\nshifts in appearance and label semantics. Few-shot adaptation via adapter or\nprompt tuning addresses limited-data tasks, but are not specifically designed to\nhandle such extreme domain shifts. Some cross-domain few-shot methods consider\nsuch domain-shifts but often use episodic settings with fixed classes, limiting\nreal-world applicability. To address this gap, we propose a novel framework MIST\n(Multiple Stochastic Prompt Tuning), which adapts CLIP to extreme domain shifts\nwith few labeled examples across all classes simultaneously. MIST uses multiple\nlearnable prompts per class to capture diverse modes in visual features, modeled\nas Gaussian distributions to improve generalization and reduce overfitting. Extensive\nexperiments show the effectiveness of the proposed framework.", "tldr": "", "keywords": ["Few-shot adaptation", "representation learning", "Vision-language Models"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/902e51f9f3659d4be4d48fc84c9a9d99a75b1ff6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a prompt tuning method for cross-domain few-shot adaptation using CLIP. Existing approaches face two key limitations: (1) they often overlook domain shifts that involve both visual appearance and label semantics, and (2) their evaluations typically exclude part of the target classes. To address these issues, the authors introduce two key techniques. First, they model prompt parameters stochastically using Gaussian distributions rather than deterministic vectors, improving robustness to domain and semantic variation. Second, they employ multiple prompts per class to better capture the diverse semantic representations within each category. The proposed method is extensively evaluated across multiple standard benchmarks"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written, allowing readers to easily follow the logical flow and understand the motivation as well as the proposed method at both the conceptual and technical levels. For example, in **Section 4.2**, the authors clearly introduce the limitation that motivates their approach, then explain the conceptual idea (“why” and “what”) before presenting the detailed technical design (“how”). In addition, **Figure 4** effectively visualizes the overall pipeline of the proposed framework.\n\n- The experimental evaluation is comprehensive, including comparisons with state-of-the-art methods, detailed ablation studies, and hyperparameter analyses such as class imbalance, number of prompts, and prompt length."}, "weaknesses": {"value": "- The problem setting in the Introduction is not clearly defined. Adapting CLIP to downstream target datasets can correspond to several existing problem formulations (e.g., few-shot adaptation, domain generalization, or open-world transfer). For instance, models such as Few-Shot Test-Time Domain Adaptation (FSTT-DA)[1] address few-shot adaptation with unlabeled target data under challenging domain and semantic shifts [2]. In this paper, the stated task is cross-domain few-shot learning, where both the domains and classes differ, but this distinction is not explicitly articulated. The authors are encouraged to include one or two sentences clarifying the setting in terms of domain, class, and label availability, and to highlight how it differs from related formulations. In addition, in Line 41, the definition of cross-domain few-shot learning may confuse readers unfamiliar with the meta-learning paradigm. It would help to explain K-way-N-shot evaluation in simple terms.\n\n- The conceptual idea of using multiple learnable prompt embeddings to represent knowledge for a specific domain or class is not entirely novel. For example, prior works [3] have introduced prompt pools containing multiple learnable prompts whose weighted combination captures domain-specific knowledge. The same concept can be directly extended to class-specific representations. The authors should clarify these conceptual overlaps in the Related Work section and explicitly emphasize how their method differs or improves upon these earlier approaches.\n\n- The proposed method is evaluated only with CLIP ViT-B/16, leaving uncertainty about whether the improvements generalize to other backbones. While computational constraints are understandable, testing on at least one additional architecture—such as ViT-L/14—on a representative benchmark would strengthen the claim of generality.\n\n[1] Meta-dmoe: Adapting to domain shift by meta-distillation from mixture-of-experts. NeurIPS 2022\n\n[2] WILDS: A Benchmark of in-the-Wild Distribution Shifts. ICML 2021\n\n[3] Adapting to Distribution Shift by Visual Domain Prompt Generation. ICLR 2024"}, "questions": {"value": "- Could the authors explicitly clarify the problem setting of this work? How are domain, class, and label availability defined in this setting?\n\n- How does this problem formulation differ from existing setups such as FSTT-DA or other few-shot adaptation methods?\n\n- In Line 41, the definition of cross-domain few-shot learning may be unclear to readers without a meta-learning background. Could the authors briefly explain K-way-N-shot evaluation in simpler terms for clarity?\n\n- Did the authors compare against or draw inspiration from existing methods that use multiple prompts for domain or class-specific adaptation? If so, please clarify these differences in Related Work or Ablation Studies.\n\n- The experiments are conducted only on CLIP ViT-B/16. Can the authors comment on whether similar performance trends are expected on larger or different architectures such as ViT-L/14?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wbzuwNR0Ve", "forum": "WYGFk5DA6c", "replyto": "WYGFk5DA6c", "signatures": ["ICLR.cc/2026/Conference/Submission10893/Reviewer_QjNq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10893/Reviewer_QjNq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788983611, "cdate": 1761788983611, "tmdate": 1762922101853, "mdate": 1762922101853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MIST (Multiple Stochastic Prompt Tuning) to address few-shot adaptation of Vision-Language Models (VLMs) like CLIP under domain shifts and label semantic misalignment. MIST introduces multiple learnable prompt vectors per class modeled as Gaussian distributions to capture multimodal visual features, enhancing generalization and reducing overfitting in data-scarce scenarios. Experiments demonstrate significant improvements over SOTA methods, particularly in ultra-low-data regimes (1-shot)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. MIST diagnoses fragmented feature representations under extreme domain shifts and proposes multiple Gaussian-distributed prompts per class to form diverse decision boundaries—a principled approach beyond standard prompt tuning.\n2.  Significant improvements over methods across multiple benchmarks, especially in 1-shot scenarios, with demonstrated robustness to class imbalance and good generalization."}, "weaknesses": {"value": "1. Weak Motivation and Limited Novelty: The method lacks clear motivation for why Gaussian-distributed multiple prompts address extreme domain shifts. The approach resembles existing GMM-based strategies without sufficient differentiation. Why Gaussian over other probabilistic models?\n\n2. Incomplete Experiments: Baselines are outdated; missing recent few-shot adaptation methods. Limited to four benchmarks (EuroSAT, ISIC, Plant Disease, ChestX). No evaluation on standard OOD benchmarks (ImageNet-R, ImageNet-A, CIFAR-10-C, DomainNet, VisDA, etc.). Missing cross-dataset generalization experiments."}, "questions": {"value": "Have you evaluated on other OOD benchmarks? How does MIST compare to recent domain generalization and OOD adaptation methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OdyYc9UDjH", "forum": "WYGFk5DA6c", "replyto": "WYGFk5DA6c", "signatures": ["ICLR.cc/2026/Conference/Submission10893/Reviewer_p6n6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10893/Reviewer_p6n6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963316225, "cdate": 1761963316225, "tmdate": 1762922101366, "mdate": 1762922101366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MIST (Multiple Stochastic Prompt Tuning), a method for adapting CLIP-like vision-language models to few-shot tasks under extreme domain and label semantic shifts. MIST introduces (i) stochastic prompt learning (each prompt represented as a Gaussian distribution) to improve generalization and (ii) multiple prompts per class to model multimodal feature distributions. The method is evaluated on the BSCDFSL benchmark (EuroSAT, ISIC, PlantDisease, ChestX) and shows improved performance on most datasets over SOTA methods published up to 2024."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The integration of stochastic prompt learning, deep prompting, and multiple prompts per class represents a novel contribution. As shown in Table 1, the proposed approach outperforms several state-of-the-art baselines."}, "weaknesses": {"value": "- Although this integration has not been explicitly explored in prior work, all three underlying ideas have already been proposed in the existing literature. Therefore, the proposed approach appears somewhat straightforward.\n- While the “k-shot all-class” setup (Lines 94-96) is interesting, the paper does not clearly explain why it is considered “closer to real-world deployment” than the widely used N-way k-shot (episodic) setting. Moreover, there seems to be no reason not to include comparisons between MIST and existing methods under the episodic setting as well.\n- More recent methods should be included in the comparison. For example, although PromptMargin [1] (published in 2025) is cited in §2, its results are not reported in Table 1.\n- Several ablation studies are missing. For instance, the effect of deep prompting and its configuration has not been evaluated. In addition, while the mean of the first prompt (i.e., ``A photo of a [CLS]'') is fixed throughout the experiments, it should also be tested as a learnable parameter.\n- Bayesian prompt learning for vision–language models has already been explored in the literature (e.g., [2, 3]). Although the experimental settings differ, these works should be discussed to better contextualize and highlight the novelty of the proposed method.\n- The listed contributions (Lines 56-65) are not orthogonal; at least points 1, 3, and 4 seem to overlap.\n- While my understanding might be incomplete, it is unclear why the summation in Equation (5) (Lines 291-294) is taken over 1 to 2C. j takes only the values 1 or 2, doesn’t it?\n\n[1] D. Brahma et al., Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts, in TMLR, 2025.\n\n[2] M.M. Derakhshani et al., Bayesian Prompt Learning for Image-Language Model Generalization, in ICCV, 2023.\n\n[3] X. Liu et al., Patch-Prompt Aligned Bayesian Prompt Tuning for Vision-Language Models, in UAI, 2024."}, "questions": {"value": "Please refer to the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O2tnfUPdoO", "forum": "WYGFk5DA6c", "replyto": "WYGFk5DA6c", "signatures": ["ICLR.cc/2026/Conference/Submission10893/Reviewer_Pyp4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10893/Reviewer_Pyp4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968546590, "cdate": 1761968546590, "tmdate": 1762922100926, "mdate": 1762922100926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}