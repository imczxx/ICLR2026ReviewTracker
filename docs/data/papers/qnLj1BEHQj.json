{"id": "qnLj1BEHQj", "number": 19458, "cdate": 1758296374865, "mdate": 1763656640886, "content": {"title": "Concept Attractors in LLMs and their Applications", "abstract": "Large language models (LLMs) often map semantically related prompts to similar internal representations at specific layers, even when their surface forms differ widely. We show that this behavior can be generalized and explained through Iterated Function Systems (IFS), where layers act as contractive mappings toward concept-specific Attractors. We leverage this insight and develop simple, training-free methods that operate directly on these attractors to solve a wide range of practical tasks, including **language translation, hallucination reduction, guardrailing**, and **synthetic data generation**. Despite their simplicity, these attractor-based interventions match or exceed specialized baselines, offering an efficient alternative to heavy fine-tuning, generalizable in scenarios where baselines underperform.", "tldr": "", "keywords": ["LLM", "attractors", "IFS", "forgetting", "synthetic data generation", "hallucinations", "translations"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30df78bc24ff6fdaefdd8fc6938cb20e2163ea2a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a novel conceptual framework for understanding the internal representations of Large Language Models (LLMs). The core hypothesis is that the observed convergence of semantically related, yet lexically diverse, prompts to similar representations at specific intermediate layers can be effectively modeled as an Iterated Function System (IFS). The authors posit that sequences of LLM layers act as contractive mappings, guiding representations toward concept-specific \"Attractors.\""}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The \"Concept Attractor\" analogy to IFS is a powerful and elegant conceptual tool. It provides a unified and intuitive explanation for several disparate, observed phenomena in LLM representation spaces, such as representational collapse and the existence of task vectors. \n\nMoreover, the paper convincingly demonstrates that this single, simple idea can be successfully applied to a diverse and important set of problems (unlearning, safety, code generation, VLM grounding, data augmentation). This suggests the \"attractor\" is a general and fundamental property of these models."}, "weaknesses": {"value": "I was one of the reviewers for this paper's previous submission (to NeurIPS). \n\nIn my previous review, I raised two main concerns:\n\n1. The paper should explicitly describe the procedure used to identify attractors.\n\n2. The paper does not provide compelling evidence that two attractors cannot converge to similar or nearby spaces within the same layer. This is because tasks like unlearning require not only editing the target mechanism but also ensuring that other mechanisms remain unchanged to combat fine-tuning issues like catastrophic forgetting.\n\nHowever, I did not find the authors to have sufficiently addressed these two issues. In particular, regarding justification for downstream tasks, in addition to the aforementioned remaining issues, there is also a lack of comparison with recent SOTA work."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JvK7muf1H3", "forum": "qnLj1BEHQj", "replyto": "qnLj1BEHQj", "signatures": ["ICLR.cc/2026/Conference/Submission19458/Reviewer_nRDX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19458/Reviewer_nRDX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761460920679, "cdate": 1761460920679, "tmdate": 1762931373574, "mdate": 1762931373574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel perspective on the phenomenon of representation convergence in LLM, hypothesizing that the internal dynamics of LLMs can be understood through the framework of Iterated Function Systems. Specifically, it suggests that layers in an LLM act as contractive mappings, driving semantically related prompts (even with diverse lexical forms) to cluster around concept-specific Attractors in LLM's latent space. These attractors are invariant sets that characterize specific concepts (e.g., \"Harry Potter\" or \"Python programming\"). \n\nThe core contribution is leveraging this \"Attractor\" concept to develop simple, training-free, and data-efficient interventions that operate directly on these Attractor vectors to solve a diverse range of practical tasks including hallucination reduction, guardrailing, and synthetic\ndata generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The hypothesis that LLM layer transformations act as contractive mappings converging to concept-specific Attractors that can be modeled by Iterated Function Systems is novel and an interesting perspective. \n2. Beyond the observation, I like the practical applications explored in this paper. Though the intervention itself is simple in its form, addition / subtraction, but its effectiveness and generalizable observations in many downstream tasks is a major strength. \n3. The paper is well-structured and clearly written.\n4. Wide experiments with model variety, LLM and VLM, various datasets are used for evaluation."}, "weaknesses": {"value": "1. While the IFS hypothesis is the conceptual backbone, the actual modeling is simplified to a single affine contractive map $\\phi_{eff}$ whose unique Attractor is a single fixed point $V^*$. This 1-map model is acknowledged as a \"first-order approximation\". The paper doesn't fully demonstrate a more complex, multiple-map IFS model needed for concepts with \"complex geometries\" or fractal structures (like those observed in Figure 4 left, e.g., numerical relationship between 1/2/3 digits). This leaves a gap for the simplified model used in practice (1-map affine transformation). To strengthen the paper, a minimal example of a multi-map IFS fit or a deeper analysis of the fractal dimension/geometry would be strong.\n\n2. The concept is defined informally (e.g., \"Lord of the Rings universe,\" \"Python programming language,\" \"toxic language\"). It is also noted that the same concept can have multiple Attractors (e.g., English text, Python style, multi-digit arithmetic), which raises questions about the robustness of the \"Attractor\" concept and the manual effort required to determine the appropriate layer and the boundary of a \"concept\" (i.e., when should Python's OOP and procedural styles be treated as one or two Attractors?). This is actually somehow related to my Weakness 1, which may also offer a more flexible and compex framework for the downstream application.\n\n3. It is a bit blur why the simple vector arithmetic would work for various downstream task. I understand the author provides a set of tasks and experiments to demonstrate the validity of the method. Substracting the Attractor vector for detoxification is somehow intuitive, but why simply adding a language Attractor (e.g., Python to Javascript) not interfere the code context but translate to another programming language? In some tasks, it is quite tricky to define the Attractor and find the operation need to be done."}, "questions": {"value": "1. Related to Weakness 3, the paper states that Attractors form at different layers for different concepts (e.g., fiction at L24, programming at L19, natural languages at L27). What is the hypothesis or metric used to determine the optimal Attractor layer for an arbitrary new concept C? Is it always the layer where the inter-prompt distance is minimized (as suggested in Section 2.1)? If the selection process requires experimentation, this slightly compromises the \"training-free\" and efficiency claims for new concepts. Can the authors formalize the process for finding the most \"contracted\" layer $l_C$?\n\nOther questions are discussed as the details in Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "okkNq5Hd6f", "forum": "qnLj1BEHQj", "replyto": "qnLj1BEHQj", "signatures": ["ICLR.cc/2026/Conference/Submission19458/Reviewer_xGFZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19458/Reviewer_xGFZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670713549, "cdate": 1761670713549, "tmdate": 1762931373013, "mdate": 1762931373013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reveals that Large Language Models (LLMs) map semantically related prompts to similar internal representations at specific layers, regardless of surface form. The authors model this convergence using Iterated Function Systems (IFS), where LLM layers act as contractive mappings toward Concept Attractors that are specific to high-level concepts. Leveraging this insight, the paper proposes a suite of simple, training-free methods that manipulate these attractors directly to address diverse practical applications, including language translation, hallucination reduction, safety guardrailing, and synthetic data generation, achieving performance comparable to or exceeding complex, specialized baselines with minimal computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel and Insightful Theoretical Framework: The paper introduces the Iterated Function Systems (IFS) and Concept Attractors as a compelling dynamic systems explanation for the phenomenon of representational convergence in LLMs, providing a new, high-level lens for mechanistic interpretability.\n2. Highly Efficient and Versatile Training-Free Intervention: The work develops and validates a set of straightforward, zero-cost (training-free) methods based on direct attractor manipulation. This offers a highly practical and general solution for a broad spectrum of tasks (e.g., translation, safety) that avoids the computational expense and complexity of fine-tuning or specialized model architectures.\n3. Strong Performance with High Practical Value: Despite their simplicity, the proposed attractor-based methods achieve performance competitive with or better than complex, dedicated fine-tuning baselines. This demonstrates significant utility, particularly for resource-constrained environments or scenarios requiring rapid, on-the-fly model steering."}, "weaknesses": {"value": "1. While the IFS analogy is powerful, the formal connection remains somewhat hypothetical and is justified more by empirical observation than by a rigorous theoretical proof. The paper does not formally demonstrate that transformer blocks satisfy the conditions for being contractive mappings in a way that guarantees convergence to a unique attractor as defined in IFS theory. The 1-map affine approximation (Eq. 3) seems like a significant simplification. \n\n2. The method for estimating an attractor (averaging activations) is simple, but its robustness is not deeply explored. More critically, the paper notes that attractors form at different layers for different concepts (e.g., L19 for code, L24 for fiction). The paper lacks a clear, systematic methodology for identifying this crucial \"attractor layer\" for a novel, arbitrary concept. This ambiguity is a significant barrier to applying the method in new domains."}, "questions": {"value": "question: \n1. Could you please elaborate on the methodology for identifying the correct \"attractor layer\" for a new concept? Is there a systematic or automated way to find the layer of maximal \"collapse\" (e.g., by tracking the inter-prompt distance from Eq. 1 across layers) without an exhaustive, manual search? This seems to be the most critical hyperparameter for your method. \n\n2. The paper simplifies the attractor to its mean vector. However, the t-SNE plots and the discussion of fractal structures suggest these attractors are regions or manifolds with complex geometries. Have you explored interventions that use a more complex representation (e.g., modeling the attractor as a subspace via PCA, or as a Gaussian distribution) rather than a single point-mass average?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bouiNz0zZu", "forum": "qnLj1BEHQj", "replyto": "qnLj1BEHQj", "signatures": ["ICLR.cc/2026/Conference/Submission19458/Reviewer_C7VY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19458/Reviewer_C7VY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752426919, "cdate": 1761752426919, "tmdate": 1762931372598, "mdate": 1762931372598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the idea that Large Language Models (LLMs) internally organize information using \"Concept Attractors\". The core phenomenon observed is that semantically related prompts, even with very different wording (e.g., \"Who is Gandalf the Grey?\" vs. \"What is the significance of Mount Doom?\"), are processed by the model's layers until their internal representations \"collapse\" into nearly identical locations in the hidden state.\n\nThe authors model this behavior through the lens of Iterated Function Systems (IFS), suggesting that the sequence of transformer layers acts as a set of contractive mappings. These mappings progressively guide any input related to a specific concept (like Lord of the Rings or the Python programming language) toward a unique, stable Attractor. The paper empirically shows that these Attractors form at different layers for different concepts and can even exhibit complex, fractal-like structures.\n\nThe main contribution is demonstrating that these Attractors can be directly manipulated using simple, training-free interventions. By estimating a concept's Attractor (often by just averaging the hidden states of a few examples ), the authors can steer the model's behavior. For instance, they reduce toxicity by subtracting the \"toxic Attractor\" from activations and perform code translation by adding the target language's Attractor to steer the generation . This \"Attractor-based intervention\" is also applied to machine unlearning (by guardrailing responses that get too close to a \"forget\" Attractor) , to reduce hallucinations in vision-language models by reinforcing the \"visual Attractor\" during generation , and to create more factual, diverse synthetic data by perturbing Attractors. These simple, computationally efficient methods are shown to match or exceed the performance of specialized, resource-heavy baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces an interesting theoretical framework to understand the phenomenon of \"clustering\" across semantically similar prompts.\n\n- The interventions the paper proposes are very lightweight from a computational point of view.\n\n- The experimental results (on the small-scale models) are compelling."}, "weaknesses": {"value": "- This method, while computationally easy to implement, requires white-box access to the model.\n\n- The experiments are done on models of relatively small scale.\n\n- The model feels a bit simplistic, I'm not sure if the hypothesis of concept attractors generalizes to more complicated real-world tasks."}, "questions": {"value": "- What conclusions from your experiments do you think would generalize to more complicated tasks and models?\n\n- Do you have any suggestions of how one can take advantage of these observations you have in your paper when given black-box access to an LLM? Would some intermediate type of access (e.g., through log-probabilities) be useful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cRQrmqj8H1", "forum": "qnLj1BEHQj", "replyto": "qnLj1BEHQj", "signatures": ["ICLR.cc/2026/Conference/Submission19458/Reviewer_wEEx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19458/Reviewer_wEEx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975692030, "cdate": 1761975692030, "tmdate": 1762931372196, "mdate": 1762931372196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}