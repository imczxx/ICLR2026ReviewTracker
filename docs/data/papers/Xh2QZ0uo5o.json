{"id": "Xh2QZ0uo5o", "number": 2190, "cdate": 1757016272721, "mdate": 1759898163801, "content": {"title": "Chain-of-Influence: Tracing Interdependencies Across Time and Features in Clinical Predictive Modeling", "abstract": "Modeling clinical time-series data is hampered by the challenge of capturing latent, time-varying dependencies among features. State-of-the-art approaches often rely on black-box mechanisms or simple aggregation, failing to explicitly model how the influence of one clinical variable propagates through others over time. We propose $\\textbf{Chain-of-Influence (CoI)}$, an interpretable deep learning framework that constructs an explicit, time-unfolded graph of feature interactions. CoI leverages a multi-level attention architecture: first, a temporal attention layer identifies critical time points in a patient's record; second, a cross-feature attention layer models the directed influence from features at these time points to subsequent features. This design enables the tracing of influence pathways, providing a granular audit trail that shows how any feature at any time contributes to the final prediction, both directly and through its influence on other variables. We evaluate CoI on mortality and disease progression tasks using the MIMIC-IV dataset and a private chronic kidney disease cohort. Our framework significantly outperforms existing methods in predictive accuracy. More importantly, through case studies, we show that CoI can uncover clinically meaningful, patient-specific patterns of disease progression that are opaque to other models, offering unprecedented transparency into the temporal and cross-feature dependencies that inform clinical decision-making.", "tldr": "We developed an AI model that traces a \"chain of influence\" in patient data to explain exactly how clinical measurements impact each other over time to predict a health outcome.", "keywords": ["Deep Learning", "Interpretable Deep Learning", "Explainable AI", "XAI", "Attention", "Clinical Time-Series", "Feature Interaction", "Healthcare", "Predictive Modeling"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a4eaf06387ce623dd8bc2472526d4788483d136.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors aim to explicitly model how the effect of one clinical variable evolves and influences others over time. They propose a deep learning framework called Chain-of-Influence (CoI), which builds a time-unfolded graph to represent feature interactions. The framework uses a multi-level attention architecture, and its performance is evaluated on the MIMIC-IV dataset and a proprietary chronic kidney disease cohort."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**S1.** The paper studies a clinically meaningful and practically relevant problem, focusing on how to model clinical variable relationships across time. This investigation supports a clearer interpretation of temporal clinical dynamics.\n\n**S2.** The proposed CoI framework, which combines Temporal-Level Attention, Feature-Level Attention, Cross-Feature Attention, and Chained Influence, is overall reasonable from a technical standpoint and aligned with the stated modeling goals.\n\n**S3.** The evaluation compares CoI with representative baseline methods on both the MIMIC-IV dataset and a proprietary chronic kidney disease cohort, demonstrating its utility for both chronic disease progression analysis and acute care tasks."}, "weaknesses": {"value": "**W1.** The proposed CoI framework mainly combines established components, especially various attention mechanisms. In particular, the Temporal-Feature Cross Influence mechanism seems to rely on a heuristic formulation through direct multiplication of selected attention terms. As a result, the methodological novelty and technical depth of the work appear somewhat limited. In addition, there are prior studies that explicitly model cross-feature interactions in electronic health records, including approaches that consider temporal changes. These related efforts should be more thoroughly discussed and included in the comparison to provide a more complete assessment of existing work and highlight the specific contributions of CoI.\n\n**W2.** Given the relatively complex architecture of CoI and the implementation optimizations introduced in Section 3.4, a detailed computational complexity analysis and/or efficiency study would be helpful. Such analysis would clarify the runtime characteristics of CoI and show how it compares to baseline methods in terms of both accuracy and computational cost.\n\n**W3.** The current set of baseline methods is not sufficiently recent or advanced in this domain. Incorporating stronger baselines would help present a clearer and more convincing empirical assessment of the strengths of the proposed framework.\n\n**W4.** The experimental evaluation could be strengthened in the following aspects:\n\n* An ablation study would help isolate the contribution of each component of the CoI framework and clarify its individual role.\n\n* An analysis of crucial hyperparameters would be useful to show the robustness of CoI across different settings.\n\n* It is important to clarify whether the interpretability analysis in Section 5.2 has been supported by medical validation, for example, by involving clinicians to assess whether the derived insights meaningfully support clinical decision-making in practice."}, "questions": {"value": "Beyond W1-W4, I have the following questions for clarification:\n\n**Q1.** In Section 3.2, the formulation for Feature-Level Attention uses $\\beta_t[k]$, whereas in Section 3.3, the Local Contribution Matrix $C$ uses $b_t[k]$. It is unclear why this substitution occurs. Additional explanations would be helpful to clarify the relationship between these terms.\n\n**Q2.** For the Temporal-Feature Cross Influence mechanism, does the design only capture pairwise feature interactions, or is it capable of representing higher-order interactions as well?\n\n**Q3.** Regarding the temporal attention visualizations in Figure 1, are the results shown for a single patient, or are they aggregated across the patient cohort?\n\n**Q4.** For the comparison of feature attributions between RETAIN and CoI in Figure 2, it would be helpful to specify which time step the plotted results correspond to.\n\n**Q5.** Figure 3 suggests that the influence between two features may be asymmetric. If this is intended, a more detailed explanation of this behavior, along with theoretical justifications, would strengthen the claim.\n\n**Q6.** The presentation of Figure 3 and its accompanying discussion could be refined so that the highlighted feature relationships are more clearly aligned with the described findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SQ5UemRAJn", "forum": "Xh2QZ0uo5o", "replyto": "Xh2QZ0uo5o", "signatures": ["ICLR.cc/2026/Conference/Submission2190/Reviewer_kP2Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2190/Reviewer_kP2Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842613441, "cdate": 1761842613441, "tmdate": 1762916121345, "mdate": 1762916121345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Chain-of-Influence (CoI), an interpretable deep learning framework for clinical longitudinal data that aims to make explicit how features interact across time and how these interactions drive the final prediction. CoI combines (i) temporal attention to learn which time points are important, (ii) feature-level attention at each time to identify the most relevant variables, and (iii) a transformer-style cross-time attention matrix to capture how information flows from time t to time t’ for each feature. The authors evaluate CoI on a proprietary Chronic Kidney Disease (CKD) cohort and on the public MIMIC-IV dataset, and they further present visual “influence networks” on the CKD data to show that the model recovers clinically plausible disease progression pathways across features."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and relevant problem. The paper tackles a clear and highly relevant problem in clinical ML: the need for auditable, interpretable models that are essential for widespread adoption in clinical practice. \n2. Focus on explainability. CoI’s ability to expose how features interact across time is genuinely useful, and the authors present evidence that the resulting explanations align with clinical intuition. Furthermore, the graph-level visualisation is particularly effective, as it could enable both auditing the model’s predictions and potentially uncovering novel statistical associations.\n3. Through their choice of datasets, the authors show that the model can handle signals with markedly different temporal granularities."}, "weaknesses": {"value": "1. All experiments seem to use data that have been regularised in time, which is very uncommon for real-world clinical data. It is unclear how CoI behaves on irregularly-sampled clinical time series or under different imputation/alignment strategies. \n2. Baselines are too narrow. The paper would benefit from comparing CoI with diverse, modern architectures like transformers-based models (i.e. BEHRT [1]) or ODEs[2]\n3. The authors report metrics such as F1, precision and recall to show the model’s performance on highly imbalanced datasets, but I cannot see any mention of which steps the authors took to address class imbalance during training.\n4. No ablation is provided to isolate the impact of temporal attention, feature-level attention, and cross-time attention on the model’s predictive performance\n\n\n[1] Li, Yikuan, et al. \"BEHRT: transformer for electronic health records.\" Scientific reports 10.1 (2020): 7155.\n\n[2] Rubanova, Yulia, Ricky TQ Chen, and David K. Duvenaud. \"Latent ordinary differential equations for irregularly-sampled time series.\" Advances in neural information processing systems 32 (2019)."}, "questions": {"value": "1. How would CoI comprare to the aforementioned baselines (BEHRT, Latent ODEs) ?\n2. How would CoI deal with irregularly-sampled longitudinal trajectories ? What would the impact be on both performance and explainability?\n3. Both datasets are imbalanced (CKD 6% ESRD, MIMIC-IV 10.8% mortality), yet the training section only states that BCEWithLogitsLoss was used, without specifying any class weighting, sampling, or focal loss. Please clarify whether any imbalance mitigation was applied, and if not, why the unweighted BCE was sufficient for the reported F1 gains.\n4. Please provide ablations isolating the impact of the model’s key components ."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kYyVi82Zi1", "forum": "Xh2QZ0uo5o", "replyto": "Xh2QZ0uo5o", "signatures": ["ICLR.cc/2026/Conference/Submission2190/Reviewer_4FJb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2190/Reviewer_4FJb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847844604, "cdate": 1761847844604, "tmdate": 1762916120680, "mdate": 1762916120680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Chain-of-Influence (CoI), a deep learning framework for interpretable clinical time-series prediction that explicitly models how features influence each other temporally. The authors argue that existing clinical models either lose temporal dynamics by aggregating information, or miss critical inter-feature relationships by treating variables independently. CoI aims to capture the cascading pathways of disease progression.\n\n**Methodology**: CoI combines multiple network modules;\n\n* Two bidirectional LSTM branches compute temporal attention and feature attention, which form a local contribution matrix that represents how important a given feature is at a specific timepoint.\n* Transformer layers produce a cross-attention matrix that captures temporal dependencies\n* These are combined into final chained influence measure, which quantifies the pairwise strength of two features at two different timesteps.\n\n**Results** : The architecture significantly outperforms non-transformer based approaches when evaluated on chronic kidney disease progression and ICU mortality prediction. The approach also provides interpretable novel knowledge graphs that 'capture clinically meaningful temporal-feature interactions'."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**: The paper makes a genuine conceptual contribution by introducing a mechanism to trace pairwise feature-to-feature influences across time. While attention-based interpretability in clinical ML is well-established, the explicit formulation of influence chains that quantify how feature i at time t affects feature j at time t' is novel. This goes beyond identifying \"what's important when\" to modeling \"how early indicators cascade through specific pathways.\" The combination of LSTM-derived local contributions with Transformer-derived temporal attention to create these influence measures is a creative synthesis of existing architectural components toward a new interpretability goal.\n\n**Quality**: The empirical evaluation is solid. CoI achieves state-of-the-art performance across two clinically diverse datasets—chronic kidney disease progression (0.960 AUROC) and ICU mortality prediction (0.950 AUROC). The consistency of gains across different clinical contexts (chronic vs acute), temporal scales (months vs hours), and data characteristics (sparse vs dense observations) suggests the approach has some generalizability. The datasets themselves are appropriate: MIMIC-IV provides public reproducibility, while the CKD cohort addresses an important clinical problem. Results are reported across multiple metrics.\n\n**Clarity**: The paper is generally well-written with clear prose and professional figures. The motivation is well-articulated, and the related work section effectively positions the contribution within existing literature. The description of each module (temporal attention patterns, feature importance, influence networks) are clear and convey the intended interpretability mechanisms effectively. However, there are significant presentation gaps: the methodology lacks an end-to-end architectural description, no diagram shows component connectivity, and critical details about loss application and gradient flow are omitted. The (potential) redundant definition of the contribution matrix C creates unnecessary confusion. These aren't stylistic issues, they're structural omissions that prevent understanding how the model actually works.\n\n**Significance**: The problem is important, interpretable clinical prediction models that can explain disease progression pathways have clear value for high-stakes medical decision-making. If the influence chains actually reflect the model's reasoning (currently unvalidated), this could represent a meaningful advance in clinical AI interpretability. The consistent performance improvements suggest practical value beyond interpretability. The work addresses temporal-feature interdependencies in a more explicit way than prior models, which aligns better with clinical understanding of how diseases progress. However, the significance is diminished by the lack of validation that the interpretability mechanisms are faithful to the model's predictions. Without this validation, clinicians cannot reliably use the influence chains for decision-making, which limits real-world impact."}, "weaknesses": {"value": "**Incomplete Architecture Specification**\n\nThe methodology section describes individual components but never specifies how they connect to produce output predictions. This is not a presentation issue, it's a fundamental gap that prevents reproduction and understanding of how the model actually works. Specific details that weren't clear to me;\n\n* How are BiLSTM outputs, Transformer outputs, and attention weights combined to compute the final prediction ŷ?\n* Where is the BCEWithLogitsLoss applied? What tensor does it operate on?\n* Both BiLSTM branches and the Transformer take the same positionally encoded embeddings as input. How do gradients from the loss reach the BiLSTM parameters? Are they part of the prediction pathway, or do they only compute post-hoc interpretability metrics?\n\nRequired additions:\n\nAn architecture diagram with the full computational graph from input to loss, and additional descriptions in text on how the BiLSTM and transformer interact to encode all of the interpretable attention maps.\n\n**Interpretability Concerns**\n\nThe paper's central contribution is interpretable influence chains, but provides zero validation that these mechanisms reflect actual model reasoning. Since the architectural connectivity is unspecified, there's no evidence that BiLSTM-derived components (α, β, C) correlate with what drives predictions. If these maps will be used by clinicians to support their decision making, we want to have some level of confidence that they correlate with the actual predicted output from the model.\n\nWhat the paper provides:\n* Feature importance rankings showing CoI and RETAIN identify similar top features (Figure 2)\n* Temporal attention patterns (Figure 1)\n* Influence chain visualizations (Figure 3)\n* Qualitative clinical interpretations (Section 5.2.3)\n\nNone of these demonstrate faithfulness. The fact that CoI and RETAIN produce highly similar feature importance rankings raises the question: what clinical interpretability value do the influence chains provide beyond what simpler RETAIN already offers?\n\n**Architectural Justification**\n\nThe paper uses two BiLSTM branches alongside a Transformer without justification via ablation studies. The stated rationale (Section 3) \"better capture long-range dependencies and bidirectional context\", something Transformers already provide through self-attention. The paper itself cites these strengths when discussing Transformers (Section 2.3). Is it possible to generate the contribution matrix through a modified transformer block? How important is the BiLSTM to the overall performance of the architecture? Table 14 shows CoI uses 8 Transformer layers but only 2 attention heads, far below standard practice (BERT uses 12 heads, GPT uses 12-16). Is the Transformer under-configured to accommodate BiLSTM parameters? Would a properly-configured Transformer alone (8-12 heads as standard) perform comparably?\n\n**Confusing Repeated Definitions**\n\nThe local contribution matrix C is defined twice with nearly identical equations (lines 152 and 196):\n\nFirst in Section 3.2 (BiLSTM attention section)\nAgain in Section 3.3 (Transformer section, with inconsistent notation: βt vs bt)\n\nAre these the same matrix? If yes, why repeat it? If no, what's the difference?\nDoes the Transformer contribute to C? The equations suggest it doesn't (only produces A), but placing the definition in the Transformer section implies otherwise. Combined with the unspecified architecture, this makes it impossible to determine which components contribute to predictions versus interpretability visualizations.\n\n**Performance on incomplete temporal sequences**\n\nThe paper trains and evaluates on patients with complete temporal histories (CKD: 8 time points over 24 months; MIMIC-IV: 48 hours of observations). However, real-world clinical deployment requires predictions at arbitrary points in a patient's timeline when complete histories are unavailable. Were any experiments conducted to evaluate how well the model performs in a more realistic deployment setting? What impact would this have on the influence chains?"}, "questions": {"value": "I've detailed major concerns in the weaknesses section. Here are some minor ones;\n\n**Minor suggestions**\n\n* Confidence intervals for the metrics to help with statistical significance.\n* Use a pre-defined threshold for pointwise metrics (eg F1, Recall, Accuracy all @ a fixed Precision). This makes comparisons between different models easier to see.\n* Does the CoI approach lead to output predictions that are better calibrated compared to other models?\n* Justification for the use of DyT instead of LayerNorm as normalization strategy in transformer blocks.\n* Explanation on how the directionality of Influence Chain is determined."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E48PViI1Kd", "forum": "Xh2QZ0uo5o", "replyto": "Xh2QZ0uo5o", "signatures": ["ICLR.cc/2026/Conference/Submission2190/Reviewer_cYzd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2190/Reviewer_cYzd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762362395488, "cdate": 1762362395488, "tmdate": 1762916120028, "mdate": 1762916120028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}