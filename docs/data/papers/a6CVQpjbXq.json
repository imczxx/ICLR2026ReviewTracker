{"id": "a6CVQpjbXq", "number": 18128, "cdate": 1758284185700, "mdate": 1759897130872, "content": {"title": "Best-of-Majority: Minimax-Optimal Strategy for Pass@k Inference Scaling", "abstract": "LLM inference often generates a batch of candidates for a prompt and selects one via strategies like majority voting or Best-of-\nN (BoN). For difficult tasks, this single-shot selection often underperforms. Consequently, evaluations commonly report Pass@$k$: the agent may submit up to $k$ responses, and only the best of them is used when computing regret. Motivated by this, we study inference scaling in the more general Pass@$k$ inference setting, and prove that neither majority voting nor BoN exhibits the desirable scaling with $k$ and the sampling budget $N$. Combining the advantages of majority voting and BoN, we propose a new inference strategy called Best-of-Majority (BoM), with a pivotal step that restricts the candidates to the responses with high frequency in the $N$ samples before selecting the top-$k$ rewards. We prove that when the sampling budget is $N=\\tilde\\Omega(C^\\*)$, the regret of BoM is $O(\\epsilon_{\\mathrm{opt}}+\\sqrt{\\epsilon_{\\mathrm{RM}}^2C^\\*/k})$, where $C^*$ is the coverage coefficient, $\\epsilon_{\\mathrm{RM}}$ is the estimation error of the reward model, and $\\epsilon_{\\mathrm{opt}}$ is the estimation error of reward at the optimal response. We further establish a matching lower bound, certifying that our algorithm is minimax optimal. Beyond optimality, BoM has a key advantage: unlike majority voting and BoN, its performance does not degrade when increasing $N$. Experimental results of inference on math problems show BoM outperforming both majority voting and BoN.", "tldr": "", "keywords": ["Inference-time scaling", "Pass@k"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f25ff4b303295f775b3cee487506d009e1a7715.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper considers scaling laws for inference: A model is required to output $k$ responses $y$ to a given prompt $x$, and the regret, compared to the optimal response, is measured according to the best one in this set of $k$ responses. In this paper the model is allowed to output a total of $N$ responses, and pass chosen $k$ ones to evaluation, based on a given estimated reward model. Thus, $N$ represents the computational cost of inference. A minimax lower bound on the regret as a function of $k$ (as well as the reward model estimation error) is stated, and the regret of two previously proposed algorithms (used in practice) Majority voting and Best-of-$N$) is analyzed. Both these algorithms are shown to be non-minimax. An algorithm (Best-of-Majority, BoM) is proposed which is shown to be minimax optimal with respect to $k$. The main idea of the algorithm is that even if a large number of responses $N$ are generated, they should be filtered to the most frequent ones, which are also assumed to be the ones in which the reward model is most reliable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper derive theoretical bounds for a timely problem of inference scaling laws. \n\n2) Two practical algorithms are analyzed, which are shown to be sub-optimal, and a new one is proposed, which is established to be minimax optimal."}, "weaknesses": {"value": "1) The main goal of the paper is to study the trade-off between the computational cost of inference, as expressed by $N$ and the regret. From this aspect, it is unreasonable that both the upper for BoM and the minimax lower bound do not depend on $N$, and thus do not capture this trade-off. \n\n2) Theorem 5.1: The result does not cover the regime $k\\lesssim N\\lesssim C^{*}(x)$. \n\n3) The algorithms and analysis are based on empirical counts over the response alphabet $\\mathcal{Y}$. It is not obvious how the performance scales with this alphabet size. \n\n4) The proposed algorithm requires the knowledge of $C^*(x)$ in order to make sure that $N$ is large enough. Similarly, the algorithm requires knowledge of a lower bound on $\\epsilon_{RM}$ which is somewhat cumbersome since the reward model is designed to have a small error."}, "questions": {"value": "1) Line 82 - We further “introduce a formal definition of scaling-monotonicity”: What is exactly the contribution of this paper to this definition, beyond Huang et al. 2025). Line 188 - “we introduce the reference policy..”: The same question. 2) Could you explain the dependence on $C^{\\star}$  and why it is essential? Under Assumption 3.2, $C^{\\star}$ is the inverse of the probability of the reference policy at the optimal response. Isn't it possible to slightly mix $\\pi_{ref}$ with a uniform distribution over the response alphabet, to make $C^{\\star}$ effectively a constant? \n\n3) Line 204 – “sufficiently many samples are observed” is unclear: Samples for reward estimation or responses?\n\n4) Line 252- “each receiving higher probability under $\\pi_{ref}$”: Higher than what?\n\n5) “Moreover, our earlier analysis reveals complementary strengths of these methods...” - this is unclear since the analysis thus (Theorems 4.1 and 4.2) only focused on impossibility bounds. \n\n6) In (B.4), I would propose to add a citation for the Chernoff bound for negatively correlated random variables. \n\n7) In the proof of Theorem B.2, what is meant by “the idea of averaging hammer”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iX424VHhcn", "forum": "a6CVQpjbXq", "replyto": "a6CVQpjbXq", "signatures": ["ICLR.cc/2026/Conference/Submission18128/Reviewer_Df6F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18128/Reviewer_Df6F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761422758229, "cdate": 1761422758229, "tmdate": 1762927891294, "mdate": 1762927891294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the Pass@k inference scaling problem for large language models (LLMs), where a model can generate $N$ candidate responses and select up to $k$ outputs, and only the best of them is used to compute the regret. The authors observe that widely used inference strategies for Pass@1 inference, such as majority voting and Best-of-$N$ (BoN), fail to exhibit desirable scaling behavior as $k$ and $N$ increase. \n\nTo address this, they propose a new inference strategy, Best-of-Majority (BoM), which ensures a $O(\\epsilon_{\\text{opt}} + \\sqrt{C^{*}\\epsilon_{\\text{RM}}^2/k})$ regret bound. Here, $C^\\star$ is the coverage constant, $\\epsilon_{\\text{RM}}$ is the estimation error of the reward model, and $\\epsilon_{\\text{opt}}$ is the estimation error of reward at the optimal response. They also provide a matching minimax lower bound for any Pass@k inference strategy. Importantly, BoM is shown to be scaling-monotonic, meaning its performance does not degrade with larger sampling budgets $N$, which separates the.\n\nEmpirically, the method is validated on GSM8K, MATH-500, and AIME24, where BoM consistently outperforms or matches the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong theoretical foundation: The authors show strong lower bounds on the regret of previous methods of majority vote and BoN, providing insightful explanations on why they do not benefit from the scaling law. \nThe construction of the BoM method, which combines the advantages of the two previous methods, is simple and intuitive. They also show matching lower and upper bounds to demonstrate the minimax optimality for BoM.\nThe theoretical model and analysis are novel.\n\n2. Good empirical validation: Experiments on various datasets align well with theory and convincingly support the claims."}, "weaknesses": {"value": "Practical relevance: \n\ni. It is slightly unclear from the text the broader impact of the Pass@k inference problem. The authors do mention the usage of Pass@k alignment in training LLMs in Section 2, but it could be beneficial to discuss its application in LLM inference or generation.\n\nii. In the paper, the BoM method is proved to be scaling monotonic, which is emphasized as a major separation from previous methods. From my understanding, this property guarantees that the error should converge to zero eventually as $N$ increases. However, in the experiments, the BoM method doesn't seem to benefit from this property, e.g., in Figure 2(a), the accuracy decreases at first, and in Figures 2(b) and 2(c), the accuracy is always $0.7$. This seems to suggest that we shouldn't use large $N$ in practice anyway. Is it because of the inherent error from the chosen reward model, i.e., $\\epsilon_{\\text{RM}}$, or is $N$ not large enough?\n\nClarity:\n\ni. The authors could provide more explanation of the intuition of the technical terms, e.g., the coverage constant, the definitions in Assumption 3.1 and Assumption 3.2, especially for audiences with diverse backgrounds.\n\nii. In Theorem 5.1, the parameter $\\alpha$ of the BoM algorithm is chosen to be $\\Omega(1/C^*)$. In the experiment, the authors set $\\alpha = 0.015$ for $N = 100$ and $\\alpha = 0.005$ for other choices of $N$. I would assume the coverage constant is hard to compute in practice, but the authors do not explain the reasons for their choices of $\\alpha$. Why are different parameters needed for different $N$? Also, I'm curious about how the accuracy varies with different $\\alpha$."}, "questions": {"value": "Please refer to the 'Weaknesses' section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kMgNRIK12W", "forum": "a6CVQpjbXq", "replyto": "a6CVQpjbXq", "signatures": ["ICLR.cc/2026/Conference/Submission18128/Reviewer_kb59"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18128/Reviewer_kb59"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795519036, "cdate": 1761795519036, "tmdate": 1762927890864, "mdate": 1762927890864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies inference-time scaling under the Pass@k framework, where the algorithm may generate up to $N$ responses but submit at most $k \\leq N$ of them. In this paper, the evaluation of algorithms focus on (1) the scaling of regret in $k$ and (ii) a notion called \"scaling-monotonic\", i.e. whether the algorithm degrades as $N$ grows (when the reward model is accurate.) \n\nThe authors show the limitations of two common baselines: (Weighted) majority voting suffers a constant regret lower bound even when the reward model is perfect; also, Best-of-N (BoN) admits a lower bound violating scaling-monotonicity. As a fix, they propose an algorithm called *Best-of-Majority (BoM)*, which can be viewed as a \"combination\" of both: BoM (1) first filters to responses whose empirical frequencies are high (a majority-style step), and then (2) selects the top-$k$ by reward (a BoN-style step). Then they show that BoM achieves an upper bound matching the minimax lower bound they show for the setting. Crucially, BoM scales with $k$ (in addition to an optimal estimation error of the reward) and satisfies the scaling-monotonicity.\n\nEmpirically, they compare BoM against majority voting and BoN. The experimental results show that BoM generally outperforms or matches the baselines -- the plots versus $k$ show BoM consistently better on MATH-500 and competitive on GSM8K/AIME24, especially for small $k$."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is well-written, with a clear structure and flow of logic. It isolates the scaling-monotonicity (which is defined formally) as a target property and then set it as an explicitly goal of algorithms. By showing how baseline approaches fail this goal, and the present a lower bound and a simple algorithm that matches it, the line of work feels convincing and clean. \n\n- I think the story that motivates the construction of the BoM makes good sense: Indeed, an ideal algorithm should balance the considerations on both uncertainty and the reward. For the former, BoM uses the empirical frequencies of generated responses to approximate the probability, and for the later, BoM then query the reward model on the surviving candidates."}, "weaknesses": {"value": "(I should probably stress that the topic of this paper is not my area. Thus I may not have the expertise to fully assess e.g. the novelty and relevance of this work.)\n\n- The uniqueness-of-optimum assumption feels restrictive. One can easily think of many tasks that admit multiple equally good answers (or near-ties). It seems that several bounds and the neat identification of $\\mathcal{C}^*$ relies on the uniqueness assumption.\n\n- It seems that BoM needs a coverage threshold that depends on $\\mathcal{C}^*$, which can be unknown and may be badly misestimate. This paper does not offer, e.g. a data-driven estimator or a quantification of how misspecification affects regret or monotonicity.\n\n- The empirical scope feels a bit narrow: all datasets are math/verifier-style. Given this, I'm not sure whether the empirical results are representative on a broader set of tasks."}, "questions": {"value": "Could you respond to the points listed in the Weakness section? (e.g. discuss how sensitive the guarantees are when there are several maximizers or when rewards are flat within a small margin, etc.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tAesXLoaB9", "forum": "a6CVQpjbXq", "replyto": "a6CVQpjbXq", "signatures": ["ICLR.cc/2026/Conference/Submission18128/Reviewer_JFFa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18128/Reviewer_JFFa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846555315, "cdate": 1761846555315, "tmdate": 1762927890469, "mdate": 1762927890469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies Pass@k inference and proves that majority voting and Best-of-N are not scaling-monotonic, then introduces Best-of-Majority (BoM)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "An optimal algorithm for inference-time algorithm"}, "weaknesses": {"value": "**Weaknesses**\n\n1. **Relation to [1].** Aminian et al. [1] also derive an **upper bound** on BoN regret. Please state their result precisely, compare assumptions and regimes (dependence on (N), and reward-model error), and explain how your bounds differ or complement theirs.\n\n2. **Comparison to InfAlign [2].** Please include an empirical and conceptual comparison to **InfAlign** [2], clearly outlining the methodological differences and where each approach is expected to excel or fail.\n\n3. **Soft-BoN vs. BoN vs. yours.** As noted in [1], BoN can over-optimize; **Soft-BoN** is proposed as a mitigation. How does **Soft-BoN** perform relative to your method—both theoretically (regret guarantees, assumptions) and empirically?\n\n4. **Asymptotics in Theorem 4.2.** Using Theorem 4.2, in the limit $(N\\\\to\\\\infty)$ the regret for a non-zero error appears to approach a **constant independent of (N)**. Please elaborate on the mechanism, the constant’s dependence on problem/reward parameters, and its practical implications.\n\n5. **Reward calibration.** Is the **reward function calibrated**—either in the formulation (assumptions/guarantees) or in experiments (procedure, metrics)? \n\n6. **“Mathematical equivalence.”** When you write “select (k) answers (up to **mathematical equivalence**) with highest frequency,” what exactly constitutes equivalence (e.g., algebraic identity, numerical equality within tolerance, format-invariant parsing)? Please define the criterion and how it’s implemented in experiments.\n\n**References**\n\n[1] Gholamali Aminian, Idan Shenfeld, Amir R. Asadi, Ahmad Beirami, Youssef Mroueh. *Best-of-n through the smoothing lens: KL divergence and regret analysis.* arXiv:2507.05913, 2025.\n\n[2] Ananth Balashankar et al. *InfAlign: Inference-aware language model alignment.* ICML, 2025."}, "questions": {"value": "please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iKWJsDvBHX", "forum": "a6CVQpjbXq", "replyto": "a6CVQpjbXq", "signatures": ["ICLR.cc/2026/Conference/Submission18128/Reviewer_ckrW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18128/Reviewer_ckrW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762282858395, "cdate": 1762282858395, "tmdate": 1762927889899, "mdate": 1762927889899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the problem of how to use a given LLM, a proxy (potentially inaccurate) reward model, and $N$ responses to a prompt (sampled iid from the LLM), to select $k$ out of the $N$ responses. The performance metric for any selection scheme is the regret between the expected reward of the best response and the expected best reward of the $k$ selected items, according to a (unknown) ground-truth reward model, also termed the pass@k regret. Within this setting, the paper argues that existing inference strategies such as Best-of-$N$ and weighted majority vote do not perform satisfactorily in that they do not give $o(1)$ regret with $N \\to \\infty$ and the are not monotone, i.e., their performance can degrade with more samples $N$ and an accurate proxy reward model. The paper goes on to design a new algorithm (Best of Majority) that, in spirit, is a 'best of both worlds' solution, blending the strengths of both these strategies using an additional probability threshold hyperparameter $\\alpha$. This algorithm is shown to achieve monotone scaling and $o(1)$ regret as $1/N$ and the proxy reward model error tend to $0$. The paper also exhibits a fundamental lower bound (in a worst-case sense with a hard instance) on the regret scaling as a function of the reward model approximation errors, the base policy coverage distribution and $k$."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem formulated and studied in the paper is very timely and relevant. Inference efficiency and quality optimization at scale carries a high potential to make real-world impact in LLM pipelines. \n\n- The paper's mathematical model of the inference problem is thorough, and captures several practical aspects, including the fact that reward model oracles used for cheaply judging quality of responses must be only approximately accurate, and that upto $k \\geq 1$ responses could be proposed for a prompt in the pass@k performance metric.\n\n- I appreciated the clear and precise technical analysis of the regret performance metric, using appropriate tools from probability and concentration methods. The results are presented with clearly stated hypotheses/assumptions and expose the underlying problem-dependent quantities clearly, e.g., $\\epsilon_{RM}$, $C^*(x)$, etc.\n\n- The paper's analysis is comprehensive, showing both algorithm-dependent and fundamental performance limits for the pass@k regret. \n\n- The paper designs a new inference algorithm (best of majority aka BoM) to circumvent the weaknesses raised about the existing inference algorithms."}, "weaknesses": {"value": "- While the negative results for existing inference algorithms such as BoN and weighted majority vote are illuminating, they are essentially of a worst-case flavor in the sense that a 'bad' instance is constructed in each case to render high $\\Omega(1)$ regret. A concern is that this is too 'pathological'; instead, bringing out the dependence of an arbitrary problem instance and its parameters in the regret lower bound per algorithm could be more insightful and, of course, a stronger result that would imply, as special cases, the worst-case instances above. \n\n- Perhaps the most significant weakness is that, while the new algorithm BoM ensures controlled regret, it also puts the burden on the deployment engineer of tuning the additional crucial hyperparameter $\\alpha \\in [0,1]$. The performance guarantee of BoM in the paper is only valid when $\\alpha$ is set proportional to the probability of the base policy on an optimal answer, $\\pi_{ref}(y^*|x)$, in addition to ensuring that $N$, the total number of generations, is above a threshold determined by the reward error $\\epsilon_{RM}(x)$ **per prompt**. I wonder what practical modalities exist to be able to set these hyperparameters in this way. There is no adequate discussion of whether these hyperparameters can be estimated or learnt from data or trials. \n\n- In a similar vein, the paper's experiments shed no additional light on how to set $\\alpha$ and $N$ 'well' depending on the problem instance and reward model error. There is no discussion on what considerations went into the choice of $\\alpha$ used in the experiments, as well as an ablation on how different values of $\\alpha$ influence overall performance. These would be essential in judging the quality and robustness of the new algorithm. \n\n- The dependence on $N$ (the total number of generations) is missing in Theorems 5.1 and 6.1. This is in contrast to the referenced work of Huang et al (2025) that clearly outlines how $N$ influences regret performance. Could the author(s) please discuss this explicitly?"}, "questions": {"value": "- Please see the 'Weaknesses' section for questions that are posed inline. \n\n- (minor) Why bother defining two coverage coefficients $C^*(x)$ and $C^*_\\infty(x)$ when you finally argue that they coincide?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C4tMDI6bbg", "forum": "a6CVQpjbXq", "replyto": "a6CVQpjbXq", "signatures": ["ICLR.cc/2026/Conference/Submission18128/Reviewer_h4Se"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18128/Reviewer_h4Se"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18128/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762934036236, "cdate": 1762934036236, "tmdate": 1762934036236, "mdate": 1762934036236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}