{"id": "KYqHcVVR92", "number": 19017, "cdate": 1758292749430, "mdate": 1759897066114, "content": {"title": "Fast SDP certification of neural networks : towards large multi-class datasets", "abstract": "We present a new quadratic model for the certification problem in adversarial robustness, which simultaneously accounts for all possible target classes. Building on this model, we propose a novel semidefinite programming (SDP) relaxation for incomplete verification. A key advantage of our approach is that it certifies robustness in a single optimization, avoiding the need for a separate resolution per class. This yields a significant computational speed-up and enables scalability to large datasets with many classes. To further gain in efficiency, we also propose an effective pruning strategy of active neurons, thus reducing the problem dimensionality and accelerating convergence.", "tldr": "We introduce a new quadratic formulation and its SDP relaxation to simultaneously certify a neural network accross all classes with pruning of all stable actives and inactives neurons and addition of tightening cuts.", "keywords": ["Adversarial robustness", "SDP relaxation", "certification"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f36a6f9d45de91d49cbe0060cf6fbc5785db0d3.pdf", "supplementary_material": "/attachment/34441061b3a9b1cb8f136e85eeb456c322319a0e.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes a single (untargeted) quadratic certification model by introducing selector binaries $\\beta_j$ so that one optimization certifies robustness across all possible targets, and then derives an SDP relaxation to solve it.  To improve scaling, the authors use chordal decompositions, pruning of stable active (and inactive) neurons and compensating for the missing quadratic couplings via McCormick-based cuts. Experiments on MNIST fully connected feed-forward networks and a 67-class composite benchmark suggest higher certified rates and/or faster runtime with respect to other target-based SDP baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Considered problem is relevant, as robust DNN-driven systems are crucial for successful deployment. The pruning of *active* neurons (not only inactive ones) and the chordal decomposition are sensible choices to reduce PSD block sizes in practice."}, "weaknesses": {"value": "Modeling disjunction via binary variables is a relatively standard and well-known approach. The contribution lies more in the SDP instantiation and pruning/cut design, but exhaustive ablation studies illustrating the clear impact of adding specific individual features to the SDP relaxation are missing.\n\n### Major points\n\n- Experimental results are very limited. Only relatively small feed-forward networks are used (up to 1800 hidden neurons). Tightness of the bound and/or scalability of the method with respect to different sizes of perturbation regions is not assessed.\n- Presentation: The paper is difficult to follow because key ideas are scattered, notation is overloaded and inconsistent (e.g., superscripts can refer to vectors or specific neurons, $u$ can mean both untargeted and unstable, and many others), with many typos and language issues (e.g., equality in lines 336–337 is mathematically unsound). Explanations for understanding experimental results only come after a few paragraphs or subsections, etc. Overall, a thorough rewrite is necessary before the contributions can be clearly and fairly assessed.\n\n### Minor points\n\n- For most of the references, `\\citep{}` should be used to facilitate reading.\n- Be consistent in writing. For instance, formulations *ReLU*, *relu*, and *Relu* all appear in the manuscript. Same with *semi-definite*, *semidefinite*, and *semi definite*. The dataset is sometimes denoted $\\mathcal{D}$ and sometimes $\\mathcal{X}$. Introducing $\\mathcal{K}$ is probably not necessary.\n- Figure 1 is never referred to in the main text.\n- Property 3 is trivial and already explained in words before being formally written down. I suggest removing it. The same can be said about Theorem 1."}, "questions": {"value": "1. Why consider a manufactured dataset with 67 classes rather than a real multiclass dataset (e.g., CIFAR-100 with small convolutional networks)?\n\n2. Wouldn't an explicit proof (even in the Appendix) of Proposition help readers understand the pruning method and, consequently, the structure of the relaxation?\n\n3. What do the percentages in Figure 2 represent?\n\n4. Why not formulating the untargeted objective for other approaches as well and comparing them under this common objective? This would isolate the impact of each modeling choice.\n\n5. For targeted approaches from the literature, how is the target index chosen? Are you solving for all possible targets? Please clarify.\n\n6. For your proposed relaxations, is it possible to analyze loss in bound tightness as a function of, say, attack magnitude, network depth, or fraction of pruned neurons? This analysis could strengthen the paper.\n\nFinally, Your approach selects $\\lfloor pn_k\\rfloor$ inter-layer RLT cuts per neuron using the top magnitudes $|W^{j}_{k+1}|$,\n computed with $n_k$.  \n\nWhy is this preferable to selecting $\\lfloor p n_k^{u}\\rfloor$? What is the sensitivity of certification rate and runtime w.r.t. $p$? Is there an empirical threshold $p_{\\min}$ below which tightness and performance degrade notably?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n1Zy9hEoiP", "forum": "KYqHcVVR92", "replyto": "KYqHcVVR92", "signatures": ["ICLR.cc/2026/Conference/Submission19017/Reviewer_4Po3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19017/Reviewer_4Po3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760835338048, "cdate": 1760835338048, "tmdate": 1762931062933, "mdate": 1762931062933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to accelerate incomplete verification of adversarial robustness by (i) casting an untargeted single quadratic objective that accounts for all classes, (ii) deriving a semidefinite relaxation augmented with tightening cuts, and (iii) introducing a pruning rule for provably stable-active neurons to reduce computational load."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The move from per-class targeted certification to a single untargeted quadratic program with class-selection binaries $\\beta_j$ is clear. Theorem 1 shows equivalence to minimizing over targets, justifying the single-SDP approach.\n(2) Tight, well-motivated relaxation: The SDP includes chordal decomposition, McCormick envelopes for $\\beta \\times z$, and two new coupling inequalities ((26)–(27)) tailored to class selection; these are technically sound choices for tightening.\n(3) The paper prunes stable active ReLU units and replace them with linear expressions, then compensates the missing quadratic terms with linear bounds. This is practical contribution with qualification in Proposition 2. \n(4) The results in Figure 2 strongly demonstrate the scalability with respect to the number of classes."}, "weaknesses": {"value": "(1) RLT cuts are formed by multiplying valid linear inequalities to obtain quadratic ones; as the number of RLT constraints grows, the candidate set can blow up combinatorially, risking intractability. The paper chooses a cut percentage  $p$ by heuristic per architecture. Please report certificate rate and runtime as functions of  $p∈(0,10,30,60,100)$% and identify the “sweet-spot” $p^*$. \n\n(2) Only fully connected networks are evaluated. Extensions to CNNs/ResNets (where SDP relaxations are common) are missing. Even a small CNN/ResNet ablation would clarify whether the method’s gains persist under weight sharing and deeper topologies.\n\n(3) Baseline coverage is weak. Beyond SDP, include LP relaxations, $β$-CROWN, and other established incomplete verifiers, under matched perturbation radii. Please sweep multiple $ϵ∈(1/255,2/255,4/255,8/255)$ and report certified accuracy–runtime curves.\n\n(4) Section 5.2 claims that $\\beta-$CROWN  is unable to tighten the bounds to eliminate all possible target classes as the number of neurons increases, but no experiment demonstrates this trend. Provide plots of (i) average margin lower bound vs. width/depth, and (ii) fraction of eliminated classes vs. network size.\n\n(5) Table 2 shows fewer certified cases with pruning, but it remains unclear why. Please quantify: (i) certified fraction vs. number of unstable ReLUs, (ii) average bound gap (lower–upper) before/after pruning, and (iii) failure mode taxonomy (e.g., timeout, solver infeasibility, loose pre-activation bounds). Wider validation across architectures/datasets is encouraged."}, "questions": {"value": "(1) How does certification rate trade off with the RLT percentage when total time is capped? Do you have empirical heuristics for tuning $p$ (e.g., based on instability rate, Lipschitz proxies, or pre-activation interval widths)?\n\n(2) Can you quantify the relaxation gap introduced by the pruning rules (Eqs. 28–31)? Under what conditions does this relaxation change the certificate (i.e., converts a would-be certificate into a failure)? Are these rules implicated in the observed failures?\n\n(3) Extend Fig. 2 with $β$-CROWN-based pruning curves vs. class count to show how complexity and certified accuracy scale. \n\n(4) How does the approach perform on CIFAR-10? Include the same ablations (RLT percentage, pruning on/off, baseline comparators, and $\\epsilon$) for a like-for-like comparison. \n\nOverall, widening the evaluation (architectures + baselines + $ϵ$) and adding the ablations above would substantively strengthen the paper’s empirical support and clarify where the proposed method is most beneficial."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "odaxqLNEvN", "forum": "KYqHcVVR92", "replyto": "KYqHcVVR92", "signatures": ["ICLR.cc/2026/Conference/Submission19017/Reviewer_WUED"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19017/Reviewer_WUED"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761340600345, "cdate": 1761340600345, "tmdate": 1762931062409, "mdate": 1762931062409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "While most neural network verifiers rely on linear relaxations of the networks to be verified, alternatives based on semidefinite relaxations provide tighter bounds, enabling more properties to be verified. However, Semidefinite Programming (SDP)-based verifiers usually need to solve one optimisation problem for each incorrect class, leading to high costs and slow verification. This work proposes an alternative formulation of the neural network verification problem as a quadratic program which verifies the robustness of a neural network across all classes simultaneously. The authors then relax this formulation to an SDP and introduce additional cuts based on McCormick envelopes. They further propose a method for pruning stably active neurons in a verification problem and, since the proposed method would break the chordal decomposition of the SDP, suggest a relaxation of the pruned problem which preserves the chordal structure. The proposed method is evaluated on a number of standard benchmarks and is shown to outperform previous verification approaches in terms of both runtime as well as the number of verified instances."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Neural network verification is an important research topic\n- The requirement to solve a number of separate optimisation problems is a major issue in SDP-based verification and tackling this is an important contribution.\n- The experimental evaluation shows a significantly improved performance compared to the baselines that were evaluated"}, "weaknesses": {"value": "- To me, the biggest weakness of this work is the empirical evaluation. The results look impressive, but the authors use outdated baselines and do not compare against newer verification approaches. The settings used for e.g. $\\beta$-CROWN are also questionable.\n  - The experimental evaluation on $\\beta$-CROWN seems unfair since the verifier is only run for very short time budgets (2-5 seconds) while the authors' proposed method is run for up to ~1600 seconds. Since $\\beta$-CROWN is a complete verifier, a fair comparison would enable its branching and run it for the same time budget as $SDP_u$. The fact that the numbers reported here are not representative is also evidenced by comparing them to e.g. those reported by [6] where the gap between $\\beta$-CROWN and the SDP verifiers is significantly smaller than the gap reported in this work.\n  - $\\beta$-CROWN has been improved by the introduction of general cutting planes generated by MILP solvers in [3] and branch-and-bound-inferred cutting planes in [4]. The method proposed by the authors should be evaluated against these newer works to benchmark how well it actually performs and not against the older $\\beta$-CROWN. The authors conduct their experiments on very small models, I understand that this is standard in SDP-based verification so I do not hold this against them. However, it does seem quite likely that a MILP solver would scale to these sizes and would therefore be able to produce effective cutting planes in GCP-CROWN.\n  - The authors do not compare their approach to newer SDP-based verifiers such as [6, 7, 8, 9], I know that at least for [8] the code has been made available so a comparison would be easy.\n  - Given the small size of the networks which benchmarks are run on, I do wonder whether improved MILP verifiers or hybrid ones such as [2] would perform on these benchmarks\n  - The paper's primary motivation is scaling to \"large multi-class datasets\", which also implies convolutional neural networks (CNNs). However, all experiments are conducted on small, fully-connected networks. The paper would be much stronger if it included at least a preliminary study on a small CNN.\n- The related work section is missing multiple important references\n  - The work cites early MILP verification approaches, but omits multiple later works on MILP-based verification which significantly improve upon the early (naive) approaches such as [1, 2]\n  - All of the state-of-the-art work on incomplete verification is missing, including for example GCP-CROWN [3], BICCOS [4] and Marabou 2.0 [5]\n  - A number of recent works on SDP-based verification are missing [6, 7, 8, 9]\n- The removal of stably active neurons in a neural network verification context has previously been proposed by [10]. The approach proposed by the authors in this paper seems more general than the previously proposed method, but I think the previous method should still be cited. The previous method applies to fewer neurons but would not break the chordal sparsity pattern and would therefore not require the additional relaxation by constraints (28-31), so it would be interesting to see a performance comparison between the two\n- The authors should provide some details on the networks that they train in Section 5.4 in terms of size. Being able to verify a neural network with 67 classes using SDP verification is great, but I wonder what the size and architecture of that network is.\n\n### Minor weaknesses and typos\n- Line 336: The authors say that \"unmodeled quadratic terms appear in Constraint (5)\". It should be made clearer that this is because of the removal of active neurons from the network which leads to new cross-layer dependencies, at the moment this is a bit difficult to understand.\n- Line 108: Incomplete verifiers are derived into a wide variety of approaches --> Incomplete verifiers are **divided** into a wide variety of approaches\n- Line 138: $W_K^j$, the $j^{th}$ row of matrix $W_K^j$ --> $W_K^j$, the $j^{th}$ row of matrix $W_K$\n- Line 210: The triangular constraint (14) introduced in Ehlers (2017) tighten --> The triangular constraint (14) introduced in Ehlers (2017) tighten**s**\n- Line 267: the DNN satisfy Property 2. --> the DNN satisf**ies** Property 2.\n- Line 308: leverage the specific structure certification problem --> leverage the specific structure **of the** certification problem\n- Line 357: where coefficient of $C_{k−1}$ are a linear combination --> where **the** coefficient**s** $C_{k−1}$ are a linear combination\n\n### References\n[1] Botoeva, E., Kouvaros, P., Kronqvist, J., Lomuscio, A. & Misener, R. (2020) Efficient Verification of ReLU-Based Neural Networks via Dependency Analysis. In: Proceedings of the AAAI Conference on Artificial Intelligence. 3 April 2020 pp. 3291–3299. doi:10.1609/aaai.v34i04.5729.\n\n[2] Liao, Y., Genest, B., Meel, K. & Aryaman, S. (2025) Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification. doi:10.48550/arXiv.2507.23197.\n\n[3] Zhang, H., Wang, S., Xu, K., Li, L., Li, B., Jana, S., Hsieh, C.-J. & Kolter, J.Z. (2022) General Cutting Planes for Bound-Propagation-Based Neural Network Verification. doi:10.48550/arXiv.2208.05740.\n\n[4] Zhou, D., Brix, C., Hanasusanto, G.A. & Zhang, H. (2024) Scalable Neural Network Verification with Branch-and-bound Inferred Cutting Planes. Advances in Neural Information Processing Systems. 37, 29324–29353.\n\n[5] Wu, H., Isac, O., Zeljić, A., Tagomori, T., Daggitt, M., Kokke, W., Refaeli, I., Amir, G., Julian, K., Bassan, S., Huang, P., Lahav, O., Wu, M., Zhang, M., Komendantskaya, E., Katz, G. & Barrett, C. (2024) Marabou 2.0: A Versatile Formal Analyzer of Neural Networks. In: Computer Aided Verification: 36th International Conference, CAV 2024, Montreal, QC, Canada, July 24–27, 2024, Proceedings, Part II. 25 July 2024 Berlin, Heidelberg, Springer-Verlag. pp. 249–264. doi:10.1007/978-3-031-65630-9_13.\n\n[6] Lan, J., Brückner, B. & Lomuscio, A. (2023) A Semidefinite Relaxation Based Branch-and-Bound Method for Tight Neural Network Verification. Proceedings of the AAAI Conference on Artificial Intelligence. 37 (12), 14946–14954. doi:10.1609/aaai.v37i12.26745.\n\n[7] Lan, J., Zheng, Y. & Lomuscio, A. (2023) Iteratively Enhanced Semidefinite Relaxations for Efficient Neural Network Verification. Proceedings of the AAAI Conference on Artificial Intelligence. 37 (12), 14937–14945. doi:10.1609/aaai.v37i12.26744.\n\n[8] Chiu, H.-M. & Zhang, R.Y. (2023) Tight Certification of Adversarially Trained Neural Networks via Nonconvex Low-Rank Semidefinite Relaxations. In: Proceedings of the 40th International Conference on Machine Learning. 3 July 2023 PMLR. pp. 5631–5660. https://proceedings.mlr.press/v202/chiu23a.html.\n\n[9] Ueda, R., Sato, T., Kobayashi, K. & Nakata, K. (2025) Interior-Point Vanishing Problem in Semidefinite Relaxations for Neural Network Verification. doi:10.48550/arXiv.2506.10269.\n\n[10] Serra, T., Kumar, A. & Ramalingam, S. (2020) Lossless Compression of Deep Neural Networks. In: E. Hebrard & N. Musliu (eds.). Integration of Constraint Programming, Artificial Intelligence, and Operations Research. 2020 Cham, Springer International Publishing. pp. 417–430. doi:10.1007/978-3-030-58942-4_27."}, "questions": {"value": "- How does the pruning method proposed by the authors compare to that proposed by Serra et al. in [10]?\n- Why is $\\beta$-CROWN only run for 2-5 seconds in the empirical evaluation?\n- How does the method proposed by the authors compare to newer SDP-based as well as other NN verification works?\n- What architecture is used for the self-trained models in Section 5.4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W5n1LyGlA7", "forum": "KYqHcVVR92", "replyto": "KYqHcVVR92", "signatures": ["ICLR.cc/2026/Conference/Submission19017/Reviewer_QWud"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19017/Reviewer_QWud"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761488935129, "cdate": 1761488935129, "tmdate": 1762931061816, "mdate": 1762931061816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper develops an SDP relaxation of neural network verification methods intended to verify a large number of classification/prediction criteria by considering the criteria holistically as a single problem rather than individual problems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tAdvances the state of practice in relaxation-based approaches for NN verification based on SDP relaxations.\n2.\tIncreases the scale of NN’s were SDP relaxations can be used."}, "weaknesses": {"value": "1.\tThe literature review is generally restricted to a single paper from 2022 and the rest being older than that.  There is quite a bit of more recent work, esp. https://arxiv.org/abs/2506.06665, which has very much improved the state of the art of relaxations to SDP for NN verification.  Oddly, this paper appears in the reference list, but is not referred to in the text. So, I am a bit concerned that this and other recent work is not compared against.\n2.\tSome parts of the model are unclear.  For example, it is not clear whether or not the beta (binary) variables are relaxed or not.  From the notation, I think the beta variables remain binary.  However, my understanding is that Mosek does not support mixed integer SDPs.  Either way, I have a couple questions below."}, "questions": {"value": "1.\tAre the binary variables relaxed or binary in the final relaxation?\na.\tIf relaxed, I would expect the relaxation to be weaker than the enumeration of all classes using SDP with the cuts that are specific to each class.  (because the optimization can assign fractional values to beta).  Is this the case? Do you have CPU and solution quality comparisons between your model enumerated per class vs. your one large model?  I am not completely sure if SDP_t is exactly this or not.\nb.\tIf not relaxed, is the main value of the combined model that it can take advantage of the branching strategies and pruning capabilities of the mixed integer solver, and not have to do a complete enumeration of all classes (e.g., the search tree is essentially one layer below the root node, with each leaf essentially corresponding to one verification class and many leaves pruned and never evaluated)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "52FKwF9Ile", "forum": "KYqHcVVR92", "replyto": "KYqHcVVR92", "signatures": ["ICLR.cc/2026/Conference/Submission19017/Reviewer_tKZa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19017/Reviewer_tKZa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867317339, "cdate": 1761867317339, "tmdate": 1762931061422, "mdate": 1762931061422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}