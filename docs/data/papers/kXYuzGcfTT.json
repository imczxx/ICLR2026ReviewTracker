{"id": "kXYuzGcfTT", "number": 7951, "cdate": 1758045548348, "mdate": 1759897820138, "content": {"title": "Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs", "abstract": "Recently, techniques such as explicit structured reasoning have demonstrated strong test-time scaling behavior by enforcing a separation between the model’s internal \"thinking\" process and the final response. A key factor influencing answer quality in this setting is the length of the thinking stage. When the reasoning is too short, the model may fail to capture the complexity of the task. Conversely, when it is too long, the model may overthink, leading to unnecessary computation and degraded performance. This paper explores and exploits the underlying mechanisms by which LLMs understand and regulate the length of their reasoning during explicit thought processes. First, we show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization, which is then used to reveal insights on the model's planning dynamics. Second, we manipulate the internal progress encoding during inference to reduce unnecessary steps and generate a more concise and decisive chain of thoughts. Our empirical results demonstrate that this ``overclocking'' method mitigates overthinking, improves answer accuracy, and reduces inference latency. Our code is attached as supplementary material.", "tldr": "We show that LLMs like DeepSeek-R1 can track and control their thinking phase via “progress vectors,” enabling faster inference, better answers, and greater transparency through an interactive reasoning bar.", "keywords": ["Test-Time Scaling", "Thinking Path Length", "Reasoning Control", "Overthinking Mitigation"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c61d8989185b45575c13838e6c65e648b28dc322.pdf", "supplementary_material": "/attachment/7fbded63db5cf51949fc92f875a0144ca03a5360.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposed learning from LLM's last layer hidden states to predict the progression of the thinking process between <think> and </think>. The progression is defined as the relative token position of the tokens in the thinking segment.\n\nThe paper proposed to learn a predictor parameterized by a single vector, a 2-layer MLP, and a single-layer GRU. Experiment results show the predictor has reasonable performance of predicting the relative position.\n\nThe paper proposed to intervene the last-layer hidden states utilizing the learned single vector, \"thinking progress vector\", to \"overclock\" the thinking process, i.e. by learning such a vector it decouples the \"progress\" concept in the hidden state vectors, and tries to manipulate it to speed up the progress. Experimental results are reported on Math500 and GSM8K using DeepSeek-R1-LLaMA-8B and DeepSeek-R1-Qwen-32B."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper shows potential to isolate the \"progress\" concept in the hidden states of LLMs and the potential possibility to speed up or slow down the thinking process of LLMs by manipulating the hidden states using the \"thinking progress vector\"."}, "weaknesses": {"value": "1. The \"progress\" defined in this paper, the relative position of tokens in generation, does not capture the semantic progression of solving a problem. The task of learning the relative position is essentially just learning to predict to total length of the generation from the hidden states, as the position relative to the start of the generation is an easy task since information is embedded in the positional encoding.\n\n2. The results reported for both models, DeepSeek-R1-LLaMA-8B and DeepSeek-R1-Qwen-32B cannot match the officially reported accuracies on Math500, it's much lower. For 2048 length limit, reported 159/500=31.8% for 8B and 316/500=63.2% for 32B are much lower than official reports 89.1% and 94.3%. This should originate from the fact that the paper is limiting the generation length to only 2048 tokens. The reviewer thinks this is an unreasonable setting since the tested models (DeepSeek R1 distilled models) inherently generate long thinking contents before giving the answer, limiting the generation to only 2048 tokens is not testing the model properly, and the conclusions drawn from this setting are not informative. If for some scenarios where a 2048 token limit is a must, then some other models that generate shorter solutions should be used, e.g. Llama-3.1 or Qwen-2.5, not the R1 distilled version."}, "questions": {"value": "To better present the potential benefit of the TPV intervention, I suggest the authors add another setting, comparing: 1) let the model generate until it end by itself; 2) let the model generate until it end by itself, and intervene the generation of every token using the proposed TPV.\n\nThen compare a) the accuracy and b) the averaged output length of the two methods. If the accuracy of the intervened generations are maintained, and the averaged output length is shorter, it would be clear evidence that the proposed intervention is useful in improving the efficiency of the reasoning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lkadgTG7UQ", "forum": "kXYuzGcfTT", "replyto": "kXYuzGcfTT", "signatures": ["ICLR.cc/2026/Conference/Submission7951/Reviewer_DLQv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7951/Reviewer_DLQv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761189083490, "cdate": 1761189083490, "tmdate": 1762919969345, "mdate": 1762919969345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the mechanisms by which Large Language Models (LLMs) regulate the length of their explicit reasoning phase, specifically in models that use structured \"thinking\" tokens. The authors claim that LLMs internally encode their relative progress within the thinking phase. They train a regressor, termed a \"Thinking Progress Vector\" (TPV), to extract this progress (a scalar value from 0 to 1) from the model's hidden states, which can be visualized as a \"progress bar.\" Then they introduce an intervention method called \"overclocking,\" which manipulates the model's hidden states by adding a scaled version of this TPV. This intervention is shown to accelerate the reasoning process, reduce the number of generated tokens, and mitigate \"overthinking.\""}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The central idea of identifying an internal representation of \"progress\" and visualizing it as a progress bar is novel and presents an interesting direction for LLM interpretability and human-computer interaction. \n2. The authors provide a thorough set of experiments on mathematical reasoning benchmarks, providing a solid empirical grounding for the paper's claims within its chosen domain."}, "weaknesses": {"value": "1. Insufficient Evidence for \"Monitoring\" vs. \"Pattern-Matching\": The paper's primary claim that the TPV captures the model's monitoring of its own reasoning process, is not sufficiently substantiated. The TPV is trained as a simple regressor mapping hidden states to a normalized token position ($j/N_k$). It is highly probable that this regressor is not learning a high-level, semantic representation of \"reasoning progress\" but is instead capturing superficial, correlational text patterns. For example, certain phrases (\"Let me double-check,\" \"Wait,\" \"The formula is...\") naturally correlate with the beginning, middle, or end of a thinking sequence. The paper's own analysis in Appendix E, which shows that words like \"okay\" and \"right\" strongly and positively impact the progress value, seems to support this \"text pattern\" hypothesis far more than a \"cognitive monitoring\" one. \n\n2. Fair Results of the \"Overclocking\" Intervention: Given the concern above, the success of the \"overclocking\" intervention feels tautological and less surprising. If the TPV $\\theta$ is simply a vector that correlates strongly with the end of a sequence, then applying an intervention $h_{\\alpha} = h + \\alpha\\theta$ is, by definition, pushing the model's hidden state closer to a representation that is associated with sequence termination (such as `<eos>` token). It is therefore an expected outcome that this intervention shortens the output. This result does not prove that the model causally uses this vector for progress control, it only proves that the learned regressor has identified a manipulable direction in the activation space that correlates with sequence length. \n\n3. Questionable Generalizability: The empirical evaluation is confined to two mathematical reasoning datasets (MATH-500 and GSM8K). This domain is highly structured and often procedural. It is highly questionable whether these findings would generalize to other, more common LLM tasks. For instance, how would a \"progress vector\" function in open-ended creative writing, complex document summarization, or dialectical tasks where the concept of \"progress\" is ambiguous, non-linear, or recursive? The paper provides no evidence or discussion on this crucial aspect of generalizability. \n\n4. Ambiguous Practical and Analytical Significance: From an interpretability perspective, the work does not provide strong evidence for the high-level claims of \"metacognition\" or \"self-monitoring.\" It primarily demonstrates that text generation in a structured task has strong correlational patterns. From an engineering perspective, the method's value is not clearly demarcated from simpler alternatives. The \"Instruct\" baseline (a prompting technique) also performs well, and the best results are achieved when combining TPV with this prompting. This makes it unclear if this complex, model-specific intervention is practically superior to more robust methods like prompt engineering or fine-tuning with a length-based reward. \n\n5. Lack of Experimental Clarity: Key details of the methodology are ambiguous or omitted. The paper states that 30 problems were sampled, with 5 responses each, to train the regressor. This implies a training set of only ~150 trajectories, which seems exceptionally small for training even a linear regressor, let alone an RNN. The authors must clarify the exact dataset size (number of trajectories $K$ and total data points $(h, p)$). It is also not explicitly stated whether the linear TPV or the RNN model was used for the main intervention experiments in Section 4."}, "questions": {"value": "1. The non-monotonic drops in progress (Fig. 5c, Fig. 9) are attributed to \"hesitation or reflection.\" An alternative, and simpler, hypothesis is that the model has simply generated a token (e.g., \"Wait\") that the TPV regressor strongly associates with an earlier phase of problem-solving (a learned text pattern). How can the authors more rigorously disentangle this \"text pattern\" hypothesis from their \"cognitive monitoring\" hypothesis? \n\n2. The raw TPV predictions are clearly non-monotonic. The paper mentions using exponential smoothing and an RNN to create a smoother progress bar. How is the final progress bar visualization (as in Fig. 1a) rendered in real-time? Is it based on the smoothed or RNN-based output? \n\n3. Could the authors clarify the intervention mechanism? Is the intervention $h_{\\alpha} = h + \\alpha\\theta$ applied at every token generation step within the `<think>...</think>` block? Furthermore, to understand the sensitivity of $\\alpha$, what is the typical scale of the TPV's squared norm ($||\\theta||^2$)? \n\n4. Could the authors comment on the hypothesized applicability and feasibility of this method for non-procedural, open-ended reasoning tasks, such as summarization or creative writing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b1pBCH1edS", "forum": "kXYuzGcfTT", "replyto": "kXYuzGcfTT", "signatures": ["ICLR.cc/2026/Conference/Submission7951/Reviewer_VS5W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7951/Reviewer_VS5W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644847622, "cdate": 1761644847622, "tmdate": 1762919968980, "mdate": 1762919968980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an interesting study towards the internal evidence of the relative position signals within the explicit thinking phase. To achieve this purpose, the authors aim to learn a 'progress vector' that can project the hidden states into the estimated progress. Then, they further examine the possiblity of overclock and downclock the reasoning process. The experimental results well demonstrate the effectiveness of the notion 'progress vector' and progress intervention made via the former."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of identifying the intrinsic mechanisms that encode a model’s relative position within its internal reasoning process is very interesting and useful.\n2. The proposal of learning a 'progress vector' is simple yet effective.\n3. The extensive expeirments well demonstrate the effectiveness of the proposed solution."}, "weaknesses": {"value": "1. The authors only choose mathematical reasoning task for study. It is only a specific subarea of LLM reasoning. More reasoning tasks from other domains should be investigated."}, "questions": {"value": "See the above Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GIQ9M6Mwox", "forum": "kXYuzGcfTT", "replyto": "kXYuzGcfTT", "signatures": ["ICLR.cc/2026/Conference/Submission7951/Reviewer_pPTH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7951/Reviewer_pPTH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064602950, "cdate": 1762064602950, "tmdate": 1762919968640, "mdate": 1762919968640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes \"thinking progress vector\" (TPV), a vector representation of what is claimed the encoding of its \"interval reasoning process\".\nThe paper uses linear models, MLPs, and RNN as tools to monitor the change of this state along the reasoning trajectory of LLMs.\nThe paper claims that intervening this vector can change the behaviour of LLMs and make it reason \"faster\", or what is called \"overclocking\" by the paper.\nEmpirical results on math reasoning tasks shows some evidence about the effect of intervening TPVs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper has some intuitive figures showing the discovered \"thinking progress vector\"."}, "weaknesses": {"value": "- The idea of estimating the progress of thinking as proposed in this paper does not make sense. It is beyond me why an LLM can maintain a state representing its reasoning progress if it does not know the correct answer to the problem even when the reasoning is just started. If an estimation of this really existed, then we would have asked this LLM to solve the halting problem, which has been mathematically proven to be impossible.\n\n- The TPV might represent some characteristic tokens marking the steps in solutions to math problems. The paper unfortunately does not provide enough study along this direction. In lines 261-263, even the paper itself writes \"a crucial question that arises is whether TPVs reflect a fundamental mechanism that the model uses to track its reasoning progress, or if they are merely residual artifacts that correlate with progress but do not play a causal role in the computation.\" this possibility is never ruled out in the rest of the paper.\n\n- The concept of reasoning progress itself is poorly defined for real-world problems. What does reasoning progress mean for writing some code for a specific task? Does reasoning ends when the code is completed? Or When LLM finishes running it in with a python interpreter? What does reasoning progress mean for the travel sales person (TSP) problem, when it is impossible to solve in polynomial time?"}, "questions": {"value": "I do not have questions for this paper. I have not checked all the experiments of the paper, so I will put a 2 for my confidence. \n\nI request that the paper be framed more accurately and align with the fundamentals of computer science in terms of the message it tries to convey."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NHrtvOw1in", "forum": "kXYuzGcfTT", "replyto": "kXYuzGcfTT", "signatures": ["ICLR.cc/2026/Conference/Submission7951/Reviewer_v9dA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7951/Reviewer_v9dA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762100221717, "cdate": 1762100221717, "tmdate": 1762919967706, "mdate": 1762919967706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}