{"id": "Z6uyHEhfue", "number": 18120, "cdate": 1758284094900, "mdate": 1759897132070, "content": {"title": "Semantic Routing in Pretrained Vision Models for Online Domain-Incremental Learning", "abstract": "Learning in the real world requires models to evolve with changing environments and cope with diverse forms of distribution shift. This is especially challenging in online domain-incremental learning, where data arrive as a non-stationary stream, each sample can be seen only once, and past observations cannot be revisited.  Although pre-trained models can be used to obtain strong initial representations, standard fine-tuning in this setting leads to forgetting and poor cross-domain generalization. Inspired by how the human brain organizes experiences around semantic concepts, we propose Semantic Adapters (SAD)—lightweight modules plugged on top of any frozen pre-trained vision encoder that leverage structured semantic knowledge to guide representation updates. By routing the updates toward semantic clusters rather than domains, SAD stabilizes learning while enabling fast, one-pass adaptation. To further enrich flexibility, we introduce SADLoRA, which augments heads with low-rank parameter updates within the encoder, further enhancing adaptability while maintaining efficiency. Extensive experiments across diverse domain shifts show that both SAD versions substantially reduces forgetting and accelerates adaptation. The proposed Semantic routing with targeted updation offers a simple, fast, scalable and a viable solution for robust continual adaptation in dynamic real-world scenarios.", "tldr": "Equipping frozen pre-trained vision encoders with semantic adapter heads to boost generalization and adaptation in online domain-incremental learning.", "keywords": ["Generalization", "Continual learning", "domain-incremental learning", "pretrained models", "catastrophic forgetting"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4a822dd571441c2d034f23460b4976869bc3f0f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of online domain-incremental learning, where models must adapt continuously to changing environments without revisiting past data. The authors propose Semantic Adapters (SAD), a lightweight modules built on top of a frozen pre-trained vision encoder that guide updates based on semantic clusters instead of domain boundaries. They further introduce SAD-LoRA, adding low-rank adaptation to enhance flexibility. Experiments show that both methods effectively reduce forgetting, improve adaptation speed, and outperform several state-of-the-art baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is concise and presents its motivation and method in an accessible way.\n\n2. The proposed methods achieve some strong empirical performance, surpassing some SOTA baselines."}, "weaknesses": {"value": "1. The proposed approach largely follows a well-established paradigm in continual learning: training separate lightweight modules for different data subsets and selecting parameters during inference based on the distance between an incoming sample and prototypes. This framework has already been widely adopted in numerous prior works such as L2P, DualPrompt, and their many successors. The paper appears to only make minor procedural adjustments (e.g., limiting training to a single epoch for \"online learning\"), without introducing fundamentally new insights or theoretical understanding. These modifications alone are insufficient to support a strong claim of novelty.\n\n2. The introduction of SAD-LoRA lacks clear motivation and conceptual connection to the main idea of semantic routing. It seems primarily to introduce additional trainable parameters within the encoder, which may inflate model capacity rather than reflect a meaningful methodological advancement. As a result, the performance gains might stem from increased parameters rather than the proposed semantic framework, making the comparison somewhat unfair.\n\n3. The schematic figures are difficult to interpret and lack adequate explanatory detailsin image captions. As currently presented, they do not effectively convey the overall idea or workflow of the proposed method, which hampers reader understanding.\n\n4. The SAD-Oracle baseline is problematic. It provides an unfair upper bound. Including it directly in the main result tables, especially near the primary baselines, can mislead readers about the practical effectiveness of the proposed method.\n\n5. The paper does not include any ablation studies or hyperparameter sensitivity analyses, leaving unclear which components actually contribute to performance improvements. Furthermore, although the work claims to address online learning, there is no evaluation or discussion of runtime efficiency or real-time adaptability, which are essential to support this claim."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mGt6oTu6Em", "forum": "Z6uyHEhfue", "replyto": "Z6uyHEhfue", "signatures": ["ICLR.cc/2026/Conference/Submission18120/Reviewer_mE4J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18120/Reviewer_mE4J"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761386662694, "cdate": 1761386662694, "tmdate": 1762927886660, "mdate": 1762927886660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of online domain-incremental learning (single-pass data access, no replay, and the need to adapt to distribution shifts) by proposing two lightweight semantics-based methods. The first is Semantic Adapters (SAD), which freezes pre-trained vision encoders, forms semantic clusters by clustering classes via language models, and assigns dedicated classifier heads to each cluster to enable cross-domain knowledge reuse. The second is SAD-LoRA, which inserts cluster-specific low-rank adapters into the final layers of the encoder to enhance adaptability. On datasets including DN4IL, Office-Home, and CORe50, both methods outperform baselines such as oEWC and ER-ACE."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Only lightweight modules are trained (with 1/30 the parameters of full fine-tuning), supporting both CNN and Transformer architectures."}, "weaknesses": {"value": "1. Insufficient Analysis of Semantic Cluster Design: It fails to verify the impact of cluster number (K) on performance or compare the effectiveness of different clustering methods/embedding sources, leaving the optimality of cluster design unsupported.\n\n2. Lack of Ablation on Loss Functions: It does not decompose the individual contributions of cross-entropy, language anchoring, and separation losses, nor does it justify the use of fixed loss weights (λ_lang=0.5, λ_sep=0.1)."}, "questions": {"value": "see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "bhDyDkFllq", "forum": "Z6uyHEhfue", "replyto": "Z6uyHEhfue", "signatures": ["ICLR.cc/2026/Conference/Submission18120/Reviewer_1HZn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18120/Reviewer_1HZn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491837087, "cdate": 1761491837087, "tmdate": 1762927886306, "mdate": 1762927886306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed method, Semantic Adapters (SAD), addresses the challenges of catastrophic forgetting and poor cross-domain generalization in online domain-incremental learning. SAD replaces traditional domain-specific adaptation with meaning-centric updates. It operates by first using a pre-trained language model (PLM) to encode and cluster class labels into a small set of coherent semantic groups, each representing a concept. For each of these semantic groups, a lightweight classifier head (SAD head) is instantiated. During training, the visual encoder remains frozen, and inputs are dynamically routed to the appropriate SAD head based on their semantic meaning, allowing learning to concentrate within concept-consistent subspaces. This design stabilizes learning, reduces interference between domains, and promotes knowledge reuse, enabling fast, one-pass adaptation.\n\nTo further enhance adaptability, the paper introduces SAD-LORA, an extension of SAD. SAD-LORA augments the core SAD framework by incorporating low-rank adapters (LoRA) into the final blocks of the frozen pre-trained vision encoder. This combination of semantic routing with targeted low-rank plasticity allows the model to recondition features more effectively to handle significant domain shifts and corruptions, such as blur or fog, while preserving stable, reusable representations. Both SAD and SAD-LORA achieve competitive or superior performance compared to existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Semantic routing effectively mitigates catastrophic forgetting by organizing learning around stable semantic concepts rather than isolated domains, preventing interference between tasks.\n\nThe method is computationally efficient, training only lightweight semantic adapter heads and optional low-rank adapters, leading to significantly fewer trainable parameters and faster online adaptation."}, "weaknesses": {"value": "- Lack of Core Technical Novelty: The proposed SAD method primarily combines well-established techniques such as pre-trained vision encoders, language model embeddings for semantic grouping, and lightweight adapter heads. It doesn't introduce fundamentally new algorithms or architectural innovations.\n\n\n- SADLORA appears to be a re-branding of the standard LoRA (Low-Rank Adaptation) technique applied within their framework, rather than proposing a novel variant or significant extension of LoRA itself.\n\n- The paper compares its online single-pass method (SAD) against several baselines that are allowed multiple training epochs (20) per domain or rely on replay buffers (e.g., MEMO, L2P, DualPrompt, SimpleCIL), which could provide them an unfair advantage in stability and performance.\n\n- The method heavily leverages a CLIP text encoder for semantic grouping and language anchoring. However, it lacks direct comparisons against other recent domain-incremental learning methods that also specifically utilize CLIP's powerful cross-modal representations.\n\n-  The paper states different LoRA placement strategies for ResNet-18 (layer4 convolutions) and ViT-B/16 (last transformer block's self-attention and MLP projections) without thoroughly justifying why these specific locations were chosen or presenting ablation studies to demonstrate their optimality."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GLp8DNyysv", "forum": "Z6uyHEhfue", "replyto": "Z6uyHEhfue", "signatures": ["ICLR.cc/2026/Conference/Submission18120/Reviewer_3Yy5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18120/Reviewer_3Yy5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813540434, "cdate": 1761813540434, "tmdate": 1762927885893, "mdate": 1762927885893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an approach for online domain incremental learning by leveraging label space information with a light-weight language model. The class labels are first clustered into semantic similar groups, and the inputs are routed to their corresponding classification systems. The proposed approach also maps the image representation to the language space, and uses LoRA in encoder blocks to increase model capacity.  Empirical results on DN4IL, OfficeHome and CORe50 indicate the proposed approach improves both final and anytime accuracy compared with existing approaches (MEMO, L2P, DualPrompt, SimpleCIL)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. I liked the idea of using language information to aid domain incremental learning, where the language representation acts as a regularizer to prevent domain overfitting.\n2. The proposed approach outperforms existing approaches on DN4IL, OfficeHome and CORe50 benchmarks."}, "weaknesses": {"value": "1. I think the paper is difficult to understand and lacks a coherent and convincing story. Many important details, such as how Dynamic Routing works exactly at line 301 are under-explained. The heatmap in figure 1 is hard to interpret and the analysis related to Fig. 2 feels superficial.\n\n2. The impact of semantic grouping is unclear.  The paper does not provide convincing evidence or ablations demonstrating the need for semantic grouping and the potential impact of incorrect routing. It would be helpful to understand results with a single cluster and with the number of clusters set equal to the number of class labels."}, "questions": {"value": "1. How dynamic routing works exactly?\n2. What is the impact of semantic grouping? What if  there is only a single cluster or when the cluster number is set to the number of class labels.\n3. What happens if an example is mis-routed and how are such cases corrected during training?\n4. What does the heatmap in Fig 1 represent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0RFPPs5aPt", "forum": "Z6uyHEhfue", "replyto": "Z6uyHEhfue", "signatures": ["ICLR.cc/2026/Conference/Submission18120/Reviewer_U5ug"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18120/Reviewer_U5ug"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965855227, "cdate": 1761965855227, "tmdate": 1762927885257, "mdate": 1762927885257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Semantic Adapters (SAD), a parameter-efficient framework for online Domain-Incremental Learning (Domain-IL), where data arrives in a non-stationary, single-pass stream."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1-Parameter efficiency and fast speed.\n\n2-Clear motivation."}, "weaknesses": {"value": "[Major] 1-Incomplete paper submission: Captions of figures 1 and 2 are incomplete. The authors write long paragraphs for motivation, which has already been verified.\n\n[Major] 2-The motivation is old: Finetuning all tasks with a unified head is not good, task-specific head is much better.\n\n[Major] 3-Unsubstantiated Dynamic Routing: The proposed dynamic routing mechanism, which constitutes the paper’s main practical contribution, lacks sufficient justification and empirical support.\n\n4-Questionable Value of SAD-LORA: The SAD-LORA variant dramatically increases the trainable parameters.\n\n5-Missing Ablation on Language Alignment: The ablation of the Language Anchoring loss is missed."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HSnx0o4Qja", "forum": "Z6uyHEhfue", "replyto": "Z6uyHEhfue", "signatures": ["ICLR.cc/2026/Conference/Submission18120/Reviewer_F168"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18120/Reviewer_F168"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995527255, "cdate": 1761995527255, "tmdate": 1762927884624, "mdate": 1762927884624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}