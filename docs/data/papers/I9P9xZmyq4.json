{"id": "I9P9xZmyq4", "number": 13793, "cdate": 1758222720979, "mdate": 1763104915439, "content": {"title": "DCRM-ViT: Domain Conditioned Residual Modulation for Multi-Domain Vision Transformers", "abstract": "Medical imaging presents significant challenges due to acoustic shadows, motion blur, and indistinct boundaries. Addressing these issues is crucial for improving diagnostic accuracy. Many conventional vision models require extensive fine-tuning on task-specific data and often lose generalizability to natural-image domains. We propose DCRM-ViT, a domain-conditioned residual modulation framework for Vision Transformers that preserves general-vision capability while adapting to diverse domains. DCRM-ViT keeps the backbone frozen and augments each block with a lightweight Residual Modulation Block (RMB) whose parameters are synthesized per sample by a Domain Router (DR.) and Parameter Synthesizer Network (PSN). The router outputs soft domain weights from input features, whereas the synthesizer maps these weights to low-rank residuals that modulate selected projections and, optionally, add a domain-aware bias to attention. Crucially, we learn routing and modulation via a bi-level optimization scheme: a short inner loop adapts RMB parameters to task supervision, while an outer loop updates DR., PSN, and RMB initializations/step sizes so the synthesized residuals generalize across medical and natural domains. Across fine-grained classification (Food101, SUN397, Stanford Cars) and medical segmentation (ultrasound, CT, MRI), DCRM-ViT improves over strong baselines while using modest trainable compute. The ablation studies confirmed the benefits of our architectural enhancements, showing improved performance and adaptability. The results demonstrate DCRM-ViT's potential to offer high diagnostic performance with reduced computational overhead of using 0.3 training min/epoch. Our code will be publicly available upon acceptance.", "tldr": "DCRM-ViT is a lightweight add-on to frozen Vision Transformers that uses domain-conditioned residual modulation and bi-level optimization to adapt to medical artifacts while preserving general image performance with minimal compute", "keywords": ["Domain Adaptation", "Vision Transformers", "Meta Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ec97ef1b11a9a7ba09d3dcd05db2b51511c40b3d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DCRM-ViT, a domain-conditioned residual modulation framework designed to adapt vision transformers efficiently across different domains. It includes lightweight residual modulation blocks (RMB), whose parameters are generated by a domain router (DR.) and parameter synthesizer network (PSN). DCRM-ViT is optimized through a bi-level scheme: a short inner loop to update RMB for task-specific learning and an outer loop to update DR., PSN, and RMB's initializations/step size for domain-specific learning. \n\nThe proposed model is evaluated on classification and segmentation datasets, and compared against prior foundation models and parameter-efficient fine-tuning (PEFT) methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* S1: This paper explores multi-domain vision transformers and PEFT, which is of interest to the community. \n\n* S2: The integration of domain router with learnable task-specific modules, RMB, isreasonable. \n\n* S3: The proposed approach shows improved classification and segmentation performance compared with baseline methods presented in the paper."}, "weaknesses": {"value": "**Major Weakness**\n\n* W1: The experimental evaluation is limited. DCRM-ViT is designed for multi-domain adaptation, but it is not compared with multi-domain or multi-task methods, such as domain-specific normalization, TAPS [R1], and AdaMV-MoE[R2]. \n\n* W2: Comparisons are restricted to 3 a little older PEFT methods, which weakens the empirical support for the claimed advantages. Comparing with newer baselines would make the paper stronger.  \n\n* W3: The experimental settings of baselines are unclear. Are they trained jointly on multiple domains/tasks jointly like DCRM-ViT or separately per domain? Without this information, fairness and validity of comparisons are uncertain.  In addition, presenting linear-probing results of foundation models would help clarify whether improvements come from better representation learning or overfitting to small datasets.\n\n* W4: Paper writing and methodological presentation need to be improved.  Currently it is hard to distinguish the authors' methodology contributions from module-by-module explanations in the method section. \n\n[R1] Wallingford, Matthew, et al. \"Task adaptive parameter sharing for multi-task learning.\" CVPR 2022.\n\n[R2] Chen, Tianlong, et al. \"Adamv-moe: Adaptive multi-task vision mixture-of-experts.\" ICCV 2023."}, "questions": {"value": "**Primary  Questions/Suggestions**\n\n* QS1: Could the authors include comparisons with SOTA multi-domain and PEFT approaches, as well as linear-probing results, to better validate the effectiveness of DCRM-ViT?\n\n* QS2: Could the authors clarify the training setups for baselines?\n\n* QS3: Consider refining the writing for clarity, particularly in the methodology section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Un0LkXGve6", "forum": "I9P9xZmyq4", "replyto": "I9P9xZmyq4", "signatures": ["ICLR.cc/2026/Conference/Submission13793/Reviewer_gei2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13793/Reviewer_gei2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761061086764, "cdate": 1761061086764, "tmdate": 1762924326192, "mdate": 1762924326192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "aHySypiRyV", "forum": "I9P9xZmyq4", "replyto": "I9P9xZmyq4", "signatures": ["ICLR.cc/2026/Conference/Submission13793/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13793/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763104914165, "cdate": 1763104914165, "tmdate": 1763104914165, "mdate": 1763104914165, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work provides a novel architecture for medical AI domains. The problem the authors focus on is poor performance of models finetuned on medical data when tested on natural image domains. To address the issues of catastrophic forgetting and non-interpretable weight changes, they suggest an architecture that learns domain weights from input features and uses synthesizers to map these weights to residuals and attention biases that allow the architecture to meta-learn strategies for generalizing across domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Complex architecture, so great to have Figure 3 highlighting all the ingredients.\n\nEvaluation against prior work is thorough and performance boost gained by this method is clearly shown, with error bars.\n\nThorough ablations of each piece of the proposed architecture."}, "weaknesses": {"value": "Motivation: Would like to see more explanation of why the cross-domain transfer you are targeting is from a model finetuned on a medical modality to natural images? In what practical applications do we need our medical models to still perform well on natural images? Are we assuming we have a generalist medical AI model that we want to be adaptive to other domains? Is the focus on natural images a proxy for generalization to perhaps other medical imagining modalities? If so, why are natural image dataset results a fair substitution? Why not do cross-domain generalization tests on other medical modalities? \n\nFor table 4, why are cross-domain results not compared against all related works, as previous tables do?"}, "questions": {"value": "See previous sections. Open to changing my score based on responses to these questions!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2PmpZISDXw", "forum": "I9P9xZmyq4", "replyto": "I9P9xZmyq4", "signatures": ["ICLR.cc/2026/Conference/Submission13793/Reviewer_eQvq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13793/Reviewer_eQvq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928471951, "cdate": 1761928471951, "tmdate": 1762924325397, "mdate": 1762924325397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DCRM-ViT, a Vision Transformer framework designed to handle both medical imaging (ultrasound, CT, MRI) and natural images within a single model. The key contribution is a domain-conditioned residual modulation approach that keeps the backbone ViT frozen while adding lightweight Residual Modulation Blocks (RMB) whose parameters are dynamically generated per-sample by a Domain Router (DR) and Parameter Synthesizer Network (PSN). The training employs bi-level optimization: an inner loop adapts RMB parameters for task-specific learning, while an outer loop meta-learns the domain routing and parameter synthesis components. The method shows improvements of 1-3 percentage points over baselines across medical and natural image datasets while using only 3.3M trainable parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Comprehensive evaluation**: Thorough experiments across multiple medical modalities (ultrasound, CT, MRI) and natural image datasets\n2. **Parameter efficiency**: Achieves reasonable performance with only 3.3M trainable parameters vs. full fine-tuning\n3. **Practical relevance**: Addresses a real clinical need for models that work across medical and natural domains\n4. **Cross-domain transfer**: Demonstrates maintained performance when switching between domains\n5. **Ablation studies**: Provides systematic ablations of architectural components"}, "weaknesses": {"value": "1. **Limited technical novelty**: The paper combines existing techniques without significant innovation. Domain adaptation through adapters, parameter generation networks, and bi-level optimization have all been explored previously.\n\n2. **Oversimplified domain modeling**: The binary medical/natural domain distinction is too coarse. Medical imaging encompasses diverse modalities with vastly different characteristics that the current approach doesn't address.\n\n3. **Questionable zero-shot evaluation**: The transformation approach (Eqs. 5-9) to enable \"zero-shot\" capabilities for non-CLIP models involves training on CLIP embeddings, which isn't true zero-shot learning.\n\n4. **Insufficient analysis**: The paper lacks visualization of learned domain weights, analysis of failure cases, or interpretation of what the domain router actually learns.\n\n5. **Missing computational analysis**: While parameter counts are reported, actual training/inference time overhead of the bi-level optimization and dynamic parameter generation is unclear.\n\n6. **Statistical rigor**: No significance testing despite small margins; confidence intervals in tables appear to be standard deviations rather than proper confidence intervals."}, "questions": {"value": "1. How does the domain router handle ambiguous images with mixed domain characteristics? Can you provide examples and domain weight visualizations?\n\n2. What is the actual wall-clock inference time overhead compared to the frozen backbone? The dynamic parameter generation seems computationally expensive.\n\n3. Can this approach scale beyond binary domain classification? How would it handle multiple fine-grained medical subdomains?\n\n4. The bi-level optimization seems complex - how sensitive is it to hyperparameter choices? What are the convergence properties?\n\n5. Could you clarify the zero-shot transformation methodology? Training on CLIP embeddings seems to invalidate the zero-shot comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MrryLflbDD", "forum": "I9P9xZmyq4", "replyto": "I9P9xZmyq4", "signatures": ["ICLR.cc/2026/Conference/Submission13793/Reviewer_7rsA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13793/Reviewer_7rsA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967561561, "cdate": 1761967561561, "tmdate": 1762924324960, "mdate": 1762924324960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DCRM-ViT, a model that preserves general capability of pre-trained vision foundation models while adapting to new medical domain data. Specifically, DCRM-ViT keeps the backbone frozen and augments each block with synthesized parameters. A meta-learning framework is adopted for optimization. Experiments on both natural image and medical image data show superior performances compared to vanilla vision foundation models and lightweight adaptation method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed DCRM-ViT method achieves strong performances across diverse datasets with only a small number of trainable parameters and small compute cost. The idea of keeping base model frozen while introducing a small number of extra parameters has been well studied in the LoRA series of work, and is expected to work when adapting to new domains, but generating these parameters on-the-fly is quite novel and effective."}, "weaknesses": {"value": "1. Overall, I get that the authors introduced different modules like Residual Modulation Block (RMB), Domain Router (DR) and Parameter Synthesizer Network (PSN) to adapt the model to new medical domain data, but I fail to grasp the high-level idea and intuition behind those designs by reading the paper. Right now they just look a bunch of new modules inserted into existing models and magically work.\n\n2. The evaluation on natural image datasets is limited. The authors are encouraged to check https://github.com/LAION-AI/CLIP_benchmark for a complete set of benchmarks. Evaluation on only CIFAR-10 and Caltech101 only covers a small number of natural image distribution.\n\n3. The drawing of figure 3 is particularly messy and confusing. I highly suggest that the authors rearrange the legend part, as now it is hard to draw correspondence between the legends and the flow chart."}, "questions": {"value": "1. How does DCRM-ViT benefit from the MAML-style meta learning? The authors claim that \"Without this nested scheme, the PSN unit would receive conflicting gradient signals\" (line 310), yet I cannot find any related experimental proof. Also, why does training on all domains and tasks lead to inferior performance? The authors are encouraged to give an intuitive explanation.\n\n2. Similarly, I do not see any ablation study about the design choice. For instance, why is generating parameters on-the-fly better? Why use a soft-parameter sharing mechanism. And there are many more. The authors are encouraged to give both intuitive explanation as well as experimental results on this matter.\n\n3. What is the base foundation model for DCRM-ViT? I cannot find any related details, though I suppose it is likely to be a CLIP model variant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nHYFtW6tXm", "forum": "I9P9xZmyq4", "replyto": "I9P9xZmyq4", "signatures": ["ICLR.cc/2026/Conference/Submission13793/Reviewer_ySsg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13793/Reviewer_ySsg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762506443299, "cdate": 1762506443299, "tmdate": 1762924324313, "mdate": 1762924324313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}