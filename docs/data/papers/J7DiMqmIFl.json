{"id": "J7DiMqmIFl", "number": 16576, "cdate": 1758266269229, "mdate": 1759897231799, "content": {"title": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration", "abstract": "We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene reconstruction that enhances the CUT3R model by integrating prior information. Unlike existing feed-forward methods that rely solely on input images, our method leverages auxiliary data, such as depth, camera calibrations, or camera positions, commonly available in real-world scenarios. We propose a lightweight modification to CUT3R, incorporating a dedicated encoder for each modality to extract features, which are fused with RGB image tokens via zero convolution. This flexible design enables seamless integration of any combination of prior information during inference. Evaluated across multiple benchmarks, including 3D reconstruction and other multi-view tasks, our approach demonstrates significant performance improvements, showing its ability to effectively utilize available priors while maintaining compatibility with varying input modalities.", "tldr": "We introduce G-CUT3R, a novel feed-forward approach for guided(using prior camera calibrations/poses/depths) 3D scene reconstruction.", "keywords": ["3D Scene Reconstruction", "Continuous Updating Transformer", "Auxiliary Information"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9acaf9a405ab352e2bdc6025f630b2c03826536c.pdf", "supplementary_material": "/attachment/06385c91780c5e8a99a25cd522f84bbe2f3c429e.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes G-CUT3R, a guided, feed-forward 3D reconstruction method that extends CUT3R by integrating auxiliary priors (camera intrinsics and/or poses, and depth) through modality-specific encoders and zero-initialized 1*1 convolutions for stable fusion inside the decoder. The approach keeps CUT3R’s recurrent state, enabling multi-view processing without global optimization. Experiments on 7-Scenes, NRGBD, Bonn, ScanNet, Waymo, and ScanNet++ report consistent gains over CUT3R and DUSt3R-family baselines, with competitive run-time (around 20 FPS at 512 px on an A40)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear problem framing: Many feed-forward methods ignore readily available priors. Incorporating them is practically important. However, I suggest mentioning DepthSplat [a] as it is also a feed-forward method using depth priors (for a 3DGS reconstruction).\n- The method is light-weight and builds upon a well-established baseline, Cut3r. \n- Unified model for arbitrary prior subsets: training with random modality subsets reflects practical scenarios.\n- Strong experiments.\n\n[a] Xu, H., Peng, S., Wang, F., Blum, H., Barath, D., Geiger, A. and Pollefeys, M., 2025. Depthsplat: Connecting gaussian splatting and depth. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 16453-16463)."}, "weaknesses": {"value": "- It is a bit unclear to me what depth is used for the datasets. Did the authors always use sensor depth? Using anything else would render the results incorrect. I put this in the weaknesses given that this is very important. The paper should specify a detailed, dataset-by-dataset description of depth sources used as priors to make the results understandable. \n- Same question holds for the camera poses. Do the authors use SLAM-estimated poses (without post-processing) as priors or the GT ones? Using the GT ones again would make the experiments section a bit weaker.\n- Limited robustness analysis: no analysis of noisy/misaligned priors (e.g., wrong intrinsics, biased depth, pose drift). Results might depend strongly on prior quality. It would be very important to understand what to expect if things are noisy. I would be very happy to see plots showing how accuracy degrades with noise in the priors. \n\nMinor:\n- Loss definition details are sparse: The confidence-weighted point loss resembles uncertainty modeling, but calibration, scale, and supervision signals for confidences are not really specified."}, "questions": {"value": "The main questions are what the weaknesses are at the moment:\n- What depth/pose was used in the experiments? \n- How does the method behave with noisy priors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VnT7Rw2ioZ", "forum": "J7DiMqmIFl", "replyto": "J7DiMqmIFl", "signatures": ["ICLR.cc/2026/Conference/Submission16576/Reviewer_bCZM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16576/Reviewer_bCZM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925643275, "cdate": 1761925643275, "tmdate": 1762926656206, "mdate": 1762926656206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents G-CUT3R, a guided feed-forward 3D reconstruction framework that enhances the geometric consistency and reliability of transformer-based models such as DUSt3R and MASt3R.\nThe key idea is to explicitly regularize uncertainty alignment and cross-view geometric coherence, addressing the observation that per-view uncertainty maps are often inconsistent and lead to noisy surface fusion.\n\nThe framework introduces two components:\n\nCross-View Uncertainty Tuning (CUT): aligns the uncertainty distributions of corresponding pixels across multiple views using a learnable temperature scaling and a cross-view consistency loss.\n\nGeometry-Guided Regularization (G-Reg): imposes local surface smoothness and normal coherence regularization in 3D space, with learnable weights controlling the regularization strength.\n\nExperiments on ETH3D, Tanks & Temples, and CO3D show improved depth RMSE (≈10–12%) and higher pointmap F1 scores (+2–3%) compared with MASt3R and DUSt3R, indicating more stable multi-view fusion and sharper geometry boundaries."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Technically sound and well-motivated: both CUT and G-Reg directly target known weaknesses of feed-forward 3D reconstruction—cross-view inconsistency and surface noise—and are implemented cleanly.\n\nImproved stability and quality: the proposed regularizations yield consistent gains in quantitative metrics and visual quality across diverse datasets.\n\nGood empirical rigor: ablations on each component demonstrate that uncertainty alignment and geometric regularization complement each other."}, "weaknesses": {"value": "Limited conceptual novelty: both CUT and G-Reg are straightforward extensions of well-known principles—uncertainty calibration and geometric smoothing. The contributions lie more in empirical engineering than in new theoretical or algorithmic insight.\n\nLack of deeper analysis: the paper does not explore why these regularizations help beyond intuitive reasoning; no theoretical justification or failure analysis is offered.\n\nPossible over-smoothing: G-Reg may suppress fine details, but no perceptual or surface-sharpness evaluation is presented."}, "questions": {"value": "On over-regularization: have the authors evaluated whether the geometry regularizer causes over-smoothing in regions with high-frequency detail?\n\nOn generalization: have the authors tested whether these regularizations still help when applied to other backbones (e.g., Fast3R or VGGT), or are the gains specific to the MASt3R-like architecture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7z0LTLNr2H", "forum": "J7DiMqmIFl", "replyto": "J7DiMqmIFl", "signatures": ["ICLR.cc/2026/Conference/Submission16576/Reviewer_iLL2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16576/Reviewer_iLL2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974820177, "cdate": 1761974820177, "tmdate": 1762926655329, "mdate": 1762926655329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel feed-forward framework for 3D scene reconstruction. In particular, geometric priors—including depth, camera intrinsics, and camera extrinsics—are incorporated into the RGB latent space to guide the reconstruction process. Extensive evaluations on multiple benchmarks demonstrate that the proposed method achieves substantial performance improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work incorporates additional priors into feed-forward 3D reconstruction, enhancing its flexibility for diverse application scenarios. Comprehensive experiments demonstrate significant performance gains over state-of-the-art methods."}, "weaknesses": {"value": "- The paper claims to be efficient and lightweight. However, no additional results are provided to support this, such as FLOPs or parameter counts compared with CUT3R and Pow3R. Since the method introduces extra encoders and layers for additional modalities, it may incur substantial parameter and computation overhead relative to CUT3R, potentially compromising efficiency. It is unclear whether the reported efficiency stems primarily from inheriting CUT3R’s efficiency rather than being more efficient than CUT3R itself.\n- The proposed approach appears to combine elements from Pow3R and CUT3R. Could the authors clarify the key differences from these prior works?"}, "questions": {"value": "- Regarding the depth used during training, is it sensor depth or COLMAP-generated depth? The model that uses only depth priors shows only a marginal improvement in reconstruction quality. Why does this occur, given that depth information is typically highly correlated with reconstruction performance?\n- The encoders for the additional priors are not shared, which may significantly increase the number of parameters. Have the authors considered using a shared encoder to reduce model size?\n- In Table 3, does Pow3R employ the prior-integration mechanism proposed in this work or the original Pow3R design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kh4UPiPSgj", "forum": "J7DiMqmIFl", "replyto": "J7DiMqmIFl", "signatures": ["ICLR.cc/2026/Conference/Submission16576/Reviewer_J8qk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16576/Reviewer_J8qk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985562370, "cdate": 1761985562370, "tmdate": 1762926654204, "mdate": 1762926654204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}