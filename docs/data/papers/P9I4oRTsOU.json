{"id": "P9I4oRTsOU", "number": 13428, "cdate": 1758217742002, "mdate": 1763513439195, "content": {"title": "CoFrGeNet: Continued Fraction Architectures for Language Generation", "abstract": "Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We pre-train our models on two public text datasets - OpenWebText and GneissWeb. Results with our models show that the perplexity and performance on downstream GLUE tasks are superior or competitive with Transformer-based architectures, with two thirds to half the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.", "tldr": "We propose continued fraction based architectural components to replace attention and ffn in transformers", "keywords": ["continued fractions", "generative AI", "language"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce0fa592c7423147379ffbe6509a0e8f08005294.pdf", "supplementary_material": "/attachment/571a3f6926f99360caeadccd472fa64348d013c5.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces CoFrGeNet, a new architecture for language generation based on continued fractions. The authors replace Transformer attention and FFN layers with CoFrNet modules, using a continuant-based gradient formulation to reduce division operations and improve efficiency. Models are pretrained on OpenWebText and GneissWeb and show comparable or better results than GPT-2-xl on GLUE and language modeling tasks, while using fewer parameters and less training time."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Novel and mathematically grounded idea — continued fractions as neural components.\n- Clear derivations and theory with interpretable structure.\n- Plug-and-play replacement for Transformer layers; minimal training changes needed.\n- Custom gradient and training schedule improve efficiency and stability."}, "weaknesses": {"value": "- No analysis of compute cost or FLOPs; efficiency claims incomplete.\n- Attention complexity (Figure 2) not analyzed with respect to sequence length.\n- Pseudocode or in-paper algorithmic overview would improve clarity, even though code is provided separately.\n- Limited scaling experiments; unclear behavior for larger models.\n- No long-context or “needle-in-a-haystack” tests to verify causal mixing."}, "questions": {"value": "- Can the authors include FLOP or runtime comparisons beyond parameter count?\n- What is the exact computational complexity of CoFr attention?\n- Would including concise pseudocode or an algorithm outline improve readability?\n- How does the model handle long-context reasoning?\n- Are there training curves showing convergence and stability trends?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YlJYO7Ol2g", "forum": "P9I4oRTsOU", "replyto": "P9I4oRTsOU", "signatures": ["ICLR.cc/2026/Conference/Submission13428/Reviewer_YdsT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13428/Reviewer_YdsT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761701595824, "cdate": 1761701595824, "tmdate": 1762924054733, "mdate": 1762924054733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common response"}, "comment": {"value": "We thank the reviewers for their constructive comments. We are glad that **novelty**, **ease of use** being a plug-in replacement and **parameter efficiency** of our architectures were appreciated. We respond below to the specific concerns of the reviewers noting that we have **uploaded an updated version of the paper** addressing the major concerns."}}, "id": "CZFN595kHI", "forum": "P9I4oRTsOU", "replyto": "P9I4oRTsOU", "signatures": ["ICLR.cc/2026/Conference/Submission13428/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13428/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13428/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763513527547, "cdate": 1763513527547, "tmdate": 1763513527547, "mdate": 1763513527547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes replacing the transformer attention and feed-forward (FFN) layers with faster components that are inspired by continued fractions. Since their formulations utilizes division-operations which are slower - they design a custom gradient formulation that reduces divisions during training (from d divisions to 1). They compare their models trained with the ladder with GPT-2-XL on perplexity and GLUE tasks and show competitive performance using fewer parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper does seem to show that the ladder-fused models get competitive performance with fewer parameters. The ladder-setup also seems easy to plug into existing architectures."}, "weaknesses": {"value": "- Latency argument needs more support - Since the final architecture for the proposed method retains the projection step and uses affine transformations - it seems like a large amount of compute is still allocated to matmul operations. Only the internal non-linearity is replaced by the optimized division introduced by the authors; It’s unclear if this is enough to claim the latency improvements (which aren’t empirically very clear as well as Table 4 shows that the best CoFreGeNet is only about 20 microseconds better than the gpt-2xl inference time ?). \n\n- Accuracy-Parameter efficiency trends are irregular - For Table-3, the biggest variants show the most competitive performance where the parameter efficiency isn’t as substantial. A few tasks show competitive performance at best (when Standard deviation is taken into consideration) so it’s not very evident that the method gives consistent accuracy improvements with greater efficiency. \n\n- Some ablations would make the stability of the method more evident: Some decisions in the paper are not very well justified right now - for instance, the effect of the training schedule adopted by the authors. Including that will make the claims more stable. \n\nComment (not a weakness): Consider placing text around the wrap tables as Table 2 and Table 3 hinder natural reading significantly."}, "questions": {"value": "- Can you provide any other metrics that improve while using your custom gradient change the optimization landscape (stability) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r0OeN2AY2z", "forum": "P9I4oRTsOU", "replyto": "P9I4oRTsOU", "signatures": ["ICLR.cc/2026/Conference/Submission13428/Reviewer_2n4a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13428/Reviewer_2n4a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047679074, "cdate": 1762047679074, "tmdate": 1762924053647, "mdate": 1762924053647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new architecture that replaces attention and feed-forward layers in Transformers with continued fraction-based modules. This design reduces parameter count and computational cost while achieving competitive performance to GPT2-xl and efficient-attention baselines on GLUE and perplexity evaluations. The authors also demonstrate training and inference speed improvements, particularly when leveraging their “continuants” formulation for gradient computation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces an original mathematical formulation (continued fractions) to model dependencies, offering a new theoretical direction beyond attention.\n\n2. Demonstrates reasonable empirical results, outperforming baselines while using fewer parameters.\n\n3. Provides clear efficiency gains in both training and inference, showing practical relevance for language modeling."}, "weaknesses": {"value": "1. My main concern lies in the evaluation scope of the proposed model. While the continued fraction-based architecture might be new and shows encouraging results, the experiments are limited to GPT2-scale models and GLUE benchmarks. This restricted evaluation leaves open questions about the model’s scalability, robustness, and generalization to more complex or diverse settings such as larger models or broader benchmarks such as reasoning and long-context tasks. A more comprehensive evaluation would strengthen the paper’s claims about efficiency and performance across different architectures and data regimes.\n\n2. The method’s implementation complexity and generalization to other architectures (e.g., MoE or diffusion-based) remain unexplored."}, "questions": {"value": "How does the model perform on larger-scale models and broader benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9zX2cP5l1I", "forum": "P9I4oRTsOU", "replyto": "P9I4oRTsOU", "signatures": ["ICLR.cc/2026/Conference/Submission13428/Reviewer_oCFe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13428/Reviewer_oCFe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762461094566, "cdate": 1762461094566, "tmdate": 1762924051659, "mdate": 1762924051659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}