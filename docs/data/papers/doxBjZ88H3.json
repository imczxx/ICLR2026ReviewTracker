{"id": "doxBjZ88H3", "number": 20782, "cdate": 1758310032350, "mdate": 1759896958893, "content": {"title": "An Information-Theoretical Framework For Optimizing Experimental Design To Distinguish Probabilistic Neural Codes", "abstract": "The Bayesian brain hypothesis has been a leading theory in understanding perceptual decision-making under uncertainty.\nWhile extensive psychophysical evidence supports the notion of the brain performing Bayesian computations, how uncertainty information is encoded in sensory neural populations remains elusive.\nSpecifically, two competing hypotheses propose that early sensory populations encode either the likelihood function (exemplified by probabilistic population codes) or the posterior distribution (exemplified by neural sampling codes) over the stimulus, with the key distinction lying in whether stimulus priors would modulate the neural responses.\nHowever, experimentally differentiating these two hypotheses has remained challenging, as it is unclear what task design would effectively distinguish the two. \nIn this work, we present an information-theoretical framework for optimizing the task stimulus distribution that would maximally differentiate competing probabilistic neural codes.\nTo quantify how distinguishable the two probabilistic coding hypotheses are under a given task design, we derive the *information gap*---the expected performance difference when likelihood versus posterior decoders are applied to neural populations---by evaluating the KL divergence between the true posterior and a task-marginalized surrogate posterior.\nThrough extensive simulations, we demonstrate that the information gap accurately predicts decoder performance differences across diverse task settings.\nCritically, maximizing the information gap yields stimulus distributions that optimally differentiate likelihood and posterior coding hypotheses.\nOur framework enables principled, theory-driven experimental designs with maximal discriminative power to differentiate probabilistic neural codes, advancing our understanding of how neural populations represent and process sensory uncertainty.", "tldr": "Information gap guides optimal experimental designs to differentiate probabilistic neural coding hypotheses by quantifying the expected decoder performance differences", "keywords": ["Bayesian brain hypothesis", "perceptual decision-making", "uncertainty representations", "probabilistic neural codes", "probabilistic population code", "neural sampling code", "information theory", "experiment optimization"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9cbab2121ac13aac9605ec7581e3c9d506a630b4.pdf", "supplementary_material": "/attachment/e43fa29b24ced521a5cb7e8eb1d2aa1622bf52b4.zip"}, "replies": [{"content": {"summary": {"value": "The paper puts forth a mathematical formalism for experimentally distinguishing between encoding of likelihood or of posterior at the level of neural populations via decoding, provides an information gap metric for the discriminability of the two, and optimizes in the space of tasks for designs which most clearly separate between the two hypotheses. This paves the way for a more precise experimental validation  of various bayesian accounts of perception and associated neural implementation in the brain. \n\nThe basic setup assumes a family of tasks where the animal needs to estimate and report a one-dimensional latent variable theta conveyed by a sensory stimulus, in two (known to the animal) contexts corresponding to different theta distributions. given the responses of a population of sensory neurons r, from which different probabilistic information (according to an experimentally matched ideal observer) is decoded via flexible deep learning models. Assumptions about the different models are incorporated in the network's architecture following Walker et al 2020. The information gap between them is the KL between the two possible encoded quantities (using discretized random variables for easy computation). The question then becomes how to optimize the distance and degree of overlap for priors in the two contexts to maximize discriminability"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Interesting question, given precise mathematical form.\n\nNovelty and significance: both likelihood and posterior hypotheses make qualitatively similar predictions in traditional experimental designs which has hampered the ability to experimentally adjudicate between probabilistic coding theories. New ways to design experiments for maximal discriminability would circumvent this problem. To my knowledge no formal framing for such experimental design exists to date. \n\nthe idea that crossentropy-based decoders trained on ground truth stimulus identity are linked to ideal observer posterior uncertainty for the same data distribution seems generally useful\n\nNumerical validation of theoretical results in the limit of enough data"}, "weaknesses": {"value": "While i think the general framework is sensible and fruitful, I have concerns about the details of the execution. \n\nAlthough the writing overall is overall pretty clear, the key text with the description of the information gap is difficult to parse and ambiguous. the logic of the construction is not clearly enough explained in that part of the text.  \nThe appendix explains it better that the goal is to link crossentropy of different decoders to ideal observer posteriors arguing that a mismatched decoder will achieve overall less crossentropy performance but the main text cannot really be understood on its own \n\nBut the weakness is that the punchline is rather weak: the results do not seem to provide much insight into how one could design better experiments beyond giving ranges of possible d' parameters that look good c.f. the theory. It is not clear if the (not negligible) experimental data collected in this setup is outside that regime so that experimentalists would know to do something differently."}, "questions": {"value": "How does the animal's own added sensory uncertainty get incorporated in the procedure ? can the (unavoidable) model mismatch bias the conclusions in any way or are there formal guarantees that it will not? \n\nIn particular, what if the local neural population encodes beliefs about related random variables but not the exact parametrization of the experiment? I am mentioning this because successful neural sampling (posterior-based) probabilistic accounts of V1 responses use a high dimensional distributed representation (think probabilistic versions of factor analysis or gaussian scale mixtures) related to orientation but not beliefs about theta per se\n\ni got a little confused about terminology, e.g. \"true posterior ...of a ideal likelihood decoder\"?  i think what is meant is an optimal decoder of likelihood or posterior beliefs from neural responses \n\nAs defined, the information gap seems to come from the ability of the agent to correctly know or infer contex, some more discussion on that would be helpful\n\nPlease explain the need for hypothesis specific information gaps, what assumptions it makes about context unknown r, and what it implies for experimental design: do you assume data exists for both a single/cued context version and an inferred/uncued context version of the same task? The elements of the math on the bounds of decoder crossentropy performance described in A1 are individually more comprehensible but the big picture logic and assumptions remain to be inferred between the lines \n\nEq3: i did not manage to follow the logic of restricting the sum to special subpairs since it seems incompatible to the traditional KL definition\n\nWhy not optimize the KL difference between the signal predicted by the two theories in a more direct way? There may be a good reason for the extra complexity but it's not coming across from the text\n\ngiven concerns about data limitations, is there a way to take into account co-occuring behavioral responses as a way to improve precision or at least to address model mismatch?\n\nHow does the number of neurons enter the information gap expressions ? It seems counterintuitive that one can achieve the assymptotic precision for representing a continuous variable with 10 neurons... does that depend on the precision of the x discretization in a substantial way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0W1Feg0FX2", "forum": "doxBjZ88H3", "replyto": "doxBjZ88H3", "signatures": ["ICLR.cc/2026/Conference/Submission20782/Reviewer_JDGu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20782/Reviewer_JDGu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834732597, "cdate": 1761834732597, "tmdate": 1762934789840, "mdate": 1762934789840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Different theories have been proposed for the neural representations of early visual areas during perceptual decision making tasks. Some proposed that neural population activity represents the likelihood function over the stimulus variable (likelihood theory), while others argued that neural activity represents samples from the posterior distribution of the stimulus variables (posterior theory). So far, it has been difficult to adjudicate between these theories, or falsify both. This theoretical paper proposed a information-theoretical approach toward better ways to distinguish these theories experimentally, although no experimental results were given or analyzed in this paper. Specially, the authors proposed a metric called information gap, that captures the performance difference between the likelihood decoder v.s. posterior decoder. They were able to analytically derive the information gap under certain conditions. Numerical experiments were used to validate these results. They further used this approach to analyze some simple perceptual decision making tasks and made suggestion about how to better design experiments to test the different theories mentioned above."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper seeks to address a problem that has been difficult to make progress on in the field.  The proposed approach is sound. \n\nThe proposed analytical approach and the results are new, although they only seem to work under some relatively simple task conditions. \n\nThe numerical simulations in the paper were thorough and served to validate the analytical results. \n\nThe approach may provide some useful guide for the design of future experiments to testing the likelihood theory v..s posterior theory of the neural responses (although there may be some questions of practically how to find the optimal design as the optimal design seems to depend on some parameters, e..g, noise in the system, that might not be easily measured in the experiments)."}, "weaknesses": {"value": "— While the authors clearly spent efforts to articulate the theoretical settings and to justify the importance of the problem, I felt there was still some ambiguity. For example, the two theories outlined were the theory of representing likelihood v..s sampling-based code. However, here sampling seems to be an additional assumption. It would be cleaner if the comparison was simply a presentation of likelihood v.s. a representation the posterior. One can also consider a theory of sampling from the normalized likelihood (which would be equivalent to posterior assuming a uniform prior). It was unclear wether sampling is essential in the model comparison. \n\n— the paper only consider prior imposed during the task. It seemed that the results were based on the assumption that the task prior can be learned perfectly. Yet, this assumption may be too strong. \n\n-- What about the long-term priors (or natural priors) the observers already learn? Can the theoretical framework also apply to these? Or would these complicate things?\n\n\n-- My most substantial concern is that the authors only considered a specific path for a relatively narrow setting (although under these settings, the results seem to be solid). I am not sure if the approach would indeed be fruitful or whether there might be easier and more fruitful approach for adjudicating between the different theories. In particular, from Section 4, I am under the impression that practically it would still be difficult to distinguish these theories (Correct me if I have the wrong impression).  I wonder if there would be more fruitful path that would also be easier. For example, instead of using a static setting, how about considering a learning paradigm instead? Suppose we start with a baseline condition, and measure the neural responses. Then we teach the observers to learn a different prior. Under the likelihood theory,, the neural representation in V1 should not change. In contrast, based on the posterior theory, the neural representation in V1 should change according to the prior learned.  So the question is, what is the advantage of the approach proposed in the current paper compared to such an alternative approach?\n\n-- The paper would be stronger if some empirical results from experiments can be shown. Even a re-analysis some previously reported experimental data to demonstrate the advantage of the current method would be useful."}, "questions": {"value": "In fig. 2c, top row, why would one still incorporate the priors given it is a likelihood decoder? Also, were ground truth prior assumed there? \n\nLine 249, what does the “task-marginalized” estimator mean? Please unpack the idea. \n\nWhat was the ground truth posterior/likelihood in the simulation studies? Were neural noise considered in addition to the stimulus noise?\n\nFrom Fig. 6, it appears that the optimal experimental design depends on the noise in the system. Practically how can we determine the noise level before designing the experiments to test the theories?\n\nFig. 6, what are the units on the x- and y-axis?\n\nThe authors show the using heavy-tailed priors would not help better distinguish different models. Would using distribution with thinner tail than Gaussian help? What about uniform priors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hJm2UmDJ95", "forum": "doxBjZ88H3", "replyto": "doxBjZ88H3", "signatures": ["ICLR.cc/2026/Conference/Submission20782/Reviewer_EZQq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20782/Reviewer_EZQq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851712170, "cdate": 1761851712170, "tmdate": 1762934739659, "mdate": 1762934739659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors develop a framework for designing a pair of stimulus ensembles that are optimized to differentiate two models of probabilistic sensory representation for Bayes-optimal observers: the \"likelihood\" model, and the \"posterior\" model.  They show through a number of simulations that the method can successfully distinguish the two models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Optimal experimental design is an important and under-studied area.\n\n* The problem posed, of developing an analysis to compare two standard models proposed for Bayesian inference in the brain, and using this to derive optimal stimulus ensembles for experimental determination of a \"winner\", is relevant and I'm not aware of any published solutions."}, "weaknesses": {"value": "* The development is, in my view, over-formalized given the intended use and implementation.  Why is it not sufficient to seek two stimulus ensembles that produce the largest discrepancy in estimates produced by the two models?  And how do the results depend on the design and training of the deep neural network decoders that are used to obtain likeilihoods or posteriors from the neural population?\n\n* The paper is built on the direct comparison of two models (likelihood encoding vs. posterior encoding), but there are other proposals in the literature as to how prior probabilities might be embedded in neural systems.  In particular, Ganguli & Simoncelli (2010/2014) propose that inhomogeneous populations of tuning curves encodes the prior, the spiking responses form a likelihood, and a weighted nonlinear decoder can approximate Bayesian estimates.  There's reasonable experimental support for these components, but it's not clear how this formulation would be categorized in the context of the authors' proposed experimental test.\n\n* The authors' formulation relies on a highly simplified description of how an organism (human or otherwise) adapts to a sensory environment. In particular, they assume the posterior observer is aware of the true prior, and is able to use it properly for inference, which seems inconsistent with the large literature on adaptation.  Perceptually, adaptation to an ensemble leads to changes in perceived value (biases) as well as changes in discriminability (variance), but the changes reported in the literature are diverse (especially changes in discriminability), and highly dependent on the perceptual attribute being tested, the adaptation stimuli, the duration of exposure, etc.  In fact,  perceptual adaptation effects have been described as \"anti-Bayesian\", because adapting to one stimulus (or a distribution concentrated in one region of the the stimulus space) leads to perceptual repulsion rather than attraction toward this modified \"prior\". Physiologically, the most commonly observed effect of adaptation is a reduction in gain, but many reports describe shifts or distortions (e.g., flank suppression) of tuning curves.  Again, these effects are quite variable, and depend on details of adapting stimulus ensemble, duration, etc.   Recent work by Tring et. al. (2024) provides a compelling explanation for observed adaptation effects in terms of coding efficiency, but this seems at odds with either the \"likelihood\"  or \"posterior\" hypothesis of the current paper.  So my concern is that experimental design proposed by the authors will lead to inconclusive results, because adaptive changes in perceptual systems are substantial (so not aligned with the \"likeilihood\" model), but are also inconsistent with a change in prior.  \n\n* In describing prior literature, the paper should make mention of work on optimal stimulus design for estimating perceptual discrimination thresholds [eg: Quest (watson/pelli 1983), Pest (Medigan 1987), Information-theoretic stimulus optimization for neurophysiology (Lewi etal 2006/2009), Quest+ (Watson 2017)]. Perhaps more relevant, work by Machens et al 2005 adaptively sought a stimulus ensemble for which a neural population (grasshopper auditory receptors) provided an optimally efficient code.  Not the same as Bayesian decoding, but analogous in spirit."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DkIYYOTsIG", "forum": "doxBjZ88H3", "replyto": "doxBjZ88H3", "signatures": ["ICLR.cc/2026/Conference/Submission20782/Reviewer_1gnw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20782/Reviewer_1gnw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978553342, "cdate": 1761978553342, "tmdate": 1762934687749, "mdate": 1762934687749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper suggests a framework, i.e. testing KL Divergence of different decoders, to determine which hypothesis is true about the perceptual coding in the brain, likelihood or posterior. Tested on simulated data on gratings with different orientations and contrast levels, it illustrates the potential use of the method for real experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well written, specifying the problem, process, and limitations clearly. It also gives a good coverage of the literature."}, "weaknesses": {"value": "Using KL-divergence and decoding performance is pretty common in modelling in neuroscience, so I don't think using it for a specific problem could be considered a huge contribution. Furthermore, the paper lacks empirical results. There are multiple public data sets that actually contain gratings with different orientation and contrasts, for example Allen Brain observatory visual coding Neuropixel data set( https://portal.brain-map.org/circuits-behavior/visual-coding-neuropixels). So I think testing the practicality of the approach is feasible for the authors. While it is kind of acknowledged in the paper, the method relies on a lot of assumptions, and most importantly optimality of the neual code. There are multiple studies showing that the code is not perfectly optimal, including those referenced in the paper, e.g Haefner, et al, Neuron 2016 in which they explained a Bayesian approach that models the bias toward early observations. Overall, many studies on perception has shown that the neural activity is too complicated to be easily interpreted by models (and that's why we don't know what exactly going on as the authors mentioned too). I can't see how this approach could give a practical solution to that."}, "questions": {"value": "- Do the simulation results hold when tested on the actual neural data (e.g on Allen data set mentioned above)?\n- How does the firing rate and noise affect the results?\n- How does the approach handle arousal, running, or other phenomena that affect the neural code in the visual cortex?\n- Decoding approaches especially by powerful methods like deep neural networks are not trustable to many neuroscientists, as the presence of information is different from using the information. For example, one might give all the neural activity in the retina to a deep neural network and infer high-order features such as objects, just like how they can do it on raw images directly. This does not mean that the retina perform object recognition. how does this affect the feasibility of the presented approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eYDEceF2ys", "forum": "doxBjZ88H3", "replyto": "doxBjZ88H3", "signatures": ["ICLR.cc/2026/Conference/Submission20782/Reviewer_UTZ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20782/Reviewer_UTZ6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216906206, "cdate": 1762216906206, "tmdate": 1762934634806, "mdate": 1762934634806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}