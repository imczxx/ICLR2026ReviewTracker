{"id": "cAopJVLKvi", "number": 2087, "cdate": 1756986677347, "mdate": 1759898170338, "content": {"title": "Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders", "abstract": "Recent multimodal large language models (MLLMs) increasingly integrate multiple vision encoders to improve performance on various benchmarks, assuming that diverse pretraining objectives yield complementary visual signals. \nHowever, we show this assumption often fails in practice. Through systematic encoder masking across representative multi-encoder MLLMs, we find that performance typically degrades gracefully—and sometimes even improves—when selected encoders are masked, revealing pervasive encoder redundancy. \nTo quantify this effect, we introduce two principled metrics: the Conditional Utilization Rate (CUR), which measures an encoder’s marginal contribution in the presence of others, and the Information Gap (IG), which captures heterogeneity in encoder utility within a model. \nUsing these tools, we observe: (i) strong specialization on tasks like OCR \\& Chart, where a single encoder can dominate with a CUR >90%, (ii) high redundancy on general VQA and knowledge-based tasks, where encoders are largely interchangeable, (iii) instances of detrimental encoders with negative CUR. \nNotably, masking specific encoders can yield up to 16% higher accuracy on a specific task category and 3.6% overall performance boost compared to the full model.\nFurthermore, single- and dual- encoder variants recover over 90% of baseline on most non-OCR tasks. Our analysis challenges the “more encoders are better” heuristic in MLLMs and provides actionable diagnostics for developing more efficient and effective multimodal architectures.", "tldr": "Adding more vision encoders to MLLMs often yields redundancy or even harms performance.", "keywords": ["Large language Model", "MLLMs", "Vision Encoder", "Machine Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7a179047ba8990c5df8e3791049ea84ef8c510c.pdf", "supplementary_material": "/attachment/253812bf1d67bcf7f2ed64c0a148474e5073c6ad.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents an investigation into the phenomenon of encoder redundancy in Multi-encoder Multimodal Large Language Models (MLLMs). The authors challenge the prevailing assumption that integrating more vision encoders, pre-trained with diverse objectives, necessarily leads to better performance. Through extensive ablation studies on models like Eagle and Cambrian-1, they demonstrate that performance often degrades gracefully—and sometimes even improves—when one or more encoders are masked. To quantify this redundancy, they introduce two metrics: the Conditional Utilization Rate (CUR), which measures an encoder's marginal contribution given the presence of others, and the Information Gap (IG), which captures the disparity in utility across the encoder set."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a critical, underexplored, and timely issue in MLLM design. As models grow more complex, understanding and mitigating redundancy is essential for improving efficiency and performance.\n2. The proposed metrics, CUR and IG, are intuitive, easy to compute, and provide a rigorous, quantitative framework for analyzing encoder contributions. They effectively capture both individual utility and overall system imbalance.\n3. The empirical evaluation is thorough. The authors test their hypothesis across multiple state-of-the-art model families (Eagle, Cambrian-1), various model sizes (3B to 13B), and a comprehensive set of benchmarks categorized by task type. The results are robust and clearly support the claims."}, "weaknesses": {"value": "1. The study is conducted on pre-trained, static models. It does not explore whether redundancy emerges during training or if dynamic, input-dependent routing of encoders (a potential solution) could better leverage the available encoders.\n2. **Narrow Scope of \"Performance\"**: Performance is primarily measured as accuracy on standardized benchmarks. Other critical dimensions like robustness, out-of-distribution generalization, calibration, and fairness are not considered. A redundant encoder might contribute to robustness even if it hurts average accuracy.\n3. **Higher-Order Interactions**: The authors correctly identify this as a limitation. CUR only measures the effect of ablating a single encoder. It cannot detect complementary pairs or higher-order synergies where two seemingly redundant encoders (low individual CUR) are both essential for performance (e.g., if removed together, performance collapses). A metric capturing these interactions would be a valuable extension.\n4. **Limited Exploration of Fusion Mechanisms**: While the paper notes that more sophisticated fusion (like Cambrian-1's SVA) does not fully mitigate redundancy, a deeper analysis is lacking. It remains unclear how different fusion strategies (e.g., MoE, cross-attention) might amplify or suppress redundancy, which is a crucial aspect of multi-encoder architecture design."}, "questions": {"value": "1. How would you suggest extending the CUR framework to quantify the complementary or synergistic effects between pairs or groups of encoders, to address the limitation you noted?\n2. What is the interplay between fusion mechanism complexity and redundancy? Could a better fusion strategy (e.g., a gating network) dynamically suppress redundant or conflicting encoders on a per-input basis, thereby making a larger ensemble more justifiable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xhho0JOUak", "forum": "cAopJVLKvi", "replyto": "cAopJVLKvi", "signatures": ["ICLR.cc/2026/Conference/Submission2087/Reviewer_VGcs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2087/Reviewer_VGcs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761312537547, "cdate": 1761312537547, "tmdate": 1762916015232, "mdate": 1762916015232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines whether adding multiple visual encoders improves multimodal large language models. It introduces two metrics—Conditional Utilization Rate (CUR) and Information Gap (IG)—to quantify encoder contribution and redundancy. Experiments across several benchmarks show that additional encoders often bring limited or inconsistent gains, and sometimes degrade performance. The study concludes that simply increasing the number of encoders does not guarantee better results, highlighting redundancy and imbalance in current multi-encoder designs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Problem framing and metrics. Treats redundancy in multi-encoder MLLMs as a first-class research object and introduces two reusable, precisely defined measures  (CUR) and (IG) that turn a vague intuition (“more encoders help”) into testable quantities."}, "weaknesses": {"value": "1. Lack of Constructive Improvements. While the proposed CUR and IG metrics help quantify encoder redundancy, the contribution appears incremental. Prior works such as Eagle have already observed and empirically analyzed similar redundancy phenomena (Fig. 4 in their paper). This paper mainly reformulates those observations into statistical metrics based on existing accuracy measures, without offering causal insights into why redundancy arises, e.g., whether it stems from overlapping feature spaces, pretraining objectives, or fusion mechanisms.\nTo strengthen the work, the authors could do further experiments and implement learnable gating or dynamic routing based on CUR (refer to MoVA), rather than only suggesting them as future work.\n\n2. Experimental Design Issues. The experimental coverage is incomplete. For models with fewer visual encoders, such as DeepSeek-VL 7B, Table 1 reports only IG, while Table 3 omits corresponding accuracy metrics. \n\n3. Insufficient Efficiency Evaluation. The paper asserts that removing redundant encoders improves efficiency but provides no quantitative evidence, such as GPU hours, FLOPs, or latency metrics. In contrast, Eagle reports detailed efficiency tables and trade-offs between accuracy and computation.\n\nThe paper offers a useful quantitative perspective on multi-encoder redundancy but lacks theoretical depth, complete experimental validation, and concrete improvement mechanisms. By integrating causal or information-theoretic reasoning, implementing dynamic encoder selection (like MoVA), and reporting efficiency metrics, the study could become substantially more constructive and impactful for the multimodal LLM community.\n\n1. MoVA: Adapting Mixture of Vision Experts to Multimodal Context\n2. Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders"}, "questions": {"value": "1. Does a more balanced utilization of visual encoders (i.e., lower IG) really lead to better practical results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O4XOhjVXch", "forum": "cAopJVLKvi", "replyto": "cAopJVLKvi", "signatures": ["ICLR.cc/2026/Conference/Submission2087/Reviewer_yWcu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2087/Reviewer_yWcu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718454555, "cdate": 1761718454555, "tmdate": 1762916014971, "mdate": 1762916014971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper mainly investigates the impact of multiple vision encoders in multimodal llms. By isolating individual encoders, the paper finds that not all vision encoders are needed in downstream tasks, some of which are even detrimental to performance. To quantify their findings, the paper introduces two metrics, CUR and IG, which jointly describe the contribution of different encoders to performance. Evaluations on popular multi-encoder mllms demonstrate that not all vision encoders are necessary during inference, as they barely drop performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and metrics are well-defined. Easy to follow.\n2. This paper challenges the view on vision encoder selection for open mllms, and provides a fresher perspective on inference selection strategy.\n3. The evaluation setups are extensive, which back up their claim about inference not needing all encoders."}, "weaknesses": {"value": "1. This paper feels also related to visual token selection strategy, I think the paper should include relevant references in related work section.\n2. In the final paragraph of the introduction, the paper also mentions \"in our setup, fine-tuning a dualencoder variant is 1.69× faster than its five-encoder counterpart.\", however, I did not find experiments about finetuning in the paper.\n3. To further show the merit of reducing vision encoders during inference, I believe it's better to also include the compution reduction (i.e. FLOPs) during evaluation, for example in Table 3.\n4. The paper positions that not all evaluations are needed to mllms. According to the experiment, I agree that when inferencing it is true. However, I believe to validate this claim completely, I believe the paper also needs to demonstrate that not all vision encoders are needed for training. For example, some vision encoders may contribute less during inference, but they might help other encoders learn better during training. The training dynamic here is still unclear.\n\nI will be happy to raise my score if the authors can address the above concerns."}, "questions": {"value": "1. Table 7 in the appendix is quite interesting. When masking all encoders, tasks that claim to be vision-centric, still receive relatively high scores. Why do you think that is?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u201LMVKS5", "forum": "cAopJVLKvi", "replyto": "cAopJVLKvi", "signatures": ["ICLR.cc/2026/Conference/Submission2087/Reviewer_vJKH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2087/Reviewer_vJKH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905148463, "cdate": 1761905148463, "tmdate": 1762916014638, "mdate": 1762916014638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is paper examines whether adding more vision encoders truly improves multimodal large language model (MLLM) performance.\nIt finds that multi-encoder MLLMs often exhibit substantial redundancy—removing some vision encoders either barely affects or even improves accuracy. To quantify this, the authors propose two new metrics: Conditional Utilization Rate (CUR): measures an encoder’s marginal contribution to overall performance. Information Gap (IG): measures imbalance in encoder usefulness within a model.\nThrough systematic encoder masking experiments on models like Eagle and Cambrian-1, they show: Many encoders are redundant or even detrimental. Some tasks (e.g., OCR & Chart understanding) rely heavily on one specialized encoder (CUR > 90%), while others (e.g., general VQA) show high redundancy. Two-encoder variants recover > 90 % of full performance with much lower computational cost.\nOverall, the study challenges the “more encoders = better performance” assumption and provides a diagnostic framework (via CUR and IG) for designing more efficient, balanced multimodal architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles an underexplored but highly relevant question in multimodal LLM design — encoder redundancy. While prior works focused on adding more encoders or improving fusion, this paper challenges the “more is better” assumption and provides a new analytical perspective.\n2. The introduction of Conditional Utilization Rate (CUR) and Information Gap (IG) offers principled and interpretable metrics for quantifying encoder contribution and redundancy, enabling future researchers to diagnose model efficiency systematically.\n3. The authors conduct extensive masking experiments across multiple representative multi-encoder MLLMs (e.g., Eagle, Cambrian-1), covering diverse benchmark categories (OCR, Chart, General, Knowledge, and Vision-Centric). The analyses are thorough and reproducible.\n4. The finding that two carefully selected encoders (e.g., EVA-02 + ConvNeXt or ConvNeXt + CLIP) can retain over 90% of full-model performance provides actionable guidance for designing more efficient multimodal architectures."}, "weaknesses": {"value": "1. While the paper introduces CUR and IG as empirical metrics, it does not offer a strong theoretical framework explaining why redundancy arises or how encoder representations overlap in feature space.\n2. The study primarily measures the effect of removing one encoder at a time (via single-encoder masking). This ignores higher-order interactions — for example, two encoders might each seem redundant individually but provide complementary information together.\n3. Can the author provide the analysis of which fusion method is most effective and efficient? How can we decide which encoders are most important before training? Does the training strategy also influence the encoder selection? These are the follow-up questions. I do appreciate what the authors did in the paper."}, "questions": {"value": "Questions are mentioned in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zBpiYXwy47", "forum": "cAopJVLKvi", "replyto": "cAopJVLKvi", "signatures": ["ICLR.cc/2026/Conference/Submission2087/Reviewer_rQtg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2087/Reviewer_rQtg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924186478, "cdate": 1761924186478, "tmdate": 1762916014323, "mdate": 1762916014323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}