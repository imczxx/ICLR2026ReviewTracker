{"id": "iitxXWqODX", "number": 5953, "cdate": 1757948370985, "mdate": 1759897942551, "content": {"title": "ALMEA: Active Learning-Enhanced Multimodal Entity Alignment with Semantic Modality Imputation", "abstract": "Multimodal knowledge graphs (MMKGs) offer enriched knowledge representation by integrating structural, visual, and textual information from heterogeneous sources. However, existing multimodal entity alignment (MMEA) approaches face significant challenges due to missing modalities and semantic inconsistencies across sources. These limitations compromise alignment robustness, especially in low-resource scenarios with limited seed pairs (i.e., manually annotated aligned entities as supervision).\n\nTo bridge the gap, we propose **Active Learning for Multimodal Entity Alignment with Semantic Imputation (ALMEA)**, a MMEA framework that integrates semantic calibration and active learning to improve alignment. Specifically, ALMEA synthesizes embeddings for missing modalities and refines semantic representations to address inconsistencies across MMKGs. This approach iteratively selects optimal candidate pairs within the learnable budget through active learning strategies, thereby acquiring richer modal information in low-resource scenarios.\n\nOn the benchmark MMKG dataset, experimental results indicate that ALMEA consistently outperforms state-of-the-art baseline models under the low-resource scenario, achieving average improvements of **5.16% in Mean Reciprocal Rank (MRR)** and **5.57% in Hits at Top-1 (Hits@1)**.\n\nOur anonymized code is available at [github.com/RTX4090123/ALMEA](https://github.com/RTX4090123/ALMEA).", "tldr": "We propose a multimodal entity alignment framework that combines semantic calibration and active learning to handle missing modalities and inconsistencies in the low-resource multimodal knowledge graphs settings.", "keywords": ["Multimodal Entity Alignment", "Missing Modality Imputation", "Latent Semantics Calibration", "Active Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/74c9590730fbc32d83665875f6113a05eec7194e.pdf", "supplementary_material": "/attachment/afe4345a0898d01753e74c9d7242773151895c49.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes ALMEA, a novel framework for Multimodal Entity Alignment under low-resource settings. ALMEA integrates semantic imputation of missing modalities with active learning to improve alignment robustness. The authors evaluate ALMEA on two datasets (FB15K-DB15K and FB15K-YAGO15K) under varying supervision budgets, and the results show consistent improvements over state-of-the-art baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses two critical challenges in MMEA, missing modalities and limited seed alignments, both of which are common in real-world KGs but often overlooked in prior work.\n\nThe experiments cover multiple settings, including ablation studies, statistical significance tests, and qualitative case analyses.\n\nThe modular structure (LSL, LSC, ACS) enhances interpretability and facilitates component-wise analysis."}, "weaknesses": {"value": "Leveraging VAE to generate the missing modalities has already been explored by GEEA and UMAEA [1]. The latter also proposes a new multi-modal OpenEA benchmark. The authors currently evaluate ALMEA only on two single-lingual datasets. It would be better to validate the effectiveness of the proposed method across multiple benchmarks.\n\nThis paper uses active learning for annotation, which is analogous to the bootstrapping or iterative algorithms used in existing EA and MMEA methods. However, the authors only present the non-iterative results of baselines. The performance advantage of the proposed method isn't that large.\n\nThe baselines (e.g., MEAFormer) are marked with \"*\" indicating reproduction. However, their results are significantly lower than those in their original paper. This is perhaps because the authors use a constant embedding dimension setting for all methods, but why?\n\nThe writing and presentation also need improvement to meet the standard of ICLR. For example, Figure 2 introduces too many terms that are not discussed (as well as Algorithm 1), and tables contain several typos.\n\n[1] Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment, ISWC 2023."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PTUqvc2wXt", "forum": "iitxXWqODX", "replyto": "iitxXWqODX", "signatures": ["ICLR.cc/2026/Conference/Submission5953/Reviewer_pQ3f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5953/Reviewer_pQ3f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637280849, "cdate": 1761637280849, "tmdate": 1762918372234, "mdate": 1762918372234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework called ALMEA for MMEA, which addresses the problem of aligning entities across MMKGs with missing modalities and semantic inconsistencies. The proposed approach integrates semantic calibration and active learning to improve alignment in low-resource scenarios, especially where manually annotated seed pairs are scarce. The ALMEA framework includes three core modules: Latent Semantic Learning, Latent Semantic Calibration, and Active Candidate Selection. Experimental results show that ALMEA outperforms existing baseline methods, demonstrating an improvement in alignment accuracy in various low-resource scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. LSL effectively synthesizes embeddings for missing modalities, which ensures that the missing modality information can still contribute to alignment tasks.\n2. The use of active learning to select the most informative entity pairs for annotation is a key strength, helping to alleviate the low-resource challenge by iteratively improving the alignment model with minimal manual intervention.\n3. The paper provides strong experimental results across two benchmark datasets. ALMEA consistently outperforms the state-of-the-art baselines, especially in low-resource settings."}, "weaknesses": {"value": "1. About novelty. One concern is that active learning has been used for weakly supervised EA, which is your mentioned low-resource scenarios, for a long time. It seems ALMEA is similar to these active learning-based methods [1,2]. Another concern is that some methods have similar performance to ALMEA in the same settings, like DESAlign [3], which is also designed for tackling the semantic consistency. So why don`t you compare and analyze them? Or maybe there are some other different settings I have ignored.\n2. While the paper provides strong qualitative results, there is limited statistical validation to support the claim that ALMEA’s improvements are significant and not due to random variation. Error bars or significance tests like paired t-tests could provide more confidence in the reported improvements.\n3. While the paper claims that global weighting is more robust, a more detailed analysis of how these different weighting strategies perform across various datasets would be beneficial. A more detailed comparison of the global weighting vs. dynamic weighting could further clarify the advantages of ALMEA's approach in different settings.\n\n\n[1].Berrendorf, Max, Evgeniy Faerman, and Volker Tresp. \"Active learning for entity alignment.\" European Conference on Information Retrieval. Cham: Springer International Publishing, 2021.\n\n[2].Liu, Bing, et al. \"ActiveEA: Active Learning for Neural Entity Alignment.\" 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021. Association for Computational Linguistics (ACL), 2021.\n\n[3].Wang, Yuanyi, et al. \"Towards semantic consistency: Dirichlet energy driven robust multi-modal entity alignment.\" 2024 IEEE 40th International Conference on Data Engineering (ICDE). IEEE, 2024."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bDN7CA0eGR", "forum": "iitxXWqODX", "replyto": "iitxXWqODX", "signatures": ["ICLR.cc/2026/Conference/Submission5953/Reviewer_iYi6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5953/Reviewer_iYi6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740493470, "cdate": 1761740493470, "tmdate": 1762918371841, "mdate": 1762918371841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new MMEA framework called ALMEA to employ active learning for maintaining semantic consistency across different KGs and addressing the low-resource scenario. ALMEA consists of three designs: Latent Semantic Learning (LSL), atent Semantic Calibration (LSC), and Active Candidate Selection (ACS). Experiments are conducted on FB15K-DB15K and FB15K-YAGO15K."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The active learning framework is firstly introduced in the MMEA field. Therefore, the design of the overall framework is novel.\n- Low-resource scenario is an important topic for MMEA research."}, "weaknesses": {"value": "- The citation format in this paper is wrong, which should be revised in the rebuttal. \\cite --> \\citep\n- The paper shows that ALMEA has lower performance gain when the data is sufficient and it mainly works for low-resource scenario.\n- The unsupervised MMEA setting is not explored in the experiments. Besides, the datasets used in the experiments are mainly FB15K-DB15K and FB15K-YAGO15K, which are monolingual (English) datasets. The multilingual datasets are not explored in the main experiments.\n- The presemtation of Table 1 can be further optimized to make it more clear."}, "questions": {"value": "- What about ALMEA's performance under unsupervised MMEA? Can active learning still work? I hope that you can add more experiments on this setting to show whether active learning works for it.\n- In the datasets you used, the entity amounts are not significantly different. Can you consider other scenario that the entity amounts of the two KGs are with a significant order-of-magnitude gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pG0n1Kgk08", "forum": "iitxXWqODX", "replyto": "iitxXWqODX", "signatures": ["ICLR.cc/2026/Conference/Submission5953/Reviewer_jwT2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5953/Reviewer_jwT2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797369794, "cdate": 1761797369794, "tmdate": 1762918371505, "mdate": 1762918371505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ALMEA, a novel framework for Multimodal Entity Alignment (MMEA) that combines semantic imputation and active learning to improve robustness under missing-modality and low-resource conditions. The method consists of three main modules:\n(1) Latent Semantic Learning (LSL) uses a VAE-based generative model to synthesize embeddings for missing modalities;\n(2) Latent Semantic Calibration (LSC) aligns cross-graph semantic distributions via KL divergence to mitigate semantic inconsistency;\n(3) Active Candidate Selection (ACS) employs a diversity-regularized subset selection strategy to efficiently choose representative entity pairs under a limited labeling budget.\nExperiments on FB15K–DB15K and FB15K–YAGO15K show consistent improvements over state-of-the-art baselines (up to +5.16% MRR and +5.57% Hits@1), especially in low-resource scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel integration of semantic imputation and active learning for MMEA, addressing both missing modalities and sparse supervision.\n\n2. Comprehensive evaluation on two benchmark MMKG datasets with multiple baselines, demonstrating consistent gains in both low- and high-resource settings. And Clear modular design (LSL, LSC, ACS) that enables ablation and interpretability.\n\n3. Strong robustness analysis, showing resilience to missing modalities and diverse candidate selection strategies."}, "weaknesses": {"value": "1. The connection between the active learning optimization and overall alignment objective lacks formal analysis or proof of convergence.\n\n2. Training involves multiple components (VAE, calibration, optimization with ADMM), which may increase computational cost and make real-world deployment difficult.\n\n2. While quantitative results are strong, more case studies or visualization of latent semantics could better illustrate the improvements brought by LSC and ACS."}, "questions": {"value": "1. Could the active learning module be combined with uncertainty-based criteria (e.g., entropy or margin sampling) to further improve sample efficiency?\n\n2. What is the computational overhead compared to MEAformer or SimDiff, and how does ALMEA scale with large-scale MMKGs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wkx4Zw0S8W", "forum": "iitxXWqODX", "replyto": "iitxXWqODX", "signatures": ["ICLR.cc/2026/Conference/Submission5953/Reviewer_kymN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5953/Reviewer_kymN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064025174, "cdate": 1762064025174, "tmdate": 1762918370569, "mdate": 1762918370569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}