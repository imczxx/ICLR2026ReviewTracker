{"id": "qP7ryQmenw", "number": 21784, "cdate": 1758321712219, "mdate": 1763578072869, "content": {"title": "TabKDE:  Simple and Scalable Tabular Data Generation with Kernel Density Estimates", "abstract": "Tabular data generation considers a large table with multiple columns -- each column comprised of numerical, categorical, or sometimes ordinal values. The goal is to produce new rows for the table that replicate the distribution of rows from the original data -- without just copying those initial rows.  The last 3 years has seen enormous progress on this problem, mostly using computational expensive methods that employ one-hot encoding, VAEs, and diffusion.  \n\nThis paper describes a new approach to the problem of tabular data generation.   By employing copula transformations and modeling the distribution as a kernel density estimate we can nearly match the accuracy and privacy-preservation achievements of the previous methods, but with almost no training time.  Our method is very scalable, and can be run on data sets orders of magnitude larger than prior art on a simple laptop.  Moreover, because we employ kernel density estimates, we can store the model as a coreset of the original data -- we believe the first for generative modeling -- and as a result, require significantly less space as well.  Our code is available here:  \\url{http://github.com/tabkde/tabkde-main}", "tldr": "Scalable tabular data generation using kernel density estimates and copula transformations, achieving near state-of-the-art accuracy and privacy with very short training time.", "keywords": ["Tabular Data Synthesis", "Kernel Density Estimates", "Coreset Compression", "Copula Transformation", "Generative model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/109e7e233c95eb4a08ed6359a2f552cedd00528d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces TABKDE, a three-stage pipeline for tabular synthesis: (1) encode each column (numeric, ordinal, categorical) into a single continuous value, (2) transform the features into a copula-based latent space, and (3) generate new samples via a KDE-style perturbation procedure before inverting back to the original schema. The approach avoids heavy generative training. Experiments on six datasets indicate reasonable distribution preservation and privacy relative to computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Simplicity and scalability.** The method avoids one-hot encodings, VAEs, and diffusion training; the generation mechanism is simple, efficient and potentially effective scalable to large-scale data.\n\n**Clear end-to-end workflow.** The encode → latent mapping → sample → inverse-copula decode pipeline is clearly presented and straightforward to follow."}, "weaknesses": {"value": "**Writing clarity.** Several sections could be revised for readability and precision.\n- Line 166 (categorical values). It appears this treatment may apply only to ordinal columns; please clarify the intended scope.\nMetric naming. Line 435: the term “DCR score” is confusing, since DCR is a distance while the “score” is a percentage-based statistic. Consider distinct names.\n- Organization. Consider moving most related work to a dedicated section so the introduction can focus on motivation and intuition for the proposed method.\n- Related work coverage. The paper should discuss copula-based approaches for synthetic tabular data, such as: Kamthe et al., “Copula Flows for Synthetic Data Generation”; Sun et al., “Learning Vine Copula Models for Synthetic Data Generation”. \n\n**Categorical encoding justification.** Additional analysis is needed:\n- [Q1.1] Any numeric encoding of a categorical column imposes an order. What is the impact on model behavior and sample quality?\n- [Q1.2] For high-cardinality categoricals, can the model reliably generate the full support, including rare categories?\n- [Q1.3] Line 175. If a categorical column is independent of numeric columns but depends on other categorical columns, how is this dependence preserved?\n- [Q1.4] Line 327. Total variation distance can underweight long-tail categories. Table 4 mixes numeric and categorical columns, which may obscure behavior. Please report per-column categorical marginals and include TV and KL metrics, with emphasis on rare classes.\n\n**Ablation study needed**\n- [Q2.1] How would the proposed encoding method work with existing generative models?\n- [Q2.2] How would the KDE model work with one-hot encoding?"}, "questions": {"value": "[Q3] Line 157. What theoretical justification or intuition supports the KDE-style generative procedure in latent space?\n\n[Q4] Line 192. Is it fair to define and evaluate DCR in the learned latent space, given that the method is tailored to that geometry? Could this favor the proposed approach over baselines evaluated in other spaces?\n\n[Q5] Line 433. Why is 68 percent considered “serviceable”? Please provide a rationale and, if possible, confidence intervals or statistical context."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bx15v5qR2R", "forum": "qP7ryQmenw", "replyto": "qP7ryQmenw", "signatures": ["ICLR.cc/2026/Conference/Submission21784/Reviewer_Ka2m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21784/Reviewer_Ka2m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761128739, "cdate": 1761761128739, "tmdate": 1762941930033, "mdate": 1762941930033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "cOheTM6KPX", "forum": "qP7ryQmenw", "replyto": "qP7ryQmenw", "signatures": ["ICLR.cc/2026/Conference/Submission21784/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21784/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763577591688, "cdate": 1763577591688, "tmdate": 1763577591688, "mdate": 1763577591688, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TabKDE, a simple, scalable, and privacy-aware framework for generating good quality synthetic tabular data by carefully combining classical statistical tools (namely, copula transformations and kernel density estimation) without relying on complex deep generative models such as GANs, VAEs, or diffusion models. The approach maps mixed-type tabular data into a numerical encoding which is then transformed to a copula-based latent space, where KDE-based sampling of new synthetic latent data takes place. The synthetic latent values are then transformed back to synthetic mixed data via an inverse copula transformation followed by a decoding step.  While TabKDE does not achieve SOTA performance with regard to fidelity and privacy metrics it still performs reasonably well and is considerably more scalable than the more complex deep generative models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes an interesting new framework for generating synthetic tabular data. (It is refreshing to see work outside of deep generative models.)\n\nWhile none of the components used by TabKDE is new, the paper combines these components in a clever and neat way.\n\nIn addition to comparing TabKDE to popular baselines in the field, the paper also perform comparisons to several hybrid models that mix elements of TabKDE with diffusion and VAE models."}, "weaknesses": {"value": "The paper presents a combination of evaluation results obtained both from its own experiments and from prior publications (specifically Zhang et al., 2024, and Shi et al., 2025). However, some of these results are not directly comparable because different data splits were used. \n\nAs stated in lines 972–976 of the paper: \n\n“We consider two ways to split data into test and train set. The numbers in Table 9 reflects the splits done by TABSYN, which we maintain for direct comparison. This split was not even in size, partially to ensure there were no categories in the Test split which were not present in the Train split. When we do not directly compare to results in the Zhang et al. (2024), we use a different random and even split.”\n\nDespite this distinction, the $\\alpha$-precision and $\\beta$-recall results in Tables 22 and 23 include values extracted from Zhang et al., even though the captions indicate that the results for TabKDE (and several other baselines) were obtained using equal-sized splits. This means the reported comparisons are not strictly valid, as they mix results based on incompatible data partitioning schemes. To ensure fair and meaningful comparisons, the authors should either re-run the TabKDE and other baseline evaluations using the original Zhang et al. splits, or recompute all baseline results from Zhang et al. under the equal-sized split protocol.\n\n\nAdditional suggestions for improvement:\n\nSometimes the paper runs its own reproduction of the SMOTE and TabSyn baselines and sometimes it does not. For instance, the results for the marginal distribution alignment (Table 4), pairwise correlation alignment (Table 5), C2ST (Table 7), and DCR (Table 9) metrics are based on the paper’s own reproduction while for $\\alpha$-precision (Table 22) and $\\beta$-recall (Table 23) it is not.  The paper should fix these discrepancies. \n\nThe list of baselines is not consistent across all metrics. For instance, the paper compares against TabDiff in Table 8 but not in Tables 4, 5, and 7. These missing comparisons should be included. Furthermore, some baselines are only presented in the Appendix tables but not in the main text ones. The paper should harmonize (i.e., have the same set of baselines) across all the results. \n\nThe paper report in Tables 15, 17, and 20, average results and standard deviations (over 10 runs) only for the TabKDE generator and only for a few selected evaluation metrics. But it should report averages and SDs across 10 runs for all metrics and baseline models. \n\nIf possible, the paper should consider increasing the number of benchmark datasets. \n\nMinor suggestions:\n\nThe style of the citations is sometimes awkward. For instance, in line 40 the citation “Copula-based data generators Patki et al. (2016)” should be changed to “Copula-based data generators (Patki et al., 2016)”.\n\nTypos:\n\nLine 21: “larger than prior art” -> “larger than prior state-of-the-art” ?\n\nLine 35: “need to paired” -> “need to be paired”\n\nLine 60: “over come” -> “overcome”\n\nLine 66: “categorical values” -> “ordinal values”\n\nLine 142: “sample” -> “sampled”\n\nLine 160: “at randomly” -> “at random”\n\nLine 194: “can comparing” -> “can compare”\n\nLine 239: line 6 of algo 3 is missing parenthesis\n\nLine 296: “Noteably” -> “Notably”\n\nLine 377: Table 6 is misplaced\n\nLine 666: “at randomly” -> “at random”\n\nLine 676: “This means, do not” -> “This means, we do not”\n\nLine 780: “can mapped back” -> “can be mapped back”\n\nLine 791: “)}” -> “})”\n\nLine 793: “$z_ij$” -> “$z_{ij}$”\n\nLine 821: “computes” -> “compute”\n\nLine 860: “define out kernel” -> “define our kernel”\n\nLine 871: “And Moreover” -> “And moreover”\n\nLine 872: “beleive” -> “believe”\n\n\nINITIAL ASSESSMENT:\n\nAlthough TabKDE does not achieve state-of-the-art performance, its strong scalability offers a practical and compelling trade-off. Moreover, the paper contributes a fresh perspective to a field largely dominated by deep generative models. \n\nI am assigning a relatively low score at this stage due to the pending clarifications regarding the experimental evaluations. However, I believe these issues can be readily addressed, and I would be happy to raise my score once satisfactory clarifications are provided."}, "questions": {"value": "In addition to Tables 22 and 23, are there any other results generated with the alternative equal sized data splits? \n\nFor the comparisons with Zhang et al., did the paper, in addition to using the same data splits, also replicate other aspects of that refercence's evaluation pipeline (such as data preprocessing steps and related procedures) to ensure the results are comparable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jari9boeSA", "forum": "qP7ryQmenw", "replyto": "qP7ryQmenw", "signatures": ["ICLR.cc/2026/Conference/Submission21784/Reviewer_nGAe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21784/Reviewer_nGAe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978855736, "cdate": 1761978855736, "tmdate": 1762941929583, "mdate": 1762941929583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a scalable and simple tabular data generation method using KDE.\nThe proposed method, TabKDE, first encodes raw data into a Euclidean space.\nTo avoid one-hot encoding for categorical features, they apply principal vector-guided encoding, which projects each category along the main principal component direction of the numerical features, preserving semantic relationships between categories.\nThen, the embedded data are mapped into a continuous latent space $[0,1]^d$ using a copula transform, which normalizes each feature while maintaining inter-feature dependencies.\n\nFor sampling, they use a KDE-based generative process, where a training point in the latent space is randomly selected, a direction proportional to the data covariance is chosen, and a new point is generated by adding a small stochastic deviation. Out-of-bound points are re-sampled to stay within $[0,1]^d$, ensuring the validity of marginal distributions.\n\nThe authors also propose a coreset construction method that selects a small representative subset of the data to approximate the same density structure, improving scalability and computational efficiency.\n\nThey claim that the proposed method outperforms several baselines in multiple aspects, including scalability, efficiency, accuracy, and privacy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea of constructing a coreset to approximate the KDE distribution is simple yet fresh, effectively improving scalability without significant loss of accuracy."}, "weaknesses": {"value": "- $\\textbf{Inconsistent Baselines}$:\nThe comparisons are not standardized --  some baselines are re-implemented, others are taken from prior work, and several (e.g., GReaT, GOGGLE) lack reported metrics, making fair evaluation difficult.\n\n- $\\textbf{Unclear Privacy Criteria}$:\nThe marks in Table 1 are not based on consistent quantitative thresholds.\nModels with near-ideal DCR values (e.g., CoDi, STaSy) are marked x, while others with higher DCR (e.g., TabDDPM) are v without clear justification.\n\n- $\\textbf{Lack of Efficiency Evaluation}$:\nDespite claiming “simplicity and scalability,” the paper provides no quantitative metrics such as training time, sampling speed, or memory footprint. Scalability is discussed only qualitatively.\n\n\n- $\\textbf{Limited Empirical Analysis}$:\nThe paper lacks ablation studies or visualizations to isolate the impact of each component (copula transform, coreset construction, etc.).\n\n\n- $\\textbf{Presentation and Formatting Issues}$:\nThe presentation quality is uneven: several tables exceed page margins, algorithm listings are poorly aligned, and figure/table placement breaks text flow.\nThese layout inconsistencies make the paper harder to follow and reduce readability."}, "questions": {"value": "1. When sampling new points from the KDE, how are categorical features mapped back to discrete categories during the inverse copula transform?\n\n\n2. Since the KDE samples may slightly move outside the $[0, 1]^d$ bounds, how does the method ensure that categorical mappings and marginal ranges remain valid after inversion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no ethic concerns"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xYkTjinrc6", "forum": "qP7ryQmenw", "replyto": "qP7ryQmenw", "signatures": ["ICLR.cc/2026/Conference/Submission21784/Reviewer_sXrU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21784/Reviewer_sXrU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003394928, "cdate": 1762003394928, "tmdate": 1762941929164, "mdate": 1762941929164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "-"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "--"}, "weaknesses": {"value": "--"}, "questions": {"value": "The authors violated the margins as defined by ICLR 2026 template at page 7 hence my recommendation is desk reject"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "acd5cZoZTf", "forum": "qP7ryQmenw", "replyto": "qP7ryQmenw", "signatures": ["ICLR.cc/2026/Conference/Submission21784/Reviewer_kD6Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21784/Reviewer_kD6Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21784/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763556810221, "cdate": 1763556810221, "tmdate": 1763556810221, "mdate": 1763556810221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}