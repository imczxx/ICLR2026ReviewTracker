{"id": "Q4ooLNOFeR", "number": 10423, "cdate": 1758170913970, "mdate": 1759897651479, "content": {"title": "Does Higher Interpretability Imply Better Utility? A Pairwise Analysis on Sparse Autoencoders", "abstract": "Sparse Autoencoders (SAEs) are widely used to steer large language models (LLMs), based on the assumption that their interpretable features naturally enable effective model behavior steering. Yet a fundamental question remains: does higher interpretability imply better steering utility? To answer this, we train 90 SAEs across three LLMs (Gemma-2-2B, Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels. We evaluate interpretability with SAEBench (Karvonen et al., 2025) and steering utility with AxBench (Wu et al., 2025), and analyze rank agreement via Kendall’s rank coefficient $\\tau_b$. Our analysis reveals only a relatively weak positive association ($\\tau_b \\approx 0.298$), indicating that interpretability is an insufficient proxy for steering performance. We conjecture the interpretability–utility gap stems from feature selection: not all SAE features are equally effective for steering. To identify features that truly steer LLM behavior, we propose a novel selection criterion, $\\Delta$ Token Confidence, which measures how much amplifying a feature changes the next-token distribution. Our method improves steering performance on three LLMs by **52.52\\%** compared to the best prior output score–based criterion (Arad et al., 2025). Strikingly, after selecting features with high $\\Delta$ Token Confidence, the correlation between interpretability and utility vanishes ($\\tau_b \\approx 0$) and can even become negative, further highlighting their divergence for the most effective steering features.", "tldr": "", "keywords": ["Sparse Autoencoders; Interpretability; Utility"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b39c768c7002a220ca72807dbbd06a52a2baefd6.pdf", "supplementary_material": "/attachment/84c759cbc9f7fd5a8857649d1164a7ddd80edcf7.zip"}, "replies": [{"content": {"summary": {"value": "This work investigates the correlation of interpretability and steering utility of SAE latents by comparing rankings via  kendall's tau coefficient, and finds low correlation. After introducing a new method for selecting SAE features for steering, the correlation surprisingly vanishes."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- This paper provides quantitative evidence that interpretability scores are not indicative of steering utility. This contribution is relevant to the field -- interpretability is often treated as a universal score for the quality of learned features. I personally updated on putting less weight on automated interpretability as a measure for overall SAE quality.\n- The writing is very clear, highlighting key takeaways, and motivating experiments.\n- They train a wide range of SAEs\n- Controlling for confounders when computing the correlation.\n- The paper uses established metrics for interpretability (SAEBench) and steering success (AxBench)"}, "weaknesses": {"value": "- Looking at any code file in the anonymous repo link throws the error \"The requested file is not found.\""}, "questions": {"value": "- Should we use SAE latents for steering at all? How does the delta token confidence based steering compare to non-SAE based steering methods from AxBench?\n- Are you open-sourcing the trained suite of SAEs?\n- Can you provide max-activating inputs for features selected by delta-token-confidence? I'd be interested to see whether they are interpretable at all.\n- Based on your results, do you deem interpretability scores as actively misleading, should we only rely on steering ability when judging SAE quality? Or are interpretability and steering utility rather complementary characteristics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yhwCvJ5Mdv", "forum": "Q4ooLNOFeR", "replyto": "Q4ooLNOFeR", "signatures": ["ICLR.cc/2026/Conference/Submission10423/Reviewer_vNrz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10423/Reviewer_vNrz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924103641, "cdate": 1761924103641, "tmdate": 1762921732792, "mdate": 1762921732792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies two aspects of SAEs evaluation: the interpretability aspect and the utility aspect (especially steering). The authors have conducted correlation analysis on 90 SAEs from 3 base models and find that there is little to no correlation between interpretability and utility scores. The authors also propose a new method that improves steering performance, and find the new method further weakens the correlation between interpretability and utility."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This work offers an interesting perspective on SAE evaluation. In particular, the authors frame it as a tension between how interpretable the features are, i.e., interpretability score, and how useful the features are for steering, i.e., utility score. This framing could inspire new evaluation metrics and design on SAEs.\n* The authors have conducted extensive experiment on 90 SAEs over 3 base models that cover common SAE architecture variations. This provides solid emperical evidence."}, "weaknesses": {"value": "*  While the paper frames the findings as a tension between \"interpretability\" and \"utility\", the experiment operationalized \"interpretability\" and \"utility\" in a much narrower sense, i.e., as performance on SAEBench (the auto-interp task) and AxBench (the steering task).\n    * For \"Interpretability\", it is unclear what exactly \"interpretability\" entails in the paper. The auto-interp score is not an intrinsic metric to the feature space learned by SAE, but rather confounded with the quality of the auto-interp pipeline and how closely it is aligned to human understandable concept space.\n    * The \"utility\" is exclusively measured as \"steering utility\", while there are clearly other downstream applications of SAEs. For example, [Peng et al. 2025](https://arxiv.org/pdf/2506.23845) argues that SAE could be used on discovering unknown concepts. I would encourage the authors to be precise here and quantify the scope of study to match what actually done in the paper, for example, in the paper title, directly stating \"steering utility\" instead of \"utility\".\n\n* The authors experimented with how different SAE architecture/features affect the gap, however, there is still a bit lack of insights on why there is a gap between \"interpretability\" and \"utility\". In particular, both interpretability score and utility score are extrinsic evaluation metrics that measure the success of a system as a whole, with SAE being one component. For example, interpretability score relies on the auto-interp pipeline, which is known to produce inaccurate description sometimes and these descriptions are often biased towards the role of the feature space in detecting concepts in inputs. Thus, is the gap between interpretability and utility a failure of the auto-interp pipeline, or more of a fundamental problem of SAEs?"}, "questions": {"value": "Please see the weakness. In particular, it would be great if authors could provide more insights/analyses into why there is a gap between interpretability and utility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7oJr5uDJop", "forum": "Q4ooLNOFeR", "replyto": "Q4ooLNOFeR", "signatures": ["ICLR.cc/2026/Conference/Submission10423/Reviewer_2ZdN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10423/Reviewer_2ZdN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964168023, "cdate": 1761964168023, "tmdate": 1762921732042, "mdate": 1762921732042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers sparse autoencoders (SAEs), a popular technique in mechanistic interpretability, and seeks to uncover the relationship between the interpretability of a latent (i.e. to what extent the latent corresponds to a robust concept) and the utility of the latent (its ability to steer downstream generation towards or away from that concept). The paper uses two popular benchmarks to encode these ideas: SAEBench for interpretability and AxBench for utility. Despite the conventional wisdom being that interpretability and utility would be strongly correlated, the paper finds only a weak positive association (0.298). Furthermore, after filtering to only the top latents in terms of their impact on the logit distribution, the association drop to near 0 (whereas one might expect it to be positive). This is a counterintuitive result. The study is comprehensive across 90 SAEs from different architectures, sparcities etc."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The study is comprehensive. It considers 90 SAEs with varying architectures and sparsity levels. This is a strong point of the paper since the results are generalisable to all SAEs. The one axis that isn't varied is the dictionary size, always being 16k; however, there is sufficient variation elsewhere.\n- The paper is generally well-written and easy to read. Figures 1 and 2 are informative.\n- The methodology used allows a hierarchical analysis of differences between models of different architectures, etc. I have questions about the statistical significance of these results (see next section), but this analysis is really nice since it allows claims of which architecture provides the smallest interpretability-utility gap. \n- Fundamentally, this is an unexpected result and should be of interest to the community. It is odd that latents which are more interpretable are not necessarily more useful for steering!"}, "weaknesses": {"value": "I've ordered these by importance.\n- **The definition of interpretability:** My main concern is around the definition of interpretability of a latent. The paper assumes a *correlational scoring* approach following one of the methods set out in [1]. Quoting from the paper under review, \"the judge drafts the description from examples and then predicts, on a held-out set\" (L135). This is correlational because it only considers when the latent activates and when it doesn't. The paper writes like this is the default definition of interpretability, and then compares the derived scores to the utility scores. However, in reality, people often evaluate interpretability using an *intervention scoring* method as well as just the correlational method [1]. To quote from Paulo et al. 2025 (the same paper used to define the AutoInterp metric: \n> \"intervention scoring, evaluates the interpretability of the effects of intervening on a feature, which we find explains features that are not recalled by existing methods.\"\n\n    In short, the authors' operationalisation of *utility* is often used by other papers as a way of measuring *interpretability*. As a result, the distinction of interpretability/utiliy is not as obvious as it may seem. It remains a very interesting result that interpretability from correlational scoring seems to be unrelated to interpretability from interventional scoring, or utility, but the phrasing of the result needs a bit of work in my opinion.\n\n- **Statistical testing:** Many of the results rely on the point value of the $\\tau_b$; however, this would benefit from showing standard errors and doing hypothesis tests to examine whether it is statistically significantly different from 0. In particular, (i) when we change it to only consider those with high Token Confidence, is the change in $\\tau_b$ statistically significant? (ii) When you compare different architectures, are any of those changes statistically significant? (iii) In Figure 4, if we add error bars to the plot, are any of the comparisons statistically significant? This seems really important to support the main results. Table 3 does show standard errors for the axis-controlled summaries, but the CIs are quite large, which makes me suspect that some of the results reported (e.g. the comparisons between architectures) are not significant.\n- **Clarification on why the gap matters:** Most readers probably work it out quickly, but explicitly describing why the gap between interpretability and utility matters for downstream tasks would be nice. (In fact, explicitly defining \"the gap\" would also be helpful).\n- **Examples:** As mentioned, I think this main result is weird and unexpected, which makes it interesting! It would be great to see some examples of features that are interpretable but not steerability, and vice versa. \n- **How do you select $k$?** A smaller point, but how do you establish $k$ in the token confidence calculation? In the appendix, it says $k=1$ for all experiments. Is this the correct interpretation? If so, I feel this should be in the main text, and it feels somewhat limiting given that the metric is explained in the context of changing the logit distribution. \n- **Writing:** In general, the writing is strong, but there are a few places where metrics are not clearly defined, e.g. *CONCEPT100*. The paper sends the reader to the appendix for a definition, but it would be best to define these terms in the main body.\n\nThis paper studies an interesting question and has a lot of potential, but I feel there are some critical questions that need to be addressed before it is ready for publication.\n\n**Typos:**\n- L053 missing year in citation.  \n- L212 citation capitalisation\n\n---\n[1] Paulo et al., 2025"}, "questions": {"value": "- I was confused by *steering gain* (sec. 4.3). From my understanding, this is a different metric to *steering score* in the earlier parts of the paper. If this is correct, are we calculating the same metric $tau_b$ in the section case (filtering for only the top token confidence latents)? If not, are these two $tau_b$ metrics directly comparable? The way I interpreted the method from the description is that we would filter for the top tokens by confidence latents, and then calculate the exact same metric, but I don't think this is the case, right? Thanks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ocxgJ0JhTU", "forum": "Q4ooLNOFeR", "replyto": "Q4ooLNOFeR", "signatures": ["ICLR.cc/2026/Conference/Submission10423/Reviewer_MCfE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10423/Reviewer_MCfE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985404840, "cdate": 1761985404840, "tmdate": 1762921731525, "mdate": 1762921731525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the extent to which SAE interpretability metrics correlate with how effective an SAE's latents are for steering, finding a small positive association. They then introduce a metric—a feature's steering effect on the log probability of the most probable tokens—that selects for features that are very effective for steering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The finding that an SAE's interpretability score doesn't correlate well with steering efficacy is interesting.\n2. The authors sweep over a comprehensive set of SAEs from different models."}, "weaknesses": {"value": "1. The result that SAE interpretability score isn't well-correlated with steering score is interesting, but I think it would be much more valuable to present an analysis of why this is the case. Do interpretable SAE features tend to have negligible steering effects? Do they tend to cause degeneration in instruction following? I feel like this result is mainly interesting insofar as it raises follow-up questions, none of which are addressed here.\n2. Similarly for the result on delta token confidence selecting for features with a high steering score: Why specifically does this work? Is it because it's selecting for features that have a non-negligible steering effect? Why doesn't this metric surface many features that make model outputs incoherent? In this case, I don't feel like the stand-alone result is very important, but it could be used as a springboard for additional analysis.\n3. The result on delta token confidence selecting for high steering score is presented as an improvement on selecting on interpretability, but in fact this is a very apples-to-oranges comparison. In one case, the result is that selecting for *SAEs* with a high interpretability score doesn't result in *SAEs* with a high steering score; in the other case, the selection is being done at the level of features. An apples-to-apples comparison could be to show that *SAEs* whose average feature has a high delta token confidence have high steering scores (computed in terms of Kendall's rank coefficient). Alternatively, you could test whether selecting for interpretability at the feature level also selects for high steering score. At minimum, you should remove claims like \"This result validates the superiority of our method\" given that you haven't actually demonstrated that delta token confidence is superior to anything. (Unless you mean that delta token confidence is superior to the other methods in table 2, in which case that should be made clear. Also, if the comparison in table 2 is being treated as a core contribution, then the authors should (1) explain what the Arad et al. (2025) S_out metric is, given that it's very similar to the delta token confidence metric used here, (2) present some analysis of why delta token confidence is more effective than S_out, and (3) motivate why introducing metrics that select for high steering score features is important.)\n4. The equation given for tau_b is actually the equation for the non-tie-corrected tau_a; it's unclear which one the authors actually used.\n5. It's possible that I'm misunderstanding something here, but the descriptions in the abstract and introduction of the result in section 4.3 seems incorrect. In the abstract it says \"Strikingly, after selecting features with high ∆ Token Confidence, the correlation between interpretability and utility vanishes (τb ≈ 0)\". However, the actual result seems to be  that selecting for high interpretability score *SAEs* does not select for *SAEs* such that \"steering gain\" is large. I can't actually tell what \"steering gain\" is in section 4.3 (it's defined as \" the percentage lift of the selected-steering score over the same SAE’s base\" which is too vague for me to parse), but the same term is used elsewhere in the paper to mean \"the increasing in steering score after selecting for high delta token confidence features.\""}, "questions": {"value": "Why are so many of the confidence intervals in table 1 identical? E.g. the upper confidence bound for every row in the \"Sparsity\" section is 0.3714 despite different tau_b values."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6fKXfGXSy8", "forum": "Q4ooLNOFeR", "replyto": "Q4ooLNOFeR", "signatures": ["ICLR.cc/2026/Conference/Submission10423/Reviewer_AA1u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10423/Reviewer_AA1u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056183478, "cdate": 1762056183478, "tmdate": 1762921731101, "mdate": 1762921731101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}