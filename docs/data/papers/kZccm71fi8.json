{"id": "kZccm71fi8", "number": 15399, "cdate": 1758250948585, "mdate": 1763725453746, "content": {"title": "Training Spiking Neural Networks with Real-Time Propagation Through Time", "abstract": "Online learning algorithms for Spiking Neural Networks (SNNs) offer a memory-efficient alternative to Backpropagation Through Time (BPTT), but suffer from two critical issues: training instability and membrane potential distribution drift. To address these challenges, we introduce Real-Time Propagation Through Time (RPTT), a novel online learning framework. RPTT computes gradients using only the spatial component and integrates two synergistic regularization mechanisms: Membrane Potential Distribution Regularization (MPDR), which statistically constrains membrane potentials to counteract distributional drift, and Spatio-Temporal Gradient Regularization (STGR), which smooths weight updates to ensure stable convergence. We theoretically prove that RPTT converges to a stationary point. Extensive experiments on CIFAR-10/100, ImageNet-1k, and DVS-CIFAR10 demonstrate that RPTT achieves state-of-the-art performance while significantly reducing memory consumption. Experimental analysis reveals that RPTT achieves strong performance by effectively alleviating the membrane potential drift. Our work thus provides an effective framework for the online training of SNNs, significantly advancing their application in dynamic and realistic environments.", "tldr": "We propose Real-time Propagation Through Time, a memory-efficient online training algorithm for SNNs that stabilizes convergence and mitigates drift of membrane potential distribution.", "keywords": ["Spiking Neural Networks", "Online Learning", "Real-time Propagation Through Time", "Drift of membrane potential distribution"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97b91e8316360676b60e53d150212b3679e3ee5a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the RPTT framework to address two critical challenges in online SNN training: instability and membrane potential drift. The method leverages an efficient spatial-gradient-only update scheme, augmented by two novel regularizers: Membrane Potential Distribution Regularization (MPDR) to counteract distributional drift and Spatio-Temporal Gradient Regularization (STGR) to stabilize the training process."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper offers a well-motivated and empirically effective solution to the important problem of online SNN training. The proposed regularizers, MPDR and STGR, are cleverly designed. The authors provide convincing experimental analysis to demonstrate their efficacy in mitigating membrane potential drift and stabilizing learning dynamics."}, "weaknesses": {"value": "1. The paper fails to specify how the backpropagation for the STGR term, particularly $\\|\\nabla\\ell_{t-1}(W_{t}^{l})\\|^{2}$, is implemented. Differentiating this term with respect to $\\mathbf{W}_{t}$ would seemingly require second-order information (i.e., a Hessian-vector product), which contradicts the paper's claim of O(N) memory complexity and the core goal of efficient online learning. \n\n2. The experimental evaluation lacks comparisons against several recent and highly relevant online learning methods, such as NDOT (Jiang et al., 2024) and OSR/OTS (Zhu et al., 2024). \n\n3. The authors claim the method is suitable for \"dynamic environments,\" but the only dynamic dataset used is CIFAR10-DVS, which exhibits weak temporal correlations. The experiments do not provide sufficient evidence to support its efficacy in truly non-stationary or dynamic settings.\n\n4. The ImageNet experiment is conducted by fine-tuning a pre-trained SLTT model rather than training from scratch. This severely weakens the claim of state-of-the-art performance on large-scale tasks. Disturbingly, this crucial detail is relegated to Appendix A.2.4 and omitted from the main manuscript, which could be misleading.\n\n5. As reported in Appendix A.2.4, the hyperparameters for RPTT vary significantly across different datasets. This high sensitivity to parameter tuning suggests that the method may lack generalizability and could be difficult to apply to new tasks without extensive tuning.\n\n6. The convergence proof for Theorem 1 relies on a critical assumption: a regularization term attenuation coefficient $c_t$ (Eq. 20) that decays over time. However, the experimental setup uses fixed, constant regularization parameters."}, "questions": {"value": "1. Please provide a detailed complexity analysis for the STGR term. How is the gradient of $\\|\\nabla\\ell_{t-1}(\\mathbf{W}_{t}^{l})\\|^{2}$ calculated in practice, and how does this align with the claimed O(N) memory complexity?\n\n2. Please justify the choice of a fixed target distribution q=N(-60, 10) for all layers and datasets. I strongly suggest authors include a sensitivity analysis on these hyperparameters to demonstrate the robustness and generalizability of your method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vg2YAENWld", "forum": "kZccm71fi8", "replyto": "kZccm71fi8", "signatures": ["ICLR.cc/2026/Conference/Submission15399/Reviewer_7eFK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15399/Reviewer_7eFK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761188540772, "cdate": 1761188540772, "tmdate": 1762925677712, "mdate": 1762925677712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RPTT, an online learning method for SNN learning. RPTT introduces MPDR to counteract distributional drift and STGR to ensure stable convergence during online learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Online learning methods save memory during training, which have constant memory cost with the number of time steps $T$.\n2. This work provides theoretical proof of the stableness of RPTT.\n3. The writing of this paper is easy to understand."}, "weaknesses": {"value": "The final performance of this work is not high enough. Specifically, Table 1 does not include results of NDOT (Jiang et al. 2024) and OSR+OTS (Zhu et al. 2024), which generally perform better than this work."}, "questions": {"value": "1. What is the necessity of updating weights at each step in SNN online learning? I don't see its advantage.\n2. In Eq.4, authors say that $Var(V_t^l)$ is used to penalize overly small variance, but its coefficient is positive. Should the sign be '-'?\n3. The STGR loss (Eq. 5) is similar to the loss in FPTT[1]. Could you give some comments on the difference between them?\n4. Why the membrane potential is bimodal in Figure 4? It differs from the Gaussian target distribution in MPDR.\n\n[1] Kag, A., & Saligrama, V. (2021, July). Training recurrent neural networks via forward propagation through time. In *International Conference on Machine Learning* (pp. 5189-5200). PMLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PMppubbxRL", "forum": "kZccm71fi8", "replyto": "kZccm71fi8", "signatures": ["ICLR.cc/2026/Conference/Submission15399/Reviewer_jPpw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15399/Reviewer_jPpw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761188729051, "cdate": 1761188729051, "tmdate": 1762925677234, "mdate": 1762925677234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RPTT, an online training method for SNNs with two regularizers: Membrane Potential Distribution Regularization (MPDR) and Spatio-Temporal Gradient Regularization (STGR). The authors provide a convergence argument, and present results on both static and neuromorphic datasets showing the memory efficiency and competitive performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Thoughtful empirical study. Experiments are conducted on both static and neuromorphic datasets, with ablations and distributional analyses.\n2. Theoretical analysis. The paper attempts at a convergence analysis, showing the importance of the proposed techniques for deriving a kind of convergence."}, "weaknesses": {"value": "1. Notation confusion. The paper overloads $t$ to mean both the SNN time step and the optimization iteration, making the description confusing.\n2. Strong theoretical assumption. The theoretical analysis assumes a loss sequence with $|l_{t+1}(W)-l_t(W)|<\\Delta_t$ and $\\sum_t \\Delta_t < \\infty$, which is essentially an asymptotically stationary setting and almost implies convergence. As a result, the theorem contributes limited new insights.\n3. Incremental novelty and practical gains. Using spatial-only gradients with low memory costs is already present in previous works. MPDR and STGR are mainly incremental regularizers rather than new training principles. In the largest-scale ImageNet experiment, the proposed method actually has no gain compared with SLTT."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qsEJtgnup3", "forum": "kZccm71fi8", "replyto": "kZccm71fi8", "signatures": ["ICLR.cc/2026/Conference/Submission15399/Reviewer_qzmx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15399/Reviewer_qzmx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935198680, "cdate": 1761935198680, "tmdate": 1762925676643, "mdate": 1762925676643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Real-Time Propagation Through Time (RPTT), a novel online learning algorithm for Spiking Neural Networks (SNNs). RPTT aims to address two key challenges in online SNN training: training instability and membrane potential distribution drift. The core of RPTT is the use of only spatial gradients for parameter updates, augmented by two synergistic regularization mechanisms:\n\nMembrane Potential Distribution Regularization (MPDR): A statistical constraint that uses KL-divergence and a variance penalty to keep membrane potentials close to a target Gaussian distribution, counteracting drift.\n\nSpatio-Temporal Gradient Regularization (STGR): A smoothing mechanism that uses a moving average of weights and a penalty on previous gradients to stabilize updates and suppress noise.\n\nThe authors provide a theoretical convergence guarantee for RPTT and demonstrate its effectiveness on static (CIFAR-10/100, ImageNet-1k) and neuromorphic (DVS-CIFAR10) datasets, showing it achieves competitive or state-of-the-art performance while significantly reducing memory consumption compared to BPTT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The explicit formulation of the membrane potential drift problem in the context of online learning and the proposal of MPDR as a direct, layer-wise solution is a novel and valuable contribution. While drift has been studied in offline settings, its exacerbation by frequent online updates is a fresh and meaningful insight. The combination of MPDR with STGR to jointly address drift and instability is a creative and well-motivated design.\n\nQuality: The paper is technically sound. The experimental section is comprehensive, covering multiple datasets and network architectures. The inclusion of a theoretical convergence analysis, despite relying on some strong assumptions, adds rigor and depth to the work. The ablation studies and membrane potential visualization (e.g., the emergence of a bimodal distribution) provide empirical support for the method's mechanisms.\n\nSignificance: The work addresses a critical bottleneck for the practical deployment of SNNs: achieving memory-efficient and stable online training. By significantly reducing memory overhead (O(N) vs. BPTT's O(TN)) and mitigating a fundamental performance-limiting phenomenon (drift), RPTT represents a tangible step towards making SNNs viable for dynamic, real-world applications on resource-constrained neuromorphic hardware."}, "weaknesses": {"value": "Insufficient Comparison with Related Work: The paper's academic impact is limited by its superficial comparison with existing methods. The performance comparison in Table 1 is useful but does not provide insights into why RPTT performs better.\n\nMissing Analysis on Membrane Potentials: A core claim is that RPTT better mitigates drift. However, there is no direct, quantitative comparison of membrane potential distributions (e.g., Z-score trajectories, KL-divergence from target) between RPTT and other online methods like OTTT or SLTT. The analysis in Fig 3(b) only compares RPTT with BPTT and OSBP (a weak baseline). Does SLTT, which uses delayed updates, also suffer less from drift than OSBP? How does RPTT compare to OTTT in this regard? This is a major missed opportunity to validate the central motivation.\n\nBaseline Currency: While some recent methods are included (e.g., SLOT, NDOT), the choice of the primary online baseline, OSBP, is weak. OSBP is not a established, strong benchmark from the literature. A more convincing comparison would involve directly integrating MPDR and STGR into a stronger and more recent online method like OTTT or the framework of SLOT to perform an ablation, demonstrating the generalizability and additive value of the proposed regularizers.\n\nWeak and Partially Inaccurate Motivation:\n\nThe statement \"to the best of our knowledge, the problem of membrane potential distribution drift has not yet been studied under online learning algorithms\" (Page 2) is too strong. While perhaps not the primary focus, works like Zhu et al. (2024) (\"Online Stabilization of Spiking Neural Networks\") directly address firing rate stability across time, which is intrinsically linked to membrane potential distribution. The authors should tone down this claim and more precisely articulate their unique focus on distributional drift via online, layer-wise regularization.\n\nThe motivation would be stronger if it included a preliminary analysis showing that existing online methods (OTTT, SLTT) indeed exhibit more severe drift than offline BPTT, thereby creating a clear gap that RPTT fills.\n\nLimited Ablation Study:\n\nThe ablation in Section 4.3 only reports final accuracy. It does not quantify the individual contribution of MPDR and STGR to reducing distribution drift. For instance, how much does the Z-score improve with MPDR alone? How does STGR alone affect the variance of the membrane potential distribution? Linking each component's effect directly to the underlying problem it is designed to solve would greatly strengthen the paper.\n\nTheoretical Limitations:\n\nThe convergence proof relies on strong assumptions (e.g., $\\beta$-smooth loss, bounded gradients, Gaussian membrane potentials) that may not hold perfectly in practice. A discussion of these limitations and the proof's practical relevance would be beneficial.\n\nThe assumption that the task sequence change is bounded ($\\sum \\Delta_t < \\infty$) is particularly strong for a non-stationary online learning setting and deserves clarification."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ThdpX69zD1", "forum": "kZccm71fi8", "replyto": "kZccm71fi8", "signatures": ["ICLR.cc/2026/Conference/Submission15399/Reviewer_JxZ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15399/Reviewer_JxZ2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965687657, "cdate": 1761965687657, "tmdate": 1762925676144, "mdate": 1762925676144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}