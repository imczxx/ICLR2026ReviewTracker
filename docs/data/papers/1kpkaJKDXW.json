{"id": "1kpkaJKDXW", "number": 8532, "cdate": 1758089933269, "mdate": 1763181373757, "content": {"title": "Learnable Tokenization for DNA Foundational Models", "abstract": "DNA language models have emerged as powerful tools for decoding the complex language of DNA sequences. However, the performance of these models is heavily affected by their tokenization strategy, i.e., a method used to parse DNA sequences into a shorter sequence of chunks. In this work, we propose DNAChunker, that integrates a learnable, dynamic DNA tokenization mechanism and is trained as a masked language model. Adopting the dynamic chunking procedure proposed by Hwang et al. (2025), our model learns to segment sequences into variable-length chunks. This dynamic chunking offers two key advantages: it's resilient to shifts and mutations in the DNA, and it allocates more detail to important functional areas. We demonstrate the performance of DNAChunker by training it on the human reference genome (HG38) and testing it on the Nucleotide Transformer and Genomic benchmarks. Further ablative experiments reveal that DNAChunker learns tokenization that grasps biological grammar and uses smaller chunks to preserve detail in important functional elements such as promoters and exons, while using larger chunks for repetitive, redundant regions.", "tldr": "", "keywords": ["Machine Learning", "Deep Learning", "DNA Language Modeling"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0af9dfc7c2c6f83b726b55f01c5f0fb02aedb335.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper adopts the adaptive tokenization idea from H-Net [1] and introduces two incremental improvements: (1) the pass-through of special tokens to the main model, and (2) a cross-attention-based dechunking scheme that replaces the original smoothing module. The authors name the resulting method DNACHUNKER and claim it achieves state-of-the-art performance in an old and easy benchmark. (In the following, I will refer to the two improvements as (1) and (2)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper demonstrates a reasonable understanding of biological concepts. The interpretability analysis employs high phyloP scores and Repeat SINE, and Figure 3 uses a gene-track-style visualization that makes the results more accessible to biological researchers.\n\n2.\tTechnically interesting but positioned as an incremental contribution. This paper proposes a learnable dynamic chunking approach. However, H-Net has already applied adaptive tokenization to DNA sequences, and prior work such as MxDNA [2] has also introduced the concept of adaptive DNA tokenization."}, "weaknesses": {"value": "1.\tFrom a machine learning perspective, the proposed improvements are incremental, and the evaluation is insufficient to demonstrate clear advantages. Adaptive and learnable tokenization methods in DNA modeling are not new—MxDNA, H-Net (which already includes DNA applications), and Life-Code [3] have each proposed distinct technical path. DNACHUNKER builds directly upon H-Net with two trick and yet provides limited ablation studies (only a brief one in Figure 2, compare DNACHUNKER and H-Net).\n\nImprovement suggestions:\n\ni. In Tables 1–2 and Figures 3–4, comparisons should include H-Net, w/o (1), and w/o (2) variants to quantify the contributions of the two claimed improvements.\n\nii. The authors should also compare with other adaptive tokenization methods—at minimum with MxDNA, a peer-reviewed and published approach. Similarly, biological interpretability comparisons in Figures 3–4 should include MxDNA for completeness.\n\n2.\tFrom the perspective of genomic model evaluation, the benchmarks used remain outdated, still focusing on short-sequence binary or ternary classification tasks typical of the DNABERT/DNABERT-2 era. Such simplistic feature-recognition tasks can often be solved by linear classifiers and no longer reflect current research standards.\n\nBy late 2025, evaluation of DNA foundation models has shifted toward regulatory genomics. For example:\n\nThe Nucleotide Transformer (published December 28, 2024) [4] introduced tasks such as eQTL, meQTL, and mutation effect prediction, showcasing the potential of large models for studying gene-regulatory mechanisms.\n\nThe July 24, 2025 paper “Evaluating the representational power of pre-trained DNA language models for regulatory genomics” [5] proposed a benchmark encompassing cell-type-specific functional genomics data for both DNA and RNA regulation.\n\nThis study revealed that current DNA pre-trained models often fail to produce embeddings that outperform raw k-mer representations.\n\nImprovement suggestions:\n\ni. Following [5], the authors could compare the pre-trained embeddings derived from k-mer, MxDNA, H-Net, and DNACHUNKER tokenizations when directly used as model inputs for downstream tasks, testing whether DNACHUNKER offers superior representational quality.\n\nii. Similarly, they could evaluate whether the output representations from models pre-trained with these different tokenizations (MxDNA, H-Net, DNACHUNKER) perform better than raw k-mer embeddings on downstream benchmarks [5].\nSuch evaluations would provide the genomics modeling community with clearer, knowledge-driven insights into the effectiveness and necessity of adaptive tokenization schemes.\n\nOverall, the paper fails to provide new knowledge or meaningful contributions to either the machine learning or biological research. The method proposed in this paper is incremental, offering limited novelty or insight. The comparative experiments are incomplete, and the ablation study is too less to explain the advantages of the proposed improvements. The benchmark datasets used for evaluation are outdated and overly simplistic.\n\n[1] Hwang S, Wang B, Gu A. Dynamic chunking for end-to-end hierarchical sequence modeling[J]. arXiv preprint arXiv:2507.07955, 2025.\n\n[2] Qiao L, Ye P, Ren Y, et al. Model decides how to tokenize: Adaptive dna sequence tokenization with mxdna[J]. Advances in Neural Information Processing Systems, 2024, 37: 66080-66107.\n\n[3] Liu Z, Li S, Chen Z, et al. Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification[J]. arXiv preprint arXiv:2502.07299, 2025.\n\n[4] Dalla-Torre H, Gonzalez L, Mendoza-Revilla J, et al. Nucleotide Transformer: building and evaluating robust foundation models for human genomics[J]. Nature Methods, 2025, 22(2): 287-297.\n\n[5] Tang Z, Somia N, Yu Y, et al. Evaluating the representational power of pre-trained DNA language models for regulatory genomics[J]. Genome Biology, 2025, 26(1): 203."}, "questions": {"value": "Please refer to the points raised in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KQfKpDtTKu", "forum": "1kpkaJKDXW", "replyto": "1kpkaJKDXW", "signatures": ["ICLR.cc/2026/Conference/Submission8532/Reviewer_nFMS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8532/Reviewer_nFMS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809124109, "cdate": 1761809124109, "tmdate": 1762920392600, "mdate": 1762920392600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely thank the reviewers for the time and effort dedicated to evaluating our work. The feedback provided was insightful and constructive. \n\nAfter careful consideration, we have concluded that the rebuttal period is insufficient to fully address the reviewers' concerns. Thus, we believe it is best to withdraw the submission at this time, and will incorporate the comments in our future manuscript. Thankyou once more for your feedbacks.\n\nSincerely, Authors."}}, "id": "RA5HLCXzGh", "forum": "1kpkaJKDXW", "replyto": "1kpkaJKDXW", "signatures": ["ICLR.cc/2026/Conference/Submission8532/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8532/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763181372911, "cdate": 1763181372911, "tmdate": 1763181372911, "mdate": 1763181372911, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DNACHUNKER, an DNA language model that integrates a learnable dynamic DNA tokenization mechanism. The paper adapts the dynamic chunking mechanism for the masked language model with bi-directional Caduceus and cross-attention layers. The paper claims to achieve the state-of-the-art performance on DNA benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is overall reasonable. Though the contribution of leveraging SSM is incremental, it is somewhat novel in DNA field.\n2. The proposed method is logically solid and the experiment results that indicate the proposed contribution is the primary source of the performance gain.\n3. The results demonstrate strong parameter efficiency and competitive accuracy across relevant benchmarks.\n4. The manuscript is clearly written, and the architecture and pipeline figures effectively support and clarify the method."}, "weaknesses": {"value": "1. The evaluation suite is not fully aligned with the claimed advantages, and it should add high-value tasks that directly probe robustness and multi-scale context such as SNV variant-effect prediction (ClinVar-style VEP), deep mutational scanning, MPRA promoter/enhancer assays, and long-range regulatory benchmarks.\n2. The evidence base is incomplete because there is no head-to-head comparison against strong recent DNA LMs (e.g., Evo1, Evo2) with matched data, training tokens, and compute budgets.\n3. Clarity and reproducibility can be improved by standardizing figure styles, adding explicit axis labels and units (bp), defining acronyms on first use."}, "questions": {"value": "1. Can the authors add head-to-head comparisons against recent DNA LMs with matched data, training tokens, and compute budgets?\n2. The authors claim emphasize robustness and multi-scale context. Can the authors include SNV variant-effect prediction and DMS assays to directly probe these properties?\n3. How is the generalization of the proposed method across species or cell types (e.g., train on human, test on mouse/yeast) or across tasks without re-learning chunks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FpVjPcP80e", "forum": "1kpkaJKDXW", "replyto": "1kpkaJKDXW", "signatures": ["ICLR.cc/2026/Conference/Submission8532/Reviewer_qiYg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8532/Reviewer_qiYg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818977134, "cdate": 1761818977134, "tmdate": 1762920392337, "mdate": 1762920392337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DNACHUNKER, a learnable tokenization scheme for genomic sequences integrated into a bidirectional masked-language model. The model adapts token boundaries that adaptive chunks align better with functional heterogeneity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces a principled, learned tokenization for DNA with hierarchical chunking and de-chunking tightly coupled to a bidirectional MLM, beyond frequency-driven BPE and fixed k-mer schemes.\n2. Good analysis for functional heterogeneity (smaller chunks in functional regions, larger ones in repetitive regions)."}, "weaknesses": {"value": "1. The paper argues that learned tokenization is more robust to small variation but does not include SNV-level benchmarks commonly used to test base-resolution modeling. Without comparisons to other DNA foundation models (e.g., NT, HyenaDNA, Evo), the central claim remains under-supported.\n\n2. On Genomic benchmarks, important baselines are absent (e.g.,  NT and DNABERT-2). DNACHUNKER is not uniformly superior. The paper should show a complete leaderboard for the chosen tasks and discuss why it underperforms.\n\n3. The method is motivated by efficiency and long-range dependency capture, yet there are no direct comparisons on long-context tasks (e.g., BEND or LRB benchmark).\n\n\n4. No sensitivity to ratio-loss targets or boundary thresholding. The tokenization robustness metric uses hand-set mixing weights (e.g., for boundary Jaccard vs. content similarity) without sensitivity analysis."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "u5QiWDCzTv", "forum": "1kpkaJKDXW", "replyto": "1kpkaJKDXW", "signatures": ["ICLR.cc/2026/Conference/Submission8532/Reviewer_54yt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8532/Reviewer_54yt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881718831, "cdate": 1761881718831, "tmdate": 1762920391634, "mdate": 1762920391634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces DNAChunker, a learnable dynamic chunking mechanism that segments genomic sequences into variable-length fragments to enhance computational efficiency and robustness. The model employs a dual-layer encoder-decoder architecture combined with a cross-attention mechanism, achieving performance close to that of larger models in masked language modeling tasks.\n\nCore Contribution: A learnable dynamic chunking mechanism is proposed, which adaptively segments sequences into variable-length fragments using a dual-layer encoder-decoder structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Technically Complex and Comprehensive: The model features dynamic chunking, a hierarchical encoder-decoder structure, and a bidirectional Caduceus module.\n- Strong Robustness: It exhibits strong robustness against sequence perturbations such as mutations, insertions, and shifts.\n- High Computational Efficiency: Achieves comparable performance while significantly reducing the number of parameters, demonstrating high computational efficiency."}, "weaknesses": {"value": "- Lack of Interpretability: The learned chunk boundaries are completely opaque, lacking validation of their association with biological functional regions.\n\n- High Engineering Complexity: The training process is cumbersome, with significant costs associated with hyperparameter tuning. The paper lacks analysis of training stability and resource consumption.\n\n- Weak Theoretical Motivation: There is no theoretical support for how dynamic chunking outperforms fixed k-mers in terms of information representation. The primary validation is conducted on masked language modeling (MLM) tasks, without demonstrating transferability or generalizability.\n\n**Improvement Suggestions:**\n\n- Conduct biological relevance validation for chunk boundaries (e.g., analysis of promoter and exon coverage).\n- Supplement reports on training time, memory usage, and FLOPs to quantify efficiency gains.\n- Demonstrate the model's transferability in generative or structural prediction tasks.\n- Compare performance and interpretability differences across various chunk sizes.\n\nIf these issues can be addressed during the rebuttal period, I would consider raising my score. Besides, the titles in the manuscript (DNACHUNKER: LEARNABLE TOKENIZATION FOR DNA LANGUAGE MODELS) and in the system (Learnable Tokenization for DNA Foundational Models) are mismatched."}, "questions": {"value": "please refer to the weaknesses part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PhWJt9jOp2", "forum": "1kpkaJKDXW", "replyto": "1kpkaJKDXW", "signatures": ["ICLR.cc/2026/Conference/Submission8532/Reviewer_ssKV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8532/Reviewer_ssKV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076109698, "cdate": 1762076109698, "tmdate": 1762920391309, "mdate": 1762920391309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}