{"id": "6rYa2BUnTt", "number": 22217, "cdate": 1758327882857, "mdate": 1759896879596, "content": {"title": "Attention and Compression is all you need for Controllably Efficient Language Models", "abstract": "The quadratic cost of attention in transformers motivated the development of cheap approximations: namely sparse or sliding window attention, convolutions and linear attention. These approximations come with limitations; they drive down in-context recall as memory in the recurrent state and compute decrease. A priori fixing this quality-compute tradeoff in an architecture means being suboptimal: some downstream applications require good in-context recall, while others require lower latency and memory. Further, these approaches require heuristic choices for attention masks, handcrafted and careful recurrent state update rules, or need to be composed with attention layers to create a hybrid architecture that complicate the design. To address this, we propose a simple architecture called the Compress & Attend Transformer (CAT) that decodes each token attending to a chunk of neighbouring tokens and to compressed chunks of the sequence so far. Choosing a chunk size trades off quality for compute and memory. Moreover, CATs can be trained with multiple chunk sizes at once, unlocking control of quality-compute trade-offs directly at test-time without any retraining, all in a single adaptive architecture.\n\nOn exhaustive evaluations on language modeling, common-sense reasoning, in-context recall and long-context understanding, CATs outperform many existing efficient baselines including the hybrids when inference time and memory matched, and is competitive with the dense transformer in language modeling while being 1.5−3$\\times$ faster and requiring 2−9$\\times$ lower memory, depending on the chosen chunk size.", "tldr": "", "keywords": ["efficient architecture", "attention", "compression", "adaptive architecture"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f3182399b66e35f36d1126d7c3debfee4defd0e5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Compress & Attend Transformers (CATs), an adaptive architecture that decodes each chunk of tokens while attending to compressed representations of all previous chunks. Training interleaves compressed chunk tokens with raw tokens and applies a custom attention mask so each token looks only within‑chunk plus compressed past, enabling parallel compression and reducing decoder attention cost. On a single model trained with multiple chunk sizes, the authors show test‑time control of the quality/efficiency trade‑off via a special indicator token. Empirically, with a 12‑layer decoder widened to 2× hidden size and a small 3‑layer compressor, CATs match or beat several efficient baselines on long‑context recall and some language‑understanding tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple, adaptive mechanism with a clear knob: CATs realize a neat dial‑a‑budget via chunk size, using a learnable indicator token plus a shared marker to separate compressed from raw tokens.\n2. Scalable training trick that avoids recurrence: The paper’s interleaving of compressed vectors into the sequence with a custom mask reuses K/V for chunk representations and reduces decoder attention.\n3. Ablations and design transparency. Useful findings include: decoder width matters (D -> 2D helps LM perplexity), compressor depth has minor impact, and large chunks trade recall for speed."}, "weaknesses": {"value": "1. Main CAT model is 1B params (wider decoder + compressor) vs 260–310M for dense/linear baselines. While serving memory is KV‑dominated, LM perplexity gains could partially reflect capacity. Indeed, a parameter‑matched dense (2D) outperforms CAT on several LM metrics, weakening the claim that CATs match dense on LM while being faster. Please provide more apples‑to‑apples comparisons at equal params/training FLOPs.  \n2. Cross‑chunk fidelity trade‑offs not fully characterized. By design the decoder never attends to raw tokens outside the current chunk. Please add ablations on cross‑chunk phenomena.  \n3. Scope limited to 4K contexts. Most results cap at 4K because of training context. Since the motivation is long‑context efficiency/recall, evaluations at 8–32K (even with extrapolation) would strengthen claims."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6UOdJh4U0z", "forum": "6rYa2BUnTt", "replyto": "6rYa2BUnTt", "signatures": ["ICLR.cc/2026/Conference/Submission22217/Reviewer_Gs4J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22217/Reviewer_Gs4J"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893362445, "cdate": 1761893362445, "tmdate": 1762942119022, "mdate": 1762942119022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Compress & Attend Transformer (CAT) as an efficient alternative to standard Transformers. CAT first partitions the input tokens into chunks and employs a lightweight compressor (bid-Transformer + projection) to obtain a compressed representation for each chunk. In the main decoder LLM, each token attends to both the local tokens within chunk and the compressed representations of all preceding chunks. This design leads to computational and memory savings proportional to the chunk size during inference. Furthermore, by training CAT with multiple chunk sizes, the resulting adaptive CAT can flexibly operate under different budgets at test time by adjusting the chunk size. Experimental results demonstrate that CAT achieves stronger language modeling and in-context recall performance compared to other efficient baselines, while delivering significant inference efficiency gains over dense Transformers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a clear and coherent exposition of the proposed CAT methodology. The compress-and-decode design is both simple and effective, offering distinctive advantages over prior approaches such as sparse attention and fixed-size recursive compression.\n\n2. The training strategy of CAT is relatively novel. Compared to dense Transformers, it achieves training efficiency gains while maintaining parallelism. The use of multiple chunk sizes during training presents a well-motivated approach to adaptive compute allocation at test time.\n\n3. The evaluation is comprehensive, covering language modeling, synthetic and real-world recall, and long-context QA, etc."}, "weaknesses": {"value": "1. **Unfair comparison:**\nTo offset performance gaps, the authors doubled the decoder dimension for CAT, resulting in **non-proportional parameter counts** across models. CAT is both **deeper and wider**, which inherently benefits in-context and knowledge capability, especially for smaller models (e.g., commonsense reasoning sensitive to these factors). In the more balanced comparison of **Table 7**, CAT actually shows some disadvantage. While the authors argue that larger CAT models remain more efficient than Transformers, this makes comparisons with other efficient baselines **meaningless**—those baselines could similarly scale up parameters while maintaining efficiency gains.\n\n2. **Weak and incomplete baselines:**\nThe efficiency analysis (Fig. 3) only reports results up to 4K length, where CAT’s **quadratic complexity** is not yet dominant. For longer contexts, CAT would exhibit significant efficiency degradation compared to baselines such as GDN or GDN-H1. Thus, improvements in recall primarily indicate a **quality–efficiency trade-off** rather than a strict superiority. Moreover, modern practice no longer relies on fixed-size state models alone, and **hybrid linear + global attention** architectures are now the norm. A fairer comparison should involve a **GDN-Hybrid with dense attention** (e.g., 1:(C-1) hybrid ratios), possibly with 2x model dimensions , which would achieve similar compute savings (wrt. CAT-C) at normal sequence lengths.\n\n3. **Limited efficiency gains:**\nReported improvements mainly arise from CAT-4 and CAT-8, which yield roughly ½× and ¼× compute/memory reductions, respectively. However, layerwise-hybrid efficient baselines (w/ swa or linear attention) may offer better trade-offs. Such layerwise scheme could extend the efficiency advantage much further (e.g., up to 1:7 or beyond), suggesting that CAT’s practical efficiency benefits might be moderate at best."}, "questions": {"value": "1. CAT partitions the input into chunks of fixed sizes. Could this design potentially limit or even harm model performance, considering that tokens within a chunk may be totally semantically unrelated? Would a more input-aware or adaptive chunking strategy, as explored in [1], be a more reasonable choice?\n\n2. Does this coarse-grained, chunk-level compression inherently disadvantage tasks that require fine-grained token-level reasoning or recall? CAT appears conceptually similar to NSA [2] without its sparse-attention branch. However, that branch which handles fine-grained long-range interactions, has proven crucial for long-context or information-dense tasks.\n\n3. The paper does not specify the exact configuration of the \"sparse attention\" baseline. If it only uses a static, input-agnostic sparse pattern, the comparison may be unfair, since CAT’s compression involves contextual interaction (via a bidirectional Transformer). For language modeling tasks (that is considered to be semantically rich), dynamic sparse attention methods (e.g., [3]) are generally more representative baselines than static patterns.\n\n---\n\n[1]  Dynamic chunking for end-to-end hierarchical sequence modeling, 2025.  \n[2] Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention, 2025.  \n[3] H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PTfEVf23z9", "forum": "6rYa2BUnTt", "replyto": "6rYa2BUnTt", "signatures": ["ICLR.cc/2026/Conference/Submission22217/Reviewer_McFn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22217/Reviewer_McFn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912385608, "cdate": 1761912385608, "tmdate": 1762942118782, "mdate": 1762942118782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Compress & Attend Transformers (CATs): a decoder that attends both to the current local chunk and to compressed representations of prior chunks. CAT trains with multiple chunk sizes so a *single model* exposes a test-time knob trading quality for compute/memory. Experiments show 1.4–3.2× faster generation and 2.2–9.5× lower memory than a dense transformer at similar quality; on in-context recall and long-context tasks, CAT often outperforms efficient baselines (Mamba2, GDN/GDN-Hybrid) and is competitive with dense attention."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Originality\n\n* Simple, general recipe: *compress past, attend to compressed past + current chunk*; trains on multiple chunk sizes to enable test-time control without retraining.\n* Clear parallel training/generation story; no handcrafted recurrent updates.\n\nQuality\n\n* Provides implementation details (attention mask, KV reuse) and complexity accounting; decoder attention scales as ($O(N^2/C)$).\n* Reports broad benchmarks (LM, LongBench, EVAPORATE recall, RULER NIAH), with CAT $>=$ baselines under time/memory-matched settings.\n\nClarity\n\n* Architecture and training are illustrated nicely.\n\nSignificance\n\n* Practical for serving: reduces KV-cache footprint while preserving recall; one model covers multiple quality/compute points."}, "weaknesses": {"value": "1. Mismatched capacity in core tables: Baselines are \\~300M params; CAT uses a wider 12-layer decoder + compressor (\\~1B). This clouds “architecture vs scale” effects. Please add size-matched CAT (~300M) and/or scaled baselines (\\~1B) in the *main* tables.\n\n2. Training cost accounting is thin: The paper notes \\~2× longer training but gives no FLOPs/wall-clock/memory breakdown. Add a table with total training FLOPs, hours on fixed hardware, and peak memory for CAT vs baselines.\n\n3. Empirical differentiation from closest work: Related work argues NSA has no inference memory savings (full KV retained), which is CAT’s key advantage, but there’s no direct *peak-memory* comparison on main tasks. Add a head-to-head memory profile vs NSA, and extend the MegaByte/Block comparison beyond a toy task."}, "questions": {"value": "1. Can you report CAT-300M on GovReport/SummScreen and/or ~1B Mamba2/GDN to isolate architecture from scale?\n2. On a long-context task (e.g., RULER S-NIAH-U at 4k/8k), report peak inference memory (GB) for CAT vs NSA under matched accuracy.\n3. Include results on your LongBench/MQAR setups to substantiate CAT’s claimed recall advantage.\n4. What happens if learned compression is replaced with mean/max pooling? Please report impact on perplexity and peak memory.\n5. Figure 3 shows 1.4–3.2× speed and 2.2–9.5× memory reductions; could you include per-layer gate/overhead accounting and end-to-end latency vs chunk size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZVBFPadJPu", "forum": "6rYa2BUnTt", "replyto": "6rYa2BUnTt", "signatures": ["ICLR.cc/2026/Conference/Submission22217/Reviewer_KFwk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22217/Reviewer_KFwk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952677264, "cdate": 1761952677264, "tmdate": 1762942118569, "mdate": 1762942118569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Many methods a-priori constrain the memory and compute use of a model, or methods that recursively compress context over time tend to be slow and difficult to optimize. At the same time, different tasks and contexts face different compute and memory requirements. In response, this work introduces CAT to compress chunks of tokens at a time."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem is well-defined and important\n- The idea of training with multiple chunk sizes to enable an adaptive model is useful and interesting, given that most efficient architectures choose the memory and compute budget a-priori\n- The results in Table 3 and Figure 3 are very interesting; for a similar latency to Mamba-2/GatedDeltaNet, CAT-8 offers much better quality. It’s also interesting that a single model is used for all the experiments, highlighting that the compute-quality can be tuned."}, "weaknesses": {"value": "The parameter scales differ substantially (CAT ≈ 1 B vs. 300 M baseline), making it difficult to isolate the contribution of the architecture. Apples-to-apples scaling ablations would strengthen the case. Especially at such small parameter scales, it’s difficult to understand these trends. \n\nI will certainly consider increasing my score if I can better understand the impact of the parameter scale differences."}, "questions": {"value": "- How does the proposed chunk-based compression approach compare to (1) log-linear attention, (2) methods that encode chunks of text using a retriever/encoder model and then use similarity-based metrics to select the most relevant chunks for decoding?\n\n- What happens to tokens on a chunk boundary? Some prior work uses sliding window attention to capture full attention for local tokens in the sequence. If a token is right past the prior chunk boundary, does it get to fully attend to the lagging few tokens, or does it only attend to the compressed version of the tokens? It would be interesting to see ablations for the boundary token perplexity vs. perplexity of tokens later within chunks that get to attend to a lot of recent tokens.\n\n- It’s surprising that GDN-Hybrid and the dense transformer model perform so poorly on FDA and NIAH 4K. It would be helpful to understand from the error analysis what the authors observe. \n\n- It is surprising that the method works well on MQAR/NIAH since those tasks have very fine-grained keys and values without many distractor tokens (especially for MQAR with many keys and values); it seems like even compressing 4 tokens could drop a lot of precision. How do you think the chunk representation encompases all the keys and values in its chunk with high fidelity? It would be interesting to perform MQAR experiments as you scale up the number of keys and values in the sequence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UiXJbF0dje", "forum": "6rYa2BUnTt", "replyto": "6rYa2BUnTt", "signatures": ["ICLR.cc/2026/Conference/Submission22217/Reviewer_7wBq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22217/Reviewer_7wBq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154266960, "cdate": 1762154266960, "tmdate": 1762942118389, "mdate": 1762942118389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Compress and Attend Transformer (CAT), an architecture intended to be a controllably efficient alternative to standard transformers. It processes input by dividing it into chunks. When decoding a new token, the model attends to a local window of nearby tokens and also to a sequence of single vector compressed representations of all past chunks. The level of efficiency is controlled at test time by choosing the chunk size (C); larger chunks mean more compression and lower computational cost. The paper also presents a method to train a single, adaptive model that can operate with multiple chunk sizes. The authors claim this single model outperforms other efficient baselines and is 1.5-3x faster and uses 2-9x less memory than a dense transformer."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The core concept of a single, adaptive model that can be adjusted at test time to trade quality for compute is a novel and potentially useful contribution. This provides a flexibility that is absent in most existing efficient architectures, which are fixed into a specific configuration at training time.\n\n2. The paper is clearly written. The central mechanism of the CAT architecture is explained well and is easy to grasp, particularly with the help of Figure 2, which provides a simple visual of the information flow.\n\n3. The goal of the paper is significant. A single model that could be deployed to serve a wide range of needs (from low latency chat to high recall analysis) would be a major practical advantage. However, whether the paper actually achieves this significance is highly questionable due to major methodological flaws."}, "weaknesses": {"value": "1. The primary weakness is the invalid comparison in the main results tables (Tables 2, 3, 4, 5). The proposed CAT model has approximately 1B parameters. This model is compared against baselines (Dense, Mamba2, GDN) that have only 300M parameters. This 3x-4x parameter disparity invalidates any claims of superior efficiency or performance. The CAT model is not a more efficient architecture; it is a much larger model.\n\n2. The paper's own parameter matched comparison, in Appendix A.3 (Table 7), directly contradicts the main claims from the abstract. When CAT is compared to a \"Dense 2D\" model of similar size, the Dense 2D model is superior on all language modeling and common sense reasoning tasks. The CAT model only wins on two specific recall datasets, suggesting the architecture is not a general improvement but rather a specialized design that trades general performance for recall. This is a critical finding that is omitted from the main paper, making the abstract's claims misleading.\n\n3. The headline claims of being \"1.5-3x faster and requiring 2-9x lesser memory\"  are made against a 300M dense transformer, not a parameter matched one. A 1B parameter model should be compared to a 1B parameter baseline. The paper notes CAT-4 is 3x faster than a parameter matched dense model (Dense 2D) , but this is a much less impressive claim when Table 7 shows it's also a worse model on standard benchmarks.\n\n4. The authors admit the CATS model takes \"twice as much time\" to train as the (much smaller) dense baseline and up to 2.35x longer than a dense model of equivalent decoder depth. This is a significant practical disadvantage that is dismissed as a \"one time cost\" but represents a major barrier to adoption."}, "questions": {"value": "1. The main tables (e.g., Table 2, 3) compare a 1B parameter CAT to 300M parameter baselines. Your own appendix (Table 7) shows that a parameter matched \"Dense 2D\" model outperforms CAT on all language modeling and common sense reasoning tasks. How can the abstract's claims of outperforming baselines be justified when your own data contradicts this?\n\n2. The paper states that a 2x wider decoder ($D_g = 2D$) is required to match perplexity. This suggests the CAT architecture itself is less parameter efficient than a standard transformer. What is the performance of a CAT model that has the same total parameter count as the 300M dense baseline (e.g., a 12 layer model with $D=1024$ split between the compressor and decoder)?\n\n3. The performance of CAT 32 is extremely poor on recall tasks. This is likely due to the bottleneck of compressing 32 tokens into one vector. Have you explored alternative compression strategies, such as compressing a chunk of $C$ tokens into $k$ vectors (where $1 < k < C$), to provide a better trade-off for larger chunk sizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yOlsFsn1Uu", "forum": "6rYa2BUnTt", "replyto": "6rYa2BUnTt", "signatures": ["ICLR.cc/2026/Conference/Submission22217/Reviewer_Ue8h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22217/Reviewer_Ue8h"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244638693, "cdate": 1762244638693, "tmdate": 1762942118147, "mdate": 1762942118147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}