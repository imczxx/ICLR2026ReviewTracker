{"id": "93Y7jSUSpk", "number": 16544, "cdate": 1758265825946, "mdate": 1759897234017, "content": {"title": "Out-of-Distribution Graph Models Merging", "abstract": "This paper studies a novel problem of out-of-distribution graph models merging, which aims to construct a generalized model from multiple graph models pre-trained on different domains with distribution discrepancy. This problem is challenging because of the difficulty in learning domain-invariant knowledge implicitly in model parameters and consolidating expertise from potentially heterogeneous GNN backbones. In this work, we propose a graph generation strategy that instantiates the mixture distribution of multiple domains. Then, we merge and fine-tune the pre-trained graph models via a MoE module and a masking mechanism for generalized adaptation. Our framework is architecture-agnostic and can operate without any source/target domain data. Both theoretical analysis and experimental results demonstrate the effectiveness of our approach in addressing the model generalization problem.", "tldr": "Graph Models Merging for Graph OOD", "keywords": ["Graph Models Merging", "Source-Free Domain Generalization", "Graph Neural Networks"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ce6ef846c698fce1e9c693a122d3ae61c822a23.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem termed Out-of-Distribution Graph Models Merging, aiming to build a generalized graph model by merging multiple pre-trained GNNs trained on different domains, without requiring access to source data. The authors propose a two-stage framework: (1) a label-conditional graph generation phase that inverts each pre-trained GNN to synthesize pseudo-graphs representing its domain knowledge, and (2) a model merging phase that employs a fine-tuned Mixture-of-Experts with learnable masks and sparse gates to integrate the knowledge of heterogeneous GNNs. Theoretical analyses are provided based on a mixture distribution assumption, and experiments o graph classification benchmarks show improvements over ensemble and model soup baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of merging multiple pre-trained graph models without source data is interesting and practically relevant in the context of model reusability and privacy-aware learning.\n\n2. The combination of graph generation and MoE-based merging is conceptually coherent and could inspire further exploration of source-free graph model integration.\n\n3. The paper is well-organized and provides technical clarity, including equations, regularizers, and ablation settings."}, "weaknesses": {"value": "1. The proposed task largely overlaps with graph-free knowledge distillation [1] and model soup [2], making the originality less substantial. The contribution seems more like a synthesis of existing ideas than a fundamentally new paradigm.\n\n2. The mixture distribution assumption $G_{T} = \\sum_{i} \\alpha_{i} G_{i}$ is overly strong and lacks empirical justification, especially in discrete graph spaces. Theoretical analysis is abstract and disconnected from the implemented MoE mechanism.\n\n3. All experiments are conducted on small, low-diversity datasets. No results are shown on large-scale or node-level tasks (e.g., OGB benchmarks). Hence, the claimed “OOD generalization” may not hold in realistic settings.\n\n4. The “domain” split based solely on edge-to-node ratio may not meaningfully reflect true distribution shifts. The evaluation therefore risks testing within-distribution variations rather than genuine OOD generalization.\n\n5. There is no investigation into how experts or masks behave, how gating decisions distribute across domains, or what knowledge is actually merged.\n\n6. Competing baselines are mainly adapted from non-graph domains; missing comparisons to recent source-free domain generalization or prompt-based GNN adaptation methods weakens the empirical argument.\n\n[1]  Graph-free knowledge distillation for graph neural networks.\n\n[2] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time."}, "questions": {"value": "1. How sensitive is the method to the number and diversity of pre-trained GNNs? Would OGMM still perform well when experts are highly redundant or of poor quality?\n\n2. How stable is the label-conditional graph generation? Are there cases of mode collapse or low diversity?\n\n3. How does OGMM scale with the number of experts (in terms of computation and performance)?\n\n4. Could the authors provide a qualitative or quantitative analysis of the learned gating and mask patterns to better interpret the merging mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "97NE4kst5x", "forum": "93Y7jSUSpk", "replyto": "93Y7jSUSpk", "signatures": ["ICLR.cc/2026/Conference/Submission16544/Reviewer_GZwp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16544/Reviewer_GZwp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579906085, "cdate": 1761579906085, "tmdate": 1762926625279, "mdate": 1762926625279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a novel problem in the domain of GNNs: Out-of-Distribution Graph Models Merging (OGMM). The authors propose a framework that merges pre-trained GNNs from multiple domains with distribution shifts to create a unified, generalized model. This approach overcomes the challenge of needing to retrain models from scratch by leveraging pre-trained models to preserve specialized domain knowledge. The two-stage process first generates label-conditional graphs using each model and then fine-tunes and merges them using a MoE module. Experimental results show that OGMM outperforms previous methods on multiple datasets, establishing a new state-of-the-art in graph model generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a novel challenge of merging out-of-distribution graph models without needing to retrain from scratch, which is both practical and impactful for real-world applications where data is scarce.\n2. OGMM consistently outperforms previous fusion methods and demonstrates robustness on large-scale datasets like REDDIT-B and NCI1, showing that it can handle diverse graph domains with different GNN architectures.\n3. The authors provide a solid theoretical framework for the problem and support it with comprehensive experiments that demonstrate the framework's effectiveness across multiple domains and datasets."}, "weaknesses": {"value": "1. The two-stage process for merging out-of-distribution models involves multiple steps, including fine-tuning and the use of the Mixture-of-Experts (MoE) module. While effective, the overall time complexity of this process could be quite high, especially as the number of pre-trained models and the size of the graphs increase.\n2. The experiments primarily focus on datasets such as REDDIT-B and NCI1, which are not necessarily representative of the most commonly encountered graph types in real-world applications. It would strengthen the paper to include additional tests on more widely used or complex datasets, such as social network graphs or biological networks, to better understand the model's applicability across various domains."}, "questions": {"value": "1. See weaknesses.\n2. How are these pre-trained GNNs selected? Are they arbitrarily chosen graph models? If not, how is their optimality determined?\n3. Negative transfer is a common issue in generalization domains. I would like to know how OGMM avoids this problem, and whether there is any theoretical or empirical evidence supporting this.\n4. Some graph-MoE works should be compared and discussed in the experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q5mbY0pYC3", "forum": "93Y7jSUSpk", "replyto": "93Y7jSUSpk", "signatures": ["ICLR.cc/2026/Conference/Submission16544/Reviewer_YmPD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16544/Reviewer_YmPD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985384357, "cdate": 1761985384357, "tmdate": 1762926624440, "mdate": 1762926624440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a new problem called out-of-distribution graph models merging, which aims to merge multiple pre-trained GNNs from different domains with distribution shifts into a single generalized model. The proposed OGMM is a two-stage framework, leveraging graph generation and a fine-tuned MoE module to enable generalization under graph OOD scenaros. The paper provides theoretical bounds on generalization error, and extensive experiments on multiple datasets demonstrate substantial performance gains over strong baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The integration of graph generation and MoE-based model fusion is conceptually coherent, enabling domain knowledge transfer at both data and model levels.\n\n2) The mixture distribution assumption and accompanying error bound provide a formal justification for the merging process.\n\n3) The framework is applicable to heterogeneous GNNs, enhancing its generality and practical relevance."}, "weaknesses": {"value": "1) The motivation for merging multiple pre-trained GNNs is not sufficiently justified. It remains unclear why model-level merging is preferable to retraining on aggregated data or simply using the best domain-specific model. No empirical or application-level evidence is given to show that scenarios requiring model-level merging without data access are common or practically constrained.\n2) Methodological novelty appears incremental, as the proposed approach largely builds upon existing techniques in graph distillation and mixture-of-experts regularization without a distinct new principle or paradigm.\n3) While the paper qualitatively illustrates synthetic graph realism, it does not quantitatively assess their fidelity to the underlying data manifold, leaving the actual extent of domain knowledge recovery uncertain.\n4) Notation suffers from inconsistencies such as {G_i}_{i\\in M}, and the overall writing is occasionally unclear, making it difficult to follow the theoretical formulation."}, "questions": {"value": "1) How robust is OGMM to the quality of pretrained GNNs? For example, if one model performs poorly or overfits its domain, does the merging process degrade significantly?\n2) Why focus masks primarily on classification heads? Have you ablated masking other layers such as message-passing modules and observed differences in generalization?\n3) How do you handle varying graph sizes or node/edge feature dimensions across domains during generation and merging? Is there preprocessing involved?\n4) Could OGMM be extended to incremental merging, where new pretrained models arrive sequentially?\n5) Would adding a contrastive or alignment loss between expert embeddings further stabilize the merging process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AENkwpCVdn", "forum": "93Y7jSUSpk", "replyto": "93Y7jSUSpk", "signatures": ["ICLR.cc/2026/Conference/Submission16544/Reviewer_1A5i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16544/Reviewer_1A5i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990073044, "cdate": 1761990073044, "tmdate": 1762926623997, "mdate": 1762926623997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel problem of out-of-distribution graph model merging, aiming to merge models pre-trained on graphs from different domains into a model that generalizes under distribution shifts. A two-stage approach is proposed: first, a graph generator is trained to generate synthetic graph data, which is then used in the second stage to fine-tune the MoE module. Experiments demonstrate that the proposed method effectively generalizes to data with distribution shifts."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed problem of Out-of-Distribution Graph Models Merging is novel.\n- The proposed method, OGMM, has a solid theoretical foundation. Generating synthetic data to extract domain knowledge makes sense.\n- The experiments demonstrate the effectiveness of OGMM and the contributions of each component."}, "weaknesses": {"value": "- The scenarios considered are limited. More results on other node classification graph datasets could be provided. Additionally, the paper only focuses the OOD scenario within a single graph, without considering cross-dataset or cross-domain scenarios.\n- The proposed OGMM relies on a mixture distribution assumption, which is not likely to hold in more complex scenarios.\n- The proposed OGMM seems to rely on many hyperparameters, some of which significantly impact model performance according to the analysis in the paper. This could limit its applicability in practical settings.\n- Although OGMM is theoretically computationally efficient, some experimental results could be provided."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3YoVmLEkFj", "forum": "93Y7jSUSpk", "replyto": "93Y7jSUSpk", "signatures": ["ICLR.cc/2026/Conference/Submission16544/Reviewer_pjDW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16544/Reviewer_pjDW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000883778, "cdate": 1762000883778, "tmdate": 1762926623475, "mdate": 1762926623475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}