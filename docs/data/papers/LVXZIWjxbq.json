{"id": "LVXZIWjxbq", "number": 15005, "cdate": 1758246724541, "mdate": 1759897336078, "content": {"title": "Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting", "abstract": "Group-Relative Policy Optimization (GRPO) is a key technique for training large reasoning models, yet it suffers from a critical vulnerability: the Think-Answer Mismatch, where noisy reward signals corrupt the learning process. This problem is most severe in unbalanced response groups, paradoxically degrading the signal precisely when it should be most informative. To address this challenge, we propose Stable Group-Relative Policy Optimization (S-GRPO), a principled enhancement that derives optimal, noise-aware advantage weights to stabilize training. Our comprehensive experiments on mathematical reasoning benchmarks demonstrate S-GRPO's effectiveness and robustness. On various models, S-GRPO significantly outperforms DR. GRPO, achieving performance gains of +2.5\\% on Qwen-Math-7B-Base, +2.2\\% on Llama-3.2-3B-Base, and +2.4\\% on Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn under 20\\% synthetic reward noise, S-GRPO maintains stable learning progress. These results highlight S-GRPO's potential for more robust and effective training of large-scale reasoning models.", "tldr": "", "keywords": ["Reasoning Model", "Large Language Model", "Group-Relative Policy Optimization", "Noise Model", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec949933d6102419ae3c3f349733c66119c2930b.pdf", "supplementary_material": "/attachment/6380ec0fc3baa947ff8038c37af222b22ed432c8.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on mitigating the thinking-answer mismatch issue of RL for LLM reasoning. The so-called thinking-answer mismatch refers to the noise of outcome rewards that raise when the model generate incorrect CoTs but end with a correct answer coincidentally. Taking inspiration from the classic label noise research area, the authors proposed S-GRPO, a simple advantage shaping technique, which reweighs the advantages estimated by group-wise normalization and meanwhile explicitly take the possible noise in rewards into consideration. Experiments on Qwen2.5 Math and Llama 3.2 models on mathematical reasoning tasks are carried out to validate the effectiveness of the proposed method."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is simple and straightforward. The method part is very easy for the reviewer to follow up with."}, "weaknesses": {"value": "- The motivation is weak in my veiw. My opinion on this point are three folds. First, there are no strong evidence (e.g., objective statistics) to validate that the so-called thinking-answer mismatch would result in critical vulnerability when one try to scale the performance with outcome rewards. As far as I can tell, in mathematical and coding tasks, the rule-based reward signals are very clear and reliable. Besides, there have been a few observations demonstrate that RL with outcome rewards can automatically compress the incorrect CoTs with coincidentally correct answers [1]. Lastly, even if the rewards are unreliable, traditional value-based RL method like PPO can capture the flaws in reasoning traces. And thus the novelty and contribution of the proposed method is relatively limited in my view.\n- There is no clear guidence on how to estimate the true reward noise ratio p. Estimation on the real noise ratio necessitate verification of the correctness of reasoning traces, which is non-trivial and in my view, rather challenging.\n- The experiments setup is relatively outdated. The authors only tune 2 Qwen2.5 Math series model and 1 Llama3.2-3B model on 8.5K samples. It is doubtful that if the scale of RL training in this manuscript is enough for validate the effectiveness of the proposed method.\n- Serveral sota baselines are missing. For example, DAPO and GSPO.\n- The authors claim that the reported accuracy is averaged among the top-3 checkpoints. How exactly do they choose the checkpoints? I wonder if there exists data leakage in model selection.\n- The visualization in this manuscript is hard for me to follow up with. Most figures are small, and the conveyed information is ambiguous and limited.\n\n[1] Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UnwbX7d59h", "forum": "LVXZIWjxbq", "replyto": "LVXZIWjxbq", "signatures": ["ICLR.cc/2026/Conference/Submission15005/Reviewer_Zh5R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15005/Reviewer_Zh5R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760958700621, "cdate": 1760958700621, "tmdate": 1762925334634, "mdate": 1762925334634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Stable Group-Relative Policy Optimization (S-GRPO), a principled method that uses optimal, noise-aware advantage weights to mitigate the \"Think-Answer Mismatch\" vulnerability in GRPO, thereby stabilizing training and significantly improving reasoning model performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  This paper try to address a significant problem: the issue of false positives where the \"Thinking\" process and the final \"Answer\" do not align.\n2.  The approach is novel to me. Instead of the common practice of trying to identify errors within the thinking process, this paper proposes to reweight the advantage values to mitigate the impact of this mismatch.\n3.  The paper is well-written, clear, and easy to understand."}, "weaknesses": {"value": "(If my understanding is incorrect, please correct me)\n\n1.  **Concerns about generalization.** The derivation, starting from Equations 5 and 6 and continuing to Equation 11, seems entirely predicated on the assumption that the random variable $r$ (reward) follows a Bernoulli distribution. However, this assumption may not always hold in practice. For instance, one might use $\\{-1, 1\\}$ reward pairs or even continuous rewards. In such scenarios, how would the proposed method be formalized? Would the new formulas still be applicable and effective?\n2.  **Evaluation methodology.** The exclusion of the AIME 2024/2025 is unacceptable. With 30 problems, the avg@32 metric could have been calculated, which is a standard practice.\n\n3.  **Difficulty in hyperparameter selection.** The choice of $p$ appears to be a significant challenge. With so many variables, such as different mathematical datasets, diverse reasoning tasks, various models, and different training settings, it seems difficult to establish an empirical rule of thumb for this parameter."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "nan"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SizLuxSXoP", "forum": "LVXZIWjxbq", "replyto": "LVXZIWjxbq", "signatures": ["ICLR.cc/2026/Conference/Submission15005/Reviewer_ARgJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15005/Reviewer_ARgJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571585377, "cdate": 1761571585377, "tmdate": 1762925333985, "mdate": 1762925333985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces S-GRPO, a noise-aware variant of Group-Relative Policy Optimization for training reasoning LLMs. It addresses the Think-Answer Mismatch problem, where noisy reward signals corrupt learning when reasoning and final answers misalign. S-GRPO derives optimal reweighting under a symmetric noise model, down-weighting unreliable groups to stabilize training. Experiments on mathematical reasoning benchmarks show consistent 2‚Äì3% accuracy gains and robustness under 20% reward noise. Analysis demonstrates smoother entropy reduction and more coherent reasoning compared to GRPO and Dr. GRPO."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly defines the Think-Answer Mismatch problem in GRPO and provides a convincing analysis of its impact on learning stability.\n2. The proposed S-GRPO introduces a principled noise-aware reweighting method derived from a theoretical foundation.\n3. Experimental results show consistent, reproducible improvements across several mathematical reasoning benchmarks."}, "weaknesses": {"value": "1. The method depends on a manually set noise parameter ùëù p, which may require case-specific tuning.\n\n2. The assumption of symmetric reward noise simplifies the training environment but may not accurately represent real-world mismatch patterns where errors are often asymmetric.\n\n3. The robustness experiments are based on artificially injected synthetic noise levels (up to 20%), which likely exceed the noise typically observed in real reasoning datasets, raising questions about whether such high noise modeling is necessary or reflective of practical scenarios.\n\n1. The figures are not very clear, and the font size is quite small. It would be helpful to use larger text and improve the overall visual clarity of the plots."}, "questions": {"value": "1. Does S-GRPO generalize beyond math reasoning tasks? Non-math reasoning tasks?\n2. The paper evaluates robustness using 10‚Äì20% synthetic, symmetric reward noise but does not clarify how this compares to the typically lower and more structured noise found in real datasets. This difference may limit the realism of the evaluation, so it would be helpful for the authors to estimate actual mismatch rates and examine performance under more realistic noise conditions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6JJdr1NjvD", "forum": "LVXZIWjxbq", "replyto": "LVXZIWjxbq", "signatures": ["ICLR.cc/2026/Conference/Submission15005/Reviewer_x1Hi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15005/Reviewer_x1Hi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915341696, "cdate": 1761915341696, "tmdate": 1762925332567, "mdate": 1762925332567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on addressing the \"Think-Answer Mismatch\" problem, a prevalent issue in Large Language Model (LLM) reasoning tasks, where the correctness of the model's reasoning process does not always align with the correctness of its final answer. The authors point out that this problem is particularly prominent in the popular Group Reward Policy Optimization (GRPO) method. This is because incorrect reasoning processes can still yield correct answers (i.e., false positives), generating noisy reward signals that severely contaminate the model's learning process. The paper proposes an optimal advantage re-weighting method, named S-GRPO, based on a symmetric noise model. This method is designed to robustly mitigate the contamination of training signals in GRPO caused by the think-answer mismatch, thereby enhancing the reasoning performance and stability of large models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear Problem Definition and In-depth Analysis: The paper provides a remarkably thorough analysis of the impact of the \"think-answer mismatch\" within the GRPO framework. Instead of merely discussing the general harm of noise, it precisely identifies, through mathematical derivation, that \"group imbalance\" is the key factor amplifying the effect of noise. This insight is both profound and enlightening.\n2. Elegant and Theoretically Grounded Methodology: S-GRPO is not presented as an empirical \"patch\" or a complex module. Rather, it is an optimal weighting strategy derived from the classic symmetric noise model by minimizing the expected squared error between the observed and true advantages.\n3. Strong Reproducibility: The paper provides detailed experimental settings and includes a link to an anonymized code repository, which significantly enhances the reproducibility of the research and is a commendable practice."}, "weaknesses": {"value": "1. Limitations of the Symmetric Noise Model: A core assumption of the proposed method is the presence of symmetric noise. However, in practical scenarios, the probabilities of false positives and false negatives may not be symmetric.\n2. Dependence on the Noise Rate p and Insufficient Guidance for its Selection: The paper observes that the optimal value of p is related to the model's scale, which is a valuable finding. However, it does not offer clear guidance on how to efficiently estimate an appropriate value for p a priori when dealing with a new model or task. This could introduce an additional hyperparameter tuning burden and uncertainty in the practical application of the method."}, "questions": {"value": "1. The paper states that the optimal value of p is related to model scale. Besides model scale, could the selection of p also be dependent on other factors, such as the difficulty of the dataset or the type of task?\n2. Does the case study in Appendix B.1 reveal a potential, undiscussed drawback of S-GRPO? Specifically, by penalizing reasoning paths that may be \"shortcuts\" or based on \"pattern matching\" but still produce the correct result,\n could S-GRPO inadvertently compel the model to attempt complex, formal reasoning paths that it has not yet fully mastered? This could, in turn, increase the risk of computational or execution errors, ultimately compromising the accuracy of the final answer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lPwak688df", "forum": "LVXZIWjxbq", "replyto": "LVXZIWjxbq", "signatures": ["ICLR.cc/2026/Conference/Submission15005/Reviewer_gbyg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15005/Reviewer_gbyg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095162185, "cdate": 1762095162185, "tmdate": 1762925331541, "mdate": 1762925331541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}