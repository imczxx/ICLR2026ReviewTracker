{"id": "VesLZukY5E", "number": 23785, "cdate": 1758348411106, "mdate": 1763707650787, "content": {"title": "How Far Can Unsupervised RLVR Scale LLM Training?", "abstract": "Unsupervised Reinforcement Learning with Verifiable Rewards (URLVR) offers a pathway for Large Language Models (LLMs) to improve without human supervision.\nParticularly, many works use model intrinsic information as rewards for URLVR, showing promising improvements, yet their potential and limitations remain unclear.\nIn this work, we revisit URLVR through the lens of intrinsic rewards.\nWe present a unified theoretical framework showing that intrinsic reward methods share a core mechanism: they trade uncertainty for performance by leveraging the model’s prior knowledge to sharpen output distributions.\nEmpirical analysis confirms this tradeoff, revealing distinct failure modes and showing that collapse is not inevitable in small, domain-specific regimes such as test-time training.\nBeyond these findings, early intrinsic reward dynamics also provide a lightweight indicator of model-task priors, complementing $pass@k$ in assessing RL trainability.\nThese insights highlight both the promise and pitfalls of URLVR, motivating future directions such as external rewards and hybrid supervision strategies.", "tldr": "We revisit unsupervised RLVR through intrinsic rewards, unifying existing methods, analyzing their impact on confidence and failure modes, discussing their potential applications..", "keywords": ["Large Language Models", "Unsupervised Reward", "Reinforcement Learning", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/729441a4af843a1185f0d43ea2a0afa772c987c9.pdf", "supplementary_material": "/attachment/e61df61cdb0de9a64f85fc366ac8766f6f9c158e.zip"}, "replies": [{"content": {"summary": {"value": "The paper unifies several intrinsic reward modeling approach under the proposed unsupervised RLVR (URLVR) framework. Using majority voting as a prototypical approach, the paper observes that the trained model converges to a deterministic policy (Theorem 1). The paper hypothesizes that the performance of the learned policy depends on the confidence of the pre-RL model. URLVR approaches improve sampling efficiency but may not necessarily add any new capability. The paper tests this hypothesis via experiments on a a few language models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper unifies a variety of methods that use implicit reward modeling. This setup provides a framework for the community to understand the tradeoffs involved with these methods\n\n- The analytical approach used in Theorem 1 clarifies the confidence vs performance trade-off. \n\n- Experimental validation and analysis is mostly sound and supports the observation made in Theorem 1 and claim related to model collapse. I do have a question on the setup that I will note under weakness/question"}, "weaknesses": {"value": "- The use of Qwen model in the analysis could use more justification. The central claim in the paper is that  the success (or lack thereof) of URLVR methods depend on the \"base/pre-RL's\" model's confidence-correctness alignment. The empirical analysis is conducted on Qwen3-family of models. No justification is provided on why these models were chosen. I would be curious to understand whether there is something the reader can learn about how this choice is better than others that may be accessible to them.\n\n- The writing in the paper could be improved\n  - The paper uses math symbols (for example Table 1) without defining all of the symbols in text. While the appendix may have the info the reader needs, it would be useful if this information is available to the reader in caption/nearby text\n  - The paper covers related papers but uses short-form etc for their names. This practice can be confusing to readers that are not as familiar with the field (like this reviewer) as the authors"}, "questions": {"value": "- Would it be possible to share any insight on why Qwen3 models may have the confidence-correctness alignment shown in the paper? Is there a way to show some language models family are more aligned than others in the sense defined in the paper?  \n\n- What is RLIF mentioned in the paper? There is another paper not cited in the draft that shows up when the reviewer did a literature search : https://arxiv.org/abs/2311.12996\n\n- I did not see an LLM usage statement in the paper or appendix. Please include one as required by ICLR 2026 guidelines"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2QwGhRR55r", "forum": "VesLZukY5E", "replyto": "VesLZukY5E", "signatures": ["ICLR.cc/2026/Conference/Submission23785/Reviewer_t95z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23785/Reviewer_t95z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761510083486, "cdate": 1761510083486, "tmdate": 1762942805905, "mdate": 1762942805905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the mechanisms underlying Unsupervised Reinforcement Learning with Verifiable Rewards (URLVR), which finetunes a language model through RL over intrinsic rewards (i.e., rewards derived based on the model itself without any external supervision). It first introduces a theoretical perspective that unifies the goal of ensemble-based and certainty-based methods. Then, the theoretical insights are demonstrated empirically. In particular, the paper shows that existing intrinsic rewards trade uncertainty for performance by sharpening the language model distribution, yet this comes at the risk of distribution collapse and reward hacking. Furthermore, it characterizes how such collapse manifests in different methods and shows that in some specific regimes it can be avoided."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic is timely, given the increase in popularity of methods using RL with intrinsic rewards for improving the capabilities of language models.\n\n2. The empirical analysis sheds light on the difference between intrinsic rewards and their potential failure patterns.\n\n3. I find the suggestion of using intrinsic reward dynamics in the initial time steps as an indicator for the potential success of URLVR, as opposed to using measures such as pass@k that require access to a verifier or labels, to be interesting. However, it is worth noting that the evaluation of this method is somewhat lacking. There is no qualitative guidance as to how one should use the intrinsic reward dynamics and there is no comparison to pass@k in terms of predictiveness or computational efficiency."}, "weaknesses": {"value": "1. The unified reward framework in Section 3.1 is currently not well-defined and its usefulness is not substantiated in the paper. Specifically, how does the right hand side of Equation (1) depend on $y$? Should the cross-entropy term $h$ be $-q^i (y | x) \\ln \\pi_\\theta^i (y | x)$? Moreover, the significance of such a unified perspective greatly depends on whether it allows characterizing similarities and differences between different instances. However, the unified framework is not really used in the paper after its definition, which raises the question of why it is necessary or helpful.\n\n2. The theoretical analysis contains potentially incorrect claims. \n    - The reward in majority voting depends on the current policy. It therefore changes during training. The closed form solution to the KL-regularized RL objective in Equation (3) holds for a fixed reward, and so it is not clear what it implies for a policy-dependent reward such as majority voting. In particular, without proof, there is no reason to believe that Equation (4) holds. Do the authors have a proof for this claim? Also, the majority reward is not formally defined: what does $maj (Y)$ stand for? How many rollouts are considered in this majority?\n    - Theorem 1 is not rigorously stated. It is therefore difficult to evaluate what it means or whether it is sound. Specifically, what does \"majority trajectories\" refer to? What are \"$k$ updates\"? Does this refer to $k$ steps of policy gradient or rather $k$ iterations of solving perfectly the KL-regularized objective and each time updating the reference policy to be the current policy?\n\n3. One of the main claims made in the paper is that the success of URLVR stems from sharpening the language model’s distribution on correct outputs, in case it already assigns such outputs a reasonable probability. However, as far as I am aware, this is already the existing conventional wisdom behind why methods such as majority voting as a reward can work (c.f. [1,2]).\n\n4. Relation to prior work is often not adequately discussed. In particular, the results of Section 5.1 are extremely similar in nature to Section 4.1 of the TTRL paper [2]. What contribution does Section 5.1 of this paper provide beyond what was already reported by the TTRL paper? Furthermore, I believe it is worth mentioning the relation to [1], which theoretically analyzes the sharpening mechanism of URLVR.\n\n[1] Zuo, Yuxin, et al. \"Ttrl: Test-time reinforcement learning.\" arXiv preprint arXiv:2504.16084 (2025).\n\n[2] Huang, Audrey, et al. \"Self-improvement in language models: The sharpening mechanism.\" ICLR 2025.\n\nReview Summary and Recommendation\n---\nOverall, I believe that the paper does not meet the requirements for publication at ICLR. The most significant limitation of the current manuscript is that it contains underspecified and potentially unsound theory. Beyond fixing this issue, it would greatly strengthen the contributions of the paper to elaborate on the unified perspective and its uses, clarify relation to prior work and conventional wisdom regarding how URLVR works, and provide quantitative evidence for the predictiveness of initial reward dynamics of the success of URLVR.\n\n\n\nAdditional (More Minor) Comments\n---\n1. The notation of Table 1 is defined only in the appendix, which makes it difficult to parse. I would recommend having the necessary notation in the main text (e.g., in the table caption).\n\n2. There are some missing implementation details, which make it difficult to gauge the significance of some empirical contributions.\n    - In Figure 4, how are the subsets chosen? Are these just chosen at random? Also, is the behavior consistent across random subsets? Intuitively, while some small subsets will not suffer from a collapse in ground truth reward, for others they should if the majority vote is incorrect in a non-negligible portion of the examples.\n    - The setup in Section 4.3.2 is unclear. For example, how many samples are used?\n    - The setup in Section 5.1 is unclear. For example, on which dataset are these experiments ran?\n\n3. The definition of reward hacking mentioned in line 355 does not seem to accord with its conventional use. If the intrinsic reward is maximized and the ground truth reward also keeps increasing, then why should this be considered hacking?"}, "questions": {"value": "--"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MD29joNDlM", "forum": "VesLZukY5E", "replyto": "VesLZukY5E", "signatures": ["ICLR.cc/2026/Conference/Submission23785/Reviewer_um7o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23785/Reviewer_um7o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666399782, "cdate": 1761666399782, "tmdate": 1762942805604, "mdate": 1762942805604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the potential and limitations of Unsupervised Reinforcement Learning with Verifiable Rewards for scaling Large Language Models , specifically focusing on intrinsic reward derived from the model's own internal signals addressing the \"supervision bottleneck,\" where obtaining human-verified labels becomes expensive. The study presents a unified theoretical framework for intrinsic rewards stating that all such rewards share a core mechanism: they improve performance by trading uncertainty for performance by leveraging the models priprs knowledge and sharpen the model's output . The authors present empirical analysis  confirming this. The authors also show that model collapses is avoidable in small domain specific settings  such as test time training."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "the paper presents a consolidated mathematical perspective that connect different intrinsic rewards methods under a single framework which is very nice. The authors also present a solid evaluation of intrinsic rewards - they analysed different failure modes for different methods. I think the paper is very insightful and written clearly and I think it's very interesting to investigate these novelity driven rewards within RL for LLM's."}, "weaknesses": {"value": "this is not necessarily a weakness but the paper really focueses on the analysis of these different intrinsic rewards and does not so much contribute any new method ontop of this. While I personally really enjoyed reading this paper I am not 100% sure this is the right venue for this."}, "questions": {"value": "how does this scale to long training on large datasets? is reward hacking inevitable? \nDo you see any issues with the limited domain? as the domain this has been tested on is quite limited to math reasoning tasks"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h2YGRSVReo", "forum": "VesLZukY5E", "replyto": "VesLZukY5E", "signatures": ["ICLR.cc/2026/Conference/Submission23785/Reviewer_pWtu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23785/Reviewer_pWtu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834366046, "cdate": 1761834366046, "tmdate": 1762942804765, "mdate": 1762942804765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a broad investigation into recent RL methods for LLMs, that operate completely without external rewards and instead use the model's own internal signals. It introduces a framework, arguing that diverse intrinsic rewards (such as majority voting or self-certainty) share a common mechanism: they \"trade uncertainty for performance\" by \"sharpening output distributions\" around the model's initially confident solutions. This convergence, the paper shows, enables the model to amplify its existing knowledge, but also risks bias lock-in if the model's confidence is misaligned with correctness. Empirically, the paper confirms this trade-off, revealing that different intrinsic reward methods fail in distinct ways; for example, some collapse into overly brief responses, while others promote repetitive verbosity. The authors demonstrate that in small, domain-specific regimes, such as test-time training, intrinsic rewards can drive stable adaptation without collapse. Finally, the paper proposes a practical application for these findings: using the early training dynamics of intrinsic rewards as a fast, lightweight indicator to assess a model's suitability and \"RL trainability\" for a specific task, complementing traditional metrics like"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Proposes to view the changes to the policy through the lens of trading uncertainty for confidence and provides a systematic, empirical analysis of the distinct failure modes of different reward types, such as length collapse for probability rewards and verbosity for entropy rewards. A practical strength of this paper is its investigation of scaling limits, which concludes that while large-scale training leads to collapse, these methods are stable and effective in small, domain-specific settings, identifying test-time training as an ideal application."}, "weaknesses": {"value": "A closely related analysis was published recently (Jun 2025) by Y. Zhang et al (No Free Lunch: Rethinking Internal Feedback for LLM Reasoning). Arguably, the core insights of these two publications are very similar; with Y. Zhang et al provide a stronger grounding in theory and provide a more in depth analysis of learning dynamics for different base-models; while this paper provides more insight into the dynamics and failure-modes of the different reward signals. Overall, the authors cite this prior work but do not discuss and contextualize  the relevance, the overlap, and potential differences."}, "questions": {"value": "Has the term URLVR been used before? My search came up empty and I find it confusing to have “Verified Reward” prominently in the name, but negate it by prefixing it with “unsupervised”. Isn't Reinforcement Learning from Internal Feedback (RLIF) the established terminology at this point?\n\nRegarding the prior (almost concurrent) work by Y. Zhang et al (No Free Lunch: Rethinking Internal Feedback for LLM Reasoning): Do you see any discrepancies between your results and conclusions and theirs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5VWflcWvFZ", "forum": "VesLZukY5E", "replyto": "VesLZukY5E", "signatures": ["ICLR.cc/2026/Conference/Submission23785/Reviewer_Kjs9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23785/Reviewer_Kjs9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128672077, "cdate": 1762128672077, "tmdate": 1762942804273, "mdate": 1762942804273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "[Global Response 1 (Part 1)] Model-Task Priors: In-Depth Analysis Across Model Families"}, "comment": {"value": "Multiple reviewers raised questions about model selection and deeper analysis of which models benefit from intrinsic rewards:\n\n- **Reviewer Kjs9 (Weakness #1)**: Notes that [1] provided \"more in-depth analysis of learning dynamics for different base-models\" and questions our depth of model analysis\n- **Reviewer pWtu (Weakness #1)**: Questions whether we contribute new insights beyond analyzing existing methods\n- **Reviewer um7o (Weakness #3)**: Suggests our sharpening insight is already \"conventional wisdom\"\n- **Reviewer t95z (Weakness #1)**: Requests justification for Qwen3 choice and asks \"whether there is something the reader can learn about how this choice is better than others that may be accessible to them\"\n- **Reviewer t95z (Question #1)**: Asks \"Is there a way to show some language model families are more aligned than others in the sense defined in the paper?\"\n\nWe address all these concerns with a unified response showing our in-depth, quantitative analysis of model-task priors that extends beyond conventional understanding.\n\n**Building upon Our Original Analysis (Appendix B.4 + Section 5.2)**\n\nIn our original submission, we have provided substantial model analysis that we acknowledge could be highlighted more clearly in the main text:\n- **In Appendix B.4**, we provided a broad observational study of training dynamics. Our appendix included a comparison of **11 models**, covering various **series** (Qwen2.5/Qwen3/OctoThinker/Llama3.1), various training **stages** (base/math base/sft/instruct), and various **sizes** (1.7B, 3B, 4B, 8B). This analysis observed that different models indeed have vastly different stability profiles (e.g., Fig. 28 to Fig. 31).\n- **In Section 5.2**, we built on these observations to propose a novel application: the early-training behavior of intrinsic-reward–based URLVR methods is positively correlated with the strength of a model’s prior. For example, the results in Fig. 6 show that Qwen models maintain stable training while Llama models collapse by step 40, suggesting that early dynamics (within ~50 steps) can qualitatively diagnose RL scalability.\n\nIn response to reviewer feedback, we now extend Section 5.2 by providing a fully quantitative method for assessing base model prior. We introduce a new metric, **Model Collapse Step**, to quantify the model–task prior, shifting the analysis from qualitative observation to a predictive quantitative measure.\n\nCompared with RLVR training or pass@k metrics, Model Collapse Step offers a more efficient way to detect **\"which base model is better (i.e., more suitable for RL)?\"**. It is well suited and useful for rapid iterative development of the foundation model, and we anticipate that it will attract more interest from industry researchers.\n\n**New Experiment: Quantifying Model-Task Prior Strength**\n\nWe measured three indicators across **7 models from 3 families** (OLMo, Llama, Qwen), all trained on DAPO-17k and evaluated on AIME24 (avg@32):\n\n1. **GT Gain (Ground Truth)**: Performance improvement from standard supervised RLVR with ground truth rewards, our gold-standard reference for true RL scalability.\n2. **Pass@k**: Computed as the gap between pass@256 and pass@1.\n3. **Model Collapse Step**: The training step when reward accuracy (majority voting pseudo-label correctness) drops below 1% during intrinsic URLVR training, our proposed diagnostic for model-task prior strength.\n\nResults:\n\n|                         | OLMo-2-1124-7B | Llama-3.1-8B | Qwen2.5-Math-1.5B | Qwen2.5-1.5B | Qwen2.5-7B | Qwen3-1.7B-Base | Qwen3-8B-Base |\n| ----------------------- | -------------- | ------------ | ----------------- | ------------ | ---------- | --------------- | ------------- |\n| **GT Gain**             | +0.42          | +1.01        | +3.96             | +3.96        | +6.67      | +7.08           | +17.08        |\n| **Pass@k**              | +6.67          | +3.33        | +30.00            | +20.00       | +60.00     | +36.67          | +56.67        |\n| **Model Collapse Step** | 34             | 40           | 160               | 221          | 245        | 280             | 383           |\n\n---\n\n### References\n\n[1] No Free Lunch: Rethinking Internal Feedback for LLM Reasoning"}}, "id": "HfurVqxCvs", "forum": "VesLZukY5E", "replyto": "VesLZukY5E", "signatures": ["ICLR.cc/2026/Conference/Submission23785/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23785/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission23785/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728758206, "cdate": 1763728758206, "tmdate": 1763729870343, "mdate": 1763729870343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "[Global Response 2] Self-Verify: Leveraging Generation-Verification Asymmetry for Scalable URLVR"}, "comment": {"value": "Multiple reviewers raised questions about our work's scope and future directions. Specifically:\n\n- **Reviewer Kjs9 (Weakness #1)**: Asks about differences from [1] and whether we contribute beyond RLIF analysis\n- **Reviewer pWtu (Weakness #1)**: Questions whether the paper is limited to analyzing existing methods\n\nWe address both concerns by highlighting a novel method beyond intrinsic rewards that we explore in our work: **self-verify**.\n\n**Building upon Our Original Analysis (Section 5.3)**\n\nWhile our main analysis (Sections 3-4) focuses on intrinsic reward methods, Section 5.3 explicitly identifies pathways to scale RL with URLVR methods at train-time, distinguishing our contribution from prior RLIF-focused work. We discuss two promising alternatives:\n\n- Leveraging unlabeled data or external rewards\n\n- Exploiting generation-verification asymmetries\n\nFor point 2, many problem domains exhibit difficulty asymmetries where verifying solutions is substantially easier than generating or answering them. For example, solving an equation or determining whether two matrices are inverses of each other. This enables models to self-reward through verification rather than confidence-based proxies, providing truly verifiable rewards aligned with task correctness without ground truth labels. This method is unsupervised and relies on internal feedback. More importantly, it is also a scalable RL approach at train time.\n\nIn our original submission, Section 5.3 proposed this direction. In response to reviewer feedback, we now provide empirical validation through a case study on the Countdown task.\n\n**Case Study:** **Self-Verify** **on Countdown Task**\n\n**Task Setup:**\n\n- Countdown task: Given numbers, form an arithmetic expression reaching a target value\n- Generation-verification asymmetry: Generation is challenging, but verification (checking if expression evaluates to target) is computationally trivial\n- Self-verify approach: The model generates solutions and evaluates them on its own, obtaining ground-truth rewards in absence of human labels\n\n**Experimental Setup:**\n\n- Model: Qwen3-1.7B-Base\n- Dataset: 4k training problems, 1k validation sampled from \"Jiayi-Pan/Countdown-Tasks-3to4\" on huggingface\n- Comparisons:\n  - **Oracle Supervision** (w/ ground truth labels as upper bound)\n  - **Trajectory-Level Entropy** (intrinsic reward method baseline)\n  - **Self-Verify** (our method using model's own verification)\n- Evaluation: Validation accuracy (avg@16), Self-Verify Reward (proxy reward used in training) and Ground Truth Reward (oracle reward computed using actual correctness)\n\nCountdown Validation (avg@16):\n\n| step                      | 0     | 60     | 120    | 180    | 240    | 300    | 360    | 420    | 480    | 540    | 600    |\n| ------------------------- | ----- | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ |\n| Oracle Supervision        | 9.57% | 62.88% | 77.55% | 80.16% | 82.77% | 83.45% | 83.52% | 84.71% | 86.08% | 86.38% | 85.06% |\n| Trajectory-Level  Entropy | 9.57% | 8.06%  | 0.07%  | 0.06%  | 0.03%  | 0.00%  | 0.00%  | 0.00%  | 0.00%  | 0.00%  | 0.00%  |\n| Self-Verify               | 9.57% | 10.11% | 14.82% | 12.69% | 28.23% | 44.17% | 49.85% | 52.07% | 52.20% | 55.83% | 58.85% |\n\nTraining Dynamics for Self-Verify (Self-Verify Reward and Ground Truth Reward)\n\n| Step                | 0     | 60    | 120    | 180    | 240    | 300    | 360    | 420    | 480    | 540    | 600    |\n| ------------------- | ----- | ----- | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ |\n| Self-Verify Reward  | 0.98% | 2.54% | 31.45% | 64.06% | 72.85% | 79.49% | 84.18% | 87.89% | 83.40% | 89.06% | 86.72% |\n| Ground Truth Reward | 3.38% | 4.69% | 8.03%  | 12.99% | 19.43% | 37.56% | 44.12% | 51.13% | 39.16% | 53.16% | 57.17% |\n\n**Key Results:**\n\nSelf-verify provides stronger supervision than intrinsic methods and approaches the effectiveness of oracle supervision:\n\n- Qwen3-1.7B-Base achieves higher validation accuracy with self-verify compared to Trajectory-Level Entropy\n- Performance even surpasses Oracle Supervision at later training steps\n\n**Crucially, reward hacking does not occur.** Although Self-Verify Reward is optimized towards 1, the validation performance and Ground Truth Reward increase throughout training. This validates that self-verify supplies stronger, more reliable signals than intrinsic methods while avoiding the reward hacking problems of confidence-based approaches.\n\n---\n\n### References\n\n[1] No Free Lunch: Rethinking Internal Feedback for LLM Reasoning"}}, "id": "iekY0eiSCq", "forum": "VesLZukY5E", "replyto": "VesLZukY5E", "signatures": ["ICLR.cc/2026/Conference/Submission23785/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23785/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission23785/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728921924, "cdate": 1763728921924, "tmdate": 1763729842432, "mdate": 1763729842432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}