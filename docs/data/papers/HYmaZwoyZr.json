{"id": "HYmaZwoyZr", "number": 15926, "cdate": 1758257179215, "mdate": 1762929929474, "content": {"title": "Mixing Configurations for Downstream Prediction", "abstract": "Humans possess an innate ability to group objects by similarity—a cognitive mechanism that clustering algorithms aim to emulate. Recent advances in community detection have enabled the discovery of configurations—valid hierarchical clusterings across multiple resolution scales—without requiring labeled data. In this paper, we formally characterize these configurations and identify similar emergent structures in register tokens within Vision Transformers. Unlike register tokens, configurations exhibit lower redundancy and eliminate the need for ad hoc selection. They can be learned through unsupervised or self-supervised methods, yet their selection or composition remains specific to the downstream task and input. Building on these insights, we introduce GraMixC, a plug-and-play module that extracts configurations, aligns them using our novel Reverse Merge/Split (RMS) technique, and fuses them via attention heads before forwarding them to any downstream predictor. On the DSNI 16S rRNA cultivation-media prediction task, GraMixC improves the R$^2$\n from 0.6 to 0.9 on various methods, setting a new state-of-the-art. We further validate GraMixC across standard tabular benchmarks, where it consistently outperforms single-resolution and static-feature baselines.", "tldr": "", "keywords": ["unsupervised learning", "hierarchical clustering", "16S rRNA", "cultivation media", "multi-resolution configurations", "tabular benchmarks"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/fb16931e460a7285b8e347a7bc6ff4023ad9f924.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose GraMixC, an approach for combining multiple hierarchical clusterings of a dataset for incorporation as features in a downstream predictor. The GraMixC method first generates multi-resolution cluster memberships using unsupervised learning methods, and then aligns these clusterings using their proposed Reverse Merge & Split method. The clusterings are then combined using attention heads, and integrated into downstream prediction. The authors demonstrate their approach on several datasets and find that incorporation of the cluster features boosts performance when added to existing deep tabular models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Despite clarity concerns noted below, the paper proposes an interesting and useful paradigm for integrating clustering results into downstream prediction models. \n- Existing experiments appear to be well executed with a good mix of synthetic, benchmark, and realistic datasets. Performance improvements seem strong, although need to be properly contextualized."}, "weaknesses": {"value": "This paper is written with insufficient clarity for the generalist ICLR audience, in my opinion. Overall, the inaccessibility of the manuscript prevents a fully informed review that is reflected in my confidence score. Detailed issues include:\n\n- The paper assumes deep familiarity with community detection literature (BlueRed Front, modularity-based clustering, parallel-DT) without providing a background or related works section. For example, the term \"configuration\" is only defined in Section 3.1 when it is central to understanding the paper. Similarly, the distinction between \"valid\" and \"pseudo\" configurations appears very late despite being referenced several times prior. Overall, this pattern of retroactively defining terms causes confusion.\n\n- I found the preliminary results section to be interesting but abrupt in its placement. Reading the paper chronologically, its not clear how the configurations were even generated by this point in the paper.\n\n- The experiments lack comparison to alternative approaches. For example, no comparison is made to DeepCluster despite the fact that it also uses clustering for downstream tasks.\n\n- In general, there is limited discussion of how GraMixC differs from other hierarchical or multi-resolution clustering methods in deep learning. This hinders readers from understanding the novelty of GraMixC's contributions. The manuscript would benefit greatly from a related works section.\n\n- Several ablations could be performed to better empirically motivate methodological choices. For example:\n  - Could fixed-γ Leiden replace BlueRed+parallel-DT?\n  - The necessity of register tokens is mentioned (page 3) but never ablated\n  - No comparison of GraMixC to more naive approaches for featurizing hierarchical clusterings."}, "questions": {"value": "I would appreciate seeing the experimental comparisons suggested above to demonstrate that GraMixC offers improvement upon more naive approaches."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "bNbYizUxDx", "forum": "HYmaZwoyZr", "replyto": "HYmaZwoyZr", "signatures": ["ICLR.cc/2026/Conference/Submission15926/Reviewer_2yMt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15926/Reviewer_2yMt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729911301, "cdate": 1761729911301, "tmdate": 1762926143873, "mdate": 1762926143873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "GpL51bWRus", "forum": "HYmaZwoyZr", "replyto": "HYmaZwoyZr", "signatures": ["ICLR.cc/2026/Conference/Submission15926/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15926/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762928902488, "cdate": 1762928902488, "tmdate": 1762928902488, "mdate": 1762928902488, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method for improving downstream prediction by using a form of multiscale clustering. The idea is to first extract configurations (hierarchical groupings of samples) learned from graphs. Then, align the training and test configurations using a proposed procedure, and finally fuse via attention heads trained for the prediction task. The authors evaluate their method on several datasets, including a biological task and a set of tabular benchmarks. Reported results show consistent improvements."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors demonstrate that the method can help improve the performance of several prediction models on multiple datasets.\nThe concept of using unsupervised learning to improve supervised tasks is interesting (although not new)."}, "weaknesses": {"value": "The paper is poorly written, hard to follow, and many details about the evaluations are unclear. \nThe motivation isn’t explained, and the flow of the abstract and intro is bad. The paper's main message is convoluted and poorly conveyed.\nIt is also not clear what in the paper is new, and what is a combination of existing methods.\n\nThe authors don’t explain the computational overhead of this method compared with simply applying the downstream classifier/regressor. Same for training.\nThe training protocol is not clear, so it's hard to judge the results. Specifically, how are hyperparameters tuned? Even the number of configurations dramatically affects the results.\nThere is no ablation (evaluating the effect of varying a parameter isn’t an ablation). The GC is an ablation for the attention, but other parts are not ablated."}, "questions": {"value": "here are no real comparisons to existing methods; the comparison to tsne, pca, ae isn’t really aligned with the paper's goal. The paper isn’t presenting a dimensionality reduction method for learning these features, so why does it make sense to compare to DR methods?\n\nWhat is the point of comparing UMAP to t-SNE using your method? Clearly, these methods generate different types of embeddings, and this visualization does not support the claims made in the paper. Additionally, the hyperparameters of t-SNE could also influence this property.\nWhat’s the added value of figure 3 in the main text? The metrics are already presented in the paper"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yAxtuh2knt", "forum": "HYmaZwoyZr", "replyto": "HYmaZwoyZr", "signatures": ["ICLR.cc/2026/Conference/Submission15926/Reviewer_QaB6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15926/Reviewer_QaB6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773707313, "cdate": 1761773707313, "tmdate": 1762926143272, "mdate": 1762926143272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the role of multi-resolution clustering configurations for improving downstream prediction. The authors introduce GraMixC, a plug-and-play module that extracts hierarchical configurations from unsupervised or self-supervised clustering, aligns them via a new Reverse Merge & Split (RMS) technique, and fuses them through attention heads before passing them to downstream predictors. The approach is evaluated on 16S rRNA cultivation-media prediction task (DSNI dataset) and several standard tabular and vision benchmarks. GraMixC improves regression and classification performance across baselines such as 3-layer MLPs, TabNet, TabTransformer, and FT-Transformer, achieving state-of-the-art results on DSNI with R² up to 0.98."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clear about its motivation with sufficient significance and quality.\n- The paper introduces a creative idea, mixing multi-resolution cluster configurations to capture global manifold structure in data.\n- GraMixC can be easily attached to existing predictors, which makes it broadly applicable.\n- The alignment procedure (RMS) and the configuration formulation are mathematically detailed, showing rigor behind the method.\n- The attention maps, ablation studies, and qualitative examples provide good intuition about why the approach works.\n- The application to microbiome data (16S rRNA) shows strong practical potential in scientific domains."}, "weaknesses": {"value": "- The paper is quite dense. Many sections, especially those describing the clustering and RMS steps, are mathematically overloaded and could use higher-level intuition or easy separation for better understanding.\n- The method largely builds upon existing clustering and attention mechanisms; the main novelty mainly lies in combining and aligning them.\n- The paper does not clearly quantify the computational overhead of multi-resolution clustering and RMS alignment compared to single-resolution or embedding-based alternatives, i.e no computational complexity clearly mentioned for the method. How does it scale to larger dataset?\n- The connection between “register tokens” in vision transformers and configurations is interesting but underexplained. The analogy could be more explicitly tied to the main contribution.\n- Especially for multimodal settings, some comparisons (e.g., contrastive or adapter-based models) would help position the work better.\n- Missing clear ablation studies regarding specific hyperparameters choice (e.g $\\theta$ and $k$)."}, "questions": {"value": "- How sensitive is GraMixC to the number of configurations? Do too many resolutions introduce redundancy or instability?\n- Could RMS alignment be learned end-to-end instead of computed heuristically?\n- What is the runtime overhead of GraMixC compared to the base models, especially on large datasets?\n- How generalizable is the method to high-dimensional continuous embeddings where discrete clustering is less natural?\n- What is the AE configuration you used for fig 8(b) performance?\n- What is the performance of GraMixC on any imbalanced datasets where some clusters are much smaller?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "u55hBi7XBa", "forum": "HYmaZwoyZr", "replyto": "HYmaZwoyZr", "signatures": ["ICLR.cc/2026/Conference/Submission15926/Reviewer_mKFU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15926/Reviewer_mKFU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995029665, "cdate": 1761995029665, "tmdate": 1762926142983, "mdate": 1762926142983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a hierarchical configuration learning module GraMixC as part of a prediction pipeline. It is evaluated on a bacterial species prediction task. The authors provide extensive experimentation and evaluation procedure"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe proposed approach is novel and is well presented\n-\tThe evaluation experiments are extensive and well documented\n-\tPerformance on the bacterial species prediction task is demonstrably improved"}, "weaknesses": {"value": "-\tThe proposed hierarchical configuration learning task is not well motivated or defined.  The evaluation protocol does not seem to be directly corresponding to the stated task of configuration learning. The main example is in predicting bacterial species. Perhaps the whole approach is a better method for the specific domain rather than a general-purpose method.\n-\tThe module seems to be evaluated in a somewhat ad hoc manner. For example, in Figure 1, bird is clustered with horse and deer but cat and dog are clustered separately from them at configuration level 1 (and from is on its own). Yet, cat and dog are closer to the horse and deer than a bird and most humans, even kids, would cluster those together (although language plays an important role in the process of categorization  https://doi.org/10.1002/wcs.96). Biology in general provides a great ground truth for evaluation of hierarchical similarity for example, using taxonomical hierarchy or species mimicry and there are plenty of benchmark datasets\n-\tThe entire set of evaluation metrics is somewhat puzzling. For example, it is unclear how attention maps are relevant to the method’s evaluation. In general, attention maps have been shown not to be particularly great about learning fine grain features. I am not sure what the authors are trying to show with the attention maps."}, "questions": {"value": "Can you clearly state and motivate the task/problem you are proposing? Can you reframe the evaluation procedure to fit the stated objective a priori?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4cgYbKXmUr", "forum": "HYmaZwoyZr", "replyto": "HYmaZwoyZr", "signatures": ["ICLR.cc/2026/Conference/Submission15926/Reviewer_Q9qr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15926/Reviewer_Q9qr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059402525, "cdate": 1762059402525, "tmdate": 1762926142602, "mdate": 1762926142602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}