{"id": "XCW1l9qcxy", "number": 23479, "cdate": 1758344397685, "mdate": 1759896812473, "content": {"title": "Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning", "abstract": "The composition of specialized tools offers a powerful approach for complex visual reasoning, particularly for tasks involving 3D spatial understanding. However, existing visual programming methods are often constrained by fixed toolsets or offline tool induction, which leads to suboptimal solutions and poor tool reuse. We introduce Transductive Visual Programming (TVP), a novel framework that dynamically evolves a library of reusable tools by learning from its problem-solving experience. TVP abstracts recurring solution patterns into new, higher-level tools, which are then used to construct simpler and more effective programs for new tasks.\nTVP abstracts recurring solution patterns into new, higher-level tools, which are then used to construct simpler and more effective programs for new tasks. On the challenging Omni3D-Bench, TVP establishes a new state of the art, outperforming both specialized vision-language models and prior visual programming systems. The evolved tools also exhibit strong generalization to out-of-domain queries on 3DSRBench, SpatialSense, and VGBench. Our work demonstrates that transductive tool evolution is a powerful and generalizable paradigm for building robust visual reasoning systems.", "tldr": "", "keywords": ["visual programming", "spatial reasoning", "tool abstraction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac8f370dcbd51024495e8dab1c5b059c08e20b98.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces an advanced visual programming method for 3D spatial understanding. The method starts with basic tools and automatically generates, stores, and optimizes useful tools (functions) while observing and solving problems. The reported quantitative results on 3D spatial reasoning benchmarks surpass all baselines, demonstrating the powerful potential of the method."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written. Figures 1 and 3 clearly present the motivation and method.\n\n- The self-evolving pipeline is carefully designed. The composition of spatial functions is novel in this area.\n\n- The quantitative results on the sampled SpatialScore-Hard are promising.\n\n- The case study and evolution curve effectively demonstrate the impact of evolution"}, "weaknesses": {"value": "Visual programming for spatial reasoning has also been used in 3D vision tasks, such as 3D visual grounding [1, 2], and [2] also employs a self-evolving process for spatial reasoning. The authors may consider discussing these works in Section 4.1.\n\n- In lines 159–161, the LLM explores $m$ programs, but only one final answer is provided. Is the final answer selected by the VLMs?\n\n- In line 169, there is mention of a quality threshold for functions, but the metric for quality is not provided.\n\n- In Figure 4(c), why is the performance with new APIs lower?\n\n- There is no analysis of the additional token costs for evolution. Is the cost worthwhile for the observed improvement in overall performance (3.4% in absolute terms, approximately 10% in relative terms)?\n\n- In line 165, does the \"final answer\" refer to the ground truth answer?\n\n- There is no analysis of the accuracy of the VLM judge.\n\n- Does the ordering of the Omni3D-Bench data used for codebase construction affect the codebase? Would altering the data order drastically impact the performance of the resulting code?\n\nReferences:\n\n[1] Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding, in CVPR, 2024.\n\n[2] Language-to-Space Programming for Training-Free 3D Visual Grounding, in EMNLP, 2025"}, "questions": {"value": "See the weaknesses section. If I have any misunderstandings, I would greatly appreciate the authors' clarification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TrpbXAAytY", "forum": "XCW1l9qcxy", "replyto": "XCW1l9qcxy", "signatures": ["ICLR.cc/2026/Conference/Submission23479/Reviewer_44Xv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23479/Reviewer_44Xv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760793843367, "cdate": 1760793843367, "tmdate": 1762942677364, "mdate": 1762942677364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Transductive Visual Programming (TVP), a novel framework that enables visual reasoning systems to evolve reusable tool libraries from problem-solving experience. TVP adopts a transductive approach: it first solves problems using basic vision tools, then abstracts recurring solution patterns into higher-level functions grounded in actual use. The architecture maintains a dual-library design: an Example Library storing verified program solutions and a Tool Library storing learned abstractions. Through iterative cycles of example accumulation, clustering, abstraction, validation, and merging, TVP progressively refines its tools and produces more efficient, accurate programs. On Omni3D-Bench, TVP achieves clear performance gains, surpassing GPT-4o and VADAR. The evolved tools exhibit strong zero-shot generalization to unseen spatial reasoning benchmarks, demonstrating robust transferability across domains."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- **Conceptual originality:** The paper introduces transductive tool evolution, which learns abstractions from experience rather than induction before use. This represents a genuine conceptual advance in visual programming and aligns well with human-like skill acquisition.  \n- **Technical soundness:** The dual-library architecture and full algorithmic specification (program generation, clustering, abstraction, validation, and merging) are rigorous and clearly grounded. The validation mechanism ensures newly learned tools remain correct and reusable.  \n- **Strong empirical results:** TVP achieves clear and consistent gains over prior visual programming systems and even large VLMs, particularly on complex 3D spatial reasoning and zero-shot transfer tasks."}, "weaknesses": {"value": "- **Limited scope of evaluation:** While visual programming was originally designed for 2D visual reasoning and perception tasks, this paper evaluates TVP only on 3D spatial reasoning. It remains unclear whether the proposed transductive abstraction also benefits conventional 2D visual reasoning benchmarks (e.g., MME, MMMU).  \n- **Heavy dependence on large proprietary models:** TVP’s components rely heavily on GPT-4o and its mini variants. It remains unclear how performance scales with smaller or open-source models, which may limit reproducibility and accessibility.  \n- **Computational overhead:** The pipeline includes iterative clustering, abstraction, validation, and merging — likely computationally expensive. The paper reports no quantitative analysis of time or resource cost, which would be important for assessing practicality."}, "questions": {"value": "1. Does TVP’s method also improve performance on conventional 2D visual reasoning tasks?  \n2. How sensitive is TVP to the choice of backbone LLM? Would similar gains be observed when replacing GPT-4o with smaller or open-source models, and could the authors provide scaling trends or partial ablations in this direction?  \n3. Could the authors provide a quantitative estimate of TVP’s computational and memory cost per iteration, and clarify whether any optimizations were applied to make the system practically deployable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wym7WJGgHM", "forum": "XCW1l9qcxy", "replyto": "XCW1l9qcxy", "signatures": ["ICLR.cc/2026/Conference/Submission23479/Reviewer_fsyr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23479/Reviewer_fsyr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537337114, "cdate": 1761537337114, "tmdate": 1762942677179, "mdate": 1762942677179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Transductive Visual Programming (TVP), a framework for visual reasoning that dynamically creates and refines its own library of tools. The core idea is to learn from problem-solving experience. TVP maintains a \"dual-library\" system:\n\n- An Example Library that stores (question, program, solution) tuples for high-quality solutions it has found.\n\n- A Tool Library that contains callable functions (tools).\n\nWhen faced with a new query, TVP retrieves similar examples from the Example Library to use as in-context demonstrations for generating a solution program. Critically, TVP periodically analyzes its Example Library, clusters similar solutions, and \"transductively abstracts\" recurring programming patterns into new, higher-level tools, which are then added to the Tool Library. This allows the system to evolve from basic tools to more complex, specialized, and reusable functions. The paper shows that this approach achieves SOTA on the Omni3D-Bench for 3D spatial reasoning and that the learned tools generalize well to unseen spatial benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The core idea of \"transductive abstraction\" from a library of successful solutions is elegant and well-motivated. It ensures that created tools are practically useful and grounded in experience, which is a clear advantage over VADAR's more speculative, question-based induction (as clearly shown in Fig 2).\n\n- The zero-shot generalization results on the SpatialScore-Hard collection (Table 2, Fig 5) are a key strength. Showing that tools learned only on Omni3D-Bench are effective on completely different datasets (3DSR-Bench, SpatialSense, VG-Bench) is a powerful demonstration that the system is learning robust and reusable reasoning patterns.\n\n- The paper includes a strong set of analyses, such as the reduction in program cyclomatic complexity (Fig 4a), the performance boost from using new tools (Fig 4c), and the visualization of the library evolution over time (Fig 6)."}, "weaknesses": {"value": "- The TVP framework itself is extremely complex and computationally expensive. For each query, it makes multiple LLM/VLM calls (retrieve, generate m programs, execute m programs, judge m programs). It then has a heavy, periodic maintenance loop that involves more LLM calls for clustering, abstraction, validation, and merging. This \"meta-cost\" of running the TVP framework is not discussed but seems prohibitively high, likely many times more expensive than just running a baseline model.\n\n- As mentioned, the field of tool-use and tool-creation is moving very fast. This paper proposes a new way to make tools. While this is a good contribution, it's an improvement on an existing line of work (VisProg -> ViperGPT -> VADAR -> TVP). It's not clear that this is a fundamentally new direction for the field, especially when compared to orthogonal approaches like differentiable soft-logic (e.g., NePTune).\n\n- The entire framework (generation, judgment, abstraction, merging) is orchestrated by powerful, closed-source models (GPT-4o and 4o-mini). This makes the system dependent on SOTA models and raises questions about its robustness. Would the framework collapse if these \"meta-LLMs\" were replaced with less capable open-source models? The quality of the \"judge\" and \"abstractor\" seems critical."}, "questions": {"value": "- Could the authors please comment on the computational cost of the TVP framework? Specifically, how many total LLM/VLM calls (and of what type, e.g., GPT-4o vs 4o-mini) are required, on average, to process a single query (including the amortized cost of library maintenance)? This \"meta-cost\" seems like a major factor in its practical utility.\n\n- The paper's related work cites other tool-creation work (e.g., Skillweaver, ASI) which also learn from \"trajectories\" or \"experience.\" Could you elaborate on the key differences between TVP's \"transductive abstraction\" and the skill-discovery methods used in these agentic works?\n\n- How robust is the TVP framework to the choice of its \"meta-LLMs\"? The system relies heavily on GPT-4o for crucial steps like quality judgment and program generation. If a weaker, open-source model (e.g., Llama3-8B) were used for these orchestration tasks, would the system still be able to successfully identify, abstract, and validate high-quality tools?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q8Nkeyg1ke", "forum": "XCW1l9qcxy", "replyto": "XCW1l9qcxy", "signatures": ["ICLR.cc/2026/Conference/Submission23479/Reviewer_JN1p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23479/Reviewer_JN1p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863218788, "cdate": 1761863218788, "tmdate": 1762942676999, "mdate": 1762942676999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Transductive Visual Programming (TVP), an innovative framework that enables visual language models to evolve by learning reusable tools from their own problem-solving experience.\n\nThe method’s dual-library closed-loop design is conceptually clear, technically comprehensive, and contributes a structured approach to self-improving reasoning in LLM-based systems.\n\nWhile the framework is well-presented and methodologically sound, key evaluation details (e.g., scoring criteria, abstraction prompts, complexity metrics, and computational cost) remain under-specified.\n\nEmpirically, the performance gains over ICL baselines are modest, and the claimed generalization largely reflects in-domain transfer rather than genuine out-of-distribution generalization.\n\nOverall, TVP is a promising and well-presented system paper: its transductive framework and dual-library design are conceptually valuable and clearly executed. Despite under-specified evaluation details and modest gains, I lean weak accept, contingent on clarifications."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Innovative and Well-Structured Framework\n\n+ The paper introduces Transductive Visual Programming (TVP), a novel and conceptually original framework that enables a model to iteratively learn reusable tools from its own problem-solving experience. Its dual-library closed-loop design (Example–Tool Library) is systematic and complete, effectively realizing a self-improving learning cycle.\n\nStrong presentation quality\n+ The paper is clearly written, logically organized, and well-illustrated with informative figures and detailed algorithms, making the methodology easy to follow."}, "weaknesses": {"value": "Lack of Transparency in Evaluation Mechanisms\n\nThe evaluation procedures governing both the Example Library and the Tool Library are under-specified, which raises concerns about reproducibility and interpretability. Specifically:\n+ Unclear criteria for Example Library admission.\nAlthough the paper states that a VLM judge scores each generated program and admits examples whose quality exceeds a threshold of τq = 8.5, it does not define the concrete scoring dimensions—such as logical correctness, semantic relevance, visual consistency, or execution success. The basis for selecting τq (e.g., validation tuning versus heuristic choice) is also not explained. Moreover, no analysis is provided regarding how scores vary across task types or problem complexity, leaving the quality control process for examples largely opaque.\n+ Opaque evaluation of tool abstraction potential.\nThe paper introduces an LLM-based cluster analyzer that outputs a textual “pattern” and a numeric “potential” score, using τpotential = 9.0 as a threshold for initiating tool abstraction. However, the work omits any description of how this score is computed, the intended scale, or the prompts used to elicit it. The rationale behind the chosen threshold is also absent. If this potential measure relies solely on LLM-as-judge scoring, it is likely susceptible to style bias and semantic drift, undermining objectivity.\n\nLimited Empirical Gains and Undefined Complexity Metric\n\nWhile the paper claims that TVP achieves improved performance and reduced program complexity through iterative transductive learning, the empirical evidence supporting these claims appears limited and partially confounded.\n+ Marginal performance improvement over ICL baselines.\nThe main results show that the Example Library-only configuration (essentially an ICL baseline) already achieves 31.7% overall accuracy, while the full TVP framework after three iterations reaches 33.3%. This modest +1.6% gain raises doubts about whether the improvement truly stems from the proposed abstraction mechanism, or instead from the growing in-context example set providing stronger template guidance to the LLM. The observed benefits may therefore largely reflect in-context pattern imitation rather than genuine tool learning. Moreover, performance surpasses the ICL baseline only at iteration 3, when computational cost and LLM usage are substantially higher—yet the paper offers no analysis of cost-effectiveness or scaling trade-offs.\n+ Undefined program complexity metric.\nThe paper reports that “program cyclomatic complexity decreases from 3.0 to 1.0,” using this as evidence that TVP learns simpler, higher-level abstractions. However, no formal definition or computation method for this complexity measure is provided. It is unclear whether this refers to classical McCabe complexity, the number of function calls, or another proxy metric. Without such clarification, the claimed reduction in complexity cannot be meaningfully interpreted or independently verified.\n\nHigh Computational Cost and Unanalyzed Efficiency\n\n+ TVP surpasses the ICL-only baseline only at iteration 3, implying that multiple costly iterations are required for modest gains (+1.6 pp). Yet the paper provides no analysis of runtime, token usage, or cost. Given that each iteration repeatedly invokes GPT-4o for program generation, judging, abstraction, and validation, the overall expense is likely high. Without quantitative efficiency reporting, it is unclear whether the observed improvement justifies the computational overhead or scales beyond small benchmarks."}, "questions": {"value": "Evaluation standards for Example and Tool Libraries\n\n+ Could the authors clarify the evaluation criteria for Example Library admission and Tool Library abstraction? Specifically, what dimensions does the VLM judge consider when scoring examples (e.g., logical correctness, semantic relevance, visual consistency)?\n+ How was the threshold τq = 8.5 chosen—through tuning, validation, or heuristic selection? Similarly, for the Tool Library, how is abstraction potential measured, what prompts are used, and on what basis was τpotential = 9.0 determined?\n+ If both evaluations rely solely on LLM-as-judge scoring, how do the authors control for potential bias or inconsistency across runs?\n\nEmpirical significance and complexity metric\n\n+ The improvement over the ICL baseline is relatively small (+1.6 pp) and only appears after three iterations. Could the authors provide more evidence that the observed gains stem from true tool abstraction rather than ICL-style template learning?\n+ Also, please clarify how program complexity is computed (e.g., McCabe complexity, function calls, or another proxy metric), and explain why its reduction should indicate better abstraction quality.\n\nComputational efficiency\n\n+ Since performance exceeds the ICL baseline only at iteration 3, what is the computational cost of running multiple iterations?\n+ Please report runtime, token usage, or cost per iteration, and discuss whether the modest gain justifies the overall expense or scales to larger datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MDcLogP2YV", "forum": "XCW1l9qcxy", "replyto": "XCW1l9qcxy", "signatures": ["ICLR.cc/2026/Conference/Submission23479/Reviewer_uXe8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23479/Reviewer_uXe8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762105040836, "cdate": 1762105040836, "tmdate": 1762942676794, "mdate": 1762942676794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}