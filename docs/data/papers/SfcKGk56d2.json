{"id": "SfcKGk56d2", "number": 2051, "cdate": 1756982319359, "mdate": 1759898172161, "content": {"title": "An Efficient Rubric-based Generative Verifier for Search-augmented LLMs", "abstract": "Search augmentation empowers Large Language Models with retrieval capabilities to overcome the limitations imposed by static parameters. Recently, Reinforcement Learning leverages tailored reward signals as a viable technique to enhance LLMs performing tasks involving search. However, existing reward modeling for search-augmented LLMs faces several limitations. Rule-based rewards, such as Exact Match, are verifiable but fragile to variations in expression and cannot be applied to long-form workloads. In contrast, generative rewards improve robustness, but designing verifiable and stable rewards for long-form workloads in dynamic corpora remains challenging and also incurs high computational costs. In this paper, we propose a unified and verifiable paradigm, ``nugget-as-rubric\", which treats atomic information points as structured evaluation criteria for different search-augmentation workloads. Short-form tasks correspond to a single rubric, whereas long-form tasks expand to multiple rubrics aligned with the question’s information needs. To support long-form settings, we design an automatic rubric construction pipeline based on query rewriting, which can automatically retrieve passages relevant to each question and extract rubrics from them, both from static corpora and from dynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a 4B-parameter efficient generative verifier under our proposed verifiable paradigm, which is trained via the idea of distillation and a two-stage strategy. Experimental results show that Search-Gen-V achieves strong verification accuracy across different workloads, making it a scalable, robust, and efficient verifiable reward constructor for search-augmented LLMs.", "tldr": "", "keywords": ["Retrieval-Augmented Generation", "Reinforcement Learning", "Verifiable Reward Modeling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b6a274908dc13d03871e0523fe7b7bcb2d91ecf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a unified and verifiable paradigm, namely ``nugget-as-rubric\", which treats atomic information points as structured evaluation criteria for different search-augmentation workloads. Short-form tasks correspond to a single rubric, whereas long-form tasks expand to multiple rubrics aligned with the question’s information needs. To support long-form settings, this paper designs an automatic rubric construction pipeline based on query rewriting, which can automatically retrieve passages relevant to each question and extract rubrics from them, both from static corpora and from dynamic online web content. Experimental results show that the proposed method and the trained model achieve strong verification accuracy across different workloads, making it a scalable, robust, and efficient verifiable reward constructor for search-augmented LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes \"nugget-as-rubric,\" a unified paradigm that treats atomic information points (nuggets) as structured evaluation criteria (rubrics). This approach successfully unifies the reward modeling for both short-form tasks (seen as a single rubric) and long-form tasks (seen as multiple rubrics). The method is designed to overcome the flaws of current reward models. It solves the \"fragility\" of rule-based rewards (like Exact Match), which perform poorly with variations in expression and cannot scale to long-form tasks. It also addresses the issues of generative rewards, which are often non-verifiable, unstable, and computationally expensive for long-form workloads.\n\n2. The paper introduces an automatic rubric construction pipeline. This pipeline uses query rewriting to retrieve relevant passages and extract nuggets from both static corpora and dynamic web content. This automated process replaces traditional manual annotation, which is costly, labor-intensive, and prone to bias.\n\n3. Experiments show that Search-Gen-V-4B achieves strong verification accuracy across different workloads. Notably, its performance is comparable to a much larger 200B+ parameter verifier model (Qwen3-235B-A22B-Instruct-2507) , making it a scalable, robust, and efficient verifiable reward constructor."}, "weaknesses": {"value": "1. While the automated rubric construction pipeline eliminates manual annotation, its iterative nature and reliance on an LLM-based judge result in slow convergence. The authors state that constructing rubrics for a single question takes, on average, one to two hours.\n\n2. The experiments for each workload (short-form and long-form) were conducted on only one representative dataset. The authors acknowledge that other datasets may have different characteristics, and future research should expand the evaluation to a wider range of datasets."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lcZ1jVLCVw", "forum": "SfcKGk56d2", "replyto": "SfcKGk56d2", "signatures": ["ICLR.cc/2026/Conference/Submission2051/Reviewer_r9Uu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2051/Reviewer_r9Uu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553438434, "cdate": 1761553438434, "tmdate": 1762916006960, "mdate": 1762916006960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Search-Gen-V, a rubric-based generative verifier for search-augmented LLMs. The key idea is to represent factual “nuggets” as structured rubrics that provide verifiable supervision for both short-form and long-form search tasks. Through an automated rubric-generation pipeline and a two-stage SFT + RL distillation process, a compact 4B-parameter verifier achieves performance comparable to much larger models on TREC, DeepResearchBench, and HotpotQA."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation and practical relevance: Addresses a genuine bottleneck in search-augmented LLMs—how to construct verifiable yet robust rewards for reinforcement learning with retrieval-based systems.\n\n- The nugget-as-rubric formulation elegantly bridges short-form and long-form search workloads under a single paradigm, improving consistency across RL reward modeling.\n\n- Search-Gen-V-4B is efficient as it achieves near-parity with 200B-scale models at significantly lower computational cost."}, "weaknesses": {"value": "- Several core components (e.g., rubric aggregation, DAPO optimization schedule, interaction between SFT and RL stages) are insufficiently detailed for replication.\n\n- The contribution mainly integrates existing ideas—rubric-based verification, nugget extraction, and reward distillation—into one pipeline rather than introducing a fundamentally new principle. The advantage of “nugget-as-rubric” over prior rubric or preference-based reward models (e.g., standard LLM judges) is not sharply articulated.\n\n- The verifier is not yet used in an RL loop to show downstream improvements. It would be better to provide some end-to-end demonstration of reward effectiveness.\n\n- No systematic study of the quality for rubrics."}, "questions": {"value": "See above. Some additional questions:\n\n- What is the runtime and cost of rubric generation per instance, and can it scale efficiently to large corpora?\n\n- How do you detect and filter erroneous or hallucinated rubrics during automatic construction?\n\n- How does the rubric weighting scheme influence performance? Have any learned aggregations been attempted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v7vmuAmKHL", "forum": "SfcKGk56d2", "replyto": "SfcKGk56d2", "signatures": ["ICLR.cc/2026/Conference/Submission2051/Reviewer_oyh1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2051/Reviewer_oyh1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815575244, "cdate": 1761815575244, "tmdate": 1762916006524, "mdate": 1762916006524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a **nugget-as-rubric** reward paradigm to deliver verifiable rewards for search-augmented models, train a 4B generative discriminator, Search-Gen-V, to assign verifiable scores for both short-form and long-form tasks (short/long text), which can be used as RL rewards or evaluation signals. The training follows two distillation-style stages: SFT → RL."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Proposes nugget-as-rubric to uniformly model short-form and long-form tasks, enabling a consistent, verifiable reward across settings.\n2. Trains a 4B Search-Gen-V via two stages (SFT → RL) whose effectiveness approaches Qwen3-235B-A22B-Instruct-2507."}, "weaknesses": {"value": "### Method\n\n1. The key notion of **“atomic golden information points (nuggets)”** within nugget-as-rubric is not explained rigorously and lacks a precise, formal definition. If this concept is derived from prior paper, the manuscript lacks **explicit citations** to those sources.\n\n2. In the RL training of Search-Gen-V, the format reward weight reaches 30%, which diverges from some mainstream setups (e.g., DeepSeek-Math). This might bias the model toward learning the format reward. It is recommended to provide reward curves to make the training dynamics clearer.\n\n### Baselines\n\n1. The evaluation datasets are limited: each of the short-form and long-form settings is validated on only 1 dataset, so generalization is not convincingly demonstrated.\n\n2. There is a lack of comparisons with other evaluation metrics. In Figure 4, the short-form workloads are compared against EM, but for long-form tasks there is no comparison to the original metrics of DeepResearch Bench (or other long-form benchmarks).\n\n3. Baseline coverage is insufficient. The method is mainly compared to other base models; it should also be compared to the generative reward model or the scalar reward model mentioned around line 159.\n\n### Experiments\n\n1. The experiments focus on the reward discriminant stage only. The paper does not demonstrate using Search-Gen-V rewards to actually train a search-augmented LLM, making it hard to validate the real effectiveness of Search-Gen-V. It is suggested to conduct RL experiments that compare Search-Gen-V against rule-based or reward model based in practice."}, "questions": {"value": "1. line 293, the paper states that Gemini-Flash aligns better with human inspection. Why, then, is Qwen3-235B used to generate the rubrics?\n\n2. line 333, the overlength penalty is introduced, but the manuscript lacks concrete details about how it is computed and applied. Could the authors clarify this component?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JPkQUEQF4V", "forum": "SfcKGk56d2", "replyto": "SfcKGk56d2", "signatures": ["ICLR.cc/2026/Conference/Submission2051/Reviewer_aboG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2051/Reviewer_aboG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849283175, "cdate": 1761849283175, "tmdate": 1762916006008, "mdate": 1762916006008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified \"nugget-as-rubric\" framework for reward modeling in search-augmented LLMs. It proposes an automatic pipeline to build rubrics from retrieved passages and trains an efficient 4B generative verifier, Search-Gen-V. Experiments show this 4B verifier achieves accuracy comparable to a 235B teacher model at a much lower computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic of this paper is crucial. The \"nugget-as-rubric\" approach provides a single, verifiable formulation that works across both short-form and long-form tasks.\n2. The automatic rubric construction pipeline reduces the need for costly manual annotation and helps to mitigate the \"pool bias\" found in traditional passage-labeling methods.\n3. The 4B Search-Gen-V model is highly efficient, addressing the high computational cost of generative rewards. It maintains strong performance, closely matching a 235B teacher model's judgments after a two-stage training strategy."}, "weaknesses": {"value": "1. The reliability of the proposed method needs further demonstration. \n    1. The correctness of the automatically generated rubrics is not independently verified. The pipeline's heavy reliance on an LLM-based Judge ($\\Psi$) means any bias or errors from this Judge are propagated into the \"ground truth\" rubrics. \n    2. The \"golden\" verification labels are derived from a teacher model (Gemini-2.5-Flash) whose own accuracy is not rigorously validated. Although the appendix includes a small human preference comparison showing a slight advantage over Qwen, this is insufficient to establish that the teacher has adequate labeling capability. Consequently, the reported F1 scores primarily reflect the student model's high *fidelity* to a potentially flawed teacher, rather than true factual accuracy. \n2. The paper lacks comparative experiments with more rule-based metrics, such as F1-score or ROUGE on short-form tasks, which are more robust baselines than Exact Match. Furthermore, the paper does not compare the reward accuracy against other powerful reward modeling approaches, nor does it include the end-to-end RL training comparison to validate the improvement over other reward modeling approaches.\n3. While the research is critical, the method is only suitable for knowledge verification for search-augmented LLMs and only on one dataset.\n4. The performance improvement of RL is limited. And the difference between Search-Gen-V-1.7B and Search-Gen-V-4B is 0.06 in average. Do these indicate the task is not very difficult?"}, "questions": {"value": "Please see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GgHZO6yC2B", "forum": "SfcKGk56d2", "replyto": "SfcKGk56d2", "signatures": ["ICLR.cc/2026/Conference/Submission2051/Reviewer_qChS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2051/Reviewer_qChS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935303606, "cdate": 1761935303606, "tmdate": 1762916005058, "mdate": 1762916005058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}