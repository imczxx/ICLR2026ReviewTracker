{"id": "mUB2N8L0vD", "number": 23882, "cdate": 1758349764633, "mdate": 1759896792720, "content": {"title": "Taming Massive Activations and Preconditioning Weights: GSR-Guided Quantization for W4A4", "abstract": "Large language model inference is constrained by memory and latency. Uniform low‚Äëbit quantization would help, but recent evidence shows massive activations‚Äîrare, extremely large, and largely input‚Äëinvariant per‚Äëtoken scalars‚Äîrather than generic channel‚Äëwise outliers. Methods that ‚Äúsmooth‚Äù activation outliers by migrating scale into weights are therefore less effective under this phenomenon. We address this by explicitly rotating activations and preconditioning weights so that both become easy to quantize.\n\nWe first identify that the \\textbf{grid-to-standard-deviation ratio (GSR)}, \n$\n\\rho^X_\\text{g} = \\frac{\\Delta_\\text{g}}{\\operatorname{std}(X_{\\text{c}})},\n$\nis a useful proxy for quantization sensitivity, as it measures the relative coarseness of quantization steps compared to the intrinsic variability of activations. Building on this insight, we introduce \\textbf{Flattened Rotation TSVD Quantization(FRTQ)}, a post-training quantization framework tailored for ultra-low-bit settings (e.g., W4A4). For activations (per-token), FRTQ learns orthogonal rotations at function-invariant points to contract GSR and stabilize quantization. For weights (per-channel), FRTQ fits a rank-$r$ truncated-SVD component to capture dominant directions, quantizes the residual, and realizes the correction via a fused low-rank path. All rotations are folded into adjacent weights, with only a single lightweight on-the-fly rotation required at the FFN down-projection.\n\nBy explicitly minimizing GSR, FRTQ aligns its updates with quantization error reduction. The method is purely post-training, requires only a small calibration set, and avoids gradient-based fine-tuning. Its alternating updates are simple, scalable, and kernel-friendly. Experiments across standard LLM backbones show that FRTQ consistently reduces GSR and improves W4A4 accuracy compared to smoothing-only or rotation-only baselines. On LLaMA-2 70B, FRTQ lowers $\\rho$ of activation by 28.69\\% compared to DFrot, and improves W4A4KV4 zero-shot accuracy by 1.25\\%, matching higher-bit baselines while incurring negligible runtime overhead.", "tldr": "FRTQ is a calibration-only W4A4 post-training quantizer that rotates activations and low-ranks weights to minimize the grid-to-stdev ratio, quells massive activation outliers, and matches higher-bit LLM accuracy without fine-tuning.", "keywords": ["Large Language Models", "Post-Training Quantization", "SVD", "Rotation", "Massive Activation"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec6fec0f3fb347b22802c22afe0565f9a54d0a11.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a post-training quantization method called FRTQ (Flattened Rotation tSVD Quantization) specifically to address the quantization challenges of large language models in the W4A4 setting. The authors note that traditional methods have limited effectiveness when dealing with \"massive activations\" (i.e., rare but extremely large activation values). Therefore, they propose: Grid-to-Standard-Deviation Ratio (GSR) as a proxy for quantization sensitivity; On the activation side, GSR is reduced through orthogonal rotation (DFRot + Uniform Preconditioning); On the weight side, a tuned truncated SVD (tSVD) is used to absorb the dominant direction and quantize the residual; All transformations are fused back into the weights, resulting in virtually no additional overhead during inference.\nExperiments show that FRTQ significantly outperforms existing PTQ methods on multiple LLaMA models, even approaching the performance of QAT methods, without requiring end-to-end training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Highly Innovative: This paper proposes GSR as a unified and scale-invariant quantization difficulty metric, with solid theoretical analysis.\n\nPractical: This method effectively addresses the difficulties of activation and weight quantization by combining rotation and low-rank decomposition, without requiring any training.\n\nExperimentally Sound: This method is validated on multiple LLaMA models and multiple evaluation datasets, with convincing results.\n\nEfficient: This method requires only a small amount of calibration data and does not require gradient backpropagation, making it suitable for practical deployment.\n\nHighly Reproducible: Detailed algorithm pseudocode and experimental setup are provided."}, "weaknesses": {"value": "Strong theoretical assumptions: The linear relationship between GSR and quantization error relies on the Laplace distribution assumption. While experimentally validated, the theoretical generalizability requires further verification.\n\nLimited model generalization: The experiments only cover the LLaMA family and do not test other architectures (such as encoder-decoder and vision-language architectures).\n\nInadequate runtime evaluation: While claiming \"negligible runtime overhead,\" no actual latency or throughput data is provided.\n\nInsufficient comparison with state-of-the-art methods: While comparing various PTQ methods, no comparison is made with recent, more advanced low-bit QAT or mixed-precision methods.\n\nThe analysis of the causes of \"massive activations\" is shallow: While noting their existence, the authors do not delve into their origins or whether they can be avoided through architectural design."}, "questions": {"value": "Supplement experimental verification of other model families (such as BERT, T5, ViT, etc.);\n\nProvide comparative data on actual inference speed and memory usage;"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9yCdhCmze1", "forum": "mUB2N8L0vD", "replyto": "mUB2N8L0vD", "signatures": ["ICLR.cc/2026/Conference/Submission23882/Reviewer_eAW1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23882/Reviewer_eAW1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636539254, "cdate": 1761636539254, "tmdate": 1762942839343, "mdate": 1762942839343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identify a metric named GSR. Empirical/theoretical analyses show that GSR is closely related to quantization error. Based on these findings, the authors further modify DFRot on the activation side, and update the low rank decomposition quantization method on the weight side. The experiment evaluations assess the proposed method to some extent."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The motivation of the paper is clear. The authors evaluate the heuristic metric GSR with both theoretical and empirical analyses, and then use the metric to improve existing quantization method.\n2. The proposed method is backed up by the experiments.\n3. The paper is well-structured overall."}, "weaknesses": {"value": "1. The notation $R$ is confusing. In Sec. 4, it appears to represent two different meanings.\n2. In Table 1, the authors states that \"SpinQuant & OSTQuant use quantization-aware training to optimize $R_1$\". First, $R_1$ is not defined elsewhere in the paper. Second, the paper should clarify what it means by quantization-aware training. Both SpinQuant & OSTQuant define themselves as Post-training Quantization (PTQ) method rather than QAT. **Given this, please justify the advantages the proposed method and explain the comparison when its performance is worse than SpinQuant/OSTQuant**\n3. Recent baseline methods/models such as FlatQuant/Qwen are not included.\n4. More benchmarks like MMLU should be included in addition to PPL and common-sense QA tasks.\n5. The description of the proposed method is difficult to follow. It is not clearly presented. I could only understand the algorithm by the pseudo code in the Appendix. Also, the multi-paragraph abstract reads oddly and feels informal.\n6. A runtime comparison with other baseline methods is required to demonstrate efficiency.\n\nMinors:\n1. In Line 193, 196 & 205, repeated \"equation\".\n2. The text in Fig.1 is too small to read."}, "questions": {"value": "See Weaknesses above. Please address Weakness 2 carefully and present the advantages of the proposed method over SpinQuant / OSTQuant / FlatQuant, especially where the proposed method underperforms."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9mtAJ4Iq2l", "forum": "mUB2N8L0vD", "replyto": "mUB2N8L0vD", "signatures": ["ICLR.cc/2026/Conference/Submission23882/Reviewer_LRtm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23882/Reviewer_LRtm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726901335, "cdate": 1761726901335, "tmdate": 1762942838992, "mdate": 1762942838992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces FRTQ, a post-training quantization method for W4A4 LLMs that explicitly reduces the grid-to-std ratio (GSR, Œî/œÉ) for both activations and weights. On activations, it performs Uniform Preconditioning (UP) to equalize per-row magnitudes before a DFRot refinement, contracting GSR without incurring quantization error. On weights, it adds a tuned SVD (tSVD) branch and row-wise ‚Ñì‚àû updates to depress per-row maxima of the INT4 residual, tightening the effective grid and thus lowering rounding error. Rotations are fused into weights (one lightweight rotation remains at FFN down-proj). Across LLaMA 7B‚Äì70B, FRTQ improves perplexity and zero-shot over QuaRot/DFRot and sometimes approaches QAT methods, while using tiny calibration and no backprop."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple and practical pipeline (UP+DFRot + tSVD), fully PTQ with minimal calibration; rotations fused, negligible runtime overhead.\n2. Ablations & statistics (GSR/QErr tables) convincingly show how each component contributes and why it helps."}, "weaknesses": {"value": "1. The font in Figure 1 is too small to read comfortably. Please increase the label and tick font sizes and/or provide a higher-resolution version in the main paper or the appendix.\n\n2. Figure 1 aggregates pre-RMSNorm activations across layers but, on the weight side, uses only layer-0 query ùëäùëÑ. To support the ‚Äúnear-Laplace tails‚Äù and the error‚ÄìGSR trend more broadly, please: (i) report CCDFs for several representative weight matrices beyond layer-0 ùëäùëÑ; (ii) show activation CCDFs at the actual quantization insertion points (e.g., inputs to the main linear projections), not only pre-RMSNorm; and (iii) include the error-vs-ùëü plots for a few deeper layers to verify the universality of the slope across the stack.\n\n3. The paper would benefit from a side-by-side comparison with DuQuant [1] under matched settings‚Äîsame checkpoints, KV precision, group sizes, calibration set size, evaluation harness/version, and decoding setup. Because FRTQ emphasizes tiny calibration (e.g., 1√ó2048 tokens for rotations, 128√ó2048 for weight calibration) and fused rotations with near-zero runtime overhead, an apples-to-apples table would clarify the accuracy/latency/compute trade-offs relative to DuQuant. If DuQuant requires materially different calibration or runtime transforms, please discuss how these differences affect the headline numbers.\n\n[1] Lin et al. DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs. NeurIPS 2024."}, "questions": {"value": "Please see the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gfZy3aN2jT", "forum": "mUB2N8L0vD", "replyto": "mUB2N8L0vD", "signatures": ["ICLR.cc/2026/Conference/Submission23882/Reviewer_Un2c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23882/Reviewer_Un2c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943047537, "cdate": 1761943047537, "tmdate": 1762942838707, "mdate": 1762942838707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Flattened Rotation tSVD Quantization (FRTQ), a post-training quantization (PTQ) framework designed for ultra-low-bit settings (e.g., W4A4). The key idea is to minimize a novel metric called the Grid-to-Standard-Deviation Ratio (GSR), defined as the quantization grid size Œî divided by the standard deviation œÉ. The authors theoretically and empirically show that quantization error scales linearly with GSR."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper‚Äôs motivation is clear ‚Äî it aims to provide a quantifiable measure (GSR) for characterizing quantization difficulty.\n- The authors combine theoretical and empirical analysis to validate the GSR metric and apply it to guide improvements in quantization.\n- The proposed approach is experimentally evaluated on multiple models, showing partial evidence of its effectiveness."}, "weaknesses": {"value": "- The authors need to compare with related recent baselines, such as FlatQuant.\n- The paper states negligible overhead, yet provides no runtime/memory comparisons. Numbers are needed to substantiate practicality.\n- The study focuses on perplexity and a set of zero-shot commonsense tasks; broader benchmarks (e.g., MMLU/BBH) would better probe reasoning/generalization and could reveal trade-offs.\n- What's the definition of $R_1$ in Table 1 caption?"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WVZaZzAP1j", "forum": "mUB2N8L0vD", "replyto": "mUB2N8L0vD", "signatures": ["ICLR.cc/2026/Conference/Submission23882/Reviewer_Vz2p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23882/Reviewer_Vz2p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995306886, "cdate": 1761995306886, "tmdate": 1762942838372, "mdate": 1762942838372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}