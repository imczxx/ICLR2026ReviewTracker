{"id": "yDbJHQlrbf", "number": 12566, "cdate": 1758208654456, "mdate": 1763650998883, "content": {"title": "Code Driven Planning with Domain-Adaptive Selector", "abstract": "Large Language Models (LLMs) have been widely adopted as task planners for AI agents in sequential decision-making problems, leveraging their extensive world knowledge. However, the gap between their general knowledge and environment-specific requirements often leads to inaccurate plans. To address this, existing approaches rely on frequent LLM queries to iteratively refine plans based on immediate environmental feedback, which incurs substantial query costs. However, this refinement is typically guided by short-term environmental feedback, limiting LLMs from developing plans aligned with long-term rewards. We propose **Co**de Driven **P**lanning w**i**th Domain-Adaptive Sele**C**tor (CoPiC). Instead of relying on frequent queries, CoPiC employs LLMs to generate a diverse set of high-level planning programs, which iteratively produce and refine candidate plans. A trained domain-adaptive selector then evaluates these candidates and selects the one most aligned with long-term rewards for execution. Using high-level planning programs as planner and domain-adaptive selector as estimator, CoPiC improves planning while significantly reducing query costs. Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC outperforms advanced LLM-based baselines, achieving an average (1) 19.14\\% improvement in success rate and (2) 79.39\\% reduction in token costs.", "tldr": "CoPiC presents a novel planning paradigm that leverages LLM to generate diverse planning programs and employs a domain-adaptive selector to select high-quality plans, reducing LLM query costs while boosting performance.", "keywords": ["LLM-based Planning", "Planning Programs", "Domain-Adaptive Selector", "Large Language Models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/efab17dab03520823133b8c5ca01ff9d6b370a62.pdf", "supplementary_material": "/attachment/40686509fc1f3e4823532541b3a5983719942efa.zip"}, "replies": [{"content": {"summary": {"value": "This work introduces CoPic, a framework to allow LLMs to generate planning-programs, which are used to generate and refine a set of candidate plans for the agent. These planning programs are generated by a LLM Planner, which are able to cheaply and efficiently generate plans. The selector selects the best plan from the plans generated by all of the programs. At runtime, this set of planning programs are used to generate candidate plans which the selector picks from to execute. During the learning phase, the history of the executions are used to refine the programs, as well as finetune the selector with RL. Results on 3 different environments show that CoPic exhibits better data efficiency and cost reduction while performing better than existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This work evaluates their method on several different and challenging benchmarks, and also against many different strong baselines, exhibiting strong benchmark results. Analysis on the Planning Quality as well as the Planning Efficiency in terms of cost is meticulous and well documented in order to show that their technique shows better performance while reducing costs. Ablations in Section 5.4, notably on the open-source LLMs as well as the program evolution are targeted and insightful."}, "weaknesses": {"value": "One weakness would be the lack of ablations around the selector module itself. The selector, as detailed in 4.1.2, is trained to output a scoring of the plans as a probability distribution across each of the candidate plans. This small selector is the only module that is trained with gradient based methods and RL, which the other non-training based baselines such as AdaPlanner and Prospector do not use. There is no ablation on this module itself, except in Section 5.4.3, where they do an ablation against a random selector policy. \n\nOne obvious ablation and experiment would be to compare with using a prompted LLM-as-a-Judge to simply select the plans that are generated by the planning programs. This ablation would provide greater insight into how this RL finetuning affects the performance.\n\nSimilarly, the baselines also lack any LLM training-based methods, opting only for feedback-based LLM planning methods [line 337] and Non-LLM training-based methods [lines 339-340]. There is already training data used during the learning phase (Section 4.2.1), so why not just do finetuning instead, maybe with some filtered behavior cloning [1], or even generating plan data and finetuning a planner policy. [2]\n\nFrom a presentation standpoint, Section 4 is somewhat difficult to read and understand. It is unclear what a \"planning program\" is, from a first read and how this kind of planning program actually generates plans to be executed. Presentation could be clearer on the structure of this kind of program as well as what the output looks like.\n\n[1] Pan, Jiayi, et al. \"Autonomous Evaluation and Refinement of Digital Agents.\" First Conference on Language Modeling.\n[2] Erdogan, Lutfi Eren, et al. \"Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks.\" Forty-second International Conference on Machine Learning."}, "questions": {"value": "During the learning phase, in Section 4.2.1, is the LLM Planner itself learning from the experience of generating planning programs at all? For instance, when it gets feedback on the performance of the plans and it updates the plans in the environment, is there any feedback to the LLM Planner itself so that it can generate better planning programs in the future? If so, it would be great to see how the performance of the initial plans of the LLM Planner performs as it sees more and more tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gzAnCKSO0b", "forum": "yDbJHQlrbf", "replyto": "yDbJHQlrbf", "signatures": ["ICLR.cc/2026/Conference/Submission12566/Reviewer_R9TH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12566/Reviewer_R9TH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894980389, "cdate": 1761894980389, "tmdate": 1762923420820, "mdate": 1762923420820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CoPiC, a planning framework that leverages LLMs and RL. Unlike existing methods that rely on frequent, costly LLM queries and short-term feedback, CoPiC generates multiple high-level plans (using LLMs) and employs a domain-adaptive selector to choose the plan most aligned with long-term rewards. The selector is fine-tuned via reinforcement learning.\nSolution was tested in different problems and showed improvements over baselines on both cost and SR."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Reduces LLM query costs while improving planning quality.\nDemonstrates generalization and domain adaptation across diffferent environments."}, "weaknesses": {"value": "Solutions are highligh dependent on the quality of the generated plans. \nSelector’s effectiveness depends on RL fine-tuning."}, "questions": {"value": "- PPDL approaches that use LLM alonside additional solvers have the potential to also be more cost effective and achieve good performance. Have you considered using one of these solutions as baseline? \n- How sensitive is CoPiC’s performance to the diversity and quality of the generated plans? \n- How does the number of generated plans affect performance (and cost)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0CyRr8KYGY", "forum": "yDbJHQlrbf", "replyto": "yDbJHQlrbf", "signatures": ["ICLR.cc/2026/Conference/Submission12566/Reviewer_4nz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12566/Reviewer_4nz9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945794723, "cdate": 1761945794723, "tmdate": 1762923419367, "mdate": 1762923419367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CoPiC, a query-efficient LLM planner that generates various code plans before having the best plan selected by a domain-active selector, executed in the environment, and replan until success. The domain-active selector is finetuned via PPO on each domain’s training sets before inference. Results are presented on ALFWorld, NetHack, and StarCraft II Unit Building showing that CoPiC achieves superior performance at ultra-low cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "$Originality$: This paper essentially introduces a method that combines code planning and a BoN judge that selects the best code plan at each step; however the judge is fine-tuned on the domain which addresses past issues with generalization to new domains.\n\n$Quality$: The performance of the method is very high compared to past methods at very low cost.\n\n$Clarity$: The motivation of the paper is clear and the method and results are clear to understand.\n\n$Significance$: LLM planning is very query-inefficient due to dense interactions at each step making bar to entry difficult; this method introduces an alternative code planning approach that brings cost down while also boosting performance."}, "weaknesses": {"value": "- This approach requires a lot of setup to be applied to a new domain compared to the zero-shot approaches tested on in the paper including\n  - Finetuning to a training dataset which may not be available; experiment 5.4.3 demonstrates the necessity of the selector making this a requirement\n  - Prompt engineering the structure of the code to ensure the agent can plan via code which requires domain expertise; the prompts in Appendix H include many examples for code including\n    - Assistant methods for ALFWorld in H.1.1\n    - Heavy prompt engineering for Nethack and StarCraft 2 in H.2.1 and H.3.2 including data imports and code structure whose structure shows in I.2 and I.3 for code evolution"}, "questions": {"value": "My main concern is that the high performance is due mainly to the effort put into initial prompts given that the code evolutions are heavily biased towards the code structure in the prompt. \n  - The ablation in 5.4.3 seems to convey that the selector is a necessary part for CoPiC to work; however, the baseline without the selector seems to fail at every other trial. This looks like a code formatting bug, why is this happening?\n  - Have the authors considered generating full-length trajectories instead of code plans? This maintains cost-effectiveness while improving ease of adoption to new domains"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iUcFfrwq1F", "forum": "yDbJHQlrbf", "replyto": "yDbJHQlrbf", "signatures": ["ICLR.cc/2026/Conference/Submission12566/Reviewer_rsyp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12566/Reviewer_rsyp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991734339, "cdate": 1761991734339, "tmdate": 1762923418833, "mdate": 1762923418833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CoPiC (Code-Driven Planning with Domain-Adaptive Selector): it uses multiple LLM-generated “planning programs” (MoE-style experts) as a low-cost plan supply, paired with a domain-adaptive selector that, at each step given the environment observation, scores candidate plans and picks the best. It then leverages interaction history to “evolve” the programs, and fine-tunes the selector with RL using LoRA+PPO, forming a closed loop of “generate–select–learn.” Compared with high-interaction step-by-step querying, CoPiC balances long-horizon return alignment with lower query cost, achieving on average +19.14% success rate and −79.39% token cost on ALFWorld, NetHack, and StarCraft II Unit Building. Ablations removing the selector / program evolution / varying the number of experts validate the contribution of each module. The core contribution is integrating code-based planning programs with a learnable selector, providing (i) a reusable, evolvable plan representation, (ii) a selection mechanism oriented to long-term returns, and (iii) a low-cost path to cross-environment generalization."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Originality: Connects “multi planning programs (MoE-like experts) → domain-adaptive selector → program evolution + selector RL fine-tuning” into a closed-loop paradigm, unifying generate–select–learn; using executable programs to carry plans enables continual evolution and reuse, distinct from single-trajectory progression or heuristic voting.\n\nQuality: Delivers significant success-rate improvements alongside large token-cost reductions across text-world, roguelike, and strategy-building environments; includes key ablations (selector removal, expert-count sweep, evolution curves) and multi-seed statistics. The pipeline, pseudocode, prompt templates, and hyperparameters are public, forming a solid evidence chain.\n\nClarity: Top-down exposition (motivation → framework → training/inference → experiments/ablations). Core concepts are defined at first mention; the overview figure clearly depicts data flow; main tables use unified metrics (SR/Step/Cost); evolution/ablation curves localize sources of gain and cost components, facilitating reproduction.\n\nSignificance: Provides a general and practical solution path to “high interaction cost, short-sighted feedback, domain gap”; effective on both closed- and open-source bases, nudging the field from “frequent-dialogue agents” toward “low-interaction, strong planning, evolvable” paradigms."}, "weaknesses": {"value": "There is a setup mismatch on ALFWorld—CoPiC learns on the training set and is evaluated zero-shot on the test set, whereas some baselines (e.g., Reflexion/AdaPlanner) perform online learning/adaptation on the test set. The paper argues fairness by claiming a similar number of learned tasks, but differences in adaptation timing and data distribution introduce confounders that compromise comparability. Since a core conclusion is “under fair conditions, we improve success rate while significantly cutting cost,” inconsistent training/evaluation paradigms—even with a similar number of tasks—can over- or under-estimate any method. I suggest adding a replication with fully aligned protocols: (i) unified training/evaluation paradigms (“train-only learning” vs. “test-time online learning”), (ii) unified base models/variants, (iii) shared seeds and hardware/concurrency, to remove bias from protocol differences."}, "questions": {"value": "Possible insufficiency of candidate “diversity”: The paper treats n>1 as evidence of diverse candidates but does not explain how to prevent highly similar/duplicate plans across experts (e.g., decoding-parameter differences, template preferences, anti-duplication constraints, similarity filtering/reranking). Diversity metrics and controls are also missing.\n\nUnclear handling of failures and exceptions: The algorithm appears to replan stepwise (“regenerate and reselect each step”), but it’s unspecified whether mid-execution failures or unmet preconditions trigger local repairs versus whole-plan replacement, and what the retry budget and triggers are."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eU0exC7wEn", "forum": "yDbJHQlrbf", "replyto": "yDbJHQlrbf", "signatures": ["ICLR.cc/2026/Conference/Submission12566/Reviewer_9w8c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12566/Reviewer_9w8c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997465956, "cdate": 1761997465956, "tmdate": 1762923418459, "mdate": 1762923418459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We sincerely thank all reviewers for their time and valuable feedback. We are encouraged that the reviewers found our work to have an excellent presentation (Reviewer 9w8c), clear motivation (Reviewer rsyp), convincing results (Reviewer 9w8c, rsyp, 4nz9, R9TH), as well as a general and practical core contribution, namely the iterative optimization of planning programs and the domain-adaptive selector (Reviewer 9w8c).\n\n### **Summary of Revision**\nIn response to the questions raised by each reviewer, we have revised our paper. The summary of these revisions is as follows.\n\n- For Reviewer 9w8c\n    - Add the result of CoPiC(TSL)—that is, CoPiC with Test Set Learning—in **Table 1** to illustrate that CoPiC still outperforms baselines under the same experimental setup.\n    - Add the Quantification of Diversity of planning programs in **Appendix E.10** to illustrate that the planning programs generated by LLM are sufficiently diverse.\n- For Reviewer rsyp\n    - Add the result of CoPiC(TSL)—that is, CoPiC with Test Set Learning—in **Table 1** to illustrate that CoPiC can also conduct online learning on the test set, similar to the baselines.\n- For Reviewer 4nz9:\n    - Add a new baseline, the PDDL-based method LLM-DP, in **Table 1**.\n    - Add an analysis of the impact of temperature variations on CoPiC's performance in **Appendix E.8**.\n- For Reviewer R9TH:\n    - Add the result of CoPiC(LaJ)—that is, CoPiC taking LLM-as-a-Judge as the selector—in **Table 1** to illustrate the necessary of domain-adaptive selector.\n    - Add a new baseline, TWOSOME, in **Table 1**.\n    - Add an example of planning program in **Section 4.1.1** for aiding understanding.\n    - Add an analysis of the impact of experiences from other type of tasks on CoPiC's initial performance in **Appendix E.9**.\n\n\nWe believe these revisions wil significantly strengthen the paper and directly address the concerns. Below, we responsed to each reviewer's points in detail."}}, "id": "StJwxxuVga", "forum": "yDbJHQlrbf", "replyto": "yDbJHQlrbf", "signatures": ["ICLR.cc/2026/Conference/Submission12566/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12566/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission12566/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763650683837, "cdate": 1763650683837, "tmdate": 1763650683837, "mdate": 1763650683837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}