{"id": "RsUtS1YOnK", "number": 19551, "cdate": 1758297178948, "mdate": 1759897033172, "content": {"title": "Differentiating without Partial Evaluation", "abstract": "In the physical sciences, the gradient of a model is often simplified into a\ncompact form ideal for a given context to be interpretable and more efficient;\nin fact, sometimes the efficiency of evaluation can be improved by an\nasymptotic factor due to symmetries. To learn interpretable surrogate models\nthat accelerate physics simulations, a differentiation system capable of\ncompact and unevaluated gradient expressions is highly desirable. However,\nstandard symbolic and algorithmic differentiation both start by partially\nevaluating the model. After this points, the gradients irreversibly become\nblackboxes with potentially obscure performance ceilings. Based on the\nobservation that composition is one of two combinators that form a complete\nbasis, we complete the chain rule with a second rule that enables\ndifferentiation without any form of evaluation. Using a prototype\nimplementation, we obtain compact gradient expressions for an MLP and a common\nphysics model that, historically, resisted algorithmic differentiation. Lastly,\nwe discuss the theoretical and practical limitations of our approach.", "tldr": "", "keywords": ["automatic differentiation", "interpretable surrogate model"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3da70de7f41f1bd32517a67437d835ed1c829bc0.pdf", "supplementary_material": "/attachment/b9868d9782d24e8c78ef13172dfe04cd25ea1bb6.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes applying reverse mode automatic differentiation (AD) to functional programs expressed as combinators, rather than traditional methods applied to imperative programs or functional programs expressed in variants of the lambda calculus. The key contribution is deriving formulas to differentiate the B and C cominators, B denoting function composition and C denoting swapping the first two arguments of a curried function. With this, AD becomes syntactic and trivial: basis functions f are replaced with their reverse mode transformations f', B is replaced with its reverse mode transformation B', and C is replaced with its reverse mode transformation C'.\n\nI haven't slogged through the math. But I will give the authors the benefit of the doubt. If there are errors, they must be minor and can be easily fixed as in principle it is possible to do what is claimed in the submission. But it is not necessary to do so, given my comments below."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The general idea of applying AD to combinators is sound and novel. The reverse mode transformation of B is well known and trivial. It is the chain rule. It has been presented in many places, among others in Pearlmutter & Siskind (TOPLAS 2008). The reverse mode transformation of C is novel but straightforward.\n\nIt would be great to see the general approach fleshed out to all combinators, a Turing complete set of combinators, or at least a more powerful set of combinators. This is not done in this paper. I encourage the authors to do so.\n\nIt would be great to see this turned into a practical and efficient AD system that is competitive with the likes of PyTorch and JAX, one that could generate efficient gradients of arbitrary code written in an inhabitable functional programming language that ran competitively on GPUs.\n\nI encourage the authors to continue this line of work to flesh out the above."}, "weaknesses": {"value": "The general claim that all other approaches to (reverse mode) AD require tracing/partial evaluation is false. Forward mode AD using dual numbers does not required tracing. Many classical AD systems, like Adifor and Tapenade do source-to-source transformation for forward and/or reverse mode AD without tracing or partial evaluation. This has been done at least since the JAKE system, Speelpenning (1980). Even for functional programming, Pearlmutter & Siskind (TOPLAS 2008) did this for the untyped lambda calculus. Many follow-on authors elaborated on this. These methods handle Turing complete languages. Since, one can formulate B and C as trivial lambda-calculus expressions, the results in this submission trivially follow from prior work.\n\nThe B and C combinators are not Turing complete. They are not even very powerful. You cannot write map and reduce in them. You cannot even swap other than the first two arguments of a function.\n\nThe key limitation of this work is that neither B nor C involve fanout. They are both linear operators. Reverse-mode AD is trivial for linear operators. Reverse-mode AD becomes difficult when there is fanout because you need to handle accumulation. Fanout is needed to handle combinators such as S and Y.\n\nThe appendix gives a method for transforming what the authors call stable iterate-to-fixed-point operators, i.e. ones that have a number of iterations are not dependent on floating point values There has been work on transforming general iterate-to-fixed-point operators."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NgPp403oQr", "forum": "RsUtS1YOnK", "replyto": "RsUtS1YOnK", "signatures": ["ICLR.cc/2026/Conference/Submission19551/Reviewer_reKi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19551/Reviewer_reKi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761430108809, "cdate": 1761430108809, "tmdate": 1762931434738, "mdate": 1762931434738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “Differentiating without Partial Evaluation” introduces an approach to symbolic differentiation that avoids the traditional reliance on partial evaluation, primarily through an alternative use of combinatory logic. The authors extend standard differentiation beyond the chain rule via a second rule derived from B/C combinators allowing gradients to be derived without any partial evaluation of the program.  The authors argue this avoids expression swell and keeps gradients symbolic until the very end, contrasting with Auto Diff systems that first evaluate or trace to a graph. This perspective on framing differentiation around a complete combinator basis and giving explicit pullback rules for B and C is an original synthesis in ML/AD literature. \nThe paper provides a good survey of existing literature on this topic. \nThe report is largely complete in scope for a theoretical proposal. It provides sufficient background, explains the motivation and theory behind the second differentiation rule, shows its application through concrete examples (MLP gradients, Hartree-Fock energy, conjugate gradient optimization), and discusses both theoretical and practical limitations. However, its primary implementation is a proof-of-concept within a domain-specific language (Julia), and lacks comprehensive benchmarks across mainstream tools or large-scale problems.  This omission of demonstration against large scale problems reduces confidence in the direct and broad use and benefit of this approach.  \nIn this paper, all symbolic differentiation is done at compile time, using pullback rules for the B and C combinators. This means that the differentiation process operates on program structure, not runtime values and relies on statically typed, dimensionally fixed programs, where all tensor shapes and types are known ahead of time.  This can be a limitation when dealing with NN and physical simulations.  The authors acknowledge this limitation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Paper is well motivated and the developments are sound and well structured.  \nThe paper presents a  conceptual advancement completing the chain rule with a second rule to avoid partial evaluation; symbolic-first gradients.\nThe benefits of this approach include Interpretability & symmetry.  Gradients remain symbolic, enabling physics-aware simplification.\nIllustrative examples across multiple domains such as NN, quantum chemistry tensors, and numerical linear algebra provide confidence.\nProofs provided in the appendix are adequate."}, "weaknesses": {"value": "No performance evaluation: no runtime/memory or large-scale benchmarks vs. JAX/Enzyme/SymPy; toolchain impact remains unquantified (acknowledged by authors)\nPractical limits: needs stable dimensions, lacks mutation support, and fixed-point/sequential iteration support is only sketched (appendix); these are crucial for many scientific codes.\nIntegration hurdles: static typing and compile-time differentiation requirement may clash with dynamic Python/Julia ecosystems; engineering path is non-trivial\nAccessibility: combinatory-logic framing may be unusual for many ML/AD practitioners, increasing the learning curve.\nThe limitations related to 1. Dimensional Stability, 2. Fixed point iteration gaps, 3, lack of mutation support appear to be  a direct consequence of the framework that relies on pure symbolic diff (no tracing or evaluation) , while this provides certain benefits, in my opinion this also imposes restriction to purely functional, statically shaped, non-iterative programs.  In that I agree with  the authors that this work is paper is more a proof of concept of a symbolic calculus and not a production-ready AD method."}, "questions": {"value": "Claim: “complete the chain rule” with a second rule based on B and C combinators.\n1. Is this new rule provably complete for all differentiable compositions expressible in combinatory logic?\n2. Can every standard AD operation be represented equivalently under your B/C pullback formulation?\n\nHow does the symbolic calculus relate formally to reverse-mode AD or the dual number formulation?\nCan the framework be seen as a category-theoretic dual of reverse-mode AD (e.g., functorial composition)?\n\n\nGiven differentiation without partial evaluation, how does one ensure equivalence to the derivative of the evaluated program rather than the unevaluated syntax tree?\n\nA formal criterion for when symbolic contraction (via delta identities) terminates in closed form would be useful.\n\nWould integrating this calculus into a JIT or AOT compiler break referential transparency, and if so, how could this be mitigated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no ethical concerns"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jMulMiHNyJ", "forum": "RsUtS1YOnK", "replyto": "RsUtS1YOnK", "signatures": ["ICLR.cc/2026/Conference/Submission19551/Reviewer_qSzs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19551/Reviewer_qSzs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775827031, "cdate": 1761775827031, "tmdate": 1762931434213, "mdate": 1762931434213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a framework for programmatic transformation of mathematical codes to their derivatives. The main difference from other AD systems is that it adopts the tacit programming style (also referred to as point-free style), where the evaluation of variables (which can represent functions) is deferred to the very end. In contrast, in most AD, one must substitute the variables/symbols representing functions with concrete instances before tracing. This is possible as the proposed framework represents programs in a DSL based on combinatory logic. In combinatory logic, there are only combinatory terms (function primitives) and combinators (higher-order functions), and it is proven to be Turing-complete. Thus, a program can be written as a transformation of primitive functions under the combinators. Usual AD only defines the pullback on function primitives, but in this framework, the pullback for the combinators, which enable one to programtically differentiate combinatory logic. It is well known that the B and C combinator forms a complete basis, so one only needs to define the pullback (VJP) for B and C, which is provided in the paper. The paper also illustrates how a tensor program can be represented using combinatory logic. Finally, the paper demonstrates how the proposed system can be used to: (a) compute the gradient of MLP with respect to network parameters; (b) compute the HF gradient with symmetry; (c) derive conjugate gradient using the symbolic gradient output from the proposed system."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides an interesting alternative to the common AD system, where one can add in symbolic simplification rule easily (which is demonstrated in the HF gradient example).\nThe paper is well written and easy to follow, and is relatively self-contained."}, "weaknesses": {"value": "My main concern is that the author failed to convey how the proposed B/C‑based differentiation differs, in capability and guarantees, from prior functional/program‑transform approaches to AD. All 3 examples demonstrated in the paper can be done with existing frameworks:\n- MLP gradient: this is quite standard and can be computed in JAX/PyTorch easily\n- Symmetry in HF gradient: one should note that this is a highly specialized application. I'd argue that using something like jax.custom_vjp suffices. Even within the proposed framework, one would first need to code the symmetry rule into the framework, so I can hardly see the benefit\n- symbolic derivation of CG: this is neat, but mainstream CAS like Maple, Mathematica, Sympy, etc can also do this easily. \n\nEvidence on a curated suite of \"hard\" patterns that cannot be done within the existing AD/CAS framework would help.\n\nThe paper claims that it avoids expression swell, which symbolic AD is susceptible to. But the paper does not provide any concrete evidence against an existing symbolic AD system, nor does it provide a theoretical justification for the claim.\n\nAlso, the paper specifically states in the introduction section that the paper only makes qualitative claims. This actually weakens the potential impact of the paper, since without numerical evidence, the benefits of the proposed framework are at best speculative. There are no quantitative evaluations: no expression‑size statistics, compile times, runtime/peak‑memory comparisons vs. tracing AD (PyTorch/JAX), source transformation (Enzyme), or symbolic systems (SymPy/Mathematica). Even small benchmarks (MLP with/without accumulation; HF with symmetry folding; end‑to‑end cost of simplification + engine execution) would significantly strengthen the claims.\n\nFinally, I would like to point out that the idea is not entirely new. The core contribution of the paper appears to be a point-free style symbolic differentiation technique that uses the pullback rule of the B and C combinator. I would like to point out the following prior works that partially encompass the main idea of the paper:\n- point-free / higher-order AD: there are many such systems in the functional programming community. For example, Haskell's AD, and [The Differentiable Curry: https://openreview.net/pdf?id=ryxuz9SzDB].\n- Using combinatory logics to formulate AD: see [Elsman et al. (2022), Combinatory Adjoints and Differentiation]\n- B-rule: [Ehrhard & Regnier (2003), differential λ-calculus].\n- C combinator swaps the order of currying, which is also known as the \"flip\" combinator. The C-rule in this paper is a direct consequence of the fact that the C combinator commutes with derivative operators, which is established much more rigorously in prior works, e.g., [Blute, Cockett & Seely, Cartesian Differential Categories] and this PhD thesis [https://cspages.ucalgary.ca/~robin/Theses/GallagherPhD.pdf].\n- representing tensor as a function from index to value: this is a common practice, which is already adopted in TVM/TACO/Mathematica, etc."}, "questions": {"value": "- It seems that it is also possible to define the pushforward for the B and C combinator. Have you thought about it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7yDQiHfyn7", "forum": "RsUtS1YOnK", "replyto": "RsUtS1YOnK", "signatures": ["ICLR.cc/2026/Conference/Submission19551/Reviewer_zHBj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19551/Reviewer_zHBj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973151170, "cdate": 1761973151170, "tmdate": 1762931433557, "mdate": 1762931433557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comment on evaluation and toolchain"}, "comment": {"value": "We thank all reviewers for their thoughtful feedback. We will address a few common concerns here as general comment.\n\n# Evaluation\n\nWe would like to emphasize that an important aspect of AD system is not their\nabsolute performance, but how it grows with human effort. Without controlling\nfor human effort, any performance advantage of an AD system can be trivially\nobfuscated by adding custom primitives and vjp rules to other systems, as\nsuggested by two reviews for the HF example. Quantifying the human effort to\nidentify such custom rules is not particularly feasible, so performance\nevaluations are potentially misleading. We believe it is better to evaluate\npotential advantage through qualitative arguments and representative cases,\nwhich we provided and will further explain.\n\nIn the high effort limit, AD systems have to compete with hand-optimized\ngradients, and the question is whether using AD can provide a faster path (than\nimplementation by hand) to near-optimal performance. The answer  based on \n[case studies](https://link.springer.com/book/10.1007/978-1-4613-0075-5) appears\ninconclusive. \nOur work tries to align AD with hand-optimized gradient by providing an \nsymbolic intermediate expression, so\nthat doing AD becomes a first step on the path to hand-optimized gradient.\n\nThe low effort regime is what motivates the functional DSL, which is designed\nto compile symbolic tensor mathematics to numerical primitives and hopefully\nsave effort for domain scientists. To deliver performance across different\nproblems and hardware, the compiler needs to optimize over a growing set of\nfast primitives described by the same abstract mathematics. What make the DSL\nimpractical is having to derive, implement, and maintain vjp rules for all\nprimitives. The B and C rules lift the problem by delaying compilation until\nafter differentiation, and gradient expressions can be mapped onto primitives\nthe same way that the model is.\n\nIn many cases, the symbolic gradient is desirable in itself for further\nsymbolic manipulation such as formulating a KTT condition or deriving a\ntangent plane projections. It can also be a physical observable such as force $F = \\nabla\\_r V(r)$,\nwhich reveals how different physical quantities are related.\n\n# Toolchain\n\nThe functional DSL is envisioned as a formalization of tensor operations with\nprogramming language concepts for the purpose of leveraging compiler\noptimization. The performance is not to be understood as executing a functional\nprogramming language because expressions should be compiled to optimized\nlibrary primitives if possible and `for`loops otherwise. For example, `(i, j)\n-> sum(k -> A(k, i) * K(k, j))` should compile to a `gemm` call. If BLAS is not\ninstalled for arguments sake, it resorts to a slow triple loop.\n\n```\nlet result = zeros(size(A, 2), size(K, 2))\n    for i in 1:size(A, 2)\n        for j in 1:size(K, 2)\n            for k in 1:size(A, 1)\n                result[i, j] = A[k, i] * K[k, j]\n            end\n        end\n    end\n    result \nend\n```\n\nMutations can be supported in principle, but it is not productive in our\ncontext. For example, `a[i] += 1` is confluent with `a = (j) -> delta(i, j, 1) + a(j)`, which is pure, so we can differentiate it and optimize the vjp back to a mutation. The problem is that mutations are not abstract\nmathematics, which our DSL is designed to describe. Instead, we let the\ncompiler generate mutating code if no better primitives are available. The main\nconcern is that the memory IO of generated code is unoptimized, but\ndifferentiating mutation also disrupts the IO pattern, so we choose to avoid\nlow-level mutations.\n\n# Claim on Partial Evaluation\n\nThe authors did not make the claim that all differentiation systems rely on\nnumerical partial evaluation, and we will rephrase that point. The claim was\nempathetically predicated on a specific subtype of problems $\\mathcal{P}(w \\mapsto f(w)(g))$, which not all\ndifferentiation systems address. The claim was further restricted to standard\ntechniques and the meaning of partial evaluation was broadened to inlining,\nwhich is one approach to the issue in source transformation systems, as\npointed out by a reference suggested by reviewer zHBj, which we will include."}}, "id": "HmNhYwcZYW", "forum": "RsUtS1YOnK", "replyto": "RsUtS1YOnK", "signatures": ["ICLR.cc/2026/Conference/Submission19551/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19551/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19551/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763702777707, "cdate": 1763702777707, "tmdate": 1763702777707, "mdate": 1763702777707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the present paper, the authors introduce an additional differentiation rule to complement the chain-rule used in automatic differentiation systems. Theoretically deriving its results, the application of the rule to skip the partial evaluation is demonstrated on a set of code examples."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The strength of this paper lies in its writeup, and the intellectual clarity with which its ideas are presented. The derivations are done with great care, and its chosen code examples help to present its ideas and aid in understanding the application of the differentiation rule."}, "weaknesses": {"value": "The weaknesses of the paper are plentiful, distilling into high-level topics before diving into them individually:\n- Misguided, or ill-informed key assumptions about automatic differentiation systems to motivate the current work\n- The _complete_ lack of evaluation of the current work\n\n#### Misguided Key Assumptions\n- line 16-17 \"gradients irreversibly become blackboxes\", there exist a wide range of automatic differentiation systems in practice. The key difference being abstraction level chosen by the automatic differentiation system. As such this core claim of the motivation does not stand up to scrutiny. Especially for source transformation AD systems such as Tapenade [1], the gradients are emitted in the source language for the user and can be inspected. But even the big modern AD systems JAX, and PyTorch permit the emission of the produced gradients [2], which can then be inspected and are hence neither blackboxes, nor riddled with \"obscure performance ceilings\".\n- The leveraging of symmetries, as envisioned by the authors, is something that is not impossible for modern automatic differentiation systems to perform. It is called custom rules. The mentioned Enzyme provides ample infrastructure for that, which can then leverage physical symmetries [3].\n- Line 48, the PyTorch citation is pointed at Baydin et al. This is wrong. The citation should read Paszke et al [4].\n\n#### Lack of Evaluation\n- The presented produced? code is never executed and as such it is not possible to quantify the performance benefits the additional differentiation rule could provide. Especially the Hartree Fork system would naturally lend itself to such evaluation where one could for example evaluate\n    - Existing AD systems (Enzyme, PyTorch/JAX, other Julia AD systems such as Zygote for example)\n    - The prototypical implementation of yours\n    - Prototype of yours leveraging the available symmetries\n- The presented code examples are not in Julia, the language in which the prototype implementation of this work is implemented in (line 244f), as such it is not apparent to the reviewer whether the proof-of-concept is leveraged in this work. Even if the prototype is not able to perform the differentiation of the presented illustrative examples, I would at a minimum expect an evaluation on AD micro benchmarks. See e.g. the tests of Enzyme [5] for a large number of micro-micro examples one could leverage here.\n\nReferences:\n1. Laurent Hascoet and Valérie Pascual. 2013. The Tapenade automatic differentiation tool: Principles, model, and specification. ACM Trans. Math. Softw. 39, 3, Article 20 (April 2013), 43 pages. https://doi.org/10.1145/2450153.2450158\n2. https://docs.pytorch.org/tutorials/intermediate/inductor_debug_cpu.html\n3. https://enzyme.mit.edu/julia/stable/#Defining-rules\n4. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. ArXiv, abs/1912.01703.\n5. https://github.com/EnzymeAD/Enzyme/tree/main/enzyme/test/Enzyme/ReverseMode"}, "questions": {"value": "- I am a little confused by one of the key claims made by the abstract with relation to interpretable surrogates (line 12-14). While the ability to inspect the gradient flow is highly conducive to interpretable surrogates it is unclear to the reviewer how the present system provides any advantage here. As mentioned previously, Tapenade is able to emit the source code of the unevaluated gradient expression for inspection, and the reviewer would contend that the need for performance of the gradient expression supersedes the need for unevaluated gradient expressions (which can yet be inspected in all of the mentioned AD systems).\n- It is entirely unclear to the reviewer whether the prototype implementation of the work is ever used throughout the paper? Would the authors be able to provide evidence that their prototype is actually functional?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "/"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CeMDvC7zpK", "forum": "RsUtS1YOnK", "replyto": "RsUtS1YOnK", "signatures": ["ICLR.cc/2026/Conference/Submission19551/Reviewer_AXQM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19551/Reviewer_AXQM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762264690206, "cdate": 1762264690206, "tmdate": 1762931432957, "mdate": 1762931432957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}