{"id": "KTlV64bQBm", "number": 22604, "cdate": 1758333407772, "mdate": 1759896857245, "content": {"title": "Flexible Participation for Differentially Private Synthetic Text Generation in Cross-Silo Federated Learning", "abstract": "In cross-silo federated learning (FL), sensitive text datasets remain confined to local organizations due to privacy regulations, making repeated training for each downstream task both communication-intensive and privacy-demanding. A promising alternative is to generate differentially private (DP) synthetic datasets that approximate the global distribution and can be reused across tasks. However, pretrained large language models (LLMs) often fail under domain shift, and federated finetuning is hindered by computational heterogeneity: only resource-rich clients can update the model, while weaker clients are excluded, amplifying data skew and the adverse effects of DP noise. We propose a flexible participation framework that adapts to client capacities. Strong clients perform DP federated finetuning, while weak clients contribute through a lightweight DP voting mechanism that refines synthetic text. To ensure the synthetic data mirrors the global dataset, we apply control codes (e.g., labels, topics, metadata) that represent each client’s data proportions and constrain voting to semantically coherent subsets. This two-phase approach requires only a single round of communication for weak clients and integrates contributions from all participants. Experiments show that our framework improves distribution alignment and downstream robustness under DP and heterogeneity.", "tldr": "", "keywords": ["Federated learning; language models; synthetic text generation; differential privacy"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3662f122f34a9565117f12c26648909625aea733.pdf", "supplementary_material": "/attachment/aee56baf30556f4e4c8927f4d920b58585fce829.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses generation of differentially private (DP) synthetic text in a cross-silo federated learning setting where clients have heterogeneous compute and data. The proposed two-stage framework lets strong (well-resourced) clients perform DP-SGD federated finetuning of a conditional generator (using control codes), while weak clients contribute via a lightweight DP voting / profiling mechanism that refines generated candidates without backpropagation. Control codes encode semantic partitions (labels/topics) and guide both generation proportions and localized voting to ensure semantically coherent refinement. Experiments on Yelp and PubMed (IID and non-IID partitions) show that (i) partial finetuning by a small fraction of clients improves over zero-shot generation, (ii) the DP voting refinement recovers much of the utility lost to DP noise and client heterogeneity, and (iii) gains hold across several downstream tasks and MAUVE / NER metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem motivation & practical relevance. Tackles a realistic cross-silo scenario: many organizations holding sizable private text, with widely varying compute budgets — a problem of immediate practical interest. \n- Simple, interpretable two-phase design. Splitting work between DP federated finetuning and DP voting is elegant: it lets expensive updates be restricted to capable nodes while still incorporating the remaining clients’ distributions. The use of control codes to structure generation is sensible and easy to implement. \n- Concrete algorithmic description and reproducibility effort. The paper includes pseudocode (Algorithm 1, A.2, A.3) and details on datasets, models, and hyperparameters (GPT-2, GPT-2-large, embedding model, MAUVE / downstream classifiers). Authors claim anonymized code in the supplement. These details aid reproducibility.\n- Empirical evidence across IID and non-IID settings. The experiments test several participation rates (1–40% Cs), different privacy settings (ε=∞ vs ε=8), and show consistent improvements from the refinement stage across tasks and metrics (classification, MAUVE, NER F1). Tables and plots are informative."}, "weaknesses": {"value": "- Baselines and ablations are limited. The baselines are (i) zero-shot pretrained generation and (ii) non-DP finetuning. Missing but important comparisons include: (a) PEFT / LoRA / adapter-style federated finetuning (compute-efficient finetuning that still requires backprop but is deployable), (b) other synthetic data generation / refinement approaches (e.g., preference-optimized or prompt-based DP synthesis), and (c) stronger ablations: effect of K (votes per example), sampling rate r in resampling, the sentence embedder choice, and synthetic dataset size s. These would better isolate where gains come from.\n- Robustness & adversarial behavior not studied. The voting stage aggregates noisy votes from Cr clients. How robust is the procedure to malicious or biased voting (e.g., a client submitting anomalous profiles/votes)? Is there an attack model (and mitigation) — e.g., outlier detection, clipping of votes, or robust aggregation? Without such analysis, a deployment risk remains.\n- Reliance on control codes and their privacy assumption. The method assumes control codes are public / non-private and that partitioning by code yields semantically coherent subsets. In practice, choosing/defining control codes can be nontrivial and may leak information if control codes correspond to sensitive labels. Please discuss sensitivity to mis-specified codes and privacy implications of broadcasting DP profiles over codes."}, "questions": {"value": "- Baselines — why not PEFT/LoRA or prompt-based refinement baselines? Can you add comparisons to parameter-efficient federated finetuning (LoRA/adapter) or to refinement approaches that do not require voting (e.g., preference optimization, prompt-tuning with public seeds)?\n- End-to-end privacy guarantee. What is the final (ϵ,δ) for the published synthetic dataset after composing DP-SGD training, profile perturbation, and vote perturbation? Show composition math or use advanced composition / moments accountant.\n- Scaling to larger LLMs / real cross-silo deployments. Do you expect the refinement gains to persist for much larger generators (e.g., modern LLMs) or when evaluating tasks beyond classification/NER? Any deployment notes (latency, single-round comm overhead for Cr)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HF62mD7lSE", "forum": "KTlV64bQBm", "replyto": "KTlV64bQBm", "signatures": ["ICLR.cc/2026/Conference/Submission22604/Reviewer_CFrw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22604/Reviewer_CFrw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761401759603, "cdate": 1761401759603, "tmdate": 1762942299179, "mdate": 1762942299179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focus on private synthetic data in FL. This research problem is meaningful when there are several downstream tasks in a federated learning setting. Authors propose a flexible participation framework that adapts to client capacities. Strong clients perform DP federated finetuning, while weak clients contribute through a lightweight DP voting mechanism that refines synthetic text. They apply control codes (e.g., labels, topics, metadata) that represent each client’s data proportions and constrain voting to semantically coherent subsets. Experiments show that the framework improves distribution alignment and downstream robustness under DP and heterogeneity.\nI like the motivation of this paper, and I think this technology can be used more widely. It's just that the current experimental results and adaptation framework (synthetic data and federated learning) make me feel that they could be better."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes a flexible participation framework for differentially private (DP) synthetic text generation in cross-silo federated learning (FL), addressing a key limitation of prior work — computational heterogeneity. \n\nThe method elegantly combines DP federated finetuning on strong clients with DP voting-based refinement from weak clients, ensuring that all participants can contribute without heavy computation.\n\nExperiments on Yelp and PubMed datasets under both IID and non-IID settings demonstrate strong results: the approach significantly improves synthetic data quality and downstream task performance while maintaining DP guarantees. The results are systematically presented and ablated, showing consistent gains from the refinement step.\n\nThis paper is well written."}, "weaknesses": {"value": "1.\tFor experiments, the privacy parameter epsilon is set as 8. Why choose this number, 8 is quite large in DP.\n\n2.\tThe computational cost of the voting step and privacy accounting (ε allocation across finetuning, profiling, and voting) could be more clearly analyzed.\n\n3.\tEvaluation primarily uses GPT-2 and GPT-2-large; extending to modern instruction-tuned or open-weight LLMs could strengthen claims of scalability and generality.\n\n4.\tFigure 1 is not easy to understand and needs to be explained in the legend.\n\n5.\tThe entire method relies heavily on predefined control codes (such as tags, topics, and metadata)."}, "questions": {"value": "1.\tHow sensitive is performance to the choice of DP budgets among the three components ((ε_train, ε_prof, ε_vote))?\n2.\tHow much communication cost is saved compared to full FL training when the number of weak clients is large?\n3.\tAre there plans to evaluate with larger models (e.g., Llama 3 or Gemma) to test scalability?\n4.\tIn Table 1 and 2, epsilon=8 is following by a downward arrow, and epsilon=8 with refinement is following by a upward arrow, why?\n5.\tThe ultimate goal of FL is \"data remains stationary, model moves\", to train a high-quality global model without concentrating on the original data. If the final output is synthetic data, then once these data are generated, we can directly use the synthetic data without federated learning. Then they will face the problem of re-centralization. Although this technically avoids the direct sharing of raw data, it seems to be a \"step backward\" in concept as it creates a new centralized dataset. Therefore, the application value of synthetic data in federated learning is questionable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a7iDPx7roV", "forum": "KTlV64bQBm", "replyto": "KTlV64bQBm", "signatures": ["ICLR.cc/2026/Conference/Submission22604/Reviewer_of8F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22604/Reviewer_of8F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535408189, "cdate": 1761535408189, "tmdate": 1762942298919, "mdate": 1762942298919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-phase framework for **Differentially Private (DP)** synthetic text generation in **cross-silo federated learning (FL)**, designed to handle **heterogeneous client resources**. \n\nThe key idea is to allow **flexible participation**: \n- **Strong clients** (with sufficient compute) perform **DP-SGD finetuning** of a pretrained LLM to adapt it to domain-specific data. \n- **Weak clients** (unable to train locally) contribute via a **lightweight DP voting mechanism** that refines the synthetic data distribution, ensuring representation of all clients. \n\nControl codes (labels, topics, metadata) guide both finetuning and refinement, partitioning data into coherent subsets. \n\nExperiments on **Yelp Reviews** and **PubMed abstracts** demonstrate that this approach improves **distributional alignment and downstream performance** of generated synthetic data, particularly under DP constraints and heterogeneous participation. Refinement with weak-client voting mitigates the utility loss typically induced by DP noise and biased finetuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel integration of heterogeneous participation and DP**: The flexible two-phase structure is a natural and elegant way to involve all clients under compute constraints. \n- **Sound motivation**: Addresses a practical gap between FL for large models and realistic cross-silo deployment, where client capacity varies widely. \n- **Technical clarity**: The description of Algorithm 1 and control-code-based conditioning is detailed and well-structured. \n- **Empirical evaluation**: \n  - Compares baselines with and without DP, and with/without refinement. \n  - Uses multiple datasets and metrics (classification accuracy/F1, MAUVE, NER). \n  - Results consistently show refinement improves performance under DP, especially with few strong clients."}, "weaknesses": {"value": "1. **Privacy accounting and budgets** \n   - The paper applies separate ε = 8 budgets for training and refinement, but it’s unclear whether these compose into a total DP guarantee or are treated independently. \n   - The choice ε = 8 is relatively high; discussion of lower-ε performance or practical implications would strengthen credibility. \n   - Clarify the full \\((\\varepsilon_\\text{total}, \\delta_\\text{total})\\) budget.\n\n2. **Evaluation of privacy–utility trade-off** \n   - All experiments use ε = 8; it would be valuable to show at least one lower ε (e.g., 4 or 2) to demonstrate robustness to stricter privacy. \n   - Plotting performance as a function of ε would better illustrate the trade-off curve.\n\n3. **Refinement mechanism interpretability** \n   - The “DP voting” phase uses noisy aggregated similarity scores. It would help to explain how KNN-based votes interact with control codes and whether Gaussian noise biases toward majority groups. \n   - Are weak clients’ votes weighted equally regardless of dataset size?\n\n4. **Baselines and ablations** \n   - The “voting” mechanism could be compared against a simpler aggregation (e.g., uniform resampling or non-DP voting) to isolate its effect. \n   - Clarify whether “pretrained + voting” (without any finetuning) was tested.\n\n5. **Conceptual framing** \n   - While the “control code” abstraction is central, it’s borrowed from prior controllable generation work; the novelty lies in its federated adaptation. \n   - The term *Flexible Participation* might overstate generality: the method still assumes clients can be cleanly partitioned into strong/weak and have known control-code profiles.\n\n6. **Relation to prior work** Could better contrast with recent LoRA-based federated adaptation (e.g., FLoRA 2024) and DP synthetic text methods that do global aggregation rather than federated (e.g. private evolution (voting), DP fine tuning with LoRA, PATE based methods)"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aTS8Yrl4Gf", "forum": "KTlV64bQBm", "replyto": "KTlV64bQBm", "signatures": ["ICLR.cc/2026/Conference/Submission22604/Reviewer_FP3m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22604/Reviewer_FP3m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957937697, "cdate": 1761957937697, "tmdate": 1762942298403, "mdate": 1762942298403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a flexible participation framework in the cross silo setting, where some clients may be imbalanced in terms of resources. In particular, some clients may have enough infrastructure and GPUs to run DPSGD on their own data, and some clients may not have enough compute power. So the method proposes to do local DPSGD on clients with compute budget, and use a method similar to Private Evolution to condition synthetic data generations towards the data on the clients which do not have much compute budget. In this way they are able to balance the benefits of training while also leveraging the data of clients that cannot train. They support their algorithm with a set of experimental evaluations on the GPT2 model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The algorithm does make sense. While it is not the most elegant thing, it resembles production-ready algorithms in industry which may be a combination of several different algorithms to handle different regimes (the algorithm proposed is essentially FedAvg + Private Evolution).\n- The participation model helps include a wide variety of clients, which allows synthetic data to represent a wide diversity of data\n- Experimental ablations are decent, with evaluations across different epsilon values, datasets, and strong client participation. They demonstrate that the refinement step is important in getting good performance"}, "weaknesses": {"value": "- Could be more explicit about the privacy composition, there are multiple steps and it is unclear how they are composed\n- The algorithm is mostly a simple combination of two existing ones (FedAvg, Private Evolution), there isn't a major methodological breakthrough. However this is not something I would hold against the paper.\n- The evaluation could be more comprehensive. For example, they could evaluate more models outside of GPT2. Second, they should compare against prior work better. The real baseline to compare against is not just the zero-shot pretrained model. It also includes the other methods mentioned in section 5, such as Private Evolution and its variants. The papers in the related work have their own evaluations against standard datasets, so the paper should run its method on those datasets, get the numbers, and compare against the results reported in those papers. The other baseline to compare against is pure FedAvg, which should be better than the proposed method but again this evaluation is needed for comparison.\n\nOverall I think this is a promising paper, but feels somewhat incomplete at the moment. For it to be published at a venue like ICLR, I would want to see solid performance gains vs Private Evolution and its follow ups."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "omEs6YKmfO", "forum": "KTlV64bQBm", "replyto": "KTlV64bQBm", "signatures": ["ICLR.cc/2026/Conference/Submission22604/Reviewer_JUqm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22604/Reviewer_JUqm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762035314294, "cdate": 1762035314294, "tmdate": 1762942298032, "mdate": 1762942298032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}