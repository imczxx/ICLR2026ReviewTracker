{"id": "QuFw7fKQBp", "number": 7103, "cdate": 1758007797660, "mdate": 1759897872439, "content": {"title": "SURE: Semantic Uncertainty Regularization for Test-Time Adaptation in Vision-Language Models", "abstract": "Test-time adaptation (TTA) aims to improve model robustness under distribution shift by exploiting unlabeled test data. Existing methods often rely on pseudo-labels, which are noisy and treated independently, ignoring both their temporal reliability and the semantic structure of the label space. We introduce SURE (Semantic Uncertainty REgularization), a framework that regularizes predictions through a dynamically evolving prototype-reliability graph (PRG). PRG captures semantic affinity across classes and the stability of confidence over time, enabling the selective propagation of reliable predictions while suppressing errors. This structure-driven regularization enforces semantic consistency and prevents error amplification. Across diverse domain-shift benchmarks, SURE consistently outperforms prior methods, offering a principled and generalizable approach to reliable TTA.", "tldr": "We propose a test-time adaptation method for CLIP that models per-class semantic drift and cross-class structural relations from pseudo-labeled samples, achieving lightweight, unsupervised visual-language adaptation without training-time access.", "keywords": ["Test-Time Adaptation; Vision-Language Models; Semantic Regularization; Distribution Shift"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ae39101c02af6dbb9e1826601daea9fbb646c935.pdf", "supplementary_material": "/attachment/317e5c2714fbfa517fb29e97ae05f51f61dbb4bc.zip"}, "replies": [{"content": {"summary": {"value": "Existing test-time adaptation (TTA) methods for vision–language models (VLM) heavily rely on model predictions, making them vulnerable to noisy pseudo-labels under distribution shifts. To address this limitation, the authors propose SURE (Semantic Uncertainty Regularization), a framework that regularizes model predictions via a dynamically evolving prototype–reliability graph (PRG). This graph enables the selective propagation of reliable predictions while suppressing erroneous ones. Extensive experiments on diverse domain-shift benchmarks demonstrate the effectiveness and robustness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental section is comprehensive — the authors evaluate their method on multiple datasets and under different settings, achieving consistently strong results. The ablation studies further validate the effectiveness of the proposed approach.\n2. The proposed method does not require updating model parameters and therefore incurs significantly lower computational overhead compared with baseline approaches."}, "weaknesses": {"value": "1. Several existing works also incorporate graph structures with CLIP, such as GraphAdapter [1]. In addition, some test-time adaptation approaches have utilized graph-based mechanisms, for example PROGRAM [2]. The authors are encouraged to discuss these graph-related studies in the Related Work section to better position their method and highlight its unique advantages.\n2. The design of the PRG appears heuristic. While it is empirically stable, the paper lacks a theoretical analysis or formal guarantee of its stability over time.\n3. It would be helpful if the authors could provide a visualization of the proposed PRG — for instance, showing which classes are more closely connected and how the stability evolves over time. Such visualization would make the workflow of the proposed algorithm more intuitive and accessible to readers.\n\n[1] GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph. NeurIPS'23\n\n[2] PROGRAM: PROtotype GRAph Model based Pseudo-Label Learning for Test-Time Adaptation. ICLR'24"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Fz3FiiJDze", "forum": "QuFw7fKQBp", "replyto": "QuFw7fKQBp", "signatures": ["ICLR.cc/2026/Conference/Submission7103/Reviewer_cYhQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7103/Reviewer_cYhQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761382083955, "cdate": 1761382083955, "tmdate": 1762919278238, "mdate": 1762919278238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SURE (Semantic Uncertainty REgularization), a test-time adaptation (TTA) framework for vision-language models that addresses distribution shift without labeled data. The core contribution is a dynamically evolving Prototype-Reliability Graph (PRG) that combines semantic affinity between class prototypes with class-wise reliability scores based on temporal confidence stability. The method performs three key operations: (1) constructing a sparse graph where edges encode both semantic similarity and prediction reliability, (2) propagating logits through this graph to regularize predictions, and (3) updating prototypes and reliability estimates based on high-confidence pseudo-labels. Evaluated on ImageNet variants and 10 cross-dataset benchmarks using ResNet-50 and ViT-B/16 backbones, SURE achieves marginal but consistent improvements over baselines like DPE and BCA with competitive inference speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### Originality\n\nThe paper presents a reasonable integration of graph-based reasoning with test-time adaptation for VLMs. The concept of coupling semantic similarity with temporal reliability through a multiplicative joint reliability matrix (Eq. 5) is a sensible design choice. The use of sliding-window averaging for adjacency matrices (Eq. 8) to stabilize graph structure is practical.\n\n### Quality\n\nThe experimental evaluation is comprehensive, covering 15 datasets across natural distribution shifts and cross-domain generalization. The ablation study (Table 4) systematically dissects component contributions, showing +1.05% gain from logit propagation and +1.24% from reliability modeling on OOD average. The stability analysis (Tables 7-8) demonstrates low variance across random seeds (<0.3% standard deviation). The efficiency analysis (Table 3) shows reasonable computational cost at 0.067s per sample.\n\n### Clarity\n\nThe paper is generally well-structured with clear motivation in Section 1 and detailed methodology in Section 4. Figure 1 provides effective conceptual visualization of the framework. Algorithm 1 presents a clear procedural overview. The notation is mostly consistent, though the reliability score formulation (Eq. 4) could be better justified theoretically.\n\n### Significance\n\nThe work addresses a practical problem of test-time adaptation under distribution shift without source data or labels. The framework achieves state-of-the-art results on multiple benchmarks, though improvements are often marginal. The method's applicability across different backbones (ResNet-50, ViT-B/16) and prompt configurations (handcrafted, ensemble, CoOp) demonstrates some generality."}, "weaknesses": {"value": "### 1. Limited Novelty and Incremental Gains\n\nThe core mechanisms are not particularly novel: prototype adaptation follows Zhou et al. (2025) (Eq. 2), cosine similarity graphs are standard in graph-based learning, and confidence thresholding for pseudo-label filtering is widely used. The main contribution is combining these with a reliability weighting scheme, but the conceptual leap is limited. More critically, **empirical gains are marginal**: on ImageNet variants with ViT-B, SURE achieves 66.23% vs. DPE's 65.93% (+0.30%) and BCA's 65.37% (+0.86%). The authors acknowledge \"the numerical margin over DPE appears modest\" and attribute significance to \"consistency across seeds,\" but this doesn't adequately address the limited practical impact.\n\n### 2. Weak Theoretical Justification for Reliability Metric\n\nThe reliability score $R_j = \\mu_j \\cdot (1 - \\frac{\\sigma_j}{\\sigma_{max}})$ (Eq. 4) is presented as an \"information-theoretic intuition\" for \"inverse uncertainty,\" yet no formal connection to entropy or information theory is established. Why should the product of mean confidence and normalized inverse standard deviation be the optimal reliability measure? The paper would benefit from: (a) theoretical analysis showing this formulation minimizes adaptation error, (b) comparison with alternative reliability metrics (e.g., coefficient of variation, entropy-based measures, Bayesian credible intervals), or (c) ablation showing sensitivity to different formulations. The choice of $\\sigma_{max} = 0.5$ appears arbitrary without justification.\n\n### 3. Insufficient Analysis of Failure Cases and Limitations\n\nThe paper lacks a critical discussion of when and why SURE fails. For instance:\n\n- **ImageNet-R saturation:** The authors note \"performance tends to saturate\" on ImageNet-R because \"low-level style cues...are less influenced by semantic drift\", but don't investigate whether this indicates fundamental limitations of semantic graph-based methods.\n- **Modest gains on stable domains:** Improvements on Pets and Cars are described as \"modest\" because \"prototypes...are already compact\", but this raises the question: when should practitioners use SURE vs. simpler baselines?\n- **Graph construction sensitivity:** What happens when semantic similarity is misleading (e.g., \"hot dog\" vs. \"dog\")? How does the method perform with class imbalance or rarely seen classes during the test stream?\n- **Negative results:** The \"+Graph w/o Rel\" variant shows -0.24% on ImageNet-A (Table 4), suggesting graph smoothing can hurt without reliability gating. This deserves deeper analysis.\n\n### 4. Hyperparameter Sensitivity and Tuning Protocol\n\nWhile Figure 3 shows robustness across hyperparameters, several concerns remain:\n\n- **Validation set usage:** The authors state \"all hyperparameters are selected based on performance on the ImageNet validation set\" (A.4). This is problematic for test-time adaptation, where access to validation labels violates the unsupervised assumption. How would practitioners tune $\\theta$, k, and L in truly label-free scenarios?\n- **Neighbor size scaling:** Setting $k = 3 log C$ is presented without justification. Why logarithmic scaling? How sensitive is performance to this choice across datasets with different C?\n- **Initialization dependence:** The method initializes $N_i^{proto} = 30000$ confident samples following Zhou et al. (2025), but doesn't analyze sensitivity to this large prior. What if only 1000 or 100 samples are available?\n\n### 5. Limited Baseline Comparisons and Missing Ablations\n\n- **No comparison with uncertainty quantification methods:** The paper claims to address \"semantic uncertainty\" but doesn't compare with established uncertainty estimation techniques (e.g., temperature scaling, ensemble methods beyond ZERO, Monte Carlo dropout, evidential deep learning).\n- **No analysis of graph structure alternatives:** Why top-k sparsification vs. threshold-based or learnable adjacency? How does performance compare to Graph Neural Network variants (e.g., GAT, GraphSAGE) or no sparsification?\n- **Missing ablation on sliding window size L:** While Figure 3 shows performance stabilizes at $L \\geq 3$, there's no analysis of computational vs. accuracy trade-offs or sensitivity to stream non-stationarity.\n\n### 6. Calibration Analysis is Superficial\n\nTable 10 shows SURE achieves 7.48 ECE on ImageNet-OOD vs. CLIP's 6.29, meaning SURE is **less calibrated** than the base model despite claims of preserving \"trustworthy confidence estimation\". The authors dismiss this by comparing only to adapted baselines (TPT, C-TPT), not addressing whether the graph regularization inherently degrades calibration. The calibration-accuracy trade-off deserves principled analysis, potentially through post-hoc calibration methods or uncertainty-aware loss terms.\n\n### 7. Writing Quality Issues\n\n- **Vague claims:** \"Unlike these efforts, our approach dynamically constructs a class-level graph\" (Section 2). How is this fundamentally different from other adaptive graph methods?\n- **Over-claiming:** \"SURE consistently outperforms prior methods\" (Abstract) is misleading given marginal gains and mixed results (e.g., ImageNet-R).\n\n### 8. Reproducibility Concerns\n\nWhile the appendix provides implementation details, key aspects remain unclear:\n\n- How are ties handled in top-k selection for graph construction?\n- How does batch size affect the sliding window buffer updates in online settings?"}, "questions": {"value": "**Q1: Theoretical Justification**\n\nCan you provide formal analysis showing $R_j = \\mu_j \\cdot (1 - \\sigma_j/\\sigma_{\\max})$ minimizes expected adaptation error or connects to information-theoretic bounds? What about alternative reliability metrics (e.g., $R_j = \\mu_j^2 / (\\mu_j^2 + \\sigma_j^2)$, inverse coefficient of variation)?\n\n**Q2: Failure Mode Analysis**\n\nUnder what specific conditions does SURE underperform simpler baselines (e.g., ProtoOnly or BCA)? Can you characterize dataset properties (class count, domain gap, label granularity) where gains are minimal vs. substantial?\n\n**Q3: Hyperparameter Tuning**\n\nHow should practitioners select $\\theta$, $k$, and $L$ without validation labels? Can you propose unsupervised selection criteria (e.g., prediction consistency, entropy-based heuristics)?\n\n**Q4: Graph Structure Alternatives**\n\nWhy is top-k sparsification optimal? Have you compared against threshold-based adjacency ($A_{jk} = W_{jk} \\cdot \\mathbb{1}(W_{jk} > \\tau)$), fully connected graphs with learned attention (GAT-style), or no graph (direct prototype update)?\n\n**Q5: Calibration Trade-off**\n\nTable 10 shows SURE has higher ECE (7.48) than CLIP (6.29) on ImageNet-OOD. Can you incorporate calibration-aware objectives (e.g., temperature scaling, Dirichlet-based losses) or post-hoc calibration to improve trustworthiness without sacrificing accuracy?\n\n**Q6: Class Imbalance and Rare Classes**\n\nHow does SURE perform when certain classes are rare or absent in the test stream? Does the initialization $\\mu_j = 1.0, \\sigma_j = 0.0$ cause over-reliance on initial prototypes for unseen classes?\n\n**Q7: Computational Bottlenecks**\n\nFor datasets with large $C$ (e.g., ImageNet's 1000 classes), does the $O(C^2)$ similarity matrix computation become prohibitive? Can you analyze scaling to $C = 10{,}000$ or $C = 100{,}000$?\n\n**Q8: Comparison with Uncertainty Quantification**\n\nHow does SURE compare to evidential deep learning (e.g., Dirichlet-based uncertainty), temperature scaling, or ensemble-based uncertainty estimation for identifying reliable predictions?\n\n**Q9: Streaming vs. Batch Settings**\n\nAlgorithm 1 processes samples sequentially. How does performance change in batch settings (e.g., mini-batches of 32 or 64 samples)? Does batching improve stability or efficiency?\n\n**Q10: Prompt Engineering Impact**\n\nTable 9 shows CoOp prompts achieve 67.88% vs. Ensemble's 66.23%. Does SURE's reliability mechanism interact differently with learned vs. handcrafted prompts? Should $k$ or $\\theta$ be adjusted based on prompt type?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nCWXAnzgUr", "forum": "QuFw7fKQBp", "replyto": "QuFw7fKQBp", "signatures": ["ICLR.cc/2026/Conference/Submission7103/Reviewer_Yk6L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7103/Reviewer_Yk6L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878859066, "cdate": 1761878859066, "tmdate": 1762919277775, "mdate": 1762919277775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SURE introduces a graph-based test-time adaptation framework for vision-language models (VLMs) under distribution shift. It constructs a dynamic Prototype-Reliability Graph (PRG) that integrates semantic similarity (from text embeddings) and temporal confidence stability of class prototypes. Predictions are refined via iterative logit propagation on PRG, prototype updates, and reliability tracking. This closed-loop mechanism suppresses error propagation and enforces semantic consistency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Principled structured regularization: First to jointly model semantic affinity and class-wise reliability evolution in TTA, moving beyond instance-level confidence.\n2. Closed-loop co-evolution of predictions, prototypes, and graph structure enables stable, error-resistant adaptation.\n3. Strong empirical performance: Outperforms entropy minimization (TENT, SAR), prototype-based (Zanella & Ben Ayed, 2024), and recent SOTA across diverse shifts and backbones."}, "weaknesses": {"value": "1. Figure 1 can be optimized: Text annotations overlap with black boxes, reducing clarity.\n2. Eq(9) propagation process is only performed once — why not iterate to convergence? Justification for single-step sufficiency is missing.\n3. Algorithm section shows no model parameter updates — does this mean adaptation is entirely prototype-driven? If so, clarify whether backbone features remain frozen and how this impacts representation drift.\n4. Eq(12) uses f to update t — unclear why the current prediction f is used to update reliability τ; risks reinforcing early noise."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "q1mBstVqiG", "forum": "QuFw7fKQBp", "replyto": "QuFw7fKQBp", "signatures": ["ICLR.cc/2026/Conference/Submission7103/Reviewer_tWAj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7103/Reviewer_tWAj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995859387, "cdate": 1761995859387, "tmdate": 1762919277158, "mdate": 1762919277158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SURE (Semantic Uncertainty Regularization), a novel approach for enhancing the stability of predictions in test-time adaptation (TTA) tasks. The core idea involves constructing and iteratively updating a Prototype-Reliability Graph (PRG), which captures inter-class semantic relationships derived from text embeddings. The PRG is dynamically refined based on the reliability of class-wise predictions during test time. Final predictions are obtained by combining the original model outputs with smoothed predictions informed by the PRG structure. The authors validate their method through comprehensive experiments under both natural distribution shifts and cross-dataset generalization scenarios, utilizing CLIP models with ResNet-50 and ViT-Base backbones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- While TTA is known to be susceptible to noisy predictions, the authors propose a principled approach to mitigate this issue by leveraging statistical measures. Specifically, they downweight classes with high standard deviations (as shown in Equation 4), thereby enhancing the reliability of the constructed PRG.\n- The experimental evaluation is comprehensive, considering both the diversity of test datasets and the inclusion of multiple backbone architectures, ResNet-50 and ViT-Base.\n- The paper provides in-depth analyses through ablation studies, test-time inference behavior, and hyperparameter sensitivity, which collectively strengthen the empirical validity of the proposed method."}, "weaknesses": {"value": "- The use of the term graph to describe inter-class relationships may be potentially misleading. In machine learning, graph typically refers to structures processed by specialized architectures such as GNNs. While the proposed representation can be interpreted as nodes and edges, the terminology might cause confusion for readers expecting conventional graph-based methods.\n- The reliability estimation in Equation 4 could be further refined. Using the maximum standard deviation as a denominator may accompany instability, as this value itself can be noisy. Alternative formulations such as leveraging the cumulative distribution function (CDF) of each class could offer more robust normalization.\n- Several baseline methods demonstrate performance comparable to SURE on specific test sets. For instance, DPE performs similarly in Table 1 (CLIP-RN50), and ZERO shows comparable results in Table 1 (CLIP-ViT-B). However, ZERO is omitted from the test-time inference comparison in Table 3, despite its relevance.\n- In Table 4, the most substantial performance gain is attributed to the ProtoOnly variant, which is not the core contribution of the paper. Although the full PRG with regularization yields additional improvements, the gains appear modest compared to the use of prototype vectors alone in some test sets (e.g., on ImageNet-A).\n- The final prediction is computed as a simple sum of the original model output and the PRG-based prediction. It remains unclear whether this combination is optimal. Exploring weighted combinations (e.g., p(y∣x)+α⋅p_graph(y∣x)) could potentially yield better results.\n- Despite the concerns raised such as the use of terminology and certain methodological choices, I am open to further discussion with the authors. I am willing to increase my score if the authors provide clear and convincing responses during the rebuttal phase."}, "questions": {"value": "Please refer to my weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gfradgoqFN", "forum": "QuFw7fKQBp", "replyto": "QuFw7fKQBp", "signatures": ["ICLR.cc/2026/Conference/Submission7103/Reviewer_yV6M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7103/Reviewer_yV6M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762043148009, "cdate": 1762043148009, "tmdate": 1762919276703, "mdate": 1762919276703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Test-time adaptation optimizes a source-trained model at inference to handle unseen distribution shifts. It typically minimizes prediction entropy directly or uses pseudo-labels, an implicit form of entropy minimization. The paper argues that these approaches overlook temporal reliability and the semantic structure of the label space, and proposes SURE, which regularizes predictions via a Prototype Reliability Graph (PRG). The PRG captures semantic affinity among classes and stabilizes confidence over time to improve reliability. Across benchmarks, the framework reports consistent gains over prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The claim that entropy minimization is not always a reliable signal for adaptation is reasonable; however, the argument that prototypes propagate noise and destabilize adaptation requires stronger evidence.\n\nThe integration of model predictions, prototypes, and a graph structure augmented with language-based semantics to correct outputs under distribution shift in VLMs is compelling and should be emphasized more clearly in the abstract and introduction."}, "weaknesses": {"value": "The performance improvement is not significant or even as expected, given the proposal of the paper. This is further evident in the ablation study as well. \nThere are several aspects of the paper that I was not able to follow, and these have been detailed in my questions below. I would also like to know why one of the baselines [1] was not included as a baseline."}, "questions": {"value": "The method’s improvements over MAP-adjusted and pseudo-label baselines are modest on a per-dataset basis, which contradicts expectations for a framework that combines multiple techniques under SURE.\n\nThe introduction lacks citations for confidence thresholding, making it difficult to evaluate known limitations and design choices.\nLines 045–047 in the introduction are hard to parse, and “class-level prediction” should be defined precisely.\n\nThe paper introduces numerous new terms for the proposal, creating inconsistency and confusion across sections; keep the name and core terminology consistent across the abstract, introduction, related work, and methodology.\n\nThe motivation for using pseudo-label confidence at L082 is unclear, given the earlier critique that pseudo-labels can be inconsistent and that high-confidence misclassifications occur; reconcile this tension explicitly.\n\nAdd a citation at L90 to support the claim that the formulation reflects information-theoretic intuition.\n\nIn Section 4.2, specify precise graph notation, including symbols for nodes, edges, messages, and update rules.\n\nFrom L066 onward, explain how pseudo labels are made reliable and how confidence values are computed and validated.\nDespite leveraging graph structure, Table 4 shows limited gains on some datasets, with the prototypes-only variant being the strongest in several cases. An analysis is needed to determine when and why PRG helps or hurts.\n\nSpecify which datasets are included in Table 3 for test-time compute and mean accuracy, and provide a detailed table covering all datasets from Tables 1 and 2 to verify whether the reported gain percentage is consistent.\n\nThe consistently lower ECE scores in Table 10 are encouraging; however, recent methods from Tables 1 and 2 should also be included in this comparison.\n\nWhy was [1] not included in the comparison tables, given its relevance, non-gradient-based adaptation, and low ECE?\n\nReferences:\n[1] Niu, Shuaicheng, et al., “Test-time model adaptation with only forward passes,” ICML 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mixTxt5dnO", "forum": "QuFw7fKQBp", "replyto": "QuFw7fKQBp", "signatures": ["ICLR.cc/2026/Conference/Submission7103/Reviewer_6KsX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7103/Reviewer_6KsX"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120916528, "cdate": 1762120916528, "tmdate": 1762919276339, "mdate": 1762919276339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}