{"id": "zRKF4ln2VE", "number": 8547, "cdate": 1758090589539, "mdate": 1759897777405, "content": {"title": "AlphaSAGE: Structure-Aware Alpha Mining via GFlowNets for Robust Exploration", "abstract": "The automated mining of predictive signals, or alphas, is a central challenge in quantitative finance. While Reinforcement Learning (RL) has emerged as a promising paradigm for generating formulaic alphas, existing frameworks are fundamentally hampered by a triad of interconnected issues. First, they suffer from reward sparsity, where meaningful feedback is only available upon the completion of a full formula, leading to inefficient and unstable exploration. Second, they rely on semantically inadequate sequential representations of mathematical expressions, failing to capture the structure that determine an alpha's behavior. Third, the standard RL objective of maximizing expected returns inherently drives policies towards a single optimal mode, directly contradicting the practical need for a diverse portfolio of non-correlated alphas. To overcome these challenges, we introduce **AlphaSAGE** (**S**tructure-**A**ware Alpha Mining via **G**enerative Flow Networks for Robust **E**xploration), a novel framework is built upon three cornerstone innovations: (1) a structure-aware encoder based on Relational Graph Convolutional Network (RGCN); (2) a new framework with Generative Flow Networks (GFlowNets); and (3) a dense, multi-faceted reward structure. Empirical results demonstrate that AlphaSAGE outperforms existing baselines in mining a more diverse, novel, and highly predictive portfolio of alphas, thereby proposing a new paradigm for automated alpha mining. Our code is available at https://anonymous.4open.science/r/AlphaSAGE-3BA9.", "tldr": "Alpha mining with Gflownets", "keywords": ["Alpha Mining", "Generative Flow Networks"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee8e80ea959cae259fc156fafac18b9101d97c87.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AlphaSAGE, a structure-aware framework for automated alpha mining in quantitative finance that integrates relational graph convolutional networks (RGCN) with generative flow networks (GFlowNets) to improve exploration efficiency, structural understanding, and diversity of discovered alphas. By representing alphas as abstract syntax trees (AST) and introducing a dense multi-faceted reward, AlphaSAGE effectively addresses reward sparsity and mode collapse issues in reinforcement learning-based approaches. Experiments on both Chinese and U.S. stock markets show that AlphaSAGE consistently outperforms baselines in terms of correlation and portfolio metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured and logically coherent, with publicly available reproduction code that ensures strong reproducibility.\n2. The authors introduce Generative Flow Networks (GFlowNets) into quantitative finance, which, compared with conventional reinforcement learning methods, can generate diverse and high-quality solutions while maintaining reward sensitivity, effectively alleviating the mode collapse problem.\n3. The paper employs a Relational Graph Convolutional Network (RGCN) to model the abstract syntax tree (AST) of formulaic alphas, effectively capturing their hierarchical structure and semantic relationships among operators.\n4. The authors design a multi-level reward mechanism that integrates multiple concerns into a dense and stable optimization signal, thereby enhancing both exploration efficiency and convergence stability.\n5. The experimental section covers both Chinese and U.S. markets, and AlphaSAGE consistently outperforms recent baseline methods across various metrics, demonstrating comprehensive and up-to-date experimental comparisons."}, "weaknesses": {"value": "1. Although the paper proposes a multi-dimensional reward function and provides ablation studies on the weighting parameters, the selection of these weights relies heavily on empirical tuning. The lack of a theoretically grounded or adaptive weighting mechanism may limit the model’s stability and robustness across different market regimes.\n2. While each module in the framework is meaningful, the overall architecture is relatively complex and computationally intensive during training, which may hinder its direct application to high-frequency or real-time trading scenarios. Moreover, the paper lacks a detailed analysis or comparison of computational cost.\n3. The background section remains somewhat vague and may not be sufficiently accessible for readers who are not familiar with quantitative finance, which slightly limits the paper’s readability for a broader audience."}, "questions": {"value": "1. The paper lacks a discussion on the reproducibility of experimental results, as it does not provide information about the variance or stability of the reported outcomes. Were the results obtained from a single training run, or were they averaged across multiple runs to ensure statistical reliability?\n2. The paper does not sufficiently elaborate on how the GNN embedding specifically influences inter-alpha correlations. In a highly collinear factor space, is there any empirical or theoretical evidence demonstrating that the proposed representation effectively mitigates redundancy or correlation among alphas?\n3. The cumulative return results on CSI500 (2022–2024) and S&P500 (2018–2020) are not as pronounced, and in some cases even underperform compared to certain baselines. Could the authors provide further analysis or explanations for these weaker results, such as data regime shifts, market noise, or model sensitivity?\n4. Although the multi-faceted reward design effectively alleviates the reward sparsity problem, the paper does not clearly demonstrate that different reward components are non-conflicting. It remains unclear how the objectives are balanced during training to avoid mutual interference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tbQHnSogkY", "forum": "zRKF4ln2VE", "replyto": "zRKF4ln2VE", "signatures": ["ICLR.cc/2026/Conference/Submission8547/Reviewer_BXEc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8547/Reviewer_BXEc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576745158, "cdate": 1761576745158, "tmdate": 1762920402540, "mdate": 1762920402540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AlphaSAGE, a method for automated mining of  alphas (interpretable signals expressed as symbolic expressions) in quantitative finance. It is a direct application of GFlowNets to quantitative-finance tasks via a new encoder for AST representation and curated rewards. Experiments on CSI300/CSI500 (China) and S&P500 (US) show consistent gains over GA, RL, and GAN-style baselines, supported by ablations and sensitivity analyses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* It is new to use a Relational Graph Convolutional Network and  Abstract Syntax Trees (AST) to encode the information within alphas for GFlowNet training.\n\n* The author designs diverse rewards, $R_{IC}$, $R_{SA}$ and $R_{Nov}$ to better guided the GFlowNet sampling."}, "weaknesses": {"value": "1. The introduced components (RGCN encoder, composite reward) are primarily engineering improvements tailored to specific applications. These designs are orthogonal plug-ins to the GFlowNet pipeline and, while they improve empirical results, they do not substantively advance GFlowNet methodology or theory—particularly regarding robust exploration.\n\n2. The paper attributes improvements in exploration and reward sparsity primarily to reward design, yet these issues originate from the inherently vast trajectory and state space in GFlowNet training. While the proposed dense and multi-faceted reward function provides a smoother optimization landscape, it only mitigates rather than resolves the intrinsic exploration problem. Effective traversal of such large spaces fundamentally requires exploration mechanisms and policy optimization strategies, which are not the focus of this work.\n\n3. The reward-reshaping component is incremental: although the design here differs, the underlying idea is well established by many prior works. Contrary to the author's claim, the work does not present a new generative framework. The approach remains largely adheres to the standard GFlowNet formulation, incorporating incremental modifications for specific applications.\n\n4. As discussed in the introduction, reinforcement learning (RL) methods are key baselines for comparison. Since the proposed encoder and reward components are orthogonal to both RL and GFlowNet frameworks, it is important to examine performance differences between them both with and without these components. Such comparisons would clarify whether the reported performance gains stem from the GFlowNet framework itself or primarily from the proposed encoder and reward designs."}, "questions": {"value": "1. What is the combination model $c(\\cdot)$? Please give detail description.\n\n2.  The 'MaxLen' in eq.(3) seems to be a important hyper-parameter to avoid overly deep ASTs. Please provide ablation study on how this hyper-parameter affect the early stop mechanism and the GFlowNet performance.\n\n3. Why do all the reported results appear to be based on single trials rather than multiple independent runs?\n\n4. What's the cumulative returns in the plotted curves?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZUl2MZCPFW", "forum": "zRKF4ln2VE", "replyto": "zRKF4ln2VE", "signatures": ["ICLR.cc/2026/Conference/Submission8547/Reviewer_fD8y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8547/Reviewer_fD8y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889368351, "cdate": 1761889368351, "tmdate": 1762920402154, "mdate": 1762920402154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AlphaSAGE, a structure-aware generative framework for automated alpha mining in quantitative finance. The method replaces reinforcement learning–based sequential generation with a Generative Flow Network (GFlowNet) to alleviate reward sparsity and promote diversity. A Relational Graph Convolutional Network (RGCN) encoder operates on abstract syntax trees to capture the structural semantics of formulaic alphas, while a multi-faceted reward combines predictive performance (IC), structure–behavior alignment (RSA), and novelty (RNOV). Experiments on Chinese (CSI300/500) and U.S. (S&P 500) markets show consistent gains in ICIR, Sharpe ratio, and drawdown metrics compared to GA- and RL-based baselines such as AlphaGen, AlphaQCM, and AlphaForge. Ablations indicate that structure-aware encoding and reward densification contribute most to performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a meaningful and challenging problem that lies at the intersection of quantitative finance, graph learning, and generative modeling. The idea of using GFlowNets to sample a diverse distribution of high-reward alphas is original within this domain and fits the practical need for low-correlation factors. The RGCN-based encoder provides a principled way to respect formula structure, improving over purely sequential representations. The dense reward design is intuitive and empirically effective, addressing the sparse-feedback issue that limits existing reinforcement-learning frameworks. Experiments span multiple markets and metrics, with ablation and sensitivity analyses that help clarify component contributions. The writing is generally clear, and the motivation for diversity-seeking generation is well articulated."}, "weaknesses": {"value": "The main concern lies in theoretical and methodological depth. The link between GFlowNet flow conservation and portfolio diversity is described only intuitively; no analytical justification or formal insight is provided. The reward composition—particularly the time-dependent weights λ(T) and η(T)—appears heuristic, and the sensitivity analysis, while helpful, does not demonstrate robustness across market regimes or noisy IC estimation. The experimental setup lacks transparency: complex baselines such as AlphaForge and AlphaQCM are not described in enough detail to ensure a fair comparison, and no significance testing or robustness checks (e.g., bootstrapping, stress periods) are presented. The novelty is somewhat incremental since the components (GFlowNets, RGCNs, multi-objective reward) are all known; the contribution lies mainly in system integration rather than a conceptual advance. Finally, presentation can be improved—several mathematical sections omit key definitions, and Figure 2 is overly dense and difficult to interpret."}, "questions": {"value": "Can the authors formalize how the GFlowNet objective encourages low correlation among generated alphas, perhaps via a diversity regularizer or entropy argument? \nHow does the framework scale with a larger alpha pool, and what is its computational cost relative to RL-based methods? \nIt would be helpful to include a robustness test on extreme market periods (e.g., 2020–2022) to verify generalization under stress. A comparison with a Transformer-based encoder could clarify whether improvements stem from graph reasoning or simply from greater model capacity. \nFinally, reporting statistical confidence intervals for ICIR and Sharpe ratios would substantially strengthen the empirical credibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LhwCPFpoiK", "forum": "zRKF4ln2VE", "replyto": "zRKF4ln2VE", "signatures": ["ICLR.cc/2026/Conference/Submission8547/Reviewer_Yaqu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8547/Reviewer_Yaqu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891473976, "cdate": 1761891473976, "tmdate": 1762920401767, "mdate": 1762920401767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AlphaSAGE, a framework for automated alpha mining in quantitative finance. By identifying three challenges of reward sparsity, inadequate structural representation of mathematical expressions, and lack of diversity in generated alphas, AlphaSAGE integrates a structure-aware encoder based on Relational Graph Convolutional Networks (RGCN) to capture the hierarchical nature of alpha expressions, a Generative Flow Network (GFlowNet) for diverse sampling of high-reward alphas, and a multi-faceted reward function that combines predictive performance, structural alignment, and novelty. Experiments on real-world market datasets demonstrate that AlphaSAGE outperforms baselines in both correlation metrics and portfolio outcomes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**S1:** The integration of GFlowNets with structure-aware RGCN encoding is sound. It tackles the diversity issue in alpha generation by sampling from a reward-proportional distribution. The multi-faceted reward design provides dense guidance, mitigating reward sparsity.\n\n**S2:** The experiments across multiple markets with multiple metrics show the superiority of the proposed method over several baselines. The ablation study validates the contribution of each component.\n\n**S3:** The framework provides interpretability through symbolic alpha expressions and a transparent combination scheme, aligning with real-world trading needs."}, "weaknesses": {"value": "**W1:** The framework may incur high computational costs, especially for large-scale alpha spaces. The paper does not discuss runtime comparisons or scalability limitations, which could affect practicality in resource-constrained settings.\n\n**W2:** The U.S. market data ends in 2020 due to \"limitations in the data source,\" which may reduce the generalizability of results to recent market regimes. Experiments could benefit from including more diverse markets or longer time periods.\n\n**W3:** The paper does not explore the impact of several hyperparameters (e.g., GNN depth, GFlowNet architecture).\n\n**W4:** More recent methods (e.g., LLM-based alpha mining) are not discussed and compared in the related work and experiments.\n\nMinor Comments\n\n**D1:** In the abstract, \"a novel framework is built upon three cornerstone innovations\" --> \"a novel framework built upon three cornerstone innovations\"\n\n**D2:** The reference format should be double-checked. Please use \"\\citep\" by default and \"\\citet\" only when the authors serve as a component of the sentence."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ipnW0V7bgv", "forum": "zRKF4ln2VE", "replyto": "zRKF4ln2VE", "signatures": ["ICLR.cc/2026/Conference/Submission8547/Reviewer_Md3w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8547/Reviewer_Md3w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983440462, "cdate": 1761983440462, "tmdate": 1762920399717, "mdate": 1762920399717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}