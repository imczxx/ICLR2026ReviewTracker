{"id": "Pa7oHHhqFa", "number": 1514, "cdate": 1756888717917, "mdate": 1763734463364, "content": {"title": "Language Models as Noisy Experts for Sequential Causal Discovery", "abstract": "Causal discovery from observational data typically assumes access to complete data and availability of domain experts. In practice, data often arrive in batches, are subject to sampling bias, and expert knowledge is scarce. Language Models (LMs) offer a surrogate for expert knowledge but suffer from hallucinations, inconsistencies, and bias. We present a hybrid framework that bridges these gaps by adaptively integrating sequential batch data with LM-derived noisy, expert knowledge while accounting for both *data-induced* and *LM-induced* biases. We propose a representation shift from Directed Acyclic Graph (DAG) to Partial Ancestral Graph (PAG), that accommodates ambiguities within a coherent framework, allowing grounding the *global* LM knowledge in *local* observational data. To guide LM interactions, we use a sequential optimization scheme that adaptively queries the most informative edges. Across varied datasets, we outperform prior work in structural accuracy and extend to parameter estimation, showing robustness to LM noise.", "tldr": "", "keywords": ["Causal Discovery", "Sequential Data", "Noisy Expert", "Language Model"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f0c30ce3b7ce38709f264758772a5c3cf7eb7a54.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents the BLANCE (Bayesian LM-Augmented Causal Estimation) framework for causal structure discovery and parameter estimation in settings where data arrives in sequential, batch-wise fashion. Traditional causal discovery methods typically assume access to complete data and the availability of reliable domain experts, but in practice, data often arrives in batches, is subject to sampling bias, and expert knowledge is scarce. BLANCE treats Language Models (LMs) as noisy surrogate experts to address both data-induced bias and LM-induced bias (such as hallucinations and inconsistency)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper makes an innovative and systematic contribution by identifying and resolving the dual biases inherent in combining LMs with sequential data: namely, data-induced bias (from issues like sampling bias in batches) and LM-induced bias (stemming from factors like hallucination and inconsistency). The BLANCE framework provides a hybrid Bayesian solution to systematically integrate and mitigate both sources of error.\n\n2. The experimental results are presented comprehensively and logically. The use of clear visualizations and metrics effectively conveys the purpose and efficacy of different experiments, allowing for a clear understanding of the framework's performance evolution during sequential learning and the specific contributions of key components (like sequential optimization and the dynamic threshold).\n\n3. The paper demonstrates a strong commitment to scientific reproducibility. By providing extensive experimental details, hyperparameter configurations, LM prompt templates, and comprehensive descriptions of the datasets and simulation processes in the appendix , the authors significantly enhance the credibility and reproducibility of their findings."}, "weaknesses": {"value": "1. The BLANCE framework, while effectively integrating LM knowledge into sequential data in an engineering sense, lacks depth in the foundational innovation of its methodology. Firstly, the framework adopts the PAG structure (implemented via the FCI algorithm) to handle uncertainty arising from latent confounders and selection bias , but this is a standard and mature theoretical tool in causal discovery theory for addressing the challenge of Causal Insufficiency, and is not original to BLANCE. Secondly, its Bayesian approach, which converts LM knowledge into a prior to guide structure learning, also appears to be a direct application of existing ideas for integrating domain knowledge within Bayesian causal discovery.\n2. Although Bayesian parameter estimation aims to address the challenge of latent confounders, its underlying assumption is the Linear Gaussian SEM. This limitation may not be applicable to many real-world scenarios featuring non-linear or non-Gaussian noise, thereby restricting the framework's general applicability.\n3. While the LM is queried to identify confounder variables , this process is itself heuristic and relies on the LM's world knowledge, offering no theoretical guarantee that the correct latent variable will be found. If the LM fails to identify the correct latent variable, the subsequent parameter estimation will be based on a flawed premise."}, "questions": {"value": "1. How is the hyperparameter $\\alpha$ in the dynamic background knowledge threshold $\\tau_i^e$ (Equation (5)), which balances the posterior entropy uncertainty and sampling uncertainty, systematically chosen by the paper? Does the optimal value of $\\alpha$ remain consistent across different datasets or varying LM noise levels? If sensitive, can a mechanism for adaptively adjusting $\\alpha$ be provided?\n\n2. To enhance the general applicability of the framework, it is suggested to extend the Bayesian parameter estimation to non-linear/non-Gaussian models. For example, by using flow-based or Variational Inference methods to handle non-linear SEMs, while retaining the ability to integrate LM priors. This would enable BLANCE to address a broader range of practical problems.\n\n3. Currently, the framework treats all LM responses as equally weighted. Considering that LMs can express confidence in their judgments in their output, this confidence could be included in the construction of the histogram, rather than just using a binary indicator function. This would provide more fine-grained modeling of LM noise.\n\n4. It could be considered to combine the sequential optimization of LM interaction with active learning strategies. Maximize information gain by querying the results of interventions or conditional interventions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NGOTJqQWng", "forum": "Pa7oHHhqFa", "replyto": "Pa7oHHhqFa", "signatures": ["ICLR.cc/2026/Conference/Submission1514/Reviewer_VGuz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1514/Reviewer_VGuz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760986748017, "cdate": 1760986748017, "tmdate": 1762915788515, "mdate": 1762915788515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a new method for merging background causal relation knowledge extracted from LLMs with traditional data-based causal discovery algorithms. The method consists in iterating between two steps: one that uses a data sample and causal discovery algorithm like FCI to learn a Partial Ancestral Graph (PAG) representation (which contains bidirected edges representing latent confounder as well as 'undecided' endpoints that represent ambiguity related to edge direction) and another one where the LLM is queried about the direction of ambiguous edges to refine. The two steps are iterated in a sequential Bayesian updating fashion to derive a posterior distribution over PAGs from a current prior distribution and data likelihood distribution. Given the possibility of latent variables, the latter step is performed by Bayesian inference, assuming the local models are linear Gaussian. Empirical results with simple benchmark DAGs as well as datasets from competitions show that the proposed method achieves better reconstruction metrics than data-driven methods alone and other use of LLM-backed causal discovery."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is overall well-written, the topic is relevant and the contribution is novel. The work is for the most part well described, except from some techniques (e.g.LLM pairwise or triplet prompting). The empirical results are promising, although only show experiments with small graphs."}, "weaknesses": {"value": "The paper is mostly an empirical approach, lacking theoretical justification. At the same time, the method requires many additional  assumptions. The authors do not discuss to which extent one can assume such assumptions in real world scenarios. The benchmarks used are very common, which means LLMs introduce a lot of bias towards the correct structure. The exception is the User Level Data datasets, where a learned DAG (using the whole dataset) is used as ground truth. The graphs are also relatively small, which makes guessing easier. \n\nA common pitfall of the use of LLM as experts is that their background knowledge might contradict a given dataset domain. The benchmarks used do not allow for testing this effect.\n\nThe proposed method has many moving parts: the LLM prior extraction as histograms, the refinement of a PAG, the use of a PAGs, the casting of the problem as Bayesian inference, the Bayesian parameter inference, and the streamed data. This makes it difficult to assess the contribution of the separated parts, and also casts out possible competitors. I would be more confident about the empirical results if those moving parts were evaluated independently, especially if the stream data was ignored at first. In fact, it is not very clear how common is this situation, where data arrives in batches and we need to iteratively update a causal dag from it."}, "questions": {"value": "I could not understand how the posterior distribution over PAGs is represented. The expert knowledge extracted from the LLM is represented as histograms over edges. How is this is combined with the likelihood to obtain a posterior? Do you assume that the posterior factorizes as edge distributions as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NDtl3wSU37", "forum": "Pa7oHHhqFa", "replyto": "Pa7oHHhqFa", "signatures": ["ICLR.cc/2026/Conference/Submission1514/Reviewer_nVWr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1514/Reviewer_nVWr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953047559, "cdate": 1761953047559, "tmdate": 1762915788411, "mdate": 1762915788411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on causal discovery with an emphasis on a setting where we get sequential batches of observational data. It makes sense to tackle data bias and LLM induced biases, but the data sampling method does not seem to completely address this issue as it is still from the same distribution. Secondly the benchmarks the paper focuses on are standard benchmarks which have been found to be memorized by LLMs in recent studies. Benchmarking GPT-3.5-Turbo for majority of the experiments which is an outdated model does not provide extensive understanding of how effective their pipeline is. Although the authors did use GPT-4o and GPT-5 for two datasets, they don't tackle the claims of memorization which can definitely skew their results as shown by recent literature. Some assumptions are very strong making the setting too simple, and would not hold in realistic setting. Overall I think the framework proposed is promising, but lack of extensive empirical evaluations and strong assumptions make it difficult to understand its effectiveness, especially in a realistic setting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This framework addresses a real gap in causal discovery, going beyond the static datasets used by prior work. Sequential (batch-wise) data arrival, where each batch can be biased or incomplete is a realistic problem that is underexplored.\n\n2. I like the shift from DAGs to PAGs to model uncertainty and ambiguity.\n\n3. Optimizing LLM calls via sequential optimization is a good idea to constrain the high inference cost of LLMs."}, "weaknesses": {"value": "Some weaknesses I would like the authors to tackle:\n\n1. BLANCE assumes a linear Gaussian SEM for all causal relations (Assumption 5). This heavily restricts its applicability to real-world, nonlinear systems (e.g., biological, economic, or text-based phenomena). Many causal discovery tasks today rely on nonlinear additive noise models (ANMs) or neural causal discovery, so results may not generalize.\n\n2. The work assumes the true causal structure remains the same across batches which is a very big assumption. In reality, sequential or streaming data can easily violate this assumption [1]. \n\n3. No clear understanding of how much ambiguity is compressed across samples by the proposed framework.\n\n4. In BLANCE, the histograms are deterministically updated meaning there’s no principled measure of how uncertain those priors are beyond empirical entropy. The “LM-derived priors” in BLANCE are empirical frequency tables that record how often the LM predicted each causal relation type. They’re called priors because they influence the next batch’s belief, but mathematically they’re pseudo-counts, not fully parameterized or normalized probability distributions. Please correct me if I am wrong here.\n\n5. It would be helpful to see how well the framework performs beyond using FCI to initialize PAGs. If the authors can provide some results for PC, Lingam, SCORE, CamML or other algorithms, to show the generalization of their framework, that can add to their framework's impact.\n\n6. While the final metrics focus only on SHD, or final graph based edges, no analysis of ambiguity or how LLM induced bias is reduced by their framework is done. Since these were the main points targetted by the authors, I would appreciate some analysis about that. \n\n7. They compare against ILS-CSL (2023) and LLM-First baselines, but omit recent hybrid approaches like [2]\n\n8. Benchmarking GPT-3.5-Turbo for majority of the experiments which is an outdated model does not provide extensive understanding of how effective their pipeline is. Although the authors did use GPT-4o and GPT-5 for two datasets, they don't tackle the claims of memorization which can definitely skew their results as shown by recent literature [3]. I understand the issue with high cost required for running inference for these models, but evaluating on a suite of open models like Llama, Qwen, Phi could have been explored.\n\n9. BNLearn is outdated for LLM based evaluation due to high chances of data contamination which are confirmed by [3]. The paper's results would have been stronger if they evaluated on recent benchmarks, or benchmarks released after the training cut off of the model. \n\nOverall I think the work is promising, but lacking in its empirical and theoretical grounding. \n\n[1] Causal Discovery for Non-stationary Non-linear Time-series Data Using Just-In-Time Modeling. Fujiwara et al. 2023\n[2] Efficient Causal Graph Discovery Using Large Language Models Bengio et al. 2024\n[3] Realizing LLMs’ Causal Potential Requires Science-Grounded, Novel Benchmarks Srivastava et al. 2025"}, "questions": {"value": "Please refer the weaknesses section, and clarify the points. I am happy to increase my score if the authors can provide justification and empirical results to tackle these points, especially the memorization issue."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TlW9jZhUpj", "forum": "Pa7oHHhqFa", "replyto": "Pa7oHHhqFa", "signatures": ["ICLR.cc/2026/Conference/Submission1514/Reviewer_yAbn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1514/Reviewer_yAbn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762024365393, "cdate": 1762024365393, "tmdate": 1762915788256, "mdate": 1762915788256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents BLANCE, a framework that treats large language models (LLMs) as noisy experts for causal discovery in Partial Ancestral Graphs (PAGs). BLANCE models large language models (LLMs) as stochastic experts answering edge-level queries.\nGiven sequential batches of observational data, an initial PAG from FCI is refined via an information-gain criterion that selects a small subset of edges to query. The LLM responses are aggregated into histograms, used to compute a dynamic inclusion threshold that updates priors for the next round. Additionally, BLANCE uses an EM-style parameter-estimation step where the LLM suggests priors over latent confounders. Experiments on six standard small-graph benchmarks (5–37 variables) show consistent improvements in Modified SHD and SID over FCI variants and prior LM-assisted baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Representation shift: DAG → PAG lets the LLM return “uncertain” edges (◦,→, ↔) instead of forcing a directed edge.\n2) The histogram → threshold → prior loop is intuitive and ablated effectively.\n3) Sequential optimisation balances exploration (entropy) vs. exploitation (proximity to inclusion threshold).\n4) Joint structure + parameter pipeline: LLM also proposes Gaussians for latent confounders (Red-Wine example).\n5) Model-agnostic: works with any PAG-producing algorithm (FCI, RFCI, GFCI).\n6) Clear motivation and algorithmic flow; appendix includes prompts and dataset details."}, "weaknesses": {"value": "1) The sequential setting invites a bandit-style analysis, but no finite-sample or regret guarantees are provided.\n2) All structure results are synthetic; the only real dataset (Red Wine Quality) is used for parameter estimation, not structural recovery.\n3) LLM usage cost (API calls, tokens, wall time) is unreported, limiting reproducibility of practical feasibility. \n4) Missing connection and baseline of amortized expert-aided discovery [1] similarly integrates expert feedback (human and LLM) in sequential fashion (with information-gain type criteria) iand handles latent confounders via learned proposal distributions. A short conceptual or empirical comparison would clarify relative cons-pros. \n5) LLM-induced bias uncalibrated – temperature ablation (Table A3) shows modest effect; no mapping from LM log-prob to posterior weight."}, "questions": {"value": "1) What is the wall-clock time and token usage for the largest benchmark?\n2) How does the selection policy scale to d ≈ 50-100 variables?\n3) Is it better to use the LLM-reported prob of edges (in text form) or log-prob induced by embedding? Could LM probabilities be calibrated into quantitative posteriors instead of binary votes?\n4) What is the performance in real-world causal graphs (e.g., Sachs, or expert-labeled biomedical/economic data)?\n5) Would a generative or amortized formulation (e.g., GFlowNet-style ancestral sampling like [1]) reduce query cost or improve uncertainty estimation? How does your method compared with such methods?\n6) Any theoretical path toward regret or error-rate bounds under an idealized consistent expert?\n\n[1] Expert-Aided Causal Discovery of Ancestral Graphs https://arxiv.org/pdf/2309.12032"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RrmXk2YVgH", "forum": "Pa7oHHhqFa", "replyto": "Pa7oHHhqFa", "signatures": ["ICLR.cc/2026/Conference/Submission1514/Reviewer_L954"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1514/Reviewer_L954"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762480612255, "cdate": 1762480612255, "tmdate": 1762915788129, "mdate": 1762915788129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}