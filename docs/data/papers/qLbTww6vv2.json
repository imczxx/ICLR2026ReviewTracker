{"id": "qLbTww6vv2", "number": 19916, "cdate": 1758300589276, "mdate": 1759897012374, "content": {"title": "Towards Identifiable Latent Additive Noise Models", "abstract": "Causal representation learning (CRL) offers the promise of uncovering the underlying causal model by which observed data was generated, but the practical applicability of existing methods remains limited by the strong assumptions required for identifiability and by challenges in applying them to real-world settings. Most current approaches are applicable only to relatively restrictive model classes, such as linear or polynomial models, which limits their flexibility and robustness in practice. One promising approach to this problem seeks to address these issues by leveraging changes in causal influences among latent variables. In this vein we propose a more general and relaxed framework than typically applied, formulated by imposing constraints on the function classes applied. Within this framework, we establish partial identifiability results under weaker conditions, including scenarios where only a subset of causal influences change. We then extend our analysis to a broader class of latent post-nonlinear models. Building on these theoretical insights, we develop a flexible method for learning latent causal representations. We demonstrate the effectiveness of our approach on synthetic and semi-synthetic datasets, and further showcase its applicability in a case study on human motion analysis, a complex real-world domain that also highlights the potential to broaden the practical reach of identifiable CRL models.", "tldr": "", "keywords": ["Latent Causal Models", "Causal representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cbc0998e2fba4d7dab2f5bff8065a812d03c302.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies identifiability for latent additive noise models (ANMs) under environment-dependent changes in causal influence among latent variables. It introduces a nonparametric “change in influence” condition (Assumption (iv)) and, combined with standard nonlinear-ICA assumptions, proves that latent ANMs are identifiable up to permutation and scaling. A VAE-style method (Sec. 4) is proposed and experiments on synthetic data, semi-synthetic fMRI, images, and Human3.6M are presented."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear, general identifiability statement. Theorem 3.1 gives an if-and-only-if-style characterization (sufficiency in Thm. 3.1; necessity via Theorem 3.4(b) + Remark 3.5) for latent ANMs under nonlinear-ICA style environments. Theorem 3.4 separates nodes that satisfy (iv) (identifiable) from those that do not (unidentifiable), with an instructive invariance counter-example.\n\n- The variational objective (Eq. 8) and sparse, u-dependent gating are a reasonable way to encourage the intended graph and environment dependence in practice, aligning with theoretical insights.\n\n- Synthetic validation includes failure of identifiability when Assumption (iv) is violated."}, "weaknesses": {"value": "- Requiring an environment $u_i$ where $\\partial g_i^u / \\partial z_j = 0$ for all parents is a perfect-intervention condition that will rarely hold exactly in real systems. The paper gives biological analogies (PPI inhibitors), but for the Human3.6M task-conditioning it’s unclear that such exact zeros occur.\n\n- Assumptions that the mapping $f$ is injective and sufficient number of environments may be restrictive."}, "questions": {"value": "You rely on a pre-fixed causal order in inference (App. G). How robust are results to random order permutations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mKHJ7qRAz1", "forum": "qLbTww6vv2", "replyto": "qLbTww6vv2", "signatures": ["ICLR.cc/2026/Conference/Submission19916/Reviewer_jEPa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19916/Reviewer_jEPa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062116085, "cdate": 1762062116085, "tmdate": 1762932085890, "mdate": 1762932085890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the causal representation learning problem with context or \"surrogate\" variables. Using prior results from nonlinear ICA literature, the authors demonstrate that variable recovery is possible for every node for which a known surrogate value exists that correspond to a perfect intervention (with recovery up to elementwise affine transformation)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Node-level or partial identifiability results are newly emerging in CRL literature, and this paper demonstrates that node-level identifiability is achievable up-to-scaling-and-shift under relatively mild conditions on the latent model using perfect/stochastic hard interventions."}, "weaknesses": {"value": "The proof of theorem 3.1, while clever, overlooks an assumption. In order to impose the Jacobians of $\\mathbf{h}\\_{\\mathbf{u}^i}$ and $\\hat{\\mathbf{h}}\\_{\\mathbf{u}^i}$ to have the same structure, one requires (i) $\\mathbf{u}^i$ value to be known, and (ii) $i$ to be known. In interventional CRL language, this is equivalent to requiring access to a perfect intervention (corresponding to using $\\mathbf{u}^i$ under assumption (iv)) on node $i$ while knowing a topological order $(1, 2, \\dots )$ among the intervention targets $i \\in \\\\{1, 2, \\dots \\\\}$. In summary, (i) theorem statements throughout the paper require to clearly communicate this fact that \"we require to know values of both $i$ and $\\mathbf{u}^i$ to recover $\\mathbf{z}_i$\", and (ii) known latent topological order assumption -- which is used implicitly in the proofs -- must be stated explicitly in the theorem statements.\n\nWhile these problems are not critical in isolation, context matters. In the case of this paper, the context is the positioning of the paper as a whole. \"Known $i$ known $\\mathrm{u}^i$\" assumption literally is what is in other CRL contexts called interventions. Therefore, the following claim from the paper becomes strictly incorrect:\n\n> Assumption (iv), originally introduced by this work, provides a condition that characterizes the types of change in causal influences contributing to identifiability ...\n>\n> ...\n>\n> We emphasize that assumption (iv) is our key contribution, formulating changes in causal influence as constraints on the function class and thus distinguishing our work from previous studies\n\nWorse yet, it defeats the very novelty this paper tries to establish: generalizing the restrictiveness of existing (interventional) CRL papers in data availability. Unfortunately, these problems are inherent to the current _narrative_ of the paper.\n\n-----\n\nA remark on remark 3.5. The technically correct necessary condition proven in the theorem is \"for a system to be identifiable, (1) it must not obey the problem setting or any of assumptions (i)--(iii), and/or (2) it must satisfy assumption (iv). This is (for obvious reasons) no more informative than the statement in theorem 3.4(b), which, while interesting, should not be presented as a general non-identifiability result.\n\n-----\n\nA note on the proofs. They are correct to my investigation barring the problems I raised. However, many of the arguments that are required to prove the said statements are outright missing from the current document. Take the example of step III of proof of theorem 3.1. Considering that $\\mathbf{J}$ are all _functions_, it takes special effort to ensure no arguments are invalidated by making structure-based claims. Most cases are resolved since the diagonals of $\\mathbf{J}\\_{\\mathbf{h}}$ are set to $1$, but it is not obvious to a reader. Similarly, it is essential to show that if both $\\mathbf{J}\\_{\\mathbf{h}}$ and $\\mathbf{J}\\_{\\hat{\\mathbf{h}}}$ are (i) lower triangular and (ii) have one-hot $j$-th rows, then the $j$-th row of $\\mathbf{J}\\_{\\mathbf{\\Phi}}$ must also be one-hot. Even then, why does this imply $\\hat{\\mathbf{z}}_i$ is an _affine_ function of $\\mathbf{z}_i$? It could be any diffeomorphism? But the math works out -- as you noted -- such that $\\mathbf{J}\\_{\\mathbf{\\Phi}}$ is a constant matrix. All these details are missing currently. I strongly suggest the authors go through these proofs and fill in all the details. This will help to prevent overlooking assumptions similar to what I stated above."}, "questions": {"value": "I defer to weaknesses section for my comments. I do not have any questions currently to the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5yEqMJxLZn", "forum": "qLbTww6vv2", "replyto": "qLbTww6vv2", "signatures": ["ICLR.cc/2026/Conference/Submission19916/Reviewer_nLhX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19916/Reviewer_nLhX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762243773770, "cdate": 1762243773770, "tmdate": 1762932085269, "mdate": 1762932085269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The papers deal with the identification of latent causal models from high-dimensional observations and provide results for the case of general additive noise latent causal models, which extends over the prior results that were limited to linear or polynomial latent causal relationships (Liu et al. 2022, Liu et al. 2024). They utilize the commonly used assumption of an exponential family distribution conditioned on surrogate variables ($u$) for the noise variables ($n$), which helps them identify the noise variables up to permutation and scaling. Then they show how the identification guarantees on the noise variables can be used to get identification guarantees on the latent variables ($z$) themselves using the assumption that the causal influence of parent node does not contain any term invariant to surrogate variable ($u$). They benchmark the proposed approach on synthetic and semi-synthetic (fMRI) data from prior works against common baselines, and introduce a new real world human motion task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The identifiability results are interesting, building upon the prior works (Liu et al. 2022, 2024), they extend the identifiability of latent causal models with restrictive linear or polynomial assumptions, to more general additive noise causal mechanisms. The extensions for partial identifiability and post non-linear causal models is also significant and novel to the best of my knowledge.\n\n- The paper is well-written with a clear description of the various assumptions needed for the theoretical results. I really like the intuitive explanations and examples offered behind the assumption 4 on the causal influences.\n\n- The proposed approach is principled and the claims made in the paper are supported by experimentation over standard benchmarks from prior works. Also, the application to real-world human motion dataset is interesting. However, the scale of the experiments is limited (refer to the Weaknesses section below for more details)."}, "weaknesses": {"value": "- One main limitation of the work is that the authors don't mention in Section 3.1 that setup and assumptions 1-3 are very similar to the prior work by Liu et al. 2024 [1]. While the authors state that their novel contribution is only assumption 4 on causal influences, but the way the rest of the setup and assumptions are presented make it seem as if the its a contribution of this work, while the prior work Liu et al. 2024 works with the exact same formulation. It is weird that the authors compare against the prior work on latent polynomial causal model  identification in the experiments section, they do not explicitly mention this in their theoretical section. I suggest the authors to make major edits to Section 3.1 and make this connection explicit. Further this should be highlighted in the introduction as well.\n\n- The scale (dimension of the latent variables) of the synthetic and semi-synthetic experiments is too small, with the maximum dimension of the latent space being 5. It would be nice to see the results with the proposed approach for larger latent dimensions ($d=10$ or $d=20$ at least). \n\n- Given the emphasis on identifying general (latent) additive noise causal models, it would be nice to benchmark the proposed approach against the methods that identify linear (latent) additive noise models [2, 3].\n\n*References*\n\n[1] Liu, Yuhang, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun Zhang, and Javen Qinfeng Shi. \"Identifiable latent polynomial causal models through the lens of change.\" arXiv preprint arXiv:2310.15580 (2023).\n\n[2] Squires, Chandler, Anna Seigal, Salil S. Bhate, and Caroline Uhler. \"Linear causal disentanglement via interventions.\" In International conference on machine learning, pp. 32540-32560. PMLR, 2023.\n\n[3] Jin, Jikai, and Vasilis Syrgkanis. \"Learning linear causal representations from general environments: identifiability and intrinsic ambiguity.\" Advances in Neural Information Processing Systems 37 (2024): 63466-63509."}, "questions": {"value": "Please refer to my the weaknesses section above for my major questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rlAvLMCLXb", "forum": "qLbTww6vv2", "replyto": "qLbTww6vv2", "signatures": ["ICLR.cc/2026/Conference/Submission19916/Reviewer_qMYX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19916/Reviewer_qMYX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762291530229, "cdate": 1762291530229, "tmdate": 1762932084743, "mdate": 1762932084743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents theoretical conditions for identifiability of CRL with latent additive noise models. The assumptions are standard in recent advances in nonlinear ICA and generative models. The authors accompany the results with estimation based on variational inference, and present synthetic ablations along with interesting applications of their setup to real-world settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The document presents thorough intuitions for the modelling choices and assumptions. This makes the whole manuscript clear in terms of readability.\n- Investigating partial identifiability for a subset of failed assumptions is a very interesting contribution.\n- The experiments are thorough and the real-world showcase is exciting."}, "weaknesses": {"value": "**Comments on theoretical analysis:**\n- **Clarity:** Please add brief proof sketches after the main theorems: A few indications each explaining the proof idea, how tools from nonlinear ICA/aux-variable methods are used, and explicit pointers to the exact appendix sections/lemmas.\n- **Sufficiency and necessity:** The paper mentions that their conditions are both **sufficient and necessary**. However, necessity here is considered under their specific model restrictions and assumptions, which is a very specific setup that requires auxiliary variables. I believe this statement should be removed as it might suggest that without these assumptions, CRL is not possible for latent ANMs. \n  - Line 71: “Notably, our analysis shows that the proposed nonparametric condition is both necessary and sufficient for identifiability under the nonlinear ICA framework, without additional constraints.”\n  - Line 108: “a sufficient and necessary condition for changes in the causal influences”\n  - For alternative solutions, [1] uses a polynomial model, which is a sufficient condition for CRL with ANMs.\n- Section 3.3 cites PNL for a two-variable case. Would it be possible to cite a multivariate version of this work (if it exists)?\n\n**Theoretical concerns**\n\nI checked the proofs and I have the following concerns:\n  - Proof of Theorem 3.1\n    - The proof introduces h, but it has not been defined anywhere in the main text or in the proof. I can see it defined in Line 897, but it’s quite hidden in the Appendix. Consider introducing this notion in the main text or your theorem proof before writing $\\hat{\\mathbf{h}}$ and $\\mathbf{h}$. A small paragraph with this notion will significantly improve clarity.\n    - **Critical issues:** The following requires major revision. Step I of this proof requires some invertible $\\hat{f}$, which recovers some latents $\\hat{z}$. For this, the authors use Lemma B.1, where they **assume there exists $\\hat{f}$ which recovers some latents**. There are several issues with this point.\n      - This Lemma is only referenced once at the end of the proof of Theorem 3.1.\n      - The proof itself introduces a **new assumption**, which is very strong and I am not sure this holds in general.\n      - The reason why it does not hold is because we have $\\dim(\\hat{z}) = \\dim(z) + \\dim(\\varepsilon)$. However, in the proof, the authors compose $\\hat{f}$ with some $\\hat{h}$, which is prohibited due to dimensionality mismatch.\n      - I believe the authors aim to replicate the strategy from GIN [2]. However, in their paper, they establish equivalence with respect to some latent variables with potentially higher dimensions. Please, see Equation (9) from Sorrenson et al. (2020).\n      - I can recommend a very quick solution to this, which is to revert back to iVAE’s setup [3] and do the convolution trick to remove the noise. It’s less flexible but quite reasonable as you base your estimation on VAEs and not normalising flows.\n    - Line 1054: “For simplicity, in the following we neglect the noise term $\\varepsilon$”. \n      - Given the above point, you can only neglect the noise term if you are able to disentangle it from your system. However, $\\hat{z}$’s dimensionality contains $d - \\ell$ dimensions which are noise. Resolving the above problem should also fix this issue (you can then just remove this sentence).\n    - Line 1119: “we can naturally use this as prior knowledge to impose the same constraint on the inference model”. \n      - This might be confusing to some readers. The correct phrasing should be that both models belong to the same model class, so you can just say that the partial derivatives in both cases come from assumption (iv). Mentioning inference and generative model is confusing, as we want to think of identifiability as the same distribution but for two generative models (with distinct $h$, $\\hat{h}$ under the same model class). \n - Lemma B.2 claims: \"determinant = 1\" $\\to$ \"h is invertible\". However, I believe this is imprecise as the invertibility comes from construction of the additive noise model structure where you have $n_i = z_i - g_i(pa_i)$. Please revise the last sentence in Line 917. \n\nThe rest of the theory looks alright to me with enough details. I would be glad to re-assess my score if you resolve my comments above.\n\n\n\n**Estimation comments**\n- Please note explicitly that the objective you use is the ELBO, indicate it is a lowerbound of the log-likelihood, and make a small note on estimator consistency. A reference to [3] and the intuitions for assumptions with some relfections on your specific setup would be ideal.\n\n**Missing Baselines**\n- Would it be possible to include CausalVAE [4] as a baseline?\n\n[1] Liu, Yuhang, et al. \"Identifiable latent polynomial causal models through the lens of change.\" arXiv preprint arXiv:2310.15580 (2023).\n\n[2] Sorrenson, Peter, Carsten Rother, and Ullrich Köthe. \"Disentanglement by nonlinear ica with general incompressible-flow networks (gin).\" arXiv preprint arXiv:2001.04872 (2020).\n\n\n[3] Khemakhem, Ilyes, et al. \"Variational autoencoders and nonlinear ica: A unifying framework.\" International conference on artificial intelligence and statistics. PMLR, 2020.\n\n[4] Yang, Mengyue, et al. \"Causalvae: Structured causal disentanglement in variational autoencoder.\" arXiv preprint arXiv:2004.08697 (2020)."}, "questions": {"value": "- Would it be possible to use a mixture model to relax the requirement of auxiliary variables?\n- I have some experience with VAEs and structure learning and generally the MCCs have a sharp decrease when increasing latent dimensions. How does your approach scale to higher dimensions? (I am not asking for additional experiments)\n- Is MCC (mean coefficiant correlation) from standard ICA literature the same as MPC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IiWBhNLfyv", "forum": "qLbTww6vv2", "replyto": "qLbTww6vv2", "signatures": ["ICLR.cc/2026/Conference/Submission19916/Reviewer_2wYV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19916/Reviewer_2wYV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762361175977, "cdate": 1762361175977, "tmdate": 1762932083674, "mdate": 1762932083674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}