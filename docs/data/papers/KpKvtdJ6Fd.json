{"id": "KpKvtdJ6Fd", "number": 4547, "cdate": 1757705125161, "mdate": 1759898027273, "content": {"title": "TIDES: Training-free Instance Detection from Semantics", "abstract": "Efforts to leverage the coarse semantic understanding of vision-language dual encoder models, such as CLIP, for dense prediction tasks without training have shown promise, particularly in training-free open-vocabulary semantic segmentation (TF-OVSS). However, instance segmentation (TF-OVIS) remains largely unexplored because dual encoder models cannot distinguish individual instances on their own. We systematically evaluate the suitability of promptable segmentation models (PSMs), such as SAM, as sources of accurate instance delineation and present TIDES (Training-free Instance Detector from Semantics), a pipeline that repurposes any pair of TF-OVSS and PSM for instance segmentation. At its core is our instance-oriented (IO) scoring, which leverages patch-level semantic alignments from TF-OVSS to re-evaluate PSM-generated masks, accurately identifying individual object instances without training, instance-level labels, or external detectors. Extensive evaluation on the MS COCO-based OVIS benchmark across multiple TF-OVSS and PSM combinations demonstrates TIDES’ flexibility and effectiveness: it surpasses the previous best TF-OVIS method by 9.2 AP and naive baselines with the original scoring by 2.7 AP.", "tldr": "We propose TIDES, a novel pipeline that integrates the semantic understanding of dual-encoders with the instance awareness of promptable segmentation models, enabling accurate instance segmentation without training.", "keywords": ["Computer Vision", "Training-Free Open-Vocabulary Instance Segmentation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a7a0f1719e7f140f0a42fc4cac2699df5b68b5f.pdf", "supplementary_material": "/attachment/092245f00bcf0a62e956538f3c4e8c81bda152ab.pdf"}, "replies": [{"content": {"summary": {"value": "Authors identify a pitfall in current literature: there are lots of work tackling training-free semantic segmentation, but few tackling training-free instance segmentation. The main reason is that backbone models used for TF-OVSS have good understanding of semantics, yet not capable of distinguishing between multiple instances."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Authors correctly identify and tackle a present problem in the computer vision landscape. While most methods aim to solve training-free semantic segmentation, in this work, authors tackle training-free instance segmentation, a much challenging task.\n\n- Authors evaluate their pipeline approach on multiple TF-OVSS and PSM models.\n\n- The method proposed by authors is adaptable to any TF-OVSS model and any PSM model. Furthermore, their performance improves as the future advancements improve the semantics and the instance-level delineation.\n\n- The pipeline solution proposed shows improvements over previous methods, and in general, well-motivated. Overall, I recommend acceptance."}, "weaknesses": {"value": "- Major drawback: There seems to be a lot of engineering hand-crafted tricks and fixed parameters (e.g. 0.2 IoU in line 367, or IoU 0.7 in line 372). There seems to be a lot of tunable hyperparameters which authors show they have a big impact in the downstream AP (Table 4 Appendix). All the ablations and experiments seem to revolve around MSCOCO dataset, possibly overfitting hyperparameter selection to this specific dataset. I believe that building a pipeline, ablating the method, and evaluating the method with a single dataset can be misleading.\n- Minor drawback: Pipeline latency grows considerably when the number of classes increases.\n- Minor drawback: The method is highly dependent on base vision-language encoder model semantic understanding."}, "questions": {"value": "- Section 4.2. Could authors clarify how are different instances of the same semantic concept differentiated from one another if the histogram distribution is very similar? (e.g. two different apples should yield two very similar histograms/embeddings).\n\n- Isn't randomly selecting 5 points (line 186) a bit unfair? Did authors experiment with sampling points around the center of the instance rather than randomly (which could cause points near the edges of the mask?). Are the same point locations shared among different models (for a fairer comparison)?\n\n- Why is SS Perf value in table 1 much higher than values obtained with Raw and IO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q86Z4iOD0m", "forum": "KpKvtdJ6Fd", "replyto": "KpKvtdJ6Fd", "signatures": ["ICLR.cc/2026/Conference/Submission4547/Reviewer_y1ie"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4547/Reviewer_y1ie"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761671425697, "cdate": 1761671425697, "tmdate": 1762917433020, "mdate": 1762917433020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TIDES, a training-free pipeline for open-vocabulary instance segmentation (OVIS). It tackles the core problem that promptable segmentation models (PSMs) like SAM, while capable of generating accurate instance masks, often assign their highest confidence scores to parts of objects rather than the complete instance. TIDES solves this by combining any semantic segmentation model with any PSM. Its key contribution is the IO scoring method, which re-evaluates all raw masks from the PSM. It converts each mask into a semantic embedding based on a histogram of patch-text alignment scores from the TF-OVSS, clusters these embeddings, and uses the local cluster density as the new IO score. Experiments show that it surpasses previous TF-OVIS SOTA and can adapt many combinations of TF-OVSS models and PSMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method achieves state-of-the-art results for training-free OVIS, surpassing the previous SOTA by a large margin.\n2. The paper provides a clear quantitative analysis of the exact problem with PSMs: their scores are misaligned with human perception of instances, even when they are capable of producing the correct masks. The solution (IO scoring) directly targets this diagnosed weakness. Ablation studies also show consistent improvements with IO scoring.\n3. Experiments with multiple semantic models and PSMs suggest the transferability of the proposed approaches."}, "weaknesses": {"value": "1. The pipeline requires a PSM to perform instance segmentation, which requires iterative prompting and merging. This is inefficient and highly dependent on PSM to provide good masks. It cannot improve results when GT masks are given. And it cannot be adapted to class-agnostic entity segmentation models.\n2. Though this paper shows good improvements over the previous SOTA, the application scenarios of the proposed method seem restricted. The design of selecting mask candidates makes it infeasible for part segmentation; it is not easy to transfer it for LLM-based reasoning segmentation.\n\nBesides that, this paper has acknowledged or implied the following weaknesses: \n\n3. The pipeline is complex with many hyperparameters and is highly sensitive to hyperparameters (Table 4). Some hyperparameters can cause a significant gap (~20% in AP).\n4. The pipeline is inefficient: it takes ~100 seconds for one image when the number of instances and number of classes increase (Table 6 and Table 7).\n\nOverall, I think the approaches come with interesting observations and good motivations. But the problem that this paper tackles is less significant and the setting has limited application scenarios."}, "questions": {"value": "1. It looks like this work is transferrable to open-vocabulary panoptic segmentation. The authors may conduct experiments on such benchmarks and metrics and make some quantitative comparisons to training-based approaches such as MaskCLIP (Ding et al) [1] and ODISE [2]. Moreover, MaskCLIP (Ding et al) [1] already includes a training-free baseline (\"w/o RMA\" in the paper) that can be compared in a fair setting. This would attract more audience and offer a sense of gaps between training-free and training-based approaches.\n\n[1] Open-Vocabulary Universal Image Segmentation with MaskCLIP, ICML 2023\n\n[2] Open-Vocabulary Panoptic Segmentation With Text-to-Image Diffusion Models, CVPR 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wEetxEISMO", "forum": "KpKvtdJ6Fd", "replyto": "KpKvtdJ6Fd", "signatures": ["ICLR.cc/2026/Conference/Submission4547/Reviewer_xytL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4547/Reviewer_xytL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918462377, "cdate": 1761918462377, "tmdate": 1762917432740, "mdate": 1762917432740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Against the background that vision-language dual encoders (e.g., CLIP) work for training-free open-vocabulary semantic segmentation (TF-OVSS) but fail in training-free open-vocabulary instance segmentation (TF-OVIS) due to poor instance distinction, and promptable segmentation models (PSMs, e.g., SAM) generate instance masks with confidence score bias, the paper addresses the limitations of existing TF-OVIS methods (e.g., Zip’s reliance on specific TF-OVSS, GroundedSAM’s need for instance annotations) by proposing TIDES. TIDES uses an instance-oriented (IO) score leveraging TF-OVSS’s patch-level semantic alignment to re-evaluate PSM masks, enabling flexible combination of any TF-OVSS and PSM without training or instance labels. Experiments on MS COCO zero-shot OVIS show TIDES outperforms naive baselines by 2.7 AP and prior SOTA Zip by 9.2 AP (best 21.0 AP). Its contributions include systematic analysis of PSM’s score bias, the general IO score, flexible TIDES pipeline, and identification of TF-OVIS’s three core challenges."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The TIDES pipeline is designed with modularity, supporting arbitrary TF-OVSS/PSM combinations. Experiments on MS COCO zero-shot OVIS are comprehensive, with consistent performance gains across 9 model combinations, verifying robustness.\n2. The problem definition (TF-OVIS’s challenges) and method logic (IO score’s semantic embedding + clustering) are clearly presented, and Section 7’s component replacement experiments effectively identify TF-OVIS bottlenecks (semantic alignment, class filtering, instance masks).\n3. It reveals three core challenges of TF-OVIS (fine-grained semantic alignment, in-context object identification, instance boundary delineation), and provides a new paradigm for fusing pre-trained models for training-free instance segmentation."}, "weaknesses": {"value": "1. Lack of Theoretical Breakthrough: The IO score relies on existing semantic alignment (from TF-OVSS) and clustering (kernel density estimation) techniques—no novel theoretical frameworks or mathematical models are proposed, making the method appear as a “combination of existing tools” rather than an innovative design.\n2. Incomplete Experimental Validation: No ablation experiments for TIDES’s core modules (e.g., removing IO score, replacing TF-OVSS with random semantic signals) to verify their necessity; no analysis of performance in special scenarios (small objects, occluded instances, dense scenes) to test robustness.\n3. Insufficient Practical Value Assessment: The paper does not compare TIDES with widely used training-free segmentation paradigms (e.g., VLM+SAM) to prove its practical advantage; nor does it analyze efficiency (e.g., inference time, memory cost) to support real-world deployment claims.\n3. Weak Visualization: Critical results (e.g., IO score’s improvement on non-uniform objects like cars) lack mask-level comparisons (e.g., PSM’s original mask vs. IO score-selected mask with IoU labels), making it hard to intuitively confirm the method’s effectiveness."}, "questions": {"value": "1.  Since the IO score uses existing semantic alignment and clustering techniques, what unique insights or design principles does it provide beyond “combining TF-OVSS and PSM”? Please explain if there is any theoretical basis for the hyperparameter settings (e.g., \\(h=0.5\\), density threshold 0.90) rather than empirical selection.\n2. Can you supplement ablation experiments to verify the necessity of each TIDES module (e.g., what happens if IO score is replaced with PSM’s original score, or TF-OVSS is removed)? Also, please add performance metrics for small objects (area < 32²) and occluded instances to show robustness.\n3. Can you provide mask-level qualitative comparisons (labeled with IoU and confidence scores) between PSM’s original predictions and IO score-optimized results? Also, please visualize semantic embedding clustering (e.g., embedding distribution of different instances) to confirm clustering effectiveness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0CSTYnb91q", "forum": "KpKvtdJ6Fd", "replyto": "KpKvtdJ6Fd", "signatures": ["ICLR.cc/2026/Conference/Submission4547/Reviewer_VPz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4547/Reviewer_VPz9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918765994, "cdate": 1761918765994, "tmdate": 1762917432443, "mdate": 1762917432443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on training-free open-vocabulary instance segmentation (TF-OVIS). It points out that while CLIP-like models have advanced training-free open-vocabulary semantic segmentation (TF-OVSS), they are weak in distinguishing instances. Meanwhile, SAM-like promptable segmentation models (PSMs) can outline instances accurately but often focus on object parts. Additionally, the academic community holds that TF-OVSS cannot be directly applied to instance segmentation. The study proposes an Instance-Oriented (IO) scoring method, which optimizes PSMs' mask selection by combining TF-OVSS semantic scores and increases SAM's correct mask selection probability by 22%. It also develops the TIDES pipeline, which integrates any PSMs and TF-OVSS for training-free instance segmentation.Experiments show that on the MS COCO benchmark, TIDES outperforms baselines by 2.7 AP and surpasses the SOTA method Zip by 9.2 AP. The paper also identifies three key technical challenges of TF-OVIS."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1、This paper distinguishes different instances within the same category by introducing a score for evaluating objectness beyond classification scores. Notably, this approach is achieved in a training-free manner, making it more applicable in practical scenarios.\n\n2、The paper leverages the powerful capabilities of current foundation models such as SAM and CLIP to achieve training-free perception. And, the experiments varify the effectiveness of the proposed method."}, "weaknesses": {"value": "1、Proposal masks generated by methods like SAM may represent different granularities. For example, a mask of the \"head\" is inappropriate when the target is \"person\", but it constitutes a complete instance for part segmentation (e.g., when the goal is to segment \"head\"). For training-free segmentation, the model should be capable of hierarchical segmentation, generating different masks based on prompts of varying granularities. This should be validated on datasets such as Pascal Part and ADE20K Part Segmentation.\n\n2、The traditional CLIP-based approach for training-free open-vocabulary semantic segmentation (OVSeg) has a limitation: it requires a predefined list of categories during evaluation, which does not qualify as true open vocabulary. In contrast, generative vision-language model (VLM)-based methods are inherently better suited for achieving open-domain perception. Please discuss the advantages of this approach (referring to the method in question) and VLM-based methods.\n\n3、The paper relies on masks generated by SAM. For hard cases such as small objects—where masks fail to be triggered by prompts—how can this missing issue be addressed?\n\n4、How is the method's inference speed, given that SAM is widely known to generate masks slowly?"}, "questions": {"value": "1、Please clarify the advantages of the propsoed framework compared with generative vision-language model (VLM)-based methods, which is more suitable for open-domain.\n\n2、How could the method solves the limitation of mask quality caused by SAM without training or tuning? And how could the method be applied in the segmentation for different granularities.\n\n3、The speed comparison of the propsoed methods and other methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fYVLXK3Yo3", "forum": "KpKvtdJ6Fd", "replyto": "KpKvtdJ6Fd", "signatures": ["ICLR.cc/2026/Conference/Submission4547/Reviewer_gxEU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4547/Reviewer_gxEU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924804097, "cdate": 1761924804097, "tmdate": 1762917432081, "mdate": 1762917432081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}