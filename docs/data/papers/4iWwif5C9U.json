{"id": "4iWwif5C9U", "number": 22571, "cdate": 1758332965349, "mdate": 1759896858800, "content": {"title": "Inverse Reinforcement Learning Using Just Classification and a Few Regressions", "abstract": "Inverse reinforcement learning (IRL) aims to explain observed behavior by uncovering an underlying reward. In the maximum-entropy or Gumbel-shocks-to-reward frameworks, this amounts to fitting a reward function and a soft value function that together satisfy the soft Bellman consistency condition and maximize the likelihood of observed actions. While this perspective has had enormous impact in imitation learning for robotics and understanding dynamic choices in economics, practical learning algorithms often involve delicate inner-loop optimization, repeated dynamic programming, or adversarial training, all of which complicate the use of modern, highly expressive function approximators like neural nets and boosting. We revisit softmax IRL and show that the population maximum-likelihood solution is characterized by a *linear* fixed-point equation involving the behavior policy. This observation reduces IRL to two off-the-shelf supervised learning problems: probabilistic classification to estimate the behavior policy, and iterative regression to solve the fixed point. The resulting method is simple and modular across function approximation classes and algorithms. We provide a precise characterization of the optimal solution, a generic oracle-based algorithm, finite-sample error bounds, and empirical results showing competitive or superior performance to MaxEnt IRL.", "tldr": "We show entropy-regularized IRL can be reduced to a few supervised learning tasks (and thus be easily solved with flexible function approximation): multiclass classification to fit behavior policy and a few regressions to solve a linear fixed point", "keywords": ["Inverse reinforcement learning", "Maximum entropy IRL", "Softmax", "Fitted Q-iteration", "Bellman"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3c5d485c85e6ae9b87d05fbd52f64c75ec28f467.pdf", "supplementary_material": "/attachment/f782c6002b1520c79038f4c15cd20b71ccfb9bb7.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the IRL problem not only from the perspective of imitation learning, i.e., to imitate the behavior of the expert, but also to more generally compute the reward function of the expert, that permits more general kinds of analysis, like evaluating policies under different dynamics. After having formulated the problem setting, the paper provides some theoretical insights on the target reward function, that give birth to a novel two-oracle algorithm. Finally, the authors provide a theoretical analysis for the proposed algorithm."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed algorithm is supported by a thorough theoretical analysis"}, "weaknesses": {"value": "- The main limitation is that the problem formulation is quite unclear. This paper lies on the quite strong assumption that the true expert reward satisfies the normalization condition in lines 139-141. This is very strong and not motivated in any way by the authors. This assumption corresponds to choosing a single (arbitrary) reward in the feasible set of the expert, as explained for instance in [1]. But why should this reward be better than others in the feasible set?\n- Another huge limitation is that almost all the results in Section 3 correspond to a rephrasing of existing results in previous works. Specifically, Lemma 1, Lemma 2 and Theorem 1 basically correspond to Theorem 1 of [2] (but see also Section 4 of [3] for Lemma 1).\n- The discussion on related works is quite poor. Specifically, the authors miss most of the recent IRL literature, including but not limited to [1,2].\n- The numerical simulations are quite trivial and do not clarify in any manner why the assumption that the true reward is normalized as in Eq. (1) is realistic or relevant.\n\n[1] Invariance in Policy Optimisation and Partial Identifiability in Reward Learning. Skalse et al.\n\n[2] Identifiability in inverse reinforcement learning. Haoyang Cao, Samuel N. Cohen,  Lukasz Szpruch\n\n[3] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning"}, "questions": {"value": "Please, see the weaknesses above.\n\n- Why do you mention ziebart 2008 instead of ziebart 2010 when considering expert behavior as expected return maximizing plus an entropy term? Ziebart 2008 has an issue that was later solved in ziebart 2010, as explained e.g., in Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review by Levine.\n- How is the initial state distributed? How are the expert data generated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7efffluJJg", "forum": "4iWwif5C9U", "replyto": "4iWwif5C9U", "signatures": ["ICLR.cc/2026/Conference/Submission22571/Reviewer_HuNX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22571/Reviewer_HuNX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760971011709, "cdate": 1760971011709, "tmdate": 1762942285569, "mdate": 1762942285569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper solves the IRL problem under the maximum entropy IRL/dynamic discrete choice (DDC) model with classification and regressions. Given oracles of obtaining $\\log\\pi$ and regression oracle, the authors show that ensuring $\\sum \\mu(a|s) r(s,a) = 0$ would yield the normalization constant, thus recover the true reward. The authors show that the a Bellman type of regression update yields linear convergence. Using sample splitting, the author obtain the sample complexity $ \\tilde{O}((1-\\gamma)^{-2} \\varepsilon^{-2}) $."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. this paper presents an interesting idea, where we can find the underlying reward function through ensuring the constraint of the original problem, to which, the authors propose to ensure the gauge choice in this paper. \n2. the proposed algorithm on finding the true value function is interesting and potentially useful when we can estimate the policy but don't know the corresponding reward function. \n3. the analysis is nice and sweet. The application of sample splitting is interesting here."}, "weaknesses": {"value": "Albeit the nice results, there are a few concerns. Appendix is checked but not rigorously. \n1. overall the contribution of this work mainly relates to the idea of ensuring the constraint of $ \\mu v = 0 $, as Theorem 1 and lemma 2 is well known, and Theorem 2 is the consequence of ensuring the constraints. The follow up algorithm is based on Thm 2, which use some type of TD estimate for solving $ v^* = P\\mu (\\gamma v^* - u^*) $. Up to this point, it is clear what this results entails, but the choice of notations and presentation confuse me. It seems like this is inherently related to policy evaluation under the MaxEntIRL framework, and I suggest the authors to reframe this more clearly. \n2. Lemma 3-4 and Thm 3 are results of general fixed-point iteration, presented in the paper's context. But the fact that the results are deriving from the deterministic setting (the analysis assumes we can get $ Tv $ in all the analysis after lemma 3 but in the algorithm it is using temporal difference type of update) concerns me. How is the deterministic analysis related to a stochastic algorithm? Am I missing something here?\n3. For the algorithm, the authors assume we know an oracle of estimating $ \\log \\pi(a|s) $. I am not sure how this is applicable to other settings but for majority of the IRL applications, $ \\log \\pi $ seems to be already very difficult to estimate. Note that the whole field of diffusion policies (e.g. VLA models) is devoted to find the logits. Knowing this, the scope of this work is quite narrow. \n4. The results in 5.2 seems to be applying known results of sample splitting with various assumptions. I am failing to distinguish what's new and what's known in the literature here. \n5. Given the literature of offline IRL [1], what's the advantage of this algorithm? I think discussion of more recent work is needed. \n6. The assumptions in this paper are quite restrictive. E.g., neither assumption 1 and 2 seems to be applicable to high dimensional problems such as robotics control (although it seems assumption 2 seems unavoidable in some degree). In real problems, how ever, the update is stochastic, which this analysis does not cover.  \n[1] Zeng, Siliang, et al. \"When demonstrations meet generative world models: A maximum likelihood framework for offline inverse reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 65531-65565."}, "questions": {"value": "Some minor grammar and notational issues:\n1. line 19-20, \"population maximum-likelihood\"\n2. line 45-46, \"from too-directly tackling the IRL problem\"\n3. the usage of v is very confusing to me. \n4. line 225, $ v^* $ or $ v^\\star $?\n5. line 277, what is $ (s,a)\\sim P $?\n6. there are many P that denotes different things. Please differentiate them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r86bg801i2", "forum": "4iWwif5C9U", "replyto": "4iWwif5C9U", "signatures": ["ICLR.cc/2026/Conference/Submission22571/Reviewer_HUTV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22571/Reviewer_HUTV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854767962, "cdate": 1761854767962, "tmdate": 1762942284559, "mdate": 1762942284559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel and simplified approach to Inverse Reinforcement Learning (IRL) within the popular maximum-entropy (MaxEnt) framework. The authors argue that existing MaxEnt IRL algorithms are often complex, involving nested optimizations, repeated dynamic programming, or adversarial training, which hinders their use with modern function approximators.\n\nThe paper's core contribution is a new characterization of the MaxEnt IRL solution. The authors first analyze a relaxed version of the problem (without reward normalization) and show that a trivial optimal solution is $r(s,a) = \\log \\pi(a|s)$ (the log-behavior policy) and $v(s,a) = 0$ (the soft value function). They then show that all solutions to this relaxed problem are equivalent to this trivial solution under a potential-based reward shaping.\n\nThe solution to the original, non-relaxed problem is then found by identifying the specific potential function that satisfies the required reward normalization (e.g., $\\mu r = 0$). This reduces the entire IRL problem to solving a linear fixed-point equation for this potential function.\n\nThis insight leads to a simple two-stage algorithm, \"Classify-then-Regress\":\n1.  Classify: Use any off-the-shelf probabilistic classifier to estimate the behavior policy $\\hat{\\pi}(a|s)$ from data, yielding the trivial solution $\\hat{u} = \\log \\hat{\\pi}$.\n2.  Regress: Use an iterative regression routine (like fitted Q-iteration) to solve the linear fixed-point equation for the value function $\\hat{v}$, which requires only a few iterations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength is its novel theoretical characterization of the MaxEnt IRL solution. The realization that $r = \\log \\pi, v = 0$ is a trivial optimal solution to the relaxed problem and that the true solution is just a potential-shaping of it is a deep and valuable insight.\n\nThe method successfully avoids the \"delicate inner-loop optimization, repeated dynamic programming, or adversarial training\" that are common failure points in other IRL algorithms. The regression loop is a stable $\\gamma$-contraction, not a nested optimization.\n\nThe paper provides a rigorous theoretical analysis, including oracle inequalities (Thm 3) and finite-sample bounds (Thm 4) that connect the final IRL error to the statistical rates of the underlying supervised learning components."}, "weaknesses": {"value": "The experiments are conducted only on small gridworlds. While they effectively demonstrate the core concept (especially the advantage in the \"Misspecified\" case), they do not show how the method scales to the large, high-dimensional, and continuous-state problems (e.g., MuJoCo) where modern IRL methods like GAIL/AIRL are typically benchmarked.\n\nThe entire algorithm hinges on obtaining a good estimate of the behavior policy $\\hat{\\pi}(a|s)$. This is a classification problem, which is well-understood, but its difficulty is non-trivial, especially in high-dimensional state spaces or when the policy is highly stochastic. The paper's analysis assumes a good $\\hat{\\pi}$ is given, but the practical performance will be bottlenecked by this step.\n\nThe finite-sample guarantees (Theorem 4) rely on Bellman completeness (Assumption 2). The authors note this is common in FQI analysis and suggest possible relaxations, but it remains a strong assumption that may not hold when using highly expressive function approximators like neural networks."}, "questions": {"value": "1.  The experimental validation is a strong proof-of-concept but is limited to gridworlds. How does the \"Classify-then-Regress\" approach scale to high-dimensional, continuous-state benchmarks (e.g., D4RL MuJoCo) compared to SOTA IRL/imitation learning methods like GAIL, AIRL, or IQ-Learn?\n2.  The algorithm's first step is to learn $\\hat{u} = \\log \\hat{\\pi}$. This seems to be the most critical step. How does the algorithm's performance (in terms of final reward and policy accuracy) degrade as the classification oracle's accuracy worsens, for example, in data-sparse regions?\n3.  The regression step (Line 6, Alg. 1) is a form of fitted iteration, which is known to suffer from OOD extrapolation errors in offline RL. Does this iterative regression step suffer from distribution shift, since the target value $\\hat{v}^{(k-1)}(s_i', \\cdot)$ is evaluated at the next state $s_i'$?\n4.  The reward normalization $\\mu r = 0$ is central to finding a unique solution. How is the reference measure $\\mu$ chosen in practice, and how sensitive is the final recovered reward $r^$ to this choice? For example, what is the practical difference between choosing $\\mu$ as a uniform distribution vs. a point mass on a specific action?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "j8oaQ8dqU9", "forum": "4iWwif5C9U", "replyto": "4iWwif5C9U", "signatures": ["ICLR.cc/2026/Conference/Submission22571/Reviewer_5bsp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22571/Reviewer_5bsp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926413903, "cdate": 1761926413903, "tmdate": 1762942283905, "mdate": 1762942283905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a simple new way to do inverse reinforcement learning—recovering the reward function behind an expert’s behavior—without complex optimization. Instead of using nested dynamic programming, it shows that the reward can be found using just classification and a few regressions. The key idea is that, under a softmax policy model, the log of the observed policy already contains most of the reward information, and a simple normalization step per state (“softmax normalization”) removes the usual ambiguity in reward values. The method is easy to implement, mathematically clean, and produces rewards that are consistent with the expert’s behavior as well as with traditional MaxEnt IRL methods, but with much lower computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is strong because it provides a simple, elegant, and computationally efficient reformulation of inverse reinforcement learning. It replaces the usual complex, iterative optimization steps with standard classification and regression tasks, making IRL both practical and theoretically grounded. The approach is mathematically clear, showing how the reward, value, and policy connect naturally under a softmax model, and it includes theoretical guarantees for identifiability and generalization. A particularly notable strength is how the paper links this framework to the economics literature on dynamic discrete choice models, revealing that the IRL reward recovery problem is formally equivalent to utility estimation in econometrics. This connection bridges two fields—machine learning and economics—offering both conceptual clarity and opportunities for cross-disciplinary methods and insights."}, "weaknesses": {"value": "The paper focuses on a specific class of normalized rewards—those constrained to integrate to zero against a reference conditional measure—but the motivation for this choice is not clearly explained. It is unclear why this normalization is preferred over alternative approaches to resolving reward identifiability, such as anchor-action methods, structural assumptions, or multi-environment formulations, as discussed in Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning, Reward Identification in Inverse Reinforcement Learning (Kim et al., 2021), Deep PQR: Solving Inverse Reinforcement Learning using Anchor Actions (Geng et al., 2020), and econometric approaches such as Identification and Estimation of a Discrete Game of Complete Information. A justification for adopting this normalization—and a comparison to these alternative frameworks—would strengthen the contribution. If absolute reward values are not meaningful, simpler methods like imitation learning or behavior cloning would suffice to reproduce expert behavior without explicit reward recovery. Conversely, if the reward scale is important for applications such as welfare analysis, transfer, or generalization, the proposed normalization might distort the reward’s magnitude and renders values incomparable across states or environments. \n\nIn addition, the proposed method appears to share strong similarities with the Deep PQR framework (Geng et al., 2020). Both algorithms first perform a classification step to estimate the policy or choice probabilities and then solve a fixed-point relation to recover the reward function. The resulting reward-estimation formulas are also closely aligned—see line 248 of the submission and Definition 1 in the Deep PQR paper—which suggest nearly equivalent functional forms. The primary distinction lies in the identification assumption leveraged: the current paper imposes a normalization constraint, whereas Deep PQR uses an explicit anchor-action assumption."}, "questions": {"value": "Could the authors elaborate on the motivation for adopting the specific normalization constraint (requiring the reward to integrate to zero against a reference conditional measure)? Why is this particular normalization more desirable than other identification strategies (e.g., anchor-action, multi-environment, or structural dynamic assumptions)?\n\nThe proposed algorithm appears structurally similar to Deep PQR—both first estimate choice probabilities via classification and then solve a fixed-point relation to recover the reward. Could the authors clarify the key methodological or conceptual differences beyond the identification assumption (normalization vs. anchor action)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uoVpunJkEp", "forum": "4iWwif5C9U", "replyto": "4iWwif5C9U", "signatures": ["ICLR.cc/2026/Conference/Submission22571/Reviewer_Tr5W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22571/Reviewer_Tr5W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762029619880, "cdate": 1762029619880, "tmdate": 1762942283641, "mdate": 1762942283641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}