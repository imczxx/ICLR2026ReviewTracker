{"id": "ue1zFeD275", "number": 1778, "cdate": 1756919129714, "mdate": 1759898187407, "content": {"title": "FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models", "abstract": "Autoregressive language models (ARMs) deliver strong likelihoods, but are inherently serial: they generate one token per forward pass, which limits throughput and inflates latency for long sequences. Diffusion Language Models (DLMs) parallelize across positions and thus appear promising for language generation, yet standard discrete diffusion typically needs hundreds to thousands of model evaluations to reach high quality, trading serial depth for iterative breadth. We introduce **FS-DFM**, Few-Step Discrete Flow-Matching. A discrete flow-matching model designed for speed without sacrificing quality. The core idea is simple: make the number of sampling steps an explicit parameter and train the model to be consistent across step budgets, so one big move lands where many small moves would. We pair this with a reliable update rule that moves probability in the right direction without overshooting, and with strong teacher guidance distilled from long-run trajectories. Together, these choices make few-step sampling stable, accurate, and easy to control. On language modeling benchmarks, FS-DFM with 8 sampling steps achieves perplexity parity with a 1\\,024-step discrete-flow baseline for generating 1\\,024 tokens using a similar-size model, delivering up to 128× faster sampling and corresponding latency/throughput gains.", "tldr": "A discrete flow matching model designed for speed without sacrificing quality.", "keywords": ["Diffusion language model", "few step generation", "flow matching"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b93351ad66e0c17cadc77ea9bff438fca5587b0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces FS-DFM (Few-Step Discrete Flow-Matching), a discrete diffusion language model trained to work across different step budgets. With step-aware training (conditioning on the intended step size and a shortcut-consistency objective integrated with RK4) and a Cumulative Scalar that average the flow rate over [t,t+h], the method reaches the 1024-step discrete-flow baseline quality using 8 sampling steps for 1024-token sequences. Experiments on FineWeb-Edu (training) and WikiText-103 (evaluation) across 0.17B, 1.3B, 1.7B parameters show that FS-DFM preserves quality with far fewer steps than standard discrete flows."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The derivation and use of the Cumulative Scalar (Eq. 12) are well motivated for finite-step transport in discrete flows. The closed-form log-ratio provides a stable update for large h and aligns with the few/one-step goal.\n2. Empirically, the method matches the 1024-step baseline in 8 steps, which is a clear sampling efficiency improvement at long sequence length (1024 tokens).\n3. The gains appear across three model sizes (0.17B, 1.3B, 1.7B), which suggests the effect is not tied to a single scale or architecture variant within the paper’s setup.\n4. A single model supports multiple step counts through explicit conditioning, so one does not need separate models for different latency budgets."}, "weaknesses": {"value": "1. Missing comparisons to similarly sized autoregressive baselines on the same data; this limits how readers relate the results to standard text generation references.\n2. No evidence the method scales to modern 7B+ model sizes where training costs become critical.\n3. Training is on FineWeb-Edu and evaluation is on WikiText-103 only; results on broader corpora or mixed-domain evaluations are not reported.\n4. The evaluation focuses on open-ended continuation; there are no results on downstream or conditional tasks (for example instruction following or machine translation), so generality is uncertain.\n5. All experiments use 1024 tokens (and 512→512 continuations)—unclear if advantages persist for shorter or longer sequences."}, "questions": {"value": "1. Does the method scale beyond 1.7B parameters? If you have partial results or ablations at 7B+, please share some metrics to clarify feasibility at that scale.\n2. What are the actual training costs for pretraining and the step-aware fine-tuning? It would help to report wall-clock times, hardware (GPU type and count), or an estimate in FLOPs, and how it compares to plain DFM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5LiB1q0c1V", "forum": "ue1zFeD275", "replyto": "ue1zFeD275", "signatures": ["ICLR.cc/2026/Conference/Submission1778/Reviewer_C2qF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1778/Reviewer_C2qF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760932965712, "cdate": 1760932965712, "tmdate": 1762915887004, "mdate": 1762915887004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FS-DFM, a discrete flow-matching model designed to generate text in a small number of steps. The method addresses the issue suffered by discrete diffusions models, namely that they require many steps to achieve string performance. The approach introduces two main components: a \"Cumulative Scalar\" to enable large generation steps and a \"Shortcut Teacher\" (using an RK-4 ODE solver) to train the model on its step budget h. The authors report strong empirical performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Inference Speed: The 128x reduction in sampling steps (from 1024 to 8) while maintaining perplexity is a highly significant and practical achievement in parallel decoding.\n\nMethodological Innovation: The identification of the \"stalling\" problem at $t \\approx 0$ led the authors to the \"Cumulative Scalar\", that is proven (Table 1) to be critical for few-step performance.\n\nStrong Empirical Ablations: The paper thoroughly justifies its design choices. The ablations clearly isolate the large gains from the Cumulative Scalar, the superiority of the RK-4 teacher, and the necessity of a Uniform source over a Mask source (Table 4) for this method.\n\nCompelling Qualitative Results: State-of-the-art diffusion models (LLaDA, Dream) fail in the 8-step regime (as expected), while FS-DFM produces coherent text."}, "weaknesses": {"value": "Training Cost: The inference speedup comes at a high price. The authors note that the RK-4 teacher doubles training cost. While this doubling of the fine-tuning cost is a practical drawback, I do not see it as a big issue.\n\nLimitation to Uniform Source: The method is dependent on a uniform source. The ablation study shows that the model fails when starting from a [MASK] (absorbing) source. I see this as a limitation, as it means the method is incompatible with the common and often better-performing masked diffusion models. I wonder if it would be interesting finding the right way of doing few step sampling in the context of absorbing state diffusions.\n\n\n\nFactorized Sampling: The underlying DFM framework relies on factorized velocities, meaning token jumps are sampled independently at each step, conditioned on the full context. This is likely the reason why the 1-step perplexity is still very high, forcing the model to need some steps to correct its own errors. Given this factorization assumption, I do not see this method being able to go down to 1 or 2 step sampling, which has been pushed quite a bit in the continuous domain."}, "questions": {"value": "Why does performance go down in figure 1 after a certain NFE?\n\nThe “diffusion duality” paper also relies on a uniform source discrete diffusion for distillation (among other things). It would be interesting to see how the method proposed in this work compares against that."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q6Jhqoq9XD", "forum": "ue1zFeD275", "replyto": "ue1zFeD275", "signatures": ["ICLR.cc/2026/Conference/Submission1778/Reviewer_2YuK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1778/Reviewer_2YuK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860309576, "cdate": 1761860309576, "tmdate": 1762915886891, "mdate": 1762915886891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FS-DFM (Few-Step Discrete Flow-Matching), a diffusion language model designed for efficient, long-text generation. The core innovation is a principled step-aware training regime that explicitly conditions the model on the intended number of steps, allowing large, stable jumps in the probability space. This is achieved via a novel Cumulative Scalar Update derived from analytical integration and stabilized through Shortcut Teachers (RK-2/RK-4 ODE solvers) and EMA distillation.\n\nFS-DFM empirically demonstrates strong few-step performance, matching the quality of its 1024-step DFM baseline in just 8 steps (a $\\approx 128\\times$ reduction in Network Function Evaluations, or NFE). The technical contributions are elegant and sound.\n\nHowever, the evaluation is narrow, primarily comparing against only LLaDA-7B, Dream-7B, and its own DFM lineage. Crucially, it lacks comparisons against modern but small, competing diffusion/flow families (SEDD [4], MDLM [2], HDLM [1], BD3LM [3], ReMDM [5]), which also prioritize fast, few-step generation and advanced metrics. This empirical omission limits the strength and generality of the central claim since they also have a comparable small-sized model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### **Technically Novel and Principled Framework**\nThe paper introduces a highly elegant framework by making the step budget (h) an explicit conditioning variable. The use of shortcut distillation (RK-2/RK-4) and EMA stabilization for self-consistency across step budgets represents a meaningful advance over prior DFM works.\n\n### **Analytically Integrated Update Rule**\nThe derivation of the cumulative scalar by analytically integrating the instantaneous velocity term ($\\kappȧ / (1–\\kappa)$) is a clean, necessary correction that robustly enables large jumps without the stalling issues of naive scalers, enhancing few-step stability.\n\n### **Strong NFE Efficiency** \nThe model convincingly demonstrates that it can match the quality of a 1024-step DFM baseline in only 8 NFEs, validating the few-step design goal.\n\n### **Reproducibility** \nArchitectural details (DiT-style backbone), training specifics, and a promise of code/model release are commendable."}, "weaknesses": {"value": "### **Incomplete Competitive Landscape** \nThe most significant weakness is the absence of comparisons to other state-of-the-art small-scaled DLMs, particularly SEDD [4], MD3LM [2], BD3LM [3] (Block-Diffusion), and HDLM [1] (Hybrid DLM). These models represent different philosophical approaches (e.g., different loss functions, hybrid noise schedules, architectural modifications) to solving the same problem. A comparison on a shared benchmark (e.g., WikiText-103) plotting perplexity/MAUVE vs. NFE is necessary to demonstrate where FS-DFM stands. Also, the theme of all prior work has been to compare the zero-shot PPL on a vast set of OOD datasets and not just one (WikiText-103).\n\n### **Inadequate Text Quality Evaluation** \nRelying solely on perplexity and token accuracy is insufficient for evaluating generation quality. As highlighted by HDLM [1] and ReMDM [5], metrics like MAUVE are better proxies for distributional similarity and coherence. The claim of \"accurate\" generation is not fully supported without them.\n\n\n### **Ambiguous Entropy Metric Scale** \nThe reported entropy values ($\\approx 7.4–8.1$) are inconsistent with common metrics used in competing DLM works ($\\approx 0.5–2$ bits/token in HDLM [1] / MDLM [2] / Bd3LM [3]). This discrepancy suggests a fundamental difference in calculation or base. The paper must clarify the precise formula, normalization, and base (nats vs. bits) used for the entropy calculation.\n\n### **Missing Hybrid Noise Schedule Experiments** \nThe ablation only contrasts mask vs. uniform noise. Prior work (HDLM [1]) has demonstrated that hybrid corruption schedules (mask-uniform mixtures) are empirically crucial for optimizing few-step stability in discrete diffusion models. I would appreciate if authors can provide an ablation using Hybrid noising scheme as well.\n\n### **Lack of System-Level Performance Metrics** \nThe \"128x faster\" claim is based on NFE reduction, which is an algorithmic but not a systems-level metric. It ignores the potential overhead of more complex model passes (e.g., from RK-4) and, crucially, the benefits that other methods gain from techniques like KV-caching and block decoding (see BD3LM [3], HDLM [1]). Reporting wall-clock latency and throughput (tokens/sec) on identical hardware is essential.\n\n\n### **References**\nPlease make sure you are comparing and citing the papers below:\n\n[1] Fathi et. al., \"Unifying Autoregressive and Diffusion-Based Sequence Generation\"\n\n[2] Sahoo et. al., \"Simple and Effective Masked Diffusion Language Models\"\n\n[3] Arriola et. al., \"Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models\"\n\n[4] Lou et. al., \"Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution\"\n\n[5] Wang et. al., \"Remasking Discrete Diffusion Models with Inference-Time Scaling\""}, "questions": {"value": "Please also refer to the **\"Weaknesses\"** section for more questions.\n\n### **Competitive Benchmarking & Generalization** \nTo fully contextualize FS-DFM's performance against the state-of-the-art, could you please provide:\n\n1- A direct comparison on WikiText-103 between FS-DFM and models like SEDD, MD3LM, BD3LM, and HDLM, plotting perplexity and MAUVE against NFE (e.g., 1, 2, 4, 8, 16 steps) for models of similar scale (e.g., ~0.17B parameters)?\n\n2- An evaluation of zero-shot perplexity on a broader set of out-of-distribution (OOD) datasets (e.g., from the PTB, LM1B, etc) to assess generalization beyond WikiText-103?\n\n\n### **Evaluation Metrics & Entropy Clarification**\n1- Could you report MAUVE scores for your FS-DFM generations and the provided baselines? This is critical to substantiate the \"accurate\" generation claim beyond perplexity.\n\n2- The entropy values reported (~7.4–8.1) differ significantly from other works. Please explicitly state the formula, the base (nats or bits), and the normalization (per-token or per-sequence) used for your entropy calculation in Section 5.2.\n\n### **Ablation on Noise Schedule**\nYour ablation studies mask and uniform sources. Following the strong results of HDLM [1], could you run an ablation where FS-DFM is trained and evaluated using a hybrid (mask-uniform mixture) noise schedule and report the resulting perplexity and MAUVE for few-step (e.g., 1, 4, 8) generation?\n\n### System-Level Performance Analysis \nThe \"128x faster\" claim is based on NFE. To translate this into a practical speedup, please provide:\n\n1-The wall-clock latency and throughput (tokens/second) for generating 1024-token (or 512-token) sequences with the N-step FS-DFM versus the N-step DFM baseline, measured on the same hardware (e.g., A100). Refer to Table 12 in the HDLM paper for more information. \n\n2- I would also love to see how FS-DFM compares against other methods given a similar wall clock instead of NFEs. \n\n3- An analysis of whether techniques like KV-caching or block decoding (as used in BD3LM [3] / HDLM [1]) are applicable to FS-DFM and, if so, what additional latency/throughput gains they provide.\n\n\n**Note:** The paper presents a valid theoretical contribution. However, given the limited scope of the empirical validation—specifically, the lack of comparison to key contemporary baselines and the omission of critical ablations and metrics discussed above—my current score cannot exceed **6**. I would be prepared to raise my score if the authors can provide compelling and comprehensive responses to the questions posed in this review."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jhhvEyBcOT", "forum": "ue1zFeD275", "replyto": "ue1zFeD275", "signatures": ["ICLR.cc/2026/Conference/Submission1778/Reviewer_toSy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1778/Reviewer_toSy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869182010, "cdate": 1761869182010, "tmdate": 1762915886783, "mdate": 1762915886783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **Few-Step Discrete Flow-Matching** (FS-DFM) for efficient diffusion language modeling. The paper's central idea is to explicitly make the model aware of finite-sized, possibly large step sizes and train accordingly. To this end, FS-DFM uses ODE solvers (Runge-Kutta) to solve the underlying Kolmogorov equations that characterize the generative continuous-time Markov process of a teacher model for step sizes $h$, and train a student model that directly outputs update logits corresponding to steps of size $h$ (the original discrete flow matching framework instead essentially only models the transition probabilities for infinitesimal steps). The model is initialized from a pre-trained discrete flow matching model, and the teacher corresponds to an exponential moving average of the student. The authors also propose a \"cumulative scalar\" that adapts the scaling of the probability velocity to the setting with large step sizes. In experimental validation, the step-size-aware FS-DFM model shows significantly better results when sampling with few steps, and reduces to regular discrete flow matching when sampling with many steps. The paper carries out various text generation benchmarks and ablation studies, measuring generative perplexity, entropy, and accuracy, and compares to regular discrete flow matching models as well as the large Dream and LLaDA models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Clarity:** For the most part, the paper is easy to follow and the presentation is good (I do have some clarification questions, though; see below).\n\n**Originality:** To the best of my knowledge, the proposed approach is novel, and using ODE solvers to solve the Kolmogorov equation to explicitly model finite-size steps is interesting. The method can be considered closely related to Self-Distillation Through Time, though (see comments below).\n\n**Numerical Results:** The numerical results are generally strong and show clear advantages of FS-DFM compared to regular discrete flow matching and discrete diffusion models (some comparisons are lacking, though; see below). \n\n**Quality and Significance:** The paper is overall of high quality. Accelerating diffusion language models is an important and popular topic in the literature, and the paper successfully tackles this problem, making the work significant."}, "weaknesses": {"value": "**Self-Distillation Through Time (SDTT) comparison and relation:** The paper only compares to standard discrete flow matching and discrete diffusion models, but not to other acceleration approaches. In particular, the method is closely related to SDTT, which distills a many-step teacher into a few-step student, very similar to the step-size-aware training with student and teacher of FS-DFM (FS-DFM uses Runge-Kutta solvers to solve the probability density evolution as teacher signal and uses as teacher the EMA of the student, whereas SDTT simply samples a teacher model, but the general idea is similar). I would suggest the authors to add quantitative comparisons to SDTT applied to discrete flow matching, and also discuss the relation between SDTT and FS-DFM more in-depth.\n\n**Presentation:** Some parts of the presentation seem unclear:\n- In equation (5), $z$ was never introduced.\n- Where does the objective in equation (8) come from? This is not clear, and there is no reference. The Gat et al. \"Discrete Flow Matching\" work does not have this exact form of the objective and most prior works train with simple reweighted cross entropy losses. I would like the authors to clarify. I also wonder whether this equation has an error: The first term has $p_{1|t}(x_t^i|x_t)$, i.e. both modeled variable and conditioning variable are at $t$. Is this correct?\n- Figure 1 has the main comparisons to regular discrete flow matching, but only mentions a range of entropy values in the caption. Since entropy is an important measure of generation diversity, and since it is important to understand whether FS-DFM leads to reduced diversity compared to regular DFM, I would suggest to do a proper analysis and comparison also for entropy in this figure, and not only for perplexity and accuracy. In fact, in line 417 in Sec. 5.2, the authors write *\"and entropy converges to the\nsame range as many-step DFM\"* -- but there is no such plot in Figure 1 showing this convergence.\n\n**Cumulative Scalar:** While the authors do motivate the approach, the cumulative scalar approach in 4.2 seems ad-hoc and not principled. It is not clear why the exact scaling $\\bar{g}_{t,h}$ should be optimal. Ultimately, this seems to be a hyperparameter and maybe another, hand-crafted scaling function would work even better (it would also be nice to visualize the adjusted and the old scalar functions to make the reader better appreciate the differences)."}, "questions": {"value": "I have a few more questions:\n- Why is it necessary to train with a teacher that is the EMA of the student itself? Couldn't we simply use a pre-trained model as teacher and solve its Kolmogorov equation with the Runge-Kutta solver for different step sizes $h$, and then train a student, similar to standard distillation settings?\n- The paper uses Runge-Kutta solvers to perform the $h$ step, in a short-cut model fashion, but these solvers, for both RK2 and RK4, seem to only do a single step. Wouldn't it be more accurate, especially for large $h$, to solve the ODE more accurately and perform Runge-Kutta integration with a finer and more accurate discretization? This would mean more overhead, but only during training, not during inference.\n- FS-DFM uses the forward KL divergence to match teacher and student. Could we also use reverse KL divergence or Jensen-Shannon divergence? And if so, how would this affect performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XnoDCQMH41", "forum": "ue1zFeD275", "replyto": "ue1zFeD275", "signatures": ["ICLR.cc/2026/Conference/Submission1778/Reviewer_tDLv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1778/Reviewer_tDLv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163458806, "cdate": 1762163458806, "tmdate": 1762915886645, "mdate": 1762915886645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}