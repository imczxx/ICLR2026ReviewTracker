{"id": "cJAtzOcAnd", "number": 20697, "cdate": 1758309130352, "mdate": 1759896963460, "content": {"title": "Comparing the learning dynamics of in-context learning and fine-tuning in language models", "abstract": "Pretrained language models can acquire novel tasks either through in-context learning (ICL)---adapting behavior via activations without weight updates---or through supervised fine-tuning (SFT), where parameters are explicitly updated. Prior work has reported differences in their generalization performance and inductive biases, but the origins of these differences remain poorly understood. In this work, we treat ICL and SFT as distinct learning algorithms and directly compare the learning dynamics they induce across medium-sized models, analyzing both the evolution of their inductive biases and the underlying internal representations. We find that ICL preserves rich input representations but imposes stronger priors inherited from pretraining, whereas SFT suppresses task-irrelevant features---potentially explaining its weaker generalization in few-shot regimes. These results highlight a mechanistic distinction between context-driven and weight-driven learning.", "tldr": "", "keywords": ["in-context learning", "supervised fine-tuning", "inductive biases", "learning dynamics"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f196c3a207474927156bab36cb166f1d355e9f7f.pdf", "supplementary_material": "/attachment/c1a566fd50db3797738786594eac27373be3f80f.zip"}, "replies": [{"content": {"summary": {"value": "The authors apply both few-shot prompting (\"ICL\") and supervised fine-tuning (\"SFT\") to a toy 2D linear classification task, analyzing how performance and representations change as more examples are added."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Demonstrates that while final performance is similar between ICL and SFT, (a) ICL has a prior towards 45 degrees while SFT has a prior towards 90 degrees, (b) ICL is sensitive to periodic orderings of example labels, and (c) SFT representations are more tightly coupled to the label space.\n* Finds support for (a) across Llama3-8B, Qwen3-8B, Gemma3-12B, and Gemma3-27B"}, "weaknesses": {"value": "* Claims in the text are often not clearly linked to the corresponding portions of the dense and complex figures. For example, the claim \"Both predictions were verified when comparing model performance across seeds for ICL\" references \"Fig.2A\" when I believe the only relevant part of Figure 2A is column 4, the claim \"we observed an overestimation (resp. underestimation) of the inferred task angle for θ = 30◦ (resp. θ = 60◦)\"  references \"Fig.2A\" when I believe the only relevant part of Figure 2A is column 3, etc. Minimally, every subfigure within the current figures needs to be labeled somehow (not just the rows), and the appropriate subfigure needs to be referenced by label wherever it is discussed. More broadly, every claim should be coupled with text that explains how to read that claim off of the corresponding figure.\n* The focus is on a single artificial task, and how these findings might generalize to other problems is not well discussed. For example, I don't know what prediction the identified bias towards a 45 degree angle would make for a real-world task like question answering.\n* Most analyses are done on Llama3-8B, so we can't tell, for example, whether findings (b) and (c) listed in the Strengths generalize to other LLMs. (The appendix shows that GPT-OSS:20B can't even learn the task, so generalization of the findings across models is an important concern.)"}, "questions": {"value": "* I don't understand how to read the \"previously seen feature bias\" off the figures. Can you explain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tZIiv59k5y", "forum": "cJAtzOcAnd", "replyto": "cJAtzOcAnd", "signatures": ["ICLR.cc/2026/Conference/Submission20697/Reviewer_H2hK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20697/Reviewer_H2hK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761600688130, "cdate": 1761600688130, "tmdate": 1762934075826, "mdate": 1762934075826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper directly contrasts in‑context learning (ICL) and supervised fine‑tuning (SFT) as learning algorithms on a tightly controlled 2‑D linear classification family. Using matched data, shot counts, and example orderings, the authors track accuracy, \"smoothness,\" confidence, inferred boundary angle, and layer‑wise representational similarity (RSA). Both ICL and SFT reach similar held‑out accuracy, but with different inductive biases and internal representations: ICL preserves richer input structure yet shows stronger from-pretraining priors (e.g., diagonal \"number‑comparison\" bias and row/column reuse), whereas SFT aligns representations along label axes, yielding higher confidence but apparent representational collapse. Ordering effects reveal short‑horizon pattern‑following in ICL. Results qualitatively persist across several model families and in a semantic (adjective‑ordered) variant, though learning there is slower."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The matched‑trajectory setup isolates algorithmic differences between ICL and SFT more cleanly than typical open‑domain benchmarks\n\n* I think the comparison and framing of SFT and ICL as two distinct learning algorithms is a useful and interesting framing"}, "weaknesses": {"value": "* I think the main limitation, as mentioned by the authors, is scope: Add one or two richer tasks (non‑linear boundaries or 3‑class variants; a small real‑text task with controlled geometry) to test whether the same ICL/SFT differences recur.\n\n* RSA uses last‑query‑token activations; alternative readouts (earlier tokens, attention heads, probing classifiers) could nuance the \"collapse vs. preservation\" story. Causality is not established\n\n* Fig. 3 compellingly shows SFT’s label‑aligned compression vs. ICL’s input‑structured geometry, even when accuracies match. I thought this was cool"}, "questions": {"value": "Does representational collapse happen with LoRA or other PEFT approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TmmuOxvD75", "forum": "cJAtzOcAnd", "replyto": "cJAtzOcAnd", "signatures": ["ICLR.cc/2026/Conference/Submission20697/Reviewer_sBMX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20697/Reviewer_sBMX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971693229, "cdate": 1761971693229, "tmdate": 1762934075349, "mdate": 1762934075349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work empirically compares the learning dynamics of in-context learning and finetuning on mid-sized language models on two synthetic tasks: a 2-D numerical classification task and a 2-D semantic classification task. This work demonstrates that, while both algorithms can learn the tasks, in-context learning is more subject to biases present in natural language training data (e.g., biases towards comparing numbers, rather than learning an arbitrary classification boundary). Furthermore, the authors demonstrate that the two learning algorithms yield radically different internal representations, with finetuning collapsing representations into two classes and ICL maintaining a greater amount of structure in representations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This work empirically addresses a broad, pressing question in the field: what is the empirical relationship between in context learning and finetuning? \n\nThe work appears empirically sound, the experiments are thorough, and the breadth of models tested (within the mid-sized scale) is fairly large.\n\nThe results regarding inductive biases of ICL are sensible, yet interesting. These results bolster broader arguments that ICL is selecting from a pool of functions learned in pretraining. The analyses of inductive bias in ICL add nuance to this discussion, indicating that there are systematic differences in the prior probability assigned to functions."}, "weaknesses": {"value": "Why not study small models as well? Even if studying very large models is computationally challenging, one might be able to discover a scaling trend in ICL vs finetuning learning dynamics by systematically studying models on the order of 1B to 27B. This would increase the impact of this work.\n\nThe RSA of the finetuned models should be further fleshed out. One guess is that the task is so trivial that the model simply converges on the correct answer early on in its computation (i.e., at an early layer). Perhaps early on in finetuning, the model’s representations look more like what you find for ICL, or perhaps there is a one-to-many comparison to be made between the early layers of the finetuned model and all of the layers in the ICL model. This setup seems especially well suited for a finding like “finetuning results in a compressed version of the computation that ICL converges on”. Studying the dynamics of what is called “representation collapse” in the text would be a very valuable contribution."}, "questions": {"value": "Why not study smaller models as well?\n\nDid you try using other nonsense labels? Or even labels with pretrained semantics? Perhaps there would be interesting divergences between ICL and finetuning that hinge on the label."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oU600VQeAj", "forum": "cJAtzOcAnd", "replyto": "cJAtzOcAnd", "signatures": ["ICLR.cc/2026/Conference/Submission20697/Reviewer_fv6R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20697/Reviewer_fv6R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014553432, "cdate": 1762014553432, "tmdate": 1762934074760, "mdate": 1762934074760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the different inductive biases and learning dynamics of in-context learning (ICL) and supervised fine-tuning (SFT) in medium-sized language models. Using a controlled 2D linear classification task, the authors directly compare ICL and SFT across matched learning trajectories (i.e., same data and example ordering), analyzing both generalization patterns and internal representations. The authors find that while both methods achieve similar final accuracy, their strategies and inductive biases differ significantly. ICL is shown to exhibit strong inductive biases inherited from pretraining, such as a \"comparison bias\" (favoring diagonal decision boundaries, $\\theta \\approx 45^\\circ$) and a \"previously-seen feature value bias\" (favoring axis-aligned boundaries). In contrast, SFT is shown to suppress task-irrelevant features, leading to internal states clustering primarily by task label. Conversely, ICL preserves more varied input-specific representations throughout its layers. These core findings are also shown to generalize to an analogous semantic classification task."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. *Clear Presentation:* The paper does an excellent job of clearly presenting its results. The use of a minimal, controlled 2D linear classification task allows for a precise and direct comparison of the learning dynamics, generalization patterns, and inductive biases of ICL and SFT.\n2. *Representational Analysis:* The representational similarity analysis (RSA) provides a clear distinction between the two learning regimes. It visually and quantitatively demonstrates SFT's representation collapse versus ICL's preservation of input structure (Fig. 3), adding a strong layer of evidence beyond simple task performance metrics.\n3. *Strong Grounding in Literature:* The findings are well-situated within the broader literature, particularly in relation to Bayesian accounts of ICL, and contribute compelling evidence to the ongoing discussion challenging \"ICL as gradient descent\" mechanisms in larger models."}, "weaknesses": {"value": "1. *Limited Scope of SFT Experiments:* The paper's central claims about SFT rely on fine-tuning experiments conducted primarily on a single model (Llama-3-8B), which the authors acknowledge as a limitation. While the ICL results are replicated across several models, the core ICL vs. SFT comparison would be greatly strengthened by a more comprehensive evaluation of the SFT condition (e.g., across more models, training hyperparameters, or regularizers) to ensure the \"representation collapse\" is a general feature of SFT and not an artifact of a specific setup.\n2. *Novelty in Context of Prior Work:* While the direct comparison between ICL and SFT is valuable, many of the core results (e.g., ICL can be sensitive to pretraining priors, SFT can be brittle, etc) have been documented in related work. The paper could do a clearer job of articulating the specific, novel contribution of its findings beyond these effects. For instance, is the key novelty the direct demonstration of how the internal representations diverge under identical data, or the specific characterization of the \"comparison\" vs. \"axis-aligned\" biases?"}, "questions": {"value": "1. *Influence of SFT Hyperparameters*: The authors note that SFT hyperparameters were not exhaustively probed (Limitations, p. 9). Given that SFT is known to be sensitive to hyperparameter choices, how confident are the authors that the observed \"representation collapse\" is an inherent feature of SFT on this task, rather than an artifact of a specific hyperparameter regime (e.g., potential over-fitting)?\n2. *Clarifying Novel Contribution*: The paper's discussion notes that ICL's sensitivity on pretraining priors and SFT's brittleness have been noted in prior work. Is the authors primary contribution the direct, matched-data demonstration of how ICL and SFT strategies diverge representationally, or the specific characterization of the competing inductive biases (e.g., \"comparison bias\" vs. \"feature value bias\")? A clearer framing of the novelty would help situate the work's impact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MxXsJTBqdC", "forum": "cJAtzOcAnd", "replyto": "cJAtzOcAnd", "signatures": ["ICLR.cc/2026/Conference/Submission20697/Reviewer_b4Qy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20697/Reviewer_b4Qy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148521196, "cdate": 1762148521196, "tmdate": 1762934074113, "mdate": 1762934074113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}