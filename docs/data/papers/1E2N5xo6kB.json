{"id": "1E2N5xo6kB", "number": 18405, "cdate": 1758287365831, "mdate": 1759897105405, "content": {"title": "Unlocking Long-Term Dependencies in Spiking Neural Networks with a Recurrent LIF Memory Module", "abstract": "Processing long sequence data such as speech requires models to maintain long-term dependencies, which is challenging for recurrent spiking neural networks (RSNNs) due to high temporal dynamics in neuron models that leaks stored information in their membrane potentials, and faces vanishing gradients during back-propagation through time. These issues can be mitigated by employing more complex neuron designs, such as ALIF and TC-LIF, but these neuron-level solutions often incur high computational costs and complicate hardware implementation, undermining the efficiency advantages of SNNs. Here we propose a network-level solution that leverages the dynamical interactions of a few  LIF neurons to enhance long-term information storage. The memory capability of this LIF-based micro-circuits is adaptively modulated by global recurrent connections of the RSNN, contributing to selective enhancement of temporal information retention, and ensures stable gradient gain when propagation through time. The proposed model outperforms previous methods including LSTM, ALIF, and TC-LIF in long sequence tasks, achieving 96.52\\% accuracy on the PS-MNIST dataset. Furthermore, our method also provides a compelling efficiency advantage, yielding up to 400× improvement compared to conventional models such as LSTM. This work paves the way for building cost-effective, hardware-friendly, and interpretable spiking neural networks for long sequence modeling.", "tldr": "", "keywords": ["Spiking Neural Networks", "Long-Term Dependencies", "Recurrent Dynamics", "Neuromorphic Computing", "Sequence Modeling"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07fa32444de4ef1eed58d884fbb87bc781041f31.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes LRMM (Local Recurrent Memory Module), a novel architecture for spiking neural networks (SNNs) that enhances long-term dependency modeling without modifying the LIF neuron model itself. Instead of introducing adaptive thresholds or multiple compartments, the authors design a network-level microcircuit of four interacting LIF neurons (NI, NM, NC, NO), forming a local recurrent memory loop. They show that this loop structure improves gradient retention in backpropagation through time (BPTT), mitigating vanishing gradients and enhancing memory duration. Extensive experiments on PS-MNIST, SHD, SSC, and Binary Adding demonstrate state-of-the-art performance compared to LIF, ALIF, TC-LIF, and even LSTM, while preserving neuromorphic efficiency (up to 400× less energy)."}, "soundness": {"value": 4}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of improving long-term memory through network-level recurrence rather than neuron-level complexity is orthogonal to most prior works (ALIF, TC-LIF, RadLIF). The proposed LRMM microcircuit is interpretable, biologically plausible, and hardware-friendly.\n\n2. Section 3.2 provides a rigorous derivation of the effective temporal gain in the NM–NC loop, offering a clear explanation for how gradient vanishing is mitigated (Eq. 18–21).This level of mathematical grounding is rare in SNN papers and adds significant credibility.\n\n3. The method outperforms on all four standard benchmarks with fewer parameters (Table 1). Particularly, LRMM achieves 96.52% on PS-MNIST and 94.7% on SHD, competitive with or better than LSTM while maintaining 400× efficiency.\n\n4. The authors systematically ablate the memory loop, gating separation, and recurrent feedback (Table 2), and visualize gate dynamics and gradient flow (Figs. 4–6), demonstrating both interpretability and robustness.\n\n5. The detailed energy accounting (Section 4.4, Appendix A.7)"}, "weaknesses": {"value": "1. Some baseline results (e.g., ALIF, RadLIF) are taken from different sources rather than reimplemented under the same training regime.\nWhile Appendix A.5 claims consistent settings, explicit mention of normalization and surrogate choices would strengthen fairness.\n\n2. Figures 1–2 are dense; neuron connections could be better annotated to show causal order. Minor grammatical and typographical errors persist (e.g., “ensures and stable gradient flow”).\n\n3. The mathematical formulas in the paper are not well formatted and are sometimes difficult to read intuitively. For instance, the use of italic vs. upright subscripts and inconsistent vector/matrix boldface could be improved according to the ICLR style guidelines. In particular, the subscripts in Equations (6–9) could be simplified to enhance readability.\n\n4. The ablation study focuses on removing entire loops or gates, but does not analyze parameter sensitivity (e.g., effect of leak constant τ, weight magnitude w_NM,NC). This limits understanding of stability boundaries.\n\n5. What would happen if the model were trained without the modulation function? This part of the method should also be included in the ablation study.\n\n6. Since the paper claims to address the long-term dependency problem, it would be clearer to include the sequence length T of each dataset in Table 1. This would make the improvements on long-sequence tasks more intuitive.\n\n7. The typesetting and layout could be polished to improve readability. Currently, the dense text and inconsistent formatting make it less smooth to read.\n\n8. In Figure 5, the font size of the text at the bottom is too small to read comfortably. Please enlarge it for better visibility.\n\n9. In Equations (6), (7), and (9), the variable $I$ is not binary, meaning that floating-point multiplications are still required. In the energy analysis, it would be helpful to provide a table showing the theoretical energy consumption for each operation (or each equation component) to allow clearer comparison.\n\n10. Please report how much GPU memory and training time each task requires compared to a standard LIF-based RSNN baseline. This would help evaluate the actual computational overhead of LRMM.\n\n11. You may consider adding pseudocode for the LRMM algorithm in the paper to make the implementation process clearer and easier to follow."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VMMVfc7FYh", "forum": "1E2N5xo6kB", "replyto": "1E2N5xo6kB", "signatures": ["ICLR.cc/2026/Conference/Submission18405/Reviewer_ncM5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18405/Reviewer_ncM5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828250557, "cdate": 1761828250557, "tmdate": 1762928110042, "mdate": 1762928110042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel recurrent spiking neural network that performs well on several long-sequence tasks. It appears to be very efficient in parameter use and can use LIF and more advanced neuronal models. The paper includes a detailed analysis of the stability when training with BPTT.  The paper does much better than several reported competitors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The description of the algorithm, stability analysis, and experimentation appear quite strong."}, "weaknesses": {"value": "There are a couple of points in the presentation that are unclear. \n\n1) The paper states \"However, the gating mechanism is not natively supported by most neuromorphic processors, hindering their efficient implementation in SNNs.\"\n\nI think it would be useful to further explain what specific aspect of the gating mechanism is not supported by hardware accelerators like Loihi 2.\n\n2)  Alternatively,RSNNs suitable for long-sequence tasks may be obtained through complex network structure designs [ref], \n\n\"ref\" is undefined.\n\n3) The paper misses what I think is a key reference and competitor to this work.\n\n   Z. Liu et al. \"LMUFORMER: LOW COMPLEXITY YET POWERFUL SPIKING MODEL WITH LEGENDRE MEMORY UNITS\", ICLR 2024. See https://arxiv.org/pdf/2402.04882.\n\nThis paper appears to outperform your work on PS-MNIST and does also very well on related speech tasks. It may, however, have many more parameters and thus a detailed comparison of the pros and cons of these different approaches seems quite important."}, "questions": {"value": "1. How does this work compare to Z. Liu et al's, LMUFormer ICLR 2024 paper in terms of novelty and complexity. Both appear to propose recurrent spiking model architectures and both do well on long-range tasks yet the approaches seem somewhat different.  If this question gets properly addressed, I think this paper would be a nice additional contribution and I would like to re-adjust my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ig0ylG0NvF", "forum": "1E2N5xo6kB", "replyto": "1E2N5xo6kB", "signatures": ["ICLR.cc/2026/Conference/Submission18405/Reviewer_AM94"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18405/Reviewer_AM94"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968452229, "cdate": 1761968452229, "tmdate": 1762928109150, "mdate": 1762928109150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Recurrent LIF Memory Module (LRMM) as a network-level solution to enhance long-term dependencies in recurrent spiking neural networks (RSNNs) while preserving the efficiency of LIF neurons. Unlike prior approaches that modify individual neuron dynamics, LRMM use 4 LIF node to build a Network-Level Micro-circuit, and control them through a key local memory loop and three dynamically modulated currents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It demonstrates that we need not design more complex, computationally expensive neurons to achieve long-term memory. Instead, it shows that through delicate network topology design, the simplest components (vanilla LIF) can achieve equivalent or even superior performance.\n\n2. The paper clearly proves mathematically why its $N_M-N_C$ memory loop solves the vanishing gradient problem. This \"effective temporal gain\" is a clear and analyzable mechanism."}, "weaknesses": {"value": "1. Although using LIF is advantageous, the complex local connections (fully connected microcircuits of four neurons) and global connections ($N_O$ to feedback to the gated FC) in LRMM may introduce significant routing overhead on physical chips, whereas many neuromorphic hardware implementations favor sparse or simpler local connections.\n\n2. The Independent Contribution of Gates: The experiment combined the three gates ($I_I, I_F, I_O$) for ablation study, but this does not reveal which gate is most critical. A more refined ablation experiment should test the impact of removing (e.g, $I_F$ (forgetting current) or $I_O$ (output current) individually). This is crucial for understanding whether LRMM truly learned LSTM-like “forgetting” and “output” control mechanisms."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3IsULqLcFd", "forum": "1E2N5xo6kB", "replyto": "1E2N5xo6kB", "signatures": ["ICLR.cc/2026/Conference/Submission18405/Reviewer_syKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18405/Reviewer_syKC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985228499, "cdate": 1761985228499, "tmdate": 1762928108613, "mdate": 1762928108613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Local Recurrent Memory Module (LRMM) based on vanilla Leaky Integrate-and-Fire (LIF) neurons to address long-term dependency challenges in recurrent spiking neural networks (RSNNs), which enhances long-term information storage via dynamic interactions of LIF neurons and global recurrent connections."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed Local Recurrent Memory Module (LRMM) achieves both superior performance and high energy efficiency.\n\n2. LRMM mitigates the vanishing gradient problem in backpropagation through time (BPTT) via its memory loop design and maintains hardware compatibility by relying on vanilla LIF neurons instead of complex neuron-level modifications."}, "weaknesses": {"value": "1.  The model’s effectiveness is currently only validated on specific long-sequence benchmarks and some simple tasks,  and it lacks performance in larger-scale network structures on more complex tasks.\n\n2. The presentation of this paper needs improvement; first, the citation format is not user-friendly for reading, and second, there are many minor errors (line 60 ref,  the figures are not cited in the main text).\n\n3. I did not grasp the specific modifications that the authors proposed for the LRMM relative to Recurrent Spiking Neural Networks (RSNNs); my understanding is that a new architecture was proposed rather than modifications at the neuron level, yet the experiments primarily compare different types of neurons."}, "questions": {"value": "Table 3 only accounts for the power consumption of synapses, while the power consumption of neurons should also be included."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I4Qgrkd2tL", "forum": "1E2N5xo6kB", "replyto": "1E2N5xo6kB", "signatures": ["ICLR.cc/2026/Conference/Submission18405/Reviewer_YcJU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18405/Reviewer_YcJU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762096489119, "cdate": 1762096489119, "tmdate": 1762928107877, "mdate": 1762928107877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}