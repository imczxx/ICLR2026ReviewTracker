{"id": "I9ED9VWZq6", "number": 15612, "cdate": 1758253156487, "mdate": 1759897295512, "content": {"title": "SpeechJudge: Towards Human-Level Judgment for Speech Naturalness", "abstract": "Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce ***SpeechJudge***, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on naturalness—one of the most fundamental subjective metrics for speech synthesis. First, we present ***SpeechJudge-Data***, a large-scale human feedback corpus of 99k speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish ***SpeechJudge-Eval***, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the best-performing model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop ***SpeechJudge-GRM***, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.", "tldr": "We propose SpeechJudge, a suite centered on speech naturalness, which includes a human preference dataset, an evaluation benchmark, and a generative reward model.", "keywords": ["speech naturalness", "human dataset", "RLHF", "generative reward model", "AudioLLM"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eb8619b37d4543e61d38af981268a159d644d6a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SpeechJudge, a comprehensive framework for aligning speech generation models with human perceptual preferences. The suite includes:\nSpeechJudge-Data: a large-scale human preference dataset (99K speech pairs) annotated for intelligibility and naturalness;\nSpeechJudge-Eval: a benchmark for evaluating automatic speech naturalness judgment;\nSpeechJudge-GRM:a generative reward model trained on the dataset using SFT + RL (GRPO) to predict human preferences.\nExperiments show that existing MOS predictors and AudioLLMs (even Gemini-2.5-Flash) perform below 70% agreement with human judgments. The proposed GRM reaches 77.2% (79.4% with inference-time scaling), outperforming a Bradley-Terry reward baseline (72.7%). The model also improves the naturalness of post-trained TTS systems when used as a reward function."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Targeted an important open problem: aligning speech generation with perceptual human judgment.\nProvides a relatively large corpus (99K pairs) of human preference labels, which could be a useful resource for benchmarking.\nIntegrates SFT and RLHF (GRPO) techniques for audio reward modeling, which conceptually connects textual RLHF paradigms with audio generation.\nOffers comparisons across a broad set of AudioLLMs (Gemini, Qwen, Kimi, GPT-4o), giving some insight into current model limitations."}, "weaknesses": {"value": "1. Ill-defined concept of “naturalness”\nThe central concept of speech naturalness is not operationally defined or quantitatively measured in an absolute or reproducible way. Annotators are instructed to make pairwise comparisons (“which one sounds more natural”), but the paper does not clarify whether they were asked to assess naturalness along specific perceptual dimensions such as prosody, timing, articulation, or spectral quality. As a result, the collected data are ordinal rather than metric, encoding only relative preference (i.e., which sample is less unnatural). This design becomes problematic in cases where both audios are of low quality—for instance, in the example on their project website (“case 1”), both clips mispronounce the word 24, leaving no meaningful basis for determining which is superior. Furthermore, the paper provides no evidence that annotators followed consistent perceptual criteria or were calibrated to established standards such as MOS or CMOS protocols.\n\n2. Dataset built entirely from synthetic TTS outputs. The authors explicitly state that all 99K samples come from synthetic TTS models (CosyVoice2, F5-TTS, MaskGCT, etc.) using synthetic prompts. This raises several issues: No human speech ground truth. The corpus lacks human-recorded references, so the model never learns what “human-like” actually sounds like. Model bias inheritance. Because all training data originate from a small set of TTS architectures, the reward model is likely to internalize architectural artifacts (e.g., vocoder noise, prosody style) rather than perceptual naturalness. Circular evaluation. The benchmark and reward model are both trained and tested on samples generated by the same model family, leading to evaluation leakage and poor generalization\n\n3. Limited analysis of annotation reliability and bias.\nWhile the paper discusses agreement levels (FA/WA/WD/FD), more detail is needed on labeler consistency, potential cultural bias in “naturalness,” and language-specific reliability. For a human-feedback dataset, these issues are central.\n\n4. Overreliance on closed models (Gemini-2.5-Flash).\nThe teacher model for generating CoT rationales is proprietary. The dependence on a non-reproducible component undermines the claim of full openness and may limit reproducibility of the SFT stage.\n\n5. Statistical analysis of results is thin.\nAccuracy gains (e.g., +4.5%) are reported without confidence intervals or significance tests. Given the subjective nature of the task, statistical reliability is important."}, "questions": {"value": "1. How were the expressive prompts and multilingual scripts quality-controlled?\n2. Did the authors analyze cases where the GRM and humans disagree—what aspects of speech cause errors?\n3. How generalizable is SpeechJudge-GRM to unseen voices (real human speech) or out-of-distribution languages (e.g., non-English/Chinese)?\n4. Could CoT reasoning introduce bias by inheriting the teacher model’s style of explanation?\n5. Did the authors analyze cases where both audios were judged “unnatural”? \n6. How do you ensure that the reward model is not overfitting to specific TTS model artifacts rather than perceptual cues?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The work involves large-scale human evaluation"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vpk4bvqXiM", "forum": "I9ED9VWZq6", "replyto": "I9ED9VWZq6", "signatures": ["ICLR.cc/2026/Conference/Submission15612/Reviewer_KixZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15612/Reviewer_KixZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760906695611, "cdate": 1760906695611, "tmdate": 1762925882392, "mdate": 1762925882392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a corpus containing paired TTS samples with human preferences, a benchmark built upon this corpus for evaluating speech naturalness, and a reward model trained on it."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The collected large-scale human preference dataset can serve as a valuable resource for research on the automatic assessment of synthesized speech quality. The paper verifies its effectiveness both as a benchmark and as training data for building a model to automatically evaluate speech naturalness."}, "weaknesses": {"value": "1. The corpus includes both regular and expressive samples, but the evaluation focuses solely on naturalness. Would it be possible to also consider expressiveness as an evaluation dimension, given that it is explicitly represented in the data?\n2. It is not clear why “tie” annotations are excluded from both the evaluation subset and the GRM training data. It might also be valuable for the model to recognize when two samples are of similar naturalness (i.e., no perceptible difference), as this could provide a more nuanced understanding of perceptual similarity.\n3. Gemini-2.5-Flash is used as the teacher model to generate CoT data for the first SFT stage. However, as shown in Table 2, its performance, while the best among compared models, is still suboptimal. Could the authors provide more insight into the quality of the CoT generated by Gemini 2.5? Was any further inspection or manual verification conducted to ensure its reliability?\n4. The paper mentions that during the RL stage, only the naturalness judgment is constrained, while the model is allowed to “autonomously optimize its reasoning and rationale generation capabilities.” I am curious about how is the model’s reasoning ability. Is there any quantitative measurement, qualitative analysis, or other inspection into the model outputs?\n5. The dataset already contains paired preference data, which, as acknowledged in the paper (Line 61), is well suited for DPO-style alignment. It might therefore be useful to include a DPO-based baseline to compare against, in order to assess how effectively the model can model human preferences.\n6. It might be informative to evaluate whether a model trained with SpeechJudge-GRM as the reward model can also predict absolute MOS scores. Since absolute MOS scores are generally more practical and efficient for automatic assessment and benchmarking of TTS quality than pairwise comparisons, such an analysis could further demonstrate the utility of the proposed data and model."}, "questions": {"value": "1. It is interesting that even with paired comparisons—rather than direct scoring of individual samples—human judgments still exhibit a notable degree of disagreement. Could the authors provide any insights or analysis on the sources of this variability?\n2. It might be preferable to have native English speakers assess the naturalness of the English speech samples, especially given the high quality of recent TTS systems, where subtle perceptual nuances may be more easily detected by native listeners.\n3. Considering the observed inconsistency among human annotations, releasing the individual ratings (in addition to the majority-vote labels) could be valuable for future research, enabling studies on annotator variability and modelling of subjective perceptual uncertainty."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nhNJOdmlHH", "forum": "I9ED9VWZq6", "replyto": "I9ED9VWZq6", "signatures": ["ICLR.cc/2026/Conference/Submission15612/Reviewer_GQeA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15612/Reviewer_GQeA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761543256494, "cdate": 1761543256494, "tmdate": 1762925881969, "mdate": 1762925881969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission introduces SpeechJudge, a comprehensive suite addressing the critical gap in aligning speech synthesis models with human perceptions of naturalness. It comprises three core components: SpeechJudge-Data, a large-scale human feedback corpus with 99K speech pairs annotated for intelligibility and naturalness across diverse languages and styles; SpeechJudge-Eval, a benchmark for naturalness judgment that reveals existing metrics and AudioLLMs (e.g., Gemini-2.5-Flash) achieve less than 70% agreement with human judgments; and SpeechJudge-GRM, a generative reward model trained via a two-stage SFT+RL process on Qwen2.5-Omni-7B, which attains 77.2% accuracy (79.4% with inference-time scaling) and outperforms classic Bradley-Terry reward models. The work also demonstrates SpeechJudge-GRM’s utility in high-quality sample selection and TTS model post-training, with plans to release all resources to advance human-aligned speech synthesis research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel and Comprehensive Resource Creation: The construction of SpeechJudge-Data—with diverse TTS models, multilingual support, and dual annotations (intelligibility/naturalness)—fills a key void in large-scale naturalness-focused human feedback corpora for speech synthesis.\nRigorous Benchmark Design: SpeechJudge-Eval provides a standardized, high-quality evaluation framework that exposes limitations of existing metrics and AudioLLMs, offering clear direction for future improvements.\nEffective Reward Model Development: The two-stage SFT (with CoT rationales) and RL (on challenging cases) training for SpeechJudge-GRM is well-motivated, and its superior performance over baseline models, along with explainability and scaling capabilities, enhances its practical value.\nDemonstrated Real-World Utility: The experiments on sample selection and TTS post-training validate the framework’s applicability, showing tangible improvements in naturalness while preserving other key attributes like speaker similarity."}, "weaknesses": {"value": "Limited Analysis of Cross-Lingual and Expressive Speech Performance: While the dataset includes cross-lingual and expressive samples, the paper lacks in-depth analysis of how SpeechJudge-GRM performs across these specific subsets, leaving uncertainty about its generalizability to diverse linguistic and stylistic scenarios.\nInsufficient Comparison with State-of-the-Art AudioLLM Judges: The evaluation of existing AudioLLMs focuses primarily on zero-shot performance with basic prompts; a more thorough comparison with fine-tuned or prompt-optimized AudioLLM judges (e.g., AudioJudge) would better contextualize SpeechJudge-GRM’s advancements.\nLack of Discussion on Annotation Bias: Although inter-annotator agreement is analyzed, there is no exploration of potential biases (e.g., linguistic, cultural) in human annotations, which could impact the reliability of the dataset and the model’s alignment with diverse human preferences."}, "questions": {"value": "How does SpeechJudge-GRM’s performance vary across specific cross-lingual settings (e.g., zh2en vs. en2zh) and expressive speech styles (e.g., emotional vs. accented), and what factors contribute to any observed differences?\nGiven that some open-source AudioLLMs show performance degradation with CoT prompts, did the authors explore alternative prompt engineering strategies (e.g., few-shot examples) for the SFT stage, and if so, what were the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rWlB52FevX", "forum": "I9ED9VWZq6", "replyto": "I9ED9VWZq6", "signatures": ["ICLR.cc/2026/Conference/Submission15612/Reviewer_dToi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15612/Reviewer_dToi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930333314, "cdate": 1761930333314, "tmdate": 1762925881456, "mdate": 1762925881456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work provides a human-annotated dataset for training automatic evaluators of TTS quality, and evaluates current automatic metrics against this dataset.  The paper also describes a new evaluator that is trained using this data by fine tuning with a standard evaluator followed by reinforcement learning on the dataset.  This new evaluator does improve other automatic metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Provides the community with a much needed resource for building automatic TTS evaluators that can be incorporated into the TTS training.\n2. The work is comprehensive in that it shows an entire pipeline with data collection, evaluation of existing models, building a new evaluation model, and then training a new TTS system on it.  The latter stages lend credibility to the primary resource - the dataset."}, "weaknesses": {"value": "1. I have some concerns about the ability of any annotator to assess fine-grained naturalness in a second language (L2) - this ability will vary widely across annotators.  This is a significant challenge for anyone to collect this kind of data, but it does make me wonder more about  evaluation other the Mandarin, English, and code switched sets all being together, since we would expect increased differential agreement in L2 settings.    The paper would be stronger if it addressed this directly in the analysis.\n2. Similarly, code-switched speech is much more difficult for systems to generate (see, for example, CS-FLEURS https://arxiv.org/abs/2509.14161 https://www.isca-archive.org/interspeech_2025/yan25c_interspeech.pdf where they had to reject a significant subsample.  I think this should be addressed directly in the paper.  I think the data are there (in that raters did label errors, so this could be reported for ZH/EN/Codeswitched).  This is mostly a minor point compared to the first point."}, "questions": {"value": "P3 153: What motivated the choice of those six models?  Are they representative enough that evaluators trained on this data will have validity across new models?\n\nFigure 1 is not readable; please make the window span the entire screen.\n\nP4 189  Can you add details on evaluator recruitment  thatare missing in the main section and left to the appendix? This is crucial for validating the dataset for use in other languages than Mandarin and are not really an afterthought - having confidence in the annotations is a main paper issue since the primary product here is a dataset.\n\nP5 256: What is the score of a most-likely-label predictor?  I am assuming that the distribution of labels, particularly after subselecting for only FA data, would not be uniform.\n\nP7 372: Please expand SFT acronym here.\n\nP9 451: Section 5.4 - I think you need to state a limitation: a true test of the effectiveness of the evaluator for training would be human evaluation.  The proxies used are fine, but having this really improve MOS would demonstrate the effectiveness of the technique.\n\nAppendix C2: Why not use standard interrater agreement metrics like Fleiss’ Kappa or Krippendorf’s Alpha?\n\nIn general, there are a lot of very simple equations that are not necessary, and can be eliminated to provide additional space for other things.  Math that is not used to make a point is not useful in the main section of the paper."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "It is not clear whether there is human subjects oversight going on here (e.g. IRB).  One sentence should clarify this."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hbiusPBV7k", "forum": "I9ED9VWZq6", "replyto": "I9ED9VWZq6", "signatures": ["ICLR.cc/2026/Conference/Submission15612/Reviewer_kfy3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15612/Reviewer_kfy3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762447716577, "cdate": 1762447716577, "tmdate": 1762925880194, "mdate": 1762925880194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}