{"id": "WhO6Km5Rku", "number": 10775, "cdate": 1758181644479, "mdate": 1763121886865, "content": {"title": "QubitCache: Quantum-Inspired Probabilistic Attention Preservation for KV-Cache Compression", "abstract": "Large language model inference suffers from quadratic KV cache memory growth that fundamentally limits long context applications. Existing compression methods achieve memory reduction through token eviction but irreversibly discard relational information essential for complex reasoning. We present QubitCache, the first framework recognizing that attention patterns between tokens constitute the primary information carrier in transformers, not tokens themselves. This insight motivates a paradigm shift from discrete token selection to continuous relational preservation through quantum-inspired encoding. QubitCache introduces a hybrid architecture where critical tokens remain in classical storage while attention patterns undergo amplitude encoding into quantum states, achieving logarithmic compression beyond classical information-theoretic limits. Unlike binary dcisions, our framework generates probabilistic attention distributions through quantum state measurements, maintaining contextual coherence via soft attention constraints. We prove QubitCache preserves rank $r$ attention structure with bounded reconstruction error, ensuring graceful degradation rather than catastrophic failure. Empirical evaluation demonstrates $7\\times$ memory reduction while maintaining 92-97\\% of baseline performance across five models and six benchmarks. Remarkably, QubitCache achieves this with only 15\\% token retention compared to 50\\% in existing SOTA methods, yet attains 15-25\\% higher F1 scores on multi-hop reasoning tasks.", "tldr": "QubitCache achieves 10× KV-cache compression by preserving attention relationships through quantum-inspired amplitude encoding instead of binary token selection.", "keywords": ["Quantum Machine Learning", "Quantum Circuit", "Large Language Model", "KV Compression", "Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/887ecca98a0f4ea6aa77df006d333141a01a65c2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a novel method that, instead of dropping tokens like prior KV cache eviction methods, encodes the attention patterns of the dropped tokens through quantum state encoding."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The method is evaluated across a wide range of models, including large-scale models up to 70B parameters. \n- Strong novelty. To my knowledge, this is the first approach that applies quantum-inspired compression to KV cache."}, "weaknesses": {"value": "**[W1]** The paper is difficult to follow for readers without prior knowledge of quantum state encoding. Please provide a more detailed background on quantum state encoding. What is the RY rotation?\n\n**[W2]** The paper includes some imprecise statements:\n- L13: “LLM inference suffers from quadratic KV cache memory growth” --> Quadratic with respect to what? The memory growth is linear in sequence length.\n- L116: Similarly, why is the memory proportional to $N^2$?"}, "questions": {"value": "**[Q1]** How were the configurations of the baseline KV cache compression methods chosen? Since their compression rate and performance heavily depend on such configurations, justifying these choices would help demonstrate that the empirical comparison is fair.\n\n**[Q2]** How does the prefill and decoding latency of this method compare with other compression methods? Additionally, providing a throughput analysis would give a more comprehensive view.\n\n**[Q3]** How would the method perform on reasoning models for long-generation tasks such as AIME or LiveCodeBench? Demonstrating this would highlight the method’s robustness under long-generation scenarios, which are not captured by the short and long-context tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6XvEAyrhc9", "forum": "WhO6Km5Rku", "replyto": "WhO6Km5Rku", "signatures": ["ICLR.cc/2026/Conference/Submission10775/Reviewer_8rQZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10775/Reviewer_8rQZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965933879, "cdate": 1761965933879, "tmdate": 1762921987843, "mdate": 1762921987843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "iQs2ctxCxp", "forum": "WhO6Km5Rku", "replyto": "WhO6Km5Rku", "signatures": ["ICLR.cc/2026/Conference/Submission10775/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10775/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763121576183, "cdate": 1763121576183, "tmdate": 1763121576183, "mdate": 1763121576183, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes QubitCache, a hybrid quantum-inspired KV-cache compression method that encodes attention patterns (rather than raw token vectors) using amplitude-style encodings and probabilistic reconstruction, while keeping a small set of critical tokens in classical memory. The authors argue this preserves relational structure across long contexts and report a 7× reduction in KV memory with 92–97% retention of baseline performance across several models and long-context benchmarks; they support this with ablations, reconstruction metrics, and qualitative generation comparisons using a 9-qubit segment encoding simulated with Qiskit on GPUs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper motivates encoding attention patterns instead of performing binary token eviction, and ties this idea to graph-theoretic and spectral arguments that are well described and supported by references and analysis. \n\n* Experiments cover multiple model families and sizes, several long-context benchmarks, ablation studies, reconstruction metrics, and qualitative samples, which together give a reasonably complete picture of where the method helps and where it does not. Reported gains on multi-hop reasoning and the memory numbers are impressive if reproducible."}, "weaknesses": {"value": "* Hardware and complexity claims feel optimistic relative to current quantum realities. The paper’s argument that 9-qubit, depth-15 circuits are directly compatible with NISQ devices and that noise can be leveraged as regularization requires stronger evidence; the experiments are classical simulations and do not demonstrate real-device results or latency/throughput comparisons that would matter in production. The extrapolation to hardware acceleration therefore remains speculative.\n\n* Important implementation details for baselines, shot counts, measurement variance, and statistical significance of reported differences are missing or relegated to the appendix. Given the fairly large claims (7× memory, 15–25% higher F1 on multi-hop), the paper should present variance estimates, run multiple random seeds or splits, and make code/configs available to ensure the results are robust and reproducible.\n\n* The approach trades memory for additional computation and potential latency, but the runtime and energy costs are not quantified. The paper gives asymptotic and GPU memory figures but does not provide wall-clock end-to-end inference latency, throughput under batching, or the cost of repeated re-encoding and measurement in streaming generation. These metrics are crucial for deployment decisions and could change whether the method is practically preferable to strong classical baselines."}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ocfwlZr69M", "forum": "WhO6Km5Rku", "replyto": "WhO6Km5Rku", "signatures": ["ICLR.cc/2026/Conference/Submission10775/Reviewer_kvp5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10775/Reviewer_kvp5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994235214, "cdate": 1761994235214, "tmdate": 1762921987394, "mdate": 1762921987394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes QubitCache, a method that retains 15% of tokens, comprising 10% of the most recent tokens, 5% of tokens with high attention scores, and 4 anchor tokens. For the remaining tokens, the model reconstructs the pruned value vectors by interpolating between neighboring value vectors and reconstructs attention scores using quantum amplitude encoding. The authors compare QubitCache with ScissorHand, H2O, StreamingLLM, and GEAR baselines on both short- and long-context understanding tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a novel perspective on KV cache compression by leveraging quantum-based encoding and reconstructing the value vectors through interpolation."}, "weaknesses": {"value": "- Concerns about experiments\n    - The paper is missing several state-of-the-art baselines, such as SnapKV [1] and PyramidKV [2].\n    - In Table 1, the compression ratio is not consistent across baselines and results are only reported for a single compression ratio for each method. It would be more informative to include a plot showing accuracy versus compression ratio (with the x-axis representing the compression ratio and the y-axis representing accuracy).\n    - Additionally, based on the memory compression ratios reported in Table 3, QubitCache retains at least 15% of the cache, implying a maximum possible compression ratio of approximately 100 / 15 = 6.67x. However, the table reports a ratio of 7.0x, which appears to be inconsistent.\n- Concerns about the method\n    - The proposed method reconstructs value vectors at inference time by interpolating neighboring value vectors and extracting attention patterns. However, the paper does not provide any discussion or measurement of the inference-time latency or acceleration ratio, which is essential for understanding the practical efficiency of the approach.\n    - Equation (6) seems to assume that value vectors are linearly aligned, which is unlikely to hold in practice. It would be valuable to verify this assumption through statistical analysis, as it may not be generally true.\n    - Furthermore, it would be interesting to explore simpler alternatives that do not rely on quantum-based amplitude encoding for reconstructing attention pattern. For instance, one could reconstruct value vectors using an averaged representation (e.g., $\\bar{V} = \\sum_{j \\in I_C} \\tilde{V}_j$) with a weighting factor ($1 - \\lambda$), or apply this in a fine-grained, group-wise manner (e.g., over every (g) tokens). Such comparisons would help clarify whether the quantum-inspired encoding provides a meaningful advantage over simpler interpolation strategies.\n- Minor\n    - The first sentence of the abstract is incorrect. The KV cache memory grows linearly with the sequence length, whereas the total FLOPs computed phase grow quadratically.\n\n[1] Li, Yuhong, et al. \"Snapkv: Llm knows what you are looking for before generation.\" NeurIPS 2024.\n\n[2] Cai, Zefan, et al. \"Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling.\", ACL 2024 Findings."}, "questions": {"value": "See the above weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not have ethics concern regarding this paper."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vNa9Ng2v5p", "forum": "WhO6Km5Rku", "replyto": "WhO6Km5Rku", "signatures": ["ICLR.cc/2026/Conference/Submission10775/Reviewer_wZek"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10775/Reviewer_wZek"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997172244, "cdate": 1761997172244, "tmdate": 1762921987026, "mdate": 1762921987026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a KV cache compression scheme inspired by quantum encoding. A handful of tokens (sink, recent, attention-critical within middle context) are preserved while the remainder are retained via amplitude encoding. The attention patterns of non-preserved tokens are probabilistically restored. This scheme ultimately enables logarithmic compression based on the number of circuit qubits required for sufficient accuracy. This approach enables 7x memory reduction while enabling high performance on a variety of long-context and short-context tasks against baselines such as H2O, Scissorhands, and GEAR."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- A fresh, quantum-inspired approach to cache compression. Offloading approaches such as MagicPIG probabilistically restore attention distributions as well, but require full-cache maintenance on the CPU, unlike this work. \n\n- The method is empirically verified to perform well on several challenging LongBench tasks and other short-context tasks against popular strategies such as H2O and Scissorhands. \n\n- The approach seems to generalize across model families."}, "weaknesses": {"value": "- Figure 2 will not be understood by the lay ML reader. More effort needs to be explaining these circuits a general ML audience. \n\n- The theoretical analysis is oversold -- the abstract and introduction implies there is some rigorous guarantee or theorem, but there is no such exposition in the primary manuscript.  \n\n- While the viability of logarithmic compression is empirically observed, it would useful to see when this approach breaks (i.e., at what number of qubits for encoding versus segment length does this approach break). \n\n- How were the LongBench tasks chosen? I'm curious as to how this approach performs on a challenging task such as LCC (long code completion?). \n\n- It is relatively well-known that existing long context benchmarks such as RULER and LongBench are easy to solve since critical tokens are sparsely distributed (i.e., lots of gibberish/noise within the context) , as opposed to benchmarks such as GSM-Infinite which contain less noisy context."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "I1SU0CthE2", "forum": "WhO6Km5Rku", "replyto": "WhO6Km5Rku", "signatures": ["ICLR.cc/2026/Conference/Submission10775/Reviewer_CkeY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10775/Reviewer_CkeY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762238375026, "cdate": 1762238375026, "tmdate": 1762921986163, "mdate": 1762921986163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}