{"id": "KJvHnl3kUv", "number": 16201, "cdate": 1758261515394, "mdate": 1763701927647, "content": {"title": "Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning", "abstract": "We aim to improve the reasoning capabilities of language models via reinforcement learning with verifiable rewards (RLVR). Recent RLVR post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RLVR alone to improve reasoning on inherently difficult tasks is less effective due to sparse rewards. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting. Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework. We derive finite-sample complexity bounds and show that when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning. Experiments across diverse datasets and models demonstrate that E2H Reasoner substantially enhances LLM reasoning.", "tldr": "", "keywords": ["LLM", "Reinforcement Learning", "Post Training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d54f5d9f6dc69dceb2ca1f8bb2b7434eb5b6ac21.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the E2H Reasoner, a reinforcement learning (RL) method for large language models (LLMs) inspired by curriculum learning. The method schedules tasks from easy to hard during training, aiming to accelerate LLM learning. Theoretical analysis shows that curriculum-based reinforcement learning (CRL) requires fewer total samples than directly training on the final task, and experimental results are generally positive."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides theoretical justification for why CRL can achieve sample efficiency, requiring fewer total samples than direct learning on the final task.\n2. The experimental results are sound and well-presented."}, "weaknesses": {"value": "1. The idea of using curriculum learning to improve RL efficiency is not novel. The paper acknowledged prior work—e.g., Chen et al., Foster et al., Bae et al., Zeng et al. which used curriculum learning ideas. The paper should also cite Yu et al. (DAPO: An Open-Source LLM Reinforcement Learning System at Scale). \n2. In the experimental results, E2H does not consistently outperform baselines such as GRPO or Self-Evolve. \n3. The paper does not clearly articulate the advantages of E2H over adaptive filtering methods such as DAPO or Self-Evolve. In fact, adaptive filtering—where pass rate determines sample filtering—has several appealing properties: \n1) Model-dependent difficulty: “Easy” and “hard” samples are relative to the specific model; what is easy for one model may be hard for another. Thus, classifying samples by difficulty a priori can be problematic. \n2) Lack of synchronization: Without adaptive scheduling, the scheduler and the model may become misaligned—for instance, the scheduler may advance to harder tasks before the model is ready."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "He7TE9O7K5", "forum": "KJvHnl3kUv", "replyto": "KJvHnl3kUv", "signatures": ["ICLR.cc/2026/Conference/Submission16201/Reviewer_rLJH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16201/Reviewer_rLJH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761439480921, "cdate": 1761439480921, "tmdate": 1762926362089, "mdate": 1762926362089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes using a curriculum learning approach where they schedule tasks from Easy to Hard during RLVR which shows better performance at the end of training. They provide convergence guarantees for this algorithm in an approximate policy iteration framework and derive finite-sample complexity bounds which show the this is more sample efficient than training without any curriculum."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes a simple method of using curriculum learning. The curriculum implicitly assumes some grouping of tasks, but they also  show that the grouping is not necessary because tasks can be clustered just using pass rates of the initial model. They also compare with different baselines and the empirical results seem sound."}, "weaknesses": {"value": "The only weakness that comes to mind is not comparing with DAPO [1] which also has an implicit curriculum because the model keeps filtering prompts that are either too easy or too hard. Could the authors compare with DAPO as well and show results on the benchmarks? \n\nAlso the paper doesn't cite Paprika [2] which also proposes a curriculum when tasks can be grouped. \n\n\n[1] DAPO: An Open-Source LLM Reinforcement Learning System at Scale (https://arxiv.org/abs/2503.14476)\n\n[2] Training a Generally Curious Agent (https://arxiv.org/abs/2502.17543)"}, "questions": {"value": "Please look at the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XNIGqhdZcn", "forum": "KJvHnl3kUv", "replyto": "KJvHnl3kUv", "signatures": ["ICLR.cc/2026/Conference/Submission16201/Reviewer_mKj1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16201/Reviewer_mKj1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939255317, "cdate": 1761939255317, "tmdate": 1762926361509, "mdate": 1762926361509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes E2H Reasoner, a curriculum reinforcement learning (CRL) approach for enhancing LLM reasoning capabilities. It decomposes complex tasks into easier subtasks, uses probabilistic schedulers (cosine and Gaussian) to gradually shift from easy to hard tasks during RL post-training, and provides empirical improvements on benchmarks like Blocksworld, Countdown, and arithmetic tasks, achieving SOTA results. Theoretically, it analyzes CRL via approximate policy iteration, proving convergence and reduced sample complexity compared to direct learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method creatively combines task decomposition with probabilistic scheduling in CRL, addressing rollout inefficiencies in difficult reasoning tasks by building skills incrementally, which makes intuitive sense and extends prior RL post-training like DeepSeek-R1.\n\n2. Theoretical analysis provides finite-sample bounds and convergence guarantees, grounding the approach in approximate policy iteration.\n\n3. Well-structured presentation with illustrative figures (e.g., task decomposition in Fig. 2, schedulers in Figs. 3-4) and precise definitions of reasoning as generalization; methods and experiments are logically sequenced."}, "weaknesses": {"value": "1. Risk of Overfitting in Task Decomposition: Decomposing hard tasks into varying difficulty levels may cause repeated exposure to similar knowledge patterns across subtasks, increasing overfitting risks, especially if subtasks overlap significantly without explicit regularization.\n\n\n2. Lack of Implementation Details for Reproducibility: Key details are missing, such as prompts used for automatic difficulty estimation (e.g., in AQuA/GSM8K) or exact hyperparameters for task grouping, raising concerns about replicating the reported effects.\n\n\n3. Limited Scope of Experiments: Evaluations focus on relatively lower-difficulty tasks like Blocksworld and arithmetic benchmarks; lacks experiments on highly challenging ones like AIME, LCB, or agent-based tasks, limiting evidence for broader applicability."}, "questions": {"value": "1. Advantages of Task Decomposition Over Traditional Curriculum Learning: What specific advantages does your task decomposition offer compared to standard curriculum learning (e.g., fixed-stage switching)? Is there a theoretical comparison on generalization, perhaps extending your API framework?\n\n\n2. Inconsistencies in Model Trends in Figure 1(a): In Figure 1(a) for Countdown, why do Qwen 1.5B and LLaMA 3.2 3B show inconsistent relative performance trends under E2H vs. base models (e.g., one benefits more at low k)? Could this relate to architectural differences or training artifacts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ga1EHEdwut", "forum": "KJvHnl3kUv", "replyto": "KJvHnl3kUv", "signatures": ["ICLR.cc/2026/Conference/Submission16201/Reviewer_kbdX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16201/Reviewer_kbdX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762086931174, "cdate": 1762086931174, "tmdate": 1762926360958, "mdate": 1762926360958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}