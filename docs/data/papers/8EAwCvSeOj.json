{"id": "8EAwCvSeOj", "number": 21162, "cdate": 1758314412564, "mdate": 1759896938910, "content": {"title": "Self-Supervised Goal-Reaching Results in Multi-Agent Cooperation and Exploration", "abstract": "For groups of autonomous agents to achieve a particular goal, they must engage in coordination and long-horizon reasoning. However, designing reward functions to elicit such behavior is challenging.  In this paper, we study how self-supervised goal-reaching techniques can be leveraged to enable agents to cooperate. The key idea is that, rather than have agents maximize some scalar reward, agents aim to maximize the likelihood of visiting a certain goal. This problem setting enables human users to specify tasks via a single goal state rather than implementing a complex reward function. While the feedback signal is quite sparse, we will demonstrate that self-supervised goal-reaching techniques enable agents to learn from such feedback. On MARL benchmarks, our proposed method outperforms alternative approaches that have access to the same sparse reward signal as our method. While our method has no *explicit* mechanism for exploration, we observe that self-supervised multi-agent goal-reaching leads to emergent cooperation and exploration in settings where alternative approaches never witness a single successful trial.", "tldr": "We reframe cooperative MARL as a goal-conditioned RL problem, enabling agents to effectively achieve shared goals with sparse feedback through emergent exploration.", "keywords": ["multi-agent RL", "goal-conditioned RL", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/daa4cb733372d3b38d33cf669fefa53548a43aa8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Independent Contrastive Reinforcement Learning (ICRL), a self-supervised independent RL learning algorithm that enables learning policies in multi-agent settings by specifying a goal to reach, without having to craft a timestep-based reward function. Namely, the reward is defined automatically as the likelihood of reaching the goal. To achieve this the authors use contrastive learning to train a critic that measures how close a state-aciton pair os to the goal in a learned embedding space. The algorithm is evaluated on cooperative and competitive benchmarks such as Starcarft and a 4-legged robot control. ICLR outperforms strong baselines like IPPO or MAPPO in these sparse reward settings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper offers an elegant approach to tackle multi-agent RL problems as a self-supevised goal reaching problem, building on top of ideas from goal-conditioned RL and independent policy learning. The paper is enjoyable to read and well-explained. The results are interesting, showing emergent learning without requiring much exploration signal and without needing to define a reward function. Experiments cover several good environments and compare against strong baselines such as PPO. The motivation to remove dependence on hand-tuned rewards is also well justified."}, "weaknesses": {"value": "- The approach assumes that meaningful goals can be defined, which mgiht not be straightforward in realistic tasks. This is briefly mentioned by authors in the limitations but no suggestions to address this are proposed.\n- Empirical results are strong but could benefit from benchmarking on more environments, running ablations to analyze which part of the algorithm leads to the most improvements, and more analysis to understand why the algorithm performs well with such sparse rewards (though the authors provide a qualitative discussion).  \n- Providing more experimental details about how baselines such as MAPPO and IPPO were calibrated (hyperparameter tuning budget) would help to assess the fairness of the comparison."}, "questions": {"value": "- In the experiments done here do all agents share the same goal? \n- Can the authors explain how the standard vs unsupervised approaches differ on a simple problem, such as going from point A to point B? The standard reward would be the negative L2 distance between the agent's position and the goal. In this simple case how would the two approaches compare, and given that the goal conditioned approach would lead to a very sparse reward, do the authors have any intuition for what would the embedding functions learn in order to perform well on this task?\n- Can the authors give example of how goals could be defined for more complex realistic RL tasks, for example in robotics or autonomous driving? \n- The plots compare performance vs steps. I wonder if runtime is also comparable between IPPO/MAPPO and ICRL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hi13Y8jp8Q", "forum": "8EAwCvSeOj", "replyto": "8EAwCvSeOj", "signatures": ["ICLR.cc/2026/Conference/Submission21162/Reviewer_n7bM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21162/Reviewer_n7bM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984876379, "cdate": 1761984876379, "tmdate": 1762941524082, "mdate": 1762941524082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Independent Contrastive Reinforcement Learning (ICRL) to address coordination and exploration in multi-agent systems facing sparse feedback signals. ICRL reframes the cooperative Multi-Agent RL (MARL) problem as a self-supervised goal-reaching task, where users specify the objective via a single goal state rather than complex reward functions. ICRL is an actor-critic algorithm that combines insights from single-agent Contrastive RL (CRL) and Independent PPO (IPPO). The critic learns representations using the symmetric InfoNCE loss to parameterize a mixed discounted state occupancy measure, which intuitively captures goal-directed temporal correlations.\nEmpirically, ICRL demonstrates superior performance and robustness across long-horizon, sparse-reward benchmarks, including multiple difficult SMAX environments. On four SMAX maps, ICRL is the only method to achieve a non-zero win rate compared to IPPO and MAPPO, both using the same sparse reward. Crucially, ICRL achieves high performance without explicit mechanisms for exploration or subgoals, displaying complex emergent cooperation, like micro-management and focus-fire in StarCraft. The paper argues that goal-conditioned contrastive learning makes the multi-agent goal-reaching problem tractable."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Originality\n\nThe primary originality lies in successfully extending self-supervised, representation-based goal-conditioned RL (GCRL) to the challenging domain of decentralized Multi-Agent RL (MARL). Prior approaches tackling sparse rewards in MARL predominantly rely on explicit mechanisms like intrinsic motivation, domain-guided search, or hierarchical generation of subgoals (e.g., MASER, CMAE, LAIES). ICRL avoids all these components, learning goal-directed behavior and exploration in a fully end-to-end manner. The formulation casts the multi-agent objective entirely as maximizing the likelihood of visiting a goal state specified by a single observation, which is highly appealing for practitioners burdened by reward engineering.\nThe paper combines Independent Learning (IL) principles (decentralized policies with parameter sharing, following IPPO) with the Contrastive RL critic design. This combination leads to a novel mixed critic objective $ρ_γ^{mix}$ that averages goal occupancy measures across agents. Although rooted in single-agent CRL, applying this representation learning approach to the partial observability and coordination challenges inherent in multi-agent systems represents a significant methodological step. Furthermore, the empirical finding that factoring a single-agent task (Ant control) into a multi-agent one can accelerate initial learning by effectively restricting the hypothesis space is a counterintuitive and original contribution to understanding MARL structure.\n\n\n- Quality\n\nThe authors perform fair comparison by providing all methods (ICRL, IPPO, MAPPO, MASER) with the exact same sparse 0/1 reward signal (+1 upon winning/reaching the goal, 0 otherwise). ICRL’s performance gap relative to strong baselines on the SMAX benchmark is massive and highly convincing. For instance, on the 2s3z, 6h v 8z, 8m, and 3s v 5z settings, ICRL achieved success rates above 84%, while IPPO and MAPPO achieved 0%. This suggests that ICRL’s goal-conditioned representation learning effectively solves the exploration difficulty inherent in these long-horizon tasks, where standard PPO variants fail to witness a single successful trial.\nThe head-to-head comparison with MASER, a specialized SOTA sparse-reward MARL algorithm, further strengthens the paper's claims, showing ICRL achieving a 60% win rate on 2s3z by step 5M, whereas MASER’s win rate was negligible. The results are also supported by statistical evidence using the probability of improvement, which shows ICRL improves upon MAPPO 94% of the time across SMAX environments. The experiments cover complex continuous control (Multi-Agent Ant) and include detailed ablations on agent specialization and the robustness of goal mapping $m_g$.\n\n\n- Clarity\n\nThe paper is well-written, clearly defining the shift from standard MARL (Eq. 1) to the goal-reaching objective (Eq. 6). The algorithm (Independent CRL, Alg. 1) is presented clearly as an alternating actor-critic approach. Through visualizations of training runs on 2s3z, the authors show ICRL agents learning complex micromanagement skills like kiting and focus-fire before observing a single success, while IPPO policies remain random. \n\n\n- Significance\n\nThe work validates multi-agent goal-reaching as a feasible paradigm, potentially allowing future MARL development to circumvent the costly and difficult process of reward engineering. The results fundamentally challenge the necessity of hierarchical decomposition or explicit intrinsic motivation mechanisms for long-horizon sparse-reward MARL, positioning ICRL as a conceptually simpler and more effective alternative than prior methods like MASER."}, "weaknesses": {"value": "- Originality\n\nThe methodological originality is primarily compositional. The work is a direct hybridization of existing established concepts: Independent Learning (IL) from MARL literature (e.g., IPPO) and Contrastive RL (CRL) from single-agent GCRL literature. The authors explicitly state they combine \"insights from recent work on goal reaching in single-agent settings\" and independent learning for MARL. The core innovation is demonstrating that this specific composition works dramatically better in the sparse MARL context than prior MARL methods.\nFurthermore, the crucial theoretical links rely on approximations or assumptions. For instance, the critic formulation assumes the overall goal g can be approximated as a function of local observations $o^{(i)}$ for the mixed critic derivation. Appendix E, which aims to provide theoretical grounding for the actor objective, proves a lower bound using a modified ICRL objective ($J_π^{ICRL,mod}$) where goals are collective observations, but then immediately notes that the actual implemented ICRL objective uses agent-specific goals ($g^{(i)}$), leading to \"weaker theoretical motivation\". Relying on empirical success despite admitted weak theoretical motivation slightly diminishes the originality of the structural design choices made for the multi-agent adaptation.\n\n- Quality\n\nThe most notable weakness in quality stems from the selection and performance of the baselines (IPPO and MAPPO). Achieving a 0.00 win rate across four out of five complex SMAX environments with standard sparse 0/1 rewards suggests that these baselines may not represent the strongest possible PPO implementations for these specific sparse settings, despite PPO's overall robustness claims in MARL. While the authors compare against MASER, a strong sparse-reward baseline, this comparison is limited to only the 2s3z environment. To robustly claim superiority over all hierarchical or explicit exploration methods, a wider comparative study against MASER or other state-of-the-art sparse MARL techniques (e.g., CMAE or LAIES, which are mentioned but not run against ICRL) would be needed.\nFor the continuous control Ant task, the result that ICRL achieves faster initial learning but levels off while single-agent CRL continues improving suggests a fundamental limitation of the Independent Learning framework: a trade-off between reduced variance (fast learning) and increased bias (lower asymptotic performance). This trade-off needs deeper systematic investigation across more than one benchmark to fully understand the generalization and asymptotic limits of ICRL, especially compared to centralized goal-conditioned approaches.\n\n\n- Clarity\n\nThe purpose, derivation, and sensitivity of the coefficient 0.01 in Eq. 7 are not discussed in the text, which obscures a part of the critic objective.\n\nSecond, the discrepancy between the theoretical derivation in Appendix E and the implemented algorithm (as discussed above) needs resolution. The authors should explicitly justify why sampling agent-specific future goals $g^{(i)}$ (the implemented choice) works well empirically, even if it lacks the theoretical bound proved for the modified objective using collective goals g.\n\nThird, the handling of discrete actions involves parameterizing the critic using the soft actor output instead of the sampled discrete action. The justification provided is that this \"slightly speeds up learning\". A more detailed explanation of why the continuous relaxation of the action (the soft output) is a better feature input for the contrastive representation learning, which is fundamentally about temporal correlation, is necessary for full clarity and technical rigor, especially given that this is an \"unconventional choice\".\n\n\n- Significance\n\nThe significance is somewhat conditional on the practical definition of the goal. The authors note the limitations: it may not always be clear how to specify certain tasks as goals. Furthermore, the surprising finding in Appendix D.2 that using an \"uninformative\" goal mapping $m_g$ (the identity map G=O) can sometimes lead to better performance than a human-selected $m_g$ complicates the core utility proposition: that goal-reaching simplifies task specification. If practitioners must default to using the full observation space as the goal (identity map) because manually defined goal features inadvertently remove useful contextual information, then the claimed benefit of defining the goal space G and mapping $m_g$ is weakened."}, "questions": {"value": "1. Given the extremely low performance of IPPO and MAPPO (0.00 win rates) when using the sparse 0/1 reward signal on challenging SMAX environments (e.g., 2s3z, 6h v 8z, 8m, 3s v 5z), have the authors tested whether incorporating minimal, standard exploration bonuses or noise injection (not goal-specific or intrinsic motivation) into these baselines could significantly close the performance gap with ICRL?\n\n2. The implementation of ICRL samples agent-specific achieved future goals $g^{(i)}$ (Eq. 9), yet the theoretical lower bound relies on a modified objective sampling the collective goal g (Eq. 10). Since the authors note the implemented choice has \"weaker theoretical motivation\", can they provide a detailed empirical analysis or justification regarding why the agent-specific goal sampling scheme was chosen over the more theoretically grounded collective goal scheme?\n\n3. The symmetric InfoNCE loss includes a regularization term 0.01⋅R(ϕ,ψ). Can the authors clarify the theoretical purpose of this specific regularization and discuss the sensitivity of ICRL's performance to the coefficient 0.01?\n\n4. The Multi-Agent Ant experiment shows ICRL learning much faster initially than single-agent CRL, but then CRL eventually surpasses ICRL. Does this pattern of faster initial learning but lower asymptotic performance hold across other factored continuous control benchmarks? If so, what design modifications (beyond increasing the sample budget) could be incorporated into ICRL to mitigate the inherent bias from the independent learning factorization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "shfhmeJiig", "forum": "8EAwCvSeOj", "replyto": "8EAwCvSeOj", "signatures": ["ICLR.cc/2026/Conference/Submission21162/Reviewer_AXSW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21162/Reviewer_AXSW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179210157, "cdate": 1762179210157, "tmdate": 1762941523377, "mdate": 1762941523377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a study on multi-agent Reinforcement Learning, specifically tackling the difficulty of learning in sparse reward environments. The authors propose an integrated method combining Goal-Conditioned RL and Contrastive Learning. Crucially, the approach leverages contrastive learning to maximize the likelihood of visiting specific goal states. The proposed method was evaluated on a subset of the StarCraft Multi-Agent Challenge (SMAC) and Multi-Agent Particle Environment (MPE) tasks, where experimental results show it  achieves non-zero rewards on several complex, sparse-reward tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental results show that the proposed method, despite its simplicity, effectively achieved good performance on a challenging subset of tasks within the SMAC benchmark.\n\n2. While some clarification on technical details are needed, the paper is generally well-organized and easy to follow. \n\n3. The reviewer noted that the emergent exploration behavior was engaging. Agents acquired fundamental skills early in training and progressively developed more sophisticated strategies, indicating that goal-conditioned RL has the potential to enhance exploration."}, "weaknesses": {"value": "1. The reviewer has some concerns regarding the paper's core technical contribution. The proposed methods, particularly the goal-conditioned reinforcement learning framework and the use of a contrastive loss, appear to share significant similarities with the existing LAGMA [a] framework. The reviewer attempted to identify other major technical innovations beyond this adaptation but did not find them clearly articulated. To assist readers in appreciating the novel aspects of this work, the reviewer suggests that the authors could strengthen the paper by explicitly delineating the primary technical differences between their proposed method and previous works, especially LAGMA.\n\n2. The experimental section would be more compelling if the authors compared their approach not only with IPPO and MAPPO, but also with goal-conditioned RL methods like LAGMA, especially given LAGMA's reported non-zero rewards in sparse reward settings for SMAX 8m, 2s3z, and 2m1z.\n\n\n[a] LAGMA: LAtent Goal-guided Multi-Agent Reinforcement Learning, Na et al. ICML 2024"}, "questions": {"value": "1. The paper's description of how goals are defined within each task environment is currently unclear. Could the authors please provide a more specific definition of what constitutes a 'goal state,' particularly for the MPE-tag and SMAC tasks?\n\n2. Is the difficulty of the SMAC tasks super-hard?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VVfUBXuUyd", "forum": "8EAwCvSeOj", "replyto": "8EAwCvSeOj", "signatures": ["ICLR.cc/2026/Conference/Submission21162/Reviewer_Tf73"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21162/Reviewer_Tf73"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216995955, "cdate": 1762216995955, "tmdate": 1762941522379, "mdate": 1762941522379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Independent CRL (ICRL), a method that extends contrastive reinforcement learning to multi-agent settings for goal-reaching tasks. The key idea is to frame multi-agent coordination as a goal-conditioned problem where agents maximize the likelihood of reaching a commanded goal state rather than optimizing a scalar reward. Each agent learns independently using shared parameters, with a contrastive critic learning representations that capture temporal relationships between state-action pairs and goals. The authors evaluate their approach on MPE Tag, StarCraft Multi-Agent Challenge (SMAC/SMAX), and multi-agent continuous control tasks, demonstrating superior performance compared to IPPO, MAPPO, and MASER baselines in sparse-reward settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is well-motivated. The goal-conditioned multi-agent framework removes the burden of reward engineering while maintaining clear task specification through a single goal state.\n\n2. The paper demonstrates positive results across diverse environments (MPE Tag, SMAC/SMAX, multi-agent continuous control) with different team sizes and complexity levels. On several SMAC environments (2s3z, 6h_v_8z, 8m, 3s_v_5z), ICRL achieves non-zero win rates while baselines fail completely. The analysis in Section 5.3 provides useful qualitative insights into emergent coordination strategies.\n\n3. Section 5.5 shows that factoring a single-agent problem into a multi-agent one can accelerate learning, which is counterintuitive and potentially valuable."}, "weaknesses": {"value": "### Major Issues\n\n1. **Unclear mechanism for early-stage learning**: The paper states that \"g_i are future states encountered after (o_i, a_i)\" (line 207), but elsewhere discusses g as a specific goal state. This creates critical ambiguities:\n   - When training the critic, are \"achieved future goals\" any future states in a trajectory, or only states where the final goal is reached? How about when training the policy?\n   - If agents never reach the goal state g* during early training, how does the InfoNCE loss (Eq. 7) trained on arbitrary future states g_i provide learning signal toward the specific goal g*?\n   - The connection between training on arbitrary achieved goals and transferring to a specific commanded goal g* needs explicit discussion.\n   \n   This is particularly important given the paper's claim that the method works in settings where \"baselines never observe a single success\"—if the goal is never observed, what guides the policy toward it?\n\n2. **Insufficient justification for key assumptions and design choices**:\n   - The method assumes the overall goal g can be approximated as a function of local observations o^(i), but provides no principled guidance for when this is applicable. For instance, in SMAC environments, how is the goal state determined to be reached when some enemies are outside all agents' observation ranges? \n   - While Lemma E.3 provides a lower bound for a *modified* version of ICRL, the actual algorithm samples agent-specific goals g^(i) rather than collective goals g. The authors acknowledge this gap between theory and practice but don't adequately explain why the empirically-used sampling scheme should work.\n\n3.. **Experimental setup concerns**:\n   - The sparse reward formulation (0/1 only at goal achievement) may be particularly disadvantageous for standard RL methods designed for dense rewards. While the authors argue this is \"natural,\" it's quite extreme and may not reflect realistic deployment scenarios.\n   - The paper identifies LAGMA (Na & Moon, 2024) as \"perhaps the most related work\" in Section 2, as both methods address goal-conditioned multi-agent RL in sparse reward settings. However, no experimental comparison with LAGMA is provided.\n\n### Minor Issues\n\n   - Line 118: s_t missing superscript (1:n)\n   - Line 132: \"Dec-POMDP\" should be spelled out on first use\n   - Line 170/Equation 5: The probability function should use larger parentheses; the current formatting makes it appear as if there are three separate equalities (A=B=C), causing potential confusion\n   - Lines 200-201: \"Formally\" is used twice in succession; rephrase for better flow\n   - Line 212/Equation 7: Uses subscript i for agent ID while the rest of the paper uses superscript (i); notation should be consistent"}, "questions": {"value": "see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1dEHlthSiO", "forum": "8EAwCvSeOj", "replyto": "8EAwCvSeOj", "signatures": ["ICLR.cc/2026/Conference/Submission21162/Reviewer_Eavo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21162/Reviewer_Eavo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762569903937, "cdate": 1762569903937, "tmdate": 1762941521738, "mdate": 1762941521738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}