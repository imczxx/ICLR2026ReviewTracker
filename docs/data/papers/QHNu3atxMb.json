{"id": "QHNu3atxMb", "number": 15354, "cdate": 1758250493809, "mdate": 1759897312147, "content": {"title": "OmniVIVO: Towards Unified Multimodal Generative Modeling for Simultaneous Language-Guided Speech and Image Synthesis", "abstract": "Recent large language models (LLMs) based on autoregressive (AR) next-token prediction have achieved remarkable success in natural language generation and are rapidly expanding to image and speech synthesis. Yet most current approaches still treat these modalities in isolation—training independent models or loosely coupling multiple generators. Even recent omni-models such as UGen and Qwen2.5-Omni mainly address understanding tasks or text–image generation and do not provide a single AR backbone capable of simultaneously producing high-fidelity images and natural speech. Inspired by the human brain’s capability to imagine and speak simultaneously, we propose OmniVIVO, a unified AR approach for modeling visual and voice modalities together, capable of generating high-fidelity images and natural speech in parallel from a single text input. Our OmniVIVO integrates a state-of-the-art AR image generator with a novel lightweight speech decoder, enabling the first unified approach for the concurrent generation of natural speech and high-fidelity images. By sharing representations across modalities within a single transformer backbone, the model learns a rich multimodal space that enables tighter semantic alignment and more efficient joint generation than existing multi-model pipelines. Through a unified backbone, OmniVIVO produces speech with high perceptual quality and naturalness, surpassing comparably sized text-to-speech (TTS) systems and being on par with state-of-the-art systems like Cosyvoice2 and VITS, while maintaining high-fidelity image generation. To quantify contextual understanding across modalities, we propose a multimodal ranking metric spanning text, speech, and images, demonstrating that OmniVIVO’s bi-modal outputs are effective in information acquisition. We construct VIVOGen, a high-quality tri-modal text–image–speech dataset that leverages OmniVIVO’s multimodal outputs, providing a valuable resource for research in multimodal learning and applications in education and language acquisition, which we will publicly release.", "tldr": "OmniVIVO is a unified large language model that simultaneously generates high-fidelity images and natural speech from a single text input.", "keywords": ["Multimodal Generation", "High-Fidelity Image Generation", "Text-to-Speech"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a2832070fbf9c57ef11d172394f2017a26b59b8.pdf", "supplementary_material": "/attachment/3c91e7526d47cb595f3c78671f38e8ce8d14e5a8.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces OmniVIVO, a novel unified autoregressive model designed to generate both high-fidelity images and natural-sounding speech concurrently from a single text prompt. The core architectural innovation is the integration of LlamaGen, which acts as a shared \"OmniCore\" backbone, with a new, lightweight speech generation branch. To preserve the strong image generation capabilities of the base model, its original parameters are frozen. The model is adapted for the dual-modality task using LoRA on the shared backbone, and only the LoRA adapters and the speech branch are trained. This training is performed exclusively on a text-to-speech objective, allowing the speech module to leverage the rich semantic representations learned by the powerful image generator. In addition to the model, the authors propose VIVOGen, a tri-modal dataset generated by OmniVIVO, and a multimodal ranking metric to evaluate the effectiveness of combined-modality outputs for information acquisition."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The concept of a single, unified backbone for simultaneous image and speech generation is a significant step forward from current multimodal systems that either focus on understanding or generate outputs in isolated modalities. The architectural design is particularly clever and pragmatic; by leveraging a powerful pretrained model and employing LoRA, the authors avoid the prohibitive cost of training a massive model from scratch while largely preserving the state-of-the-art performance of the image generation component. \n\n2. The empirical results are strong and well-presented. The ablation study convincingly demonstrates the superiority of their adaptation strategy over a baseline trained from scratch. Furthermore, the model achieves impressive speech quality, even outperforming specialized text-to-speech systems like CosyVoice2 in naturalness, which is a remarkable feat for a joint image-speech model. The contribution of the VIVOGen dataset and the initial exploration of a multimodal ranking metric are also commendable additions that can foster future research in this area."}, "weaknesses": {"value": "1. Both T2A and T2I already have very advanced methods. This paper proposes a task for simultaneous image and audio generation, which I don't think is strongly related. Since there's no need for video output, images and audio can be generated separately. Therefore, if we look at these two tasks separately, the model proposed in this paper is completely uncompetitive.\n\n2. The evaluation of the model's multimodal capabilities feels preliminary and lacks rigor. The \"Multimodal Coherence\" score is presented in a vacuum without any baseline, making it difficult to interpret the score of 3.79/5. A simple baseline, such as evaluating the coherence of outputs from two separate state-of-the-art/close-source models, would provide crucial context. \n\n3. Similarly, the proposed \"Multimodal Ranking\" metric, while interesting, seems designed to confirm the intuitive hypothesis that more modalities are better, rather than to critically assess the quality of OmniVIVO's multimodal binding against other possible systems. The paper's central claim that the model \"learns a rich multimodal space that enables tighter semantic alignment\" is not substantiated with any analysis; the work would be significantly more impactful if it included probes or visualizations of the internal representations to demonstrate this emergent property. \n\n4. The claim that image performance is \"fully preserves\" is contradicted by the reported drop in Inception Score."}, "questions": {"value": "I would suggest that the authors first explain the value of outputting images and speech simultaneously, and then demonstrate the advantages of the method proposed in this article compared to the cutting-edge T2I and T2A models."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The authors claim to release the VivoGen dataset, but do not mention the data source of this data in the submission, nor do they mention anything related to open source license in the ethics section."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UejZA1tZeL", "forum": "QHNu3atxMb", "replyto": "QHNu3atxMb", "signatures": ["ICLR.cc/2026/Conference/Submission15354/Reviewer_FQBP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15354/Reviewer_FQBP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761498088618, "cdate": 1761498088618, "tmdate": 1762925640967, "mdate": 1762925640967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniVIVO, a unified autoregressive (AR) multimodal generation model that can produce high-fidelity images and natural speech simultaneously from a single text input. \nThe method integrates a pretrained AR image generator (LlamaGen) with a lightweight speech decoder through LoRA adaptation, enabling parallel visual and voice generation within one Transformer backbone.\nThe authors also introduce a multimodal ranking metric and release a small tri-modal dataset (VIVOGen).\nExperiments show that OmniVIVO achieves comparable image quality to LlamaGen and competitive speech quality to VITS/CosyVoice2, while enabling joint multimodal synthesis."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Unified multimodal generation:** The paper presents a unified autoregressive framework that can generate both speech and images from a single text input. Unlike prior works that treat the two modalities separately, OmniVIVO integrates them within one Transformer backbone using lightweight LoRA adaptation. This design is simple and efficient, showing that joint visual and speech generation is feasible within a shared architecture.\n\n- **Introduction of a multimodal ranking metric:** The paper proposes a multimodal ranking metric to evaluate how different modality combinations (text, speech, image) affect information acquisition. This metric complements traditional subjective (MOS) and objective (WER/CER) evaluations used in speech synthesis, offering a broader view of multimodal generation performance.\n\n- **Dataset contribution – VIVOGen:** The paper releases VIVOGen, a tri-modal dataset containing paired text, image, and speech data. Although the dataset is small, it provides a useful resource for future research. The dataset design is consistent with the paper’s unified multimodal generation framework."}, "weaknesses": {"value": "- **Lack of novelty:** The proposed method mainly combines several existing techniques, including the LlamaGen autoregressive image generator, Flow Matching for spectrogram reconstruction, and the HiFi-GAN vocoder for waveform synthesis. The overall framework represents a straightforward integration of these existing components rather than a fundamentally new algorithmic contribution or learning paradigm.\n\n- **Degradation of image generation performance:** The integration of the speech generation branch and LoRA adaptation notably reduces image generation quality. As reported in Table 2, OmniVIVO achieves an Inception Score (IS) of 6.93 ± 0.65, compared to 7.78 ± 0.78 for the original LlamaGen. This indicates that the added multimodal components negatively affect the visual fidelity of generated images.\n\n- **Lack of comparative study with existing unified multimodal models:**  The paper evaluates multimodal coherence only within OmniVIVO and does not include comparisons with existing unified multimodal models such as Ming-Omni: A Unified Multimodal Model for Perception and Generation. Ming-Omni is also capable of handling both speech and image modalities under a single architecture. Without a direct comparison to such related work, it remains unclear whether OmniVIVO provides any tangible improvement in cross-modal alignment or coherence over prior unified multimodal generation systems."}, "questions": {"value": "- **Q1.** Could the authors clarify what specific methodological or conceptual novelty OmniVIVO introduces beyond integrating existing components such as LlamaGen, Flow Matching, and HiFi-GAN?\n\n- **Q2.** Table 2 shows that OmniVIVO’s Inception Score (6.93 ± 0.65) is significantly lower than LlamaGen’s (7.78 ± 0.78). Have the authors analyzed the reasons for this performance drop?\n\n- **Q3.** The paper reports subjective evaluations for multimodal quality. Are there any objective multimodal alignment metrics (e.g., CLIP-type similarity, audio–visual retrieval accuracy) that could complement these results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a98DBLYnp6", "forum": "QHNu3atxMb", "replyto": "QHNu3atxMb", "signatures": ["ICLR.cc/2026/Conference/Submission15354/Reviewer_DJ9M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15354/Reviewer_DJ9M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761531876982, "cdate": 1761531876982, "tmdate": 1762925640418, "mdate": 1762925640418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniVIVO, a unified autoregressive model designed to simultaneously generate high-fidelity images and natural speech from a single text input. The architecture integrates a frozen, state-of-the-art image generation backbone with a lightweight speech decoder branch. By leveraging shared Transformer representations and LoRA-based adaptation, the model enables both modalities to be produced in parallel, accomplishing both audio and image generation tasks. The work also provides the VIVOGen dataset, which comprises 100 samples."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The model employs a unified generative framework achieved by applying LoRA adaptation on a frozen image generator. This strategy effectively maintains reasonable computational and training overhead while establishing new cross-modal generation capabilities.\n\n- The provision of the VIVOGen dataset is a positive contribution that can further promote the development of tri-modal research."}, "weaknesses": {"value": "-  The proposed method lacks sufficient innovation and does not demonstrate a clear advantage when compared to existing state-of-the-art omni-modal generation works (e.g., EMU, MIO,Unified-IO).\n\n- Experiments can be more complete. Specifically, the model underperforms the baseline on the image generation task, and there is a critical absence of comparative results against existing omni-modal models.\n\n- The quality of the dataset has not been subjected to in-depth quality assessment or verification, as only data samples are shown. The authors should consider supplementing the work with more comprehensive experimental validation of the dataset quality.\n\n- There are some presentation concerns: figure and table captions could be more detailed, and certain evaluation metrics (e.g., Inception Score) should be explained more thoroughly.\n\n[1].Cui, Y.et al. Emu3.5: Native Multimodal Models are World Learners. ArXiv. https://arxiv.org/abs/2510.26583\n[2].Wang, Z., et al.  MIO: A Foundation Model on Multimodal Tokens. ArXiv. https://arxiv.org/abs/2409.17692\n[3].Lu J, Clark C, Zellers R, et al. Unified-io: A unified model for vision, language, and multi-modal tasks[J]. arXiv preprint arXiv:2206.08916."}, "questions": {"value": "same as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4N8eRLdbS3", "forum": "QHNu3atxMb", "replyto": "QHNu3atxMb", "signatures": ["ICLR.cc/2026/Conference/Submission15354/Reviewer_G9ar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15354/Reviewer_G9ar"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984617283, "cdate": 1761984617283, "tmdate": 1762925639951, "mdate": 1762925639951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}