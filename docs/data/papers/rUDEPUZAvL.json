{"id": "rUDEPUZAvL", "number": 2274, "cdate": 1757046675831, "mdate": 1759898158964, "content": {"title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning", "abstract": "Recent advancements underscore the significant role of GRPO-based reinforcement learning methods and comprehensive benchmarking in enhancing and evaluating text-to-image (T2I) generation. However, (1) current methods employ pointwise reward models (RM) to score a group of generated images and compute their advantages through score normalization for policy optimization. Although effective, this reward score-maximization paradigm is susceptible to reward hacking, where scores increase but image quality deteriorates. This work reveals that the underlying cause is illusory advantage, induced by minimal reward score differences between generated images. After group normalization, these small differences are disproportionately amplified, driving the model to over-optimize for trivial gains and ultimately destabilizing the generation process. To this end, this paper proposes PREF-GRPO, the first pairwise preference reward-based GRPO method for T2I generation, which shifts the optimization objective from traditional reward score maximization to pairwise preference fitting, establishing a more stable training paradigm. Specifically, in each step, the images within a generated group are pairwise compared using preference RM, and their win rate is calculated as the reward signal for policy optimization. Extensive experiments show that PREF-GRPO effectively differentiates subtle image quality differences, offering more stable advantages than pointwise scoring, thus mitigating the reward hacking problem. (2) Additionally, existing T2I benchmarks are limited to coarse evaluation criteria, covering only a narrow range of sub-dimensions and lacking fine-grained evaluation at the individual sub-dimension level, thereby hindering comprehensive assessment of T2I models. Therefore, this paper proposes UNIGENBENCH, a unified T2I generation benchmark. Specifically, our benchmark comprises 600 prompts spanning 5 main prompt themes and 20 subthemes, designed to evaluate T2I models’ semantic consistency across 10 primary and 27 sub evaluation criteria, with each prompt assessing multiple testpoints. Using the general world knowledge and fine-grained image understanding capabilities of Multi-modal Large Language Model (MLLM), we propose an effective pipeline for benchmark construction and evaluation. Through meticulous benchmarking of both open and closed-source T2I models, we uncover their strengths and weaknesses across various fine-grained aspects, and also demonstrate the effectiveness of our proposed PREF-GRPO.", "tldr": "", "keywords": ["Reinforcement Learning", "Text-to-Image", "GRPO"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d836853206f8816f433a4f0df286a727baef2bd6.pdf", "supplementary_material": "/attachment/12e37f98cece0694c51be0c7b8aab0721185ddda.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces PREF-GRPO, a pairwise preference reward-based GRPO method for text-to-image generation, and UNIGENBENCH, a unified benchmark for fine-grained evaluation. The authors claim their method addresses reward hacking by shifting from reward score maximization to pairwise preference fitting. While the paper tackles an important problem in T2I reinforcement learning, there are significant concerns regarding the experimental setup and claims validation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies an important problem in T2I reinforcement learning - reward hacking caused by illusory advantage.\n2. The introduction of UNIGENBENCH with fine-grained evaluation dimensions is valuable for the community.\n3. The pairwise preference approach is conceptually interesting and aligns with human evaluation processes."}, "weaknesses": {"value": "1. Unfair Baseline Comparisons: The experimental setup raises significant concerns:\n- PREF-GRPO uses UnifiedReward-Think while baselines use UnifiedReward without the thinking mechanism, creating an unfair advantage\n- HPS is outdated (the community now primarily uses HPSv2, ImageReward, and MPS)\n2. Insufficient Evidence for Reward Hacking Mitigation: The paper fails to provide convincing evidence that PREF-GRPO actually alleviates reward hacking:\n- No analysis of training dynamics beyond basic reward curves\n- Missing monitoring metrics on established benchmarks like GenEval throughout training\n- The comparison in Table 1 only shows final performance, not the stability of the training process\n3. Limited Ablation Studies: The paper lacks sufficient ablation studies to understand the contribution of different components of PREF-GRPO."}, "questions": {"value": "- Have you conducted any analysis to show that PREF-GRPO actually reduces reward hacking behaviors rather than just achieving better final scores?\n- How does the computational cost of PREF-GRPO compare to baseline methods, especially as group size increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iV2oIQgCen", "forum": "rUDEPUZAvL", "replyto": "rUDEPUZAvL", "signatures": ["ICLR.cc/2026/Conference/Submission2274/Reviewer_7okx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2274/Reviewer_7okx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986583286, "cdate": 1761986583286, "tmdate": 1762916170295, "mdate": 1762916170295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a GRPO method for tuning diffusion models with rewards along with a comprehensive benchmark for evaluating text-to-image models. Compared to previous GRPO methods for diffusion/flow models (e.g. {Flow,Mix,Dance}-GRPO), the proposed approach (Pref-GRPO), does not directly compute a reward per sample, but rather computes the reward over the full batch. This appears to make the advantage computation more reliable, leading to better performance. \nAdditionally, the paper also introduces \"UniGenBench\", which has several categories for comprehensive evaluation is evaluated using the LLM-as-a-judge methodology with Gemini 2.5 Pro."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "I think the paper proposes an interesting mechanism to deal with the issue of reward-hacking in the reward optimization of diffusion models, which is backed by fairly strong results on several benchmarks. \n\nThe proposed UniGenBench also appears to be a useful addition in terms of evaluating newer models. While I'm not entirely sure that this would become a widely used benchmark given that the field has several benchmarks already (GenEval, T2I-Compbench, DPG-Bench, GenAI-Bench, TIFA etc.), it does have its merits."}, "weaknesses": {"value": "[Major]\n\nComparisons with Previous GRPO work: I think the most important question is regarding the performance of Pref-GRPO compared to other formulations (Flow-GRPO, DanceGRPO etc.). The key claim being made in the paper is that the pairwise reward formulation is better suited to compute advantages for optimizing the model compared to existing work. While there seem to be some results in Fig. 2, Tab. 4-6, I'm not exactly sure how these settings compare to the previous GRPO methods. \n\n\n[Minor]\n\nA slightly curious aspect of the paper is that the benchmark and mehtod are quite orthogonal; i.e the benchmark on its own can provide useful analysis, while the method could also be validated on existing benchmarks, and to some extent the paper feels like 2 disjoint works stitched together."}, "questions": {"value": "The only question I'd really like to have full clarity is the differences and advantages over other GRPO frameworks for diffusion models. While I'm leaning to accept the paper, I think answering this comprehensively would be ideal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m7uImdbfa5", "forum": "rUDEPUZAvL", "replyto": "rUDEPUZAvL", "signatures": ["ICLR.cc/2026/Conference/Submission2274/Reviewer_mni3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2274/Reviewer_mni3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987333927, "cdate": 1761987333927, "tmdate": 1762916169993, "mdate": 1762916169993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PreF-GRPO, a pairwise preference reward-based Group Relative Policy Optimization (GRPO) method designed to address the reward hacking problem in text-to-image (T2I) reinforcement learning. By shifting from pointwise reward maximization to pairwise preference fitting, the method aims for more stable learning and better alignment with nuanced human preferences. Additionally, the authors present UniGenBench, a new benchmark for T2I generation that evaluates models across comprehensive primary. They also fine-grained sub-dimensions using an automated pipeline based on Multi-modal Large Language Models (MLLMs). Through extensive experiments, the authors demonstrate that PreF-GRPO achieves notable improvements in both image quality and semantic consistency compared to existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[+] For the reward hacking problem in existing GRPO methods, this paper identifies “illusory advantage” resulting from minimal score differences and their amplification during normalization\n\n[+] It is well motivated that replace pointwise rewards with pairwise preference-based win rates, which is illustrated in Figure 1.\n\n[+] The description and motivation of UniGenBench as a benchmark is solid, with Figure 3 and Figure 4 substantiating its comprehensiveness."}, "weaknesses": {"value": "[-] There is a lack of formal analysis on the illusory advantage phenomenon. It would be better to have a more rigorous analysis quantifying the expected change in variance between score-based and pairwise win-rate rewards.\n\n[-] Most ablations focus on comparing PreF-GRPO to existing pointwise methods or naive adaptations, without the adversarial preference model errors analysis.\n\n[-] The assertion that PreF-GRPO produces more “human-aligned” or “faithful” outputs is plausible but not decisively demonstrated without human assessment."}, "questions": {"value": "1. Can the authors provide a more formal mathematical analysis of reward variance and its amplification?\n1. How well does  MLLM-based automated evaluation (via Gemini2.5-pro) agree with human annotators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OAUwfQcOaa", "forum": "rUDEPUZAvL", "replyto": "rUDEPUZAvL", "signatures": ["ICLR.cc/2026/Conference/Submission2274/Reviewer_GeGc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2274/Reviewer_GeGc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995896737, "cdate": 1761995896737, "tmdate": 1762916169665, "mdate": 1762916169665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper brilliantly identifies and solves \"reward hacking\" in T2I models. It argues the problem is \"illusory advantage\": standard reward models give similar images very similar scores, which, when normalized, create huge, noisy, and fake \"advantages\" that make training unstable.\n\nThe fix, PREF-GRPO, is elegant. Instead of using flawed absolute scores, it makes images in a group compete. It uses a pairwise model (\"A is better than B\") and gives each image a \"win rate.\" This signal is far more stable and stops the model from hacking. As a huge bonus, the paper also introduces UNIGENBENCH, a new, super-detailed benchmark for T2I evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The \"illusory advantage\" diagnosis is brilliant.\n\n- The \"win rate\" solution is an elegant and effective fix.\n\n- The visual proof is undeniable; the qualitative images show this method works and the others don't.\n\n- It also contributes a fantastic new benchmark."}, "weaknesses": {"value": "The main potential issue is computational cost. A \"win rate\" for a group of 8 images requires 28 pairwise comparisons ($O(G^2)$), versus just 8 for the baseline ($O(G)$). This seems significantly slower per training step, which the paper doesn't heavily focus on."}, "questions": {"value": "How much does the $O(G^2)$ complexity of pairwise comparisons slow down the actual wall-clock training time compared to the $O(G)$ baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CeysWwGYSy", "forum": "rUDEPUZAvL", "replyto": "rUDEPUZAvL", "signatures": ["ICLR.cc/2026/Conference/Submission2274/Reviewer_gvS1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2274/Reviewer_gvS1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762839814837, "cdate": 1762839814837, "tmdate": 1762916169550, "mdate": 1762916169550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}