{"id": "c8Ft3246KD", "number": 8099, "cdate": 1758062913655, "mdate": 1763442638691, "content": {"title": "A Sharp KL-Convergence Analysis for Diffusion Models under Minimal Assumptions", "abstract": "Diffusion-based generative models have emerged as highly effective methods for synthesizing high-quality samples. Recent works have focused on analyzing the convergence of their generation process with minimal assumptions, either through reverse SDEs or Probability Flow ODEs. The best known guarantees, without any smoothness assumptions, for the KL divergence so far achieve a linear dependence on the data dimension $d$ and an inverse quadratic dependence on $\\varepsilon$. In this work, we present a refined analysis that improves the dependence on $\\varepsilon$. We model the generation process as a composition of two steps: a reverse ODE step, followed by a smaller noising step along the forward process. This design leverages the fact that the ODE step enables control in Wasserstein-type error, which can then be converted into a KL divergence bound via noise addition, leading to a better dependence on the discretization step size. We further provide a novel analysis to achieve the linear $d$-dependence for the error due to discretizing this Probability Flow ODE in absence of any smoothness assumptions. We show that $\\tilde{O}\\left(\\tfrac{d\\log^{3/2}(\\frac{1}{\\delta})}{\\varepsilon}\\right)$ steps suffice to approximate the target distribution corrupted with Gaussian noise of variance $\\delta$ within $O(\\varepsilon^2)$ in KL divergence, improving upon the previous best result, requiring $\\tilde{O}\\left(\\tfrac{d\\log^2(\\frac{1}{\\delta})}{\\varepsilon^2}\\right)$ steps.", "tldr": "", "keywords": ["diffusion models", "probability flow ODEs", "score based generative models", "convergence analysis"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/971d2335e73675976ea2fbae8180833925fc5e31.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper theoretically proves that $O\\left(\\frac{d}{\\varepsilon}\\right)$ diffusion step is sufficient to reach KL divergence $O\\left(\\varepsilon^{2}\\right)$ to a blurred target distribution by taking a probability-flow ODE step then adding a small forward-noising step. It improves on prior $O\\left(\\tfrac{d}{\\varepsilon^{2}}\\right)$ iteration complexity rates under similar assumptions on time averaged score-estimation error. This is based on the observation that the discretization error contribution to KL divergence drops like $O(\\frac{1}{K^{2}})$ as the number of steps $K$ increases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents strong theoretical analysis with genuinely new ideas. The framework and bounds are carefully developed: assumptions are explicit. It is a well written paper that improves on previously known iteration complexity under minimal score estimation assumption."}, "weaknesses": {"value": "There is no experiment validating whether the improved bounds translate to any practical improvements."}, "questions": {"value": "Experimentally showing that the discretization error contribution to KL divergence drops like $O(\\frac{1}{K^{2}})$ would put the paper on much stronger footing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vmshdFTBH9", "forum": "c8Ft3246KD", "replyto": "c8Ft3246KD", "signatures": ["ICLR.cc/2026/Conference/Submission8099/Reviewer_EBD6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8099/Reviewer_EBD6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761597778966, "cdate": 1761597778966, "tmdate": 1762920081902, "mdate": 1762920081902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that a hybrid sampling scheme consisting of a two time step backward ODE followed by one time step of forward SDE leads to a KL error that can be bound proportional to $1/\\varepsilon_{\\text{score}}$, improving on Benton et al (2023/4) that establishes a bound in $1/\\varepsilon^2_{\\text{score}}$. This error bound only concerns the distribution at some stopping time $t_1$."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed sampling scheme is simple, with a single score approximation by iteration. However, it does not correspond to the dynamic of a continuous equation (see Question).\n* The convergence result improves on Benton et al. regarding dependency on $\\varepsilon_{\\text{score}}$."}, "weaknesses": {"value": "The paper presentation should be improved.\n\nAlgorithm 1 lacks of clarity: \n* line 2: Why sample from $t_K$ not $T = t_{K+1}$ (line 236) as a start using the standard normal distribution (line 238)?\n* For the last step $k=1$, what is used to define $h_0 = t_{0}-t_{-1}$?\n* What is the purpose of lines 9 and 10 recalling notation of distributions within the algorithm?\n\nAlso Theorem 3.1 is badly stated regarding stepsize/time discretization. $t_0<t_1<...$ are first given as a fixed discretization sequence, but then $h_k$ is given thus fixing the $t_k$s by a fixed rule.\n\nl. 180: Empirical counterpart: Should initialization be mentioned here? Also confusing notation $t_{k}$ and $p_{t-1|t}$ in the same line.\n\n\nMinor remarks:\n* l. 076: dependence or dependency?\n* l. 111: DDPM \"showed that the corresponding denoising kernels are also Gaussian\" It is my understanding that the kernels are chosen to be Gaussian by design. \n* l. 161: The dependcy in $t$ should also appear in $\\varepsilon$, or speak only of the marginal of the process not the OU process itself.\n* l. 180: donot\n* l. 209: Say that $z(t)$ is the variance exploding counterpart\n* l. 214: space issue\n* l. 248: Algorithm For Diffusion -> for\n* l. 316: +1 is missing in first term of RHS.\n* l. 354: $T_{est}$ police issue for est\n* Several references are ArXiv of published papers, eg Benton et al 2023 -> ICLR 2024, Song et al 2023 -> ICML 2024."}, "questions": {"value": "The proposed scheme is:\nTwo time step backward EI ODE followed by one time step of forward SDE:\n\n$$\n\\hat{x}'_{k-0.5} = \\exp(h\\_{k}+h\\_{k-1}) \\hat{x}'\\_{k} + (\\exp (h\\_{k}+h\\_{k-1})-1)s(t_k,\\hat{x}'\\_{k})\n$$\n\n$$\n\\hat{x}'\\_{k-1} = \\exp (-h\\_{k-1}) \\hat{x}'\\_{k-0.5} + \\sqrt{1-e^{-2 h\\_{k-1}}}\\eta_k.\n$$\nThis can be written as a one step scheme:\n$$\n\\hat{x}'\\_{k-1} = \\exp (h\\_{k}) \\hat{x}'\\_{k} + (\\exp (h\\_{k})- \\exp (-h\\_{k-1}))s(t\\_k,\\hat{x}'\\_{k}) + \\sqrt{1-e^{-2 h\\_{k-1}}}\\eta_k.\n$$\n\nThis looks close to a convex combination of a backward EI step for the SDE and a backward EI step for the ODE (although not exactly). My question is: Can you interpret the proposed scheme as the discretization of a backward SDE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tuY1AUr20Y", "forum": "c8Ft3246KD", "replyto": "c8Ft3246KD", "signatures": ["ICLR.cc/2026/Conference/Submission8099/Reviewer_LSV4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8099/Reviewer_LSV4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917458129, "cdate": 1761917458129, "tmdate": 1762920081511, "mdate": 1762920081511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the convergence rate of DDPM in terms of KL divergence and establishes an iteration complexity of $O(d/\\varepsilon)$, improving upon previous results in Benton et al. (2023). In particular, the authors first quantify the discretization error via the corresponding ODE and then translate this error into a KL divergence bound by injecting noise into the current sample."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The theoretical result provides an improved convergence rate in KL divergence compared with Benton et al. (2023)."}, "weaknesses": {"value": "The main contribution of this paper lies in extending the previously established total variation bound to a KL divergence bound. However, this extension appears to be of limited significance, as it does not improve the dependence on either the dimensionality or the target accuracy.\n\nMore importantly, the authors claim to introduce a novel technique that **controls the KL divergence for the probability flow ODE with injected noise through the squared $l_2$ discretization error of the probability flow ODE (as stated in Lemma A.1)**. Nevertheless, this observation has been widely recognized and utilized in prior works, which the authors fail to acknowledge. \nA lot of researches has focused on reducing the KL divergence by designing ODE-based sampling methods aimed at improving the squared $l_2$ discretization error of the probability flow ODE. For example, [1] investigated a second-order strategy, [2,3] analyzed the randomized midpoint method, and they improve the convergence rate of diffusion models. While it is fair to note that this paper is among the first to apply such a technique to the first-order method (i.e., DDPM), the claimed methodological novelty is overstated.\n\nIn summary, given the limited theoretical advancement and the overstatement of technical novelty, I find it difficult to provide a positive assessment.\n\n[1]. Gen Li, and Changxiao Cai. Provable acceleration for diffusion models under minimal assumptions. arXiv preprint arXiv:2410.23285, 2024.\n\n[2]. Shivam Gupta, Linda Cai, and Sitan Chen. Faster Diffusion Sampling with Randomized Midpoints: Sequential and Parallel. arXiv preprintarXiv:2406.00924, 2024.\n\n[3] Gen Li, and Yuchen Jiao. Improved convergence rate for diffusion probabilistic models. The Thirteenth International Conference on Learning Representations, 2024."}, "questions": {"value": "Please refer to the Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "skFRJ7UwXo", "forum": "c8Ft3246KD", "replyto": "c8Ft3246KD", "signatures": ["ICLR.cc/2026/Conference/Submission8099/Reviewer_mkej"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8099/Reviewer_mkej"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244629031, "cdate": 1762244629031, "tmdate": 1762920081109, "mdate": 1762920081109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}