{"id": "Ry8jLSYIUG", "number": 1493, "cdate": 1756887299302, "mdate": 1759898206078, "content": {"title": "We Can Hide More Bits: The Unused Watermarking Capacity in Theory and in Practice", "abstract": "Despite rapid progress in deep learning–based image watermarking, the capacity of current robust methods remains limited to the scale of only a few hundred bits.\nSuch plateauing progress raises the question: how far are we from the fundamental limits of image watermarking?\nTo this end, we present analysis that establishes upper bounds on the message-carrying capacity of images under PSNR and linear robustness constraints. \nOur results indicate theoretical capacities are orders of magnitude larger than what current models achieve.\nOur experiments show this gap between theoretical and empirical performance persist, even in minimal and amenable to analysis setups.\nThis suggests a fundamental problem.\nAs a proof that larger capacities are indeed possible, we train ChunkySeal, a scaled-up version of VideoSeal, which has 4x larger capacity, i.e.,  1024 bits, all while preserving image quality and robustness. \nThese findings demonstrate modern methods have not yet saturated watermarking capacity, and that significant opportunities for architectural innovation and training strategies remain.", "tldr": "Our theoretical bounds show watermarks can encode orders of magnitude more bits than they currently do. Thus, we train a model that embeds 1024 bits with the same quality and robustness as a 256 bit model.", "keywords": ["watermarking", "theory", "provenance", "misinformation", "safety", "transparency"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b9197ac4e727181d9821109d7421d29266bf56e5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the theoretical capacity of image watermarking and highlights the large gap between theoretical upper bounds and practical deep learning performance. The authors derive capacity bounds under PSNR constraints by interpreting PSNR as an equivalent l2-ball constraint within the image cube. By counting integer points within the cube-ball intersection, they estimate the achievable bit capacity and demonstrate that current models (e.g., VideoSeal) use only a fraction of the potential.\nEmpirical results show that under a simple grayscale + PSNR setup, VideoSeal fails to reliably encode 1024 bits, while linear or handcrafted embedding-decoding schemes can succeed with 1024–2048 bits. The proposed method, ChunkySeal,  demonstrate that scaling up the baseline achieves 4× higher capacity (256 → 1024 bits) while maintaining quality and robustness, but still remains far from the theoretical limits."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Clear theoretical framework linking PSNR and l2 constraints : The cube–sphere intersection formulation provides intuitive and quantitative insight into capacity bounds.\n- Well-controlled experiments isolating key variables : Simplified settings (single grayscale image, PSNR constraint only) help pinpoint that architectural/optimization limits (not data or format) cause the current capacity gap.\n- Practical contribution (ChunkySeal) : A straightforward scaling of the embedder/extractor boosts performance, serving as a sanity check and strong baseline for future work.\n- Inclusion of robustness considerations: The paper extends its theory to cover linearized transformations (LinJPEG, rotation, scaling), proposing heuristic and conservative bounds."}, "weaknesses": {"value": "- Limited formal robustness analysis : The provided bounds for transformations (Bounds 10–13) are heuristic or overly conservative. Non-linear effects such as quantization and rounding are not analytically handled.\n- PSNR as a potentially weak perceptual proxy : The paper’s reliance on PSNR ignores perceptual discrepancies—two images with identical PSNR can differ visually. Extensions using LPIPS or MS-SSIM would better reflect real-world perceptual constraints.\n- Lack of in-depth analysis on model failure causes : While VideoSeal’s underperformance is empirically demonstrated, the architectural or optimization bottlenecks (e.g., skip connections, normalization, bandwidth limits) are not deeply dissected.\n- Simplified image-space assumption : The capacity derivation assumes a BMP-like uncompressed pixel grid. Real-world formats (JPEG, PNG) involve non-linear compression steps not fully captured, even with the LinJPEG approximation."}, "questions": {"value": "- Validity of PSNR and L2-ball equivalence : Have you tested whether two perturbations with equal PSNR but different visual artifacts yield consistent capacity results? Would using perceptual metrics (LPIPS, MS-SSIM) alter the theoretical limit?\n- Failure analysis of VideoSeal : What specifically prevents VideoSeal from scaling beyond 1024 bits—optimization instability, insufficient representation capacity, or architectural bottlenecks? Any diagnostic results (e.g., layer-wise activation spectra) to support this?\n- Completeness of Figure 2 cases : Does Figure 2 fully capture all geometric cases? What happens if the sphere’s center lies along cube edges or planes (partial overlap)? Are there discontinuities or nonlinear capacity changes in these intermediate configurations?\n- Image format generalization : How would your capacity estimation adapt to real formats like JPEG (non-linear quantization) or PNG (filter-based compression)? Can LinJPEG capture these effects accurately, or are there measurable deviations?\n- Theoretical limits vs. hyperparameter tuning : If a theoretical limit exists, why can’t it be reached through simple hyperparameter sweeps (e.g., reconstruction loss weight )? Is the gap due to optimization dynamics or representational constraints? A quantitative analysis (e.g., singular value decomposition of the embedding mapping) would clarify this.\n\nEfficiency and practicality of ChunkySeal.ChunkySeal reaches higher bit capacity but at the cost of ~760M parameters. How feasible is this in deployment scenarios? Could lighter architectures (e.g., tiled embeddings, structured transforms) achieve similar performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "c40b0G1aZa", "forum": "Ry8jLSYIUG", "replyto": "Ry8jLSYIUG", "signatures": ["ICLR.cc/2026/Conference/Submission1493/Reviewer_851W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1493/Reviewer_851W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761421003990, "cdate": 1761421003990, "tmdate": 1762915784586, "mdate": 1762915784586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a formalization of watermarking capacity and shows how many DL approaches to watermarking do not achieve this capacity. This work then proposes a new methodology that is able to use 1024 bits encoded."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper derives a first principal approach to understanding a fundamental question in watermarking: the theoretical limits of capacity that images can hold/embed under image quality and also robustness. Current literature usually uses on the order of 100-200 bits which is sufficient for many cases but this work highlights that this is under-represented. The authors show that using up to 1024 bits in practice has little to no performance drop."}, "weaknesses": {"value": "I think that the current suite of attacks are kind of basic. I would ideally like to see some more modern attacks (regeneration, rinsing, and maybe even a combination of a lot of attacks). I think that these settings can really test the robustness of the method."}, "questions": {"value": "- I would be curious to understand the theoretical formulation of a combination of attacks.\n- I would also like to see if there is a principled way to understand regeneration attacks in your current framework.\n- (The regeneration/other tests I asked for I mostly care about for empirical validation/comprehensiveness.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hDMBMBu3PN", "forum": "Ry8jLSYIUG", "replyto": "Ry8jLSYIUG", "signatures": ["ICLR.cc/2026/Conference/Submission1493/Reviewer_Rj44"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1493/Reviewer_Rj44"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761433857633, "cdate": 1761433857633, "tmdate": 1762915784456, "mdate": 1762915784456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether modern deep watermarking is approaching the capacity–quality–robustness limit. Rather than working within the classical Gel’fand-Pinsker problem [1], the authors adopt a geometric, high-dimensional grid view to analyze the trade-off between PSNR and capacity under perfect decoding, first in the noise-free case and then under linear distortions. Empirically, a handcrafted construction comes close to the PSNR-only bound, and an expanded model (Chunky Seal) achieves ~4× capacity while maintaining PSNR and robustness comparable to VideoSeal. These results suggest that current deep watermarking systems remain far from the achievable limit in terms of capacity at a given quality/robustness level.\n\n[1] S. Gelfand and M. Pinsker, “Coding for channel with random parameters,” Prob. of Control and Inf. Th., vol. 9, no. 1, pp. 19–31, 1980."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper tackles a fundamental and important question: after roughly several years of progress in deep learning–based watermarking, are we actually approaching the limit of the quality–robustness–capacity trade-off? Rather than starting from the classical information-theoretic setting, the authors proceed from a high-dimensional grid perspective and derive, step by step, the maximum information capacity, the capacity under a PSNR constraint, and the capacity under linear distortions.\n\n2.\tEmpirically, a handcrafted watermark in the noise-free setting approaches the theoretical upper bound, while under noise the Chunky Seal model achieves higher capacity yet similar PSNR and robustness to Video Seal, further indicating current limits of deep learning watermarking performance.\n\n3.\tThe theoretical development is reasonable and clear: it analyzes the limitations of deep models and articulates a plausible theoretical upper limit."}, "weaknesses": {"value": "1.\tThe related work is not sufficiently comprehensive. The paper does not adequately cite and explain existing traditional information-theoretic analyses, making it hard to evaluate the advantages of the proposed geometric high-dimensional grid approach over prior, thoroughly studied capacity analyses from the information-theoretic perspective.\n\n2.\tThe current capacity analysis remains limited to linear distortions; discussion of non-linear distortions is still quite limited.\n\n3.\tThe observed capacity gains via tiling are unusual, yet the paper provides little analysis of why this phenomenon occurs.\n\n4.\tIn the noise-free case, the paper proposes a handcrafted method that nearly attains the theoretical optimum; however, it remains unclear how to approach the theoretical capacity under noise.\n\n5.\tIn the high-capacity experiments, the paper does not compare against LISO [1], which achieves 4 bpp at ~25 dB PSNR with near-100% accuracy in the noise-free case. A study of high-capacity watermarking should analyze and compare with this method.\n\n[1] Chen X, Kishore V, Weinberger K Q. Learning iterative neural optimizers for image steganography. ICLR 2022."}, "questions": {"value": "1.\tFor non-linear distortions, if a theoretical analysis is not feasible, do the authors have empirical methods to predict or measure the upper-bound capacity?\n\n2.\tThe finding that tiling increases capacity is quite unexpected. Do the authors have an analysis of why this occurs? Why does direct training at high capacity tend to fail?\n\n3.\tThe straightforward expansion of Video Seal yields a model roughly 11× larger, yet the capacity is still only 0.0052 bpp. This does not appear to be a viable path toward the paper’s proposed theoretical upper limits. What new design ideas do the authors have for future models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pLFj3yBfVh", "forum": "Ry8jLSYIUG", "replyto": "Ry8jLSYIUG", "signatures": ["ICLR.cc/2026/Conference/Submission1493/Reviewer_GG8c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1493/Reviewer_GG8c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548152570, "cdate": 1761548152570, "tmdate": 1762915784295, "mdate": 1762915784295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}