{"id": "FF9QVQduAu", "number": 6598, "cdate": 1757990135808, "mdate": 1763722512299, "content": {"title": "Towards a Foundation Model for Crowdsourced Label Aggregation", "abstract": "Inferring ground truth from noisy, crowdsourced labels is a fundamental challenge in machine learning. For decades, the dominant paradigm has relied on dataset-specific parameter estimation, a non-scalable method that fails to transfer knowledge. We challenge this paradigm by introducing CrowdFM, a foundation model for label aggregation. At its core, CrowdFM is a bipartite graph neural network that is pre-trained on a vast, domain-randomized synthetic dataset. By leveraging a size-invariant initialization and attention-based message passing, it learns universal principles of collective intelligence and generalizes to new, unseen datasets. Extensive experiments on 22 real-world benchmarks show that our single, fixed model consistently matches or surpasses bespoke, per-dataset methods in both accuracy and efficiency. Furthermore, the representations learned by CrowdFM readily support diverse downstream applications, such as worker assessment and task assignment. Code and pre-trained models will be made publicly available upon acceptance.", "tldr": "We created CrowdFM, the first foundation model for label aggregation, which is pre-trained once on synthetic data to accurately combine noisy crowdsourced labels on any new dataset without retraining.", "keywords": ["Label Aggregation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/73d50d550e1d88bd099b0beb1c53b00bc23486bb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CrowdFM, a graph neural network-based \"foundation model\" for crowdsourced label aggregation, pretrained on synthetic data to enable retraining-free, cross-dataset generalization. The model is evaluated on 22 real-world datasets and shows strong empirical performance and efficiency. However, the main weakness of this paper is its failure to acknowledge and position itself with respect to highly relevant prior work—specifically, the hyper label model (Wu et al., ICLR 2023). The methodology, motivation, and even the technical design of CrowdFM are very similar to those of the hyper label model, with the only substantive difference being the application domain (crowdsourcing vs. programmatic weak supervision) and the technical details in training data generating and the graph neural network architecture design. The paper overstates its novelty and does not provide a meaningful discussion or comparison to this existing work. As a result, the contribution is incremental and the claims of being the \"first\" to propose such a foundation model are not justified."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper addresses a real and important challenge in crowdsourced label aggregation: the need for scalable, retraining-free aggregation methods that generalize across datasets.\n- The paper provides a synthetic data generation process for training such models.\n- CrowdFM achieves fast inference times, comparable to simple methods like Majority Voting, while outperforming more complex dataset-specific models."}, "weaknesses": {"value": "1.  Lack of proper acknowledgement of closely related work. The most significant issue with this paper is its failure to properly acknowledge and position itself with respect to existing work that is nearly identical in methodology and motivation. In particular, the ICLR 2023 paper[1] that proposes a hyper label model for programmatic weak supervision that is, in essence, the same as the approach in this paper:\n  - Both solve the label aggregation problem. \n  - Both aim for cross-dataset generalization and retraining-free inference.\n  - Both papers propose a GNN-based model that is pretrained on synthetic data to learn a generalizable label aggregation function.\n  - Both use size-invariant initializations and GNN architectures to handle variable numbers of annotators (workers/LFs) and tasks.\n  - Both demonstrate that their model, once trained, can be applied to new datasets in a single forward pass, outperforming dataset-specific methods in both accuracy and efficiency.\n\nWhile the paper claims to be the \"first foundation model for label aggregation,\" this is not accurate. The hyper label model[1] is a direct precedent, and the technical similarities are substantial. The only notable difference is the application domain: The hyper label model[1] focuses on programmatic weak supervision (labeling functions), while this paper focuses on crowdsourcing (human workers). However, the mathematical setup is essentially the same (a bipartite label matrix with noisy annotators) in these two domains.\nThe lack of discussion, comparison, or even citation of this highly relevant prior work is a serious omission. This gives the misleading impression that the proposed approach is novel, when in fact it is a straightforward adaptation of an existing method to a closely related domain.\n\n2. The paper repeatedly claims to be the \"first\" to propose a foundation model for label aggregation, and to introduce a new paradigm. Given the existence of[1], these claims are overstated.\n\n3.  Given the existence of the hyper label model approach[1], the main contribution of the paper seems to be modifications to the synthetic data generator and GNN architecture.\n\n[1] Wu et al., \"Learning Hyper Label Model for Programmatic Weak Supervision\" (ICLR 2023)"}, "questions": {"value": "1. Are the authors aware of the hyper label model[1]? Can the authors clarify the technical and conceptual differences between CrowdFM and the hyper label model, beyond the application domain? Please explicitly discuss this prior work in the related work section, and provide a detailed comparison (both conceptual and empirical) to clarify what is novel in CrowdFM.\n2. How does CrowdFM perform compared to the hyper label model?\n3. What new insights does CrowdFM provide for the crowdsourcing setting on top of what is discussed in[1].\n4. Is CrowdFM able to provide any theoretical guarantees as in[1].\n\n[1] Wu et al., \"Learning Hyper Label Model for Programmatic Weak Supervision\" (ICLR 2023)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AFzpEQAD1T", "forum": "FF9QVQduAu", "replyto": "FF9QVQduAu", "signatures": ["ICLR.cc/2026/Conference/Submission6598/Reviewer_QfN2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6598/Reviewer_QfN2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761371511023, "cdate": 1761371511023, "tmdate": 1762918923483, "mdate": 1762918923483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CrowdFM, the first foundation model for crowdsourced label aggregation. It replaces traditional per-dataset parameter estimation with a pretrained bipartite graph neural network trained on a vast, domain-randomized synthetic dataset. By leveraging a size-invariant initialization and attention-based message passing, it learns universal principles of collective intelligence and generalizes to new, unseen datasets. Experiments on 22 real-world datasets show that CrowdFM matches or outperforms state-of-the-art methods in both accuracy and efficiency, while its learned representations also support worker assessment and task assignment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.The proposed CrowdFM is the first foundation model for label aggregation, enabling cross-dataset generalization without retraining and marking a key step toward universal label aggregation.\n\n2.CrowdFM further exhibits strong versatility, as its pretrained representations can be directly adapted to multiple downstream tasks such as worker assessment and task assignment without additional training.\n\n3.Extensive experiments on 22 real-world benchmarks demonstrate that the proposed single, fixed model consistently matches or surpasses dataset-specific methods in both accuracy and efficiency."}, "weaknesses": {"value": "1.The paper lacks a systematic quantitative analysis of synthetic and real-world crowdsourced data. Although the authors validate generalization on real datasets, they do not analyze the distributional differences of key parameters (e.g., worker ability θ, task difficulty β, task guessing rate c) between synthetic and real data, making it difficult to assess how well the learned patterns reflect real human annotation behaviors. Besides, how to obtain these sampling ranges?\n\n2.Although the parameters in the synthetic data generator are randomly sampled, the paper lacks a sensitivity analysis of key parameters. Random sampling alone cannot fully verify the model’s robustness across different crowdsourcing conditions. It is recommended to include such an analysis to strengthen the credibility of CrowdFM’s generalization claims.\n\n3.With regard to the comparison results, statistical tests are needed in the comparison results. The detailed description about statistical tests for comparisons of multiple algorithms on multiple datasets can be found from the papers such as Statistical comparisons of classifiers over multiple data sets.\n\n4.The paper lacks ablation studies to verify the individual contributions of key components (e.g., attention-based aggregation, size-invariant initialization, and synthetic data diversity). Without such analysis, it is difficult to determine which design choices are most critical to the model’s performance gains."}, "questions": {"value": "The same as the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "enZwvoVcya", "forum": "FF9QVQduAu", "replyto": "FF9QVQduAu", "signatures": ["ICLR.cc/2026/Conference/Submission6598/Reviewer_7XU2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6598/Reviewer_7XU2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869959981, "cdate": 1761869959981, "tmdate": 1762918923027, "mdate": 1762918923027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Traditional methods (e.g., Dawid–Skene, GLAD, EBCC) typically adopt a dataset-specific modeling paradigm: they require training model parameters from scratch for each new dataset, resulting in poor generalization. Meanwhile, Majority Voting (MV), widely used in industry, requires no training but ignores differences in annotators' capabilities, leading to limited accuracy.\nTo bridge this gap, the authors propose a universal aggregation paradigm: by pre-training a bipartite GNN on large-scale, domain-randomized synthetic datasets, the model learns transferable \"collective intelligence\" aggregation rules. During the inference phase, the model can be directly applied to any real crowdsourced dataset without fine-tuning or retraining."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tStrong paradigm innovation: For the first time, the foundation model concept was introduced into crowdsourced label aggregation, breaking the dataset-specific paradigm that has persisted for decades.\n2.\tThe problem definition has practical significance: Shifting the aggregation of crowdsourced labels from the \"dataset-by-dataset modeling\" paradigm to the \"unified fundamental model\" paradigm aligns with the urgent demand of the industrial sector for scalable and retraining-free systems.\n3.\tThe experiments were thorough: 22 real datasets covered text, images, and audio, and included extreme scale/density scenarios, verifying the generalization ability.\n4.\tThe accuracy rate and running time were reported, and it was pointed out that some methods failed due to insufficient memory"}, "weaknesses": {"value": "1.\tHow can we ensure that the distribution of synthetic data is sufficient to cover the feature space of real crowdsourcing tasks? Is there any performance degradation under certain types of tasks (such as extremely unbalanced ones)?\n2.\tCan CrowdFM be regarded as the specialization of GraphFM in the field of crowdsourcing? Has the pre-training objective of the existing GFM been borrowed?\n3.\tThe performance for extremely large-scale data (such as Senti and Fact) slightly decreases, but the reasons are not discussed.\n4.\tThe absence of ablation experiments makes the contributions of each module unclear\n5.\tAlthough it includes recent works such as EBCC, GOVERN, TiReMGE, etc., some new methods for 2024-2025 are missing (such as Zhang et al., KFNN and IWBVT of NeurIPS 2024 are cited but not used as baselines);\n6.\tThe superparameters such as the number of layers L and dimension d of GNN were not discussed in detail.\n7.\tThe \"Sim-to-Real Gap\" between synthetic data and real data has not been fully quantified\n8.\tHas the model truly learned the \"principle of collective intelligence\", or has it merely fitted the statistical patterns of synthetic data? How to prove."}, "questions": {"value": "Same as weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9KfRZDWWAo", "forum": "FF9QVQduAu", "replyto": "FF9QVQduAu", "signatures": ["ICLR.cc/2026/Conference/Submission6598/Reviewer_BaKD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6598/Reviewer_BaKD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989504904, "cdate": 1761989504904, "tmdate": 1762918922652, "mdate": 1762918922652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims at building a foundation model for crowdsourced label aggregation that generalizes across heterogeneous datasets without dataset-specific training. It first generates a synthetic crowdsourced data generator to produce diverse synthetic datasets, and then trains a bipartite graph neural network which can be generalizable to other data and tasks. Experiments show the foundation model is efficient and achieve performance comparable to or superior to state-of-the-art aggregation methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Generating the synthetic data and then train a generalizable GNN model to approximate it is a smart idea to unify the data-specific crowdsourcing models. I think it is essentially a type of knowledge distillation, which distills the knowledge from the data generator to a GNN with less input features (e.g. task difficulty/worker ability, which are generally regarded as latent parameters in traditional probabilistic models). \n2. The method is sound, and the presentation is clear.\n3. Experiments show good accuracy with more efficiency."}, "weaknesses": {"value": "1. The backbone model is relatively simple and not new, but the design is a good fit for crowdsourcing, e.g. the size-invariant initialization for worker/task emebddings. I do not see it as a weakness, but just not a novel contribution. I still like the whole framework of building synthetic data and train a simple domain-indepedent model as foundation models.\n\n2. The possible limitation of the model is it may lack the flexibility to adapt to the case when workers/tasks have their attributes, since these nodes are initialized with fixed pretrained embeddings.\n\n3. Another possible weakness is the limitation of the data generator. Although it is a quite general scheme, there might be other graphical models that it cannot cover; and also the prior distribution of the parameters might cause some bias for the pretrained foundation model."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uqv9k29RNr", "forum": "FF9QVQduAu", "replyto": "FF9QVQduAu", "signatures": ["ICLR.cc/2026/Conference/Submission6598/Reviewer_mWdt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6598/Reviewer_mWdt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762314254365, "cdate": 1762314254365, "tmdate": 1762918922309, "mdate": 1762918922309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}