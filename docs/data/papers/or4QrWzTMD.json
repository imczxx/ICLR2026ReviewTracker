{"id": "or4QrWzTMD", "number": 13854, "cdate": 1758223747120, "mdate": 1759897408526, "content": {"title": "UWYN (Use Only What You Need): Efficient Inferencing in 2D and 3D Vision Transformers", "abstract": "Efficient computation for Vision Transformers (ViTs) is critical for latency-sensitive applications. However, early-exit schemes rely on auxiliary controllers that introduce non-trivial overhead. We propose UWYN, an end-to-end framework for image classification and shape classification tasks that embeds exit decisions directly within the transformer by reusing the classification head at each residual block. UWYN first partitions inputs via a lightweight feature-threshold into “simple” and “complex” samples: simple samples are routed to a shallow ResNet branch, while complex samples traverse the ViT and terminate as soon as their per-block confidence exceeds a preset confidence level. During the ViT pass,UWYN also dynamically prunes redundant patch embeddings and attention heads to further cut computation. We implement and evaluate this strategy on both 2D (ImageNet, CIFAR-10, CIFAR-100, SVHN, BloodMNIST) and 3D (ModelNet-40, Scan Object NN) benchmarks. UWYN reduces Multiply-Accumulate operations (MACs) by over 75% compared to SOTA models, e.g., LGViT [ACM MM ’23] achieving 83.29% accuracy on CIFAR-100 and 84.39% on ImageNet. We also show faster inference with minimal accuracy loss.", "tldr": "We enable image and shape classification with multiple orders of magnitude lesser MACs, through a novel early exit Vision Transformer coupled with a simpler ResNet model for simpler data instances.", "keywords": ["Efficient image classification", "Efficient shape classification", "Transformers", "Early exit"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f1c5a233b0f818a45f709c4dce505ddf91ee218.pdf", "supplementary_material": "/attachment/4b8e0cfc8e49d3b43354f1ec08eacfa07ac2a404.pdf"}, "replies": [{"content": {"summary": {"value": "The paper presents the UWYN (use only what you need) method for efficient inference in ViTs. This algorithm increases efficiency by, among other things, utilizing an early-exit policy for classification, routing easy-to-classify samples prior to sending them through the full model, and pruning attention heads and patches which contain redundant information. The early exit policy has little parameter/compute overhead as it utilizes the existing classification head at each residual block. In both 2D and 3D datasets, this method exhibits minimal accuracy loss with significant increases in inference speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of this paper include the introduction of a method that requires very little training to significantly increase the efficiency of ViTs through routing, early exit and patch/head pruning. The method takes advantage of the redundancy that exists in these models to compress and extract information more efficiently. Finally, the method is comprehensibly tested through several benchmarks showing significant improvement for model efficiency without substantially decreased performance. It seems that the combination of all these elements is a novel contribution to this area of research. I also think the routing pre-ViT is a clever idea."}, "weaknesses": {"value": "I think there are a lot of strengths in the paper. That being said, I was very confused how the head/patch pruning model was trained. Specifically, how do you handle the nondifferentiality of subselecting a subset of heads/tokens to use? Additionally, the overall architecture of the model seemed somewhat vague. For example, in the ViT, how were the FFNs specified?"}, "questions": {"value": "Please describe how the head/patch pruner is trained.\n\nPlease give a more detailed description of the architecture for the models used in the evaluations in section 4.\n\nOut of curiosity, what would happen to the performance if you had a strict budget instead of a threshold for heads/pruned patches (e.g. only utilize the top k heads/patches per layer)? I suppose this would be more difficult to train. \n\nHow was the pre-ViT router developed? do you think would it be better to train this for each individual model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hEHv4CEkxv", "forum": "or4QrWzTMD", "replyto": "or4QrWzTMD", "signatures": ["ICLR.cc/2026/Conference/Submission13854/Reviewer_eXmY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13854/Reviewer_eXmY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595877278, "cdate": 1761595877278, "tmdate": 1762924375381, "mdate": 1762924375381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method that incorporates the both a selective inference strategy with a parallel, hybrid architecture with ViT/CNN, and a block-wise early-stop and attention pruning strategy for ViT. The architecture can be used for 2D/3D classification, and achieves better results than its selected baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The hybrid dual branch + head pruning method is quite intuitive and easy to understand\n- The experiment results are competitive against its baselines"}, "weaknesses": {"value": "- The novelty of the method is very limited, as both main pruning techniques involved in the framework (hybrid architecture/dual branch, early stopping/head pruning) are not original\n- Vast majority of the baseline methods compared in the experiments are from 2022-23 era. This is a fast-moving research area and newer baselines are lacking.\n- Details are lacking for how UWYN implements the object detection task.\n- Minor issues  - line 336 PartialFormer (?)"}, "questions": {"value": "- How dod you come up with the #param number for UWYN? ResNet50 alone has 23.9M params yet the paper reports 22.86m\n- In reality, I assume you'll have to load both branches in the memory for inference. Is there an evaluation on how much memory overhead this dual-branch design would bring?\n- Can UWYN handle dense prediction tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sNiTglbEDA", "forum": "or4QrWzTMD", "replyto": "or4QrWzTMD", "signatures": ["ICLR.cc/2026/Conference/Submission13854/Reviewer_3j3z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13854/Reviewer_3j3z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711167529, "cdate": 1761711167529, "tmdate": 1762924374932, "mdate": 1762924374932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of dynamically allocating computational resources in efficient ViT based on input complexity. Specifically, simple inputs are processed by a shallow ResNet for efficiency, while complex inputs are routed to a ViT equipped with an early-exit mechanism for enhanced feature extraction. In the ViT branch, a classifier is appended to each block to determine whether the confidence of the predicted class exceeds a predefined threshold, triggering an early stop if satisfied. Experimental results demonstrate the effectiveness of the proposed framework."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The idea of dual-branch architectural design for inputs of different complexity is interesting. And the input complexity analysis via Sobel convolution is efficient.\n\n2. The method is introduced in detail, enhancing its reproducibility.\n\n3. The efficiency comparisons are conducted on edge devices."}, "weaknesses": {"value": "1. Messy format. Please use \"[]\" for in-line references rather than \"()\" to distinguish different usages. There are too many brackets in the manucsript.\n\n2. Some state-of-the-art efficient ViT methods are missing, such as [1,2,3].\n\n[1] Vasu, Pavan Kumar Anasosalu, et al. \"Fastvit: A fast hybrid vision transformer using structural reparameterization.\" ICCV, 2023.\n\n[2] Hatamizadeh, Ali, et al. \"Fastervit: Fast vision transformers with hierarchical attention.\" ICLR, 2024.\n\n[3] Wang, Ao, et al. \"Repvit: Revisiting mobile cnn from vit perspective.\" CVPR, 2024."}, "questions": {"value": "Refer to the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2rA7FFpGCs", "forum": "or4QrWzTMD", "replyto": "or4QrWzTMD", "signatures": ["ICLR.cc/2026/Conference/Submission13854/Reviewer_ya9Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13854/Reviewer_ya9Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801558963, "cdate": 1761801558963, "tmdate": 1762924374256, "mdate": 1762924374256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}