{"id": "173Pq3F31r", "number": 12180, "cdate": 1758206197588, "mdate": 1759897526988, "content": {"title": "Bridging Piano Transcription and Rendering via Disentangled Score Content and Style", "abstract": "Expressive performance rendering (EPR) and automatic piano transcription (APT) are fundamental yet inverse tasks in music information retrieval: EPR generates expressive performances from symbolic scores, while APT recovers scores from performances. Despite their dual nature, prior work has addressed them independently. In this paper, we propose a unified framework that jointly models EPR and APT by disentangling note-level score content and global performance style representations from both paired and unpaired data. Our framework is built on a transformer-based sequence-to-sequence (Seq2Seq) architecture and is trained using only sequence-aligned data, without requiring fine-grained note-level alignment. To automate the rendering process while ensuring stylistic compatibility with the score, we introduce an independent diffusion-based performance style recommendation (PSR) module that generates style embeddings directly from score content. This modular component supports both style transfer and flexible rendering across a range of expressive styles. Experimental results from both objective and subjective evaluations demonstrate that our framework achieves competitive performance on EPR and APT tasks, while enabling effective content–style disentanglement, reliable style transfer, and stylistically appropriate rendering. Demos are available at https://jointpianist.github.io/epr-apt/.", "tldr": "We propose a unified framework for piano transcription and rendering.", "keywords": ["piano transcription", "expressive performance rendering", "disentangled representation learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d58d6a595d5c5c85055a3bf8a0b14a6a6826124e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a unified transformer-based framework for joint expressive performance rendering (EPR) and automatic performance transcription (APT). The model disentangles note-level score content from global performance style representations using both paired and unpaired data, trained solely with sequence-level alignment. One of the contributions is brought by a diffusion-based performance style recommendation (PSR) module that generates style embeddings directly from the score, ensuring stylistic coherence and enabling flexible style transfer. Another contribution includes a seq2seq formulation of EPR without note-level alignment which avoids the need for finely aligned training data. Objective and subjective evaluations demonstrate that the proposed framework achieves competitive performance on EPR and APT tasks, while disentangling content and style to support reliable and stylistically consistent rendering."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strengths of the paper are:\n- The proposal of a unified model for joint expressive performance rendering and automatic performance transcription is novel and sound, although the concept of performing joint tasks (such as recognition or synthesis) is not new\n- The paper is nicely written\n-   Experimental plan is solid and includes comparisons with several baselines, and a perceptual test\n- The (online) demo is convincing"}, "weaknesses": {"value": "The main weaknesses of the paper are:\n-\tThere are no evidence that the results of the proposed method are statistically different from those obtained by the baseline methods\n-\tThe perceptual test is conducted on only five pieces, which limits the generalizability of the results across a broader range of styles, despite the examples being drawn from different composers.\n-\tWhy there is disentanglement of score content and performance style representations is not clearly described.\n-\tThere is no figure of complexity of the model (training and inference)"}, "questions": {"value": "1. Why statistical relevance of the results is not included (table 1 to 3) ?\n2. The score encoder f_g,X(.) is introduced in section 3.4. What is the difference of this score encoder from the content encoder for score f_c,X(.) ? is this additional notation needed ?\n3. The disentanglement of score content and performance style representations is claimed but not really clear. What in the method facilitates the disentanglement ? (in other word, why distanglement is achieved)  ?\n4. It does not seem obvious to conclude from figure 4 that the PSR generated styles closely mirror those extracted from real performances. Can you further elaborate why this can be concluded ?\n5. What is the complexity of the model (training and inference) compared to baselines ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wX8NV1ww8X", "forum": "173Pq3F31r", "replyto": "173Pq3F31r", "signatures": ["ICLR.cc/2026/Conference/Submission12180/Reviewer_uBVc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12180/Reviewer_uBVc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761301341057, "cdate": 1761301341057, "tmdate": 1762923131585, "mdate": 1762923131585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unified, transformer-based framework for jointly modeling Expressive Performance Rendering (EPR) and Automatic Piano Transcription (APT), two fundamental and inverse tasks in Music Information Retrieval (MIR). The core contribution lies in the effective disentanglement of note-level score content ($Z_c$) and global performance style ($Z_s$) representations. This is achieved through a multi-task, sequence-to-sequence architecture trained on both paired (ASAP) and unpaired (MuseScore, YouTube) data.\n\nA key strength of this framework is its ability to operate on sequence-aligned data, removing the common and cumbersome dependency on fine-grained, note-level alignment. Furthermore, the authors introduce a novel diffusion-based Performance Style Recommendation (PSR) module, which learns to generate appropriate and diverse style embeddings ($Z_s$) conditioned solely on the score content. This enables high-quality, automated expressive rendering without requiring a reference performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The joint modeling of the inverse EPR and APT tasks is an elegant and powerful formulation. By leveraging the duality of these tasks (EPR learns to add style, APT learns to remove it), the model develops a robust and semantically meaningful disentanglement of content and style. This joint approach is a novel contribution to the field of symbolic music modeling.\n\n* The most significant practical contribution is the move away from note-level alignment. The reliance on finely-aligned datasets has been a major bottleneck in performance modeling, as such datasets are small and expensive to create. By demonstrating competitive results using only sequence-level alignment (and further leveraging unpaired data), this work opens the door to training on much larger and more diverse datasets, which is a crucial step forward for the field.\n\n* The methodology is sound and the paper is technically strong.\n  * The architectural design choice (note-level sequence $Z_c$ vs. global single-vector $Z_s$) is a simple and effective inductive bias for disentanglement.\n  * The introduction of the diffusion-based PSR module is a creative and modern approach to solving the problem of \"what style to apply\" when no reference is given.\n  * The experimental validation is thorough. It includes comprehensive objective metrics for both APT (MUSTER, ScoreSimilarity) and EPR (KL, MAE, alignment), as well as crucial subjective listening tests that confirm the quality of the generated performances.\n  * The disentanglement is not just claimed but empirically verified through well-designed identification tasks (Table 4) and latent space visualizations (Fig 3), which clearly show the style vector $Z_s$ captures performer/composer information while $Z_c$ does not.\n\n* The paper is exceptionally well-written and easy to follow. Figure 1 provides an excellent and clear overview of the complete framework and the relationships between the different tasks. The concepts of content/style and the mechanism for disentanglement are explained intuitively."}, "weaknesses": {"value": "* The framework is evaluated exclusively on classical piano music. While this is a standard benchmark, the authors' claim of style disentanglement would be further strengthened by testing on genres with more starkly different expressive conventions (e.g., jazz, pop). The authors note this as future work, but a brief discussion of the potential challenges (e.g., improvisation, complex swing rhythms) in extending the model would be beneficial.\n\n* The use of unpaired data from YouTube, transcribed via a SOTA audio-to-MIDI model, is a clever strategy for data augmentation. However, this introduces a potential domain bias. The style encoder might be learning artifacts specific to that transcription model rather than \"true\" human performance style. The results seem robust, but a brief acknowledgment or discussion of this potential methodological caveat would be appropriate."}, "questions": {"value": "1. The PSR module uses a global content embedding $e_g$ to generate a single, global style vector $Z_s$. Does this global vector have sufficient capacity to guide performances where the intended style changes significantly between sections (e.g., a piece with a slow, gentle introduction (Adagio) followed by a fast, aggressive main theme (Presto))? Or does the model tend to \"average out\" the style over the entire piece?\n\n2. Regarding the unpaired performance data (transcribed from YouTube): Did the authors observe any \"model-like\" artifacts in the learned style representations, or does the model seem robust to this potential domain shift? For example, does the style space show any clustering based on the source of the data (ASAP vs. YouTube)?\n\n3. Related Work Suggestion: The paper's contribution of leveraging sequence-aligned data over note-level alignment is highly significant. We strongly suggest the authors also discuss and cite the recent work by Jung et al. (2025), \"Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio\" (arXiv:2505.12863v1). This paper shares a similar and ambitious motivation of unified multi-modal translation (extending to score images and audio) and also highlights the importance of facilitating large-scale learning from sequence-aligned pairs, rather than relying on strict note-level alignment. Citing this work would help to better contextualize the paper's contribution within this important and emerging research trend."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yMNXV3yzPx", "forum": "173Pq3F31r", "replyto": "173Pq3F31r", "signatures": ["ICLR.cc/2026/Conference/Submission12180/Reviewer_eiLe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12180/Reviewer_eiLe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551604697, "cdate": 1761551604697, "tmdate": 1762923131259, "mdate": 1762923131259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified transformer-based framework for jointly modeling expressive performance rendering (EPR) and automatic piano transcription (APT) by disentangling note-level score content and global performance style representations. It leverages paired and unpaired data without requiring note-level alignment, and introduces a diffusion-based performance style recommendation (PSR) module to generate style embeddings from scores alone. The authors claim competitive performance on benchmarks, effective disentanglement for style transfer, and stylistically appropriate rendering. Contributions include the joint modeling approach, the PSR module, and an alignment-free Seq2Seq formulation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper demonstrates solid experimental validation, achieving competitive results on EPR and APT tasks using the ASAP dataset, with both objective metrics (e.g., MUSTER and ScoreSimilarity for APT, variance/KL/MAE for EPR) and subjective listening tests. \n\nThe disentanglement of content and style is a meaningful addition, enabling style transfer and recommendation, which adds interpretability and controllability to the tasks. The use of unpaired data and masked reconstruction objectives allows scalable training without strict alignment, addressing a practical limitation in prior work. \n\nOverall, this represents an incremental but technically sound advancement in the niche area of symbolic music processing, with clear significance for music information retrieval applications like education and analysis."}, "weaknesses": {"value": "The novelty is somewhat limited for the broader ICLR community, as the core architecture—a dual-tower encoder-decoder setup with disentangled representations—builds on classic Seq2Seq models like MASS, without introducing fundamentally new insights that could generalize beyond music tasks. \n\nWhile the style disentanglement is emphasized, the paper does not deeply address challenges in score readability; for instance, the transcribed scores exhibit errors in time signatures and poor note readability (e.g., improper grouping or stem directions), which is a critical limitation for practical APT applications, as each performer's bias introduces non-strict alignments that require more sophisticated post-processing. \n\nExperiments are adequate but could be strengthened with more diverse OOD evaluations or comparisons to recent baselines like those in Zhang et al. (2024) for EPR. Additionally, the reusable insights (e.g., for general disentangled learning) are constrained to this small domain, reducing broader impact."}, "questions": {"value": "1. Could the authors clarify how the model handles performer-specific biases in score-performance alignment? For example, in APT, how do you ensure the output scores are readable and musically coherent (e.g., correct time signatures, proper note grouping)? The examples in the paper seem to have errors in this regard—would incorporating additional regularization or post-processing improve this, and have you experimented with it?\n\n2. For the PSR module, how does it perform on scores from underrepresented genres or composers? Could you provide quantitative evidence (e.g., style appropriateness metrics) on OOD data like ATEPP to show generalization?\n\n3. The paper mentions competitive performance, but some metrics (e.g., onset/offset deviations in APT) lag behind alignment-dependent baselines. What additional experiments (e.g., with more unpaired data or fine-tuning) might close this gap, and could this change the evaluation of the alignment-free approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xxV3LvrM6f", "forum": "173Pq3F31r", "replyto": "173Pq3F31r", "signatures": ["ICLR.cc/2026/Conference/Submission12180/Reviewer_zyQd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12180/Reviewer_zyQd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883474799, "cdate": 1761883474799, "tmdate": 1762923129873, "mdate": 1762923129873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a model for joint transcription and expressive performance rendering for classical piano music. The model is based on a VAE-like architecture with a performance style recommendation module to better sample performance latent code."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The task of jointly modeling performance and score is very important in computer music, especially in symbolic music understanding and generation. The content-style disentanglement is a reasonable way to jointly model music performance and score, and the methodlogy is in general novel. \n\nThe paper is generally well-written."}, "weaknesses": {"value": "There are many things unclear about the model's performance:\n\n1. It is unclear whether the joint architecture improves the performance compared to single model. The ablation study in D.3 does not stress this point.\n\n2. It is uncleared whether the disentanglement works on unpaired data. The framework lacks explicit inductive biases (except for KL loss) to perform such disentanglement. All composer and performer in Fig. 3 (assumed also used in Tab. 4) seem to be included in the paired dataset (correct me if I am wrong), so the model's disentanglement ability on unseen performer/composer remains unclear.\n\n3. It is uncleared whether the unpaired data helped or not. As shown in Table 11, the effect of unpaired data on APT performannce is quite limited, and the performance does not always increase when you put in more unpaired data.\n\nFig 8-13 are very hard to read. \n\n1. It would be better to combine them into a single sheet music, with different colors (e.g., red: missing, blue extra) to denote inconsistency between the ground truth and the prediction.\n2. I totally understand that the model does not predict a key signature, but it would still be better to show Fig. 13 under the ground-truth key signature, otherwise it makes little sense to a musician.\n\nLastly, the performance of the system on APT is far from practical use. I understand that performance to score is considered a very hard task in modern MIR, so it is generally hard to improve. Still, I do not think this paper made significant contribution to the task."}, "questions": {"value": "Sec 3.2: \"Our framework supports training on both paired and unpaired data.\" - again, does it support training only on unpaired data?\n\nIn the demo page, most human performance have sudden note cutoffs that are very unnatural, which I suspect to be some errors in data preprocessing. This might affect the subjective evaluation as well. Could the author double-check the rendering and MIDI preprocessing code?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u0Eaki8mov", "forum": "173Pq3F31r", "replyto": "173Pq3F31r", "signatures": ["ICLR.cc/2026/Conference/Submission12180/Reviewer_dQyX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12180/Reviewer_dQyX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996559672, "cdate": 1761996559672, "tmdate": 1762923129119, "mdate": 1762923129119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}