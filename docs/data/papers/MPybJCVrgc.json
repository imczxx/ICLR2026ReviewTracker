{"id": "MPybJCVrgc", "number": 2231, "cdate": 1757037965447, "mdate": 1759898161517, "content": {"title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for Large Language Models", "abstract": "Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantizaiton and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation Al acceleractors.", "tldr": "We propose SQ-format, a hybrid-precision data format that achieves lower bit-width and higher throughput without sacrificing accuracy, guiding next-generation hardware design.", "keywords": ["Post-Training Quantization", "Mixed-precision representation", "Hardware acceleration", "Large Language Models"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/855cf60fc68b59537eb8f30af7a3ff56a966c2be.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SQ-Format for post-training quantization (PTQ). The format divides a tensor into blocks, and in each block, a portion of elements (controlled by sparsity ratio s) are quantized to a higher precision such as INT8, while the rest are quantized to a lower precision such as INT4. The goal is to leverage the sparsity of high-precision elements and achieve speedups close to full low-precision matmuls. The authors propose two algorithms for compressing weights and activations to SQ-Format. Their accuracy results are promising (both for weights and static/dynamic activations)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written.\n- Extensive experiments + good set of baselines.\n- Interesting algorithmic contributions, including the static activation masks."}, "weaknesses": {"value": "- The idea is not novel. Quantization + higher precision outlier formats have been considered by multiple works including SpQR (which the paper cites) and QUIK [1].\n- The runtime improvement are more like a promise (if such hardware is designed), hence calling the method hardware-friendly in the title is a bit questionable.\n- I am not convinced by the speedups. While there seems to be a kernel implemented for the static version, its throughput is only reported on a single matmul.\n\n[1] https://arxiv.org/pdf/2310.09259"}, "questions": {"value": "1. Regarding the first two contribution listed in the intro (SQ-Format definition and implementation / Pareto improvement): I believe the definition is not novel, as I mentioned above. Additionally, can the authors provide a clear plot of accuracy vs runtime to show pareto superiority against other baselines (static activations would suffice, as you already have kernels for them)? In the current state, I'm not convinced merely from Tables 1 and 2 that there's pareto improvement.\n\n2. More broadly, can you include speedup numbers in Tables 1 and 2 (for weights and static activations)?\n\n3. Would it be possible to apply the SQ-Format to both operands? What are the challenges? A brief discussion would suffice.\n\n4. The algorithm picks the most \"important\" elements to keep in higher precision. Aside from importance, looking at the low-precision quantization error could also interesting here. For example, if an important element can be perfectly captured by the low-precision component, then there is no reason to keep it in high precision. As a suggestion, wouldn't a combination of importance and low-precision error potentially achieve better results?\n\n5. Any idea why the static activation strategy prefers smaller bank sizes? To me it seems counter-intuitive.\n\nIn general, I believe the paper's novelty is somewhat questionable, and the pareto superiority is not convincing. I would be open to increasing my score, depending the authors' rebuttal regarding novelty and questions 1 and 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lgWEYZhXvZ", "forum": "MPybJCVrgc", "replyto": "MPybJCVrgc", "signatures": ["ICLR.cc/2026/Conference/Submission2231/Reviewer_CDnV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2231/Reviewer_CDnV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733648780, "cdate": 1761733648780, "tmdate": 1762916154333, "mdate": 1762916154333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Post-training quantization (PTQ) is crucial for LLM deployment, but current hardware makes low-bit quantization and sparsification hard to balance for accuracy and efficiency (e.g., W4A8 offers similar peak TOPS as W8A8; GPU 2:4 sparsity often hurts accuracy). The authors propose a unified Sparse‑Quantized Format (**SQ‑format**) that leverages high‑precision sparse acceleration and extends it to low‑precision matmul, enabling static compression for outlier‑skewed activations and yielding Pareto gains in performance vs. throughput on existing GPUs and future hardware. They report state‑of‑the‑art PTQ results, specify required hardware support, and provide design insights for next‑generation AI accelerators."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well organized.\n2. This paper proposes a novel quantization format.\n3. The proposed method shows SOTA performance."}, "weaknesses": {"value": "1. I think the results of this paper are not easy to reproduce, since the authors do not include code. It is hard to believe the training-free approach can achieve much better performance than training-based SpinQuant, as demonstrated in the paper.\n2. The baseline performance of this paper is inconsistent with their original paper.\n3. The authors do not include E2E speedup results and memory costs of the proposed method. This is very important for the application of the proposed format. Only theatrical analysis is not reasonable.\n4. This paper has claimed that the proposed method supports FP quantization. I believe they should include results for such a setting."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "usZNlQ1493", "forum": "MPybJCVrgc", "replyto": "MPybJCVrgc", "signatures": ["ICLR.cc/2026/Conference/Submission2231/Reviewer_3cJk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2231/Reviewer_3cJk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751730828, "cdate": 1761751730828, "tmdate": 1762916153805, "mdate": 1762916153805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of efficiently deploying large language models under both quantization and sparsity constraints, aiming to reduce model size and computation while maintaining accuracy. The authors propose SQ‑format, a unified sparse-quantized data format that encodes weights and activations in mixed precision and uses masks to indicate high-precision elements, allowing block-wise efficient computation. They introduce algorithms to determine which elements to quantize at low precision for weights and activations, using either static or dynamic mask strategies. Extensive experiments on LLaMA‑3 and Qwen‑3 models show that SQ‑format achieves comparable or better accuracy than prior quantization or sparsity methods while improving throughput and hardware efficiency. SQ‑format provides a hardware-friendly approach for mixed sparse-quantized LLMs, enabling practical deployment on accelerators without sacrificing performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- SQ‑format combines sparsity and quantization in a single representation, facilitating efficient computation on modern hardware.\n\n- The paper carefully considers practical deployment, proposing both static and dynamic mask strategies to balance accuracy and efficiency.\n\n- Evaluations on multiple LLMs (8B–70B) with standard benchmarks demonstrate that SQ‑format preserves accuracy while improving throughput.\n\n- Addresses a key deployment challenge for LLMs, bridging the gap between algorithmic innovations and hardware execution."}, "weaknesses": {"value": "- Limited comparison to extreme low-bit settings: The paper mainly evaluates INT4–INT8 and moderate sparsity; performance in ultra-low bit scenarios (e.g., W4A4) is unclear.\n\n- Complexity of mask design: Dynamic mask selection may introduce runtime overhead, and static masks require careful calibration, which may complicate practical adoption.\n\n- Specific to current hardware: While hardware-friendly, the proposed format is tuned to modern GPUs; applicability to other accelerators (TPU, AI chips) is not fully validated.\n\n- Additional storage overhead: Maintaining masks for sparse/high-precision elements increases memory usage, which may be non-trivial for very large models.\n\n- The algorithm design is simple and lacks novelty, but it imposes a very heavy burden on deployment. Although the final results are indeed good, there are still concerns about the future prospects of this method."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qCaWkzi2Dc", "forum": "MPybJCVrgc", "replyto": "MPybJCVrgc", "signatures": ["ICLR.cc/2026/Conference/Submission2231/Reviewer_Bt9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2231/Reviewer_Bt9D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828357712, "cdate": 1761828357712, "tmdate": 1762916153522, "mdate": 1762916153522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SQ‑format, a unified, hardware‑friendly data format that combines sparsification and quantization for large language models (LLMs). It encodes a tensor as a sparse high‑precision component for critical values and a dense low‑precision component for the rest, thereby achieving a better trade‑off between accuracy and throughput. The authors present algorithms to apply SQ‑format to weights and activations, and provide hardware design insights. Empirical results on multiple LLMs show that SQ‑format reaches near W4A8 accuracy while maintaining W4A4‑level throughput."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper introduces the SQ-format, which integrates sparsification and quantization into a unified data representation, bridging the gap between algorithmic compression and hardware efficiency for improved throughput and accuracy.\n\nThis paper introduces SQ-format for weights and activations, achieving significant throughput gains while maintaining near W4A8-level accuracy, effectively balancing efficiency and performance.\n\nThis paper introduces a static activation splitting strategy that reduces runtime overhead, making SQ-format more practical for deployment on current AI accelerators."}, "weaknesses": {"value": "1. The paper proposes a unified sparse + quantized format (SQ‑format) combining high‑precision for critical values and low‑precision + sparsity for the rest. However, previous work, such as SpQR: A Sparse‑Quantized Representation for Near‑Lossless LLM Weight Compression (Dettmers et al., 2023), already investigates the idea of preserving a small subset of weights in high precision and quantizing the remainder. While SQ‑format adds the “bank” structure and hardware‑mapping discussion, the paper could do more to clearly highlight what is novel beyond those prior methods.\n\n2. The authors argue that SQ‑format is “hardware‑friendly” and outline required hardware support, but they offer only simulation or theoretical throughput estimates—not measured latency, power, or memory‑bandwidth results on real GPUs or accelerators."}, "questions": {"value": "1. How do you select the ratio of high‑precision elements in the “bank” structure (bank size  b, sparsity rate s) as model size increases (e.g., 8B → 70B)?\n\n2. Can you quantify the extra memory bandwidth or branching overhead introduced by decoding the SQ‑format (high/low precision mix + sparsity mask) compared to a uniform low‑precision format?\n\n3. If activation distributions shift (e.g., instruction‑tuned or domain‑adapted LLMs), how robust is the static activation split strategy, and what is the accuracy or latency impact?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kPJe7QyqjQ", "forum": "MPybJCVrgc", "replyto": "MPybJCVrgc", "signatures": ["ICLR.cc/2026/Conference/Submission2231/Reviewer_eKea"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2231/Reviewer_eKea"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996230755, "cdate": 1761996230755, "tmdate": 1762916153201, "mdate": 1762916153201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}