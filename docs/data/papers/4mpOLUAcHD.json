{"id": "4mpOLUAcHD", "number": 24011, "cdate": 1758351632451, "mdate": 1759896786796, "content": {"title": "FiRE: Fine-Grained Ranking Evaluation for Machine Translation", "abstract": "Developing reliable machine translation (MT) systems hinges on our ability to distinguish superior translations from inferior ones—but existing evaluation paradigms, whether limited to coarse overall rankings or misaligned with human preferences, fail to deliver interpretable, fine‑grained feedback in reference‑free settings. We present a Fine-Grained Ranking Evaluation method (FiRE) that leverages off‑the‑shelf large language models to perform criterion‑driven pairwise comparison across three complementary dimensions—faithfulness, fluency, and consistency of style—rather than producing a single holistic judgment. To enable rigorous meta‑evaluation of evaluation paradigms in the absence of any suitable testbed, we construct the first human‑annotated, reference‑free benchmark for fine-grained ranking evaluation, achieving substantial inter‑annotator agreement. Through meta‑evaluation on this benchmark, FiRE demonstrably outperforms leading regression‑based and error‑analysis metrics in aligning with human comparative judgments, while providing more informative insights into translation quality. Finally, our examination of LLM evaluator biases (position and self-enhancement) and their handling of tied cases offers guidance for more nuanced MT evaluation.", "tldr": "", "keywords": ["Machine Translation", "Evaluation", "LLM-as-a-judge"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/29820bc5062ddbef14e2264a2e4a6749ad077173.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes fine-grained pair-wise ranking evaluation of machine translation using off-the-shelf LLMs. The evaluation is done on three complimentary dimensions: faithfulness, fluency, and consistency of style. The paper also provides the first human-annotated fine-grained machine translation evaluation benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Fine-grained human-annotated evaluation data was collected for meta-evaluation. The human annotations have high inter-rater agreement.\n2. The paper provides comparison against strong baselines.\n3. The paper studies the issue of position bias, showing strong position bias of LLMs and the need for position bias mitigation.\n4. The paper also examines the LLM's preference for their own generations.\n5. Performance on easy vs hard examples are shown, which validates that when human annotators find an example difficult the LLMs also struggle."}, "weaknesses": {"value": "1. The paper only focuses on high resource languages and the author's collected data. It is understandable that collecting data for low resource languages is difficult. However as mentioned on table 10, existing MQM annotations could probably be directly mapped to the three evaluation criteria under consideration. An analysis on these MQM datasets would provide an independent verification of the proposed approach on independent datasets.\n2. For position bias experiments, results have not been reported for each of the three evaluation criteria. Whether LLMs show more or less position bias on these different criteria would be an interesting research question."}, "questions": {"value": "Was there any cases where the LLMs did not follow the prompt instruction and generated nonsensical answers? If so, how was it handled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1miMB2Doyi", "forum": "4mpOLUAcHD", "replyto": "4mpOLUAcHD", "signatures": ["ICLR.cc/2026/Conference/Submission24011/Reviewer_Fo13"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24011/Reviewer_Fo13"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662762000, "cdate": 1761662762000, "tmdate": 1762942897144, "mdate": 1762942897144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The title promises a fine grained ranking evaluation. While reading the paper it becomes clear that this only means to split the feedback into ‘faithfulness’, ‘fluency’, and ‘consistency of style’. The authors defined style and fluency as a different class, but I would even argue that both are part of the same broader topic which I would have called fluency (maybe naturalness?). This is leaving only two data points per compared translation pair instead of one.\nThis is much less fine grained than MQM which gives way more detailed information about each sentence and isn’t limited to simply ranking. MQM scores can easily be converted into a ranking, this leaves only the price as an advantage of this method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Releases a ranking benchmark which distinguishes between faithfulness, fluency, and consistency of style.\n\nThey analyzed the bias of the LLM evaluators.\n\nThe data will be released."}, "weaknesses": {"value": "While having three scores is more fine grained than one, it still provides much less information compared to the MQM granularity.\n\nThe authors used language pairs where no existing WMT results are public which would have been easy to compare to."}, "questions": {"value": "Why did you use the comparably small NLLB-200-1.3B model when the results were so weak that you had to downsample it? Using the 3.3B model should not have been any issue from the hardware perspective. If you could run the Qwen2-72B model using the NLLB-MoE-52B should also have been possible.\n\nWhy didn’t you use a language pair supported by WMT? That way you could have used submitted results and compared directly to MQM results or other metrics evaluated at WMT. You can find the ende, enes, and jazh data here: https://github.com/google/wmt-mqm-human-evaluation/tree/main/generalMT2024\nIt’s obviously too late now to change it since you already collected the human rating, but it seems like an odd choice to build a dataset which can not be directly compared to existing data.\n\nWhy did you separate ‘consistency of style’ from ‘fluency’, but no subcategories for the ‘faithfulness’ part? I would say that changing the style also breaks the ‘fluency’ (in a broader sense not as defined in the paper). MQM provides a lot more categories, also for ‘faithfulness’ (by MQM called Accuracy)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q6K7cEDBBo", "forum": "4mpOLUAcHD", "replyto": "4mpOLUAcHD", "signatures": ["ICLR.cc/2026/Conference/Submission24011/Reviewer_Kave"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24011/Reviewer_Kave"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738046745, "cdate": 1761738046745, "tmdate": 1762942896911, "mdate": 1762942896911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper presents a new LLM-based evaluation method, FIRE, by ranking two translations of the same source\nthree ranking criteria are distinguished: adequacy, fluency and style, as well as overall quality\nthe rankings are performed by language models on English-Chinese and Russian-Chinese translations and then compared with rankings of human evaluators\nthe FIRE results are more similar to human judgements than KIWI, xCOMET, metricX and MT-ranker"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The evaluation on three different criteria provides more insights than only an overall scores\n\nThe method is clearly explained"}, "weaknesses": {"value": "the data samples apparently consist only of isolated sentences, so that the context is not taken into account\nseveral recent studies have shown that the evaluation should be done on a paragraph level, not on isolated sentences\n\nsome parts are not fullly clear (see questions)"}, "questions": {"value": "034  why is BLEU (and other similar metrics) called a regression-based metric? What is the regression there? \n\nMaybe the idea is that they are reference-based?  \nso \"similarity-based\" or \"overlap-based\" would be a good description\n\nbased on a single score\n\n\n053: one source sentence: meaning that the evaluation is on isolated sentences, without context\n\n\n180: which systems are NMT (encoder-decoder) and which are LLMs (decoder only)? \n\n258: why report only DeepSeek-R1 results and not of other used models? \n\n268: aligns with the majority vote of human annotations: \nWhy not compare each data point with each data point? \nOverall result (majority vote) might be similar even though the actual annotations are quite different\n\n395: a figure discussed in the main text should not be in appendix\n\nthe organisation of a paper should be in a way that the reader does not need to look into Appendix at all\n\n421: position bias has already been discussed in 4.1"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9VDHGDVLpH", "forum": "4mpOLUAcHD", "replyto": "4mpOLUAcHD", "signatures": ["ICLR.cc/2026/Conference/Submission24011/Reviewer_svgb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24011/Reviewer_svgb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935693053, "cdate": 1761935693053, "tmdate": 1762942896592, "mdate": 1762942896592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FiRE, a machine translation evaluation framework which focuses on reference free pairwise ranking across broad categories (faithfulness, fluency and style consistency). A benchmark is created and various evaluation methods are analyzed against that benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed framework is clear and conceptually intuitive and simple\n* The benchmark can be a good contribution for subsequent evaluations of MT systems\n* Experimentation and ablations appear reasonably thorough"}, "weaknesses": {"value": "I have several doubts/questions about the methodology. I'm listing here the main questions but see also the questions below.\n* on the benchmark creation, is there a mechanism for quality control? I didn't find enough details on the human annotators and any guardrails for quality.\n* I believe Section 4.5 may be one of the main claims of the paper, i.e. that the proposed framework is better than prior ones. However, that section is very small and lacks details for each row presented in Table 3.\n\nGiven the amount of uncertainties and questions, I'm currently leaning towards a weak reject and would encourage the authors to provide more details."}, "questions": {"value": "204-205: “our 3-annotator 3-class setting (which typically yields lower κ values) shows comparably substantial inter-annotator reliability (κ = 0.57 − 0.81).”: do you have an intuition or explantation about this?\n\nTable 1: why are there different number of pairs across categories and why not only retain pairs with all 4 categories?\n\n244-246: “To derive the overall pairwise judgment, we further aggregate all errors across the three criteria into a single composite error-based score.”: what is the aggregation method?\n\n201: “three annotators evaluate the pairwise comparisons”: I believe it is important to provide more details on the annotators, the guidelines given to them and quality control process for the data collection. Otherwise, it puts into question the validity of the benchmark created.\n\nWas position consistency also checked for human evaluators? If not, it could be valuable information.\n\n------------- Typos/Presentation\n\nFigure 1: using gloss for the Chinese text would be good to better understand the explanation.\n\n116:  “semi-automate this process using PLMs”: introduce acronym\n\n184-185: “ and three closed-source systems (GPT4o1 , DeepL, LanMT)”: indicate the dates for all closed source systems, not only gpt-4o\n\n306: “The aggregation procedure is described in Appendix D.”: In my opinion the procedure should be described in the main body of the paper, not the appendix as this is an important detail."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wwUGI2vC7K", "forum": "4mpOLUAcHD", "replyto": "4mpOLUAcHD", "signatures": ["ICLR.cc/2026/Conference/Submission24011/Reviewer_GbcT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24011/Reviewer_GbcT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101017617, "cdate": 1762101017617, "tmdate": 1762942896283, "mdate": 1762942896283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}