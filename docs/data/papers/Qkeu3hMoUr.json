{"id": "Qkeu3hMoUr", "number": 7228, "cdate": 1758012205050, "mdate": 1759897864977, "content": {"title": "ELASTIQ: EEG–Language Alignment with Semantic Task Instruction and Querying", "abstract": "Recent advances in electroencephalography (EEG) foundation models, which capture transferable EEG representations, have greatly accelerated the development of brain–computer interfaces (BCI). However, existing approaches still struggle to incorporate language instructions as prior constraints for EEG representation learning, limiting their ability to leverage the semantic knowledge inherent in language to unify different labels and tasks. To address this challenge, we present ELASTIQ, a foundation model for EEG–Language Alignment with Semantic Task Instruction and Querying. ELASTIQ integrates task-aware semantic guidance to produce structured and linguistically aligned EEG embeddings, thereby enhancing decoding robustness and transferability. In the pretraining stage, we introduce a joint Spectral–Temporal Reconstruction (STR) module, which combines frequency masking as a global spectral perturbation with two complementary temporal objectives: random masking to capture contextual dependencies and causal masking to model sequential dynamics. In the instruction tuning stage, we propose the \\textbf{Instruction-conditioned Q-Former (IQF)}, a query-based cross-attention transformer that injects instruction embeddings into EEG tokens and aligns them with textual label embeddings through learnable queries.\nWe evaluate ELASTIQ on 20 datasets spanning motor imagery, emotion recognition, steady-state visual evoked potentials, covert speech, and healthcare tasks. ELASTIQ achieves state-of-the-art performance on 14 of the 20 datasets and obtains the best average results across all five task categories. Importantly, our analyses reveal for the first time that explicit task instructions serve as semantic priors guiding EEG embeddings into coherent and linguistically grounded spaces. The code and pre-trained weights will be released.", "tldr": "We present ELASTIQ, a foundation model for EEG–Language Alignment with Semantic Task Instruction and Querying.", "keywords": ["BCI", "EEG", "EEG Foundation Model"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22f7ec8d8715b964e8a3c32f3da9cc584877197b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces ELASTIQ: a new foundation model connecting brain activity and language representations.\nIt is composed of two stages:\n- Spectro-Temporal Reconstruction (STR): a self-supervised learning task where the model reconstructs frequency-masked EEG signals (bidirectional and next token) to capture frequency and temporal structure.\n- Instruction-based Query Fusion (IQF): at fine-tuning time, textual instructions are injected into EEG representations via FiLM conditioning and a Q-Former. This allows the model to align EEG embeddings with text embeddings in order to steer the brain features through language.\n\nELASTIQ is tested on 20 datasets across 5 families of tasks and achieves SOTA results on 14/20 datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The combination of STR and IQF is well explained and technically sound.\n- The evaluation is broad, across 20 datasets, and the consistent performance improvement across different paradigms supports generalization.\n- Language alignment seems to affect predictions deeply. This is not a superficial multimodal trick.\n- ELASTIQ outperforms larger models like NeuroLM despite being compact.\n- Multiple ablations and analyses are performed, giving credibility to the different design choices."}, "weaknesses": {"value": "- Statistical Rigor\n\nThe authors do not mention how well they tune their baseline models. There is also a lack of statistical tests. \nAll tables including the main one (Table 1) do not show any information about variance/standard deviation across multiple seeds. \nThis is problematical especially for little gains like the one given by the Informed Task & Targets (Table 2). \nThe only table including infos on standard deviation is Table 5 (comparison with NeuroLM). This point is a critical weakness of the work.\n\n- Language Prior Fairness\n\nAnother concern is about the model's use of pretrained text embeddings. It injects strong semantic knowledge (e.g. \"left hand\") which baseline models do not have. \nWithout baselines using similar priors, it's hard to understand how much of the improvement comes from EEG modeling. \n\n- Instruction Robustness\n\nInstruction robustness is not deeply tested: small variations in the instructions could change results. \nWe don't know how sensitive the model is to the given instruction.\n\nI’d be happy to revisit my rating should the authors address the points raised in this review."}, "questions": {"value": "- How can you prove that the model is not \"borrowing\" from the language model? Did you implement text-prototype classification for baselines to control for language priors? How to isolate gains due to IQF and gains due to semantic targets?\n\n- How sensitive are results to paraphrases or synonyms? Can you report variance over 10 randomized paraphrases per task? \n\n- What's the performance if all 20 datasets are evaluated with unseen subjects ? Why changing the splitting rule using an arbitrary rule like 30 subjects? \n\n- Can you systematically report statistical test results and use cross-validation for each result?\n\n- Any interesting results on the queries? Importance of their numbers? Any Specialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZZYqAkOHwv", "forum": "Qkeu3hMoUr", "replyto": "Qkeu3hMoUr", "signatures": ["ICLR.cc/2026/Conference/Submission7228/Reviewer_KRrL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7228/Reviewer_KRrL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760563984406, "cdate": 1760563984406, "tmdate": 1762919372558, "mdate": 1762919372558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current EEG foundation models (EEG-FMs) and EEG–language foundation models still face two major limitations. First, temporal and frequency information are often reconstructed in separate branches, which may hinder the model’s ability to jointly capture the cross-domain dependencies inherently present in EEG signals. Second, task instructions are typically concatenated with the EEG sequence in a superficial manner, rather than being semantically integrated into the EEG representations. This limits the model’s capacity to effectively leverage linguistic information for guiding EEG feature learning. To overcome these challenges, the authors propose **ELASTIQ**, a foundation model for **EEG–Language Alignment with Semantic Task Instruction and Querying**. ELASTIQ introduces a series of technical innovations designed to address the aforementioned limitations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Pretraining EEG foundation models (EEG-FMs) is a worthwhile and promising research direction. Although this paper has several weaknesses, it makes a meaningful contribution to this emerging area.\n\n2.In particular, pretraining EEG-FMs through alignment between EEG and text representations is a novel and valuable attempt that deserves recognition and encouragement."}, "weaknesses": {"value": "1.While the introduction claims to address two key challenges, the method section is not organized in a way that reflects this logic, which weakens the theoretical persuasiveness of the paper’s central argument.\n\n2.The method section is divided into two stages: **pretraining** and **instruction training**. While this structure resembles that of large language model (LLM) training, the paper only describes *how* the model is trained, without adequately explaining *why* these design choices are effective or theoretically grounded.\n\n3.In **Section 2.1**, the subsections—*Spectral Masking*, *Convolutional Tokenizer*, *Mask Token Modeling Branch*, *Causal Token Modeling Branch*, and *Joint Objective*—lack coherent narrative flow. The logical relationships among these components are unclear, and the purpose and theoretical rationale for each remain underexplained.\n\n4.**Section 2.2** exhibits the same issue: it lists several components without offering clear motivations or theoretical justifications for their integration.\n\nThe above issues constitute the main reasons for my **weak reject** recommendation. Overall, the proposed method appears to be a combination of several technical modules rather than a theoretically supported framework.\n\n5.Furthermore, the paper claims that temporal and frequency information are reconstructed in separate branches, yet no comparative experiment is provided against baselines following this structure. \n\n6.Similarly, in **Section 3.4**, the benchmark comparison is made against a “no-instruction” setting; however, the introduction describes prior approaches as “simply concatenating instructions to EEG sequences.” As a result, the experimental evidence in Section 3.4 does not convincingly support the claims made in the introduction."}, "questions": {"value": "1.Which baseline models reconstruct temporal and frequency information in separate branches? Could you provide comparative results?\n\n2.Could you include a comparison with methods where task instructions are only concatenated to the EEG sequence?\n\n3.In the notation $X^{C \\times T} \\in \\mathbb{R}$, there appears to be an error. Since $X$ represents a matrix, it should be written as $X \\in \\mathbb{R}^{C \\times T}$. Please confirm or correct this notation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f1KB74zMWi", "forum": "Qkeu3hMoUr", "replyto": "Qkeu3hMoUr", "signatures": ["ICLR.cc/2026/Conference/Submission7228/Reviewer_dK6C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7228/Reviewer_dK6C"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761210226560, "cdate": 1761210226560, "tmdate": 1762919371858, "mdate": 1762919371858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ELASTIQ, a novel EEG foundation model that explicitly aligns neural representations with natural language instructions through a two-stage framework—spectral-temporal pretraining and instruction-conditioned fine-tuning. By introducing task semantics as priors via an Instruction-conditioned Q-Former, the method demonstrates strong empirical gains across 20 diverse EEG datasets, setting new state-of-the-art results on 14 of them."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. ELASTIQ is rigorously evaluated across 20 diverse EEG datasets and achieves consistently competitive performance, demonstrating strong generalization across tasks. \n\n2. The method thoughtfully builds on BERT-style masked reconstruction but extends it in a neuroscientifically meaningful way: by incorporating both random and causal temporal masking alongside a novel frequency-band masking strategy, it better captures the spectral-temporal structure inherent in EEG. \n\n3. The authors back up their claims with thorough and insightful visualizations, which convincingly illustrate how language guidance shapes EEG representations, adding significant interpretability and credibility to the approach."}, "weaknesses": {"value": "1. The authors fail to provide a clear description of ELASTIQ’s inference procedure. Specifically, it remains unclear how the model performs inference on downstream tasks\n\n2. There is no discussion or empirical evaluation of the computational overhead introduced by incorporating a language model.\n\n3. In my view, a key motivation for integrating language models with EEG decoding is to unify the heterogeneous label spaces across diverse EEG tasks (e.g., as demonstrated in NeurLM), thereby enabling zero-shot or few-shot generalization and reducing the reliance on task-specific heads. However, the authors evaluated ELASTIQ only in a conventional single-task setting—training and testing on isolated tasks with a task-specific instruction tuning stage. Under this paradigm, the language model introduces unnecessary complexity without delivering its promised key advantage: label space unification and cross-task generalization. Consequently, the motivation for integrating a language model appears unpersuasive, as if the model has been treated as an end in itself rather than a means to address a defined problem.\n\n4. The sensitivity of the model's performance to the precise wording of the task instruction remains unclear. For instance, it would be important to know if replacing a declarative instruction like \"This is an MI task\" with an imperative one like \"Please decode this MI EEG data\" leads to significant performance variation. An ablation study on instruction phrasing would strengthen the understanding of the model's robustness.\n\n5. The direct comparison with NeurLM is unfair, as NeurLM is a multi-task model while ELASTIQ is evaluated under a single-task fine-tuning paradigm."}, "questions": {"value": "Please see in weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RlHWEUybHY", "forum": "Qkeu3hMoUr", "replyto": "Qkeu3hMoUr", "signatures": ["ICLR.cc/2026/Conference/Submission7228/Reviewer_Kx5H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7228/Reviewer_Kx5H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761283536365, "cdate": 1761283536365, "tmdate": 1762919371448, "mdate": 1762919371448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ELASTIQ, a foundation model for EEG–language alignment that integrates semantic task instructions and label semantics into EEG representation learning. It introduces a joint Spectral–Temporal Reconstruction (STR) module for pretraining and an Instruction-conditioned Q-Former (IQF) for aligning EEG embeddings with textual targets via query-based cross-attention. Evaluated on 20 diverse EEG datasets across five BCI tasks, ELASTIQ achieves state-of-the-art performance on 14 datasets and the best average results overall, demonstrating the effectiveness of explicit language guidance in improving EEG decoding generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper evaluates the proposed method on 20 diverse EEG datasets across five BCI tasks, using standardized metrics (balanced accuracy and Cohen’s Kappa) and consistent preprocessing, which facilitates fair comparison with baselines.\n\n- ELASTIQ incorporates task instructions and label semantics via an instruction-conditioned Q-Former, offering a more principled approach to EEG–language alignment than simple concatenation used in prior work.\n\n- The paper provides detailed descriptions of preprocessing, model architecture, training protocols, and plans to release code and pretrained weights, which supports reproducibility."}, "weaknesses": {"value": "- The paper claims that integrating semantic task instructions improves EEG–language alignment and generalization. However, all evaluations are conducted under standard fine-tuned or within-dataset settings—no zero-shot or cross-task transfer experiments are presented. Crucially, it remains unclear whether the proposed instruction tuning enables *zero-shot generalization* to unseen tasks or datasets with different label spaces. Without such evidence (e.g., training on motor imagery with “left/right” labels and testing on a new dataset with “grasp/pinch” without fine-tuning), the claimed advantage of language-guided alignment over conventional multi-task learning is unsubstantiated.\n\n- The proposed architecture effectively functions as a multi-task classifier that conditions on textual instructions. Yet the authors do not compare against a simple but strong alternative: a shared EEG encoder with multiple task-specific classification heads (or a single head trained on all tasks with label embeddings). Such a baseline would help isolate whether performance gains stem from the instruction mechanism itself or merely from joint multi-task training. The absence of this comparison weakens the claim that instruction conditioning offers a *qualitative* improvement over existing paradigms.\n\n- The authors rely on BERT or SBERT for text encoding, despite the availability of more powerful and efficient modern embedding models (e.g., Qwen3-embedding, E5, or Voyage). Since the language model is frozen and not fine-tuned, switching to a stronger encoder would incur minimal computational overhead but could significantly impact alignment quality. The failure to explore or justify this choice raises concerns about suboptimal design and missed opportunities for stronger baselines or ablations."}, "questions": {"value": "1. The paper positions instruction tuning as a mechanism to align EEG with language semantics for improved generalization. However, all evaluations are conducted under fine-tuned or within-dataset settings. If the model cannot perform zero-shot inference on a new dataset—even with the same task type but different label names (e.g., “grasp” vs. “left hand”)—what is the practical advantage of using natural language instructions over conventional multi-task learning with shared representations? Could the authors clarify the intended use case of instruction tuning if zero-shot cross-task or cross-dataset transfer is not feasible?\n\n2. The authors rely on BERT (2019) and SBERT (2019) for encoding instructions and labels. What motivated the use of these older models? Have the authors experimented with newer text encoders, and if so, what were the results?\n\n3. The paper uses UMAP visualizations and ablation studies to suggest that instructions reshape the EEG embedding space. While illustrative, these methods remain largely qualitative. Are there more rigorous or quantitative analyses—such as probing tasks, representational similarity analysis (RSA), or intervention-based causal tests—that could reveal *how* and *to what extent* instructions influence feature learning? For instance, does instruction conditioning primarily affect task discrimination, noise robustness, or cross-subject alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EuKVkDDwdf", "forum": "Qkeu3hMoUr", "replyto": "Qkeu3hMoUr", "signatures": ["ICLR.cc/2026/Conference/Submission7228/Reviewer_H4pe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7228/Reviewer_H4pe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804909821, "cdate": 1761804909821, "tmdate": 1762919370785, "mdate": 1762919370785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address a key limitation of most existing EEG foundation models—their inability to incorporate language instructions and their neglect of the semantic information in classification targets—by conditioning learned EEG representations on the text embeddings of task instructions and training the model to predict the text embeddings of target labels. To enhance EEG–language alignment beyond what was achieved in the previously proposed NeuroLM, the authors integrate language information directly into EEG representations through instruction conditioning, rather than deferring this integration to the Transformer’s attention module. This study represents a meaningful step forward in incorporating large language models (LLMs) into EEG decoding and raises awareness within the community of the importance of embedding semantic information in EEG-based modeling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Overall, most of the arguments and claims — including the identified limitations of existing EEG-FMs, the proposed advantages of ELASTIQ, and the effectiveness of its architectural components — are well supported by targeted experiments presented from Section 3.3 to Section 3.9. These results provide convincing empirical evidence for the soundness and effectiveness of the proposed framework.\n2. The authors highlight a distinctive advantage of leveraging large language models (LLMs) in EEG decoding — the ability to incorporate semantic information of classification targets rather than relying on arbitrary label IDs. The language capability of LLMs enables the mapping of class labels into an embedding space that preserves their semantic relationships, thereby benefiting the IQF-based classification. This claim is substantiated by the comparative analysis in Section 3.6 and offers valuable insights for the future design of EEG decoding algorithms.\n3. The evaluation protocols are carefully designed to ensure fair comparisons across models by controlling for confounding variables such as training stability and the size of evaluation datasets. This methodological rigour enhances the credibility of the reported results.\n4. The curated downstream tasks encompass 20 datasets spanning five key EEG application domains, providing a strong empirical foundation for assessing the generalisability of ELASTIQ across diverse tasks. This breadth of evaluation surpasses that of most prior EEG foundation model studies."}, "weaknesses": {"value": "1. While ELASTIQ demonstrates strong performance across most datasets, a deeper analysis of the cases where its performance falls short compared to other models would provide valuable insights into its weaknesses and potential limitations, and guide future model improvements.\n2. The current training–validation splitting strategy may introduce randomness that affects the reported results. Conducting comparison experiments with multiple random splits and applying statistical tests to assess the significance of the observed performance differences would make the claim of ELASTIQ achieving state-of-the-art results more convincing.\n3. Including a dedicated section that briefly outlines the key distinctions between NeuroLM — the previous EEG-language foundation model — and ELASTIQ would clarify this paper’s specific contributions and highlight its novelty within the existing literature.\n4. The term “cross-domain dependencies” introduced in Line 66 is not explicitly defined or elaborated upon in later sections. Providing a clearer definition and explaining how the proposed joint STR module captures these dependencies would strengthen the conceptual soundness and persuasiveness of the paper’s claims in the introduction."}, "questions": {"value": "1. What is the rationale of choosing two parallel transformers (bidirectional + causal) for handling EEG tokens? Considering the relatively small improvement of using both transformers versus only one of them, as illustrated in Table 4, how do the authors comment on the trade-off between doubled computational cost and marginal increase in decoding performance? \n2. What is the advantage of query-based cross attention design in Q-former over the joint modelling of EEG and text tokens? What could be the key factor(s) makes the latter stronger in guiding EEG representations with instructions as demonstrated in Section 3.8?\n3. In Equation (9) of Section 2.2, the matrix $W_{\\gamma \\beta}$ has shape $2N \\times d$ while the embedding vector $e_{\\text{ins}}$ has shape $k$. How do you ensure this multiplication is valid? Have you adjusted to ELASTIQ model dimension to the same value as language encoder? (i.e., setting $d = k$) Further, how exactly the vector $(\\gamma, \\beta)$ is defined is confusing. Since $\\gamma$ is multiplied with $\\boldsymbol{m}$ by element-wise multiplication in Equation (10), one would expect the shape of $\\gamma$ to be the same as $\\boldsymbol{m}$. But the definition of shape of $\\gamma, \\beta$ in Line 205 is contradicting with the expected shape. \n4. The phrase “lower-rank query subspace” is slightly imprecise in linear algebra terms. You could say “the query matrix has lower-rank” than the EEG embedding or the attention scores are computed in a “lower-dimensional subspace”.\n5. Line 462 to 463 mentions “ELASTIQ was re-implemented under the same settings as NeuroLM.” Could the authors explain which settings or framework of NeuroLM are used for evaluating the modules proposed by ELASTIQ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6TVwP4w3dV", "forum": "Qkeu3hMoUr", "replyto": "Qkeu3hMoUr", "signatures": ["ICLR.cc/2026/Conference/Submission7228/Reviewer_EFFQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7228/Reviewer_EFFQ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975407169, "cdate": 1761975407169, "tmdate": 1762919370313, "mdate": 1762919370313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}