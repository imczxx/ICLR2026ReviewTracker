{"id": "0gDKeKwFj6", "number": 12820, "cdate": 1758210559017, "mdate": 1759897482869, "content": {"title": "Episodic Knowledge Binding: a New Challenge for LLM Continual Learning", "abstract": "Large language models (LLMs) excel at learning individual facts but fail at a fundamental aspect of human cognition: binding related episodes through shared elements. Unlike humans, that effortlessly retrieve all encounters with a person or visits to a location after learning each separately, we demonstrate through controlled experiments that LLMs trained on single-event question-answering pairs cannot generalize to exhaustive multi-event retrieval.  We formalize Episodic Knowledge Binding as the challenge of retrieving multiple related episodes when training lacks explicit multi-event supervision.\n\nDifferently from catastrophic forgetting, where models lose previously learned information, this binding failure persists even when training on aggregated data without temporal confounds, showing that models do not spontaneously develop multi-event retrieval from separate training points. Leveraging synthetic episodic narratives, we reveal a consistent binding gap across model scales (3B--13B and GPT-4.1) and narrative lengths (10--100 events): models attain high accuracy when entities appear in single events, but performance collapses when multiple related episodes must be retrieved. We find that (unsurprisingly) binding becomes harder with more events and that model scaling (more surprisingly) offers only minimal relief within our tested range. To address this problem, we propose Generative Cued Replay (GCR), \nthat (i) inherently operates in a continual learning manner and, inspired by hippocampal memory consolidation, (ii) queries the model's parametric memory for related episodes when processing new events,  (iii)  synthesizing multi-event training data without storing past episodes at each new training step. This approach significantly improves binding without architectural changes, offering a practical method compared to exhaustive multi-event supervision which is both computationally infeasible as well as inherently more rigid.\nWe release our Episodic Knowledge Binding benchmark to enable future research on this fundamental capability that LLMs are currently lacking.", "tldr": "We characterize a new challenge in LLM continual learning where models trained on separate episodic events, fail to semantically bind them. We provide a benchmark and human-inspired baseline which we termed generative cued recall.", "keywords": ["Continual Learning", "Episodic Memory", "Knowledge Binding", "Catastrophic Forgetting", "Large Language Models", "Sequential Learning", "Multi-Event Retrieval", "Benchmark"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/324f510d486cdcd16911b9713f8766bfd4ea972e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to address the episodic knowledge binding problem in LLMs; that is, enhancing their ability to interlink and recall multiple related events, each defined by a tuple of time, space, entity, and content. The authors generate a series of synthetic episodic events and evaluate model performance on memory recall using a lenient recall metric. They find that models like Llama and GPT-4.1 perform poorly on this task. To improve performance, they employ Supervised Fine-Tuning (SFT) with a key innovation: for each new event, the model first generates related past events (e.g., those sharing the same entity or time). These related events are then merged with the current event to create the SFT training data, rather than using the raw event alone. The authors demonstrate that this sophisticated method for generating training data leads to improvements in episodic memory recall."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper takes on an important question regarding binding problems for episodic events in LLMs. The authors show that augmenting simple SFT with a more sophisticated, relation-aware method for organizing training data effectively enhances the model's episodic memory capabilities."}, "weaknesses": {"value": "The manuscript would benefit from significant revisions to improve its clarity and structure, as several typos and an ineffective narrative flow currently hinder readability (see minor comments for specific examples).\n\nWhile I appreciate the effort to categorize the related literature in Table 2, its utility is limited without clear, commonly-agreed-upon definitions for the column headers (e.g., \"knowledge binding,\" \"rehearsal,\" \"continual learning\"). This makes it difficult to assess the accuracy of the taxonomy.\n\nThe core methodological proposal for enhancing episodic memory involves querying the model's own parametric memory to retrieve events related to a new episodic event *k*. These recalled events are then synthesized with event *k* to create the training data for SFT. A more fundamental concern is the paper's heavily engineering-focused presentation, which lacks scientific insight into why the proposed method should work. The approach remains a form of SFT, which compresses episodic information into the model's parametric memory. If the fundamental limitations of parametric memory (e.g., binding failures, catastrophic forgetting) are the root cause of the binding problem, it is unclear why a solution that ultimately relies on further compressing information into that same parametric memory would resolve these issues. The manuscript would be significantly strengthened by a deeper theoretical discussion of this apparent paradox.\n\nMinor comments:\nLine 136: \"e.g..,\" --> \"e.g.,\"\nLine 323: in \"structure. 7\", \"7\" should be removed?\nLine 399: in Figure 2's caption \"List all times Emma was in Tokyo\" --> \"List all times Mary was in Tokyo\"? The figure shows Mary (colored in red), but in the main text the running example was Emma. This is very confusing."}, "questions": {"value": "Another key concern of the proposed method is the risk for model collapse or instability. The training data consists of a combination of the model's own recollections and the new event. Why this self-referential training data generation, where a significant volume of synthetic data is compressed into the weights via SFT, does not lead to degenerative feedback or overfitting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "heECBjlWKm", "forum": "0gDKeKwFj6", "replyto": "0gDKeKwFj6", "signatures": ["ICLR.cc/2026/Conference/Submission12820/Reviewer_8HcV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12820/Reviewer_8HcV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761380684798, "cdate": 1761380684798, "tmdate": 1762923626077, "mdate": 1762923626077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies **Episodic Knowledge Binding** as a fundamental limitation in large language models: their inability to retrieve multiple related episodes through shared elements when trained only on individual, single-event examples. Unlike humans who naturally connect separate experiences (e.g., recalling all encounters with a person), LLMs fail at exhaustive multi-event retrieval despite excelling at learning isolated facts.\nThe authors distinguish this from catastrophic forgetting, showing the binding failure persists even with aggregated training data. Through controlled experiments using synthetic episodic narratives, they demonstrate a consistent **binding gap** across model scales (3B–13B parameters and GPT-4.1): models achieve high accuracy for single-event queries but experience severe performance collapse for multi-event retrieval. Surprisingly, model scaling offers minimal improvement within the tested range.\nTo address this, the paper proposes **Generative Cued Replay (GCR)**, inspired by hippocampal memory consolidation. When processing new episodes, GCR queries the model for related prior episodes and synthesizes multi-event training data without storing past episodes. This approach improves binding without architectural changes, offering a practical alternative to computationally infeasible exhaustive multi-event supervision.\nThe authors release an Episodic Knowledge Binding benchmark to facilitate future research on this capability gap in current LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel Problem Formulation**: The paper identifies and formalizes Episodic Knowledge Binding as a distinct challenge from catastrophic forgetting, addressing a fundamental gap between human and LLM memory systems that has been largely overlooked in prior work.\n- **Comprehensive Empirical Analysis**: The study systematically evaluates the binding gap across multiple model scales (3B-13B parameters and GPT-4.1) and narrative complexities (10-100 events), demonstrating consistency of the phenomenon and providing important insights about scaling limitations.\n- **Clear Distinction from Existing Problems**: The authors carefully distinguish episodic binding from catastrophic forgetting by showing the problem persists even with aggregated (non-sequential) training data, establishing this as a fundamental limitation rather than a continual learning artifact.\n- **Biologically-Inspired Solution**: Generative Cued Replay (GCR) draws meaningful inspiration from hippocampal memory consolidation, providing both theoretical grounding and practical effectiveness without requiring architectural modifications.\n- **Practical Applicability**: The proposed GCR method addresses a real scalability challenge—the computational infeasibility of exhaustive multi-event supervision—while achieving significant improvements through synthetic data generation from the model's own parametric memory.\n- **Benchmark Release**: The authors provide an Episodic Knowledge Binding benchmark to facilitate reproducibility and future research on this important capability gap.\n- **Well-Motivated Research Question**: The paper addresses a cognitively fundamental capability that is essential for practical applications requiring comprehensive information retrieval, making the work both theoretically interesting and practically relevant."}, "weaknesses": {"value": "**1. Insufficient Support for \"Binding is Learnable\" Claim**\n\nThe paper claims that Train(SEQ+MEQ) \"proves binding is learnable,\" but this conclusion is not adequately supported. Train(SEQ+MEQ) merely demonstrates that models can memorize multi-event QA pairs when explicitly supervised and produce correct output formats—an unsurprising result of direct supervision. Critically, it does NOT demonstrate that genuine \"binding\" (flexible association and retrieval of related episodes) has been learned, nor that models can generalize to unseen multi-event queries.\n\nThe distinction between memorization and binding is fundamental to the paper's thesis. To substantiate the \"learnable\" claim, the authors must demonstrate: (1) generalization to held-out multi-event queries not seen during training, (2) transfer to different types of binding relationships, and (3) evidence that models develop flexible retrieval mechanisms rather than task-specific lookup tables. The paper should include experiments testing generalization on held-out multi-event queries, analysis of binding novel entity-attribute combinations, and systematic comparison of binding quality versus mere task completion.\n\n**2. Inadequate Figure Documentation**\n\nFigure 1 lacks sufficient explanation. The meaning of numbers in parentheses is unclear, and \"narrative types\" are not defined or explained in the caption or main text, making the figure difficult to interpret without extensive cross-referencing.\n\n**3. Inconsistent Terminology**\n\nThe paper inconsistently uses \"accuracy\" and \"recall\" throughout, creating confusion about which metric is being reported and whether they are being used interchangeably or represent different measurements.\n\n**4. Poor Paper Organization**\n\nThe paper suffers from structural issues that impede readability and comprehension:\n\n- **Missing Related Work Context**: Important baseline methods should be explained in the related work section with explicit comparisons to the proposed GCR method. Readers need to understand how GCR differs from and improves upon existing approaches.\n\n- **Misplaced Implementation Details**: Section 3.1 contains implementation details that would be better suited for the appendix, cluttering the main narrative with technical minutiae.\n\nA restructuring that moves technical details to appendices while expanding related work comparisons and benchmark documentation in the main text would significantly improve the paper's clarity and impact.\n\n\n\n**5. Non-Deterministic Evaluation Methodology**\n\nThe paper's reliance on \"LLM as a judge\" for extracting model predictions introduces indeterminacy into the evaluation process. While this approach may offer flexibility in parsing varied response formats, it creates several concerns:\n\n- **Reproducibility**: Different LLM judge implementations, versions, or even sampling parameters could yield different recall scores for identical model outputs, making results difficult to reproduce across different research groups.\n\n- **Evaluation Validity**: The LLM judge itself may have parsing errors, biases, or inconsistencies that confound the measurement of the actual binding capability being studied.\n\n- **Transparency**: Without deterministic evaluation, it becomes difficult to diagnose whether performance differences stem from genuine binding improvements or variations in the judge's extraction quality."}, "questions": {"value": "1. **Regarding the \"binding is learnable\" claim**: How do you distinguish between memorization of specific multi-event QA pairs and genuine binding capability in your Train(SEQ+MEQ) experiments? Can you provide evidence of generalization to held-out multi-event queries that were not seen during training?\n\n2. **On evaluation of binding vs. task completion**: Have you tested whether models trained with Train(SEQ+MEQ) can bind novel entity-attribute combinations that differ from training examples? What experiments demonstrate that flexible retrieval mechanisms are learned rather than task-specific lookup tables?\n\n3. **Clarification on Figure 1**: What do the numbers in parentheses represent in Figure 1? Can you define and explain what \"narrative types\" means in the context of your experiments?\n\n4. **Metric consistency**: The paper uses both \"accuracy\" and \"recall\" throughout. Are these terms being used interchangeably, or do they represent different measurements? Can you clarify which metric is reported in each section and provide consistent definitions?\n\n5. **LLM-as-judge reproducibility**: Given that your evaluation relies on an LLM judge to extract model predictions, how do you ensure reproducibility across different implementations, LLM versions, or sampling parameters? Have you conducted any analysis on the inter-rater reliability or consistency of the LLM judge?\n\n6. **Deterministic evaluation baseline**: Would it be possible to include a deterministic evaluation setting (e.g., structured output formats with rule-based parsing) alongside the LLM judge to provide a reproducible baseline for the benchmark?\n\n7. **Benchmark documentation**: Can you provide more details about the benchmark in the main paper, including baseline method comparisons, comprehensive dataset statistics, and representative samples? What are the specific evaluation protocols that other researchers should follow?\n\n8. **Related work discussion**: The related work section currently only contains a taxonomy table (Table 2) with comprehensive discussion deferred to Appendix E. This is insufficient for the main paper. Can you provide a proper related work discussion in the main text that explains key prior methods (e.g., experience replay, elastic weight consolidation, memory-augmented approaches) and explicitly positions your work relative to these techniques? The table could be condensed or moved to the appendix to make room for this critical discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cDvqpMNTqt", "forum": "0gDKeKwFj6", "replyto": "0gDKeKwFj6", "signatures": ["ICLR.cc/2026/Conference/Submission12820/Reviewer_CpjG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12820/Reviewer_CpjG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930749910, "cdate": 1761930749910, "tmdate": 1762923625522, "mdate": 1762923625522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper defines Episodic Knowledge Binding (EKB): if training only on single-event QA pairs, can an LLM retrieve all related episodes matching a cue (multi-event QA) from parametric memory? The paper creates synthetic narratives with tuples (t, s, ent, c) as training data and shows that models excel on single-event queries but collapse on multi-event retrieval, and the gap widens with narrative length and is not fixed by scale. They propose Generative Cued Replay (GCR): when a new event arrives, query the current model for related prior episodes and merge them with the new event to synthesize multi-event supervision online. GCR is shown to improve multi-event binding vs. continual single-event fine-tuning, with strongest gains when hallucination filtering is applied."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem statement is clear, highlighting an important problem of learning the relations across multiple events.\n- Evaluation spans two model families and multiple sizes (Llama-3 3B/8B/13B and GPT-4.1 variants), and multiple narrative lengths (10/30/100), supporting generality claims.\n- The writing is clear and easy to follow."}, "weaknesses": {"value": "- GCR could use more training compute than other baseline methods. When compared to baselines like single-event fine-tuning etc., it is not clear if the number of FLOPs or tokens are controlled.\n- Missing Baselines: RAG/external memory is dismissed based on prior work; there is no RAG or memory module baseline. In addition, prior work has shown that rephrasing or other data augmentation techniques can improve down-stream task performance. These baselines are not explored at all.\n- Evaluation uses an LLM judge with lenient recall and counts an answer correct only at recall = 1.0; the paper does not report human audit or inter-annotator checks for this metric, leaving reliability uncertain.\n- Figure 3 is over-stretched vertically and hard to read."}, "questions": {"value": "1. Can you report FLOPs or total tokens processed for each training condition (SEQ, SEQ+MEQ upper bound, and each GCR variant)? Are these budgets comparable across methods?\n2. What error analysis have you done for different training methods (SEQ, SEQ+MEQ, and GCR)? How do these failure cases reveal about the shortcomings of each method?\n3. With a multi-turn RAG baseline, how does performance compare to GCR on episodic binding? Is this task non-trivial for retrieval-only methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L62qdk9Vc3", "forum": "0gDKeKwFj6", "replyto": "0gDKeKwFj6", "signatures": ["ICLR.cc/2026/Conference/Submission12820/Reviewer_xKvt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12820/Reviewer_xKvt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952643620, "cdate": 1761952643620, "tmdate": 1762923624339, "mdate": 1762923624339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}