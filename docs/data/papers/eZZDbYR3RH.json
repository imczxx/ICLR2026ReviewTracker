{"id": "eZZDbYR3RH", "number": 11149, "cdate": 1758191085822, "mdate": 1763467542761, "content": {"title": "SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images", "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled generalizable, on-the-fly reconstruction of sequential input views.\nHowever, existing methods often predict per-pixel Gaussians and combine Gaussians from all views as the scene representation, leading to substantial redundancies and geometric inconsistencies in long-duration video sequences. To address this, we propose SaLon3R, a novel framework for Structure-aware, Long-term 3DGS Reconstruction. To our best knowledge, SaLon3R is the first online generalizable GS method capable of reconstructing over 50 views in over 10 FPS, with 50% to 90% redundancy removal. Our method introduces compact anchor primitives to eliminate redundancy through differentiable saliency-aware Gaussian quantization, coupled with a 3D Point Transformer that refines anchor attributes and saliency to resolve cross-frame geometric and photometric inconsistencies. Specifically, we first leverage a 3D reconstruction backbone to predict dense per-pixel Gaussians and a saliency map encoding regional geometric complexity. Redundant Gaussians are compressed into compact anchors by prioritizing high-complexity regions. The 3D Point Transformer then learns spatial structural priors in 3D space from training data to refine anchor attributes and saliency, enabling regionally adaptive Gaussian decoding for geometric fidelity. Without known camera parameters or test-time optimization, our approach effectively resolves artifacts and prunes the redundant 3DGS in a single feed-forward pass. Experiments on multiple datasets demonstrate our state-of-the-art performance on both novel view synthesis and depth estimation, demonstrating superior efficiency, robustness, and generalization ability for long-term generalizable 3D reconstruction. Code will be released.", "tldr": "", "keywords": ["3D Gaussian Splatting", "3D Reconstruction", "Generalizable Gaussian"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ce1140220ac2d9bb556fdb7aa2c1ca912b0a63d2.pdf", "supplementary_material": "/attachment/ce9209af2fde563df58538b6f15c2832b16333ec.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SaLon3R for online generalizable 3D Gaussian Splatting (3DGS) reconstruction from unposed image sequences. The method achieves redundancy removal and performs well on novel view synthesis tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a novel architecture, SaLon3R, to address the challenging problem of long-term, online, and generalizable 3DGS reconstruction from unposed images.\n2. The ablation studies clearly demonstrate the contribution of each individual component."}, "weaknesses": {"value": "1. The method exhibits a strong dependence on the backbone network. The overall performance is highly contingent on the quality of the initial pose and geometry estimates provided by the pretrained reconstruction model, CUT3R.\n2. The method is explicitly designed for static scene reconstruction. However, real-world online \"long-term\" videos often contain dynamic objects, which limits the practical application scope of this work."}, "questions": {"value": "1. Several recent works appear to be missing from the discussion, such as AnySplat[1]. AnySplat utilizes VGGT as a backbone for feed-forward 3DGS, which seems highly relevant given that your architecture also employs VGGT in an ablation study. \n\n2. The datasets used for evaluation are all indoor scenes (e.g., ScanNet, Replica). It would be better if there are more types of scenario evaluations to prove generalization, such as in the wild scene.\n\n[1] Jiang, Lihan, et al. \"AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views.\" arXiv preprint arXiv:2505.23716 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9W6BZ6DHGe", "forum": "eZZDbYR3RH", "replyto": "eZZDbYR3RH", "signatures": ["ICLR.cc/2026/Conference/Submission11149/Reviewer_tBnx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11149/Reviewer_tBnx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745964502, "cdate": 1761745964502, "tmdate": 1762922315237, "mdate": 1762922315237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "qRiuX9ZAOa", "forum": "eZZDbYR3RH", "replyto": "eZZDbYR3RH", "signatures": ["ICLR.cc/2026/Conference/Submission11149/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11149/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763467541959, "cdate": 1763467541959, "tmdate": 1763467541959, "mdate": 1763467541959, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SaLon3R, a feed-forward pose-free 3D Gaussian reconstruction framework designed for long image sequences. The method builds on CUT3R to estimate camera parameters and pointmaps, followed by a Gaussian prediction head, saliency-based voxel quantization, and a lightweight point transformer for refinement. The system targets online Gaussian reconstruction with redundancy reduction, claiming >10 FPS performance and competitive novel-view synthesis and depth estimation on indoor datasets such as ScanNet, ScanNet++, and Replica.\n\nAlthough the pipeline functions and is evaluated on standard indoor benchmarks, the contribution appears incremental relative to recent pose-free generalizable 3DGS models and CUT3R-based pipelines. Several design elements overlap with contemporary work (e.g., AnySplat), and the method does not demonstrate generalization beyond constrained indoor scenes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Reasonable formulation combining CUT3R with Gaussian prediction, voxel quantization, and spatial refinement.\n\n2. Online inference and redundancy pruning are practically valuable for Gaussian-based scene representations.\n\n3. Competitive indoor results on depth and novel-view synthesis benchmarks, with clear comparisons to relevant baselines such as FreeSplat and NoPoSplat.\n\n4. Implementation efficiency with reported >10 FPS online performance and reduced Gaussian counts."}, "weaknesses": {"value": "1. Incremental contribution. The core advances over CUT3R appear modest. The method essentially adds a Gaussian head plus filtering and refinement modules on top of a strong existing reconstruction backbone. The online capability and geometry fidelity are largely inherited from CUT3R, rather than introduced by novel algorithmic components.\n\n2. Limited novelty relative to AnySplat and related work. The approach is conceptually very close to AnySplat, which also leverages feed-forward pose-free reconstruction and voxel-space pruning. The proposed “saliency” score serves a similar role to AnySplat's confidence-based importance weighting for voxel aggregation. The paper does not sufficiently articulate conceptual differences or empirical advantages over this line of work.\n\n3. Restricted experimental scope and lack of generalization evidence. Although CUT3R itself demonstrates strong cross-domain generalization across indoor/outdoor and static/dynamic settings, this work evaluates almost exclusively on indoor scenes (ScanNet/ScanNet++/Replica). It is unclear whether the introduced modules maintain or degrade CUT3R’s broad generalization capacity. No results are presented for outdoor scenes, dynamic scenarios, or more challenging real-world data distributions.\n\n4. Unclear scalability and robustness. While the paper focuses on long sequences, it does not examine long-range drift, global scene consistency, or stability in more complex environments where CUT3R’s estimates deteriorate. The method appears tailored to small-scale indoor reconstructions rather than general long-term scene mapping."}, "questions": {"value": "1. How does the proposed saliency-aware pruning differ fundamentally from AnySplat’s confidence-based pruning? What measurable improvements arise specifically from your design choices?\n\n2. Does your pipeline retain CUT3R’s generalization to outdoor and dynamic scenes? If not, why is performance restricted to indoor static domains?\n\n3. Can you provide ablation or comparative experiments showing performance in more diverse environments, including outdoor datasets and dynamic scenarios?\n\n4. How does your method behave when CUT3R’s pose and depth predictions drift over longer sequences? Is there any mechanism for global alignment or drift correction?\n\n5. Does the Gaussian-based training bring any benefit to pose or pointmap accuracy compared to CUT3R without freezing the corresponding heads?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rmRi394Jmq", "forum": "eZZDbYR3RH", "replyto": "eZZDbYR3RH", "signatures": ["ICLR.cc/2026/Conference/Submission11149/Reviewer_ndDE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11149/Reviewer_ndDE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890546359, "cdate": 1761890546359, "tmdate": 1762922314568, "mdate": 1762922314568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SaLon3R, an online 3D Gaussian reconstruction model that performs scene reconstruction on-the-fly. The proposed framework is built upon the online point reconstruction model CUT3R, with additional modules for predicting Gaussian latents and a saliency map. To address the issue of excessive Gaussians as the number of input views increases, the method first quantizes Gaussians under the guidance of the saliency map and then refines them using a point transformer. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on online Gaussian reconstruction tasks, evaluated via novel-view image and depth rendering quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method achieves SOTA performance on online Gaussian reconstruction benchmarks.\n\n- The Gaussian refinement module effectively reduces the number of Gaussians while improving reconstruction quality."}, "weaknesses": {"value": "- Re-training baseline methods such as PixelSplat, MVSplat, and NoPoSplat may not yield fair comparisons, as additional in-house noise could be introduced during re-training on new datasets. Moreover, these baselines were trained on a fixed, small number of views, so comparisons on larger view counts may be less meaningful. It would be helpful to also evaluate SaLon3R on the datasets that the baseline models are trained on.\n\n- In the Adaptive Growth stage, what happens if pruning considers only the predicted alpha values? An ablation study is needed to justify the necessity of the proposed Structure-aware Saliency Map, which is claimed as a key contribution.\n\n- Some technical details are missing, such as the view selection strategy used during inference for Gaussian refinement.\n\n- In Fig. 2, the Saliency-aware Quantization module includes an update operation that is not explained in Sec. 3.3, even though the figure refers to that section."}, "questions": {"value": "- Can the proposed method be applied to outdoor scenes, such as DL3DV datasets?\n- How does the proposed method compare with multi-view Gaussian reconstruction models such as AnySplat [1]?\n\n[1] Jiang, Lihan, et al. “AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cuBVMvebIR", "forum": "eZZDbYR3RH", "replyto": "eZZDbYR3RH", "signatures": ["ICLR.cc/2026/Conference/Submission11149/Reviewer_C9v5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11149/Reviewer_C9v5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989875323, "cdate": 1761989875323, "tmdate": 1762922313909, "mdate": 1762922313909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}