{"id": "8v9kqmFeHg", "number": 234, "cdate": 1756732086393, "mdate": 1763563297282, "content": {"title": "Robust Non-negative Proximal Gradient Algorithm: Theory and Applications", "abstract": "Proximal gradient algorithms (PGA), while foundational for inverse problems like image reconstruction, often yield unstable convergence and suboptimal solutions by violating the critical non-negativity constraint. We identify the gradient descent step as the root cause of this issue, which introduces negative values and induces high sensitivity to hyperparameters. To overcome these limitations, we propose a novel multiplicative update proximal gradient algorithm (SSO-PGA) with convergence guarantees, which is designed for robustness in non-negative inverse problems. Our key innovation lies in superseding the gradient descent step with a learnable sigmoid-based operator, which inherently enforces non-negativity and boundedness by transforming traditional subtractive updates into multiplicative ones. This design, augmented by a sliding parameter for enhanced stability and convergence, not only improves robustness but also boosts expressive capacity and noise immunity. We further formulate a degradation model for multi-modal restoration and derive its SSO-PGA-based optimization algorithm, which is then unfolded into a deep network to marry the interpretability of optimization with the power of deep learning. Extensive numerical and real-world experiments demonstrate that our method significantly surpasses traditional PGA and other state-of-the-art algorithms, ensuring superior performance and stability.", "tldr": "", "keywords": ["Proximal gradient algorithm; Optimization; Robustness; Inverse problems"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/da57a69b4fc1761d3d7e9940c360bd5434792d77.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a modification to the Proximal Gradient Algorithm (PGA)'s additive gradient step, with a multiplicative update using a Sliding Sigmoid Operator (SSO). The proposed update maintains the  non negativity and boundedness of an update variable. A sliding parameter controls the effective step size."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The novel multiplicative modification to the gradient step, which keeps iterates non negative: This is an interesting idea, with guarantees shown for monotonicity of the error, and equivalent interpretation to traditional gradient descent.\n\nThe toy problems set up clearly motivate the intuitions, and the performance is superior to the reported baselines. The proposed method also performs modestly well for practical datasets.\n\nThe algorithm is adapted for an unrolled deep neural network to demonstrate its applicability in data-dependent settings."}, "weaknesses": {"value": "1. Some of the claims in the paper are misleading. The broad statement “first to use multiplicative updates for positivity in the PGA context”  is a very strong claim. There have been many multiplicative schemes that attempt to preserve positivity. (Such as Mirror/ exponential descent, projected PG, and re-parametrization updates.) The strong claims could be more justified if they compared against such existing updates upon PG. \n\n\n2. The authors mention \"a straightforward solution\" to enforce non-negativity in eq. (6), and build on the intuition for their SSO. It is true that the ratio rule (Lee-Seung 2000) for setting the step size has it's drawbacks. However, there are other fixes (e.g. projected/ proximal methods with indicator-re-parametrization, and mirror/exponential-gradient). Thus it should not be presented as a \"straightforward option\".\n\n\n3. The work could be more convincing if they acknowledged these alternative methods, and clearly mention them in the baselines. The extensive results could be included in the supplementary. The authors did include a projection-style baseline (apply ReLU/Softplus after the GD step) and report it underperforms both vanilla PGA and their SSO-PGA, arguing that post-projection “loses information from negative values”. However, this is not sufficient to convince the superiority of SSO over \"all\" existing gradient step updates, in the context of preserving non-negativity."}, "questions": {"value": "Would appreciate if the authors can address the comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ceqYo4ZDrt", "forum": "8v9kqmFeHg", "replyto": "8v9kqmFeHg", "signatures": ["ICLR.cc/2026/Conference/Submission234/Reviewer_hNdr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission234/Reviewer_hNdr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830440040, "cdate": 1761830440040, "tmdate": 1762915476705, "mdate": 1762915476705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "sKAw3zRXMb", "forum": "8v9kqmFeHg", "replyto": "8v9kqmFeHg", "signatures": ["ICLR.cc/2026/Conference/Submission234/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission234/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763563296372, "cdate": 1763563296372, "tmdate": 1763563296372, "mdate": 1763563296372, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel proximal gradient algorithm SSO-PGA that replaces the standard additive gradient descent step with a multiplicative update based on a Sliding Sigmoid Operator (SSO). This design enforces non-negativity by construction, improves robustness to hyperparameters, and is embedded into a deep unfolding framework for multi-modal image restoration. The authors provide theoretical convergence guarantees, numerical experiments, and comparisons on inverse problems  (pansharpening, flash-guided denoising), showing consistent improvements over PGA baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The replacement of the standard additive gradient descent step in proximal gradient algorithms (PGA) with a multiplicative update governed by a learnable, bounded Sliding Sigmoid Operator to enforce the non-negativity constraint in the solution is a novel and interesting approach.\n* The authors prove that the proposed SSO-PGA iteration is equivalent to a standard gradient step, which motivates its incorporation to deep unfolding deep networks.\n* The reported comparisons on Multispectral image Fusion and Flash/Non-Flash denoising show  consistent improvements over standard PGA and various sota methods"}, "weaknesses": {"value": "* While this work claims novelty in using a multiplicative update to enforce non-negativity, similar ideas exist in Non-Negative Matrix Factorization (NMF) and EM-type algorithms for Poisson inverse imaging problems, which the authors miss to mention. \n* This work compares against several sota methods but ommits comparisons to explicitly non-negative deep networks that utilize projected gradient layers as in Kokkinos and Lefkimmiatis, 2018 and classical constrained optimization solvers that enforce non-negativity via projection (FISTA-based solutions) or barrier methods.\n*  In Theorem 2 the condition 0 <= a <= 2/k ||H||^2 -1 is unclear. If  the denominator in the right hand side becomes bigger that 2 the condition is impossible to satisfy. \n* The inverse problems studied in this work are rather limited making it unclear whether the proposed approach can show any benefits when applied in other widely studied inverse imaging problems, including demosaicking, supperesolution, deblurring, etc\n\n\n\n\n\n-- References\n\n* Filippos Kokkinos, Stamatios Lefkimmiatis, \"Deep Image Demosaicking using a Cascade of Convolutional Residual Denoising Networks\"; Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 303-319"}, "questions": {"value": "While the authors provide meaningful comparisons with PGA methods, comparisons with other optimization strategies (Majorization-Minimization methods including FISTA) that can successfully enforce the non-negativity constraint in the solution are missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m5wtfCz5KH", "forum": "8v9kqmFeHg", "replyto": "8v9kqmFeHg", "signatures": ["ICLR.cc/2026/Conference/Submission234/Reviewer_qYhz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission234/Reviewer_qYhz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918862854, "cdate": 1761918862854, "tmdate": 1762915476484, "mdate": 1762915476484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors introduce a modified proximal gradient algorithm (PGA) for non-negative inverse problems, replacing the standard gradient descent step with a learnable sigmoid-based multiplicative operator. The resulting method, termed SSO-PGA, is claimed to inherently enforce non-negativity, improve stability, and offer convergence guarantees. The authors also propose an unfolded deep network version of their algorithm and validate it on multimodal restoration and remote sensing experiments.\n\nWhile the proposed idea of a learnable multiplicative update is novel and potentially interesting, the paper is fundamentally flawed in its theoretical framing. It contains several misunderstandings of basic convex optimization principles, inaccurate statements about the behavior of standard proximal algorithms, and errors in the provided proofs. As a result, the theoretical contributions cannot be trusted in their current form. On the positive side, the experimental section is clear and the empirical results for remote sensing are promising, but they do not compensate for the conceptual and theoretical issues in the paper."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The experiment in remote sensing imaging is reasonably well presented and suggest that the proposed method performs competitively.\n2. The idea of using a sigmoid-based multiplicative update as a way to enforce non-negativity is original."}, "weaknesses": {"value": "1. The mathematical groundings of the paper contain a large number of errors and inaccuracies.\n2. The paper is not well related to current works. This makes it difficult to assess its novelty."}, "questions": {"value": "**Major comments**\n1. The abstract and introduction make factually incorrect claims such as “PGA often yields unstable convergence.” This is misleading at best: proximal gradient algorithms are among the most well-established optimization methods, with strong convergence guarantees in convex and even certain non-convex settings. If the authors wish to discuss instability, they should clearly define what kind of “instability” they refer to (e.g., sensitivity to step size, oscillations in non-convex cases), and support this with references or evidence.\n2. The literature review omits essential prior work on replacing proximal operators with learned or non-standard mappings, notably: Plug-and-Play Priors (Venkatakrishnan et al., 2013), Meinhardt et al., Learning Proximal Operators (CVPR 2017), Ryu et al., \"Plug-and-Play Methods Provably Converge with Properly Trained Denoisers\", Pesquet et al., \"Learning Maximally Monotone Operators for Image Recovery\". These works provide much more rigorous frameworks for combining learned components with optimization steps, and the authors should position their approach relative to them. \n3. The claim that “the update rule in Eq. (4) may yield negative values” is incorrect. Indeed, if $g$ contains the non-negative orthant $g = \\iota_C$ with $C=\\mathbb{R}_+$, the proximal operator is simply the projection on $C$ which enforces non-negativity automatically. If $g$ is the sum of $\\iota_C$ and another convex function, although there is no closed-form in general, the prox will be positive valued (elementwise). There is no need to modify the gradient step.\n4. The proof of Theorem 1 is invalid. The value $\\rho_i$ depends on $y^{t-1}$, creating a circular argument. $\\exists \\rho_i$ assumes that $\\rho_i$ is independant of $y^{t-1}$. Otherwise, this creates a circular dependency (or the functional $\\nabla E$ needs to be changed). Furthermore, the derivation confuses scalar and vector quantities: gradients of real-valued functions are vectors, so the equality proposed only holds pointwise and this should be specified by the authors. \n5. Theorem 2: The argument that the algorithm “converges to a local minimum” because a sequence is non-increasing and bounded below is incorrect. A non-decreasing bounded sequence may not converge to zero; the authors need to establish that the gradient norm tends to zero, and then resort to analysis arguments for completing the proof. But as is, the authors just show that the cost function is upper-bounded, which is far from a convergence result.\n6. Several lemmas are standard results from convex analysis and add little value in their current form without clear adaptation to the proposed method. Their proof should be ommited. The text also conflates the inverse problem with the data-fidelity term (e.g., line 213). This distinction should be clarified.\n7. The proposed “stability” argument is not grounded in standard optimization theory. PGA already guarantees stable and monotone convergence under mild assumptions. If the authors wish to argue for robustness to step size or noise, this must be formalized and supported by analysis or empirical ablation.\n8. The notation $f^*$ in line 275 appears to denote a Jacobian transpose, which should be clarified.\n9. Problem (24) is trivial: it has a closed-form solution when H is the identity, so it does not meaningfully demonstrate algorithmic benefits.\n10. Figure 6 should include a convergence plot over many more iterations (e.g., 100+) and track the evolution of the objective function or iterate norm, as is standard in optimization papers (e.g., Ryu et al., Pesquet et al.).\n\n**Minor comments**\n\n1. Notations are inconsistent throughout. The authors have introduced $f$ in the beginning but switch for $\\mathcal{E}$ in the middle.\n2. Did the authors expeirment plug-and-play algorithms or diffusion for the considered experiments? I think this would potentially be good candidates for this problem."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yWd2YfMl9m", "forum": "8v9kqmFeHg", "replyto": "8v9kqmFeHg", "signatures": ["ICLR.cc/2026/Conference/Submission234/Reviewer_y36G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission234/Reviewer_y36G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934400039, "cdate": 1761934400039, "tmdate": 1762915476385, "mdate": 1762915476385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel proximal gradient algorithm called SSO-PGA, which replaces the standard additive gradient descent step with a multiplicative update rule using a Sliding Sigmoid Operator (SSO). The authors claim that this modification inherently enforces non-negativity constraints, improves stability, and enhances convergence behavior. The method is further unfolded into a deep network and evaluated on two image fusion tasks: multispectral image fusion and flash-guided non-flash denoising. Experimental results show that SSO-PGA outperforms traditional PGA and several state-of-the-art methods in terms of both quantitative metrics and visual quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed multiplicative update rule based on the Sliding Sigmoid Operator is novel and offers an interesting alternative to traditional gradient descent."}, "weaknesses": {"value": "- The motivation for using a multiplicative update to enforce non-negativity is weak. Non-negativity can be easily handled in traditional optimization via projection or by including a non-negativity constraint in the proximal operator. The authors did not compare SSO-PGA against such standard constrained optimization baselines.\n\n- When the optimization algorithm is unfolded into a deep network, the original properties of the iterative algorithm (e.g., convergence guarantees) may not be preserved. In such cases, non-negativity can be trivially enforced using activation functions (e.g., ReLU or Softplus), which the authors briefly compared but did not thoroughly justify why SSO is fundamentally better in the deep learning context.\n\n- The title and claims of the paper are overly broad. The method is only validated on image fusion tasks, yet the title and abstract suggest a general-purpose optimization improvement. This overstates the contribution and applicability.\n\n- The explanation of why SSO-PGA performs better—beyond non-negativity—is insufficient. The improvement may stem from other factors such as adaptive step sizes or better gradient conditioning, which are not sufficiently analyzed."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I97yFQbtrw", "forum": "8v9kqmFeHg", "replyto": "8v9kqmFeHg", "signatures": ["ICLR.cc/2026/Conference/Submission234/Reviewer_Sr9p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission234/Reviewer_Sr9p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984397843, "cdate": 1761984397843, "tmdate": 1762915476279, "mdate": 1762915476279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}