{"id": "oXYZHg7HiZ", "number": 2218, "cdate": 1757033659279, "mdate": 1759898162274, "content": {"title": "Interactive Post-Training for Vision-Language-Action Models", "abstract": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation. Without requiring shaped rewards or value models, RIPT-VLA achieves state-of-the-art results across a wide range of tasks and benchmarks. It improves the lightweight QueST model by up to 21.2% in few-shot settings, achieving state-of-the-art 94.3% on LIBERO-90, and pushes the large-scale OpenVLA-OFT model to achieve 97.5% on the LIBERO 4-Suite benchmark. Remarkably, when only one demonstration is given, RIPT-VLA enables an unworkable SFT model (4%) to succeed with 97% success rate within 15 iterations. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision. Code and checkpoints will be released (We included anonymous code in the supplementary material for review).", "tldr": "", "keywords": ["Reinforcement Learning", "VLA"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f41f95ec4c28c5ad1fb3293ad55e5e6df1a38300.pdf", "supplementary_material": "/attachment/329642a67a99a51341fe24918fecea75522f3333.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes RIPT-VLA, advancing VLA models via enabling interactive post-training with dynamic rollout sampling and stable reinforcement learning methods.\nExperiments on various VLA benchmarks demonstrate that RIPT-VLA outperforms existing methods and has significant generalisation ability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents a simple yet efficient training recipe for VLA models, including RLOO[1] and Dynamic sampling[2].\n\n\n>reference\n>>\n>> [1] Buy 4 reinforce samples, get a baseline for free!\n>>\n>> [2] DAPO: An Open-Source LLM Reinforcement Learning System at Scale."}, "weaknesses": {"value": "1. The main concern is the lack of novelty. Dynamic sampling is commonly used in LLMs, such as DAPO [1], which is not cited properly.\n2. The citation of RLOO is wrong! It should be [1], but using [2] in this paper.\n3. For VLA models, the experiments in real robotics should be considered.\n\n\n>reference\n>>\n>> [1] DAPO: An Open-Source LLM Reinforcement Learning System at Scale.\n>>\n>> [2] Buy 4 reinforce samples, get a baseline for free!\n>>\n>> [3] Attention, Learn to Solve Routing Problems!"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o7t9cEG7SU", "forum": "oXYZHg7HiZ", "replyto": "oXYZHg7HiZ", "signatures": ["ICLR.cc/2026/Conference/Submission2218/Reviewer_PkAa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2218/Reviewer_PkAa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574828527, "cdate": 1761574828527, "tmdate": 1762916147366, "mdate": 1762916147366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RIPT-VLA, a post-SFT training stage for VLA models. It employs a critic-free RL\nalgorithm to fine-tune policies using sparse, binary environmental rewards, aiming to improve\nperformance in low-data regimes. The method is shown to improve performance significantly\nover SFT-only baselines on several simulated benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of this paper is showing that a simple, critic-free RL algorithm can effectively\nfine-tune VLA models from sparse binary rewards. This is a practical and stable approach, and the\nresults convincingly show its ability to boost performance, especially when expert demonstrations\nare scarce."}, "weaknesses": {"value": "1. The main contribution of this paper is the dynamic sampling strategy, which, while effective,\nis more of an engineering improvement than a fundamentally new RL algorithm. The novelty\nlies more in the successful application and adaptation of this critic-free paradigm to the VLA\ndomain, rather than in inventing the RL method itself.\n\n2. All experiments are conducted in simulation. While this is standard practice, policies trained\nwith RL can be sensitive to the sim-to-real gap. The paper claims the method is \"practical,\"\nbut does not discuss the potential challenges of transferring an interactively trained policy to\na physical robot, where interaction is costly and system dynamics may differ.\n\n3. The paper highlights data efficiency in terms of the number of expert demonstrations.\nHowever, it doesn't quantify the interaction cost required by the RL stage (i.e., the total\nnumber of environment steps or wall-clock time needed for rollouts). This is a critical metric\nfor any RL method, as online interaction is often the real bottleneck, far more so than offline\nSFT data."}, "questions": {"value": "1. The authors should quantify the interaction cost. How many total environment steps (or wallclock\nhours) were required for RIPT-VLA to converge? How does this \"online\" cost compare to\nthe \"offline\" cost of collecting the SFT data for the baselines?\n\n2. The dynamic sampling strategy filters out contexts that are either \"too easy\" (all successes)\nor \"too hard\" (all failures). Is there a risk of catastrophic forgetting on the \"easy\" tasks that\nthe policy no longer sees? Have the authors evaluated performance on these filtered-out\ncontexts post-training?\n\n3. How sensitive is the method to K (number of rollouts per context)? The paper uses K=8/16.\nWhat happens if K is very small (e.g., 2), which might be more practical on a real robot?\n\n4. What new challenges do you foresee in applying RIPT-VLA to a real robotic system? For\ninstance, how would the algorithm handle noisy state estimation, physical safety constraints,\nor the impossibility of perfectly resetting the environment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FqI5Mg1zh0", "forum": "oXYZHg7HiZ", "replyto": "oXYZHg7HiZ", "signatures": ["ICLR.cc/2026/Conference/Submission2218/Reviewer_ZxRz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2218/Reviewer_ZxRz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964728375, "cdate": 1761964728375, "tmdate": 1762916146802, "mdate": 1762916146802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RIPT-VLA, an interactive post‑training stage that follows pretraining and supervised fine‑tuning for vision‑language‑action models. In Stage 3 the policy is rolled out in a multitask environment and receives only binary success or failure rewards. The update rule is a critic‑free variant of PPO that couples leave‑one‑out advantages with importance‑weighted updates and a simple dynamic sampling rule that discards rollout groups whose advantages are all zero, which the authors argue stabilizes training when tasks are either trivially solved or always fail. The method applies to both tokenized action heads and continuous regression heads by adding a lightweight scale head so log probabilities are available for PPO. Algorithm 1 and Section 4 detail the training loop and the dynamic sampling design. \n\nExperiments are entirely in simulation on LIBERO suites and Meta‑World 45. On Stage‑2 small models, RIPT gives larger absolute gains, for example QueST improves from 82.7 to 93.6 average success across LIBERO Goal, Spatial, Object, Long and sees 18 points on LIBERO‑Long. On a strong Stage‑1+2 large model, OpenVLA‑OFT, gains are modest in absolute terms, from 96.7 to 97.5 average. The paper also reports many‑task results on LIBERO‑90 and ML45, few‑shot SFT regimes, and cross‑scenario or cross‑goal generalization curves in the appendix. Table 1 on page 8 and Table 2 on page 9 summarize the core numbers, Figure 2 on page 9 shows the few‑shot curve, and Figures 3-4 in the appendix show generalization plots. The ethics statement explicitly notes that all experiments are in simulation only."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "• Clear, simple recipe for interactive post‑training after SFT. Algorithm and the dynamic sampling heuristic are easy to implement. Section 4 and Algorithm 1 are well written. \n\n• Some gains in the areas that matter most for data scarcity. On LIBERO‑Long and few‑shot settings, the improvements over SFT are moderate for Stage 2 models, and the “1‑demo to workable policy” result is an interesting result if generalizable.  Certainly gains are shown, though only one (seemingly easy) example was shown in the paper.\n\n• The method covers both tokenized and continuous action heads by adding a small scale head for regression policies, which increases applicability across current VLA families. Section 4.3 describes this clearly. \n\n• Using RL as a third stage generally seems like a good idea idea, though a binary signal might be challenging to apply broadly."}, "weaknesses": {"value": "• Binary reward is probably too constraining for many real settings and it's perhaps interesting to used a learned reward signal.  \n\n• External validity. All results are in simulators, with LIBERO and ML45, and the paper does not demonstrate the method on a physical robot or discuss how robust success detectors would be implemented in hardware. For example, how might you replay an exact context?  This would need to include the environment state, so you'd have to do environmental resets, which seems impractical. Real‑robot experiments or a concrete plan for doing this in the real world would be important for impact for a paper like this. \n\n• Limited absolute gains on a strong Stage‑1+2 baseline. In Table 1 the OpenVLA‑OFT average moves from 96.7 to 97.5. This suggests diminishing returns when the model already encodes broad world knowledge, which the paper should acknowledge more directly in the discussion. An analysis of this would help readers decide when Stage‑3 is worth the extra rollouts.\n\n• It would be nice to know how this might take us to a world of generalization beyond just a per-task SFT.  While I understand this is what the community benchmarks on, I'd hope a more general RL framework like this might also enable generality across tasks as well.\n\n• Dynamic sampling discards groups with all‑zero or all‑one advantages. This can bias learning away from the hardest contexts that currently always fail and from already‑solved but still informative contexts.  It would be nice to still learn from these examples.\n\n• Comparison to “more SFT” is missing. Since Stage‑3 is an extra training phase, it ought to beat a carefully matched alternative that collects a small number of additional SFT demonstrations. I don't believe I saw this in the paper?  The few‑shot curves show that more SFT helps, but there is no direct human‑effort trade‑off between X additional demos and Y Stage‑3 rollout steps. It would be nice to add a controlled study that matches person‑hours  or data collection time or just raw samples.\n\nSmall things:\nAbstract: leave-on-out -> \"leave-one-out\""}, "questions": {"value": "* How would you construct reliable success signals in the real world for the LIBERO‑like tasks and how sensitive is RIPT to false labels? Do you expect dynamic sampling to amplify errors when detectors are noisy?  While the paper presents interesting results in simulation, I'm concerned that this approach is pretty impractical to apply in the real world (major concern).  Can you comment on this?\n\n* What is the sample complexity of Stage‑3 relative to adding demonstrations? It would be nice to compare against just adding a few more SFT examples.\n\n* For the continuous head, does training the scale head for NLL change the original OFT policy’s action distribution or degrade performance before RL begins?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r4UP65BbEM", "forum": "oXYZHg7HiZ", "replyto": "oXYZHg7HiZ", "signatures": ["ICLR.cc/2026/Conference/Submission2218/Reviewer_1KyZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2218/Reviewer_1KyZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762210222138, "cdate": 1762210222138, "tmdate": 1762916146094, "mdate": 1762916146094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}