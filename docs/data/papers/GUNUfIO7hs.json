{"id": "GUNUfIO7hs", "number": 17002, "cdate": 1758271100435, "mdate": 1759897205017, "content": {"title": "Contextual Forgetting: Mitigating Knowledge Obsolescence for Safe Lifelong Robot Learning", "abstract": "Lifelong Robot Learning, in its pursuit of general intelligence, confronts a critical yet overlooked challenge: endogenous safety risks arising from \"knowledge obsolescence.\" When a once-optimal policy becomes detrimental after an environmental shift, the conventional Continual Learning (CL) paradigm, which focuses on \"remembering,\" lacks an active \"forgetting\" mechanism, posing significant risks in the physical world. To address this, we introduce \"Contextual Forgetting,\" a novel mechanism, and design a Knowledge Validity Module (KVM). The core of KVM is a principled risk assessment framework based on an Energy-Based Model (EBM), enabling it to actively identify and mitigate hazardous interactions caused by knowledge inapplicability. We validate the efficacy of this framework by deeply integrating it with CODA-Prompt, an advanced CL algorithm. Experiments demonstrate that KVM significantly reduces catastrophic failures caused by knowledge obsolescence without sacrificing learning efficiency, providing a rigorous solution for building safer and more reliable lifelong learning robotic systems.", "tldr": "", "keywords": ["Lifelong Robot Learning", "Robot Safety", "Knowledge Obsolescence"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c2058003cd6452c62ea04a6bd161c1f8f9cacce.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a contextual forgetting algorithm to enable autonomous agents to forget knowledge they have once learned but is inappropriate in new contexts, e.g., after a distribution shift. The key component is a knowledge validity module that maps contexts into a Gaussian distribution, whose mean and variance may activate a dynamic safety threshold. Risk is then assessed via an energy-based model. Experiments on different robot benchmarks show that that the novel method outperforms baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Continual or lifelong learning is a relevant topic, and forgetting plays a central role. As such, the contribution is timely.\n\n2) The evaluation and comparison to the state-of-the-art is extensive."}, "weaknesses": {"value": "1) I don't quite follow the central claim that there is no mechanism for \"how to forget.\" When typing something like \"learning to forget\" in Google Scholar, various papers pop up that have developed such mechanisms. Also, context-based learning has been largely explored. Also there, old knowledge is not applied anymore as soon as we are in a new context. I think there is some more discussion of related literature required to adequately position the contributions of the paper. \n\n2) The authors promise a detailed theoretical justification in the appendix. I think it would be good to add some key elements to the main body. Also, the theorem in the appendix is missing a proof.\n\n3) The description of the method lacks important details. What exactly is the context $c_\\mathrm{raw}$? Is this something we are somehow observing? Or that we need to learn? How does $\\tau_\\mathrm{adaptive}$ enter the algorithm exactly? What is the feature representation $\\mathbf{h}$? What is $\\alpha_t$ that is mentioned in Section 3.2? \n\n4) The appendix seems a bit disorganized with various margin violations."}, "questions": {"value": "1) Can you clarify how your contributions relate to existing \"learning to forget\" frameworks and to general context-based learning?\n\n2) Is $c_\\mathrm{raw}$ something that we somehow assume as given, or is this something that is learned from observations?\n\n3) Can you clarify the algorithmic details (How does $\\tau_\\mathrm{adaptive}$ enter the algorithm exactly? What is the feature representation $\\mathbf{h}$? What is $\\alpha_t$ that is mentioned in Section 3.2?)?\n\n4) Can you provide some motivation/justification for the form of the validity score?\n\n5) What exactly is the feature representation $\\mathbf{h}$ that is used in the computation of the energy function?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OFI7idL53r", "forum": "GUNUfIO7hs", "replyto": "GUNUfIO7hs", "signatures": ["ICLR.cc/2026/Conference/Submission17002/Reviewer_a1oh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17002/Reviewer_a1oh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485572557, "cdate": 1761485572557, "tmdate": 1762927022863, "mdate": 1762927022863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address the “knowledge obsolescence” issue in continual learning, which means that a skill, once optimal in a specific context, might become a direct cause of catastrophic failure. To address this issue, the paper proposes Knowledge Validity Module (KVM) to actively identify and mitigate hazardous interactions caused by knowledge inapplicability. By integrating KVM with a continual learning algorithm CODA-Prompt, the paper conducted experiments to demonstrate that KVM significantly reduces catastrophic failures caused by knowledge obsolescence without sacrificing learning efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe studied “knowledge obsolescence” problem is interesting and underexplored in the literature.\n\n2.\tThe experimental results shown in the paper are good."}, "weaknesses": {"value": "1.\tThe presentation can be further improved. First, some important preliminary information is not provided, such as the basic setting of this work (and CODA-Prompt) , which makes it difficult to understand and evaluate the method section. Second, in section 3, the KVM is naively introduced without sufficient discussion. For example, why does KVM map raw multimodal context into a Gaussian distribution? What is the CL core? Third, in the experimental part, the paper makes some claims without solid empirical support. For example, in line 302, the paper says, “We attribute this to KVM’s ”Contextual Forgetting” mechanism”. Since the proposed method is CODA-Prompt + KVM and CODA-Prompt is missing in table 2, it’s not appropriate to claim that the benefit is from KVM not CODA-Prompt. And the discussion from line 370 to line 374 also looks subjective. \n\n2.\tThe “knowledge obsolescence” is an interesting topic. But the paper use examples like first “lifting hard pizza” task and then “lifting soft pizza” to highlight the “knowledge obsolescence”, which is not convincing. “lifting hard pizza” task and “lifting soft pizza” seem like two different tasks, and the failure of “lifting soft pizza” might be because this is a new task the model needs to learn but not “knowledge obsolescence”. In language models, an example of “knowledge obsolescence” is that the current President of USA is Trump not Biden who was President two years ago. Some other works, like knowledge/representation editing[1,2], also study “knowledge obsolescence” in language models. \n\n3.\tIt's claimed that the proposed method may significantly reduce catastrophic failures caused by knowledge obsolescence without sacrificing learning efficiency. I'm curious whether the KVM has a negative effect on previous capability, such as the performance of “lifting hard pizza” after learning “lifting soft pizza”?\n\n\n[1] Mass-Editing Memory in a Transformer\n[2] EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models"}, "questions": {"value": "1.\tHow to choose the hyperparameter k?\n\n2.\tSome format issues in subtitle 4.3, page 31, page 34.\n\n3.\tWhat is the definition of catastrophic failures? What is the Catastrophic Failure Rate in Table 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cCNsZMWoSJ", "forum": "GUNUfIO7hs", "replyto": "GUNUfIO7hs", "signatures": ["ICLR.cc/2026/Conference/Submission17002/Reviewer_pxpu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17002/Reviewer_pxpu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667462376, "cdate": 1761667462376, "tmdate": 1762927022267, "mdate": 1762927022267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of knowledge obsolescence in lifelong robot learning, where once useful knowledge becomes unsafe or harmful as the environment changes. Rather than focusing on preserving knowledge, the authors discuss mechanisms to discard outdated or unsafe information (forgetting) intentionally. \n\nContributions:\n\n1. The paper introduces a mechanism, \"Contextual Forgetting\" via a KVM that evaluates the relevance and safety of learned policies over time.\n\n2. Results indicate that CF–KVM reduces catastrophic failures caused by outdated knowledge while maintaining comparable learning efficiency to baseline continual learning methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper challenges the conventional assumption in continual learning that forgetting is always harmful. The perspective on how learning systems might handle outdated or risky behaviors is interesting.\n\n2. The topic of knowledge obsolescence and its link to safety is timely."}, "weaknesses": {"value": "Major concerns:\n\n1. The paper emphasizes safety and hazard reduction. However, all experiments are conducted in simulation using LIBERO and SimplerEnv, which are generalization and robustness benchmarks rather than safety evaluations. There are no quantitative safety metrics, no modeling of physical risk, and no real-robot validation.\n\n2. The contribution of the proposed Knowledge Validity Module (KVM) and its Energy-Based Model component is unclear. The ablation study in Table 4 reports small numerical differences but lacks statistics such as standard deviation, mechanistic interpretation, and a clear definition of \"catastrophic failure\".\n\n3. The paper uses an energy defined in Eq. 5-6. However, such an energy framework cannot distinguish novel-but-safe from old-and-dangerous interactions, as both are far from past safe examples in feature space. This causes either: (1) false negatives allowing catastrophic old policies, or (2) false positives blocking necessary safe exploration. The method section should include relevant discussions."}, "questions": {"value": "Q1: Can the authors provide more discussions and justifications on real-robot validation with quantitative physical risk metrics?\n\nQ2: Table 4 shows only 1.9% improvement (70.5% vs 68.6%) when adding KVM. Is this statistically significant? \n\nQ3: How does the energy model (Eq. 5-6) distinguish novel-but-safe from old-and-dangerous interactions? Can authors provide precision/recall metrics for the experiment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jA0Npz7PpK", "forum": "GUNUfIO7hs", "replyto": "GUNUfIO7hs", "signatures": ["ICLR.cc/2026/Conference/Submission17002/Reviewer_bUwU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17002/Reviewer_bUwU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963903592, "cdate": 1761963903592, "tmdate": 1762927021751, "mdate": 1762927021751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}