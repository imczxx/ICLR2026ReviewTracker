{"id": "mK7i1td7uQ", "number": 9671, "cdate": 1758133890073, "mdate": 1759897705463, "content": {"title": "ConforFormer: Representation for Molecules Through Understanding of Conformers", "abstract": "Recent years have seen a growing interest in machine learning approaches for chemical tasks. The best existing methods focus on building base models that combine molecular graphs (“2D structures”) with atomic coordinates in 3D to predict molecular properties, typically through pre-training followed by fine-tuning on benchmark datasets. However, current approaches require retraining the entire model for each prediction task, using published weights only as initialization. While this enables state-of-the-art performance, it limits practical deployment, as real-world datasets are often too small to support the stable retraining of large models. Importantly, the 3D geometry of a molecule holds crucial information for predicting its properties, but a single molecular graph usually corresponds to several 3D geometries, called conformers, introducing ambiguity into the inference process. Typical solutions rely on molecular graphs, but this approach is not easily generalizable beyond organic molecules. Here, we present ConforFormer, a method that explicitly accounts for the diversity of 3D conformations of a molecule to derive a task-agnostic and conformation-agnostic vector representation. This model serves as a foundational framework, producing embeddings that can be generated once and directly applied to downstream tasks, including property prediction and structural similarity, without extensive fine-tuning.", "tldr": "The paper presents a new conformation-agnostic, task-agnostic dense embedding for molecular 3D structures obtained through contrastive learning.", "keywords": ["computational chemistry", "foundation models", "embeddings", "transformers", "cheminformatics", "representation learning", "contrastive learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39b91073e81766b6c2e9d4c6dd780243e621dd2a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper modifies Uni-Mol by retraining the model without “flat” structures (molecules with all Z coordinates set to zero), finding minimal impact on performance; introducing a contrastive learning objective to align embeddings across multiple 3D conformations, though the novelty over Uni-Mol’s existing contrastive loss is unclear; and exploring the use of the Organic Molecules (OMol) dataset instead of the original Uni-Mol dataset, which did not lead to significant improvements. Overall, the method consists of incremental adjustments rather than a fundamentally new approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is detailed and uses domain-specific terminology appropriately."}, "weaknesses": {"value": "Overall, the paper reads more like a technical report on modifications to Uni-Mol rather than presenting a strong methodological contribution.\n\n- **Modification of Uni-Mol:**\n   - The authors identified that the original Uni-Mol model included “flat” structures (atoms with Z = 0) in ~9% of the training data and retrained the model without them, finding negligible performance degradation.\nWhile this is a useful insight, it is minor. The original design helps Uni-Mol generalize to molecules with incomplete 3D information or planar molecules (e.g., aromatic rings, graphene fragments). Showing that removing this feature has little effect is interesting but not a major contribution.\n  - The paper proposes a contrastive learning objective to align embeddings across multiple conformations. However, Uni-Mol already applies contrastive learning across conformations, where positive pairs are conformations of the same molecule and negatives otherwise.\nIt is unclear how the proposed contrastive loss differs from the original Uni-Mol approach, and this should be clarified.\n  - The exploration of the Organic Molecules (OMol) dataset instead of the original Uni-Mol dataset did not lead to noticeable performance improvements.\n- The authors do not include a baseline using only graph data (without 3D information). Without this, the effectiveness of 3D conformations cannot be properly evaluated, especially when some works has shown that using RDKit-generated 3D coordinates can even lead to degrade performance (RDKit is not very accurate in this task).\n- **Terminology and ML Understanding:** \n  - The manuscript misuses some CS/ML terminology. For example, “retrain” is incorrectly used where “fine-tune” is meant (e.g., abstract: *current approaches require retraining the entire model for each prediction task, using published weights only as initialization* → this is actually fine-tuning).\n  - Moreover, the statement *current approaches require retraining the entire model for each prediction task, using published weights only as initialization* is inaccurate at a deeper level: pretrained models can be used as feature extractors to train other models for downstream tasks. This is one of the main motivations of the paper, undermining the importance of the actual problem they are trying to solve. Overall, this suggests a weak understanding of ML concepts.\n  - The claim *from a physical point of view, molecular graphs do not exist* is unconvincing as an argument against this datatype. Many representations (FASTA sequences, DNA sequences, or even text) do not physically exist in the same sense, yet are useful abstractions. This argument does not support the proposed approach.\n  - The claim *structural formulas work well for the chemistry of organic molecules, but for more complex compounds* is misleading. The sentence implies that *more complex compounds* (such as organometallics) are not organic molecules, which is not entirely accurate. Moreover, I think this work focuses on building models for representing organic compounds.\n  - The term *task-agnostic* in the abstract is an overstatement, since the authors still fine-tune their models on downstream tasks."}, "questions": {"value": "How does your use of contrastive learning differ from that in Uni-Mol?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S5APtJ6O5D", "forum": "mK7i1td7uQ", "replyto": "mK7i1td7uQ", "signatures": ["ICLR.cc/2026/Conference/Submission9671/Reviewer_a89c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9671/Reviewer_a89c"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709641721, "cdate": 1761709641721, "tmdate": 1762921190291, "mdate": 1762921190291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a pretraining strategy that incorporates conformational information to enhance molecular representation learning. The authors argue that modeling 3D space is essential for capturing molecular properties. While the motivation is sound, the manuscript suffers from several critical issues that undermine its contribution."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The topic of incorporating 3D information is well-motivated."}, "weaknesses": {"value": "1. **Misrepresentation of Prior Work**  \n   The authors claim that “no chemical embedding model capturing the diversity of 3D molecular conformations has yet been published.” This statement overlooks a substantial body of literature on conformer-aware pretraining. Several existing models explicitly incorporate 3D conformational diversity, and the lack of engagement with these works raises concerns about the novelty and scholarly rigor of the paper.\n\n2. **Limited Novelty**  \n   The proposed techniques—pretraining on conformers and freezing the backbone while fine-tuning only the final MLP layer—are well-established practices in molecular machine learning. The manuscript does not present sufficient innovation beyond these standard approaches.\n\n3. **Underwhelming Performance**  \n   As shown in Tables 1 and 2, the model's performance falls short of state-of-the-art methods across multiple benchmarks. The results do not convincingly demonstrate that the proposed approach yields meaningful improvements in molecular representation quality."}, "questions": {"value": "**Questionable Embedding Behavior**  \n   Figure 5 presents a pair of conformers with substantial geometric differences. If ConforFormer-OMol had truly learned a robust understanding of 3D molecular structure, the cosine similarity between these embeddings should be significantly lower. This example casts doubt on the model’s ability to distinguish conformational nuances."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Uyehy6qUoi", "forum": "mK7i1td7uQ", "replyto": "mK7i1td7uQ", "signatures": ["ICLR.cc/2026/Conference/Submission9671/Reviewer_mwTZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9671/Reviewer_mwTZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713719029, "cdate": 1761713719029, "tmdate": 1762921189759, "mdate": 1762921189759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the use of advanced contrastive learning techniques to enhance molecular representation learning. By introducing the ConforFormer framework, the authors aim to develop conformation-invariant molecular embeddings that capture 3D geometric information without relying on explicit molecular graphs. Although the idea is conceptually interesting and relevant to modern chemical foundation models, the method shows limited novelty beyond existing architectures such as Uni-Mol, and several claims lack sufficient experimental or theoretical support."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Using advanced contrastive learning to improve molecular representation learning is an interesting research topic."}, "weaknesses": {"value": "1. The presentation of the paper is poor, making it difficult to understand the research problem and motivation it aims to address. In the abstract, the authors argue that existing methods using published weights only as initialization have certain limitations.  \nI do not think this is a real limitation, since most approaches intentionally leverage pretrained foundation models to support various downstream tasks. In the introduction, the authors claim that real-world datasets are often too small to allow stable retraining, which is also not accurate, as many domain adaptation techniques—such as few-shot learning and data augmentation—can effectively address this issue.  \n\n2. The proposed method lacks novelty. The so-called *“new weakly supervised contrastive learning objective”* is essentially the standard contrastive loss without any additional innovation. The authors claim to propose a novel structure called **ConforFormer**, but it is architecturally identical to **Uni-Mol**, except for the added contrastive learning objective.  Also, some other paper already has used Contrastive Learning for 3D molecular representation learning, see [1]\n\n3. Some claims in the paper lack sufficient evidence. For example, the paper mentions *“a benchmark evaluating the model's ability,”* but there is no open-source release or supporting evidence provided to describe the benchmark in detail.  \n\n[1]Qin, Jiayu, et al. \"A probability contrastive learning framework for 3D molecular representation learning.\" Advances in Neural Information Processing Systems 37 (2024): 58058-58076."}, "questions": {"value": "What are the detailed definitions of the loss terms in the total loss (e.g., L_token, L_coord, L_distance)?  \nHow were these terms computed, and how were the coefficients (5, 10, 2) determined — empirically or theoretically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lIkf9pmdvN", "forum": "mK7i1td7uQ", "replyto": "mK7i1td7uQ", "signatures": ["ICLR.cc/2026/Conference/Submission9671/Reviewer_W54S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9671/Reviewer_W54S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857525776, "cdate": 1761857525776, "tmdate": 1762921189471, "mdate": 1762921189471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ConforFormer, a Transformer-based molecular representation model that aims to learn conformation-invariant molecular embeddings through contrastive learning across different 3D conformers of the same molecule. The goal is to obtain general molecular representations that capture structural consistency without requiring task-specific fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets a meaningful and relevant problem, how to build robust molecular representations that account for 3D conformational variability.\n2. The proposed framework is conceptually clear and easy to follow, with a reasonable motivation and solid experimental setup.\n3. The introduction of the PharmIsomer benchmark provides an interesting way to evaluate whether models can distinguish between conformers and isomers. The writing and figures are clear, making the overall presentation accessible."}, "weaknesses": {"value": "1. Limited technical novelty: The approach mainly extends existing ideas from contrastive learning and 3D molecular representation (e.g., Uni-Mol) without introducing substantial methodological innovation.\n2. The results do not show clear or consistent improvements over strong baselines such as Uni-Mol; in some benchmarks, performance is even slightly worse. This weakens the paper’s contribution, since if training the baseline is not computationally expensive, practitioners would still prefer to fine-tune an existing model rather than use ConforFormer’s frozen representation."}, "questions": {"value": "Please refer to the cons"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sHvaXh11dG", "forum": "mK7i1td7uQ", "replyto": "mK7i1td7uQ", "signatures": ["ICLR.cc/2026/Conference/Submission9671/Reviewer_PKKJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9671/Reviewer_PKKJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138265741, "cdate": 1762138265741, "tmdate": 1762921189198, "mdate": 1762921189198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}