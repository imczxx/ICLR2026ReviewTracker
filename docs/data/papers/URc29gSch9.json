{"id": "URc29gSch9", "number": 21316, "cdate": 1758316192479, "mdate": 1759896928752, "content": {"title": "Factual and Musical Evaluation Metrics for Music Language Models", "abstract": "Music language models (Music LMs), like vision language models, leverage multimodal representations to answer natural language queries about musical audio recordings. Although Music LMs are reportedly improving, we find that current evaluations fail to capture whether their answers are correct. Specifically, for all Music LMs that we examine, widely-used evaluation metrics such as BLEU, METEOR, and BERTScore fail to measure anything beyond linguistic fluency of the model's responses. To measure the true performance of Music LMs, we propose (1) a better general-purpose evaluation metric for Music LMs adapted to the music domain and (2) a factual evaluation framework to quantify the correctness of a Music LM's responses. Our framework is agnostic to the modality of the question-answering model and could be generalized to quantify performance in other open-ended question-answering domains. We use open datasets in our experiments and will release all code on publication.", "tldr": "We propose a better general-purpose evaluation metric for Music LMs adapted to the music domain and and a factual evaluation framework to quantify the correctness of a Music LM's responses.", "keywords": ["music language models", "music captioning", "music question-answering", "evaluations", "benchmarking"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce1c8fb4a98f51ba62ad7b059a886355ca8e991c.pdf", "supplementary_material": "/attachment/1dc6b081e03cdb9d261014dc243241b11b8a6857.pdf"}, "replies": [{"content": {"summary": {"value": "The paper demonstrates the limitations of current metrics for evaluating music language models; it then proposes a new metric based on the CLAP audio-text embeddings which more accurately captures the performance of music LMs. Finally the paper proposes a methodology for converting the output of music LMs into a set of labels which can then be evaluated using standard classification or retrieval metrics."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a timely and important problem in the music AI literature, which is on the limitations of current (NLP-based) evaluation metrics to evaluate the performance of music language models. The proposed metric based on CLAP embeddings is a simple and yet very effective solution to address this issue. The paper also evaluates a representative range of music LMs using benchmark open datasets."}, "weaknesses": {"value": "* The third and final contribution of this work is relatively disjointed from the first two contributions. At the same time, the paper does not acknowledge the limitations between the \"factuality protocol\", which appears to only be effective for evaluating performance on a fairly narrow set of multi-label classification tasks related to music (e.g. audio tagging, instrument identification).\n* Evaluations in 3.4 only use one variant of CLAP, and could have been expanded as to cover other CLAP variants but also other audio-text embeddings - as to answer the question on whether CLAP is better suited as a model to evaluate music LM performance.\n* Section 5 does not include any critical reflections on this work, and does not acknowledge any limitations or areas of possible or future improvement.\n* There is no mention in the paper on the possible availability of the evaluation metrics, e.g. as a package or library so that they can be adopted by the community."}, "questions": {"value": "* Section 3.4: I would suggest to include other CLAP variants but also include other common audio-text embeddings in comparative evaluations. This is in order to answer the question on whether CLAPtext is really a good predictor of musicLM performance over other such audio-text models.\n* The \"factual QA\" work in section 4 (I have put the term factual in quotes since I do not believe the term is appropriately used in the paper, given that there are many other possible factual questions not covered by the methodology) converts music LM outputs into a set of tags or labels. This is appropriate for evaluating performance on specific downstream tasks such as audio tagging or instrument identification, however does not capture other important tasks in the wider MIR literature - or new tasks which involve temporal grounding and involve inferences on musical sequences.\n* Section 5 can be radically rewritten as to include a more critical and informative discussion on the strengths and areas of development of the proposed methodologies - leading also to possible directions for future work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dQ1jDvwO5x", "forum": "URc29gSch9", "replyto": "URc29gSch9", "signatures": ["ICLR.cc/2026/Conference/Submission21316/Reviewer_ouqf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21316/Reviewer_ouqf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761085356491, "cdate": 1761085356491, "tmdate": 1762941690949, "mdate": 1762941690949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on metrics for Music LMs and Factual QA evaluation. It shows that six commonly reported metrics for evaluating Music LMs fail to measure these models’ ability to extract information from audio inputs. The authors also propose a new, musically-aware similarity metric, CLAPText. The experimental results using paraphrase and adversarial text show CLAPText is robust on those tricky cases."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper provides a valuable analysis of the music domain, addressing the limitations of commonly used metrics such as BLEU in effectively evaluating models in this context. \n- The proposal to use CLAPText as an evaluation metric is a reasonable and intuitive approach. Furthermore, the experimental results with adversarial text offer a suggestive and insightful contribution to the field."}, "weaknesses": {"value": "-\tThe highest-scoring case (paraphrase) and the lowest-scoring cases (adversarial/random) have been evaluated. However, as a validation of the evaluation metric, it is also necessary to confirm whether diverse cases can be appropriately ranked in order. For example, when there are partial differences (e.g., only some instruments are incorrect, or the information is partially correct but includes additional new incorrect information), the score should change gradually to reflect these differences.\n-\tWhile the weaknesses in adversarial cases have been demonstrated, they can be considered special cases. Ideally, the evaluation should enable the comparison between models to determine which one is better. The \"Correct\" case in Table 2 seems to correspond to the performance evaluation results, but the rankings appear to differ across the metrics. It would be better to verify whether the scores indicated by CLAPText provide more appropriate evaluation values (e.g. the correlation with human evaluation results)."}, "questions": {"value": "- The Model columns in the last row of Table 2 are empty, which may look like typos. \n- The term \"Prompt\" in Table 2 is unclear. Wouldn't it be more appropriate to clearly indicate that it replaces the reference used during evaluation, such as \"Reference Answer.\"? (In that case, the \"Reference answer\" in Figure 1 should be changed to \"Gold answer\" or something.)\n- It would be better to include the actual prompts used for generating paraphrases and adversarial sentences in the Appendix.\n- In Table 4, comparing the F1-scores, LlaMA-Adapter seems to perform better than MU-LlaMA. When comparing the evaluation values for the models in MusicQA-Jamendo in Table 2 for the \"Correct\" case, only ROUGE follows that order. Similarly, for Llama-Adapter and SALMON, Llama-Adapter performs better, but only ROUGE follows that order in any of the data in Table 2. From these results, if Llama-Adapter is a strong model for instrument recognition and genre classification, is it possible that ROUGE evaluates the actual model outputs better than CLAPText?\n- It would be better to have some analysis (e.g., human verification) to ensure that the adversarial texts actually contain the expected content."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R72FOgLD99", "forum": "URc29gSch9", "replyto": "URc29gSch9", "signatures": ["ICLR.cc/2026/Conference/Submission21316/Reviewer_S1xe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21316/Reviewer_S1xe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761497247769, "cdate": 1761497247769, "tmdate": 1762941690571, "mdate": 1762941690571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a critical look at evaluation protocols for Music LMs, providing both a better quantitative metric for open-ended QA and an automated evaluation pipeline for factual correctness of open-ended responses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors' motivation is strong, as the current state of evaluation for Music LMs being rather dubious.\n- The construction of the factuality evaluation protocol in particular is reasonably strong. Such a framework gets at the heart of the failure modes of many modern Music LMs."}, "weaknesses": {"value": "- It feels as if this paper is caught between two similar yet practically orthogonal contributions: ClapText and their factual evaluation protocol. This muddies the overall flow and contribution of the paper, as substantially more content is dedicated to the weaker results (ClapText) over the correctness evaluation.\n- Overall, the evaluation with ClapText is not massively convincing. First off, it is unclear why the authors opted for random sampling of audio prompts rather than Gaussian noise (as done in previous work [1,2]) for their \"random\" evaluation. The fact that gaussian noise causes prototypical performance degradation is *the point* of using it, as it ensures absolutely no information contained in the audio, while the choice of random audio gives no gaurantee that there isn't information present in the random sample that is relevant to the given query (one could imagine that in a set of reasonably similar songs, this might be more common than you'd think). More importantly, the results show that the ClapText scores are higher for the *adversarial* prompt than for **any** of the \"Correct\" category. This seems to imply that either (1) ClapText is still sensitive to correctness but the model tested outputs are so bad that they are more wrong than the actively incorrect adversarial prompt or (2) while more robust than the other metrics, ClapText is still fooled by the adversarial responses relative to the possibly *good* outputs of the model when given correct prompts. In the present manuscript, it is thus impossible to tell which case the results are in (as there is no independent evaluation of the tested models' responses), and hence it is hard to claim any strict improvement from the ClapText metric.\n- While the pipeline for the factuality evaluation seems strong, the overall evaluation of it is relatively limited. It is unclear to me why the only tested models are ones that have been long since passed by more SOTA systems [2,3]. This makes the evaluation hard to conclude anything from, as it is hard to tell whether the performances shown would continue for more modern systems. This also connects back to the ClapText evaluation, in that using more modern systems (Qwen-2-Audio, Qwen-3-Omni, Audio Flamingo 2, Audio Flamingo 3) would be useful for improving evaluation. While these systems are trained on more general audio rather than just music, they have clear documented improved performance over music-only models on QA tasks [2,3].\n- One small point, it would be useful throughout the manuscript to change the reference of \"Music LMs\" to \"Large Audio Language Models (LALMs), as this is the standard nomenclature increasingly being taken in the community [1,2], and \"MusicLMs\" runs the risk of confusing readers with the series of music *generation* models that using AR transformers (including the one expliciltly called MusicLM [4, 5]).\n\nOverall, I think there are too many issues in the present manuscript to recommend acceptance, but I do believe the present draft could be reasonably improved by expanding the evaluation suite to use more modern models and performing a more careful analysis of the ClapText metric.\n\n[1] Kumar, Sonal, et al. \"Mmau-pro: A challenging and comprehensive benchmark for holistic evaluation of audio general intelligence.\" arXiv preprint arXiv:2508.13992 (2025).\n[2] Zang, Yongyi, et al. \"Are you really listening? boosting perceptual awareness in music-qa benchmarks.\" arXiv preprint arXiv:2504.00369 (2025).\n[3] Weck, Benno, et al. \"Muchomusic: Evaluating music understanding in multimodal audio-language models.\" arXiv preprint arXiv:2408.01337 (2024).\n[4] Agostinelli, Andrea, et al. \"Musiclm: Generating music from text.\" arXiv preprint arXiv:2301.11325 (2023).\n[5] Copet, Jade, et al. \"Simple and controllable music generation.\" Advances in Neural Information Processing Systems 36 (2023): 47704-47720."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gJAOwIVlov", "forum": "URc29gSch9", "replyto": "URc29gSch9", "signatures": ["ICLR.cc/2026/Conference/Submission21316/Reviewer_cizx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21316/Reviewer_cizx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763832477, "cdate": 1761763832477, "tmdate": 1762941690301, "mdate": 1762941690301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that common text metrics (BLEU, METEOR, BERTScore) fail to assess whether Music LMs actually use audio. It proposes (i) CLAPText, a music-aware text similarity metric, and (ii) a factual QA protocol that converts open-ended responses into structured labels to compute precision/recall/F1. Experiments on MusicQA-style data and FMA/MusicNet tasks suggest text metrics cannot distinguish correct-audio from random-audio baselines, while the proposed metrics better reflect audio grounding."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear problem framing. Highlights a real gap: NLP metrics reward fluency, not audio-grounded correctness.\n\nSimple, reusable tools. CLAPText is a practical drop-in; the factual QA pipeline is modality-agnostic.\n\nBaseline design is insightful. Random-audio and adversarial paraphrase conditions stress-test audio use vs. surface text overlap.\n\nOpen datasets/code intent. Encourages reproducibility and broader adoption."}, "weaknesses": {"value": "Questionable data quality for MusicQA. A large portion of prompts/answers stems from MPT-7B generation; prior work reports hallucinations and low musician approval (e.g., MusiLingo) — weakening conclusions about “correct” vs. “random” gaps when “correct” itself is noisy.\n\nModel coverage is thin. Aside from SALMONN, evaluated models are not the strongest current baselines; conclusions about metric validity would be more convincing with Qwen-Audio, Qwen2.5-Omni, ChatGPT-5, Gemini 2.5 Pro, etc.\n\nAdversarial answers lack expert validation. No musician audit to confirm that “adversarial” outputs are truly incorrect yet linguistically plausible.\n\nNarrow attribute set. Factual QA focuses on genre and instrument; key musical facets (emotion/mood, tempo, key/tonality, meter, timbre attributes) are omitted, limiting external validity.\n\nPotential prompt sensitivity. Results may hinge on prompting; robustness across prompts and extraction rules is under-analyzed."}, "questions": {"value": "Quality control of “correct” references.\nWhat fraction of MusicQA items were musician-verified? Please report inter-annotator agreement and an error taxonomy (hallucination, ambiguity, label noise). How do results change when restricting to a high-quality subset?\n\nStronger baselines.\nCan you include evaluations of Qwen-Audio / Qwen2.5-Omni / ChatGPT-5 / Gemini 2.5 Pro to test whether CLAPText and the factual protocol still separate correct-audio from random-audio with state-of-the-art models?\n\nAdversarial validation.\nWill you run a blind musician audit on adversarial answers to confirm factual incorrectness and estimate the false-negative rate of your protocol?\n\nAttribute coverage.\nCan you extend factual QA to tempo (BPM ranges), key/scale, meter, dynamic markings, and emotion/mood labels? Even small, well-vetted subsets would strengthen claims of generality.\n\nNoise/no-audio controls.\nPlease add no-audio, shuffled-audio, and additive-noise baselines to quantify how CLAPText and factual F1 degrade with audio corruption.\n\nPrompt and extractor robustness.\nHow sensitive are results to (a) prompt templates and (b) the keyword-extraction LLM? Consider reporting variance across prompts and an ablation using rule-based extractors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "82qFzWwsul", "forum": "URc29gSch9", "replyto": "URc29gSch9", "signatures": ["ICLR.cc/2026/Conference/Submission21316/Reviewer_pLNp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21316/Reviewer_pLNp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939979211, "cdate": 1761939979211, "tmdate": 1762941689806, "mdate": 1762941689806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}