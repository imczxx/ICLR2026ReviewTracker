{"id": "h2iWDKEIEt", "number": 5665, "cdate": 1757926367767, "mdate": 1759897962142, "content": {"title": "Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Networks", "abstract": "Semantic segmentation networks (SSNs) are central to safety-critical applications such as medical imaging and autonomous driving, where robustness under uncertainty is essential. However, existing probabilistic verification methods often fail to scale with the complexity and dimensionality of modern segmentation tasks, producing guarantees that are overly conservative and of limited practical value. We propose a probabilistic verification framework that is architecture-agnostic and scalable to high-dimensional input-output space. Our approach employs conformal inference (CI), enhanced by a novel technique that we call the **clipping block**, to provide provable guarantees while mitigating the excessive conservatism of prior methods. Experiments on large-scale segmentation models across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrate that our framework delivers reliable safety guarantees while substantially reducing conservatism compared to state-of-the-art approaches on segmentation tasks.", "tldr": "", "keywords": ["Semantic Segmentation", "Conformal Inference", "Verification", "Robustness"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9a3de153867384da5becc6258da7eb5ccf385f43.pdf", "supplementary_material": "/attachment/7a0c5bbb4987d87d1a4d267ef9a968b29b2bc40e.zip"}, "replies": [{"content": {"summary": {"value": "Semantic segmentation networks (SSNs) are important in safety-critical domains such as medical imaging and autonomated driving. Existing probabilistic verification techniques struggle to scale to the high complexity and dimensionality of modern SSNs, often resulting in guarantees that are overly conservative and of limited practical use. The authors introduce a scalable and architecture-independent probabilistic verification framework based on conformal inference (CI). They integrate a novel clipping block mechanism that refines uncertainty calibration while preserving formal guarantees. Experiments on large-scale segmentation benchmarks - including CamVid, OCTA-500, Lung Segmentation, and Cityscapes - show that the approach provides reliable safety assurances and tighter guarantees than state-of-the-art probabilistic verification methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The authors deal with an interesting and important topic.\n- Current research in this area is described by the authors.\n- It's good that all the preliminaries are explained, which is quite a lot.\n- The different used datasets are good."}, "weaknesses": {"value": "- Right at the beginning, terms such as randomized smoothing and conformal inference are simply assumed to be familiar. A brief explanation and, in particular, a distinction between the two would be helpful for understanding. Not so easy to follow if you have no prior knowledge of CI - everything is derived, but little explanation is given as why it works.\n- The preliminaries (section 2) and a large part of section 3 (including algorithms 1 and 2) are taken almost one-to-one from the reference Hashemi et al.\n- I would like to see more experiments in comparison to the method used by Hashemi et al., as the approach presented by the authors is very similar to this.\n- The computing power required for these experiments is already very high. It would be interesting to compare this with randomized smoothing.\n- The Cityscapes results in Appendix D are not available.\n- The introduction refers directly to Appendix A, where no notation is provided and is therefore difficult to understand.\n- I find that the results are hardly described/explained."}, "questions": {"value": "Conformal inference with reachset in the form of a convex hull, the surrogate model, and the PCA approach have all already been done in Hashemi et al.'s paper. What is the novelty of the approach presented here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oiWdYNzbOC", "forum": "h2iWDKEIEt", "replyto": "h2iWDKEIEt", "signatures": ["ICLR.cc/2026/Conference/Submission5665/Reviewer_RAZP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5665/Reviewer_RAZP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761145963604, "cdate": 1761145963604, "tmdate": 1762918183098, "mdate": 1762918183098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is interested in verification of semantic segmentation networks and obtaining coverage guarantees of their outputs. The proposed approach defines a surrogate model by computing the convex hull of outputs obtained from a training set of adversarial data, and then projects new points on the convex hull by solving a linear programming problem. To deal with high dimensional data, the method relies on PCA learned from the training samples to reduce the dimension before computing the convex hull and the projection. The simple surrogate model can then be efficiently verified."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- **Originality:** The method is novel in its direct use of a convex hull of logits to define the surrogate reachset. This is a more direct geometric approach that avoids the need for an additional training step (unlike the ReLU network in Hashemi et al. 2025)."}, "weaknesses": {"value": "- **Clarity:** The paper suffers from a severe lack of clarity in defining its contribution. The method and explanation heavily rely on and closely mirror Hashemi et al. (2025). The only substantive difference appears to be the replacement of an additional learned ReLU surrogate network with the direct convex hull projection. This strong reliance makes it nearly impossible to evaluate the paper's unique contribution and necessitates a much clearer and more explicit discussion of the differences.\n- **Significance:** The paper claims improved scalability and efficiency as key motivations, stating the prior work \"could not handle our perturbation dimensions.\" This is directly contradicted by the results presented in Hashemi et al. (2025) on the same dataset and settings, often with faster runtime. This undermines the paper's core claim about the need for a new, more efficient approach.\n- **Quality:** The experimental section is limited and not convincing. The absence of a comparison with the results obtained by the method in Hashemi et al. (2025) (or any other method) makes the presented results impossible to contextualize or evaluate."}, "questions": {"value": "- Could you explicitly clarify the novel technical and conceptual contributions of this paper? Specifically, beyond replacing the ReLU network with the direct convex hull computation, what fundamental differences exist, and why are they necessary for this problem?\n- Could you provide a direct, quantitative comparison of the coverage guarantees, runtime, or any other relevant metric, against their results?\n- Could the proposed approach be extended to other pixel-wise prediction tasks, e.g. depth estimation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8V2w7IA5cW", "forum": "h2iWDKEIEt", "replyto": "h2iWDKEIEt", "signatures": ["ICLR.cc/2026/Conference/Submission5665/Reviewer_6pG7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5665/Reviewer_6pG7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761590972307, "cdate": 1761590972307, "tmdate": 1762918182738, "mdate": 1762918182738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors propose a probabilistic verification framework for semantic segmentation networks (SSNs) using conformal inference enhanced with a novel clipping block. The method addresses limitations of existing work by replacing a trained ReLU surrogate model with a convex hull projection approach. They conduct experiments on 4 large-scale segmentation datasets, namely, CamVid, OCTA-500, Lung segmentation, and Cityscapes and demonstrate scalability to perturbations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The clipping block is training-free which is a big advantage compared. Hence it can be used in a plug-and-play manner with any existing model.\n- The authors provide extensive formal proofs and guarantees regarding probabilistic coverage.\n- Extensive experiments (4 large and popular segmentation datasets, several perturbation dimensions and magnitudes)"}, "weaknesses": {"value": "- The use of PCA and linear programming to project onto a convex hull in high dimensions is computationally expensive\n- Authors do not provide any study of sensitivity to N (parameter in PCA)\n- Authors should compare to other baselines such as randomized smoothing, hashemi etc, atleast on the toy example\n- Authors should provide analysis of how changing confidence levels ($\\delta_1, \\delta_2$) affects tightness of robustness\n- Currently the authors focus on norm perturbations but it would be interesting if authors could provide a discussion on other perturbations like blur, affine transformations, brightness etc\n- The authors only use $l \\infty$ but it would be interesting to compare $l_1,l_2$ too"}, "questions": {"value": "Please see weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FSApVsPgjN", "forum": "h2iWDKEIEt", "replyto": "h2iWDKEIEt", "signatures": ["ICLR.cc/2026/Conference/Submission5665/Reviewer_SLRv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5665/Reviewer_SLRv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898160783, "cdate": 1761898160783, "tmdate": 1762918182421, "mdate": 1762918182421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new probabilistic verification framework for semantic segmentation networks (SSNs). Existing methods for certified robustness (especially randomized smoothing) struggle to scale to high-dimensional segmentation tasks. The authors propose a scalable, architecture-agnostic approach combining conformal inference (CI) with a novel clipping block surrogate model. The clipping block projects network outputs onto a convex hull formed from training logits, avoiding the need for training a separate surrogate network (as in Hashemi et al., 2025). The approach provides provable $(\\epsilon, l, m)$ probabilistic guarantees, supports general $L_p$ perturbations, and reduces conservatism. Experiments on CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrate improved scalability and less conservative robustness bounds compared to prior CI and randomized smoothing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The “clipping block” is an elegant, training-free replacement for surrogate ReLU networks, avoiding fidelity and scalability issues.\n- The paper extends CI to large-scale probabilistic reachability with formal $(\\epsilon, l, m)$ guarantees.\n- Demonstrated on realistic, high-dimensional datasets and large segmentation models (UNet, BiSeNet, HRNetV2).\n- The paper systematically contrasts its approach with prior CI and randomized smoothing methods.\n- Includes an anonymous toolbox and detailed algorithmic pseudocode."}, "weaknesses": {"value": "- While multiple datasets are used, the evaluation focuses on a narrow type of perturbation (darkening) and may not reflect broader robustness (e.g., geometric or semantic transformations).\n- The empirical section omits comparisons to recent certified robustness methods beyond Hashemi et al. (2025) and smoothing approaches.\n- The convex hull projection step is computationally heavy; PCA-based dimensionality reduction mitigates this but may introduce approximation bias.\n- The paper’s exposition is dense and overly mathematical in sections 3,4, which may obscure intuition for readers less familiar with CI.\n- No ablation on PCA vs. clipping: It’s unclear how much each contributes to scalability and accuracy improvements."}, "questions": {"value": "- How sensitive is the robustness value (RV) to the choice of calibration size $m$ and the PCA dimensionality $N$?\n- Can the convex hull projection scale beyond the datasets tested?\n- How would this approach handle distributional shift in test data; does the CI guarantee still hold under covariate drift?\n- Could the clipping block approach be combined with randomized smoothing to strengthen guarantees?\n- What is the empirical runtime or memory bottleneck for convex hull construction as t and n grow?\n- Is it possible to simplify the presentation for the reader, and add additional retails (relevant background etc.) in the Appendix?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BGTUDEeS8v", "forum": "h2iWDKEIEt", "replyto": "h2iWDKEIEt", "signatures": ["ICLR.cc/2026/Conference/Submission5665/Reviewer_LpMV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5665/Reviewer_LpMV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981782808, "cdate": 1761981782808, "tmdate": 1762918182130, "mdate": 1762918182130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}