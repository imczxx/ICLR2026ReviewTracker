{"id": "cXKzStnOIQ", "number": 23175, "cdate": 1758340551704, "mdate": 1759896828698, "content": {"title": "LATTE: Learner-Adaptive Teacher-Forced Reflection for Advancing Deep Search", "abstract": "Deep search in LLMs hinges on efficiently acquiring external knowledge and up-to-date information to ground reasoning and generation. However, deep search agents often over-trust internal reasoning, terminate prematurely, and under-use external tools, resulting in brittle long-horizon performance. To address this, we introduce LATTE, a mixed-policy reinforcement learning framework that integrates teacher-forced, learner-adaptive reflection to provide oriented guidance that explicitly pushes the model to reflect, extend search rounds when evidence is insufficient, and increase the probability of beneficial tool calls. At each on-policy iteration, we seed reflective trajectories from the current policy’s deep-search rollouts and inject teacher-forced critiques and corrections at decision points that govern whether to continue or stop the search and whether to defer to a tool or proceed with self-reasoning. By conditioning guidance on the learner’s observed behavior and uncertainty, LATTE preserves on-policy updates while narrowing the gap between supervision and policy behavior, yielding an implicit curriculum focused on current failure modes (e.g., premature stopping, missed or delayed tool deferral, shallow exploration). Empirically, LATTE raises calibrated tool-use rates, lengthens effective search depth, and improves task success as well as training stability in advancing deep search optimization.", "tldr": "", "keywords": ["Large Language Model", "Deep Search", "On-Policy RL"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00f109f7ad53adcb3938985f7c5701a0b407f690.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes LATTE, a mixed-policy reinforcement learning framework that integrates teacher-forced, learner-adaptive reflection into deep search agents. The key innovation is injecting structured reflection at critical decision points (whether to continue/stop search, whether to use tools) during on-policy rollouts. Unlike prior work that generates reflections offline, LATTE conditions teacher feedback on the learner's current policy state, producing \"learner-adaptive\" reflections. The method combines GRPO for on-policy optimization with reflection-augmented trajectories. Experiments on HLE and GPQA show improvements over baselines, with LATTE-7B achieving 72.1 on GPQA and 11.9 on HLE."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tClear conceptual contribution: The distinction between \"learner-adaptive\" (on-policy) and \"frozen\" (off-policy) reflection is well-articulated, with good intuition for why on-policy reflection matters.\n2.\tPractical framework: The teacher-forcing strategy with explicit intervention rules (Figure 1) is simple and implementable, making the work reproducible.\n3.\tSolid ablation studies: Tables 2 and 3 provide meaningful comparisons of forcing methods and reflection strategies, supporting design choices."}, "weaknesses": {"value": "1.\tLimited technical novelty: The core contribution is essentially combining existing techniques (GRPO + teacher forcing + reflection) with an \"on-policy\" twist. The algorithmic innovation is incremental—the main difference from prior work is when and how reflections are conditioned, not fundamentally new methods.\n2.\tNarrow experimental validation: (1) Only 2 benchmarks (HLE, GPQA), both science-focused QA; (2) No evaluation on open-ended research tasks\n3.\tComputational costs not reported: Training time, inference latency, cost of teacher model calls, API costs for web searches—all missing. This is critical for assessing practicality\n4.\tLimited analysis of when/why it helps: (1) Case study (Figure 3) shows one success example but no failure analysis; (2) No breakdown by question difficulty or type; (3) No analysis of what types of errors the reflection catches vs. misses\n5.\tUnclear generalization: What about other tools or information sources?"}, "questions": {"value": "1.\tTeacher model specification: What model serves as the teacher T? How is it trained/selected? What are the computational costs?\n2.\tOn-policy vs off-policy: Can you provide direct evidence that on-policy reflection is better because of distribution alignment, not just because of using a different/better teacher?\n3.\tReflection frequency: How often does teacher forcing actually trigger? Figure 2a shows it decreases—does this mean the method becomes less important later in training?\n4.\tComparison with Meta-Researcher: Meta-Researcher achieves 73.2 GPQA (Table 1 of their paper), higher than LATTE's 72.1. How does your method compare on the same setup?\n5.\tAblation on RL: What happens with just SFT on teacher-forced trajectories, without RL? Is RL necessary?\n6.\tTool call quality: Does increasing tool calls always help? Could the method lead to unnecessary tool overuse?\n7.\tFrozen reflections with better teacher: If you use a stronger frozen teacher (e.g., GPT-4), would that close the gap with adaptive reflection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jDlmpsWDmV", "forum": "cXKzStnOIQ", "replyto": "cXKzStnOIQ", "signatures": ["ICLR.cc/2026/Conference/Submission23175/Reviewer_UYqT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23175/Reviewer_UYqT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761408096735, "cdate": 1761408096735, "tmdate": 1762942543660, "mdate": 1762942543660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LATTE, a mixed-policy reinforcement learning framework that integrates teacher-forced, learner-adaptive reflection at critical decision points during on-policy rollouts. Rather than relying solely on end-to-end rewards or offline supervision, LATTE dynamically seeds reflective trajectories from the agent’s own behavior and injects step-wise critiques and corrective action plans precisely when decisions are made. This approach tightly couples exploration with timely, decision-centric feedback, while preserving the stability and credit assignment advantages of on-policy learning.\n\nEmpirically, LATTE demonstrates significant improvements over deep search baselines lacking such adaptive reflection: it enhances calibrated tool use, extends effective search depth, boosts task success rates, improves sample efficiency, and stabilizes training. These gains highlight a key insight—supervising the meta-cognitive control of reasoning (e.g., when to reflect, when to backtrack, when to invoke tools) is more effective than supervising only final outcomes. By enabling agents to better recognize uncertainty, trace back errors, and adaptively escalate to external resources, LATTE advances the development of more self-aware and strategically capable language agents in RLVR settings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "(1) Well-written (2) Detailed experiment (3) The problems related to training efficiency that have been solved are distinctive and seem valuable to the industrial sector"}, "weaknesses": {"value": "N/A"}, "questions": {"value": "I do not know this specialized research field very well. I will adjust my score and optimize my review document based on the evaluations of other expert reviewers and my performance during the rebuttal period."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "STZLf1JLpk", "forum": "cXKzStnOIQ", "replyto": "cXKzStnOIQ", "signatures": ["ICLR.cc/2026/Conference/Submission23175/Reviewer_aGcw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23175/Reviewer_aGcw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479827792, "cdate": 1761479827792, "tmdate": 1762942543459, "mdate": 1762942543459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. This paper presents LATTE, a mixed-policy RL framework for LLM deep search, addressing issues like over-trusting internal reasoning via teacher-forced, learner-adaptive reflection.\n2. It injects targeted critiques and corrective actions at decision points during on-policy training, using a mixed objective (GRPO-based RL + SFT) to align supervision with the learner’s behavior.\n3. On HLE and GPQA benchmarks, LATTE-7B outperforms open-source 32B models and search-augmented baselines, boosting tool use, search depth and task success."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Self-reflection is a key ability for large reasoning models\n2. The experiments validate the effectiveness of the proposed method"}, "weaknesses": {"value": "1. The proposed on-policy Teacher-Forcing Strategy seems to be a decoding strategy that utilizes the outcome reward. It may improves the trajectory better, but it is not reasonable to run RL on these trajectories, since it is off-policy (the behavior policy is changed due to the decoding strategy).\n2. There are some related works that tune LLMs to learn self-reflection, which should be considered for comparison.\n3. The evaluation should include more challanging benchmarks for robust evaluation."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qJRE7kDcz2", "forum": "cXKzStnOIQ", "replyto": "cXKzStnOIQ", "signatures": ["ICLR.cc/2026/Conference/Submission23175/Reviewer_mrbg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23175/Reviewer_mrbg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900878908, "cdate": 1761900878908, "tmdate": 1762942543215, "mdate": 1762942543215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}