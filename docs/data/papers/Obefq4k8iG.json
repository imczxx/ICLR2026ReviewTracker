{"id": "Obefq4k8iG", "number": 7791, "cdate": 1758036457984, "mdate": 1759897832162, "content": {"title": "Horizon Imagination: Efficient On-Policy Training in Diffusion World Models", "abstract": "We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. \nCurrent methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. \nWe propose Horizon Imagination, an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel, coupled with a stabilization mechanism and a novel sampling schedule that decouples denoising budget from decay horizon, i.e., the effective number of observations over which denoising is applied. \nExperiments on Atari 100K and Craftium show that our approach maintains control performance with half the denoising budget and achieves superior generation quality under varied schedules. \nCode is released open-source https://anonymous.4open.science/r/horizon-3EA1.", "tldr": "", "keywords": ["world models", "diffusion", "model-basedreinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b827954ee800df82cc9ae5ff6e15b87a0699b04.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To address the low sampling efficiency of existing diffusion world models, especially the quadratic computational cost arising from the product of environment steps and diffusion steps, the authors introduce Horizon Imagination, an algorithm that enables more efficient on-policy sampling of discrete stochastic policies within diffusion world models. The authors make two key improvements over prior diffusion model sampling methods. First, they introduce a horizon sampling schedule that separates the total denoising budget from the decay horizon, allowing flexible and effective configuration. They also propose a novel sampling strategy that stabilizes policy output actions during denoising and prove it preserves the original sampling distribution. Experiments on Atari 100K and Craftium show that Horizon Imagination greatly reduces denoising steps and outperforms previous schedules like pyramidal schedule in generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The diffusion-based world model studied in this paper is a rapidly growing research area with significant value in both offline data generation and online policy learning. It holds great promise for substantially reducing the cost of real-world interactions.\n\n2. The authors effectively resolve the instability that occurs when diffusion models and policies interact to jointly sample multi-step trajectories by introducing a theoretically grounded stable action sampling method. This approach significantly enhances the quality and stability of long-horizon trajectory generation.\n\n3. Compared with the previous pyramidal schedule, the proposed horizon schedule provides a more general formulation. By decoupling the decay horizon from the denoising budget, it enables more consistent denoising schedules and achieves higher-quality generation."}, "weaknesses": {"value": "1. The stable action sampling algorithm proposed by the authors is only applicable to discrete action spaces, which limits the applicability of the Horizon Imagination framework in more general environments with continuous action spaces.\n\n2. A key weakness is the limited scope of the experimental comparisons. The control performance results in Section 5.2 are structured as an internal ablation study, comparing the proposed parallel method only against an autoregressive baseline within their own framework. The paper does not benchmark its end-to-end performance against other established world model agents, such as those discussed in the related work. The absence of direct, end-to-end SOTA comparisons makes it difficult to fully assess the proposed method's relative performance and efficiency in the field.\n\n3. Training for 19 or 27 hours on A100 for relatively simple games like Atari seems somewhat costly. Conducting experiments in more challenging environments could make the proposed method more practically valuable.\n\n4. There is an extra quotation mark at the end of line 446."}, "questions": {"value": "1. Are there any possible exploratory directions for extending the idea of Horizon Imagination to continuous action spaces?\n\n2. Algorithm 1 only includes the world model sampling process. Could the authors provide a complete pseudocode of the entire training pipeline, including world model training and RL algorithm updates? This would help readers gain a clearer understanding of the overall framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CfRqlTXK9a", "forum": "Obefq4k8iG", "replyto": "Obefq4k8iG", "signatures": ["ICLR.cc/2026/Conference/Submission7791/Reviewer_8WiC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7791/Reviewer_8WiC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484900212, "cdate": 1761484900212, "tmdate": 1762919836502, "mdate": 1762919836502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes Horizon Imagination that tackles a common problem in model-based reinforcement learning with diffusion world models. In each step, the policy receives an observation from the world model, which is subsequently used to generate an action. The world model then predicts the next observation. This sequential dependency on the policy leads to a long inference time that is a bottleneck in fast training. Horizon Imagination, therefore, proposes denoising multiple future observations in parallel in conjunction with a stabilization mechanism to account for the changes in the actions and a sampling schedule. The effectiveness of the proposed method is shown on several benchmark environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an interesting and important problem in world models for policy learning.  I think the proposed method is quite important for the RL field."}, "weaknesses": {"value": "- I think the presentation of the paper can be improved. \n \n - Please also see my questions."}, "questions": {"value": "- I found it a bit difficult to follow. As a non-expert in Flows/Diffusion models, I was a bit confused with the notation. Is it for every z several denoising time steps sampled in Eq. 1, or is it only one denoising time step per sample?\n\n- Section 4.2 states that Eq. 1 requires knowing all actions; however, the velocity field v_\\theta is expecting actions $a_{<t}$, which are all actions in the history starting from the current time-step $t$. Doesn't this mean that there is actually no need for knowing future actions, as stated in Section 4.2?\n\n- From my understanding, the paper proposes a method that turns the strict autoregressive structure of the world model (i.e., policy gives action, world model predicts observation, ...) into a parallel inference version. However, I don't understand how this autoregressive structure is still retained in the current version? I think the autoregressive prediction structure still needs to hold in general for world models. \n\n- Given that the policy is queried for noisy variables z, aren't there superscripts to the actions in Eq. (1) missing?\n\n- Section 4.3 mentions using the REINFORCE algorithm. It is well-known that the REINFORCE has high variance in the gradient estimates, whereas the reparameterization trick has smaller variance in the gradients. Is there a reason why the reparameterization trick was not used, e.g., in connection with the Gumbel softmax [1] policy representation that allows using the reparameterization trick? \n\n- Is the proposed method also applicable to continuous state-action spaces? What would be necessary in this case? \n\n[1] E. Jang, et al. Categorical Reparameterization with Gumbel-Softmax. ICLR 2017."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fUP9Kx9WRE", "forum": "Obefq4k8iG", "replyto": "Obefq4k8iG", "signatures": ["ICLR.cc/2026/Conference/Submission7791/Reviewer_irHg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7791/Reviewer_irHg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838691853, "cdate": 1761838691853, "tmdate": 1762919835616, "mdate": 1762919835616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Horizon Imagination, an on-policy training procedure for reinforcement learning with diffusion world models that denoise multiple future observations in parallel. This is done by introducing a stable discrete-action sampling mechanism to avoid spurious action flips during denoising, and a novel Horizon schedule that decouples the denoising budget from the decay horizon ν, enabling sub-frame budgets and finer control of compute–quality trade-offs. Experiments on Atari 100K and Craftium show the method maintains control performance with only half the denoising budget and improves generation quality under parallel configurations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed horizon schedule is a neat design: by fixing ν while varying B, it breaks the tight coupling seen in pyramidal schedules, allowing consistent temporal denoising behavior across budgets and enabling sub-frame B < h operation.\n \nThe stable action sampling a(π,ω) for discrete policies is elegant and theoretically justified: action changes between denoising steps are bounded below by total variation distance and above by a derived l_1  term, greatly reducing unnecessary flips during denoising. \n\nSolid empirical analysis: (a) action-consistency experiments demonstrate near-optimal behavior vs. TV lower bound and strong improvements vs. naïve sampling; (b) control performance comparison across ν/B settings; (c) generation quality vs. ν/B via FVD and MSE Clarity."}, "weaknesses": {"value": "The proposed method, especially the action sampling, is only for discrete action spaces. This limits applicability to many continuous-control tasks \n\nThe paper would benefit if a per-stage runtime analysis and real-time control throughput (fps) comparison were presented to show the improvement. \n\nIt would help to connect the theoretical bound to returns—e.g., does reduced action-flip rate correlate with improved advantage estimates or policy gradient variance?"}, "questions": {"value": "Do you foresee a principled extension of a(π,ω)to continuous spaces (beyond discretization)?\n\nHow sensitive are returns to policy entropy regularization when using a(π,ω)? Is there a sweet spot where action stability helps most?\n\nFVD/MSE trends favor certain ν/B regimes; can you relate those trends directly to control performance (e.g., correlation analyses across seeds)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8wwzvjrDO3", "forum": "Obefq4k8iG", "replyto": "Obefq4k8iG", "signatures": ["ICLR.cc/2026/Conference/Submission7791/Reviewer_kptM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7791/Reviewer_kptM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955801654, "cdate": 1761955801654, "tmdate": 1762919834661, "mdate": 1762919834661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}