{"id": "mVlIKLiizr", "number": 14076, "cdate": 1758228049505, "mdate": 1759897391888, "content": {"title": "NEO — No-Optimization Test-Time Adaptation through Latent Re-Centering", "abstract": "Test-Time Adaptation (TTA) methods are often computationally expensive, require a large amount of data for effective adaptation, or are brittle to hyperparameters. Based on a theoretical foundation of the geometry of the latent space, we are able to significantly improve the alignment between source and distribution-shifted samples by re-centering target data embeddings at the origin. This insight motivates NEO – a hyperparameter-free fully TTA method, that adds no significant compute compared to vanilla inference. NEO is able to improve the classification accuracy of ViT-Base on ImageNet-C from 55.6\\% to 59.2\\% after adapting on just one batch of 64 samples. When adapting on 512 samples NEO beats all 7 TTA methods we compare against on ImageNet-C, ImageNet-R and ImageNet-S and beats 6/7 on CIFAR-10-C, while using the least amount of compute. NEO performs well on model calibration metrics and additionally is able to adapt from 1 class to improve accuracy on 999 other classes in ImageNet-C. On Raspberry Pi and Jetson Orin Nano devices, NEO reduces inference time by 63\\% and memory usage by 9\\% compared to baselines. Our results based on 3 ViT architectures and 4 datasets show that NEO can be used efficiently and effectively for TTA.", "tldr": "We introduce an optimization-free and hyperparameter-free TTA method, which achieves state-of-the-art results in limited adaptation data settings, at no additional computational cost compared to not adapting.", "keywords": ["test-time adaptation", "domain adaptation", "on-device"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b963994de3a4d7a08750004a13e8704e4c1ebe2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes NEO, a simple test-time adaptation method that performs feature re-centering by subtracting the mean of test embeddings. The method is motivated by the Neural Collapse theory and aims to handle distribution shifts without optimization or access to source data. Experiments are conducted mainly on ImageNet-C and related benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is simple and easy to implement.\n2. The paper provides a clear and intuitive geometric explanation based on Neural Collapse.\n3. Experimental results show that the approach achieves comparable accuracy to some optimization-based TTA methods while requiring less computation."}, "weaknesses": {"value": "1. Lack of methodological novelty. The core idea of this paper’s method is highly similar to the Back-to-Shift mechanism in FOA[1], particularly under the special case where the source feature mean equals zero. Similar effects have already been explored in that setting. The authors are encouraged to further clarify the distinctions between NEO and existing work, and to elaborate on the method’s genuine theoretical or practical contributions.\n2. The effectiveness of NEO strongly depends on the assumption that the source-domain embedding mean is zero. If this assumption does not hold (e.g., when models use different normalization mechanisms such as BatchNorm or GroupNorm), the performance of the method may degrade significantly. To verify the generality of this assumption, it is recommended to include additional experiments using models with BatchNorm, GroupNorm, and other normalization layers, and to compare NEO’s robustness across these settings.\n3. Insufficient validation of batch-size independence. Despite the claim that the method is batch-size agnostic, the experiments only apply test-time adaptation to a limited number of samples (e.g., a single small batch), and the adapted model is reused for all remaining evaluations—raising questions about true batch-size independence. To substantiate this claim, the authors should perform full test-time adaptation on all test samples using different batch sizes (e.g., 1, 8, 64, 512), where the entire adaptation process is repeated independently for each batch size, and then compare the resulting performance to assess true batch-size independence.\n4. Lack of evaluation under label shift scenarios. When class prior distributions differ between the source and target domains, simply subtracting the current batch mean may introduce bias and fail to properly correct distribution shifts. It is recommended to include experiments under label shift settings, as used in studies such as SAR[2], rather than evaluating adaptation only on samples from selected classes.\n5. The experiments that adapt using only one batch or one class are compared solely with the “no adaptation” baseline, without including other TTA methods. This limited comparison is insufficient to demonstrate whether NEO still outperforms existing methods under few-sample or class-restricted adaptation scenarios.\n6. Insufficient comparison in continual adaptation experiments. In the continual adaptation section, the paper compares NEO with only a few existing methods. It is recommended to include common anti-forgetting TTA methods such as EATA[A], ViDA[B] and KFF[C] to more comprehensively demonstrate NEO’s performance in long-term adaptation scenarios.\n7. The method is evaluated using only 512 adaptation samples, which may favor NEO. Optimization-based TTA methods typically employ conservative strategies (e.g., small learning rates) to prevent overfitting. When adaptation data is limited, a slightly higher learning rate may substantially improve their performance. Therefore, this setup may underestimate the performance of other methods.\n8. Incomplete disclosure of hyperparameter settings. For some baseline methods—especially those using the same model architecture but with undisclosed configurations—the paper does not provide complete hyperparameter details. To ensure reproducibility and fair comparison, the appendix should include the full hyperparameter settings for each method and model.\n\n[1] Test-Time Model Adaptation with Only Forward Passes. ICML 2024.\n\n[2] Towards stable test-time adaptation in dynamic wild world. ICLR 2023.\n\n[A] Efficient test-time model adaptation without forgetting. ICML 2022.\n\n[B] ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation. ICLR 2024.\n\n[C] Class-aware Domain Knowledge Fusion and Fission for Continual Test-Time Adaptation. NIPS 2025."}, "questions": {"value": "1. Could the authors leverage the neural collapse assumption to explain why existing entropy minimization–based test-time adaptation (TTA) methods are prone to feature or classifier collapse during adaptation?\n2. Could the authors further discuss, from a theoretical perspective, whether the neural collapse assumption still holds—and whether NEO remains effective—in realistic scenarios such as those with class imbalance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A3JxI7Yobm", "forum": "mVlIKLiizr", "replyto": "mVlIKLiizr", "signatures": ["ICLR.cc/2026/Conference/Submission14076/Reviewer_4bB3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14076/Reviewer_4bB3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848370899, "cdate": 1761848370899, "tmdate": 1762924556195, "mdate": 1762924556195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces NEO, a hyperparameter-free and optimization-free Test-Time Adaptation (TTA) method that operates by re-centering latent embeddings at the origin. It leverages theoretical insights from neural collapse to justify that centering corrupted test embeddings improves alignment with the clean feature space. NEO requires only forward passes, negligible compute, and no access to source data. Experiments across four datasets (ImageNet-C/R/S, CIFAR-10-C) and three ViT architectures show consistent improvements over prior TTA methods under its own evaluation setting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The studied forward-only test-time adaptation problem is practical and interesting.\n\nNEO’s one-line implementation (replacing nn.Linear with a custom layer) is elegant and simple to use. It adds no backprop, parameters, or optimization loops.\n\nEvaluations on low-power devices (Jetson Orin) highlight NEO’s edge-readiness—rarely studied in TTA works."}, "weaknesses": {"value": "The reported results of the baselines appear questionable. While I understand that the authors may have used different evaluation settings, this explanation is not convincing, as the reported performance is significantly lower than in the original papers. Since this work does not aim to introduce a new evaluation protocol, it should follow the established settings used in prior works such as FOA to ensure fair comparison. As a result, I remain doubtful about the reported results, which undermines confidence in the claimed effectiveness of the proposed method.\n\n\nThe theoretical analysis relies on idealized assumptions—such as balanced classes, cross-entropy training, and zero-centered clean features—which may not fully hold in real-world scenarios. It also remains unclear how well these assumptions apply to models whose features are not zero-centered."}, "questions": {"value": "I am also somewhat confused about the Peak Memory comparison on the Jetson device, as the relative differences between methods seem inconsistent with results typically observed on standard GPUs (e.g., A100).\n\nAdditionally, the influence of batch size has not been thoroughly analyzed, particularly for the continual version of NEO.\n\nIt would be helpful to clarify whether the method can maintain strong performance under non-i.i.d. settings (e.g., imbalanced label distributions) and whether it also generalizes effectively to CNN-based architectures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1xlAenq4jQ", "forum": "mVlIKLiizr", "replyto": "mVlIKLiizr", "signatures": ["ICLR.cc/2026/Conference/Submission14076/Reviewer_rtzZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14076/Reviewer_rtzZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901359351, "cdate": 1761901359351, "tmdate": 1762924555403, "mdate": 1762924555403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NEO, a hyperparameter-free test-time adaptation (TTA) method that addresses distribution shifts by re-centering corrupted embeddings at the origin. The core contribution is leveraging neural collapse theory to justify a simple yet effective approach: computing the global centroid of target embeddings and subtracting it to align features with source data. NEO requires no backpropagation, adds minimal computational overhead (storing only a single vector), and demonstrates competitive or superior accuracy compared to seven baseline TTA methods on ImageNet-C, CIFAR-10-C, ImageNet-R, and ImageNet-S using Vision Transformer architectures. The method achieves 59.2% accuracy on ImageNet-C with ViT-Base (vs. 55.6% without adaptation), operates efficiently on edge devices, and maintains robustness even when adapting with as few as one sample or one class."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality:** Novel connection between neural collapse theory and TTA. Propositions 4.1-4.2 provide theoretical justification showing $\\mu_G = 0_d$ under neural collapse, making $\\Delta_G = \\tilde{\\mu}_G$. The empirical finding that shifts affect <50 of 768 dimensions for 80% of samples motivates the global alignment strategy effectively.\n\n**Quality:** Comprehensive evaluation across three ViT architectures, four datasets, seven baselines. Strong robustness demonstrated: single-sample adaptation, continual learning, edge deployment (Raspberry Pi, Jetson). Figure 3b validates theory with 0.49 cosine similarity to source. Cross-corruption analysis (Figure 6) provides practical deployment insights.\n\n**Clarity:** Well-structured progression from empirical observations â†’ theoretical justification â†’ algorithm. Figure 1 illustrates single-line implementation elegantly. Algorithm 1 is concise and clear.\n\n**Significance:** Addresses critical TTA limitations (cost, memory, hyperparameters) with strong practical value. Achieving highest accuracy (59.2% vs. 58.4% FOA) while being 63% faster demonstrates real-world applicability."}, "weaknesses": {"value": "**Unverified Theoretical Assumptions:** Theory relies on neural collapse, but no empirical validation that ViT models exhibit NC1-NC4 properties (within-class collapse, equiangular tight frame, $\\mu_G \\approx 0_d$). Proposition 4.2's unconstrained features and balanced class assumptions are strong yet unverified. Need: (a) measure NC metrics on evaluation models, or (b) ablations showing robustness when assumptions fail. Class imbalance in real deployments contradicts balanced class requirement.\n\n**Architecture Specificity:** Claims architecture-agnostic but only evaluates ViTs. CNNs (different inductive biases) may not exhibit same latent properties. Given TENT/SAR target CNNs, this gap is critical. Need: test on at least one CNN (ResNet-50) or explicitly restrict claims to Transformers.\n\n**Missing Failure Analysis:** NEO loses to Surgeon on 3/15 corruptions (Gaussian/Shot/Impulse) and FOA on brightness, with no explanation. Theory assumes global shifts, but what about class-specific components? Figure 3b shows class-wise $\\Delta_c$ gains marginal improvement (0.64 vs. 0.51), suggesting untapped potential.\n\n**Weak FOA Comparison:** FOA is most similar (backprop-free, 2 forward passes, efficiency-focused). Paper doesn't explain conceptual differences or why NEO wins (59.2% vs. 58.4%). Is the gain from no-source-data or better adaptation? Need ablation: give NEO source statistics like FOA to isolate re-centering contribution.\n\n**Continual Hyperparameter Contradiction:** NEO-Continual uses EMA parameter $\\alpha$, contradicting \"hyperparameter-free\" claim. No guidance on setting $\\alpha$, no sensitivity analysis despite continual adaptation being a key contribution. Need: sensitivity across $\\alpha \\in [0.01, 0.9]$ or explicit limitation acknowledgment.\n\n**ViT-Base Anomaly Unexplained:** Figure 4a shows ViT-Base gains less than ViT-S/L consistently, contradicting theory that dimensionality shouldn't matter. This pattern deserves investigation."}, "questions": {"value": "1. Can you measure NC1-NC4 metrics on your ViT models? Specifically: $\\Sigma_W \\approx 0$, equiangular class means, self-duality, $\\mu_G \\approx 0_d$? How robust is NEO when these fail?\n\n2. Have you tested NEO on CNNs (ResNet-50, EfficientNet)? Would you expect similar effectiveness if neural collapse differs?\n\n3. How does performance degrade with class imbalance? At what ratio does $\\mu_G = 0_d$ break down?\n\n5. Is NEO's advantage over FOA from (a) no source data or (b) better adaptation? Can you ablate with source statistics?\n\n6. How to set $\\alpha$ in NEO-Continual? Show sensitivity analysis across $\\alpha \\in [0.01, 0.9]$.\n\n7. Why does ViT-Base underperform ViT-S/L (Figure 4a)?\n\n9. Can pre-computed corruption taxonomy enable zero-shot adaptation (Figure 6)?\n\n10. Quantify exact FLOPs overhead for NEO vs. vanilla inference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NxaFo7afAJ", "forum": "mVlIKLiizr", "replyto": "mVlIKLiizr", "signatures": ["ICLR.cc/2026/Conference/Submission14076/Reviewer_HPPs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14076/Reviewer_HPPs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947282801, "cdate": 1761947282801, "tmdate": 1762924554219, "mdate": 1762924554219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}