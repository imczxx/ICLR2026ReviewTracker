{"id": "G8unnJKi0v", "number": 786, "cdate": 1756818090305, "mdate": 1759898241968, "content": {"title": "FHDM-KGE: Fuzzy Hierarchical Modeling and Dual Mixture-of-Experts for Knowledge Graph Embedding", "abstract": "Real world knowledge graphs (KGs) exhibit rich hierarchical structures, and effectively modeling such structures is crucial for learning high-quality representations and boosting downstream reasoning performance. However, existing hierarchy-aware KGE methods suffer from two key limitations: (i) hard layer assignment inevitably causes information loss for boundary or multi-role entities, and (ii) the neglect of relational cross-layer differences restricts the expressiveness of relation embeddings. To overcome these issues, we propose FHDM-KGE, a Fuzzy Hierarchical Modeling with Dual Mixture-of-Experts framework for knowledge graph embedding (KGE). First, we introduce a differentiable SpringRank-based fuzzy hierarchy that assigns entities to multiple layers with soft memberships, preserving multi-level semantics. Then, we design a dual MoE architecture: an entity-side MoE (EMoE) module gated by fuzzy memberships to capture intra-layer nuances, and a relation-side MoE (EMoE) module guided by head–tail hierarchical differences to model cross-layer relational patterns. The resulting entity and relation embeddings are scored with a ConvE decoder. Experiments on multiple public benchmarks demonstrate that FHDM-KGE consistently outperforms strong baselines, validating the effectiveness of combining fuzzy hierarchical modeling with dual MoE specialization.", "tldr": "", "keywords": ["Knowledge graph embedding", "fuzzy hierarchical modeling", "dual mixture of experts"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/961e64f0d6f2a597f12b75a689daa175071966cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method that assigns entities to multiple hierarchical levels in order to capture their specific hierarchical characteristics. It introduces a Mixture-of-Experts (MoE) mechanism for both entity and relation embeddings, enabling the model to effectively leverage hierarchical information for knowledge graph completion. Finally, ConvE is employed as the base decoder for triple scoring, and several auxiliary loss functions—including those related to MoE and a Hierarchical Contrastive Loss—are incorporated to guide the training of the knowledge graph completion (KGC) task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The fuzzy hierarchical modeling effectively captures multi-level semantics and reduces information loss compared to hard hierarchy methods.\n- The dual Mixture-of-Experts design enables adaptive and specialized representation learning for both entities and relations.\n- The integrated multi-loss training framework ensures stable optimization and leads to good performance across benchmarks."}, "weaknesses": {"value": "- The use of the SpringRank loss may degrade modeling performance for relations without clear hierarchical directionality, such as symmetric relations.\n- The case studies provided in the experimental analysis are too simple to demonstrate whether the model truly captures hierarchical features. It would be helpful to further analyze what distinctive characteristics exist among entities at different levels.\n- The motivation for using the MoE framework to learn entity and relation embeddings is not fully convincing. Could the authors provide a deeper analysis of how the MoE gating weights relate to entities of different hierarchical levels?\n- This paper lacks a clear theoretical explanation of how the SpringRank loss interacts with the fuzzy hierarchy. SpringRank enforces discrete directional ordering, while fuzzy membership models continuous semantics.\n- The proposed method introduces many loss terms and hyperparameters, making the model overly complex and difficult to tune.\n- The source code is not publicly available, making it hard to reproduce the results."}, "questions": {"value": "- In the experiments, did the authors combine the results of head entity prediction (i.e., inverse relation prediction) and tail entity prediction, or were only tail predictions considered?\n- How does the proposed method’s training and inference time compare to ConvE? Does it incur additional computational cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kYQepQnGN4", "forum": "G8unnJKi0v", "replyto": "G8unnJKi0v", "signatures": ["ICLR.cc/2026/Conference/Submission786/Reviewer_Hni4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission786/Reviewer_Hni4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761034014928, "cdate": 1761034014928, "tmdate": 1762915604404, "mdate": 1762915604404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses two key limitations of existing hierarchy-aware knowledge graph embedding (KGE) methods: information loss from hard layer assignment for boundary/multi-role entities and insufficient expressiveness due to neglecting relational cross-layer differences. The authors propose FHDM-KGE, a framework integrating fuzzy hierarchical modeling and a dual mixture-of-experts (MoE) architecture. First, a differentiable SpringRank-based fuzzy hierarchy assigns entities to multiple layers with soft memberships to preserve multi-level semantics. Second, a dual MoE design—entity-side MoE (EMoE) gated by fuzzy memberships and relation-side MoE (RMoE) guided by head-tail hierarchical differences—captures intra-layer nuances and cross-layer relational patterns, respectively. The model uses a ConvE decoder and an integrated loss function combining KGE objectives, hierarchy-consistency constraints, and expert-balancing regularization. Extensive experiments on FB15K-237, WN18RR, and YAGO3-10 datasets demonstrate that FHDM-KGE outperforms traditional and hierarchy-aware baselines in link prediction tasks, with ablation studies validating the effectiveness of each component."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies critical limitations in prior hierarchy-aware KGE methods and proposes a well-motivated solution that integrates fuzzy hierarchical modeling with dual MoE specialization.\n2. The paper is well-structured and clear, with detailed descriptions of the methodology, experimental setup, and results. Complex concepts (e.g., fuzzy membership computation, MoE gating mechanisms) are explained with mathematical formulations and visualizations, making the work accessible to researchers in the KGE field."}, "weaknesses": {"value": "1. The dual MoE modules rely on expert-balancing regularization to avoid collapse, but the paper does not explore alternative MoE routing strategies (e.g., top-k gating or sparse activation) that are widely used in large-scale models. Comparing with these strategies could provide insights into the trade-offs between computational efficiency and expressiveness, especially for large knowledge graphs with millions of entities/relations.\n2. The case study is limited to two specific queries and a single baseline (HAKE). Expanding the case study to include more diverse query types (e.g., multi-hop relations, cross-domain hierarchies) and additional baselines (e.g., SHLDKE, which is the strongest baseline on FB15K-237) would provide a more comprehensive understanding of the model’s strengths in real-world scenarios."}, "questions": {"value": "1. How does the model handle knowledge graphs with highly unbalanced hierarchical structures (e.g., some layers having far more entities than others)? Does the Gaussian kernel-based membership assignment lead to biased layer distributions in such cases, and if so, how could this be mitigated?\n2. The EMoE uses fuzzy memberships as gates, while RMoE relies on head-tail hierarchical differences. Have the authors considered combining these two signals (e.g., using entity memberships to refine relation expert gating) to further improve relational embedding quality? If not, what are the potential challenges of such an integration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iWrFr5PPDx", "forum": "G8unnJKi0v", "replyto": "G8unnJKi0v", "signatures": ["ICLR.cc/2026/Conference/Submission786/Reviewer_S69D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission786/Reviewer_S69D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902308372, "cdate": 1761902308372, "tmdate": 1762915604288, "mdate": 1762915604288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a hierarchical extension of MOMOK with multi-perspective design. The proposed approach could capture and represent the hierarchical information of entities and relations through differentiable fuzzy hierarchical structures and a dual MoE architecture. The experimental results show the effectiveness of the developed model on most of the datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Soft hierarchical modeling: The paper effectively considers the soft hierarchy of entities and relations by proposing a fuzzy hierarchical mechanism. This approach enhances both entity and relation representations via hierarchical experts. Compared with rigid categorization, this design is conceptually more reasonable. Moreover, a dedicated loss function is devised to fit the proposed framework, making the model self-consistent.\n\n2. Theoretical clarity and interpretability: This paper provides comprehensive theoretical details, and the overall model architecture is clearly illustrated.\n\n3. Performance improvement: The proposed model achieves a significant performance gain compared with existing baselines."}, "weaknesses": {"value": "1. Writing and formatting issues: The paper suffers from several presentation issues. For instance, in line 289, both W_{k1} and W_{k2} are written as W_{k}. The use of italic, roman, and bold fonts in formulas is inconsistent. Equation (6) lacks explanations for Z_e and Z'_e. In addition, spacing problems appear in both the Introduction and Section 4.3.\n\n2. Insufficient experimental support: The authors are suggested to include the following:  \na. Ablation studies are conducted only on the FB15k-237 dataset. Results on other datasets are necessary for a more comprehensive evaluation.  \nb. The internal structure of each expert should be explicitly ablated to demonstrate the effectiveness of its internal design.  \nc. The paper should verify whether different experts indeed capture different hierarchical levels of attention or semantic focus.  \nd. The core claim of the paper is supported by only a single case study, which is insufficient. Additional qualitative or quantitative evidence is required to confirm that entities and relations are distributed effectively across different hierarchical levels."}, "questions": {"value": "1. In Section 4.2, both WN18RR and FB15k-237 show significant performance improvements, whereas YAGO3-10 remains nearly unchanged compared with the baseline. What causes this discrepancy?\n\n2. What is the computational efficiency of the proposed model? A discussion or comparison in terms of training/inference time or parameter count would strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PF1isQBxfN", "forum": "G8unnJKi0v", "replyto": "G8unnJKi0v", "signatures": ["ICLR.cc/2026/Conference/Submission786/Reviewer_YCXt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission786/Reviewer_YCXt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924266147, "cdate": 1761924266147, "tmdate": 1762915604161, "mdate": 1762915604161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Real world knowledge graphs (KGs) exhibit rich hierarchical structures, and effectively modeling such structures is crucial for learning high-quality representations and boosting downstream reasoning performance. To overcome the limitations of hard layer assignment and neglect of relational cross-layer differences, we propose FHDM-KGE, a Fuzzy Hierarchical Modeling with Dual Mixture-of-Experts framework that introduces a differentiable SpringRank-based fuzzy hierarchy assigning entities to multiple layers with soft memberships, and designs a dual MoE architecture with entity-side and relation-side modules. Experiments on multiple public benchmarks demonstrate that FHDM-KGE consistently outperforms strong baselines, validating the effectiveness of combining fuzzy hierarchical modeling with dual MoE specialization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a differentiable SpringRank-based approach that assigns entities to multiple layers with soft memberships, effectively addressing the information loss problem caused by hard layer assignment in existing methods.\n2. The proposed dual mixture-of-experts design systematically captures both intra-layer entity nuances and cross-layer relational patterns, providing more expressive embeddings than prior work.\n3. Extensive experiments on multiple benchmark datasets demonstrate consistent improvements over strong baselines across various evaluation metrics, validating the effectiveness of the proposed approach."}, "weaknesses": {"value": "1. The framework figure does not clearly highlight the proposed novel components, as most visualized details focus on common existing modules like RGCN and standard MoE architectures rather than emphasizing the fuzzy hierarchy mechanism and dual MoE design.\n2. The paper lacks critical analysis of MoE behavior such as expert activation patterns and specialization, which would demonstrate whether entities from different hierarchical levels or relations connecting different layers actually activate distinct experts as intended by the design.\n3. No code is provided with the submission, as the authors only promise to release the implementation upon acceptance."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X67DmuMqlt", "forum": "G8unnJKi0v", "replyto": "G8unnJKi0v", "signatures": ["ICLR.cc/2026/Conference/Submission786/Reviewer_pwjQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission786/Reviewer_pwjQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762223760689, "cdate": 1762223760689, "tmdate": 1762915604019, "mdate": 1762915604019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}