{"id": "oXgxHVTqcm", "number": 10346, "cdate": 1758167807021, "mdate": 1759897656973, "content": {"title": "Guided Sampling in Reinforcement Learning for LLM Reasoning", "abstract": "Reinforcement Learning (RL) has proven effective at fine-tuning Large Language Models (LLMs) to improve the precision of their chain-of-thought reasoning. However, these methods typically rely on outcome-based rewards, without directly supervising the cognitive process of reflection. Consequently, while the model's ability to complete reasoning tasks is optimized, its capacity to identify and recover from its own errors within a single, continuous line of thought is not explicitly trained. In this work, we introduce Guided Sampling, a framework designed specifically to cultivate this missing reflection ability. Guided sampling casts the exploration phase as a sequential process where, upon generating an incorrect response, the model is prompted to re-evaluate its flawed reasoning and continue its generation. This technique creates a direct optimization pressure on the act of reflection itself, shifting the learning objective from merely finding a correct answer to actively correcting a wrong one. Experimental results demonstrate that by explicitly training for reflection, our GSRL framework is able to not only surpass traditional RL methods in final task accuracy but also fosters a more robust, self-correcting reasoning process.", "tldr": "", "keywords": ["Reinforcement Learning", "Large Language Model", "Vision Language Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d2e3a775e6e19ef07633979f4e4f92ce66bda2a3.pdf", "supplementary_material": "/attachment/d6e26522f638c795ae0b3968db5ac492f33585c4.zip"}, "replies": [{"content": {"summary": {"value": "This paper works on explicitly imparting the reflection ability into policies during RL training. The models are prompted to check their wrong responses and regenerate a new response."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ It is new to explicitly incorporate the reflection process into the RL training for vision-language reasoning tasks.\n+ The gains are consistent on multiple benchmarks (e.g., MathVerse and MathVista).\n+ The authors provide both qualitative and quantitative analyses suggesting the model learns to reflect and correct prior errors."}, "weaknesses": {"value": "+ at least one model beyond Qwen series should be reported. it is thus unclear whether the success is specific to the Qwen model families.\n+ the training time of different methods should be reported, as the proposed sequential method increases the sampling budget. The reflection naturally needs to be conducted after one attempt is finished, and cannot benefit from parallel sampling. \n+ The proposed approach benefits from intermediate verifier feedback unavailable to baseline methods, which might partly explain its advantage.\n+ (minor) the subfigures in figure 3 are hard to read."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MIWzKmdHyP", "forum": "oXgxHVTqcm", "replyto": "oXgxHVTqcm", "signatures": ["ICLR.cc/2026/Conference/Submission10346/Reviewer_fwUK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10346/Reviewer_fwUK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761349028277, "cdate": 1761349028277, "tmdate": 1762921677210, "mdate": 1762921677210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Guided Sampling (GS), a reinforcement learning training framework designed to enhance the self-correction ability of large language models (LLMs) during reasoning. Instead of discarding incorrect responses, GS treats them as intermediate states and appends a reflection prompt (e.g., “Wait, let me re-evaluate…”) to trigger the model to reflect and revise its answer within the same reasoning trace.\n\nKey contributions are as follows:\n1. Proposes Guided Sampling, a novel RL sampling pipeline that explicitly trains models to reflect on failures;\n2. Designs segment-level rewards to separately credit initial reasoning and subsequent reflection;\n3. Demonstrates consistent improvements over GRPO and Best-of-N across both text-only (GSM8K, MATH) and multimodal (MathVista, MathVerse, MM-Math) reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: This paper addresses a key limitation of current RL methods (e.g., GRPO) that optimize only for final answer correctness, ignoring the reasoning process and self-correction behavior.\n\nS2: This paper formulates reflection as a sequential MDP, where failed attempts become intermediate states, and reflection is triggered by lightweight prompts. This is intuitively reasonable and easy to implement.\n\nS3: This paper conducts extensive experiments on both text-only and multimodal math reasoning tasks, showing consistent gains across 3B and 7B model sizes.\n\nS4: This paper systematically ablates components (guidance, clipping, segment-level advantages), showing incremental benefits from each design choice.\n\nS5: This paper provides detailed analysis of training dynamics, including reward curves, KL divergence, clip magnitudes, and response length, showing that GS encourages longer and more deliberate reasoning."}, "weaknesses": {"value": "W1: While the idea of training reflection is useful, the core mechanism—appending a prompt after failure to encourage revision—is conceptually similar to prior work like Self-Refine, Reflexion, or iterative prompting strategies. \n\nW2: The method assumes access to reliable feedback (e.g., math answer verifiers) at training time. This limits generalizability to open-ended or subjective tasks where such verifiers are unavailable.\n\nW3: Although the paper shows higher reflection success rates, it lacks deep semantic analysis of whether the model truly understands and fixes its mistakes or just surface-level corrections.\n\nW4: Main comparisons are limited to GRPO and Best-of-N. Missing comparisons with stronger self-correction or RL methods (e.g., Reflexion, Self-Refine, RL with self-generated critiques).\n\nW5:  The main baseline is Qwen2.5/Qwen2.5-VL. Besides the Qwen series, if this paper considers other thinking/no-thinking series (such as Deepseek and Llama), will there be a performance improvement similar to that of Qwen.\n\nW6: Only one reflection step is allowed per query. This restricts the potential of the method and avoids harder questions like whether the model can iteratively improve over multiple steps."}, "questions": {"value": "Q1: What happens if the intermediate verifier is wrong (e.g., marks a correct answer as incorrect)? Could this mislead the model into unnecessary or even harmful reflection?\n\nQ2: How would the method perform on open-ended or subjective tasks (e.g., coding, Search)? How would you design reflection triggers without verifiable rewards?\n\nQ3: Could the model learn to intentionally generate wrong answers first and then “correct” them to maximize reward? Are there mechanisms to prevent such reward hacking?\n\nQ4: Why is reflection limited to only one step? Is there a performance saturation or training instability issue with multi-turn reflection?\n\nQ5: How to define a segment, what is the difference between it and splitting step/chunking, and how to determine whether the content of a segment is reasonable?\n\nQ6: Can GS enhance the performance of other thinking/no-thinking models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DuBMEaLOQe", "forum": "oXgxHVTqcm", "replyto": "oXgxHVTqcm", "signatures": ["ICLR.cc/2026/Conference/Submission10346/Reviewer_vZ34"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10346/Reviewer_vZ34"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641614376, "cdate": 1761641614376, "tmdate": 1762921676825, "mdate": 1762921676825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Guided Sampling (GS), a  RL sampling framework designed to improve self-reflection and error correction capabilities in LLMs. Unlike standard RL approaches that rely solely on outcome-based rewards, GS redefines exploration as a sequential, reflective process, where incorrect model outputs trigger a natural language prompt for reflection and regeneration. The method introduces segment-level credit assignment, separating initial reasoning and reflection, and applies relaxed clipping to stabilize optimization on reflection segments. Experimental results demonstrate consistent and significant improvements over GRPO across both text-only and multi-modal reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a critical limitation in RL for LLMs: the lack of explicit training for self-correction within a single reasoning trajectory.\n   - The method significantly improves performance across multiple benchmarks. Gains are especially notable in harder problems."}, "weaknesses": {"value": "- Since reflection involves an additional round of sampling, it inevitably increases computational time. From a training efficiency perspective, is the proposed method truly more efficient than GRPO?\n- The experiments are conducted on instruct-tuned models. Would the proposed method still be effective for reasoning-oriented models that already possess strong reflection capabilities?"}, "questions": {"value": "- The evaluation is conducted exclusively in the math domain. How well does the method generalize to other domains after training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HQ98cHFmI9", "forum": "oXgxHVTqcm", "replyto": "oXgxHVTqcm", "signatures": ["ICLR.cc/2026/Conference/Submission10346/Reviewer_ET5k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10346/Reviewer_ET5k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798402067, "cdate": 1761798402067, "tmdate": 1762921676403, "mdate": 1762921676403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Guided Sampling (GS), a novel reinforcement learning (RL) framework that enhances reflection-driven reasoning in large language models (LLMs). Traditional RL methods such as PPO and GRPO optimize outcome-based rewards that emphasize correctness of the final answer but fail to explicitly train the model’s ability to self-diagnose and correct reasoning errors mid-trajectory.\n\nThe authors reframe exploration as a sequential reflective process: when a model produces an incorrect answer, it receives a “reflection trigger” cue (e.g., “Wait, that doesn’t seem right”) prompting it to reassess and continue reasoning. This transforms the sparse terminal reward structure into dense mid-trajectory credit assignment, explicitly optimizing both “thinking” and “reflection” behaviors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Soundness\n\nThe authors provide: A clear formulation of multi-turn refinement loops and reward routing. Careful reward normalization and advantage assignment to prevent interference between reasoning and reflection tokens. These improvements are well-motivated and grounded in prior RLVR and GRPO analyses.\n\n2. Strong Empirical Validation\n\nResults are substantial and well-presented: On MathVista/MathVerse/MM-Math, GS yields +2–3 pp average gains over GRPO. Enough ablation is done regarding the training details in Section 3.3"}, "weaknesses": {"value": "1. Ablation Scope\n\nIt lakes ablation on other alternative training details, such as other ways to do reward normalization (e.g. per segment) together with segment/trajectory-level advantage assignment."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qt0D5WLV52", "forum": "oXgxHVTqcm", "replyto": "oXgxHVTqcm", "signatures": ["ICLR.cc/2026/Conference/Submission10346/Reviewer_G9gM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10346/Reviewer_G9gM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874224864, "cdate": 1761874224864, "tmdate": 1762921675625, "mdate": 1762921675625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}