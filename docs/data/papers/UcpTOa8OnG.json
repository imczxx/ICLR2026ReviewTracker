{"id": "UcpTOa8OnG", "number": 2139, "cdate": 1756996190758, "mdate": 1763656848027, "content": {"title": "FakeXplain: AI-Generated Images Detection via Human-Aligned Grounded Reasoning", "abstract": "The rapid rise of image generation calls for detection methods that are both interpretable and reliable. Existing approaches, though accurate, act as black boxes and fail to generalize to out-of-distribution data, while multi-modal large language models (MLLMs) provide reasoning ability but often hallucinate. To address these issues, we construct FakeXplained dataset of AI-generated images annotated with bounding boxes and descriptive captions that highlight synthesis artifacts, forming the basis for human-aligned, visually grounded reasoning. Leveraging FakeXplained, we develop FakeXplainer which fine-tunes MLLMs with a progressive training pipeline, enabling accurate detection, artifact localization, and coherent textual explanations. Extensive experiments show that FakeXplainer not only sets a new state-of-the-art in detection and localization accuracy (98.2% accuracy, 36.0% IoU), but also demonstrates strong robustness and out-of-distribution generalization, uniquely delivering spatially grounded, human-aligned rationales.", "tldr": "This work fine-tunes a Vision Language Model based on human-annotated data to classify AI-generated images and pinpoint where and why it considers so.", "keywords": ["Vision Language Models", "Image Forensics", "AIGC Detection"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9993971f616b7df90219107e1b3df95ae2df0967.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents FakeXplain, an artifact detection, localization, and explanation method. To train the FakeXplainer, they first curate a synthetic dataset with region and textual annotations for artifacts (FakeXplained). Finetuning MLLMs on the FakeXplained dataset improves the detection and localization accuracy, as well as robustness and generalization on out-of-distribution cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper studies an interesting task in artifact detection, localization, and explanation. They notice that the existing methods or datasets face challenges such as hallucination and low human alignment. They aim to build a reliable and interpretable MLLM-based system.\n- They have solid contributions, including curating a dataset with 28 T2I models and using it to achieve a big improvement across different MLLMs.\n- Extensive experiments and analysis are provided."}, "weaknesses": {"value": "- Some important technical details are unclear, such as the image generation and filtering process. \n- For the Table 1 experiment, it is unclear what benchmark they use. If they are using their own test split, then it is less impressive. Additionally, all the baseline methods have very good accuracy, which makes it seem doubtful how challenging the benchmark is."}, "questions": {"value": "- For the image generation, how many images are generated by each model? How many images per model for each prompt? What's the filtering criteria and rate?\n- For tag annotation, could you show the distribution of the tags (diversity, and distribution train/val/test)? When dividing data for SFT and RL, do you use the tag to control difficulty? \n- What is the benchmark you use for Table 1? Could you show the correlation between accuracy and tags? In what category does your method perform better, and how do SFT and RL improve the performance distinctively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I2unaMq82M", "forum": "UcpTOa8OnG", "replyto": "UcpTOa8OnG", "signatures": ["ICLR.cc/2026/Conference/Submission2139/Reviewer_N3fb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2139/Reviewer_N3fb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723329759, "cdate": 1761723329759, "tmdate": 1762916046964, "mdate": 1762916046964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FakeXplained, a new dataset for explainable AI-generated image (AIGI) detection, featuring human-annotated bounding boxes and captions for synthesis artifacts. The authors use this to train FakeXplainer, a model based on an SFT+RLHF (GRPO) pipeline, which achieves high detection accuracy (98.2%) and provides human-aligned explanations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Valuable Dataset: The FakeXplained dataset is a strong contribution, enabling the development and benchmarking of explainable AIGI detectors.\n2. Strong Empirical Results: The model achieves SOTA accuracy (98.2%). The human preference study, showing near-parity with human annotators, is a compelling validation of the \"human-alignment\" claim."}, "weaknesses": {"value": "1. Limited Methodological Novelty: The paper's methodological contribution is somewhat limited. The core SFT+GRPO training pipeline is a successful application and adaptation of existing frameworks (e.g., from DeepSeek-Math) rather than a new algorithm.\n2. Inherent Limitation of the Task Formulation: The paper's core premise, \"Human-Aligned Grounded Reasoning,\" focuses by definition on human-perceptible flaws. This is an inherent challenge for all current work in explainable AIGI detection, as it does not address the separate problem of detecting SOTA images that may be \"fake\" but lack obvious, semantically explainable artifacts.\n3. Potential for Overfitting: There is a notable mismatch between the modest dataset size (8,772 images) and the large models being fine-tuned (32B parameters). This poses a risk of overfitting.\n4. High Computational Cost: The SOTA performance relies on a 32B parameter model, which, as the authors note, incurs substantial computational costs. The reported inference time of 7.8 seconds per image (on 2x A100 GPUs) and 41 hours of training (on 8x A100 GPUs) limits the method's practical applicability in real-time or resource-constrained environments."}, "questions": {"value": "1. Since the SFT+GRPO pipeline is adapted from existing work, what is the specific, non-trivial novelty introduced in the model structure or training details that makes this method unique for AIGI detection?\n2. The authors should explicitly discuss the limitation mentioned in Weakness #2 in their conclusion. Clarifying that the scope of this work is \"detecting visible, explainable artifacts\" rather than all AIGI, would strengthen the paper's positioning.\n3. To improve reproducibility, the authors should report the specific values for all critical hyperparameters, such as the batch size, reward relaxing constant $\\eta$ and the GRPO sampling number $G$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cXtdhrRu4z", "forum": "UcpTOa8OnG", "replyto": "UcpTOa8OnG", "signatures": ["ICLR.cc/2026/Conference/Submission2139/Reviewer_iUyJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2139/Reviewer_iUyJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880421641, "cdate": 1761880421641, "tmdate": 1762916046412, "mdate": 1762916046412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenges of interpretability and generalization in AI-generated image detection. The authors argue that existing methods are often \"black boxes\" that fail to generalize to out-of-distribution (OOD) data, while multi-modal large language models (MLLMs) tend to \"hallucinate\" explanations lacking reliable visual grounding. To overcome this, the paper introduces two key contributions: 1) The FakeXplained dataset, a novel collection of AI-generated images meticulously annotated by humans with bounding boxes and descriptive captions that pinpoint specific synthesis artifacts, providing a foundation for human-aligned, grounded reasoning. 2) The FakeXplainer method, a detector developed by fine-tuning an MLLM on this dataset using a progressive training pipeline (SFT + GRPO). Experiments demonstrate that FakeXplainer achieves state-of-the-art performance in both detection accuracy (98.2%) and artifact localization IoU (36.0%), while also showing strong OOD generalization and robustness. Its primary advantage is the ability to generate spatially grounded, human-aligned textual explanations for its verdicts, effectively mitigating the hallucination issues common in MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method provides trustworthy, spatially grounded textual explanations for AI detection, mitigating MLLM hallucination.\n\nThe paper introduces the FakeXplained dataset, featuring meticulous human annotations (bounding boxes and captions) essential for training reliable, explainable models."}, "weaknesses": {"value": "The model shows strong performance on the current dataset. To further validate its generalization and mitigate potential overfitting risks, it would be beneficial to test its zero-shot capabilities on an external dataset, such as Chamelon[1].\n\nThe analysis could be further strengthened by discussing and comparing against other relevant explainable MLLM-based detectors, for instance, FakeVLM[2], So-Fake[3], and FakeScope[4].\n\nTo provide a more comprehensive picture of the method's capabilities, it would be valuable to extend the evaluation to include additional testable benchmarks, such as Fakebench[5] and LOKI[6].\n\n[1] A Sanity Check for AI-generated Image Detection. \n\n[2] Spot the fake: Large multimodal model-based synthetic image detection with artifact explanation. \n\n[3] So-Fake: Benchmarking and Explaining Social Media Image Forgery Detection\n\n[4] Fakescope: Large multimodal expert model for transparent ai-generated image\n\n[5] FakeBench: Probing Explainable Fake Image Detection via Large Multimodal Models\n\n[6] Loki: A comprehensive synthetic data detection benchmark using large multimodal models"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uUdz0eWKK5", "forum": "UcpTOa8OnG", "replyto": "UcpTOa8OnG", "signatures": ["ICLR.cc/2026/Conference/Submission2139/Reviewer_bATs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2139/Reviewer_bATs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987070806, "cdate": 1761987070806, "tmdate": 1762916045872, "mdate": 1762916045872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FakeXplained, a dataset of 8,772 AI-generated images annotated with region-level bounding boxes and short captions (≈5.42 per image), as well as image-level tags, and FakeXplainer, a progressive SFT→RLHF (pGRPO) fine-tuning pipeline for MLLMs that detects, localizes, and explains synthesis artifacts. The method optimizes three rewards—classification, grounding, and format. Experiments demonstrate that the method yields strong results: overall 98.2% accuracy and 36.0% IoU on their splits, with ablations showing large gains over SFT-only and fixed-weight RL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear problem framing + resources. A well-specified dataset, an expert-guided annotation protocol (23 annotators), and concrete artifacts improve supervision for grounding and reasoning.\n- Strong empirical results. High in-distribution accuracy/IoU, consistent ablation results (e.g., captions/boxes matter), and human-preference-parity trends support the claim of explainable detection.\n- Method integrates structures and learning. The <think>/<tag>/<verdict> output format aligns neatly with reward design, reducing formatting errors and encouraging grounded explanations."}, "weaknesses": {"value": "- Computational demands. Best results rely on large MLLMs (e.g., Qwen-2.5-VL-32B) with noted deployment costs.\n- Lenient quality-control thresholds may admit noisy labels. The QC accepts region matches at just 20% IoU and image-level tag accuracy of 1/3, which risks training/evaluating on imprecise boxes and weak tag agreements—particularly harmful for grounding-heavy RL."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5yttHtCB7N", "forum": "UcpTOa8OnG", "replyto": "UcpTOa8OnG", "signatures": ["ICLR.cc/2026/Conference/Submission2139/Reviewer_gGWh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2139/Reviewer_gGWh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762274594687, "cdate": 1762274594687, "tmdate": 1762916045092, "mdate": 1762916045092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}