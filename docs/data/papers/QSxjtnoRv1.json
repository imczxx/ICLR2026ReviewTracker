{"id": "QSxjtnoRv1", "number": 17723, "cdate": 1758279807272, "mdate": 1763655813485, "content": {"title": "Unifying Multi-Scale Design in Time-Series Forecasting", "abstract": "Multi-scale modeling in time-series forecasting, which seeks to capture cross-scale relationships for modeling complex dependencies, is increasingly popular. While previous work lacks principled foundations, we unify existing scaling methods into a scaling operator family, providing a general theoretical basis for multi-scaling methods and revealing two key limitations of current models: static scaling and inflexible cross-scale modeling.\nTo address these limitations, we propose SiGMA (Single Gaussian Multi-scale Architecture), a simple yet principled multi-scale framework. It enables position-wise scaling via the learnable discrete Gaussian (LDG) kernel grounded in scale-space theory, coupled with a lightweight MLP processor for efficient cross-scale interaction. We evaluate SiGMA comprehensively on long- and short-term forecasting benchmarks against state-of-the-art multi-scale baselines. SiGMA outperforms all competitors on both tasks, achieving the best performance in 55 out of 80 long-term evaluation settings. Beyond accuracy, SiGMA improves training speed by up to 3.8 times and reduces memory consumption by up to 5.3 times compared to the strongest competitors.", "tldr": "We unify existing multi-scale forecasting methods and propose SiGMA, a simplified yet adaptive architecture for multi-scale time-series modeling.", "keywords": ["Time-series forecasting", "Multi-scale Analysis", "Deep learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c39fadd8faf4a36c7afcd91708e3a76f5be0f3ac.pdf", "supplementary_material": "/attachment/4dfb025248c8b8ded708ad1d5d8a728e3fdc4f50.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies to unify scaling-related methods of TSF in one theory framework, based on which the authors also provide a simple and effective mutli-scale forecasting model. The authors conduct theory analysis to unify existing scaling operators and existing multi-scaling TSF methods as well."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Authors provide a unified definition and framework to define and study existing scaling operators. Based on the authors' definitions, they also study and unify the existing multi-scale-scalign-based time series forecasting methods through a framework. This is quite interesting and inspiring.\n2. The proposed SIGMA method depends upon the proposed analyzing method. It makes sense to propose such method based on the authors' theory, and such method is shown to improve performance."}, "weaknesses": {"value": "1. Though I'm not quite familiar with related math, but the proposed definition (i.e. definition 3.1) seems similar to a smooth multi-parameter scale-space/dilation family, followed by scale-dependent downsampling; \"Non-expansiveness\" equals being 1-Lipschitz, and \"Entropy Reduction\" equals a coarse-graining property. Moreover, it seems a part of theorem 3.2 (i.e. wavelet decomposition is scaling operator) relies on some assumptions stated in Appendix (i.e. Low-passdominance). Though I think these assumptions hold in most conditions, it's better to state them when proposing the theorem 3.2. Nevertheless, the definition seems clear and reasonable, not playing with difficult math concepts.\n\n2. The training time comparison. AMD is a complicated framework, comparing training time agains which would seem a bit non-fair. Nevertheless, since SIGMA and AMD are all multi-scale framework, I think this is a small weakness."}, "questions": {"value": "Please see weakness. Also:\n\nRecent observations (e.g. https://www.arxiv.org/abs/2510.02729) show that these time series forecasting benchmarks have almost been saturated. Other concerns come from the fact that it doesn't seem to make sense that one simple neural network can reach sota for all time series tasks without any context (e.g. https://neurips.cc/virtual/2024/108471), so perhaps we should combine more things together (e.g. features, NNs, strategies, etc.). What's your opinion on these thoughts? Compared to proposing methods that risk overfitting these datasets, what future direction do you think would be good for further research of our time series community?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0Y46s2s60v", "forum": "QSxjtnoRv1", "replyto": "QSxjtnoRv1", "signatures": ["ICLR.cc/2026/Conference/Submission17723/Reviewer_GCRB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17723/Reviewer_GCRB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760509184969, "cdate": 1760509184969, "tmdate": 1762927554665, "mdate": 1762927554665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies to unify scaling-related methods of TSF in one theory framework, based on which the authors also provide a simple and effective mutli-scale forecasting model. The authors conduct theory analysis to unify existing scaling operators and existing multi-scaling TSF methods as well."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Authors provide a unified definition and framework to define and study existing scaling operators. Based on the authors' definitions, they also study and unify the existing multi-scale-scalign-based time series forecasting methods through a framework. This is quite interesting and inspiring.\n2. The proposed SIGMA method depends upon the proposed analyzing method. It makes sense to propose such method based on the authors' theory, and such method is shown to improve performance."}, "weaknesses": {"value": "1. Though I'm not quite familiar with related math, but the proposed definition (i.e. definition 3.1) seems similar to a smooth multi-parameter scale-space/dilation family, followed by scale-dependent downsampling; \"Non-expansiveness\" equals being 1-Lipschitz, and \"Entropy Reduction\" equals a coarse-graining property. Moreover, it seems a part of theorem 3.2 (i.e. wavelet decomposition is scaling operator) relies on some assumptions stated in Appendix (i.e. Low-passdominance). Though I think these assumptions hold in most conditions, it's better to state them when proposing the theorem 3.2. Nevertheless, the definition seems clear and reasonable, not playing with difficult math concepts.\n\n2. The training time comparison. AMD is a complicated framework, comparing training time agains which would seem a bit non-fair. Nevertheless, since SIGMA and AMD are all multi-scale framework, I think this is a small weakness."}, "questions": {"value": "Please see weakness. Also:\n\nRecent observations (e.g. https://www.arxiv.org/abs/2510.02729) show that these time series forecasting benchmarks have almost been saturated. Other concerns come from the fact that it doesn't seem to make sense that one simple neural network can reach sota for all time series tasks without any context (e.g. https://neurips.cc/virtual/2024/108471), so perhaps we should combine more things together (e.g. features, NNs, strategies, etc.). What's your opinion on these thoughts? Compared to proposing methods that risk overfitting these datasets, what future direction do you think would be good for further research of our time series community?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Y46s2s60v", "forum": "QSxjtnoRv1", "replyto": "QSxjtnoRv1", "signatures": ["ICLR.cc/2026/Conference/Submission17723/Reviewer_GCRB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17723/Reviewer_GCRB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760509184969, "cdate": 1760509184969, "tmdate": 1763725456167, "mdate": 1763725456167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a principled theoretical framework for multi-scale time-series forecasting by defining a novel \"scaling operator family.\" Using this framework, the authors identify two key limitations in prior work: the use of non-adaptive, fixed scaling strategies and inflexible, staged cross-scale fusion mechanisms. To address these issues, they propose SIGMA (Single Gaussian Multi-scale Architecture), a simple yet powerful model. SIGMA's multi-scale generator is based on the learnable discrete Gaussian (LDG) kernel, which is grounded in scale-space theory and enables adaptive, data-dependent scaling. Its multi-scale processor is a lightweight MLP that efficiently fuses multi-resolution information in a single, flexible step. Comprehensive experiments on both long- and short-term forecasting benchmarks show that SIGMA achieves state-of-the-art accuracy, decisively outperforming recent, more complex models while being significantly more efficient in terms of training speed and memory consumption."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "*   **Novety:** The paper's most significant contribution is the introduction of a formal framework for multi-scale modeling. By defining a \"scaling operator family\" through the properties of non-expansiveness and entropy reduction (Definition 3.1), the authors provide a much-needed theoretical foundation for a field previously driven by heuristics. The decomposition of multi-scale models into a \"generator\" and \"processor\" is an elegant abstraction that successfully unifies and clarifies the limitations of a wide range of existing methods (Theorem 4.2). This framework is a valuable and lasting contribution to the community.\n\n*   **Theoretical Contribution:** The design of SIGMA is a direct and compelling instantiation of the paper's theoretical principles. The choice of the learnable discrete Gaussian (LDG) kernel as the generator is not ad-hoc; it is rigorously justified as the unique operator that satisfies the discrete scale-space axioms (Theorem 5.3). This provides a deep theoretical underpinning for the model's adaptive scaling capability. The architectural simplicity that follows—pairing the principled LDG kernel with a lightweight MLP processor—is a testament to the power of the framework, directly addressing the identified limitations of prior work.\n\n*   **Empirical Performance:** The empirical validation is exceptionally strong and thorough. SIGMA demonstrates clear state-of-the-art performance across a wide array of long-term (Table 2) and short-term (Table 3) forecasting benchmarks. Achieving the best performance in 55 out of 80 long-term settings is a dominant result that is hard to dispute. The baselines are numerous, recent, and highly relevant, making the comparison robust and convincing.\n\n*   **Computational Efficiency:** Beyond its superior accuracy, SIGMA's efficiency is a standout feature. The results presented in the efficiency analysis (Figure 3) are remarkable, showing improvements of up to 3.8x in memory and 5.3x in training speed over the strongest competitors. This is a rare combination of achieving both higher accuracy and higher efficiency, making SIGMA not only a theoretical but also a practical breakthrough. This advantage stems directly from the simplified, single-step architecture that avoids the sequential bottlenecks of staged fusion models."}, "weaknesses": {"value": "In my opinion I found that authors have primarily concern the precise nature of the model's adaptivity and the scope of its components.\n\n1.  **Limited Novelty of the \"Processor\" Component:** While the multi-scale generator (the LDG kernel) is highly novel and theoretically motivated, the multi-scale processor is a standard two-layer MLP. The innovation lies in the *simplicity* of the fusion process and the design of the inputs to the MLP (concatenating the smoothed and residual components), rather than in the processor's architecture itself. The paper's success suggests that with a sufficiently powerful and principled generator, an extremely simple processor is all that is needed, but the processor component itself is not a novel contribution.\n\n2.  **Ambiguity in the Scope of \"Adaptive Scaling\":** The paper rightly claims its scaling is \"adaptive\" because the scale parameters `s` are learned from data, a clear advantage over fixed, hand-picked scales. However, the term \"adaptive\" could be interpreted in multiple ways. As presented, the scale parameters `s` appear to be part of the model's weights, learned during training and fixed at inference time. While they are specific to each time step's position within the input window, they do not dynamically adapt to the specific values of an incoming time series at test time. This is a more limited form of adaptivity than, for example, an attention mechanism that computes data-dependent weights for every new input.\n\n3.  **Channel Independence as an Inherent Limitation:** The paper adopts a channel-independent approach, which is a common and effective strategy for ensuring applicability to multivariate settings and improving efficiency. However, this design choice inherently prevents the model from explicitly learning and leveraging cross-channel correlations at the multi-scale level. For datasets where such correlations are critical (e.g., the interactions between different sensors in a complex system), this could be a limitation compared to models that employ cross-channel mixing mechanisms."}, "questions": {"value": "*   **Question 1:** The ablation study (Table 4) convincingly shows that using an invalid scaling operator like a standard convolution performs worst. From your theoretical perspective, which property of the scaling operator family (non-expansiveness or entropy reduction) do you believe is more critical for performance, and why does a standard learnable convolution fail to satisfy it?\n\n*   **Question 2:** To clarify the nature of adaptivity: are the scale parameters `s ∈ R^L` learned as fixed model parameters during training, or are they dynamically computed based on the input sequence `x` at inference time? If they are fixed, have you considered a dynamic approach, and what would be the trade-offs?\n\n*   **Question 3:** The performance gains are attributed to both the adaptive generator and the flexible processor. Could you provide an ablation that isolates their contributions? For instance, how would a model perform that uses the LDG kernel but with a more complex, staged processor from a prior work, or conversely, a model with fixed scaling (e.g., mean pooling) but with your simplified MLP processor?\n\n*   **Question 4:** The paper demonstrates a single instantiation of the proposed generator-processor framework. Given the generality of your \"scaling operator family,\" have you experimented with or can you speculate on other potential instantiations? For example, could a learnable wavelet transform that satisfies your definition serve as an alternative generator?\n\n*   **Question 5:** The model's efficiency is a major advantage. However, the experiments use a fixed input length of L=96, which is shorter than what some state-of-the-art models use. How does SIGMA's performance and computational cost scale as the input lookback window `L` increases significantly (e.g., to 336 or 720)?\n\n*   **Question 6:** Your framework's success with channel independence is impressive. In which specific real-world scenarios or dataset types do you foresee this assumption being a significant limitation, and how might your framework be extended to incorporate principled cross-channel interactions without sacrificing its core efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gET1MS4g1H", "forum": "QSxjtnoRv1", "replyto": "QSxjtnoRv1", "signatures": ["ICLR.cc/2026/Conference/Submission17723/Reviewer_pAQg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17723/Reviewer_pAQg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803296122, "cdate": 1761803296122, "tmdate": 1762927554093, "mdate": 1762927554093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a unified theoretical framework for multi-scale modeling in time series forecasting, showing that most exisiting multi-scale approaches can fit into the framework.\nThe paper introduces a new multi-scale modeling method called SIGMA. Experiments show that SIGMA beats SOTA on both long-term and short-term tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The theoretical framework looks strong, and the design of SIGMA has clear motivation. The experiment result also looks nice."}, "weaknesses": {"value": "1. The theoretical framework, though looks solid, seems redundant to me. The two limitations, as well as the solutions proposed in SIGMA, look straightforward which you could easily arrive without the theoretical machinery. The theory feels **post-hoc** justification rather than guiding principle\n2. The proof of theorem 3.2 in Appendix A proposes a new assumption, which is improper. Assumptions should be stated clearly in the main paper. Besides Assumption A.1 (Low-pass dominance) looks pretty strong, which may not hold for all real-world time series, especially those with strong periodic components or high-frequency dominant signals.\n3. I don't think it is proper only comparing with multi-scale baselines as many models without explicit multi-scale designs naturally handle both long-term and short-term (e.g., PatchTST, DLinear variants).\n\nThere are some other minor issues related to their presentation, for example the standard deviation of three runs is not reported, etc."}, "questions": {"value": "Can you provide more justification for Assumption A.1? Can you also compare the result with non-multi-scale baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eklbSnP8RO", "forum": "QSxjtnoRv1", "replyto": "QSxjtnoRv1", "signatures": ["ICLR.cc/2026/Conference/Submission17723/Reviewer_STwe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17723/Reviewer_STwe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805509937, "cdate": 1761805509937, "tmdate": 1762927553660, "mdate": 1762927553660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a unified theoretical framework for multi-scale modeling in time series forecasting, showing that most exisiting multi-scale approaches can fit into the framework.\nThe paper introduces a new multi-scale modeling method called SIGMA. Experiments show that SIGMA beats SOTA on both long-term and short-term tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The theoretical framework looks strong, and the design of SIGMA has clear motivation. The experiment result also looks nice."}, "weaknesses": {"value": "1. The theoretical framework, though looks solid, seems redundant to me. The two limitations, as well as the solutions proposed in SIGMA, look straightforward which you could easily arrive without the theoretical machinery. The theory feels **post-hoc** justification rather than guiding principle\n2. The proof of theorem 3.2 in Appendix A proposes a new assumption, which is improper. Assumptions should be stated clearly in the main paper. Besides Assumption A.1 (Low-pass dominance) looks pretty strong, which may not hold for all real-world time series, especially those with strong periodic components or high-frequency dominant signals.\n3. I don't think it is proper only comparing with multi-scale baselines as many models without explicit multi-scale designs naturally handle both long-term and short-term (e.g., PatchTST, DLinear variants).\n\nThere are some other minor issues related to their presentation, for example the standard deviation of three runs is not reported, etc."}, "questions": {"value": "Can you provide more justification for Assumption A.1? Can you also compare the result with non-multi-scale baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eklbSnP8RO", "forum": "QSxjtnoRv1", "replyto": "QSxjtnoRv1", "signatures": ["ICLR.cc/2026/Conference/Submission17723/Reviewer_STwe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17723/Reviewer_STwe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805509937, "cdate": 1761805509937, "tmdate": 1763664434273, "mdate": 1763664434273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper builds a time-series forecaster from discrete scale-space theory. It adopts the discrete Gaussian (LDG) operator—a principled low-pass smoother with scale (s)—to create multi-scale representations (K_s * x) and fuses them with a small MLP for prediction. The goal is to combine interpretability (well-behaved smoothing, semigroup structure) with simplicity and efficiency. Experiments on standard benchmarks report competitive accuracy with a lightweight design."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper recasts multi-scale time-series modeling through discrete scale-space theory, arguing that any symmetric, linear, shift-invariant smoothing family satisfying standard axioms (semigroup, DC preservation, NELE) is uniquely the discrete Gaussian. It then builds a learnable LDG operator with position-adaptive scale $s_i$ , forms multi-scale representations, and fuses them with a small MLP to forecast. The authors claim: (i) principled, interpretable multi-scale smoothing; (ii) efficiency; (iii) competitive accuracy on common benchmarks."}, "weaknesses": {"value": "1. The axiomatic uniqueness result is classical and sound, but the uniqueness guarantee does not necessarily transfer to the trained model.\n\n2. The LDG kernel raises scalability concerns—position-adaptive scales imply $(O(L^2))$ time/space per sequence with nontrivial numerical stability, which can become prohibitive on long sequences.\n\n3. Value lies in packaging a well-known prior (discrete Gaussian scale-space) into a light, learnable operator. Methodological originality is limited (multi-scale smoothing + small MLP). Practical impact is constrained by low-pass bias and narrow experiments.\n\n4. Writing is generally clear, but strong phrases (“unique/principled”) are not tempered by discussion of assumption mismatches and failure cases (e.g., high-frequency-dominant series). The evaluation lacks high-pass comparisons, so the figures/ablations don’t clearly position the method."}, "questions": {"value": "Does the “uniqueness” you prove actually show up in the trained model—not just in theory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I7clb87fyb", "forum": "QSxjtnoRv1", "replyto": "QSxjtnoRv1", "signatures": ["ICLR.cc/2026/Conference/Submission17723/Reviewer_MsED"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17723/Reviewer_MsED"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927064383, "cdate": 1761927064383, "tmdate": 1762927553245, "mdate": 1762927553245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall response to all reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their detailed, constructive, and insightful feedback. Your comments have substantially improved both the theoretical clarity and empirical validation of our work. Below, we summarize the main changes in the revised manuscript.\n\n---\n\n## Textual revisions\n\n- **Assumption validity and theoretical scope (MsED, STwE, GCRB)**:  \n  - Replaced the low-pass dominance assumption with a dataset-level forecastability condition with empirical validation. *[Section 2; Appendix B (Table 5)]*\n  - Reformulated the operator-level requirement of entropy reduction into energy reduction with empirical analysis. *[Section 3 (Definition 3.1, Figure 2); Appendix C (Figure 6)]* \n  - Clarified that all theorems remain valid under the mild conditions.  *[Appendix A.1–A.4]*\n\n\n- **Clarifying \"adaptive\" scaling terminology (pAQg)**:  \n  Stated clearly that position-wise scales are learned per dataset and fixed at inference. *[Lines 16, 18, 48–49, 73–75, 92, 253, 267, 282, 285, 301, 364–365, 423, 446, 463, 504]*\n\n- **Design guide for extended scaling operator families (pAQg)**: Added a general guideline for constructing extended scaling operator families. *[Appendix F]*\n\n\n---\n\n## Experimental additions\n\n- **Ablation on generator and processor (pAQg)**: Added ablations that replace the lightweight MLP multi-scale processor with a more complex one. *[Table 4]*\n\n- **Error bars (STwe)**: Reported the standard deviation over three runs for SiGMA and the second-best baselines.  *[Appendix D]*\n\n- **Hyperparameter sensitivity on input length (MsED, pAQg)**:  Analyzed how performance and computational cost change across different input lengths. *[Appendix E]*\n\n- **Comparison with non-multi-scale baselines (STwe)**:  Added experiments comparing SiGMA with non-multi-scale baselines for both long-term and short-term forecasting. *[Appendix G]*\n\n  \n\n---\n\nThank you again for your careful evaluation. Any further feedback would be very welcome and valuable for improving this work."}}, "id": "427TPO830w", "forum": "QSxjtnoRv1", "replyto": "QSxjtnoRv1", "signatures": ["ICLR.cc/2026/Conference/Submission17723/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17723/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission17723/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763657310883, "cdate": 1763657310883, "tmdate": 1763657310883, "mdate": 1763657310883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}