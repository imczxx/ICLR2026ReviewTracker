{"id": "KE43G03vI7", "number": 11598, "cdate": 1758202305454, "mdate": 1759897565331, "content": {"title": "Discovering and Leveraging Entropy-Complexity Relationships for Efficient Large Language Model Reasoning", "abstract": "Large Language Models (LLMs) suffer from \"overthinking\" — generating excessive reasoning chains even for simple problems, leading to computational inefficiency and potential accuracy degradation. To address this inefficiency, we systematically show that response entropy serves as an effective intrinsic measure of problem complexity in LLM reasoning. Our key insight is that token-level entropy during generation provides a principled signal for complexity assessment, where low entropy indicates high confidence suitable for direct answers, while high entropy signals the need for detailed reasoning. Building on this entropy-complexity relationship, we propose a novel two-stage training framework for adaptive reasoning. In Stage 1, we use Supervised Fine-Tuning (SFT) on NoThinking exemplars (concise direct answers without explicit reasoning) to endow the model with concise answering capability. In Stage 2, we perform offline Proximal Policy Optimization (PPO) with an entropy-aware reward function to train models to dynamically select between concise and full reasoning modes based on problem complexity. This offline approach offers greater stability and efficiency compared to online RL methods. Experiments on MATH500, AIME24 and GPQA benchmarks demonstrate that our method significantly reduces response length while maintaining accuracy, validating entropy as both a diagnostic tool and training signal for efficient LLM reasoning. Our code is available via https://anonymous.4open.science/r/Efficient-Reasoning-8BA6.", "tldr": "", "keywords": ["Large Language Model; Efficent Reasoning; Entropy; Overthinking"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/643784e462e2910635275c91026e2f2dd04f82c5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on reducing the overthinking problem in LLMs. It shows that token-level entropy correlates with problem complexity, allowing models to decide when to answer directly or perform full reasoning. The method includes a two-stage training process: supervised fine-tuning on simple “NoThinking” samples, followed by offline PPO with an entropy-aware reward."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The experimental details in this paper are thorough, and experiments have been conducted on multiple models."}, "weaknesses": {"value": "1. The correlation with difficulty in Figure 2 is not clearly explained. Although it is presented as a key motivation, there is no detailed discussion in the paper. It would be helpful to provide more context and clarification regarding this metric.\n2. The paper identifies the problem of excessive reasoning in large models, where long reasoning chains reduce computational efficiency. However, the entropy-based approach discussed in the paper does not seem directly related to this issue. The paper seems to address this problem by imposing an **Efficiency Penalty** on reasoning length. This aspect feels somewhat inconsistent with the earlier points made in the article.\n3. The optimization algorithm presented in the second phase differs significantly from PPO. I did not observe the calculation of advantage functions that adhere to the Bellman equation, which makes the method appear more like GRPO. Referring to it as PPO might be misleading.\n4. It would be beneficial for the paper to include a more thorough discussion of related works, such as [1, 2, 3].\n5. I recommend adding more ablation studies to the paper, particularly experiments evaluating the impact of different weights like $w_1$, $w_2$, $w_3$, and the influence of each component in the reward function on the results.\n6. The paper lacks comparisons with related experiments, such as [4, 5]. Including such comparisons would strengthen the paper's contributions.\n7. A significant concern is that the relationship between problem difficulty, entropy, and model overthinking is not sufficiently clarified. While some correlation experiments are presented, they are not comprehensive enough. It would be valuable to see more detailed analyses or theoretical proofs addressing these interconnections.\n\n\n\n[1] The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models\n\n[2] Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning\n\n[3] REASONING WITH EXPLORATION: AN ENTROPY PER-SPECTIVE ON REINFORCEMENT LEARNING FOR LLMS\n\n[4] Token-Budget-Aware LLM Reasoning\n\n[5] CoT-Valve: Length-Compressible Chain-of-Thought Tuning"}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jpE0FWEjAf", "forum": "KE43G03vI7", "replyto": "KE43G03vI7", "signatures": ["ICLR.cc/2026/Conference/Submission11598/Reviewer_1kVB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11598/Reviewer_1kVB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839882003, "cdate": 1761839882003, "tmdate": 1762922679080, "mdate": 1762922679080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an entropy-driven training framework to reduce overthinking in large language models. By leveraging token-level entropy as a measure of problem complexity, the model learns to choose between concise NoThinking and full Thinking modes. A two-stage training process, which leverages supervised fine-tuning followed by offline PPO, teaches the model to adaptively allocate reasoning effort. Experiments on MATH500, AIME24, and GPQA show significant reductions in response length with maintained or improved accuracy across multiple model sizes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper identifies token-level entropy as an intrinsic, continuous measure of problem complexity. This is more fundamental and fine-grained than previous heuristics, such as response length, providing a reasonable signal for adaptive reasoning."}, "weaknesses": {"value": "1.The evaluation primarily centers on mathematical reasoning benchmarks, which, while offering clear distinctions in difficulty levels that align well with your approach, raises questions about generalizability. The broader applicability of the entropy-complexity relationship and the effectiveness of the proposed training framework to other domains, such as commonsense reasoning, remains to be verified.\n2.The experiments utilize models distilled from DeepSeek-R1. While such distilled models inherently possess stronger reasoning capabilities, it remains unclear whether the original non-distilled models would yield consistent results with your current experimental findings. Furthermore, the performance of other model architectures, such as GLM and Mistral, is also not yet verified.\n3.The current work lacks discussion on the multi-component base reward. The experiments omit the exploration of the weighting scheme assigned to the three distinct rewards."}, "questions": {"value": "1.There are issues with Figure 1 on page 2. First, the \"simple question\" is positioned at the top of the figure, which is mistaken in the caption. Second, there is an inconsistency: the \"simple question\" is listed as \"What is 50% of 240?\" but is solved as \"15% of 240\" in the subsequent steps.\n2.There is a typo in Figure 2: \"deepseek15B\" should be corrected to \"deepseek1.5B\".\nPage 4, Line 198: It is recommended to provide the full, official name of the model. The current designation \"deepseek-1.5B\" is ambiguo"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h6y4KmTDZd", "forum": "KE43G03vI7", "replyto": "KE43G03vI7", "signatures": ["ICLR.cc/2026/Conference/Submission11598/Reviewer_Ddkx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11598/Reviewer_Ddkx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946032627, "cdate": 1761946032627, "tmdate": 1762922678629, "mdate": 1762922678629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Summary:**\nThe paper studies how LLMs tend to overthink by producing unnecessarily long reasoning chains even for simple problems. It identifies response entropy as a reliable indicator of model confidence and problem complexity, showing that entropy correlates with task difficulty. Building on this insight, the authors propose a two-stage framework: supervised fine-tuning for direct answering on easy cases, followed by offline PPO with a confidence reward to encourage adaptive reasoning. Experiments on multiple reasoning benchmarks demonstrate reduced reasoning length with competitive or improved accuracy.\n\n**Contributions:**\n\n1. Systematically validate that response entropy serves as an effective intrinsic measure of problem complexity in LLM reasoning.\n2. Introduce a confidence reward based on response entropy during the PPO phase, with ablation results showing that this design improves performance and reduces generation length."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality:** The paper offers a novel perspective by validating *response entropy* as an intrinsic measure of problem complexity and incorporating it into offline PPO via a confidence reward.\n\n**Quality:** The analysis and experiments are comprehensive and convincing, with ablation results clearly supporting the proposed design.\n\n**Clarity:** The paper is clearly written, well organized, and provides sufficient implementation details for reproducibility.\n\n**Significance:** The findings provide practical insights for improving reasoning efficiency and adaptability in LLMs."}, "weaknesses": {"value": "1. **Comparison with alternative adaptive reasoning methods:** The paper could include stronger baselines, such as other adaptive reasoning strategies (e.g., AdaptThink [1], AdaCoT [2]).\n2. **Discussion of internal uncertainty and problem difficulty:** The paper lacks sufficient discussion of prior studies on how internal uncertainty relates to problem difficulty (e.g., [3]), which would better situate the contribution within existing literature.\n\n**References:**\n\n[1] AdaptThink: Reasoning Models Can Learn When to Think, [https://arxiv.org/pdf/2505.13417](https://arxiv.org/pdf/2505.13417)\n\n[2] AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning, [https://arxiv.org/pdf/2505.11896](https://arxiv.org/pdf/2505.11896)\n\n[3] Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens, [https://arxiv.org/pdf/2505.18237](https://arxiv.org/pdf/2505.18237)"}, "questions": {"value": "1. **Reward weighting:** In formula (4), how were the weights for the different reward components determined? Specifically, why is the accuracy reward set to 0.3 rather than being the largest among the three? Were alternative weight configurations explored, and if so, what were the outcomes?\n\n2. **Design of the confidence reward:** The confidence reward promotes higher certainty in non-thinking mode and lower certainty in thinking mode. Could the authors clarify the rationale for this design? In formula (7), the reward appears to be applied only when the generated answer is correct. If the answer is correct, why not always encourage higher confidence? Conversely, why is the reward set to zero when the answer is incorrect?\n\n3. **Correlation in Figure 2:** How is the “Correlation with Difficulty” on the y-axis of Figure 2 computed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WeDMNyRWRh", "forum": "KE43G03vI7", "replyto": "KE43G03vI7", "signatures": ["ICLR.cc/2026/Conference/Submission11598/Reviewer_Hkfd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11598/Reviewer_Hkfd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980939304, "cdate": 1761980939304, "tmdate": 1762922678167, "mdate": 1762922678167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work tackles the overthinking problem in LLMs, where they produce unnecessarily long reasoning chains for simple tasks. The authors identify token-level response entropy as an intrinsic indicator of problem complexity and use it to guide adaptive reasoning. They propose a two-stage training framework: (1) Supervised fine-tuning on concise “NoThinking” exemplars to enable short, confident answers, and (2) offline PPO with an entropy-aware reward to let the model choose between concise and full reasoning based on task difficulty. Experiments on MATH500, AIME24, and GPQA show that this method reduces response length while maintaining accuracy, confirming entropy’s value as both a diagnostic signal and a training objective for efficient reasoning in LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Letting LLMs not over-think on simple questions is an important question with practical applications\n2. The finding of the entropy of response correlates well with problem difficulty is interesting. \n3. The results of reducing response lengths while keeping accuracy are good."}, "weaknesses": {"value": "1. The definition of problem difficulty needs further clarification and justification.\n2. For base model selection, why not consider Qwen-2.5/3 reasoning models or other alternatives?\n3. Discuss how this method compares with the GPT-5 router and what advantages it offers."}, "questions": {"value": "Please see above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q9aUOkUqhe", "forum": "KE43G03vI7", "replyto": "KE43G03vI7", "signatures": ["ICLR.cc/2026/Conference/Submission11598/Reviewer_qXEm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11598/Reviewer_qXEm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993148817, "cdate": 1761993148817, "tmdate": 1762922677694, "mdate": 1762922677694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}