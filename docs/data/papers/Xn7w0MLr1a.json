{"id": "Xn7w0MLr1a", "number": 17916, "cdate": 1758281999763, "mdate": 1759897145371, "content": {"title": "UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities", "abstract": "Audio generation, including speech, music and sound effects, has advanced rapidly in recent years.\nThese tasks can be divided into two categories: time-aligned (TA) tasks, where each input unit corresponds to a specific segment of the output audio (e.g., phonemes aligned with frames in speech synthesis); and non-time-aligned (NTA) tasks, where such alignment is not available.\nSince modeling paradigms for the two types are typically different, research on different audio generation tasks has traditionally followed separate trajectories.\nHowever, audio is not inherently divided into such categories, making a unified model a natural and necessary goal for general audio generation.\nPrior works on universal audio generation remain limited: auto-regressive models struggle with NTA tasks, while diffusion models often overlook TA tasks.\nIn this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching.\nWe propose a dual-fusion mechanism that temporally aligns audio latents with TA features and integrates NTA features via cross-attention in each model block.\nTask-balanced data sampling is employed to maintain strong performance across both TA and NTA tasks.\nUniFlow-Audio supports omni-modalities, including text, audio, and video.\nBy leveraging the advantage of multi-task learning and the generative modeling capabilities of flow matching, UniFlow-Audio achieves strong results across 7 tasks using fewer than 8K hours of public training data and under 1B trainable parameters.\nEven the small variant with only $~$200M parameters shows competitive performance, highlighting UniFlow-Audio as a potential non-auto-regressive foundation model for audio generation.\nCode and models will be available at https://anonymous3387a8c.github.io/uniflow_audio.", "tldr": "", "keywords": ["universal audio generation", "flow matching", "multi-task learning", "temporal alignment"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/681d3efecd4193ac971a6472b873a34da62f91a2.pdf", "supplementary_material": "/attachment/e53d5bdacb990a36be5a5552dab996163b118e98.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents UniFlow-Audio, a unified flow matching–based audio generation framework that handles both time-aligned (TA) and non-time-aligned (NTA) tasks within a single non-autoregressive model. A dual-fusion mechanism and block-wise fusion enable task-specific conditioning without interference, and the model achieves competitive or superior performance across seven audio tasks while reducing sampling steps compared to diffusion-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**1. Unified Framework for Diverse Audio Tasks**\n\nSuccessfully integrates both TA and NTA tasks (e.g., TTS, SE, V2A, T2A) under a single flow matching model — an elegant and generalizable design.\n\n**2. Effective Fusion and Task Handling**\n\nThe proposed block-wise dual-fusion mechanism effectively balances multi-task learning while maintaining performance across modalities.\n\n**3. Well-Written and Clearly Structured Paper**\n\nThe paper is well-organized and easy to follow, with clearly described limitations and detailed design explanations that improve overall readability and transparency."}, "weaknesses": {"value": "**1. Lack of Analysis on Fusion Depth**\n\nAlthough block-wise fusion improves performance, the paper does not analyze which layers (early or late) contribute most to the gain.\n\n**2. Missing Comparison with Prior Unified Models**\n\nThe paper lacks direct experimental comparisons with existing unified frameworks such as UniAudio or AudioX, making the claimed advantage of UniFlow-Audio less convincing. (Despite of line 318)"}, "questions": {"value": "**1. On Fusion Depth Analysis**\n\nThe paper demonstrates that block-wise fusion is more effective than input-level fusion, but it does not specify which layers (early, middle, or late) contribute most to this improvement. Could the authors provide additional ablation or simple explanation to show how fusion depth impacts performance?\n\n**2. On Comparison with Prior Unified Models**\n\nThe paper mentions that previous unified models (e.g., UniAudio, AudioX) fail to perform consistently across TA and NTA tasks.\nCould the authors evaluate or explain these prior models under both TA and NTA settings to verify whether UniFlow-Audio achieves uniformly higher performance, or whether its advantage mainly comes from averaging stronger results across specific tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xU64kTCL1v", "forum": "Xn7w0MLr1a", "replyto": "Xn7w0MLr1a", "signatures": ["ICLR.cc/2026/Conference/Submission17916/Reviewer_SnsA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17916/Reviewer_SnsA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655546435, "cdate": 1761655546435, "tmdate": 1762927733588, "mdate": 1762927733588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniFlow-Audio, a unified non-autoregressive (NAR) framework for diverse audio generation tasks using flow matching. The work divides audio generation problems into two fundamental types, time-aligned (TA) (e.g., TTS, SE, V2A) and non-time-aligned (NTA) (e.g., T2A, T2M), and aims to handle both categories within a single model.\n\nThe key contributions include:\n- Dual-fusion mechanism: temporally aligned addition for TA tasks and cross-attention for NTA tasks, with task-irrelevant streams replaced by dummy embeddings to avoid interference.\n- Block-wise integration of TA/NTA information across Transformer layers.\n- Task-balanced sampling to mitigate data imbalance between TA and NTA tasks.\n\nThe model is trained on 7.7khours of public data (< 1B parameters) and evaluated on seven tasks spanning text, audio, and video input modalities. Experiments show competitive or superior results over single-task diffusion or autoregressive baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The distinction between TA and NTA tasks is well motivated and provides a unified view of disparate audio generation problems.\n- The dual-fusion mechanism with dummy embeddings is new, practically effective way to disentangle fusion paths without separate models.\n- The paper systematically validates dual-fusion, block-wise fusion, and balanced sampling, showing each contributes meaningfully. Evaluation across seven tasks demonstrates strong parameter efficiency and cross-task synergy, particularly impressive for a 200m-parameter model trained on modest data."}, "weaknesses": {"value": "- The paper explicitly omits UniAudio (Yang et al., 2024) and AudioX (Tian et al., 2025) from quantitative tables, citing data-size or task-scope differences. However, this makes it difficult to gauge real progress: both are unified systems targeting similar objectives, and reporting normalized comparisons (e.g., per-hour or per-parameter efficiency) would better situate the contribution. Moreover, for several tasks (e.g., TTS, SE), the chosen baselines such as NaturalSpeech 2 or DOSE are not necessarily the strongest on the specific datasets used (LibriTTS, VoiceBank+Demand). Including more competitive or domain-matched baselines.\n- While coverage is broad, each task is shallowly evaluated. Subjective MOS studies lack listener statistics.\n- Dense implementation detail sometimes obscures the intuition (e.g., Section 3.4). A clearer separation between conceptual and engineering design would improve readability."}, "questions": {"value": "- While we acknowledging the differences on data, it is still necessary to report the results from uniaudio or audiox with clear noting about the data differneces\n- How stable is training when mixing TA and NTA tasks? Any gradient interference observed?\n- For subjective MOS, how many raters and samples per task were used? Any significance testing?\n- Is the same VAE shared across all domains, or are domain-specific VAEs used during pre-training?\n- For NTA tasks where L_{dur-seq} is omitted, does the imbalance affet the shared backbone?\n- How are the dummy embeddings initialized? Are they learned per task?\n- For task balanced sampling, you upsample T2A and T2M. Did you test continuous weighting in the loss instead of discrete resampling? How sensitive are results to this ratio?\n- Since AudioSR outputs 48khz and you downsample to 24kHz for comparison, does this favor UniFlow-Audio? Would an upsampled version maintain quality?\n- As in UniAudio, some joint inter-task benefits are observed, Did you test whether training on V2A data improves T2A or TTS or vise versa? Some insight on inter-task transfer would highlight the benefit of unification.\n- Can UniFlow-Audio generalize to unseen input types (e.g., image → audio) given CLIP encoders, or is each task explicitly conditioned?\n- The explanation about noise amplification in SE is interesting, could you provide numerical trends or error bars to support that observation?\n- Please report training and inference time per second of audio and GPU utilization to substantiate the efficiency claims of flow matching.\n- The total 7.7 kh of data mixes speech, music, and environmental sounds. How much of the performance improvement arises from cross-domain exposure versus architectural changes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HmzfKlfT1h", "forum": "Xn7w0MLr1a", "replyto": "Xn7w0MLr1a", "signatures": ["ICLR.cc/2026/Conference/Submission17916/Reviewer_DJhv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17916/Reviewer_DJhv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729265930, "cdate": 1761729265930, "tmdate": 1762927733130, "mdate": 1762927733130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a unified audio generation model that can generate speech, audio and music for several tasks including text-to-speech, text-to-audio, speech enhancement, etc. The architecture design based on flow matching aims to solve the time-aligned and non-time-aligned tasks simultaneously. The experiments show the improvements obtained by the proposed unified model compared to individual task-specific models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Flow matching architecture is verified effective for unified modeling.\nBetter performance is achieved by the proposed flow matching model, compared to task-specific models.\nThe discussion of unified models from the perspective of time alignment is novel. \nInstruction-based task specifications are enabled for the unified model."}, "weaknesses": {"value": "The novelty is lacking, given there are several previous work on unified audio generation models. This paper only creates a new flow matching-based architecture for unified audio model development.\nThe motivation of the proposed framework is not strong enough. Why previous autoregressive models are inferior in time-alignment task is not clearly articulated. In a language model style unified model, the time-alignment seems not a big issue, as the implicit self-attention mechanism takes care of the alignment automatically. \nThe experiments can be improved, with previous unified models compared, to show the advantages of the proposed model. Current experiments are not benchmarked with any previous unified model.\nThe necessity of explicit consideration of time alignment is not well demonstrated."}, "questions": {"value": "The author may want to illustrate the motivation, why the designed architecture is better in tackling time-alignment and non-time-alignment tasks, with some analysis or observations to better motivate. \n\nSome previous works, e.g. E2 TTS, have shown that the alignment between condition and the target is not a problem in diffusion/flow machine-based architectures, it may be also worth analyzing the duration adaptor that provides the alignment explicitly (for time-aligned tasks) with ablation studies to check contributions of the duration adaptor.\n\nEq (1) is strange, the function Attn() has two identical input arguments. Also, Eq (1) seems different from what’s shown in Fig. 2. In Eq(1), the sum of attention outputs and content representation C yields C^I, the so called task-involved content embedding. However, Fig. 2 shows C is used as query for the attention module to obtained the C^I. The figure and the equation could be made clearer.\n\nTable 1 doesn’t compare with previous unified models, e.g. UniAudio. Although it can be argued that previous unified models, e.g. UniAudio, use much more data and parameters than the proposed model. Then it may be a fair question to answer: Is there an advantage of the proposed architecture compared to the previous unified model architectures that are trained on the same data size using with similar parameter sizes? Which can also partially show the necessity of explicitly considering time alignment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zCz9Ee77k3", "forum": "Xn7w0MLr1a", "replyto": "Xn7w0MLr1a", "signatures": ["ICLR.cc/2026/Conference/Submission17916/Reviewer_m9Bu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17916/Reviewer_m9Bu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882042429, "cdate": 1761882042429, "tmdate": 1762927732502, "mdate": 1762927732502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}