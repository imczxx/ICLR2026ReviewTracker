{"id": "Zb3yuOZVDN", "number": 2033, "cdate": 1756979008143, "mdate": 1759898173020, "content": {"title": "CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning", "abstract": "Allocating more computation during inference time (test-time scaling) improves language model performance, especially for reasoning tasks.\nHowever, popular methods like Best-of-$N$ sampling often show diminishing returns as $N$ increases.\nTo address this inefficiency, we introduce a general $\\textbf{test-time calibration framework}$ that adaptively modifies the model toward high-reward reasoning paths, with theoretical guarantees of improving the lower bound of expected reward under finite sampling, all without large language model (LLM) retraining.\nWithin this framework, we propose $\\textbf{CarBoN}$ (Calibrated Best-of-$N$), a two-phase method that first explores the solution space and then learns a calibration of the logits via an input-specific temperature $T$ and additive shift vector $\\delta$, guiding generation toward more reliable reasoning.\nExperiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency, with up to $4\\times$ fewer rollouts to reach the same accuracy, while often achieving higher accuracy under fixed budgets.\nWe also analyze the complementary roles of $T$ and $\\delta$ in balancing output diversity and correctness, and demonstrate that the framework also generalizes to step-level sampling strategies such as beam search.", "tldr": "", "keywords": ["test-time scaling", "calibration", "LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f719940e74587bf6ca3a45df9159e8227f7c3dea.pdf", "supplementary_material": "/attachment/b8e85dc976ba79a74ea8fdf5f97e911b928001b0.zip"}, "replies": [{"content": {"summary": {"value": "CarBoN introduces test-time calibration for LLM reasoning, inspired by the observation that Best-of-N sampling discards N-1 trajectories after selecting the highest-scoring output. The method splits inference budget N into exploration and exploitation phases, reusing high-scoring discarded samples to learn input-specific calibration parameters (temperature T and additive shift δ) that guide subsequent generation toward more reliable reasoning paths."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Clever reuse of discarded trajectories\n- Ablations isolate T and δ contributions"}, "weaknesses": {"value": "- Theorem 2 assumes unique optimal output y*, unrealistic for mathematical reasoning with multiple valid solution paths. \n- The \"4× fewer rollouts\" compares CarBoN@N=64 accuracy vs baseline@N=256 accuracy—conflating sample efficiency with computational efficiency. Both methods use N PRM calls for their respective budgets. \n- Severely outdated experimental setup: Models are almost a year old + MATH500 is a weak evaluation dataset at this point. Even if this paper gets in, by ICLR 2026, these models would be almost 18+ months old.\n- The calibration depends on the PRM quality, and PRMs are themselves unreliable, especially for reasoning models. \n- Single PRM across all experiments. We don't know how the quality of PRM can affect the empirical results.\n\n\nMinor nit:\n- Figure 5: Title has \"of\" spelled as \"f\""}, "questions": {"value": "- Does this work on current reasoning models, such as Qwen3?\n- The dependence on PRMs is not ideal. Any ideas on how to get rid of this dependence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "srBA1oXpfU", "forum": "Zb3yuOZVDN", "replyto": "Zb3yuOZVDN", "signatures": ["ICLR.cc/2026/Conference/Submission2033/Reviewer_Kako"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2033/Reviewer_Kako"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876734082, "cdate": 1761876734082, "tmdate": 1762915998125, "mdate": 1762915998125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new method to improve test-time reasoning through a two-phase process. In the first phase, it performs naïve parallel sampling with a fixed temperature. Then, it trains two lightweight parameters: one is the temperature, and the other is a bias vector before the language model head, to recalibrate the output distribution for each problem. After that, it runs another round of parallel sampling using this updated distribution, and finally selects the answer. This method relies on a continuous reward model to rank the responses. Additionally, the paper provides a theoretical argument for why this recalibration works and presents empirical evidence showing that the method improves upon the naive parallel sampling baseline by one or two points across benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is straightforward, making it easy to implement and test.\n\n2. The paper provides detailed empirical results across multiple models from various open-source families.\n\n3. The paper discusses how to combine the proposed method with more complex token-level sampling strategies beyond just naïve parallel sampling. \n\n4. The paper offers an interesting analysis showing that the learned temperature is higher for more difficult problems, which aligns well with the intuition behind the design."}, "weaknesses": {"value": "1.  The theory statement provided in the paper is not very related to the empirical result. The major result Theorem 1 essentially state that if the probability of the unique correct solution increases after training, then Pass@N will increase as well. This statement is rather trivial and is true for any method essentially.\n\n2. The paper tested the methods on some small datasets such as AIME that only contains 30 problems. However, the paper does not provide the average performance over multiple random seeds. This makes it hard to judge whether the improvement of 1 problem is statistically significant. For some larger datasets like MATH, the paper didn't provide error bars.\n\n3. The temperature chosen for baseline is tuned on a LLaMA 1B model. It is not clear that whether this temperature will be suboptimal for Qwen models tested.\n\n4. [Minor] The synthetic example in Figure 1 is not very easy to understand as the configuration is not described in detail in the main paper."}, "questions": {"value": "1. See weakness 2, can the authors provide some qualitative estimation of the randomness to better justify the significance of the experiments?\n\n2. What is the typical wall time for training $T$ and $\\delta$ for every problem. How does this compare with the sampling time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3xygGUH4O9", "forum": "Zb3yuOZVDN", "replyto": "Zb3yuOZVDN", "signatures": ["ICLR.cc/2026/Conference/Submission2033/Reviewer_RJpm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2033/Reviewer_RJpm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032887346, "cdate": 1762032887346, "tmdate": 1762915997865, "mdate": 1762915997865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces calibrated sampling for language model to push the model's output distribution towards more promising outputs. The method modifies the logits via adding a linearly transformed vector and choosing a new sampling temperature, which are optimized through a simple optimization. The paper applies this technique to Best-of-N by increasing the chance of generating the top results from the exploration phase, and observes accuracy improvements. Some theoretical justifications are provided."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. I find the idea and the technique fascinating and believe it might have good potential. The calibration technique is a simple way to shift the output distribution of the LLM towards more desirable outputs, without the need for any training.\n\n2. The paper is generally easy to follow, though I find some theoretical parts too vague.\n\n3. I like the analysis of calibrated temperature for various difficulty levels."}, "weaknesses": {"value": "1. My main concern about the paper is the limited breadth and rigor of the experiments:\n\n1A: The AIME results are not informative at all as they seem to be a single run on only 30 questions. It is essential to perform multiple independent runs and report the average accuracy percentage to have a meaningful figure. The current results are too noisy to make any conclusions from. \n\n1B: With unreliable AIME results, the paper's evidence comes down to only one benchmark. I find it necessary to test the method on a variety of benchmarks, ideally on different subjects and final answer styles (multiple choice, short answer, open ended, etc). I am concerned some special property MATH questions might have been helpful for these results. The additive vector $\\delta$ is effectively a shift in the final hidden space and it might behave differently. It is perhaps ok, but it needs to be tested to have a conclusive test of effectiveness.\n\n1C: It is also important to report error bars to understand the evaluation is accurate enough for the drawn conclusion. I expect for MATH500, this is just a matter of reporting, but perhaps AIME will need more runs.\n\n2. Unfortunately, the theoretical results provide little to no support for the algorithm's promise. I understand that the main contribution of the paper is empirical. However, the theoretical results carry very little information and were mostly a distraction from the paper's strengths.\n\n2A: The theorem statement is very weak. The statement is on a very loose lower bound and under strong explicit (mentioned in the thorem) and implicit assumptions. The implicit one being the fact that the goal is to sample the highest reward, which is only true under perfect rewards. Moreover, most of the theorem result is based on \"calibration increases the probability of $y^*$\".  This is a very strong assumption.\n\n2B: The statement of Lemma 1 (both in main text and appendix) uses new complicated quantities without their formal definition and is hard to parse. Nonetheless, the result does not seem to be surprising. The fixed parameters are expectedly not the ones that create the highest chance of generating $D_{calib}$. I like the insight as a motivation for the algorithm, though.\n\n3: As another path to improve the paper, I would suggest investigate the underlying dynamics more. The vector $\\delta$ is a shift in hidden space and some more visualizations may be possible. Currently, only temperature is discussed.\n\n4: Further discussions on the choice of hyperparameters is welcome: $\\lambda_\\delta$, $k$, $N_1$, $N_2$"}, "questions": {"value": "Please see weaknesses. One additional questions:\n\nWhat do you think about using the rewards in the exploration phase as the weights of the outputs in calibration loss, instead of using them just for choosing which outputs to use."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VB7iKmumUU", "forum": "Zb3yuOZVDN", "replyto": "Zb3yuOZVDN", "signatures": ["ICLR.cc/2026/Conference/Submission2033/Reviewer_soou"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2033/Reviewer_soou"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762231444475, "cdate": 1762231444475, "tmdate": 1762915997706, "mdate": 1762915997706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}