{"id": "M5MfDi4gJO", "number": 23396, "cdate": 1758343150880, "mdate": 1763716671887, "content": {"title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs", "abstract": "Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prompt crafting while maximizing performance, remain confined to text, ultimately limiting the full potential of MLLMs. Motivated by this gap, we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts. To tackle this problem, we then propose the Multimodal Prompt Optimizer (MPO), a unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates but also guides the selection process of candidate prompts by leveraging earlier evaluations as priors in a Bayesian-based selection strategy. Through extensive experiments across diverse modalities that go beyond text, such as images, videos, and even molecules, we demonstrate that MPO outperforms leading text-only optimization methods, establishing multimodal prompt optimization as a crucial step to realizing the potential of MLLMs.", "tldr": "We introduce the problem of multimodal prompt optimization and propose the multimodal prompt optimizer, to harness the full capacity of multimodal large language models beyond text.", "keywords": ["Multimodal Large Language Models", "Multimodal Prompt Optimization", "Prompt Optimization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f191067f16c6b8261e3e58229e41394531810936.pdf", "supplementary_material": "/attachment/66ab2ee3be539964201f36e0e110e43d8a79b323.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes the Multimodal Prompt Optimizer (MPO), which jointly refines textual and non-textual prompts through alignment-preserving updates and employs a Bayesian UCB strategy for efficient candidate selection. Experiments across images, videos, and molecules show that MPO outperforms text-only baselines, highlighting multimodal prompt optimization as a key step toward advancing MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper expands the prompt optimization space beyond text to incorporate multiple modalities, which represents an interesting and promising research direction.\n- MPO shows strong performance across diverse modalities (images, videos, molecules), demonstrating its effectiveness.\n- The paper is clearly written and easy to follow."}, "weaknesses": {"value": "Although the paper claims that MPO is not an Instance-Specific Prompting and Optimization method but rather aims to discover a single, reusable prompt that enhances performance across an entire task, the evaluation still relies on fine-grained dataset partitioning and reducing the number of classes to control task difficulty. This design choice appears to weaken the claimed generalization ability of the proposed method, suggesting that its effectiveness in more complex or diverse scenarios remains to be validated."}, "questions": {"value": "- Is there a clear order in which the three Exploration Operators designed in this paper are called, and how are they combined? Figure 6 shows that there doesn't appear to be a clear order in which operators are called, and the combinations are arbitrary. The original paper mentions that these operators systematically expand, refine, and recombine non-textual prompts. Could you explain this systematic approach in more detail?\n- In the Appendix (lines 665–670), the authors mention that for certain datasets, they further selected groups containing three or four distinct species to maintain a balanced level of difficulty. However, this strategy effectively reduces the overall task complexity, as the generated multimodal prompts only need to distinguish among a small number of categories. It is recommended that the authors further discuss whether the proposed method can still maintain good performance and stability when the number of categories increases significantly. \n- The generation of non-textual multimodal prompts in this paper appears to rely heavily on high-performance image generation and editing models. It is suggested that the authors clarify the extent to which their method depends on the specific capabilities of these models. If alternative image generation or editing models were used, would the proposed approach still maintain its effectiveness and robustness? If the authors can provide convincing explanations or additional evidence regarding these questions, I would be willing to consider increasing the score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "khnsyc7e7v", "forum": "M5MfDi4gJO", "replyto": "M5MfDi4gJO", "signatures": ["ICLR.cc/2026/Conference/Submission23396/Reviewer_DhB1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23396/Reviewer_DhB1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555955167, "cdate": 1761555955167, "tmdate": 1762942644685, "mdate": 1762942644685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the prompt optimization of MLLMs. Unlike prior approaches that only optimize text prompts, the authors incorporate information from another modality into the prompt.\nTo optimize the multi-modal prompt, the authors propose MPO which consists of two key components: alignment-preserving exploration of multimodal prompt space and prompt selection with prior-inherited Bayesian UCB."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea is good, which revolutionizes the conventional prompt structure for MLLMs.\n2. The paper is well-written, with clear motivation and challenges.\n3. The experiments are thorough."}, "weaknesses": {"value": "1. In the first paragraph of section 3.2, the authors mentioned that \"a naive approach that independently updates textual and non-textual components risks producing misaligned prompts\". Have the authors tested this naive approach (is it Random Image Prompt in Figure 4? If yes, what are the real performance values instead of performance gains?).\n2. When identifying the failure set, how is your multimodal prompt initialized? As it contains both text and image, how are they selected and organized?\n3. Another concern is the efficiency: extra models are included to refine the multi-modal prompt. Compared to the baseline methods which optimize textual prompt only, what is the time (or computation steps) used by MPO to complete one iteration?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nlQ4UHVRpy", "forum": "M5MfDi4gJO", "replyto": "M5MfDi4gJO", "signatures": ["ICLR.cc/2026/Conference/Submission23396/Reviewer_wMqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23396/Reviewer_wMqy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761609922556, "cdate": 1761609922556, "tmdate": 1762942644112, "mdate": 1762942644112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Multimodal Prompt Optimization (MPO), a new framework that extends traditional text-only prompt optimization to multimodal large language models (MLLMs) by jointly optimizing textual and non-textual components of prompts. Specifically, the method combines alignment-preserving exploration, which updates text and image prompts coherently based on failure-driven feedback, with a prior-inherited Bayesian UCB selection strategy that leverages parent–child prompt priors to improve sample efficiency. Experiments across image, video, and molecular tasks show that MPO outperforms strong text-only baselines while significantly reducing evaluation cost, suggesting that multimodal prompt optimization can better unlock the reasoning potential of MLLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The combination of alignment-preserving exploration with prior-inherited Bayesian UCB is technically sound and elegantly balances multimodal consistency with sample efficiency.\n\n2.The paper is clearly written and well-structured, with intuitive figures and pseudocode that align closely with the algorithmic flow, accompanied by informative ablations and visual analyses."}, "weaknesses": {"value": "1.The paper frames “multimodal prompt optimization” as discrete text + generated visual prototypes, which risks conflation with learnable soft-prompting (e.g., MaPLe) and needs clearer terminology and positioning.\n\n2.The notion of “parent prompt” and its construction (especially for multi-parent mix) is not formalized.\n\n3.Including powerful image/video generators injects generator priors and extra compute, so gains may stem from tooling rather than the method itself.\n\n4.Efficiency is measured via evaluation counts rather than real cost, omitting overheads from generation and long-context prompting.\n\n5.Cross-modal alignment relies on a single metric (e.g., DSG)."}, "questions": {"value": "1.Which posterior quantile is used for Bayes-UCB, and how do your theoretical conditions differ from standard Bayes-UCB assumptions?\n\n2.If you replace generated prototypes with retrieved in-distribution examples or OOD random images, how do performance and alignment change?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CMG7qZkRT8", "forum": "M5MfDi4gJO", "replyto": "M5MfDi4gJO", "signatures": ["ICLR.cc/2026/Conference/Submission23396/Reviewer_oxEi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23396/Reviewer_oxEi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761685761259, "cdate": 1761685761259, "tmdate": 1762942643750, "mdate": 1762942643750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on multimodal prompt optimization for Multimodal Large Language Models (MLLMs), addressing two critical gaps in existing text-only Automatic Prompt Optimization (APO): underutilization of MLLMs’ multimodal capabilities, and inherent challenges (cross-modal inconsistency, sparse high-quality candidates) in multimodal prompt spaces.\n\nKey contributions:  \n1. Formalizes the multimodal prompt optimization problem, defining optimal prompts as text-nontext pairs \\((t,m)\\) that maximize MLLM performance on target tasks.  \n2. Proposes the **MPO framework**:  \n   - *Alignment-Preserving Exploration* (via cohesive backpropagation, joint multimodal update, and 3 complementary operators) ensures text-image semantic consistency;  \n   - *Prior-Inherited Bayesian UCB* leverages parent-child prompt performance correlation to solve cold-start, reducing evaluation budget by 42%-70%.  \n3. Validates on 10 datasets across 3 modalities (image/video/molecule), outperforming text-only APO (e.g., +8.6% accuracy on CUB-200-2011).  \n\nMPO fills the multimodal APO gap for MLLMs, with strong cross-model/multimodal generalization and practical efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Originality**: Breaks the text-only limitation of existing APO methods, first formalizing the multimodal prompt optimization problem (defining prompts as text-nontext pairs \\((t,m)\\)). It also creatively improves Bayesian UCB by leveraging parent-child prompt performance correlation to solve cold-start, extending bandit-based selection to multimodal scenarios innovatively.  \n- **Quality**: Conducts rigorous validation—covering 10 datasets across 3 modalities (image/video/molecule), cross-model tests (Qwen2.5-VL, Gemma3), and ablation studies (verifying alignment mechanisms/operators’ necessity). Results are reliable and generalize well.  \n- **Clarity**: Clearly presents problem formulation, MPO’s two core components (with formulas and flowcharts), and experimental design. The logical structure is straightforward, enabling easy understanding of the framework.  \n- **Significance**: Fills the gap of multimodal prompt optimization for MLLMs, reduces evaluation budget by 42%-70% for practicality, and provides a foundational framework for future multimodal prompt research."}, "weaknesses": {"value": "1. **Lack of Validation on Mainstream MLLM General Benchmarks, Limiting Evidence of Universal Adaptability**  \nThe current experiments rely solely on custom task-specific datasets (e.g., CUB-200-2011, PlantVillage) and fail to validate MPO on widely recognized MLLM multimodal benchmarks, leaving its ability to enhance MLLMs’ general capabilities unsubstantiated. Specifically:  \n- It omits **static multimodal foundational benchmarks** (e.g., MME, MMBench), which focus on core perceptual capabilities of MLLMs (e.g., image-text matching, attribute recognition)—there is no evidence that MPO can optimize prompt performance for these fundamental tasks.  \n- It excludes **long-video dynamic modality benchmarks** (e.g., VideoMME), which require handling temporal alignment between long-sequence videos and text (e.g., locating specific clips, understanding temporal logic). The paper’s existing alignment mechanism, designed for static images, remains untested for such long-video scenarios, casting doubt on its effectiveness.  \n- It neglects **cross-modal complex reasoning benchmarks** (e.g., ScienceQA, MathVista, Geometry3k)—benchmarks that demand MLLMs integrate multimodal information to solve logical reasoning or mathematical problems. Since the paper only validates classification/prediction tasks, there is no proof that MPO-optimized prompts can improve complex reasoning performance, leaving MPO’s adaptability to general MLLM scenarios unconfirmed.  \n\n2. **Unaddressed Implicit Deployment Costs, Missing Cost Comparison with Text-only APO**  \nWhile the paper emphasizes that the prior-inherited Bayesian UCB reduces evaluation budget by 42%–70%, it overlooks the significant computational overhead of generating multimodal candidates. Modal-specific generators (e.g., GPT-Image, video editing models) used for image/long-video clip generation incur much higher costs than text prompt generation: for instance, single-image generation takes 2–5 seconds (vs. 0.1 seconds for text generation), and diffusion-based image generators require 3–5 times more GPU memory than text models. Critically, the paper fails to compare the **total deployment cost of MPO** (including iterative multimodal generation overhead) with that of text-only APO. If the implicit costs of multimodal generation offset or even exceed the saved evaluation budget, MPO’s practical utility in real-world deployment would be severely undermined—this key cost trade-off is entirely unaddressed."}, "questions": {"value": "Same as the section of **Weaknesses**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "by3ZgcFE5g", "forum": "M5MfDi4gJO", "replyto": "M5MfDi4gJO", "signatures": ["ICLR.cc/2026/Conference/Submission23396/Reviewer_oPXv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23396/Reviewer_oPXv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912698288, "cdate": 1761912698288, "tmdate": 1762942643488, "mdate": 1762942643488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper defines the new problem of multimodal prompt optimization and introduces MPO, a framework that jointly optimizes textual and non-textual prompts for multimodal large language models. The method combines alignment-preserving exploration across modalities with a prior-inherited Bayesian UCB strategy for efficient prompt selection. Experiments on diverse modalities (images, videos, and molecules) show consistent improvements over existing text-only prompt optimization baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The idea of extending prompt optimization beyond text is timely and relevant, filling a gap in the growing MLLM literature.\n\n(2) The paper is technically clear, well-written, and supported by convincing experiments on multiple modalities and model backbones.\n\n(3) The proposed Bayesian prior mechanism for efficient search adds a nice practical touch that improves optimization stability."}, "weaknesses": {"value": "(1) The conceptual jump from text-only to multimodal prompt optimization is natural but not as novel as it’s presented; many parts resemble standard multimodal conditioning or input co-optimization.\n\n(2) The analysis lacks stronger insights into why multimodal prompts help; results show improvement but don’t probe interpretability, modality interactions, or failure cases."}, "questions": {"value": "(1) How much of the gain actually comes from the added non-textual modality rather than the optimization process itself? A controlled text-only ablation using the same search procedure would clarify this.\n\n(2) How robust is the “alignment-preserving” exploration? When the generated visual component drifts semantically, does the optimization recover or collapse?\n\n(3) The Bayesian UCB prior sounds appealing, but how sensitive is the performance to the prior strength? Could it bias the search toward mediocre parents if the correlation assumption breaks?\n\n(4) Since the framework depends on GPT-based visual and molecular generators, how reproducible is this pipeline for researchers without access to those models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YthohZUI8l", "forum": "M5MfDi4gJO", "replyto": "M5MfDi4gJO", "signatures": ["ICLR.cc/2026/Conference/Submission23396/Reviewer_z13R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23396/Reviewer_z13R"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918801045, "cdate": 1761918801045, "tmdate": 1762942642589, "mdate": 1762942642589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}