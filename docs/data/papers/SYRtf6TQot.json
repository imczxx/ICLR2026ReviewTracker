{"id": "SYRtf6TQot", "number": 6116, "cdate": 1757953258467, "mdate": 1763723674277, "content": {"title": "Efficient Quantization-Aware Adaptation for Visual Foundation Models", "abstract": "Efficient strategies to jointly adapt and deploy large language models have seen a growing need under resource-limited conditions for downstream applications.\nHowever, when applied to visual foundation models, existing methods typically incur either high GPU memory consumption during adaptation or extra computation costs introduced by the adapters at deployment.\nIn this paper, we propose **E**fficient **Qu**antization-aware **A**daptation (EQuA) that achieves high efficiency in both adaptation and deployment for visual foundation models. \nWe observe that dominant memory consumption arises from intermediate activations cached for backpropagation in the deep backbone and activation quantizers. To address this issue, we split a lightweight sub-network from the backbone during adaptation as a side adapter branch, and tailor two adaptation strategies to eliminate these cached activations, thereby significantly reducing memory consumption.\nAt deployment, the side adapter branch is merged back into the backbone, yielding a quantized model without any extra computation costs.\nExtensive experiments on representative visual foundation models and diverse downstream tasks exhibit that EQuA achieves an elegant trade-off between performance and efficiency. \nFor example, EQuA yields over 70\\% GPU memory reduction compared to state-of-the-art baselines while maintaining competitive performance.", "tldr": "", "keywords": ["visual foundation models", "quantization", "efficient adaptation", "quantization-aware training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b5e4b68d56d215eccd393824ab6aac18a4f0a81e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes EQuA (Efficient Quantization-aware Adaptation) for visual foundation models. It addresses high GPU memory use in adaptation and extra deployment computation of existing methods by splitting a lightweight SSA from the backbone (freezing the memory-intensive MIB) and using SAQF/BAQF strategies. EQuA cuts over 70% adaptation memory, keeps competitive performance, and has 1.38× inference speedup. Its contributions include being the first for visual models, the mergeable SSA, SAQF/BAQF, and superior performance-efficiency trade-off via experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper pioneers a mergeable Sub-network Side Adapter (SSA) and SAQF/BAQF strategies for visual foundation models, solving the activation-dominated memory issue unaddressed by LLM-focused methods.\n2. Extensive experiments on SAM/ViT across diverse tasks (e.g., segmentation, classification) with strict baselines show EQuA’s 70%+ memory reduction, competitive performance, and 1.38× speedup, ensuring high quality.\n3. It clearly explains visual model pain points, details EQuA’s \"split-adapt-merge\" pipeline, and enables resource-constrained deployment, advancing real-world use of visual models."}, "weaknesses": {"value": "1. The paper only evaluates EQuA on SAM and ViT, lacking tests on other representative visual foundation models like CLIP or Stable Diffusion, limiting the generalizability of its efficiency claims to broader visual task scenarios.\n2. While analyzing hyperparameters like ($D_s$) and LoRA rank, the paper provides no discussion on EQuA’s performance under varying input image resolutions or batch sizes beyond 2 and 32, leaving gaps in understanding its efficiency across practical deployment conditions.\n3. The paper compares EQuA with QST but does not fully address why its stochastic SSA channel selection is more robust than deterministic top-$D_s$ selection in long-term fine-tuning, with no analysis on overfitting trends across more epochs to validate generalization."}, "questions": {"value": "1. Could you provide supplementary experiments or analysis to explain if EQuA can adapt to visual foundation models like CLIP or Stable Diffusion, and what modifications are needed if not?\n2. Could you show how EQuA’s memory, training time, and performance change with different batch sizes and image resolutions, and if these affect hyperparameter settings?\n3. Could you supplement ablation studies on overfitting trends of Strategy A vs. Strategy C over more epochs or smaller datasets, and explain why stochastic selection avoids overfitting better?\n4. Could you report GC’s additional memory overhead, impact on training throughput, and effectiveness across different models or bit-widths?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9YuONd5lyh", "forum": "SYRtf6TQot", "replyto": "SYRtf6TQot", "signatures": ["ICLR.cc/2026/Conference/Submission6116/Reviewer_NEZo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6116/Reviewer_NEZo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791665672, "cdate": 1761791665672, "tmdate": 1762918476613, "mdate": 1762918476613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "EQuA splits ViT blocks into a frozen backbone and a lightweight side adapter, fine-tuning only the adapter with quantization-aware strategies. Alternating SSA fine-tuning and block-wise activation quantizer tuning cuts training memory (>70%) while preserving accuracy, and merges adapters at deployment to avoid extra computation, enabling efficient 4-bit inference."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper directly addresses activation-dominated memory in vision QAT via SSA/MIB splitting and SSA-only backprop, achieving large training memory savings without deployment overhead.\n2. The authors provide a sound, reproducible design: activation-scaled channel selection, gradient checkpointing to inject backbone signals, and LoRA-in-SSA for parameter efficiency.\n3. Experiments are comprehensive and convincing: competitive accuracy with >70% memory reduction, outperforming QST, and preserving 4-bit speedups validated by CUTLASS-based latency."}, "weaknesses": {"value": "1. The channel selection strategy lacks detailed ablation. It lacks performance–memory trade-off curves across channel split ratios p and comparisons of alternative importance proxies (e.g., |W|, E[|A|], E[A^2]|W|, gradient-based metrics).\n2. SAQF partitions the weight by channel without theoretical motivation or justification. I wonder how the authors consider its potential harm for inter-channel correlations. Furthermore, the authors do not assess whether the channel importance remains stable across different data distributions or input domain.\n3. The method relies on alternating SAQF and BAQF, which raises concerns about training stability and may require careful manual tuning of the alternation frequency, number of steps, and learning-rate schedule. Could the authors provide an analysis of the robustness of this strategy?"}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i3Tj0XP7RC", "forum": "SYRtf6TQot", "replyto": "SYRtf6TQot", "signatures": ["ICLR.cc/2026/Conference/Submission6116/Reviewer_wo2P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6116/Reviewer_wo2P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875218684, "cdate": 1761875218684, "tmdate": 1762918475495, "mdate": 1762918475495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose EQuA, which splits the network into a Memory-Intensive Backbone (MIB) and a lightweight Sub-network Side Adapter (SSA), blocks backprop through the MIB to remove large activation checkpoints, and alternates two procedures: SAQF (side-adapter QAT) and BAQF (block-wise activation-quantizer finetuning). At deployment, the SSA is merged back into the backbone, incurring no extra inference compute. Experiments report large training-memory reductions (>70% in some cases) with competitive accuracy, and up to 1.38× inference speedups at low bit-widths."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work brings the reparameterization idea to adaptation by blocking gradients through the memory-intensive backbone and routing them through a lightweight side adapter (alternating SAQF/BAQF). It directly tackles the true QAT memory bottleneck while staying deployment-aligned, since the adapter is merged back into the backbone at inference.\n2. The approach shows large, consistent gains: over 70% reduction in training memory with accuracy comparable to strong baselines, and up to ~1.38× end-to-end inference speedups at low bit-widths across multiple models and tasks."}, "weaknesses": {"value": "1. Error modeling is very interesting. However, in this paper the authors did not further analyze the relation between sub-net design and error propagation. This makes it hard to understand why the authors choose the proposed sub-net structure.\n2. The task distribution might influence the weight split results. How do we understand the pros and cons of the proposed methods when we need to adapt to a batched tasks?"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DLCcMpvRss", "forum": "SYRtf6TQot", "replyto": "SYRtf6TQot", "signatures": ["ICLR.cc/2026/Conference/Submission6116/Reviewer_mEHg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6116/Reviewer_mEHg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988744035, "cdate": 1761988744035, "tmdate": 1762918474952, "mdate": 1762918474952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework (EQuA) for joint quantization and adaptation tailored for visual foundation models. The framework splits model weights based on importance, obtaining a sub-network component (adapter) called SSA for gradient propagation and updates, while other weights only undergo forward propagation. This approach reduces intermediate activations required for gradient computation. To enhance model training, two strategies are introduced: SAQF for optimizing SSA and BAQF for fine-tuning the merged weights, thereby minimizing quantization errors. Experimental results demonstrate that this method outperforms baseline QST, while reducing memory usage during training compared to other SOTA methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The selection of a subset of weights (SSA) for quantized fine-tuning is sound and novel (Eq.6 & Eq.7).\n2. EQuA reduces memory usage during training.\n3. In the visualizations presented in the paper, EQuA yields ​​favorable​​ results.\n4. The strategy of splitting and merging for SSA and MIB is flexible and can be used for further exploration in other researches."}, "weaknesses": {"value": "1. The paper lacks an overall description in the method section, particularly for the ​​initial quantization​​. For instance, the training pipeline is unclear. It omits how the initial model is prepared (e.g., PTQ, QAT, or a full-precision model) before applying EQuA fine-tuning. The paper should also specify the source for quantizer: are the quantization parameters for the MIB and SSA weights calibrated independently on their respective subsets, or jointly on the original, full-precision weight tensor? Additionally, Figure 2 fails to illustrate the quantizer, which hinders readability.\n2. It is important to report the cost of the weight importance calculation and SSA selection, detailing the computational cost (e.g., time) and resource footprint (e.g., memory) incurred by this stage.\n3. The practical advantage of using LoRA is questionable. As shown in Table 3, the significant reduction in trainable parameters does not result in savings in training time or memory, while slightly hurting performance.\n4. The robustness of the SSA selection strategy requires further demonstration. Since the method relies on probabilistic sampling of SSA channels, the results of EQuA may be unstable. Reporting the standard deviation over multiple experimental runs would provide more compelling evidence (a small table is enough)."}, "questions": {"value": "The paper presents an interesting approach for efficient fine-tuning of foundation models. However, **the practical advantages and core motivation of the EQuA framework** require *further clarification*, particularly from a deployment-centric perspective. For real-world applications of foundation models, the primary concerns are typically inference efficiency and maintained model performance, rather than time / memory in training. While EQuA reduces training memory, it does not lead to a significant improvement in training time due to the overhead of managing partially fine-tuned weights. Moreover, memory constraints can often be alleviated through practical means like training with more GPUs​, hardware upgrades or gradient accumulation without compromising the final model capability.\n\nSome limitations of EQuA related to my question:\n- According to Table 5, EQuA offers no inference advantage over PEQA, with similar training time.\n- More critically, as shown in Table 1, the method incurs significant performance degradation on certain datasets (e.g., >2% drop on CAMO) compared with PEQA, raising concerns about its practicality.\n- Furthermore, the trade-off explored in Table 4 appears problematic: adjusting $D_{s}$ to improve training efficiency comes at the cost of a substantial performance drop, making it difficult to identify a viable solution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DU8RrTLAFY", "forum": "SYRtf6TQot", "replyto": "SYRtf6TQot", "signatures": ["ICLR.cc/2026/Conference/Submission6116/Reviewer_qotj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6116/Reviewer_qotj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762766324948, "cdate": 1762766324948, "tmdate": 1762918474548, "mdate": 1762918474548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a method (EQuA) to reduce memory footprint for fine-tuning vision foundation models, like SAM or ViT. The main challenge in this task is that the memory consumption is dominated by cached activations needed for backward prop as opposed to the size of the model, i.e. number of parameters, which means, typical model compression + tuning scheme, like QA-LoRA, will not be effective in terms of reducing the peak memory usage. The proposed method first identifies important weights and sliced a small portion of $W_V$, $W_{proj}$ in self-attention module and $Lin_1$, $Lin_2$ in MLP module into a side branch (SSA) while the majority will be kept in the a backbone branch (MIB). Although tuning only SSA while keeping MIB frozen could reduce the cached activations, in order to maintain the model performance/quality, a few additional techniques were also employed, i.e. gradient checkpointing, LoRA, and block-wise activation quantization fine-tuning (BAQF). Experimental results of ViT-B on VTAB (Table 2) and SAM-H on medical, natural, and agricultural benchmarks (Table 1) are mostly comprarable to QA-LoRA, with a few specific tasks at ~3% lower than QA-LoRA while peak memory usage is ~1/4 compared to QA-LoRA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. reasonable amount of experiments and ablation studies.\n2. relevant and useful details are provided in Appendix."}, "weaknesses": {"value": "**1. gradient checkpointing**\n\nIn Section 4.3, at first, the author emphasized that the reduction in memory usage was mainly because backward gradients were only calculated in the side branch, while backward propagation through the backbone branch was intentionally bypassed. Consequently, the cached activations were on the order of N × B × Ds instead of N × B × D. However, immediately on Line 279 the author stated that the gradients in backbone branch were still needed and computed using gradient checkpointing. In other words, the memory saving and the reduction in cached activations seems to be mainly achieved by gradient checkpointing and not because of the splitting of the side branch. This section together with Table 3 ablation study may cause quite some confusions and may need further clarifications. For example, some discussions to the following questions should be included:  \n\n- Assuming \"SSA only\" condition (row 2) in Table 3 refers to the case where the gradient computation in backbone branch was entirely skipped. This table should also include the condition in which the backbone gradients were computed and gradient checkpointing was not employed. This condition is expected to cache all the activations in both side branch and backbone branch, which should have similar memory consumption with reference methods. This would give readers a comparison with the SSA+GC condition (row 3) in Table 3.\n\n- If gradient checkpointing is applied to other reference methods, e.g. QA-LoRA, in a similar way, what would be the memory consumption?   \n\n- Since gradient checkpointing still needs to cache some activations in backbone, if Table 3 row 2 (SSA-only) does not enable the gradients computation in backbone branch, why would Row 3 (SSA+GC) have the same memory consumption as Row 2?\n\n\n**2. Q K layers**\n\nIn the proposed scheme, Q and K in the self-attention modules are not sliced like the other linear layers. It would be helpful for readers if the author added a few sentences in Section 4.2 explaining the reasons or challenges behind keeping Q and K unsliced.\n\n\n**3. Inference efficiency**\n\nAs shown in Table 5, memory footprint at inference stage is much smaller than fine-tuning stage. Therefore, even for a GPU with limited memory can afford a range of batch size choices. Batch size dependency is a very useful information when discussing the latency and inference efficiency. Author may want to consider including a few data points with different batch size for Table 5, or separate the inference part as a line plot with batch size on the x-axis."}, "questions": {"value": "please see Weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "20zZ7o6C17", "forum": "SYRtf6TQot", "replyto": "SYRtf6TQot", "signatures": ["ICLR.cc/2026/Conference/Submission6116/Reviewer_XqgE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6116/Reviewer_XqgE"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6116/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762922326075, "cdate": 1762922326075, "tmdate": 1762922326075, "mdate": 1762922326075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}