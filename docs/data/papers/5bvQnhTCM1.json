{"id": "5bvQnhTCM1", "number": 14674, "cdate": 1758241420822, "mdate": 1763283661489, "content": {"title": "AffoGato: Learning Open-Vocabulary Affordance Grounding with Foundation Models", "abstract": ".", "tldr": "We introduce AffoGato, an open-vocabulary affordance grounding framework with three stages: automatic generation of Affo-150K, pretraining Gato-3D/2D models on this data, and fine-tuning that demonstrates strong open-vocabulary capabilities.", "keywords": ["Affordance", "Open-Vocabulary", "Vision-Language Models", "Foundation Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/29f363be08cd42166a16dc8e6fce0255c6cc222c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework named AffoGato for open-vocabulary affordance grounding of both 3D point clouds and 2D images. In addition, this paper introduces a large-scale synthetic dataset Affo-150K, capturing diverse human-object interaction scenarios. It also proposes a data annotation pipeline that leverages foundational models to generate large-scale training data, eliminating the need for manual annotation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper establishes the AffoGato automated data generation pipeline and the resulting Affo-150K dataset, addressing the bottlenecks encountered in previous studies on existing datasets.\n* This paper unifies the typically separate tasks of 2D and 3D availability localization. The Gato model for both modalities shares a simple architectural principle and demonstrates strong generalization performance."}, "weaknesses": {"value": "* The “unseen” evaluation setup exhibits critical ambiguities: A major weakness is the lack of clear specifications regarding the unseen evaluation setting in LASO dataset (Table 2). The paper fails to clarify whether the object categories designated as unseen in the LASO benchmark are present in the Affo-150K train split. If overlap exists, then the performance gains reported by GATO pre-trained on Affo-150K do not measure true zero-shot generalization to novel categories. This ambiguity significantly undermines the interpretability and impact of the main results. \n* Insufficient generalization experiments: The paper primarily aims to address the open vocabulary problem, experiments are confined to the “unseen object” classification. Given the vast diversity of the proposed Affo-150K dataset, the failure to construct and evaluate more challenging settings, such as the “unseen affordance” classification constitutes a shortcoming. This omission deprives the paper of stronger evidence for genuine open vocabulary generalization. \n* Performance is not superior on the “Seen” setting: The untrained Gato-3D model performs worse than PointRefer on the “Seen” setting of LASO. This indicates that its architectural design itself holds no significant advantage, and the performance improvement on the Seen split is almost entirely attributable to the new pretraining dataset. Furthermore, it has not been compared against the superior methods GEAL[1] and GREAT[2], which were published at CVPR 2025.\n* Limited Method Innovation: The Gato model largely achieves unified 2D and 3D affordance grounding by adapting existing architectures with specific modality encoders. The primary innovation lies in the data generation process rather than the model architecture itself.\n\n[1] Lu D, Kong L, Huang T, et al. Geal: Generalizable 3d affordance learning with cross-modal consistency[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 1680-1690.\n\n[2] Shao Y, Zhai W, Yang Y, et al. Great: Geometry-intention collaborative inference for open-vocabulary 3d object affordance grounding[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 17326-17336."}, "questions": {"value": "See the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1vxcAttv1z", "forum": "5bvQnhTCM1", "replyto": "5bvQnhTCM1", "signatures": ["ICLR.cc/2026/Conference/Submission14674/Reviewer_4QKw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14674/Reviewer_4QKw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761387042803, "cdate": 1761387042803, "tmdate": 1762925045585, "mdate": 1762925045585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We thank all reviewers for the thoughtful feedback, constructive suggestions, and detailed comments on our submission. We appreciate the time and care each reviewer invested in evaluating the paper.\n \nAfter reflecting on the reviews, we have decided to withdraw the submission. Several points raised by the reviewers, especially those concerning dataset quality analysis and comparisons with more recent 3D affordance baselines, require deeper investigation and more extensive experimentation than what can reasonably be addressed during the rebuttal period.\n \nAt the same time, we remain confident in the importance of the core direction of this work: scaling up affordance grounding with large-scale, automatically generated supervision. This motivation is consistent with a growing body of research where synthetic or pseudo-labeled data produced by foundation models has enabled meaningful advances despite inevitable annotation noise. We believe that affordance grounding, in both 2D and 3D, will similarly benefit from scalable automatic annotation pipelines, and we plan to further refine our methodology, evaluation protocol, and analysis for future versions of the work."}}, "id": "QPNpZLshNN", "forum": "5bvQnhTCM1", "replyto": "5bvQnhTCM1", "signatures": ["ICLR.cc/2026/Conference/Submission14674/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14674/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763283660729, "cdate": 1763283660729, "tmdate": 1763283660729, "mdate": 1763283660729, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AFFOGATO, a unified framework for 2D and 3D open-vocabulary affordance grounding. It automatically constructs a large-scale affordance training dataset by leveraging foundation models to generate affordance-related queries and corresponding spatial locations. Based on this process, the authors build Affo-150K, a synthetic dataset designed to advance affordance grounding beyond predefined affordance and object categories. The AFFOGATO architecture integrates a modality-specific visual encoder, a text encoder, and a text-conditioned heatmap decoder. Extensive experiments show that the proposed framework achieves promising results across both 2D and 3D affordance grounding tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The release of an open-vocabulary affordance dataset may be interested to the community.\n2. The paper is well-organized and easy to understand.\n3. Experiments on both 2d and 3d affordance detection are conducted."}, "weaknesses": {"value": "1. The architecture is relatively classical, lacking technical novelty.\n2. Although the work leverages large-scale automatic data generation effectively, it does not sufficiently examine the inherent annotation noise (which cannot be simply addressed by Error mitigation) or its implications for the training process.\n3. The comparison on Table 2 shows that the improvement seems marginal. And, some 3D affordance methods published in 2024 or 2025 in the related work are not compared."}, "questions": {"value": "1. How is quality of the generated affordance data?\n2. How does the proposed method compared to the advanced methods proposed in 2024 or 2025 (Table 2 only contains PointRefer)?\n3. The CLIP encoder has limited ability to understand very complex open-vocabulary affordance queries. How does integrating large language models performs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VIQLULd1to", "forum": "5bvQnhTCM1", "replyto": "5bvQnhTCM1", "signatures": ["ICLR.cc/2026/Conference/Submission14674/Reviewer_FYsz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14674/Reviewer_FYsz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729545815, "cdate": 1761729545815, "tmdate": 1762925045182, "mdate": 1762925045182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a unified framework for 2D and 3D affordance grounding that leverages large foundation models to automatically generate training data, eliminating manual annotation. It also presents Affo-150K, the largest synthetic affordance dataset to date, containing 150K 3D objects and corresponding textual affordance annotations. The proposed Gato-3D and Gato-2D models achieve state-of-the-art results across benchmarks, showing strong generalization to unseen object categories"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper leverages multiple foundation models (Gemma, Molmo, MobileSAM) for affordance annotation, which offers a scalable solution to affordance data annotation.\n\n- A large-scale and diverse dataset (Affo-150K) is created whcih enables open-vocabulary affordance grounding across 3D and 2D domains."}, "weaknesses": {"value": "- Reliance on pre-trained foundation models for data annotation could introduce noise and the mitigation plans are not clearly introduced in the manuscript. For example, using SAM to segment affordance region  from multipe 2D renderings does not guarantee consistency across all views. Moreover, the affordance region may not be visible in all views. An additional step of fusion across all rendered 2D views is necessary, however, is not sufficiently discussed.\n\n- Since dataset is fully annotated by multiple foundation models, it raises the question of whether such a dataset is necessary for evaluating affordance prediction in the future. Training model using the proposed dataset is more like distilling from multiple foundation models. Then, why don't we directly use the multiple foundation for inference directly?\n\n- The proposed task is still akin to segmentation, following the existing definition over affordance prediction. However, affordance reasoning should move beyond spatial localization toward a more comprehensive understanding of interactions. The new challenges and opportunities are not addressed in this dataset."}, "questions": {"value": "- Please clarify the details of strategies resolving inconsistency predictions made by SAM on multiple 2D views.\n\n- Justify the need for training/distilling a separate model from the annotations generated by existing foundation models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hDSHEHdvAg", "forum": "5bvQnhTCM1", "replyto": "5bvQnhTCM1", "signatures": ["ICLR.cc/2026/Conference/Submission14674/Reviewer_oQtu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14674/Reviewer_oQtu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959752222, "cdate": 1761959752222, "tmdate": 1762925044490, "mdate": 1762925044490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AffoGato tackles open-vocabulary affordance grounding in both 3D and 2D, taking a 3D object or image plus a natural-language query and outputting a spatial affordance heatmap. It builds an automated pipeline that uses foundation models (Gemma, Molmo) to generate interaction queries, point cues, and multi-view masks and fuses them into 3D heatmaps, yielding the 150K-object Affo-150K dataset (based on 4 subsets of Objaverse). It introduces simple Gato-3D and Gato-2D models that couple pretrained part-aware vision encoders with a text-conditioned heatmap decoder for cross-modal localization. Pretraining on Affo-150K strengthens open-vocabulary generalization and boosts performance when fine-tuned on existing benchmarks. Experiments report state-of-the-art results on LASO and AGD20K, with notable gains on unseen categories and consistent improvements in standard metrics. Overall, the paper shows a unified route to affordance grounding by combining automated 3D supervision with lightweight text-conditioned decoders."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Large-scale, Objaverse-based dataset (Affo-150K) with automated multi-view supervision; broader coverage than prior affordance sets.\n\n- Unified 3D+2D formulation with open-vocabulary queries; pretraining transfers well and improves downstream benchmarks.\n\n- Practical pipeline choices (e.g., PartField, MobileSAM + VLM prompts) produce usable volumetric and image-space heatmaps with minimal manual labeling.\n\n- Clear presentation and tooling (viewer/demos). Open source."}, "weaknesses": {"value": "- The annotation pipeline is close in spirit to prior part-centric auto-labeling (e.g., PartSLIP-style), so novelty is more incremental than conceptual.\n\n- The annotation methodology relies on high-quality 3D datasets, which are more limited in size, variety, and background than 2D data; thus, it is difficult to extend.\n\n- Architectural contribution is intentionally minimal; no ablation studies on GATO. Limited analysis of when the simple decoder fails (small parts, occlusions, cluttered scenes). More examples can be added to B.2."}, "questions": {"value": "- Ambiguity & multi-part affordances\n    \n    When a query legitimately maps to multiple parts, how consistent is the annotation pipeline across views, seeds, and objects? For the model side, does the decoder reliably produce multi-peak heatmaps when appropriate? \n    \n- Out-of-distribution generalization\n    \n    Please show zero-shot and fine-tuned results on OOD objects and queries that are rare/absent in Affo-150K (e.g., novel categories, uncommon affordances, synonyms). Clarify where fine-tuning helps most (which affordance types/part sizes) versus failure cases that reveal dataset bias."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "X7RM2IPcrP", "forum": "5bvQnhTCM1", "replyto": "5bvQnhTCM1", "signatures": ["ICLR.cc/2026/Conference/Submission14674/Reviewer_77gK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14674/Reviewer_77gK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762261504850, "cdate": 1762261504850, "tmdate": 1762925043848, "mdate": 1762925043848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}