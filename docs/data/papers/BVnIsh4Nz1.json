{"id": "BVnIsh4Nz1", "number": 7875, "cdate": 1758040383268, "mdate": 1763709613201, "content": {"title": "On the Reasoning Abilities of Masked Diffusion Language Models", "abstract": "Masked diffusion models (MDMs) for text offer a compelling alternative to traditional autoregressive language models. Parallel generation makes them efficient, but their computational capabilities and the limitations inherent to their parallelism remain largely unexplored. To this end, we characterize what types of reasoning problems MDMs can provably solve and how efficiently. We do this by connecting MDMs to the well-understood reasoning frameworks of chain of thought (CoT) and padded looped transformers (PLTs) in the finite-precision log-width setting: We show that MDMs and polynomially-padded PLTs are, in fact, equivalent in this setting, and that MDMs can solve all problems that CoT-augmented transformers can. Moreover, we showcase classes of problems (including regular languages) for which MDMs are inherently more efficient than CoT transformers, where parallel generation allows for substantially faster reasoning.", "tldr": "We prove that masked text diffusion models are equivalent to padded looped transformers, can solve all problems that chain-of-thought transformers can, and are more efficient on certain problem classes due to their parallel generation mechanism.", "keywords": ["diffusion language models", "formal language theory", "boolean circuits", "expressivity", "transformers", "masked diffusion models", "chain of thought", "looped transformers"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2db8937ba155e0f415bcf6363e9e4ff9fa16ee07.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper analyzes the computational capability of MDMs, which is composed of a planner and a predictor, both implemented by transformers. One core finding is the equivalence between MDMs and PLTs under the discussed setting. Based on this equivalence, this paper characterizes the computational capability of MDMs and provides a comparison between MDMs and CoT Transformers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and provides a detailed discussion of the problem setting, related work, and its theoretical assumptions.\n2. The target question, the computational capability of MDMs, is practically relevant and important.\n3. The proposed \"planner-predictor\" formulation for MDMs is reasonable.\n4. The approach of linking MDMs to the PLTs is natural and allows the use of well-studied conclusions about PLTs."}, "weaknesses": {"value": "1. Positional encodings. The main theoretical results appear to rely on very strong assumptions about the positional encodings. As discussed in Appendix D.1, the PEs are constructed to carry complex algorithmic information (e.g., the results of division and modulo operations). This assumption seems to offload a large part of the required computation from the Transformer's computation mechanism onto the input representation.\n2. Idealized planner.  In the proofs, the planner is a Transformer designed to perfectly execute a specific algorithm. It is unclear if the practical confidence-based unmasking planner could replicate this perfect capacity.\n3. Empirical study. While the work is theoretical, the results would be strengthened by even simple empirical studies to see the practical performance of the predicted capabilities (e.g., the efficiency of MDMs on parallelizable tasks versus CoT)."}, "questions": {"value": "1. Positional encodings. How much of the computational capability attributed to MDMs (e.g., in Theorem 3.2) is actually due to the pre-computed information in the PEs rather than the Transformer’s computation mechanism? This should be more clearly discussed.\n2. Idealized planner. How could practical planners approximate the performance of the idealized planners? A more detailed explanation or empirical investigation of this gap would strengthen the paper's real-world implications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hvQ8vQKbxf", "forum": "BVnIsh4Nz1", "replyto": "BVnIsh4Nz1", "signatures": ["ICLR.cc/2026/Conference/Submission7875/Reviewer_rJ1T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7875/Reviewer_rJ1T"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761452408813, "cdate": 1761452408813, "tmdate": 1762919908850, "mdate": 1762919908850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the reasoning capabilities of MDMs from the standard complexity theory perspective. The main message of this paper is that MDMs excel on highly parallelizable or ambiguous-generation tasks, achieving strong results with few denoising steps and easy steering via subtask learning. The authors also provide rigorous theoretical statements to support this claim."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper works on a topic that previous work didn't explore at all: theoretically exploring the MDM's reasoning ability. Although the community believes that MDM can achieve better performance than a causal model in tasks that require non-causal thinking, formalizing that intuition would've been needed for the community. Last but not least, the paper is clearly well-written, even people who aren't familiar with complexity theory can easily follow."}, "weaknesses": {"value": "This paper doesn't provide any empirical results, which is not actually in the scope, though. Moreover, although the introduction is clearly well-written, Figure 1 gives too much information and is even a bit hard to follow. Also (as far as I understand), this paper's theoretical analysis is based on a remasking scheme (where we remask the unmasked tokens again), which people don't really use in their large-scale Masked Diffusions. Although the authors provide two MDM references that use this scheme, my thought is that the remasking strategy is actually not the mainstream sampling approach, at least by far."}, "questions": {"value": "- How does the theory result affect under without remasking strategy?\n- Are there (at least some prior work's) empirical results that can support the author's claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uj8KmJVGph", "forum": "BVnIsh4Nz1", "replyto": "BVnIsh4Nz1", "signatures": ["ICLR.cc/2026/Conference/Submission7875/Reviewer_ytmY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7875/Reviewer_ytmY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878954255, "cdate": 1761878954255, "tmdate": 1762919908401, "mdate": 1762919908401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers [Part 1]"}, "comment": {"value": "We thank all reviewers for their thoughtful feedback and for recognizing the novelty and usefulness of our theoretical framework. Several common questions emerged across the reviews, which we address here before responding to individual concerns.\n\n## On Positional Encodings\n\nThe PEs we use indeed contain basic arithmetic information (like division and modulo results) that finite-precision transformers cannot compute on their own. This \"offloading\" of computation to the PEs is a standard approach in the expressivity literature on fixed-precision transformers \\[1, 2, 3, 4\\]. In fact, PEs in prior analyses often encode the entire structure of a circuit to be simulated (to prove some expressivity lower bounds). In contrast, our PEs do not depend on a specific circuit to be simulated, but are rather simple functions of the input position $n$ and input length $N$. We adopt this model to allow for a fair comparison with existing results on finite-precision CoT and PLTs—in a sense, any overstatement of capability applies equally to all models studied in this line of research.\n\nThe overall computational capability in the L-uniform transformer family is deliberately split between the PEs and the transformer's computation mechanism. The expressivity of any fixed-precision transformer is limited (to a subset of $AC^0$). This means such models cannot compute functions like division, modulo, or even binary representations of integers on their own \\[5\\]. The PEs provide the necessary information that the $AC^0$ mechanism lacks. As we detail in App. D.1, this includes essential data like the binary representations of position/length and the results of arithmetic operations, which are known to be outside $AC^0$.\n\nTherefore, the transformer mechanism in Theorem 3.2 cannot compute these values itself. It must look up this pre-computed (but still simple, log-space) information from the PEs. Our constructions do not go through with a fixed-precision transformer mechanism alone. In particular, an issue arises when *chaining decoding steps*, as modulo and division information is needed for the correct indexing of the decoded information—hence, one could see the general L-uniform PEs as a prerequisite for \"looping\" to increase computational power compared to a fixed-depth model.\n\nWe have moved the main takeaways from App. D.1 into the discussion section of the main paper (in red) to emphasize this more clearly.\n\n## On Empirical Validation\n\nWe first note that our analysis is motivated by existing empirical results that show MDMs outperforming autoregressive models on tasks that allow for parallel reasoning. A classic example is Sudoku, where MDMs with parallel decoding are known to outperform autoregressive models \\[6, 7\\]. Moreover, \\[8\\] find that MDMs surpass auto-regressive models in complex reasoning and long-term planning tasks, such as Countdown and Sudoku. In contrast, \\[9\\] designs a benchmark specifically to test parallelizable reasoning. The authors find that MDMs perform worse than autoregressive models on non-parallelizable tasks.\n\nNonetheless, we agree that further empirical validation would be an interesting addition. In fact, we have run a set of preliminary experiments to investigate this. Our initial findings were not yet conclusive, with the main issue being that MDMs are more difficult to train than CoT models. The lack of literature on training MDMs on formal languages means that there is no standard recipe, which left their performance suboptimal; while some results aligned with our theory (such as comparable performance with exponentially fewer steps on tasks like state tracking), other aspects were not clear-cut.\n\nWe feel these initial experiments are thus not yet comprehensive enough to include. Designing proper empirical tests to isolate the specific theoretical properties we identify would certainly be valuable, but it is quite non-trivial. We believe the theoretical contributions of this work are substantial enough to stand on their own and to motivate and inform a further systematic empirical study.\n\nTo make this point clearer in the paper, we have expanded the discussion section (marked in red) to discuss this as an important direction for follow-up research."}}, "id": "0HXDcJ0yS0", "forum": "BVnIsh4Nz1", "replyto": "BVnIsh4Nz1", "signatures": ["ICLR.cc/2026/Conference/Submission7875/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7875/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7875/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763626343602, "cdate": 1763626343602, "tmdate": 1763626343602, "mdate": 1763626343602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a formal theoretical analysis of the reasoning capabilities of Masked Diffusion Models (MDMs) for language generation. It establishes equivalence between MDMs and Padded Looped Transformers (PLTs) under finite-precision, log-width settings, and compares their expressivity to Chain-of-Thought (CoT) transformers. The authors show that MDMs can simulate CoT reasoning (with some overhead), and are provably more efficient on parallelizable problems. They also identify a \"sequentiality bottleneck\" in CoT transformers, which MDMs can overcome due to their parallel nature. The paper concludes that MDMs are better suited for parallelizable reasoning tasks, while CoT is more efficient for inherently sequential ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides formal proofs and complexity-theoretic characterizations of MDMs, grounding their reasoning capabilities in well-understood models like PLTs and CoT transformers.\n2. It introduces the idea of a \"sequentiality bottleneck\" in CoT and shows how MDMs can leverage parallelism, offering a clear separation in expressive efficiency for certain problem classes (e.g., regular languages, NC1).\n3. The idealized MDM model is motivated by practical implementations, and the authors show how their theoretical framework aligns with real-world MDM behaviors, such as confidence-based unmasking and resampling."}, "weaknesses": {"value": "1. The paper is purely theoretical and lacks experimental validation of the claims. While theoretical depth is valuable, even synthetic experiments could help illustrate the practical implications of the findings.\n2. The assumptions may be too strong. The analysis relies on idealized assumptions (e.g., finite-precision, log-width transformers, perfect planners/predictors), which may not reflect real-world limitations of MDMs or PLTs.\n3. The reasoning tasks considered (e.g., regular languages, circuit complexity classes) are formal and abstract. It’s unclear how the results translate to more complex, open-domain reasoning tasks commonly faced by LLMs."}, "questions": {"value": "1. Do you plan to validate your theoretical findings empirically? For example, can you design controlled experiments to show that MDMs outperform CoT on parallelizable tasks like expression evaluation or state tracking?\n2. How do your assumptions affect the realism of the model? What are the implications of relaxing assumptions like perfect approximation or uniform unmasking? How would your results change under noisy or learned planners?\n3. Can your framework be extended to other reasoning paradigms? For instance, could similar analyses be applied to latent diffusion models, state-space models, or hybrid autoregressive-diffusion architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SEX99UDl9U", "forum": "BVnIsh4Nz1", "replyto": "BVnIsh4Nz1", "signatures": ["ICLR.cc/2026/Conference/Submission7875/Reviewer_vzeH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7875/Reviewer_vzeH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088846817, "cdate": 1762088846817, "tmdate": 1762919908119, "mdate": 1762919908119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers [Part 2]"}, "comment": {"value": "## On Idealized Assumptions\n\nTo tractably analyze MDMs, we had to make some modeling assumptions (e.g., finite-precision, perfect planners). We emphasize that these assumptions are standard in the theoretical analysis of language models \\[1, 2, 3, 4\\] and thus provide a \"common vocabulary\" for comparing the expressivity of different architectures like MDMs, PLTs, and CoT.\n\n**On the Idealized Planner:** Our constructive proofs (for lower bounds) indeed rely on a planner that perfectly executes an algorithm, an idealization of confidence-based planners. How a trained confidence-based planner can learn to approximate this ideal behavior is an important and more complex question—one of learnability rather than pure expressivity. As is standard in expressivity analysis of language models, expressivity is a necessary first step before one can analyze how to train a planner to represent such functions. While learnability of formal languages is an active area of research, we are not aware of any existing results that would characterize the learnability of specific functions required for planning (e.g., modulo and division) apart from the known issue of the sensitivity of modular counting \\[10\\], which indeed makes learning modulo difficult for transformers.\n\nWe will add a paragraph to the discussion section (in red) to explicitly discuss this gap.\n\n---\n\n\\[1\\] Li et al. Chain of thought empowers transformers to solve inherently serial problems.  \n\\[2\\] Saunshi et al. Reasoning with latent thoughts: On the power of looped transformers.  \n\\[3\\] London and Kanade. Pause tokens strictly increase the expressivity of constant-depth transformers.  \n\\[4\\] Svete et al. Exact expressive power of finite-precision looped transformers.  \n\\[5\\] Merrill and Sabharwal. A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers.  \n\\[6\\] Kim et al. Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions.  \n\\[7\\] Horvitz et al. No Compute Left Behind: Rethinking Reasoning and Sampling with Masked Diffusion Models.  \n\\[8\\] Ye et al. Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning.  \n\\[9\\] Kang et al. ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs.  \n\\[10\\] Hahn and Rofin. Why are Sensitive Functions Hard for Transformers?"}}, "id": "zjtNvhPhMN", "forum": "BVnIsh4Nz1", "replyto": "BVnIsh4Nz1", "signatures": ["ICLR.cc/2026/Conference/Submission7875/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7875/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7875/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763626368024, "cdate": 1763626368024, "tmdate": 1763626368024, "mdate": 1763626368024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a formal analysis of the reasoning and computational capabilities of MDM. Under a finite-precision, logarithmic-width transformer setting, the authors prove that MDMs are theoretically equivalent to PLTs and can simulate CoT reasoning. They further show that MDMs are provably more efficient only for parallelizable problems (e.g., regular languages), where parallel denoising enables faster reasoning, while for inherently sequential tasks (e.g., P-complete problems) MDMs offer no efficiency gain and may even be less practical due to architectural overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper offers a novel and rigorous theoretical framework for analyzing the reasoning capability of masked diffusion models, a topic that has been largely unexplored.\n\n2. It provides clear conceptual connections between MDMs, chain-of-thought transformers, and padded looped transformers, helping to unify different reasoning paradigms under one formulation.\n\n3. The analysis yields meaningful theoretical insights into when MDMs can achieve efficiency gains through parallelism, offering guidance for future research on diffusion-based reasoning models."}, "weaknesses": {"value": "1. The theoretical framework relies on strong assumptions about positional encodings, which are constructed to include arithmetic information (e.g., division and modulo) that real transformers cannot compute, potentially overstating MDMs’ practical capability.\n\n2. The analysis is purely theoretical, without even minimal empirical validation or illustrative experiments to verify whether the predicted efficiency gains appear in practice."}, "questions": {"value": "1. The paper makes strong assumptions about positional encodings, requiring them to include arithmetic information such as division and modulo. Do widely used schemes like RoPE [1] satisfy these assumptions?\n\n2. The authors argue that for P-complete problems, MDMs cannot benefit from parallelism and that the absence of KV-cache makes autoregressive models preferable. Would this conclusion still hold if we consider recent MDM variants that incorporate KV-cache [2]?\n\n[1] Su et al. RoFormer: Enhanced Transformer with Rotary Position Embedding.\n\n[2] Wu et al. Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2O4UPCJTlo", "forum": "BVnIsh4Nz1", "replyto": "BVnIsh4Nz1", "signatures": ["ICLR.cc/2026/Conference/Submission7875/Reviewer_nitu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7875/Reviewer_nitu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762091688039, "cdate": 1762091688039, "tmdate": 1762919907784, "mdate": 1762919907784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}