{"id": "80552WurUJ", "number": 12276, "cdate": 1758206790083, "mdate": 1759897520723, "content": {"title": "Training A Foundation Model to Represent Graphs as Vectors", "abstract": "This paper aims to train a graph foundation model that is able to represent any graph as a vector preserving structural and semantic information useful for downstream graph-level tasks such as graph classification and graph clustering. To learn the features of graphs from diverse domains while maintaining strong generalization ability to new domains, we propose a multi-graph-based feature alignment method, which constructs weighted graphs using the attributes of all nodes in each dataset and then generates consistent node embeddings. To enhance the consistency of the features from different datasets, we propose a density maximization based mean alignment method. The original graphs and generated node embeddings are fed into a graph neural network to achieve discriminative graph representations in contrastive learning. More importantly, to enhance the information preservation from node-level representations to the graph-level representation, we construct a reference distribution module without using any pooling operation. The experimental results of zero-shot/few-shot graph classification and graph clustering show that our model outperforms strong competitors.", "tldr": "", "keywords": ["graph neural network", "graph classification"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/923fddb21b8559f7d182184a952c000909543792.pdf", "supplementary_material": "/attachment/f44dcc028e2187f626d746080146953fd2bea787.zip"}, "replies": [{"content": {"summary": {"value": "The authors a Graph Foundation Model (GFM) designed to learn universal graph-level representations to preserve structural and semantic information. The key objective is to train a model that can embed any graph into a vector representation suitable for downstream tasks, e.g., graph classification, clustering, few-shot, and zero-shot tasks.\n\nThis consists of three main components: (1) Multi-graph Feature Alignment (Weighted graphs are introduced based on node attributes across datasets to generate consistent node embeddings that align features from diverse graph sources. (2) Density Maximization Mean Alignment (DMMA) to enhance feature consistency across datasets, with convergence guarantees. (3) Reference Distribution Module to preserve information flow from node to graph-level representations by aligning to a reference distribution.\nA theoretical generalization bound is provided to justify the approach’s transferability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I like the idea of building a domain-general graph representation model, aligning with the recent foundation models in non-Euclidean domains. The multi-graph-based feature alignment and DMMA algorithm offer an interesting approach to unifying representations across heterogeneous datasets.\n\n- A theoretical depth is introduced\n\n- It is practical in few-shot and zero-shot settings"}, "weaknesses": {"value": "- The overall pipeline (alignment + contrastive learning) resembles existing multi-domain alignment and contrastive graph representation frameworks; the incremental novelty may be limited without deep architectural or conceptual differences.\n\n- It is unclear how the model scales with the number or size of graphs, given the need to construct and align across multiple weighted graphs simultaneously.\n\n- The graph structure preservation problem: It is unclear whether the proposed approach surpasses or is equivalent to 1-WL expressivity."}, "questions": {"value": "- How does the proposed model’s expressive power compare to that of 1-WL or higher-order WL GNNs? Does the alignment mechanism improve the ability to distinguish non-isomorphic graphs? Does the alignment mechanism improve the ability to distinguish non-isomorphic graphs?\n\n- In a cross-domain setup, does the proposed approach compare with baselines, e.g., contrastive learning methods?\n\n- How does the model address or exploit the trade-off between global graph structure preservation and diverse feature preservation?\n\n- Could the authors clarify how the multi-graph feature alignment handles heterogeneous node attributes (e.g., categorical vs. continuous features) across datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RyyVzKEgZn", "forum": "80552WurUJ", "replyto": "80552WurUJ", "signatures": ["ICLR.cc/2026/Conference/Submission12276/Reviewer_auzx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12276/Reviewer_auzx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760783356741, "cdate": 1760783356741, "tmdate": 1762923212828, "mdate": 1762923212828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GraphVec-FM, a language model–free graph foundation model (GFM) designed to encode entire graphs into fixed-dimensional vectors for graph-level downstream tasks such as classification and clustering. To address challenges like attribute inconsistency across domains, missing node features, and information loss in pooling, the authors introduce (1) a multi-graph–based feature alignment strategy using Gaussian kernels and SVD, (2) a theoretically grounded density-maximization mean alignment algorithm to resolve sign ambiguity and align cross-domain embeddings, and (3) a reference distribution module that replaces conventional pooling with MMD-based similarity to virtual reference graphs. The method is supported by a generalization error bound and evaluated on few-shot, zero-shot, and clustering tasks, showing consistent improvements over strong baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "S1. Strong theoretical grounding: The paper provides convergence guarantees for the mean alignment algorithm (Theorem 4.1) and a non-trivial generalization error bound (Theorem 4.2), which enhances the methodological rigor.\n\nS2. Code availability: The authors commit to open-sourcing the implementation, which will benefit reproducibility."}, "weaknesses": {"value": "W1. Limited evidence for cross-domain generalization:\nWhile the paper correctly identifies that “graph patterns from different domains exhibit significant variation,” the pretraining uses only five biochemical datasets (ENZYMES, DD, NCI1, etc.), which are structurally and semantically similar. The downstream evaluation on social/computer vision graphs is impressive, but the paper lacks analysis to explain why this transfer works. For instance, are the aligned embeddings truly domain-agnostic?\n\nW2. Overstated claim about graph-level task difficulty:\nThe assertion that “graph-level tasks are often more challenging” (line 85) is unsubstantiated and potentially misleading. Node-level tasks are equally critical and nontrivial. More importantly, a truly general GFM should support both node- and graph-level tasks. Restricting scope to graph-level tasks limits the model’s applicability and appeal.\n\nW3. Insufficient experimental details in figures:\nFigure 4 (Appendix H.3), which shows performance vs. number of pretraining datasets, does not specify the testing dataset used (ENZYMES, DD, NCI1, NCI109, or Mutagencity?). Without this, the result is hard to interpret.\n\nW4. Several minor typos appear throughout the manuscript: for example, Line 147: “atrribute matrix” → “attribute matrix”"}, "questions": {"value": "Q1. Cross-domain alignment mechanism:\nHow does the proposed feature alignment ensure that embeddings from biochemically pretrained graphs remain meaningful for social networks, where topology (e.g., power-law vs. small-world) and semantics differ drastically?\n\nQ2. Extensibility to node-level tasks:\nIs the current framework compatible with node-level tasks (e.g., node classification)? If not, what architectural changes would be needed to support a unified GFM for both graph- and node-level predictions? Could you show the results of GraphVec-FM under the task of node classification?\n\nQ3. Clarification of Figure 4:\nOn which downstream dataset was Figure 4 (Appendix H.3) evaluated? Please specify in the caption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5c93Arqwzv", "forum": "80552WurUJ", "replyto": "80552WurUJ", "signatures": ["ICLR.cc/2026/Conference/Submission12276/Reviewer_J9XU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12276/Reviewer_J9XU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761166257497, "cdate": 1761166257497, "tmdate": 1762923212205, "mdate": 1762923212205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GraphVec-FM, a foundation model for graph-level representation learning across domains. The model employs multi-graph construction for feature alignment, a density maximization mean alignment algorithm, and a reference distribution module to generate graph embeddings. Experiments on few-shot/zero-shot classification and clustering show improvements over several baselines. However, critical issues regarding scalability, incomplete ablation studies, unexplained experimental results, and insufficient baseline comparisons lead me to feel disappointed with this paper."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. Easy to follow.\n\nS2. Principled Theoretical Framework with Convergence Guarantees.\n\nS3. Comprehensive Cross-Domain Evaluation Demonstrating Transfer Capability."}, "weaknesses": {"value": "**W1. Critical Scalability Issues Undermining Foundation Model Viability.** Table 11 in Appendix H.7 reveals a severe memory bottleneck that fundamentally questions the model's applicability as a foundation model: the method requires 46.42 GB RAM on a dataset of only 9,629 graphs (deezer ego net), compared to 1.1 GB for ProNoG [1], representing a 42-fold increase in memory consumption. While the authors acknowledge this overhead stems from global graph construction and storage, they propose the Nyström approximation as a remedy but provide no empirical validation of its effectiveness in terms of accuracy versus efficiency trade-offs. For a paper claiming to present a \"foundation model,\" the absence of scalability analysis beyond 10K graphs is a critical omission. Modern foundation models are expected to scale to millions of instances; without evidence that the proposed approach can handle 100K+ graphs with acceptable memory footprint, the practical utility of this method remains unsubstantiated. Furthermore, the paper lacks inference time benchmarks, which are crucial for deployment scenarios, and provides no comparison of memory-efficient alternatives such as mini-batch global graph construction or online SVD updates.\n\n**W2. Incomplete Ablation Studies Failing to Isolate Modular Contributions.** The paper claims three core methodological contributions (multi-graph construction in Section 4.1, density maximization mean alignment in Section 4.2, and reference distribution module in Section 4.4), but the ablation studies do not adequately separate their individual impacts. Table 1 presents only \"w/o mean alignment,\" showing modest improvements of 1 to 2 percentage points, while Table 9 (Appendix H.5) examines \"w/o reference distribution\" and reveals inconsistent benefits: on REDDIT-BINARY, mean readout achieves identical accuracy (77.79%) to the full model, and on multiple datasets the improvement is marginal (0.82% on COIL-RAG, 1.10% on Letter-med). Critically absent is an ablation comparing single global graph (Q=1) versus multi-graph construction (Q=6), which is purportedly a key innovation. Without systematic ablation across all combinations of these three components, it is impossible to determine which elements are essential and which are redundant, undermining the scientific rigor of the contribution claims.\n\n**W3. Unexplained Contradictory Results Regarding Unsupervised Pretraining.** Table 2 presents a puzzling phenomenon that the authors fail to address: unsupervised pretraining outperforms supervised pretraining on 2 out of 7 datasets (Cuneiform: 50.09% vs 45.32%; IMDB-M: 47.70% vs 46.76%), contradicting the expected behavior that label supervision should improve task-relevant representations. The authors provide no analysis of why removing supervision improves performance in these cases, leaving several plausible explanations unexamined: (1) overfitting to source domain labels that do not transfer well, (2) label noise in the pretraining datasets, (3) superior augmentation diversity in the unsupervised contrastive objective (Eq. 16) compared to supervised contrastive loss (Eq. 15), or (4) fundamental issues with the supervised loss formulation. This lack of analysis raises concerns about whether the supervised pretraining is properly tuned and whether the experimental conclusions are reliable, as the inconsistency suggests potential methodological issues in the training protocol.\n\n**W4. Insufficient Comparisons with Recent Graph Foundation Models.** The experimental comparisons in Tables include several methods spanning from 2020 to 2025, including prompt-tuning approaches (EdgePrompt [2], GPF [3], GraphPrompt [4]) and contrastive pretraining methods (GraphCL [5], SimGRACE [6]). However, several directly relevant graph foundation models that explicitly target cross-domain pretraining are notably absent from the comparison. GraphFM [7] is cited in Related Work (Section 2) and is explicitly designed for scalable multi-graph pretraining across heterogeneous domains, making it a directly comparable baseline, yet no experimental comparison is provided. Similarly, GraphAny [8] presents a foundation model for node classification that the authors acknowledge can be adapted to graph-level tasks (as mentioned in Section 2), but it is not included in the benchmarks. Without comparisons to these recent foundation models that address the same core challenge of cross-domain generalization, it is difficult to assess whether the proposed method's improvements stem from the novel technical contributions (multi-graph construction, mean alignment, reference distribution) or merely from differences in model capacity, pretraining data, or hyperparameter tuning. Additionally, while the authors critique LLM-based GFMs in Section 2 for information loss and computational cost, they provide no empirical evidence supporting these claims; a direct comparison on at least one dataset, or at minimum a detailed discussion of computational requirements and accuracy trade-offs, would strengthen the positioning of the work.\n\n**W5. Inadequate Treatment and Analysis of Attribute-Free Graphs.** The paper's approach to graphs without node attributes, using truncated SVD of the self-looped adjacency matrix A+I (Eq. 6), is presented as a straightforward solution, but its impact on cross-domain generalization is not rigorously evaluated. This method generates features based purely on graph topology, which may not carry semantic information comparable to domain-specific attributes in biochemical graphs (chemical properties, bond types) or computer vision graphs (pixel features, spatial coordinates). The strong performance on REDDIT-BINARY (77.79%, Table 2) suggests the approach works in some cases, but the paper provides no controlled experiment isolating the effect of Eq. 6 versus alternative structural encodings (e.g., node centrality, graphlet counts, positional encodings). More critically, there is no analysis of how the semantic mismatch between topology-derived features and rich domain attributes affects transfer learning; for instance, when pretraining on attribute-rich molecular graphs and transferring to attribute-free social networks. This gap undermines the claim that the model achieves \"domain-agnostic\" representations, as the feature generation process itself introduces domain-specific biases that are not characterized.\n\n**W6. Overclaimed Theoretical Contributions with Limited Novelty and Validation.** Theorem 4.2 provides a generalization bound for the proposed model, but the result offers limited insight beyond standard learning theory. The bound scales as O(1/√(MN)) via Rademacher complexity analysis, which is a well-established technique, and depends critically on β = ||Z̃||_F, a data-dependent quantity that is neither controlled by the model design nor analyzed empirically. The proof in Appendix E relies heavily on existing tools (McDiarmid's inequality, Dudley entropy integral, Bartlett's covering number results) without introducing novel proof techniques. Most problematically, the theorem is not validated empirically: Figure 4 shows accuracy versus number of pretraining datasets, but does not plot the theoretical bound for comparison, leaving it unclear whether the bound is tight or merely a loose upper limit. Similarly, Theorem 4.1 guarantees convergence of Algorithm 1 but provides no convergence rate, no characterization of solution quality (global vs. local optimum), and no analysis of sensitivity to initialization, all of which are important for practitioners considering adoption of the method.\n\n\n\n[1] Yu, X., Gong, Z., Zhou, C., Fang, Y., & Zhang, H. (2025). SamGPT: Text-free graph foundation model for multi-domain pre-training and cross-domain adaptation. In Proceedings of the ACM Web Conference 2025, pp. 1142-1153.\n\n[2] Fu, X., He, Y., & Li, J. (2025). Edge prompt tuning for graph neural networks. In The Thirteenth International Conference on Learning Representations.\n\n[3] Fang, T., Zhang, Y., Yang, Y., Wang, C., & Chen, L. (2023). Universal prompt tuning for graph neural networks. Advances in Neural Information Processing Systems, 36, 52464-52489.\n\n[4] Liu, Z., Yu, X., Fang, Y., & Zhang, X. (2023). GraphPrompt: Unifying pre-training and downstream tasks for graph neural networks. In Proceedings of the ACM Web Conference 2023, pp. 417-428.\n\n[5] You, Y., Chen, T., Sui, Y., Chen, T., Wang, Z., & Shen, Y. (2020). Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33, 5812-5823.\n\n[6] Xia, J., Wu, L., Chen, J., Hu, B., & Li, S. Z. (2022). SimGRACE: A simple framework for graph contrastive learning without data augmentation. In Proceedings of the ACM Web Conference 2022, pp. 1070-1079.\n\n[7] Lachi, D., Azabou, M., Arora, V., & Dyer, E. (2024). GraphFM: A scalable framework for multi-graph pretraining. arXiv preprint arXiv:2407.11907.\n\n[8] Zhao, J., Mostafa, H., Galkin, M., Bronstein, M., Zhu, Z., & Tang, J. (2024). GraphAny: A foundation model for node classification on any graph. arXiv preprint arXiv:2405.20445."}, "questions": {"value": "**Q1.** How does the model's memory consumption and runtime scale to datasets with 100,000+ graphs, which are common in real-world foundation model applications, and what is the empirical accuracy versus efficiency trade-off when using the proposed Nyström approximation [1]?\n\n**Q2.** Can you provide full ablation results systematically comparing all combinations of the three claimed contributions (single vs. multi-graph construction, with/without mean alignment, with/without reference distribution) to establish their individual necessity?\n\n**Q3.** What explains the counterintuitive result that unsupervised pretraining outperforms supervised pretraining on Cuneiform (50.09% vs. 45.32%) and IMDB-M, and is this due to overfitting, label noise, or fundamental issues with the supervised contrastive loss formulation?\n\n**Q4.** Why were recent graph foundation models such as GraphFM [2] and GraphAny [3] not included as baselines, and can you provide at least one direct comparison to validate the claimed improvements over state-of-the-art methods?\n\n**Q5.** Can you quantify the impact of using topology-derived features (Eq. 6) versus rich semantic attributes on transfer performance through controlled experiments, and how does this affect the \"domain-agnostic\" representation claim?\n\n[1] Williams, C., & Seeger, M. (2000). Using the Nyström method to speed up kernel machines. Advances in Neural Information Processing Systems, 13.\n\n[2] Lachi, D., Azabou, M., Arora, V., & Dyer, E. (2024). GraphFM: A scalable framework for multi-graph pretraining. arXiv preprint arXiv:2407.11907.\n\n[3] Zhao, J., Mostafa, H., Galkin, M., Bronstein, M., Zhu, Z., & Tang, J. (2024). GraphAny: A foundation model for node classification on any graph. arXiv preprint arXiv:2405.20445."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rZXMoRsfKm", "forum": "80552WurUJ", "replyto": "80552WurUJ", "signatures": ["ICLR.cc/2026/Conference/Submission12276/Reviewer_ph5T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12276/Reviewer_ph5T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921848271, "cdate": 1761921848271, "tmdate": 1762923211652, "mdate": 1762923211652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GraphVec-FM, a foundation model that encodes graphs into vectors for graph-level tasks. It aligns features across domains, enhances consistency through a density-based mean alignment algorithm, and preserves node information via a reference distribution module. Experiments on public datasets demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes novel techniques with theoretical guarantees.\n\n2. The experimental results demonstrate strong generalization ability across diverse domains and tasks."}, "weaknesses": {"value": "1. The motivation could be further supported by dedicated experimental evidence.\n\n2. An ablation study on some key components is missing, which would help clarify their individual contributions.\n\n3. In Table 1, the proposed method shows clear advantages on ENZYMES and DD but not on other datasets. A deeper analysis would strengthen the authors’ understanding of when and why the method performs best.\n\n4. There is a minor typo in the last sentence of Page 25: “we also provide the full version of Table 4 with NMI in 8.”\n\n5. Table 9 appears to be uncited in the main text."}, "questions": {"value": "1. In Table 2, why does the Unsupervised GraphVec-FM outperform the supervised version on IMDB-M and Cuneiform?\n\n2. Why is the performance of GraphVec-FM in Table 3 higher than in Table 2 for IMDB-M?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "991cu3YwZ1", "forum": "80552WurUJ", "replyto": "80552WurUJ", "signatures": ["ICLR.cc/2026/Conference/Submission12276/Reviewer_kZL6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12276/Reviewer_kZL6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944946469, "cdate": 1761944946469, "tmdate": 1762923211054, "mdate": 1762923211054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}