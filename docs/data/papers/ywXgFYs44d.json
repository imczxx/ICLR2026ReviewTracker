{"id": "ywXgFYs44d", "number": 14946, "cdate": 1758245897808, "mdate": 1759897339821, "content": {"title": "Opponent Simulation as Inference-time Scaling for Self-improving Agent: Case Study of Repeated Negotiations", "abstract": "Large language models (LLMs) have recently emerged as powerful decision-makers across a wide range of reasoning-intensive tasks. While prior work has made great progress in single-agent environments, less effort has been devoted to settings where LLMs must engage in \\emph{repeated} and \\emph{strategic} interactions without prior knowledge about the opponents. In such settings, traditional self-play or offline training, though robust against worst-case adversaries, do not fully leverage the flexibility of LLMs to continually self-improve based on interaction feedback. To address this, we introduce a general inference-time framework called best-of-$N$ sampling with opponent simulation (\\ours), with a case study in repeated negotiation games. The framework scales inference-time computation by embedding the principles of a classical game-theoretical learning dynamic, \\emph{fictitious play (FP)}, into practical LLM implementations: (i) for the belief formation step, we introduce a separate LLM as an opponent model that in-context learns to imitate the \\emph{time-averaged} behavior of the opponent from past interactions; (ii) for the best response step, we perform BoN by simulating future outcomes using the opponent model, where candidates are generated through a structured strategic brainstorming process. Empirical evaluations on two repeated negotiation games, the buyer-seller negotiation and the resource exchange negotiation, demonstrate that our method achieves  significant self-improvement over repeated interaction compared with various baselines, offering a lightweight and scalable approach to strategic reasoning and decision-making.", "tldr": "We develop a framework for improving LLMs agents in strategic reasoning and decision-making tasks", "keywords": ["strategic reasoning", "llm negotiations", "inference-time techniques"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f67c53d49da71d7e990466342af45991debf5e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes BoN-oppo-simulation, an inference-time framework for enabling LLMs to self-improve in repeated strategic interactions, such as negotiations, without offline training. Drawing from fictitious play (FP), it uses:\n\n-An opponent model (separate LLM) that in-context learns time-averaged opponent behavior from history, with optimism under uncertainty for exploration.\n\n-Best-of-N (BoN) sampling: Generate candidates via strategic brainstorming, simulate trajectories with the opponent model, and select the best based on rewards.\n\nEvaluated on buyer-seller and resource exchange games, it shows self-improvement over episodes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Embedding FP into LLM inference is somewhat novel, extending test-time compute scaling to multi-agent strategy. The opponent model as a simulator, with OFU, provides principled adaptation."}, "weaknesses": {"value": "**Concept Confusion:** Prompt Engineering vs. Inference-Time Techniques: The paper positions its method as a \"novel inference-time technique\" (Sec. 2) superior to prior works like Fu et al. (2023), which it categorizes as \"advanced technique for automatic prompt engineering.\" However, this distinction is unclear and potentially overstated. The proposed framework relies heavily on specialized prompts (e.g., for brainstorming strategies, reflection, opponent summarization in App. A)—which is essentially prompt engineering.  \n \n**Missing Baselines:** The related work   surveys LLM agents for strategic decision-making, citing Diplomacy agents (Bakhtin et al., 2022; Xu et al., 2025), opponent modeling (Yu et al., 2025), and others like Werewolf (Xu et al., 2023/2024). However, these are not empirically compared, despite their relevance to repeated, multi-agent strategy. A glaring omission is Richelieu (Guan et al., 2024) (Zhenyu Guan, Xiangyu Kong, Fangwei Zhong, and Yizhou Wang. Richelieu: Self-evolving llmbased agents for ai diplomacy. Advances in Neural Information Processing Systems, 37:123471–123497, 2024.), a self-evolving LLM agent for AI Diplomacy that uses hierarchical planning and self-play for adaptation—directly aligned with this paper's self-improvement goals. Richelieu achieves human-like performance without human data, making it a strong baseline for testing BoN-oppo-simulation in more complex games. Without these comparisons, the empirical claims feel insular, limited to toy negotiations against stationary opponents.\n\n**Limited novelty**: Embedding FP into LLM inference has been studied by: Game of Thoughts: Iterative Reasoning in Game-Theoretic Domains with Large Language Models (AAMAS 2025)\n\n**Limited Scope and Generalizability**: Evaluations are confined to two negotiation games. Why do we care about both games? How does this extend to more complex games (e.g., Diplomacy with partial observability, or continuous-action spaces like auctions)? Or non-negotiation settings (e.g., repeated Prisoner's Dilemma, bandits with LLM opponents)?  \n\n Opponents are mostly stationary LLMs (e.g., Gemini without inference-time boosts). While they test dynamic budgets/costs, more adversarial or human opponents would strengthen claims. The paper acknowledges multi-agent limits in Sec. 6, but this feels like a key gap.\n\n**Computational Overhead** is unclear.  BoN with simulations seems requiring high costs."}, "questions": {"value": "See the above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9zajFWxt18", "forum": "ywXgFYs44d", "replyto": "ywXgFYs44d", "signatures": ["ICLR.cc/2026/Conference/Submission14946/Reviewer_oQJh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14946/Reviewer_oQJh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760848624860, "cdate": 1760848624860, "tmdate": 1762925284875, "mdate": 1762925284875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the BoN-oppo-simulation framework, which facilitates adversarial negotiation reasoning for LLMs at inference time. The interactions between LLM agents are modeled through the lens of fictitious play. A case study in repeated negotiation games demonstrates that the proposed method achieves significant self-improvement over repeated interaction compared with various baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is presented with exemplary clarity, greatly aided by its well-defined and intuitive game setting.\n\n- It makes a significant and pragmatic contribution by introducing a tractable, inference-time LLM framework for competitive negotiation scenarios.\n\n- The methodology is comprehensive and well-grounded, and I believe it to be reproducible, as evidenced by the provided example outputs.\n\n- The paper presents a highly interesting and original integration of game-theoretic reasoning into LLMs. This integration is supported by substantial theoretical justification."}, "weaknesses": {"value": "- The presentation of the game-theoretic model could be enhanced by incorporating a visual overview, such as a diagram, to illustrate the overall framework and interaction flow.\n\n- In Section 3.1, the description of two games could be better grounded in the LLM context, clarifying how their designs specifically leverage or challenge LLM reasoning capabilities."}, "questions": {"value": "- On Page 1, the minimax strategy appears overly conservative in scenarios that are not highly adversarial. Could the authors clarify what level or degree of adversarial setting this work specifically targets?\n\n- Traditional adversarial game theory offers a variety of well-established models. Why do the authors choose to model the problem via fictitious play, rather than adopting other well-established game formulations?\n\n- When multiple LLMs engage in repeated games as described in this paper, is there a risk of uncontrolled amplification (e.g., hallucination) emerging during the interaction process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fzKLveVjpn", "forum": "ywXgFYs44d", "replyto": "ywXgFYs44d", "signatures": ["ICLR.cc/2026/Conference/Submission14946/Reviewer_oqQo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14946/Reviewer_oqQo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905270251, "cdate": 1761905270251, "tmdate": 1762925284023, "mdate": 1762925284023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the limitation of LLMs in achieving continual self-improvement during repeated strategic interactions with no prior knowledge about the opponents. To tackle this problem, it proposes BoN-oppo-simulation, a lightweight inference-time framework that embeds the game-theoretic principle of fictitious play (FP) into LLM implementations. The framework uses a separate LLM as an opponent model to learn and imitate the opponent’s time-averaged behavior from past interactions for belief formation, and adopts structured strategic brainstorming to generate candidate responses, then selects the optimal one via opponent-simulated future outcomes for best response. Empirical experiments on buyer-seller and resource exchange negotiation games across multiple LLMs demonstrate that the framework outperforms the baselines, motivating the necessity of self-improvement for repeated strategic decision-making."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a novel BoN-oppo-simulation framework. It lies in the non-trivial fusion of classical fictitious play (FP) in game theory with LLM inference-time scaling, that it embeds FP’s core steps (belief formation, best response) into practical LLM pipelines, enabling continual self-improvement via interaction feedback without fine-tuning for repeated strategic decision-making.\n2. This work provides some theoretical analysis to justify the necessity of self-improvement and the \"no-regret\" property of the FP-based design. And it also empirically tests the proposed framework with granular implementation details across LLMs and negotiation games to demonstrate the efficacy. \n3.  The paper focuses on a critical real-world constraint: LLMs’ deployment in dynamic multi-agent settings where LLMs cannot rely on pre-trained/fine-tuned policies for unknown opponents. This paper may provide a practical solution for real-world strategic LLMs."}, "weaknesses": {"value": "1. The paper frames the framework as lightweight and scalable. However, it provides insufficient quantitative analysis of inference-time computational cost compared with baselines. For example, BoN sampling with N=5 and multi-turn opponent simulation may incur higher costs, but the paper does not discuss tradeoffs between performance and computational efficiency, which is critical for real-world deployment.\n\n2.  The paper states that the proposed framework is a general inference-time framework. However, i) it lacks analysis of opponent model generalization across LLM architectures, that more details on how the opponent model’s performance varies with different base models are needed; ii) the dynamic opponents tested in this paper are relatively simple, and the paper does not evaluate performance against highly adaptive opponents. These may limit the confidence in the framework’s robustness to adversarial dynamic strategies.\n\n3. The paper claims structured brainstorming generates more diverse candidates than i.i.d. sampling but relies solely on standard deviation as a diversity metric.  I think it is hard to confirm if brainstorming truly explores more strategic space or only varies numerical values. In natural language negotiations, maybe more dimensions and diversity metrics, e.g., measuring semantic similarity, should be considered for the quantitative comparison of the candidate generations.\n\n4. The number of candidates N is set to 5 by default. However, the paper lacks investigation of how hyperparameters, e.g., N, impact the performance and the computational cost.  The analysis of hyperparameter sensitivity should be considered for practical deployment, as users need to know how to tune N for their resource constraints."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4a0522SZJ8", "forum": "ywXgFYs44d", "replyto": "ywXgFYs44d", "signatures": ["ICLR.cc/2026/Conference/Submission14946/Reviewer_LZah"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14946/Reviewer_LZah"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905639399, "cdate": 1761905639399, "tmdate": 1762925283484, "mdate": 1762925283484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BoN-oppo-simulation, an inference-time self-improvement framework for repeated strategic interactions. It operationalizes the fictitious-play idea for LLM agents by (1) building an opponent model via in-context learning of past interactions and (2) using best-of-N (BoN) candidate generation with opponent simulation to evaluate and select actions by simulating future trajectories. The approach is evaluated on two repeated negotiation games (buyer–seller and resource exchange), compared to several baselines (including BoN variants and “thinking”/CoT baselines), and shown to yield consistent self-improvement across episodes and across backbone models. The paper contains theoretical motivation (Proposition on non-existence of a dominant fixed policy), methodological description, experiments with several LLM backbones, and discussion of limitations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The mapping of fictitious play (belief formation + best response) to an inference-time recipe for LLMs is intuitive and well motivated: keep a time-averaged opponent model and use simulation to compute approximate Q’s for candidate actions. This gives a conceptually clean approach to test-time self-improvement without parameter updates.\n2. The paper combines (i) in-context opponent modeling with an OFU (optimism-in-face-of-uncertainty) design in prompts and (ii) a structured “brainstorming → concrete action” candidate generation to increase candidate diversity; both are sensible and practically relevant. The distinction between BoN-oppo, BoN-eval, BoN-simulation (CoT), and BoN-oppo (iid) is useful for isolating which component yields gains."}, "weaknesses": {"value": "1. The mapping to fictitious play is conceptually appealing, and Proposition 4.1 is useful to argue the need for adaptation. However, there is no formal analysis of when the inference-time BoN + opponent simulation reliably approximates best responses in multi-turn natural language games (e.g., error amplification through simulated rollouts when opponent model is imperfect). A short theoretical or empirical analysis of sensitivity to opponent-model error would substantially strengthen claims.\n2. The approach uses substantial inference-time compute (N candidates and multi-step simulation). The paper discusses scalability and explores N and l in plots, but it lacks concrete wall-clock cost or token-cost tradeoff tables (how much extra latency per turn for N=5 vs N=10, or simulated tokens produced per candidate). For deployment, such numbers matter.\n3. The paper shows some experiments where opponents also use BoN variants, but more systematic evaluation of adversarial or fast-learning opponents (or ablations where the opponent model intentionally lags) would clarify limits. There is only partial evidence for dynamic opponents."}, "questions": {"value": "1. The authors note the limitation to 2-agent settings; extension to larger multi-agent scenarios remains open. What if the task scales to >2 agents?\n2. Would a full version of prompt text an exact scoring/evaluation be available to readers? For an inference-time, prompt-sensitive method, full prompt text and exact scoring/evaluation formats are essential. (The appendix references prompts, but these must be easy to copy and run for others.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X0GGW0eqC1", "forum": "ywXgFYs44d", "replyto": "ywXgFYs44d", "signatures": ["ICLR.cc/2026/Conference/Submission14946/Reviewer_Vpgx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14946/Reviewer_Vpgx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980993390, "cdate": 1761980993390, "tmdate": 1762925283044, "mdate": 1762925283044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}