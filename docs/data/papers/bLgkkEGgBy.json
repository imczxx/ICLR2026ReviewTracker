{"id": "bLgkkEGgBy", "number": 7680, "cdate": 1758031740849, "mdate": 1763580221792, "content": {"title": "Learning for Highly Faithful Explainability", "abstract": "Learning to Explain is a forward-looking paradigm recently proposed in the field of explainable AI, which envisions training explainers capable of producing high-quality explanations for target models efficiently. Although existing studies have made attempts through self-supervised optimization or learning from prior explanation methods, the Learning to Explain paradigm still faces three critical challenges: 1) self-supervised objectives often rely on assumptions about the target model or task, restricting their generalizability; 2) methods driven by prior explanations struggle to guarantee the quality of the supervisory signals; and 3) depending exclusively on either approach leads to poor convergence or limited explanation quality. To address these challenges, we propose a faithfulness-guided amortized explainer that 1) theoretically derives a self-supervised objective free from assumptions about the target model or task, 2) practically generates high-quality supervisory signals by deduplicating and filtering prior explanations, and 3) jointly optimizes both objectives via a dynamic weighting strategy, enabling the amortized explainer to produce more faithful explanations for complex, high-dimensional models. We re-formalize multiple well-validated faithfulness evaluation metrics within a unified notation system and theoretically prove that an explanation mapping can simultaneously achieve optimality across all these metrics. We aggregate prior explanation methods to generate high-quality supervised signals through deduplicating and faithfulness-based filtering. Our amortized explainer leverages dynamic weighting to guide optimization, initially emphasizing pattern consistency with the supervised signals for rapid convergence, and subsequently refining explanation quality by approximating the most faithful explanation mapping. Extensive experiments across various target models and image, text, and tabular tasks demonstrate that the proposed explainer consistently outperforms all prior explanation methods across all faithfulness metrics, highlighting its effectiveness and its potential to offer a systematic solution to the fundamental challenges of the Learning to Explain paradigm.", "tldr": "This work overcomes three key challenges in \"Learning to Explain\" paradigm by jointly optimizing a model-agnostic objective and filtered prior explanations to generate more faithful explanations.", "keywords": ["Explainability; Faithfulness; Learning to Explain"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/92c6a475481ef43f2a9bbb008d48b40a7e3ead83.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses key challenges in the \"Learning to Explain\" paradigm by proposing DeepFaith, a framework for training highly faithful amortized explainers. The authors identify that existing methods are limited by restrictive assumptions in self-supervised objectives or by the low quality of supervisory signals from prior explanation methods.  To overcome this, DeepFaith introduces a three-part solution: (1) a theoretically-derived, model-agnostic self-supervised objective based on a unified view of faithfulness metrics; (2) a novel pipeline for generating high-quality supervisory signals by aggregating, deduplicating, and filtering explanations from existing methods based on their faithfulness; and (3) a dynamic joint optimization strategy that combines both objectives to ensure rapid convergence and high final explanation quality. Extensive experiments across image, text, and tabular data show that DeepFaith consistently outperforms prior explanation methods across a suite of ten faithfulness metrics."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper provides a clear motivation and problem definitions. The introduction clearly differentiates faithfulness from plausibility and articulates the gap between qualitative interpretability and quantitative fidelity. \n-  This paper provides strong theoretical grounding for the self-supervised objective.  The paper makes a laudable effort to build its self-supervised objective on a firm theoretical foundation, rather than relying on heuristics or task-specific assumptions. The authors reformalize ten different, widely used faithfulness metrics into a unified notational system, distinguishing between \"saliency\" and \"permutation\" perspectives. The paper theoretically proves that an optimal explanation mapping exists that simultaneously maximizes performance across all ten considered faithfulness metrics. \n-  This paper provides a novel faithfulness-aware learning framework.  It defines synthetic supervision using controlled feature perturbations (Sec. 3.1; Fig. 2), bridging causal interpretability with optimization. The faithfulness loss integrates causal relevance with sparsity and smoothness priors, balancing interpretability and fidelity.\n-  This paper provides a comprehensive empirical evaluation. The authors consider image, text, and tabular datasets and various architectures, such as ResNet,  Deit, Transformer, and LSTM, demonstrating improvement across explainers (GradCAM, IG, LRP), confirming generalizability."}, "weaknesses": {"value": "- This paper lacks an analysis of the computational cost of training.  The paper emphasizes the fast inference speed of the amortized explainer but does not quantify or discuss the potentially massive upfront computational cost required for training. The signal generation phase involves running $K$ different explanation methods (including computationally expensive ones like Kernel SHAP and Score-CAM) on thousands of training samples, and then computing 10 faithfulness metrics for each generated explanation.  This pre-processing cost is likely to be very high but is not reported, making it difficult to assess the overall efficiency of the framework.   The training process involves calculating $\\mathcal{L}_{LC}$, which requires $K$ perturbations and forward passes through the target model for each sample in a batch. This adds significant overhead compared to simply training on a static dataset of prior explanations. Total training times are not provided.  While DeepFaith is shown to be fast at inference, the comparison can be misleading. Many of the fastest baselines (e.g., Saliency, Input x Gradient) are also the ones that perform most poorly on faithfulness, while the methods that are more faithful (e.g., Integrated Gradients, Kernel SHAP) are much slower. A plot of faithfulness vs. runtime would provide a more nuanced view of the trade-offs.\n\n- There are still some writing issues, for example, the citation is mixed with the body text, making it hard to read. The notation $\\bar{\\pi}$ in the formula for the NEG metric in Table 1 is undefined.  The notation in Table 1 is very compact. The formula for Faithfulness Correlation (FC), for instance, implies a correlation over all $2^n$ subsets of features, \nwhich is computationally intractable and differs from the Monte Carlo sampling used in the actual loss function $\\mathcal{L}_{LC}$.      \n   \n- This framework introduces a new set of sensitive hyperparameters that require manual tuning for each task, somewhat undermining the goal of full automation. Table 12 shows that both the `Similarity Threshold` for deduplication and the `p-quantile` for filtering were set to different values for each of the 12 explanation tasks. The paper provides no methodology for how these values were chosen, suggesting a manual, trial-and-error process that is contrary to the overarching goal of automating explainability. The performance of the filtering pipeline is contingent on the initial set of $K$ baseline methods. The paper uses a large set of 14 methods for images and 8 for other modalities, but does not analyze how the results would change with a smaller or different set of priors."}, "questions": {"value": "- What was the rationale for choosing a Transformer Encoder architecture for the explainer across all modalities, including tabular data where simpler architectures might be more common? \n- Regarding Algorithm 1, how sensitive is the dynamic weighting strategy to the choice of the hyperparameters $\\epsilon$, $e$, and $C$? \n- The ablation study in Table 3 shows that training with only $\\mathcal{L}_{LC}$ performs very poorly. Does this suggest that the self-supervised objective alone is insufficient to guide the explainer to a good solution from a random initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fmPsAQb1Zp", "forum": "bLgkkEGgBy", "replyto": "bLgkkEGgBy", "signatures": ["ICLR.cc/2026/Conference/Submission7680/Reviewer_FfQQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7680/Reviewer_FfQQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760759117023, "cdate": 1760759117023, "tmdate": 1762919743100, "mdate": 1762919743100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a faithfulness-guided amortized explainer that jointly optimizes self-supervised and prior-based objectives, and the experiments show improved faithfulness and efficiency across different datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a explainer that integrates self-supervised and prior-based objectives, offering a unified perspective for the Learning to Explain paradigm.\n\n2. Experiments across image, text, and tabular datasets and the paper is clear-written."}, "weaknesses": {"value": "1. In the Introduction, the authors identify three key challenges: generalization, supervision quality, and convergence. However, the descriptions of these problems remain somewhat abstract. Specifically, the statement “self-supervised objectives rely on assumptions about the target model” could be better supported with a concrete example, such as the failure of L2X under correlated features, to make the motivation more convincing.\n\n2. In Algorithm 1, they presents a variance-based switching rule for adjusting α, but the paper provides no theoretical guarantee of stability or convergence. And Fig 3 still shows noticeable oscillations during training, suggesting limited robustness of the proposed mechanism.\n\n3. The experiments include many baselines for image, but text and tabular tasks only use a few baselines. This imbalance makes the evaluation heavily vision-centric. Adding some representative methods for text and tabular data would strengthen fairness.\n\n4. The explanation units are different across modalities, and no experiment examines cross-modal transfer or unified explainer training. Therefore, the claim that the framework generalizes across modalities is not yet empirically supported."}, "questions": {"value": "The authors may refer to the weaknesses and address them in the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sGwxtfgVan", "forum": "bLgkkEGgBy", "replyto": "bLgkkEGgBy", "signatures": ["ICLR.cc/2026/Conference/Submission7680/Reviewer_JuTS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7680/Reviewer_JuTS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761529710691, "cdate": 1761529710691, "tmdate": 1762919742629, "mdate": 1762919742629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DeepFaith, an amortised explainer that generates faithful explanations for a target model’s predictions in a single forward pass. The goal is to address two long-standing problems in Learning to Explain (LtE):\n- self-supervised explanation models tend to be unstable, and often make task/model assumptions (e.g. separability of features), and frequently fail to converge for high-dimensional models.\n- distilled explanation models (student networks trained to imitate a teacher explainer) inherit teacher bias/noise and are upper-bounded by teacher quality.\n\nDeepFaith is built around three components:\n- Theoretical unification of faithfulness: The paper rewrites 10 widely used faithfulness metrics under one formal framework using perturbation operators and correlation measures. It then shows that a single optimal faithful explanation mapping exists, jointly optimal across these metrics. It shows that the ranking induced is also optimal for perturbation-based ranking metrics. \n- Local Correlation loss ($L_{LC}$): The explainer is trained to align its predicted importance scores with the actual effect of perturbing/removing subsets of features on the model’s output. In other words, subsets that the explainer says are important should, in fact, cause the model’s prediction to drop when those features are masked.\n- Curated supervision + dynamic training ($L_{PC}$): Instead of blindly distilling a single attribution method, DeepFaith constructs a pseudo-labelled supervision set as follows: for each input, it gathers explanations from many attribution methods, deduplicates near-duplicates, and keeps only those that are highly faithful on that exact input across several metrics. The explainer is first trained to imitate these curated high-faithfulness maps, and then gradually shifted toward the self-supervised tasks using a dynamic controller that adapts. The controller increases reliance on the self-supervised loss once the supervised loss stabilises, and can revert if training destabilises.\n\nDeepFaith is evaluated on 12 (dataset, model) pairs across three modalities and multiple architectures. Across 10 standard faithfulness metrics, DeepFaith achieves the best average rank on all tasks and outperforms the attribution methods it partly learned from. Ablation studies show that removing any of the key pieces degrades performance. The paper argues that DeepFaith therefore delivers (i) explanations that are more faithful to model behaviour, (ii) in a single forward pass at inference time, and (iii) across multiple data modalities and network architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes a single mathematical lens that captures 10 commonly used faithfulness metrics and argues that there exists an optimal explanation mapping that is jointly optimal across all of them. It claims that correlation-style metrics and perturbation-style metrics are not fundamentally competing criteria but can be satisfied simultaneously. \n\nRather than naively distilling from a single teacher explainer, the paper proposes a per-sample curation pipeline: generate many explanations, deduplicate them, and select only those that empirically score best across multiple metrics for that specific input. This is a clever way to ensure the distilled model surpasses the teacher's abilities.\n\nDeepFaith combines a supervised imitation loss and a self-supervised faithfulness loss with an adaptive weighting schedule. The training controller monitors the stability of the supervised loss and shifts the weight toward self-supervision once the model is stable, then bounces back to supervision if it destabilises. This offers an explicit control loop to manage convergence. Prior LtE methods tend to be either pure distillation (stable but capped) or pure self-supervision (principled but unstable).\n\nThe paper doesn’t restrict itself to a single modality; it claims a single template that applies to CNNs, ViTs, LSTMs, Transformers, and MLPs across images, natural language, and tabular data."}, "weaknesses": {"value": "Missing comparisons to prior LtE/amortised explainers: The paper positions itself as advancing the Learning-to-Explain paradigm, i.e., training a neural explainer that produces explanations in a single forward pass. But in the experiments, the baselines are almost entirely classic post-hoc attribution methods (Integrated Gradients, GradCAM++, Occlusion, Kernel SHAP, LIME, etc.).\n\nIt's missing are head-to-head comparisons against other amortised / LTE-style methods:\n- L2X \n- CXPlain\n- FastSHAP\n- VerT / selection-and-rationale models for vision/text that jointly learn to point and predict.\n\nThese methods are discussed in related work, but they are not quantitatively included in the results tables. Currently, the only measured comparisons are to non-amortised attribution methods; if DeepFaith cannot clearly outperform other learned explainers, it would weaken the paper's contributions.\n\nDeepFaith’s curated supervision set is built by:\n- Generating multiple candidate explanations from K attribution methods for each input.\n- Computing all 10 faithfulness metrics for each candidate explanation on that specific input.\n- Keeping only the top explanations by those metrics (after deduplication).\n- Training the explainer to imitate those survivors, then refining.\n\nAt test time, DeepFaith is evaluated on those same 10 metrics.\n\nThis creates a risk of training on the test: DeepFaith is at least partially optimised to produce explanations that look good under exactly the metrics it’s later ranked by. Yes, self-supervision introduces a new structure by forcing local correlation between model behaviour and perturbations. But without an experiment that uses held-out metrics, it’s hard to conclude DeepFaith is genuinely, broadly more faithful rather than just better at exploiting the metrics.\n\nThe theory relies on assumptions about monotonic relationships between removal and insertion effects and consistent perturbation baselines. In practice, faithfulness metrics vary widely in how they perturb features. If two metrics use different perturbation baselines, it’s not apparent that a single explanation can be jointly optimal for both. The paper should be explicit about whether all metrics were instantiated using a shared perturbation scheme or whether there are any assumptions.\n\nEach DeepFaith explainer is trained separately for each target model. The paper doesn’t test whether an explainer trained on one model can be reused for another model in the same domain (e.g., training on DeiT and applying it to EfficientNet on ImageNet). If there’s no transfer at all, then amortisation helps inference latency but does not reduce the number of explanation models.\n\nAll evaluation metrics are model-faithfulness metrics: does ablating important features change the model’s output in the expected direction? The paper also leans on real-world motivation, where humans must interpret the explanations. There’s no human study, or even a small qualitative preference experiment, in the main results to demonstrate that DeepFaith explanations are more clinically meaningful than others.\n\nThe curated supervision pipeline depends on thresholds:\n- cosine similarity thresholds for deduplication,\n- p-quantile cutoffs for filtering which explanations count as faithful,\n- and dynamic controller hyperparameters for toggling $\\alpha$.\n\nThe main results don’t present a sensitivity study."}, "questions": {"value": "Right now, you curate supervision using all 10 faithfulness metrics, and you evaluate on those same 10. Can you train DeepFaith while excluding one or two metrics from the curation filter, and then report performance specifically on those held-out metrics?\nIf DeepFaith still outperforms baselines on those unseen metrics, it would strengthen your claims.\n\nCan you include at least one amortised explainer baseline per modality? If not, at least justify clearly why they are not comparable. Right now, this missing comparison is a significant experimental gap.\n\nThe theory assumes a relationship between removal-based and insertion-based perturbation effects and essentially treats them as monotonic and compatible. In practice, these can be instantiated with different perturbation baselines (e.g., zeroing, blurring, masking).\nDid you enforce a consistent perturbation operator across all metrics in your experiments, or are you making any assumptions? It would be helpful to state it explicitly.\n\nFor the Local Correlation loss $L_{LC}$:\n- How many feature subsets are sampled per input/batch to estimate the correlation, and how sensitive is performance to that number?\n- Do you observe high gradient variance or unstable optimisation when using $L_{LC}$ alone?\n- You show training dynamics for the whole method (including $\\alpha$ adapting over time); can you also share curves for the $L_{LC}$-only variant to illustrate the claimed instability in isolation?\n\nIf I train DeepFaith for a DeiT classifier on ImageNet, how well does it explain an EfficientNet classifier on the same ImageNet labels without retraining?\n\nDid you run any preference checks with human annotators or domain experts? If you didn’t, can you comment on whether the high-faithfulness explanations ever look spurious? In general, I would encourage the authors to conduct a human study or a small qualitative preference experiment.\n\nHow sensitive is performance to the hyperparameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LL5pcwO31U", "forum": "bLgkkEGgBy", "replyto": "bLgkkEGgBy", "signatures": ["ICLR.cc/2026/Conference/Submission7680/Reviewer_NUZJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7680/Reviewer_NUZJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740645687, "cdate": 1761740645687, "tmdate": 1762919742190, "mdate": 1762919742190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **DeepFaith**, a method for generating explanations of neural network predictions within the “learning-to-explain” framework. The authors argue that previous approaches fall into two main categories: (1) *Self-supervised methods* that interact directly with the model to optimize a faithfulness objective, but rely on strong assumptions about the model; and (2) *Signal-based methods* that generate explanations using post-hoc techniques and then train a separate explainer model on these explanations, but do not explicitly optimize for faithfulness. DeepFaith is proposed as a hybrid approach that aims to leverage the strengths of both paradigms. It does so by generating a large set of explanations using diverse post-hoc methods and then guiding training using faithfulness-driven objectives. The authors justify this design by proving that different notions of faithfulness, including saliency-based and perturbation-based attribution measures, are aligned, supporting the idea of optimizing toward a unified faithfulness objective. In practice, DeepFaith is trained using two dynamically weighted objectives: the first encourages learning general explanation patterns, while the second shifts training toward optimizing faithfulness through a specific loss function. The method is evaluated across a wide range of vision and language benchmarks, where it consistently outperforms standard post-hoc explanation methods in terms of faithfulness metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of combining both the “learning from explanation signals” paradigm and the optimization via faithfulness objectives is interesting and, to the best of my knowledge, novel (though I may not be aware of all prior work in this area, which are many). However, the paper would benefit from stronger motivation and additional empirical evidence supporting the need for this approach (see Weaknesses).\n\n2. The unification of various notions of saliency and attribution appears potentially useful in certain contexts.\n\n3. The paper is generally well-written and easy to follow.\n\n4. The authors present extensive experiments across a wide range of models, showing that their method achieves improved faithfulness compared to post-hoc approaches."}, "weaknesses": {"value": "1. While the authors compare their method against post-hoc explanation techniques, they do not include comparisons to self-supervised approaches that directly optimize a faithfulness objective without relying on explanation signals. As a result, it remains unclear whether generating these explanation signals is necessary at all and the theoretical analysis does not justify this design choice.\n\n2. The paper argues that self-supervised methods that directly optimize a faithfulness loss inherently rely on assumptions about the model, but this claim is insufficiently supported. The authors should clarify why such assumptions are required and provide concrete reasoning or evidence to motivate this statement.\n\n3. Regarding the theoretical contributions: the presented proofs are relatively straightforward and unsurprising. The claim of unifying multiple attribution metrics and explanation methods is not entirely novel - prior work has already shown that many attribution methods optimize a shared class of objectives (e.g., Yu et al., NeurIPS 2019, among many others). The authors should better articulate what distinguishes their analysis from these earlier results and explain what new insights it provides."}, "questions": {"value": "1. What justifies the claim that self-supervised approaches necessarily rely on assumptions about the model? Which assumptions are unavoidable, and why?\n\n2. Why is it essential to generate explanation signals at all, rather than simply optimizing the faithfulness loss directly? What would change, conceptually or empirically, if we skipped the explanation generation step?\n\n3. There is a substantial line of work on self-explaining models that produce explanations during inference. If DeepFaith were instead trained using a self-explaining architecture that generates explanations optimized via the faithfulness-based objective, how would this fundamentally differ from the proposed approach here, beyond the potential trade-off in accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YYsHWfuXXt", "forum": "bLgkkEGgBy", "replyto": "bLgkkEGgBy", "signatures": ["ICLR.cc/2026/Conference/Submission7680/Reviewer_kSSW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7680/Reviewer_kSSW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744292174, "cdate": 1761744292174, "tmdate": 1762919741864, "mdate": 1762919741864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}