{"id": "Bxxdz07CDp", "number": 2840, "cdate": 1757269475787, "mdate": 1763732334862, "content": {"title": "Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation", "abstract": "Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principles. Our approach introduces a three-stage pipeline—**Knowledge Identifier**, **Organizer**, and **Adapter** (**IOA**)—that systematically identifies knowledge deficiencies in student models, organizes knowledge delivery through progressive curricula, and adapts representations to match the cognitive capacity of student models. We integrate Bloom's Mastery Learning Principles and Vygotsky's Zone of Proximal Development to create a dynamic distillation process where student models approach teacher model's performance on prerequisite knowledge before advancing, and new knowledge is introduced with controlled, gradual difficulty increments. Extensive experiments using LLaMA-3.1/3.2 and Qwen2.5 as student models demonstrate that IOA achieves significant improvements over baseline distillation methods, with student models retaining 94.7\\% of teacher performance on DollyEval while using less than 1/10th of the parameters. Our framework particularly excels in complex reasoning tasks, showing 19.2\\% improvement on MATH and 22.3\\% on HumanEval compared with state-of-the-art baselines.", "tldr": "This paper introduces a pedagogically-inspired data synthesis framework that distills knowledge from teacher to student language models through deficiency diagnosis, curriculum structuring, and stage-wise adaptation.", "keywords": ["Data Synthesis", "Large Language Model", "Knowledge Distillation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8c70f0a6b9b4f4f1433cb536b32c5da9199c640c.pdf", "supplementary_material": "/attachment/7857264b0bf42e8696efa47c0a82db7089b0f39b.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes to distill knowledge from close-sourced LLMs to small language models in a three-stage data synthesis pipeline with Knowledge Identifier, Organizer, and Adapter, inspired by pedagogical principles. Specifically, the Knowledge Identifier locates knowledge deficiencies of student models, which the Organizer leverages to schedule a curriculum in which the synthesized data is partitioned for progressive learning. The Organizer is inspired by Bloom’s Mastery Learning Principles and Vygotsky’s Zone of Proximal Development. Following the two components, a knowledge Adapter is proposed to rewrite the training data to match the cognitive constraints of student models. Experiments conducted with both LLaMA and Qwen series models as student models show consistent improvements of the proposed framework over baseline distillation methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposal of the knowledge identifier is novel and sound. The definition of the knowledge deficiency can be used beyond knowledge distillation in analyzing the capabilities of language models.\n\n2. A good leverage of Bloom’s Mastery Learning Principles to construct curriculum data that incentivize progressive learning of the small models.\n\n3. The paper is structured and written in a clear way."}, "weaknesses": {"value": "1. It seems to me that the paper lacks in comparison to previous work on improving knowledge distillation with curriculum learning, either in the related work section or the experiments. As the paper proposes this 3-stage knowledge distillation pipeline, which is closely relevant to curriculum learning, it would be considered incomplete by me if there is no comparison to works in curriculum learning.\n2. The difference between Table 1 and Table 2 is whether using OpenAI o1 or DeepSeek R1 as the teacher model. I feel the information in these two tables overlaps. As Table 2 also takes large space, it would be better if we could incorporate Table 1 and 2 into one table and include other exciting analysis results from other dimensions into the main text.\n3. The relationship between related works in  \"Language Model Distillation\" to this work\n4. Typo, line80: stag- -> stage"}, "questions": {"value": "1. Line 195: Any analysis on the value of $\\Delta(k)$ over datasets? Why 0.3 is a good threshold?\n\n2. What's the intuition behind using $P_T$ in eq.3? As eq.3 is just a measurement of the implicit dependency between two knowledge modules in the student model, would it be more memory efficient if we drop $P_T$?\n\n3. What's the Prerequisites function in eq.7? How do you define the prerequisite knowledge?\n\n4. How often is the $\\tau_\\text{mastery}$ in eq.9 sufficed without generating remedial data? I think figuring out this is significant to know the additional computation led by the Mastery-Based Progressive Learning.\n\n5. Do you observe catastrophic forgetting in the Mastery-Based Progressive Learning process? To be more specific, do you have studies on $\\frac{P_S(k’)}{P_T(k’)}, k’ \\not\\in s_i $?\n\n6. Line 267-291: How to measure the  SLMs’ cognitive constraints in ADAPTER. Do you assume the cognitive ability of SLM increases during training or remains static?\n\n7. Figure 3 shows that it is consistently better when you use $\\tau_{\\text{mastery}}=0.95$ instead of your default setting 0.9. Why not set 0.95 as your default setting? Or how much more computation is incurred by increasing $\\tau_{\\text{mastery}}$ from 0.9 to 0.95 becauseof recursively generating remedial data? This question also relates to question 4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dWX6O7fSpF", "forum": "Bxxdz07CDp", "replyto": "Bxxdz07CDp", "signatures": ["ICLR.cc/2026/Conference/Submission2840/Reviewer_HaWf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2840/Reviewer_HaWf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761561924863, "cdate": 1761561924863, "tmdate": 1762916402525, "mdate": 1762916402525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for creating synthetic data for knowledge distillation from large to small language models in a black-box setting. The authors propose a pedagogically-inspired framework from LLM knowledge distillation, which consists of knowledge identification, organization, and adaptation. It identifies knowledge deficiencies in the student, organizes the knowledge using a curriculum, and generates and adapts the training data so that the student can better internalize it. The experiments show that IOA outperforms baseline synthetic-data-based distillation methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Strong results in the self-instruct-like synthetic data generation setting, outperforming other synthesis-based distillation approaches.\n- Clear, theoretically grounded motivation with a connection to how humans learn, which makes a lot of intuitive sense.\n- Extensive experiments with multiple benchmarks, student models, and a particular focus on statistical significance. The method provides consistent improvements across diverse tasks (instruction following, math, code).\n- The \\tau_{ZPD} method peaking at around 0.10-0.15 is an interesting finding, showing that the training data should neither be too difficult nor too easy for the student models for optimal distillation results."}, "weaknesses": {"value": "- The released code appears to be non-functional and doesn't implement the described pipeline. For instance, the function decompose_knowledge returns hard-coded modules instead of deriving from the data. Key components are mocked. This doesn't correspond to what is written in the paper, and the paper cannot be reproduced.\n- Some methodological details could be better described in the paper (see my questions below).\n- IOA depends on many hand-selected/tuned hyperparameters, which might affect generalizability. The method doesn't seem overly sensitive to the ablated hyperparameters (J_i, \\tau_{ZPD}, \\tau_{mastery}), but some other hyperparameters like the threshold m for the selection of knowledge modules, \\alpha for severity, and \\tau_{high}, \\tau_{low}, and \\tau_{gap} are underanalyzed. \n- IOA consistently outperforms synthesis-based baselines in controlled settings, but there's a significant performance gap to the state-of-the-art distilled models (IOA: 6.3 % on AIME2024, DeepSeek-R1-Distill-Qwen-!.5B: 28.99 % on AIME2024). DeepSeek-R1 distillation likely uses orders of magnitude more training prompts than IOA's seed set, making the numbers incomparable. However, Figure 3 (left) indicates that IOA gets limited benefits from increasing the number of synthetic samples J_i. Hence, a question remains: would IOA maintain its relative advantage if scaled to a much larger seed set, which appears necessary for results close to SOTA? A compute-matched baseline with a larger open-source dataset (e.g., OpenThoughts [1]) that is filtered/sampled to match IOA's total compute budget (e.g., 30k-50k prompts) and trained with standard SFT would indicate how much value the curriculum adds.\n- The problem framing in the intro can be questioned. The intro states that \"more recent distillation methods in the LLM era discard the need for white-box access.\" This recency-based characterization is questionable. White-box distillation approaches are an active research direction and are used in practice (see e.g., DistiLLM-2 [2] and Qwen3 technical report [3]). The intro states that \"most merely treat distillation as a simple two-stage process: generating synthetic data with LLMs and then training SLMs on it\". Among the benchmark methods in Tables 1 and 2, at least Lion, Star-Agents, and MADA are not simple two-stage methods. Hence, claiming that \"previous works overlook the fact that KD should be a systematic learning process\" can be questioned. Furthermore, the work could more clearly acknowledge the line of research on on-policy distillation [4], which is built on the idea of the teacher correcting the mistakes generated by the student. Also, the Orca paper that argued that SLMs shouldn't simply learn to replicate the outputs of LLMs ([5]) is closely related to this work (especially the idea that the knowledge representations should be adapted to the learner's cognitive level), and you could consider citing it. Hence, the paper's novelty is mostly the overall combination, not necessarily the individual components.\n\n[1] Guha, E., Marten, R., Keh, S., Raoof, N., Smyrnis, G., Bansal, H., ... & Schmidt, L. (2025). OpenThoughts: Data Recipes for Reasoning Models.\n\n[2] Ko, J., Chen, T., Kim, S., Ding, T., Liang, L., Zharkov, I., & Yun, S. Y. (2025). Distillm-2: A contrastive approach boosts the distillation of llms. ICML\n\n[3] Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., ... & Qiu, Z. (2025). Qwen3 technical report. arXiv preprint arXiv:2505.09388.\n\n[4] Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Garea, S. R., Geist, M., & Bachem, O. (2024). On-policy distillation of language models: Learning from self-generated mistakes. ICLR\n\n[5] Mitra, A., Del Corro, L., Mahajan, S., Codas, A., Simoes, C., Agarwal, S., ... & Awadallah, A. (2023). Orca 2: Teaching small language models how to reason. arXiv preprint arXiv:2311.11045"}, "questions": {"value": "- Section 3.2 lacks implementation details, particularly related to the probe task creation. How do you assign validation examples from D_{seed} to knowledge modules k? The paper states that the probe tasks P_k come from the validation split, but doesn't explain whether this mapping is manual, fully automatic, or something in between. Then, is the hierarchy D generated end-to-end for each training run separately? If so, how is the validation example mapping done? Finally, it appears that the prompts corresponding to the generation of the hierarchy D are missing\n- The ablations in Table 3 could be explained in more detail. What do -identifier, -organizer, and -adapter mean? For instance, how does the organizer module work if you do not have identified knowledge modules? In addition, if the authors have sufficient computational resources, I would also like to see an ablation without GenerateRemedialData (if none of the ablations in Table 3 already correspond to this).\n- How do you estimate the values of the conditionals in Equation 3? It seems like you would need multiple checkpoints with PS(k_i)/PT(k_i) above 0.9 and below 0.7 for the conditionals to make sense, and for that, you'd need many training runs. Are these included in the wall-clock time estimations of Figure 4? More generally, which steps of the pipeline are included in Figure 4?\n- The method is used on a well-curated D_{seed}. Do you have any intuition whether the performance would translate to a random public sample or a larger D_{seed}, for which human curation is unscalable?\n- An on-policy logit-distillation baseline with a smaller teacher (e.g., from the DeepSeek-R1-Distill family for matched tokenizers) would strengthen the case for the paper in stating that black-box distillation can be used as a general alternative for white-box (logit-based) distillation. Furthermore, on-policy distillation and IOA could perhaps be combined for even greater performance.\n\nOverall, the discrepancy between the code and what is written in the paper should be clarified before this paper is ready for acceptance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "15ZN5mQ1mJ", "forum": "Bxxdz07CDp", "replyto": "Bxxdz07CDp", "signatures": ["ICLR.cc/2026/Conference/Submission2840/Reviewer_wP76"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2840/Reviewer_wP76"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642138869, "cdate": 1761642138869, "tmdate": 1762916401493, "mdate": 1762916401493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a pedagogically-inspired framework for knowledge distillation in LLMs. Unlike prior methods, which treat knowledge transfer as a straightforward supervised fine-tuning (SFT) procedure, this work conceptualizes distillation as a systematic, learning-aware process, guided by established educational principles. The proposed framework, IOA (Identifier–Organizer–Adapter), operates in three stages: it first diagnoses knowledge deficiencies in the student model, then sequences knowledge delivery into a progressive curriculum, ensuring mastery of foundational concepts before introducing more complex material, and finally adapts and simplifies knowledge representations to align with the capacity of the student."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work introduces a pedagogy-inspired perspective to knowledge distillation, framing it as a systematic learning process rather than a straightforward supervised fine-tuning task on synthetic data generated by the teacher LLM. This represents a novel conceptual contribution.\n\n- Extensive experiments across instruction-following, reasoning, and coding benchmarks demonstrate substantial performance gains over baseline distillation methods, highlighting the effectiveness of the proposed pipeline. The ablation studies further show each component’s necessity.\n\n- The paper is well-written and clearly structured."}, "weaknesses": {"value": "- The framework relies on multiple heuristics (e.g., thresholds for knowledge gaps, mastery gating, module decomposition, curriculum chunking). While some sensitivity analyses are provided, it remains unclear how well these heuristics generalize across tasks or domains, potentially requiring careful manual tuning.\n\n- Although the paper evaluates against several strong synthetic-data baselines, it does not include comparisons with the most recent knowledge distillation methods such as ABKD, DistillM‑2, or SuperCorrect.\n\n- Certain references (e.g., lines 54, 352, 952) are missing the publication year."}, "questions": {"value": "See weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HStvbv40kF", "forum": "Bxxdz07CDp", "replyto": "Bxxdz07CDp", "signatures": ["ICLR.cc/2026/Conference/Submission2840/Reviewer_GGqR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2840/Reviewer_GGqR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827556149, "cdate": 1761827556149, "tmdate": 1762916401153, "mdate": 1762916401153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The research introduces IOA (Identifier-Organizer-Adapter), which is a pedagogically inspired framework for knowledge distillation from large language models to smaller models. This framework brings educational theories (Bloom's Mastery Learning and Vygotsky's Zone of Proximal Development) to life in the form of a three-stage pipeline that helps to systematically identify knowledge gaps, organize curriculum progression, and adapt knowledge representations. The results obtained from several benchmarks clearly show the advantages of the proposed method over baseline distillation methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel pedagogical concepts that map educational principles (Bloom's Mastery Learning, Vygotsky's ZPD) to concrete distillation mechanisms that distinguish this work from other ad-hoc data synthesis approaches.\n\n2. Comprehensive experimental validation with extensive benchmarks via testing with multiple teacher and student models.\n\n3. Thorough ablation studies and appendices demonstrating each component contributes meaningfully with extended experiments like hyperparameter robustness, additional student models, and agentic tasks, which greatly support reproducibility.\n\n4. Honest limitations section acknowledging theoretical gaps and practical constraints"}, "weaknesses": {"value": "1. While the pedagogical inspiration is appealing, the paper lacks proper justification for why these specific educational principles should transfer to neural network learning, as humans learn through sparse, interactive experiences with semantic understanding while LLMs optimize loss surfaces through gradient descent over massive corpora. \n\n2. Critical hyperparameters (τ_gap=0.3, τ_high=0.9, τ_low=0.7, τ_dep=0.3, α=0.7, τ_ZPD=0.15, τ_mastery=0.9) appear empirically tuned rather than principled. The paper states these are \"empirically set\" but provides limited ablation on sensitivity. Figure 3 shows some robustness, but some clarifications are needed for justification. (for example, why is τ_mastery=0.9 optimal rather than 0.85 or 0.95?)\n\n3. No proof for the mastery-based progression converges in algorithm 1 (lines 11-13). The worst-case number of remedial iterations is not mentioned anywhere in the paper.\n\n4. The difficulty constraint (Equation 8) assumes \"P_S(k) represents the average difficulty level\" but performance ≠ difficulty. A model might also fail due to missing knowledge, not cognitive overload.\n\n5. A minor mistake in figure 1 caption: \"Anology\" → \"Analogy\""}, "questions": {"value": "1. What happens if the teacher LLM produces an inconsistent or incomplete knowledge hierarchy? How sensitive is IOA to decomposition quality?\n\n2. How does the framework handle cyclic knowledge dependencies in the graph if they ever happen?\n\n3. What happens with τ_mastery = 1.0 (perfect mastery required) or τ_ZPD = 0 (no difficulty control)?\n\n4. How does IOA perform when the teacher is only moderately stronger than the student (e.g., 7B teacher → 3B student)?\n\n5. What's the gain from the topological+ZPD curriculum over a simple easy-to-hard ordering by task difficulty or skill-based decomposition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ayNOFEawQ1", "forum": "Bxxdz07CDp", "replyto": "Bxxdz07CDp", "signatures": ["ICLR.cc/2026/Conference/Submission2840/Reviewer_EkXd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2840/Reviewer_EkXd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762744722209, "cdate": 1762744722209, "tmdate": 1762916400897, "mdate": 1762916400897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}