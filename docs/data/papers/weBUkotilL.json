{"id": "weBUkotilL", "number": 23024, "cdate": 1758338365056, "mdate": 1759896835509, "content": {"title": "Gaussian Motion Field for High-Performance Video Compression", "abstract": "Neural video representations have advanced video compression technologies, yet many remain decoding-heavy and struggle to model high-frequency motion. To this end, we introduce Gaussian Motion Field (GMF), a 2D Gaussian–Splatting video codec that represents each frame with a compact set of Gaussians updated by a learned motion field. By predicting per-Gaussian deformations for temporal interpolation, GMF reduces temporal redundancy and requires substantially less capacity than traditional methods that rely on keyframe compression and complex motion estimation. In contrast to NeRV-style models with deep convolutional upsampling, GMF integrates shallow MLPs with lightweight Gaussian representations for efficient decoding. This design yields high storage efficiency and extremely fast decoding: over 1,000 FPS on a single GPU, amounting to roughly a 50$\\times$ speedup over recent methods such as HiNeRV, while maintaining comparable visual quality.", "tldr": "", "keywords": ["Video Compression", "Representation Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/15e2f1a40fad13a717d06f52285f0ffcb0837fa6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Gaussian Motion Field (GMF) for video compression. GMF first learns several key frames using the 2D Gussian Splatting models. Then, it models the motion using a deformation prediction network, which predicts the variation of Gaussian attributes from the bidirectional reference key frames. Finally, 2D Gaussian primitives at arbitrary time index are obtained by applying the predicted deformation. GMF showcases a comparable compression efficiency to HiNeRV and a quite fast decoding speed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. GMF achieves impressive improvements on the processing speed.\n2. The overall idea is simple, and the paper is well-orgnaized."}, "weaknesses": {"value": "1. The rate-distortion performance of GMF has not been compared against state-of-the-art video compression models, e.g., NVRC and DCVC-RT [1]. HiNeRV (published in 2023) is a relatively outdated baseline. It is not convincing enough to achieve a RD performance which is just comparable to HiNeRV.\n2. Limited novelty: The proposed GMF seems just a simple extension of 2D Gaussian Splatting representation (GaussianImage) using the 4D reconstruction techniques proposed in 4DGS, lacking inspiring designs for the video compression task.\n3. This paper lacks a detailed analysis on the reasons for coding speed improvements. It is important to find out which factor is most important to acceleration. For example, does the acceleration mainly come from the shallow network of GMF, or are the improvements mainly contributed by the fast-rendering property of Gaussian Splatting?\n4. Introduction of how to building the Gaussian motion field (Section 3.3) is too brief. Clear definition of variables and technical details are abbreviated, making it difficult to reproduce the method. Please refer to Question 2.\n\n[1] Towards practical real-time neural video compression. CVPR 2025."}, "questions": {"value": "1. Has the time of training the GMF model been counted into the encoding time? Does the FPS in Table 2 correspond to only the frame rendering time? If so, what is the actual encoding/decoding time (i.e., including the time consumption of model compression and entropy coding) of GMF?\n2. $F_m$, $F^{\\text{fwd}}$, and $F^{\\text{bwd}}$ in Section 3.3 (line 215) have not been clearly defined. How to extract these features? How to aggregate them in the network?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y1NaJtvaoI", "forum": "weBUkotilL", "replyto": "weBUkotilL", "signatures": ["ICLR.cc/2026/Conference/Submission23024/Reviewer_Zgt9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23024/Reviewer_Zgt9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761715092200, "cdate": 1761715092200, "tmdate": 1762942482480, "mdate": 1762942482480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Gaussian Motion Field, a novel neural video compression method that combines 2D Gaussian representations with a lightweight motion field for efficient temporal interpolation. The approach aims to address limitations in both traditional codecs and recent implicit neural video representations by reducing spatial and temporal redundancy through Gaussian-based frame modeling and motion primitives. The authors demonstrate that GMF achieves decoding speeds exceeding 1,000 FPS—roughly 50× faster than recent methods like HiNeRV—while maintaining competitive visual quality and compression performance on standard datasets such as UVG and MCL-JCV."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Hybrid Representation: The combination of explicit 2D Gaussians for keyframes and an implicit motion field for dynamics is innovative and well-motivated. This hybrid design effectively balances complexity and effectively reduce bitrate\n2. Exceptional Decoding Speed: The reported decoding speed of over 1000 FPS is a significant improvement over existing neural video codecs and makes the method highly practical for real-time applications.\n3. Efficient Design Choices: The use of factorized motion planes and tile reuse demonstrates a thoughtful approach to model compression and efficiency."}, "weaknesses": {"value": "1. The experimental comparisons could be more comprehensive. While this paper primarily compares traditional codecs and neural representation-based codecs, its comparison with other GS-based video compression methods is not thorough. Particularly, the rate-distortion curves lack experimental results from relevant GS-based approaches (such as [1, 2, 3]). Including these comparisons would more comprehensively demonstrate the performance advantages of the proposed method.\n2. The presentation and methodology description in the paper still have room for improvement. For example, how are the 2D Gaussians in keyframes initialized, and how are the relevant initialization parameters selected?\n\n\nRef:\n\n[1] GaussianVideo: Efficient Video Representation and Compression by Gaussian Splatting (CVPRW2025)\n\n[2] An Exploration with Entropy Constrained 3D Gaussians for 2D Video Compression (ICLR2025)\n\n[3] GSVC: Efficient Video Representation and Compression Through 2D Gaussian Splatting (NOSSDAV 2025)"}, "questions": {"value": "1. Previous works have also utilized mechanisms like MLPs to achieve deformable Gaussians. What are the main differences between this work and those prior approaches?\n2. What is the bitrate composition of different components after encoding? \n3. In Sec 3.3 Blend Rendering, the authors said \"We spatially associate Gaussians whose centers, scales,\n and orientations fall within a predefined threshold and interpret their opacities as visibility scores\", could you give a more detailed description of the process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XPbEAic89V", "forum": "weBUkotilL", "replyto": "weBUkotilL", "signatures": ["ICLR.cc/2026/Conference/Submission23024/Reviewer_SKpf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23024/Reviewer_SKpf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729901127, "cdate": 1761729901127, "tmdate": 1762942482185, "mdate": 1762942482185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, Gaussian Motion Field (GMF) is proposed for representing video content efficiently. GMF utilizes 2D Gaussians together with a motion field that predicts the deformation of the 2D Gaussians at different time steps. GMF demonstrates fast decoding with high video reconstruction quality and good compression performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The method is novel. Applying 2D Gaussians with a lightweight implicit neural representation is an efficient way to represent video but has not yet been studied.\n- The results are promising. For example, it achieves performance comparable to SOTA INR methods for video compression, while providing significantly faster decoding."}, "weaknesses": {"value": "- Although GMF shows promising compression performance, the ablation study does not cover the compression techniques such as tile reuse, quantization, and the context model for entropy coding.\n- Providing more formal details on the method, for example in the form of equations, would be helpful for understanding.\n- The figures for qualitative comparison are low quality. It is hard to compare the proposed method with baselines of similar quality (e.g., HiNeRV)."}, "questions": {"value": "- Is the number of training iterations counted in frames or in whole videos?\n- Are the results of the baseline methods obtained by your own reproduction, or are they taken from other papers?\n- Why do the HiNeRV results in Figure 7 not match those in the original paper? (The lowest-rate point seems to have a bpp much higher than the one reported there.)\n- How is the number of parameters controlled for the video representation task?\n- How are the Gaussian parameters compressed?\n- While the model is optimized only for L1 loss, why is the MS-SSIM performance comparable to, or even better than, INR-based methods that are jointly optimized for L1 and SSIM/MS-SSIM?\n- Why is the compression performance of GMF much better than other methods on the Beauty video? A PSNR of 35.63 with only 3.86M parameters is actually much better than many SOTA methods such as VTM. The same observation holds for the Shake video.\n- What is the setting of HM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CHO1zduKqh", "forum": "weBUkotilL", "replyto": "weBUkotilL", "signatures": ["ICLR.cc/2026/Conference/Submission23024/Reviewer_KtBi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23024/Reviewer_KtBi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964599062, "cdate": 1761964599062, "tmdate": 1762942481831, "mdate": 1762942481831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Gaussian Motion Field (GMF), a novel hybrid approach for efficient video representation and reconstruction. The core idea is to combine the strengths of explicit and implicit representations: using 2D Gaussians as explicit primitives to represent static frame content, and an implicit neural motion field to model the dynamics. The authors evaluate GMF on standard video reconstruction tasks, comparing it against state-of-the-art methods like NeRV, HiNeRV, and others.  Results, both quantitative (e.g., PSNR, bitrate) and qualitative, demonstrate that GMF achieves reconstruction quality on par with or exceeding existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The fusion of explicit 2D Gaussians with an implicit motion field is a creative and well-justified design.  It effectively leverages the compactness and rendering speed of explicit graphics primitives with the flexibility and smoothness of neural implicit functions for motion modeling.\n\n(2) By emphasizing faster decoding speeds, the work addresses a critical practical bottleneck in learned video compression and representation.  This makes the proposed method highly relevant for real-time applications, which is a major contribution beyond just improving reconstruction metrics."}, "weaknesses": {"value": "(1) I think in the experimental section, the author should incorporate a comparison with the latest 3DGS-based method, which is also an implicit representation, to prove the effectiveness of this method.\n\n(2) For Figure 7, I hope the author can provide the BD-Rate and BD-PSNR metrics for quantitative comparison to precisely display the performance improvement values of this method compared to other methods. Furthermore, the author did not make a comparison with the latest video compression SOTA methods (DCVC-FM [1], DCVC-RT [2]).\n\n[1] Li, Jiahao, Bin Li, and Yan Lu. \"Neural video compression with feature modulation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[2] Jia, Zhaoyang, et al. \"Towards practical real-time neural video compression.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "478Gtwy1xE", "forum": "weBUkotilL", "replyto": "weBUkotilL", "signatures": ["ICLR.cc/2026/Conference/Submission23024/Reviewer_oJYU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23024/Reviewer_oJYU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979442743, "cdate": 1761979442743, "tmdate": 1762942481506, "mdate": 1762942481506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Gaussian Motion Field (GMF), a neural video codec that represents frames using 2D Gaussians and models temporal dynamics through a learned motion field. The method achieves over 1,000 FPS decoding speed (50× faster than HiNeRV) while maintaining comparable visual quality. Key innovations include bidirectional deformation prediction, motion primitive decomposition, and progressive training with tile reuse compression."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The speedup over HiNeRV is significant and addresses a real bottleneck in neural video compression, making the method more viable for practical deployment.\n2. The combination of explicit 2D Gaussians for spatial structure with implicit motion fields for temporal dynamics is intuitive and interesting\n3. The paper is well-written with effective visualizations that clearly communicate the technical approach."}, "weaknesses": {"value": "1. On MCL-JCV dataset, GMF significantly underperforms HiNeRV across most bitrates and the PSNR gap widens dramatically as bitrate decreases.\n2. The evaluation is insufficient for a video compression paper: Currently evaluated only on: UVG: 7 videos, 1920×1080, primarily static camera scenes and MCL-JCV. Standard practice in video compression research requires evaluation on: HEVC Common Test Conditions (CTC), especially class B~E,  which covers diverse scenarios.\n3. The paper claims in Table 2 to report \"encoding and decoding speeds\" with format \"X/Y FPS\". However, there are severe contradictions: Paper states: \"The averaged training time for each video is about 2 hours\" (Appendix A.4), if 32.0 FPS is encoding speed, then for a 600-frame video: 600/32.0 ≈ 18.5 seconds."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AtbvUYhS4l", "forum": "weBUkotilL", "replyto": "weBUkotilL", "signatures": ["ICLR.cc/2026/Conference/Submission23024/Reviewer_W7e7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23024/Reviewer_W7e7"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986854815, "cdate": 1761986854815, "tmdate": 1762942480838, "mdate": 1762942480838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}