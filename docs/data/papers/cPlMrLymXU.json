{"id": "cPlMrLymXU", "number": 7050, "cdate": 1758006055843, "mdate": 1759897875179, "content": {"title": "Unlocking the Power of Mixture-of-Experts for Task-Aware Time Series Analytics", "abstract": "Time Series Analysis is widely used in various real-world applications such as weather forecasting, financial fraud detection, imputation for missing data in IoT systems, and classification for action recognization. Mixture-of-Experts (MoE), as a powerful architecture, though demonstrating effectiveness in NLP, still falls short in adapting to versatile tasks in time series analytics due to its task-agnostic router and the lack of capability in modeling channel correlations. In this study, we propose a novel, general MoE-based time series framework called PatchMoE to support the intricate ``knowledge'' utilization for distinct tasks, thus task-aware. Based on the observation that hierarchical representations often vary across tasks, e.g., forecasting vs. classification, we propose a Recurrent Noisy Gating to utilize the hierarchical information in routing, thus obtaining task-sepcific capability. And the routing strategy is operated on time series tokens in both temporal and channel dimensions, and encouraged by a meticulously designed Temporal \\& Channel Load Balancing Loss to model the intricate temporal and channel correlations. Comprehensive experiments on five downstream tasks demonstrate the state-of-the-art performance of PatchMoE.", "tldr": "", "keywords": ["Time Series Analytics"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ffba85b8d71635184c40d544af22c179091d10bb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PatchMoE, a task-aware Mixture-of-Experts framework tailored for time series analysis. Unlike traditional MoE architectures, PatchMoE incorporates a Recurrent Noisy Gating mechanism to leverage hierarchical, task-specific representations across different layers. The routing process is guided by a Temporal & Channel Load Balancing Loss to model sparse and intricate correlations. The framework is designed to support multiple downstream tasks such as forecasting, anomaly detection, imputation, and classification. Experimental results on various datasets demonstrate PatchMoE’s superior ability to model task-specific temporal and channel relationships, showcasing its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper conducts extensive experiments on various tasks and datasets to verify the effectiveness of the proposed model.\n2. The paper is well-structured and easy to follow."}, "weaknesses": {"value": "1. Although the starting point of this paper is to achieve task-aware modeling, the proposed method is rather implicit: the CKA similarity varies across different tasks, so the model needs to consider multi-layer information during routing. I think that considering multi-layer information is not directly related to task awareness, and the paper does not provide any experiments showing whether the CKA similarity changes accordingly when using the proposed method.\n2. The proposed method contains many components, while the ablation study is too coarse to validate their effectiveness:\n - How is RNG-router compared with the original router in NLP using a simple linear layer?\n - Why use sampling in routing instead of directly taking topK?\n3. Some technical details are not so clear:\n 1. In Equation (3), is MSA applied in a channel-independent way?\n 2. In Equation (4), how is GRU cell applied? $(N \\times n)$ is merged with batch dimension or with latent dimension?\n 3. Despite being applied to temporal and channel separately, the original source of the balancing loss should be explicitly cited."}, "questions": {"value": "1. In my understanding, the proposed model is still channel-independent: 1. MSA is applied to each channel independently; 2.the RNG-based MOE only takes one token (and the same token in previous layers) as input. How does this capture cross-channel dependency?\n2. As the original motivation of MOE is to replace dense FFN layers for efficiency. How is the efficiency of patchMOE compared with dense layer and original MOE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UqD2qUhx3v", "forum": "cPlMrLymXU", "replyto": "cPlMrLymXU", "signatures": ["ICLR.cc/2026/Conference/Submission7050/Reviewer_Zbts"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7050/Reviewer_Zbts"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623257578, "cdate": 1761623257578, "tmdate": 1762919246199, "mdate": 1762919246199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PatchMoE, a novel Mixture-of-Experts (MoE) framework tailored for time series analysis. The key contributions are as follows:\n\n1.\tRecurrent Noisy Gating (RNG - Router): Employs hierarchical task-specific routing to explore cross-layer representational variances;\n2.\tTemporal & Channel Load Balancing Loss: Promotes sparse and balanced utilization of experts to capture intricate temporal and channel correlations;\n3.\tHybrid Design of Shared and Routed Experts: Facilitates the modeling of both general patterns and task-specific dynamics.\n\nBased on the mentioned characteristics, PatchMoE is a flexible and general architecture that effectively enhances the capabilities of the temporal Transformers."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**S1**\n  A recurrent noisy router, which is shared across layers and conditions routing on the representations of previous layers through a GRU, represents a judicious approach to integrating hierarchical information into the routing process. The paper explicitly states that during training, routing scores are sampled in the continuous score space via [specific method], and at inference, these scores are replaced by [specific method]. This strategy is well-suited for stabilizing top-gating.\n\n**S2**\n  The Temporal & Channel Load Balancing Loss is designed to prevent expert collapse and promote structured sparsity along the time and channel axes. This is a valuable objective when dealing with multivariate data, especially when maintaining a channel - independent backbone.\n\n**S3**\n  The combination of shared experts, which are responsible for general patterns, and routed experts, which are for specialization, aligns with the best practices in sparse Mixture - of - Experts (MoE) systems. This approach also aids in managing the trade-off between capacity and specialization."}, "weaknesses": {"value": "**W1**  \n   The paper appears to adopt non-overlapping patches by default. Nevertheless, previous research indicates that overlapping patches with stride can efficiently mitigate information loss and yield superior performance.  \n   - The authors are expected to elucidate the reasons for choosing non-overlapping patches, such as whether it is due to efficiency, model stability, or other factors.  \n   - Incorporating a comparative experiment (comparing overlapping and non-overlapping patches) would further enhance the work, thereby justifying this design decision.\n\n**W2**  \n - Provide the full training objective $ L _{\\text{total}} = L _{\\text{task}} + \\lambda L _{\\text{bal}} $, the values or schedules for $ \\lambda $, $ \\alpha $, $ \\beta $ per task/dataset, and a brief sensitivity study. This will clarify how strongly the balancing terms influence optimization across tasks.\n\n**W3**  \n   - Hidden state initialization and management: the router is shared across layers, but the initialization of $h _0$ per input, whether it is reset per sequence or carried across batches, and any truncation policy are not described.\n   - Task heads and losses for anomaly detection, imputation, and classification are not detailed in the main text, making it hard to attribute gains to the backbone versus the heads.\n\n**W4**  \n   Typos and table annotations (e.g., “Bond” should be “Bold”; “Routher” should be “Router”; “ETThm2” vs. “ETTm2”; “N^r” vs. ) and a few inconsistent variable names hamper readability."}, "questions": {"value": "See W1-W4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "whFdzONKjj", "forum": "cPlMrLymXU", "replyto": "cPlMrLymXU", "signatures": ["ICLR.cc/2026/Conference/Submission7050/Reviewer_kR3k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7050/Reviewer_kR3k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632519402, "cdate": 1761632519402, "tmdate": 1762919245733, "mdate": 1762919245733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PatchMoE, a Mixture-of-Experts (MoE) framework designed for task-aware time series analytics. Unlike traditional MoE models that are task-agnostic, PatchMoE introduces Recurrent Noisy Gating (RNG-Router) to incorporate hierarchical representations across layers, enabling task-specific routing decisions. It further introduces a Temporal & Channel Load Balancing Loss to capture both temporal and cross-channel correlations — a notable challenge in time-series contexts where dependencies exist in both dimensions. The model is evaluated comprehensively across five major tasks — forecasting, anomaly detection, imputation, and classification — showing consistent state-of-the-art (SOTA) performance across over 25 benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Joint Temporal-Channel Modeling.\nThe Temporal & Channel Load Balancing Loss is a thoughtful addition that effectively addresses channel independence in transformers. Empirical evidence shows significant gains in multivariate datasets (e.g., Electricity, Solar), indicating the model’s ability to capture sparse cross-channel correlations without sacrificing efficiency\n2. Comprehensive Evaluation.\nThe experiments span univariate and multivariate forecasting, anomaly detection, imputation, and classification — an unusually broad and rigorous evaluation suite. The model consistently outperforms strong baselines (iTransformer, PatchTST, TimeMixer++, etc.), demonstrating robustness across diverse domains and metrics"}, "weaknesses": {"value": "1. Limited Theoretical Analysis.\nWhile empirical performance is strong, the paper lacks formal analysis of why RNG-Router improves routing stability or how the Temporal-Channel loss optimizes sparsity theoretically. A deeper exploration (e.g., gradient dynamics or convergence properties) would improve the conceptual depth.\n\n2. Efficiency and Scalability Discussion Missing.\nAlthough the authors mention model efficiency, they do not quantify computational overhead compared to PatchTST or iTransformer. Given the recurrent gating and dual balancing loss, it is unclear how well PatchMoE scales to extremely long sequences or high-channel datasets beyond those tested.\n\n3. Limited Interpretability Discussion.\nWhile MoE offers modular interpretability potential (expert specialization), the paper does not analyze expert behaviors or visualizations of expert routing distributions, which would strengthen the understanding of task-specific routing."}, "questions": {"value": "1. How sensitive is PatchMoE to the number of experts (Nr) and routing sparsity (Top-k)?\n2. Does RNG-Router generalize across tasks (e.g., trained jointly vs. task-specific fine-tuning)?\n3. What's the impact of the different treatment of eq (7) during training and inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SGlVabntFd", "forum": "cPlMrLymXU", "replyto": "cPlMrLymXU", "signatures": ["ICLR.cc/2026/Conference/Submission7050/Reviewer_Pee9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7050/Reviewer_Pee9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922228294, "cdate": 1761922228294, "tmdate": 1762919245353, "mdate": 1762919245353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PatchMoE, a Mixture-of-Experts (MoE) framework tailored for task-aware time series analytics. Unlike existing MoE architectures designed for text or vision, PatchMoE introduces a Recurrent Noisy Gating (RNG-Router) that dynamically leverages hierarchical layer representations to capture task-specific characteristics across various time series tasks (forecasting, anomaly detection, imputation, classification). Additionally, it incorporates a Temporal & Channel Load Balancing Loss to model sparse correlations and maintain diversity in expert routing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Proposes a **unified and generalizable MoE framework** for multiple time-series tasks.\n- Introduces a **Recurrent Noisy Gating (RNG-Router)** to adapt routing based on hierarchical representations, improving task specificity.\n- **Temporal & Channel Load Balancing Loss** effectively addresses the lack of channel correlation modeling in standard Transformers.\n- Demonstrates **strong empirical results** across diverse benchmarks (forecasting, anomaly detection, imputation, classification).\n- Clear ablation studies validating the contribution of each component (RNG-Router, shared experts, load-balancing loss).\n- Comprehensive comparisons with modern baselines (e.g., TimeMixer, Time-MoE)."}, "weaknesses": {"value": "- [1] **Incremental novelty despite strong performance**\n  - The architectural contributions, though well-motivated, largely build on known MoE principles.\n  - The introduction of a recurrent router and balancing loss feels like a careful **engineering enhancement** rather than a fundamental conceptual advance .\n- [2] **Performance heterogeneity across metrics and tasks**\n  - While PatchMoE achieves SOTA in many tables (e.g., Table 1–3), **relative improvements vary widely** across datasets and evaluation criteria.\n  - For instance, it shows **clear superiority in reconstruction-oriented metrics (MSE, MAE)** but **smaller or inconsistent gains** in discriminative metrics such as accuracy and F1.\n  - **In some anomaly detection datasets (e.g., MSL, NYC)**, the margins over CATCH are marginal or reversed in AUC  .\n\n\n- [3] **Overcomplex design vs. marginal efficiency analysis**\n  - The proposed architecture involves **multiple MoE layers with dual experts (shared + routed)** and an RNG mechanism for every layer, potentially increasing training cost.\n  - However, there is **no quantitative discussion of training or inference efficiency** compared to single-expert Transformers like iTransformer or PatchTST.\n\n- [4] **Lack of analysis on why it works well**\n  - Although the paper provides ablations (Table 13) and visualization of routing weights (Figure 6), these analyses focus only on component removal or qualitative trends."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SF09YPxOSG", "forum": "cPlMrLymXU", "replyto": "cPlMrLymXU", "signatures": ["ICLR.cc/2026/Conference/Submission7050/Reviewer_nh9u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7050/Reviewer_nh9u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974039387, "cdate": 1761974039387, "tmdate": 1762919244905, "mdate": 1762919244905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}