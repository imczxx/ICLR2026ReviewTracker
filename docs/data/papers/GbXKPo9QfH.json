{"id": "GbXKPo9QfH", "number": 23711, "cdate": 1758347435091, "mdate": 1759896800411, "content": {"title": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures", "abstract": "Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: \\url{https://anonymous.4open.science/r/llm-jepa-0C6F/README.md}.", "tldr": "", "keywords": ["LLM", "JEPA", "fine-tuning", "pretraining"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/648ac56c53f512b2f67831d168a0935c2f1b1eb1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper, LLM-JEPA, brings the Joint-Embedding Predictive Architecture (JEPA) that was originally introduced in the ViT domain to LLMs (specifically, text-code generation LLMs). In the vision domain, JEPA self-supervisedly to learn to predict the representations of some target image patches, given some context image patches. The underlying assumption is that all patches represent the same thing. In this paper, LLM-JEPA self-supervisedly learn to predict the representations of the code, given prompt text, and the underlying assumption is that the prompt and the code represent the same thing. LLM-JEPA is done simply by adding additional predictor tokens and a prediction loss. The authors show that this method improves standard LLM metrics significantly without overfitting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tAlthough the JEPA idea is not new, bringing it to the LLM domain is novel.\n-\tThe authors acknowledge the underlying assumption of JEPA: they require multiple views of the *same* information as in text-code generation, but also go further to explore how it generalizes to QA tasks.\n-\tThe authors applied Loss Dropout and observed dual wins in saving compute and improving accuracy.\n-\tThe writing flows naturally (especially with the help of the natural questions), and is a pleasure to read. Having pseudo code, implementation (eg, Line 152) is a strong plus."}, "weaknesses": {"value": "-\tLine 161: recommend adding a simple diagram to illustrate how the Pred() is added to an LLM transformer. The description is a bit confusing: beyond the standard transformer, is there a separate prediction network? Are the predictors also generated in k auto-regressive loops or one-shot?\n-\tRecommend showing a visualization of the mask in Line 188. Readers can imagine this looks like two upper-triangular matrices along the diagonal, but adding a diagram makes it more intuitive.\n-\tTable 1 has no description in the main text. Does it intend to show failure cases?\n-\t(Minor) The citation style does not follow the ICLR 2026 template https://iclr.cc/Conferences/2026/AuthorGuide (i.e., xxx et al.)."}, "questions": {"value": "-\tIn Eq.1, recommend noting somewhere what XEnt() stands for. Typically, people just write CrossEntropy. Similarly, does NTP stand for next token prediction? Please note it somewhere.\n-\tIn Line 161, is the [PRED] added after the concatenated text-code sequence or in between? (as mentioned above, a diagram would simplify things here).\n-\tIn Line 166, could the authors elaborate on what it means to use the embedding of the last predictor token? Is it the hidden_state? \n-\tLine 170 typo: “lies in obtained” -> “lies in obtaining”?\n-\tIn Line 190, could the authors explain which two forward passes? What are the purposes of each two?\n-\tIn Figure 3, the blue curve is when k=0, but in Line 166, the authors say in this case the predictor is trivial. Does being trivial mean JEPA is turned off, or mean it is most effective?\n-\tIn Figure 3, right side, we observe that even the NTP finetuning loss is almost the same at stabilization (yellow/blue), but at evaluation time their accuracy in fact differs a lot (51 vs 71). This is interesting. Do the authors have a hypothesis for the reason?\n-\tLine 292: Are the wrong/missing/extra labels compared against the GT or any valid regex? The proposed method’s output at line 292 has two additional `{}`.\n-\tLine 373: using L2 norm degrades accuracy from 71% to 2%, which is surprising how sensitive it is to the distance metric. Is this number correct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PjWbjqDvDM", "forum": "GbXKPo9QfH", "replyto": "GbXKPo9QfH", "signatures": ["ICLR.cc/2026/Conference/Submission23711/Reviewer_iw7d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23711/Reviewer_iw7d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760736424463, "cdate": 1760736424463, "tmdate": 1762942778357, "mdate": 1762942778357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Additional Experiments and Analyses"}, "comment": {"value": "_Thank you for the constructive feedback from all reviewers. We conducted a series of additional experiments in response to the comments, and we summarize the key findings below._\n\n## Efficient Hyperparameter Search\n\nWe found that the optimal $k$ values consistently cluster around 0–1 or 3–4, and moreover, when the optimum does not occur at $k = 0$ or $k = 3$, the optimal $\\lambda$ is always the same $\\lambda$ that performs best at those two anchor points. This yields an efficient search strategy:\n\n1. Evaluate all $\\lambda$ values at $k \\in$ {0, 3} and identify the best $(\\lambda, k)$.\n2. From the best $(\\lambda, k)$, iteratively evaluate $(\\lambda, k + 1)$ until no further improvement is observed.\n\nUnder this scheme, the search cost is reduced from $N\\cdot M$ to $2N + O(1)$, where $N$ is the number of $\\lambda$ values and $M$ is the number of $k$ values. We verified that this procedure reliably recovers the optimal $(\\lambda, k)$ across all experiments reported in the paper.\n\n## Predictor Token Design \n\nFor $k > 0$, we append multiple predictor tokens: [PRED_1], …, [PRED_k], to the text and use the hidden state of [PRED_k] as the predicted representation. To better understand the mechanism, we examined whether performance gains come primarily from (i) the **number** of predictor tokens or (ii) the **variety** of distinct predictor tokens. We conducted ablations using identical predictor tokens, and the results (see table below) show only minor differences. This suggests that the dominant factor is simply increasing the number of prediction steps (FLOPs), rather than requiring distinct token embeddings.\n\nConfig | Distinct PRED | Same PRED\n-|-|-\nLlama 3.2 / GSM8K | 36.36 $\\pm$ 0.20 | 36.74 $\\pm$ 0.70\nOpenELM / SYNTH | 25.40 $\\pm$ 2.40 | 25.01 $\\pm$ 1.60\n\n## Standalone Linear Predictors\n\nAn alternative design is to replace the [PRED] tokens with a standalone linear predictor network. A linear predictor is sufficient because the mapping from `Text` to `Code` representations is nearly linear (Table 14, Fig. 3 left). However, in the fine-tuning regime this approach suffers from a **cold-start problem**—the linear head must be trained entirely from scratch. Empirically, a straightforward linear predictor yields consistently subpar performance compared to using [PRED] tokens (last column).\n\n$\\lambda=0.5$, linear | $\\lambda=1$, linear | $\\lambda=2$, linear | $\\lambda=1, k=1$\n-|-|-|-\n70.16$\\pm$1.87 | 67.93$\\pm$2.70 | 64.54$\\pm$4.99 | 71.46$\\pm$1.34\n\n## Representation Alignment Analysis\n\nFigure 6 shows that LLM-JEPA preserves and even improves the alignment of text representations, whereas standard fine-tuning disrupts this alignment. Intuitively, a well-aligned representation space should facilitate **extrapolation** and improve **generalization**. To test this hypothesis, we constructed a controlled dataset that maps `Text` of the form `“lines with a number repeated k or more times”` to the corresponding `Code` (regular expression) `“([0-9]){k,}”` for $k \\in [1, 9]$.\n\nWe measured the **minimum pairwise cosine similarity** among the representations. LLM-JEPA collapses these representations almost onto a single line (min cosine similarity $\\approx$ 0.995), outperforming both the base model and the regularly fine-tuned model. Moreover, the regularly fine-tuned model fails to generalize to the unseen case $k = 1$ (absent in the training data), whereas LLM-JEPA can generalize correctly. This supports the intuition that improved representation alignment leads to stronger extrapolation behavior. Note that `{1,}` is equivalent to `+`, but the regularly fine-tuned model failed both.\n\nConfig | LLM-JEPA | Base Model | Regular Fine-tuning\n-|-|-|-\nCosine similarity | 0.995$\\pm$0.002 | 0.914 | 0.698$\\pm$0.049\nAccuracy | 95.56$\\pm$6.09 | 0 | 88.89$\\pm$0.0\n\n## Exploring JEPA Loss Dropout Rates\n\nWe conducted additional experiments to study the effect of JEPA loss dropout rates. At extremely high dropout (1 – 0.0625), we observed that varying $\\lambda$ does not yield meaningful accuracy improvements. This suggests a simplified search strategy: **keep $\\lambda$ fixed**. Using this approach, we found that accuracy remains stable even at a dropout rate of 1 – 0.125, better than previously identified 1 – 0.25.\n\n1 - 0.5, $\\lambda=1$ | 1 - 0.25, $\\lambda=1$ | 1 - 0.125, $\\lambda=1$ | 1 - 0.0625, $\\lambda=1$ | 1 - 0.0625, $\\lambda=2$ | 1 - 0.0625, $\\lambda=0.5$\n-|-|-|-|-|-\n73.42$\\pm$1.00 | 73.31$\\pm$0.76 | 73.20$\\pm$0.46 | 71.53$\\pm$1.27 | 70.68$\\pm$1.06 | 70.63$\\pm$1.17\n\n## Averaging Representations\n\nWe performed an additional ablation in which we replaced the final hidden state (hidden\\_state[-1]) with the average of all hidden states. This modification degraded performance.\n\nAveraging Representation | hidden\\_state[-1]\n-|-\n65.46$\\pm$3.51 | 71.46$\\pm$1.34"}}, "id": "nzndvEMsOU", "forum": "GbXKPo9QfH", "replyto": "GbXKPo9QfH", "signatures": ["ICLR.cc/2026/Conference/Submission23711/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23711/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23711/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763358560299, "cdate": 1763358560299, "tmdate": 1763358560299, "mdate": 1763358560299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LLM-JEPA, a novel training objective that brings Joint Embedding Predictive Architectures (JEPA) into large language models. By combining the standard autoregressive next-token prediction loss with a JEPA-style embedding prediction loss, the model improves representation quality while maintaining generative capability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work systematically introduces JEPA into LLM training for the first time, bridging a long-standing gap between vision and language representation learning.\n\n2. Demonstrates consistent empirical improvements across diverse architectures, scales, and datasets.\n\n3. Provides clear analysis of embedding structure (t-SNE, SVD), showing how JEPA regularizes the representation space.\n\n4. Maintains generation ability while enhancing abstraction and generalization."}, "weaknesses": {"value": "1. Training cost is substantially higher due to multiple forward passes (≈2× compute), as the authors acknowledge.\n\n2. Hyperparameter tuning (λ, k) appears expensive and unstable across settings.\n\n3. The approach depends on multi-view data (e.g., text–code pairs), limiting applicability to generic text-only corpora.\n\n4. The paper could discuss more about how JEPA objectives affect downstream reasoning or interpretability beyond accuracy metrics.\n\n5. It would be interesting to see whether JEPA acts more like regularization or representation alignment—the current discussion is mostly empirical."}, "questions": {"value": "1. Could JEPA be extended to pure text tasks using data augmentation (e.g., paraphrase or Q–A pairs)?\n\n2. How sensitive is the method to the choice of distance metric (cosine vs. L2)?\n\n3. During large-scale pretraining, how do you plan to manage compute cost — would loss dropout scale effectively to billions of tokens?\n\n4. Did you observe any degradation in generation diversity or fluency due to the embedding-space constraint?\n\n5. Can the JEPA loss be interpreted as encouraging semantic disentanglement or latent structure emergence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1SMZg6W2FL", "forum": "GbXKPo9QfH", "replyto": "GbXKPo9QfH", "signatures": ["ICLR.cc/2026/Conference/Submission23711/Reviewer_waki"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23711/Reviewer_waki"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761527411056, "cdate": 1761527411056, "tmdate": 1762942777933, "mdate": 1762942777933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LLM-JEPA, a method that augments standard autoregressive LLM training (next-token prediction) with a JEPA-style embedding-space objective that forces embeddings of two views to align via a predictor network. The JEPA term is added to the generative cross-entropy loss with a balancing weight λ, and the predictor is implemented by appending small numbers of special [PRED] tokens that reuse model weights. The authors implement a custom attention mask to obtain separate view embeddings efficiently, introduce random JEPA-loss dropout to reduce compute overhead, and provide a thorough empirical study across multiple model families (Llama3, Gemma2, OpenELM, OLMo). They show consistent accuracy gains, representation improvements (t-SNE, SVD, near-linear mapping from text to code embeddings), and robustness to overfitting; they also provide ablations on loss choices, predictor placement, and loss dropout."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The formulation (LLLM + λ · JEPA) and practical predictor design (tied weights via [PRED] tokens) are simple, elegant, and directly applicable to existing transformer models without architectural surgery.\n2. Results include full fine-tuning and LoRA, pretraining experiments, several datasets (including code and QA tasks), multiple model sizes, statistical significance testing (five seeds), and useful visualizations (t-SNE, SVD) that support the claimed representational benefits."}, "weaknesses": {"value": "1. Most experiments are fine-tuning or pretraining on relatively small/targeted corpora. While dropout amortizes cost, it remains unclear how LLM-JEPA behaves (performance, wall-clock, memory) at large pretraining scales used for modern foundation models.\n2. The paper presents empirical evidence (SVD, near-linear mapping), but lacks deeper theoretical analysis or causal ablations that would clarify when and why the embedding alignment improves downstream generation and reasoning"}, "questions": {"value": "1. The paper reports grid search results and suggests keeping λ·(1−α) constant with dropout, but could you provide concrete heuristics or automated tuning strategies? How sensitive are gains to λ when moving to very different view types or sequence lengths?\n2. The method uses the last-token hidden state as the embedding and appends [PRED] tokens as a tied predictor. Have you tried averaging representations, CLS-like tokens, multilayer predictors, or decoupled predictors? How robust are the results to these choices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sqPyaVZSl1", "forum": "GbXKPo9QfH", "replyto": "GbXKPo9QfH", "signatures": ["ICLR.cc/2026/Conference/Submission23711/Reviewer_qoq5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23711/Reviewer_qoq5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806338066, "cdate": 1761806338066, "tmdate": 1762942777737, "mdate": 1762942777737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel training objective $\\mathcal{L}_{\\mathrm{LLM-JEPA}}$ for LLM training, aiming at introducing the successful JEPA insights, i.e., directly predicting from embedding space to absorb abstraction knowledge, from vision into the LLM studies, which mainly rely on the reconstruction ability on token level. LLM-JEPA recognizes the naturally paired data (e.g., text & code or Q & A in this paper) as different views of the same thing. The objective is to predict from the embeddings of one view to the embeddings of another view. Experiments show the performances of LLM-JEPA on both the reconstruction ability and predictability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of introducing JEPA from vision to language is novel. Compared to the representation learning in LLM, which mainly focuses on obtaining good embeddings, LLM-JEPA doesn’t neglect its generative capabilities, and experiments show that the additional JPEA loss helps both pretraining and finetuning.\n- I like the analysis in the experiment part, including showing that next-token prediction loss cannot optimize the JPEA objective and visualizing the t-SNE and the singular value to verify the strengths of LLM-JEPA."}, "weaknesses": {"value": "- Though efficient, I doubt the usage of this method, especially as its strong dependency on multi-view data, which may limit the application to real-world industrial LLMs. Do you have some blueprints of the future work for more universal scenarios (e.g., unsupervised pretraining via data augmentation on texts that don’t have paired views)?\n- Another weakness, based on my understanding, is the additional training complexity and hyperparameter search cost. From Fig. 7 in appendix, there is no clear law of the best $(\\lambda, k)$ combination, which is also mentioned in the limitation part."}, "questions": {"value": "- For the predictor, you append $k$ `[PRED]` tokens to reuse the model’s weights. Can you elaborate more on the design intuition of this setting, especially when $k>0$? Why sometimes a larger $k$ would be better?\n- In Table 3, I find that InfoNCE surprisingly performs much worse than metrics. Do you have some explanations on this? Does it just mean the predictability is more crucial than the contrastiveness in this task?\n- In this paper, both the text and code encoders are initiated with a same LLM with same weights. In prior practice in JPEA [1], the target encoder is an EMA of the context encoder weights, which can enhance the training stability. Can the EMA design be applied to LLM-JPEA?\n\n## References\n\n[1] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. CVPR 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3tyHy3HKxB", "forum": "GbXKPo9QfH", "replyto": "GbXKPo9QfH", "signatures": ["ICLR.cc/2026/Conference/Submission23711/Reviewer_j7Db"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23711/Reviewer_j7Db"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915881063, "cdate": 1761915881063, "tmdate": 1762942777491, "mdate": 1762942777491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}