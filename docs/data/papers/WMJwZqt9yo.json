{"id": "WMJwZqt9yo", "number": 22494, "cdate": 1758331830044, "mdate": 1759896862736, "content": {"title": "Unsupervised discovery of the shared and private geometry in multi-view data", "abstract": "Studying complex real-world phenomena often involves data from multiple views (e.g. sensor modalities or brain regions), each capturing different\n\taspects of the underlying system. Within neuroscience, there\n\tis growing interest in large-scale simultaneous recordings across multiple\n\tbrain regions. Understanding the relationship between views (e.g., the neural\n\tactivity in each region recorded) can reveal fundamental insights\n\tinto each view and the system as a whole. However, existing methods to\n\tcharacterize such relationships lack the expressivity required to\n\tcapture nonlinear relationships, describe only shared sources\n\tof variance, or discard geometric information\n\tthat is crucial to drawing insights from data. Here, we present SPLICE: a neural network-based method that infers disentangled,\n\tinterpretable representations of private and shared latent variables from\n\tpaired samples of high-dimensional views. Compared to competing methods, we\ndemonstrate that SPLICE **1)** disentangles shared and private\nrepresentations more effectively, **2)** yields more interpretable\nrepresentations by preserving geometry, and **3)** is more robust to\nincorrect a priori estimates of latent dimensionality. We propose our approach as a general-purpose\nmethod for finding succinct and interpretable descriptions of paired data\nsets in terms of disentangled shared and private latent variables.", "tldr": "We develop a novel method for learning geometry-preserving representations of shared and private information from multiview data, demonstrating superior interpretability and disentangling than existing methods.", "keywords": ["manifold learning", "multi-view", "neuroscience", "neural network", "multi-region"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1c15422aabab3751e9c3174db83437921d82532.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SPLICE, a novel framework for learning disentangled and interpretable representations of shared and private latent variables from paired observations of high-dimensional data. \n\nThis is a fundamental problem with implications across multiple domains, from neuroscience to multimodal representation learning. The authors provide a conceptually elegant and theoretically motivated approach grounded in predictability minimization and manifold geometry preservation. They validate SPLICE on three complementary scenarios, demonstrating substantially superior performance to prior methods. \n\nThe paper is well-structured, methodologically coherent, and convincingly analyzed, offering both theoretical insight and practical utility."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The use of predictability minimization for enforcing first-order independence between shared and private latents is original and intuitively appealing, addressing information leakage issues in earlier methods.\n\nThe experiments are carefully designed to illustrate disentangling, interpretability, and robustness to latent dimensionality mis-specification.\nThe application to real neural data adds strong credibility and relevance.\n\nBeyond neuroscience, the framework provides a general solution for analyzing multiview high-dimensional data, potentially benefiting multiple research communities."}, "weaknesses": {"value": "1. While SPLICE is presented as a general framework, the actual performance is closely tied to specific neural architectures used for encoding, decoding, and measurement. It remains unclear whether the proposed disentangling principles will generalize across architectures or scales without extensive hyperparameter tuning. A discussion or an analysis of architectural sensitivity would strengthen the claims.\n\n2. While the approach is conceptually elegant, some arguments (e.g., the assumption that the full manifold is the cross-product of submanifolds) remain heuristic rather than theoretically justified.\n\n3. Preserving submanifold geometry improves interpretability but may come at a computational or robustness cost, particularly for noisy or large-scale data."}, "questions": {"value": "1. The proposed method relies on different neural architectures to learn different subproblems (shared, private, measurement). Given this dependence, to what extent can the theoretical ideas meaningfully guide the design of simpler or more scalable architectures?\n\n2. Given the adversarial training component, what specific stabilization strategies were most effective in preventing mode collapse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "coYyhmHtUd", "forum": "WMJwZqt9yo", "replyto": "WMJwZqt9yo", "signatures": ["ICLR.cc/2026/Conference/Submission22494/Reviewer_UzvA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22494/Reviewer_UzvA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761515616192, "cdate": 1761515616192, "tmdate": 1762942240800, "mdate": 1762942240800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method called SPLICE, which uses private and shared latent space to produce disentangled and interpretable representations. The proposed method also contains geometry identification and preservation components, which increases the fidelity of the latent space, leading to interpretable intrinsic sub-manifolds representations that preserves geometry. The proposed method is validated on controlled simulation datasets, and also is applied on neural datasets with different brain regions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors study the important problem of private/shared information disentanglement, which is a highly important problem in computational neuroscience.\n- The proposed geometry identification and preservation method is novel, and very relevant in the neuroscience domain. The produced latent geometry can help study small-scale datasets, potentially help with scientific discoveries. \n- The experimental designs are very detailed. Key ablations such as latent dimensions are carefully studied."}, "weaknesses": {"value": "- This work [1] seems highly relevant, which also proposes a private/shared latent space disentanglement method for neuroscience datasets. The authors should benchmark this work with the proposed work. The authors should also consider benchmarking with more advanced transformer architectures, such as [2, 3].\n\n[1] Liu, Ran, Mehdi Azabou, Max Dabagia, Chi-Heng Lin, Mohammad Gheshlaghi Azar, Keith Hengen, Michal Valko, and Eva Dyer. \"Drop, swap, and generate: A self-supervised approach for generating neural activity.\" Advances in neural information processing systems 34 (2021): 10587-10599.\n\n[2] Liu, Ran, Mehdi Azabou, Max Dabagia, Jingyun Xiao, and Eva Dyer. \"Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers.\" Advances in neural information processing systems 35 (2022): 2377-2391.\n\n[3] Chau, Geeling, Christopher Wang, Sabera Talukder, Vighnesh Subramaniam, Saraswati Soedarmadji, Yisong Yue, Boris Katz, and Andrei Barbu. \"Population transformer: Learning population-level representations of neural activity.\" ArXiv (2025): arXiv-2406.\n\n- The presented experiments are very small scale. The authors should consider include results from larger datasets."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LXdu4sRqKU", "forum": "WMJwZqt9yo", "replyto": "WMJwZqt9yo", "signatures": ["ICLR.cc/2026/Conference/Submission22494/Reviewer_R5mr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22494/Reviewer_R5mr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974299176, "cdate": 1761974299176, "tmdate": 1762942240525, "mdate": 1762942240525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "They propose a new deep learning method to reveal shared latent variables and separate latent variables across two datasets (or \"views\"). The architecture consists in two non-linear auto-encoders which swap parts of their latent space. In order to ensure that the private part of the latent does not leak shared information, they adapt an adversarial strategy proposed by (Schmidhuber 1992), predictability minimization, where a decoder aims at predicting the other dataset, while the global objective is to minimize the performance of that decoder."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- I find the method original yet simple, and well justified conceptually. I appreciate the elegant adaptation of the predictability minimization scheme to this problem.\n- The 3 experiments demonstrating the quality of the method and superiority over other methods (DMVAE and non-linear CCA) are fitting and convincing. The experiments are also nicely complementary, going from toy datasets to real neuroscience problems.\n- The writing is very clear for the most part (and some of my questions may have more to do with my own limited expertise than with clarity).\n- The results presented in experiments 2 and 3 are quite promising from the point of view of scientific discovery in neuroscience, and I believe that this architecture has the potential to be useful in many scientific applications beyond neuroscience."}, "weaknesses": {"value": "1. \"Using shared latents from one view to reconstruct the other view guarantees that private information does not leak into the shared latents\" => Not sure of that because the shared latent could as well contain other information discarded by the decoder. Please tone down claim if my assessment is correct.\n2. See Questions about robustness of the method (may not be weaknesses if properly addressed)\n3. Remark: Reading the intro I was missing many citations such as reduced-ranked regression (RRR), CCA, non-linear CCA such as Barlow Twins, CKA, SSL, equivariant SSL, but then found most of them in Discussion. It would be better to announce in Intro that a lot of references will be found in discussion, and at least cite RRR and CCA in intro (ideally applied to neuroscience datasets)."}, "questions": {"value": "1. On robustness (I): how many hyperparameters and how finetuned they need to be to the specific datatset? This information does not come through from reading the paper.\n2. On robustness (II): \"We use these measurement networks in an adversarial disentangling scheme: in predicting the opposite region’s observations as well as possible, the measurement networks try to exploit any shared information that has leaked into the private latents.\" => This requires careful implementation in my experience to avoid cyclic behaviors. Did you experience these cyclic behaviors? How did you mitigate them? Is your solution robust?\n3. I did not understand the rational for step 2 of the method (in particular Projecting onto Submanifolds). Please explain the rational more concisely.\n4. \"Importantly, SPLICE confined virtually all private latent variance to two dimensions\" => what regularization is responsible for that?\n5. LGN - V1 experiment: unclear to me what insights could we have gained from SPLICE here?\n\n\ntypo:\n-zˆA  sometimes has the small hat sometimes the big hat"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5bJnb7U6B1", "forum": "WMJwZqt9yo", "replyto": "WMJwZqt9yo", "signatures": ["ICLR.cc/2026/Conference/Submission22494/Reviewer_P8Fy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22494/Reviewer_P8Fy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762193324899, "cdate": 1762193324899, "tmdate": 1762942240295, "mdate": 1762942240295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a two-stage approach for unsupervised discovery of geometric structures in shared and private latent variables in multi-view data. The first stage uses an autoencoder while optimize for cross-latent (un-)predictability to induce disentanglement of shared and private latent variables. The second stage uses manifold learning technique to preserve geometric structures of data in latent space. Experiments are provided for rotated MNIST dataset, as well as synthetic and real data from neuroscience."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well written and easy to follow. The experiments showcase some synthetic and real scenarios where shared/private latent geometry is important but overlooked by the considered baselines, where the SPLICE outperforms. An application of the method in neural decoding is shown."}, "weaknesses": {"value": "1. My main concern lies in novelty of the paper. While the applications to shared-private latent modeling in multi-view settings might be new, the cross-reconstruction framework used in Step 1 uses well-known technique for the purpose of aligning/disentangling latent representations (Schmidhuber, 1992; Chen et al., 2021). The manifold learning technique in Step 2 is from existing literature (L-ISOMAP), and isometry is also a widely adopted prior for representation learning in existing literature.\n\n\n2. Missing discussions and experiments with related works:\n- An advantage of SPLICE is to retain geometric structure of data, by encouraging isometry between data space and latent space. This seems similar to an existing line of work on geometric structure preservation in disentanglement, e.g. (Gropp et al., 2020, Lee et al., 2022, Uscidda et al., 2025). However, there is no discussion or comparison of SPLICE with these methods.\n- Another advantage of SPLICE is to disentangle shared/private latents \"without a priori knowledge of latent dimensionality\" (line 81-83). Related works on this topic are missing, e.g., (Gui et al., 2025) showed that multi-modal contrastive learning adapts to intrinsic dimension of shared latent variables, or (Shrestha et al., 2025) where the authors aimed to tackle content-style learning with unknown latent dimensionality.\n\n\n3. While qualitative results are provided for Experiment 2, an extensive quantitative result (e.g., with R^2 metric) was not provided.\n\n**References**\n\nSchmidhuber, “Learning Factorial Codes by Predictability Minimization”, Neural Computation, 1992.\n\nChen et al., “Exploring Simple Siamese Representation Learning”, CVPR, 2021.\n\nGropp et al., “Isometric Autoencoders”, arXiv:2006.09289, 2020.\n\nLee et al., “Regularized Autoencoders for Isometric Representation Learning”, ICLR, 2022.\n\nUscidda et al., “Disentangled Representation Learning with the Gromov-Monge Gap”, ICLR, 2025.\n\nShrestha et al., “Content-Style Learning from Unaligned Domains: Identifiability under Unknown Latent Dimensions”, ICLR, 2025.\n\nGui et al., “Multi-modal Contrastive Learning Adapts to Intrinsic Dimensions of Shared Latent Variables”, arXiv:2505.12473, 2025."}, "questions": {"value": "1. Since SPLICE is a two-step procedure, which step is mainly responsible for the reported good performance in the experiments? What would happen if another method for disentangling shared-private latent variables is used for Step 1, together with L-ISOMAP in Step 2?\n2. Did the authors try other manifold learning methods in place of L-ISOMAP for Step 2?\n3. What are other potential applications for SPLICE, besides neuroscience?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Oorn2shn8U", "forum": "WMJwZqt9yo", "replyto": "WMJwZqt9yo", "signatures": ["ICLR.cc/2026/Conference/Submission22494/Reviewer_NoRJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22494/Reviewer_NoRJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762366555494, "cdate": 1762366555494, "tmdate": 1762942239945, "mdate": 1762942239945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript presents SPLICE, a two-step neural network approach for unsupervised disentanglement in multi-view data, aiming to infer interpretable, non-linearly mixed shared and private latent representations while preserving their intrinsic submanifold geometry. The method first employs a crossed autoencoder and the predictability minimization objective for effective disentangling, and then refines the latents using a geometry-preserving loss derived from estimated geodesic distances along the submanifolds (calculated efficiently using landmarks). Evaluated on simulated CV data (rotated MNIST and SPRITES), simulated neural activity, and a real-world neurophysiological dataset, SPLICE yields interpretable representations, and exhibits robustness to mis-specified latent dimensionality compared to state-of-the-art disentangling and shared-only methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Disentanglements include robustness to misspecification of latent dimensions as well as the loss regularization parameter $\\lambda_{geo}$.\n- Disentanglements show clearer separation compared to baselines and superior interpretability via geometry preservation.\n- Rigorous experimental analysis spans well defined simulations covering rotated MNIST, SPRITES as well as simulated and real neural spiking datasets."}, "weaknesses": {"value": "- Computational Efficiency:\n  - How does the runtime of SPLICE compare to the other baseline approaches?\n- Clarity:\n  - The methodology in Step 2 - Geometry Identification and Preservation -  requires clarification.\n  - Which method did you use to estimate the nearest neighbor graph?\n- Hyperparameter Determination and Stability:\n  - Clarify the rationale for taking multiple gradient steps for the measurement networks per autoencoder update, as mentioned in the manuscript. The hyperparameter for this is currently missing.\n  - Algorithms 1 and 2 suggest frequent resetting of the measurement networks. Please discuss the convergence stability for practitioners.\n  - State whether a fixed parameter set of $n_{msr}$ and $T_{restart}$ worked effectively across all datasets or if extensive tuning was required."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M53GEPrTcL", "forum": "WMJwZqt9yo", "replyto": "WMJwZqt9yo", "signatures": ["ICLR.cc/2026/Conference/Submission22494/Reviewer_k3Xn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22494/Reviewer_k3Xn"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission22494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762811401523, "cdate": 1762811401523, "tmdate": 1762942239698, "mdate": 1762942239698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}