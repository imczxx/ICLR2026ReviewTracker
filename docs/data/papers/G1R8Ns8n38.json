{"id": "G1R8Ns8n38", "number": 13213, "cdate": 1758215150645, "mdate": 1759897456133, "content": {"title": "Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO", "abstract": "We present a fine-grained theoretical analysis of the performance gap between reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) under a representation gap. Our study decomposes this gap into two sources: an explicit representation gap under exact optimization and an implicit representation gap under finite samples. In the exact optimization setting, we characterize how the relative capacities of the reward and policy model classes influence the final policy qualities. We show that RLHF, DPO, or online DPO can outperform one another depending on type of model mis-specifications. Notably, online DPO can outperform both RLHF and standard DPO when the reward and policy model classes are isomorphic and both mis-specified. In the approximate optimization setting, we provide a concrete construction where the ground-truth reward is implicitly sparse and show that RLHF requires significantly fewer samples than DPO to recover an effective reward model---highlighting a statistical advantage of two-stage learning. Together, these results provide a comprehensive understanding of the performance gap between RLHF and DPO under various settings, and offer practical insights into when each method is preferred.", "tldr": "We present a fine-grained theoretical analysis of the performance gap between RLHF and DPO under a representation gap.", "keywords": ["preference learning", "reward modeling", "RLHF", "DPO", "model mis-specification", "sparsity"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/747cba13c9de23d13fc7708259d3685fbb1f4cf1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a theoretical framework for understanding the performance differences between DPO, Online DPO, and two-stage RLHF through the lens of (reward and policy) model misspecification. The authors systematically analyze four scenarios based on whether misspecification occurs in the policy model, the reward model, both, or neither. The key contribution lies in establishing theoretical conditions under which DPO and Online DPO can outperform RLHF. By examining how different types of misspecification affect algorithm performance, the paper offers insights on algorithm selection tailored to specific optimization contexts. Through empirical experiments comparing DPO and pairwise PPO across each misspecification scenario, the results make connections with the theoretical contributions."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. **Addresses a significant problem in the RLHF community.** Understanding the gap between DAAs and RLHF is a crucial first step toward closing them and improving alignment algorithms in practice."}, "weaknesses": {"value": "1. **Writing requires substantial improvement.** i) Several equations overflow in the preliminaries and theoretical analysis sections. ii) Key notations (e.g., $\\rho$ for the regularized value function) are used without proper introduction. iii) : Section 3 should be restructured to highlight only the most significant theoretical contributions with in-depth analysis. Less critical conditions and propositions should be moved to the appendix. Focusing the main text on cases where DPO or Online DPO demonstrably outperform RLHF would strengthen the narrative.\n2. **Empirical analysis is insufficient** i)only GPT-2 as the base model, ii) single harmless dataset, iii) only pairwise PPO for baseline comparison, iv) Both DPO and PPO are sensitive to hyperparameter choices. The paper does not demonstrate sufficient effort in hyperparameter tuning, which is critical for fair comparison.\n3. **The claim that DPO outperforms RLHF is not rigorously established.** There is a fundamental difference in learning settings: DPO/(RLHF) is an offline/(online) algorithm. The underlying sampling distribution suggests that RLHF should set an upper bound for DPO performance in the absence of model misspecification. However, the authors claim an equivalence relationship between RLHF and DPO without rigorously defining or explaining this claim. The conditions under which this equivalence holds need clearer theoretical justification."}, "questions": {"value": "1. **How do you justify your experimental design for inducing model misspecification?** In your experiments, you freeze certain parts of the LLM or reward model to create misspecification scenarios. However, how can you determine that full parameter tuning does not itself introduce model misspecification? This concern is particularly salient given that you are using models with fewer than 1B parameters, where capacity limitations may inherently lead to misspecification even with full fine-tuning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IOZPTPVPVM", "forum": "G1R8Ns8n38", "replyto": "G1R8Ns8n38", "signatures": ["ICLR.cc/2026/Conference/Submission13213/Reviewer_NeaD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13213/Reviewer_NeaD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760777674695, "cdate": 1760777674695, "tmdate": 1762923904038, "mdate": 1762923904038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the relative strengths of DPO, Online-DPO, and RLHF under reward model misspecification , policy misspecification, or both. The theoretical analysis in this work both differs significantly from prior work and is potentially of more practical interest to practitioners. The authors show that with a misspecified reward model, DPO is superior to RLHF; with a misspecified policy model, RLHF is superior to DPO, and when both models are misspecified there exist environments where either method is better. The authors then connect these findings to statistical efficiency when learning from limited data, showing that RLHF can reduce the estimation error relative to DPO. Finally, the authors present some empirical evidence corroborating their theoretical analysis."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This is really great work. The paper is excellently written and easy to follow. The claims are novel, interesting, and rigorous. The paper clearly fills a gap in existing literature focused on characterizing the differences between DPO and RLHF."}, "weaknesses": {"value": "The empirical analysis could be more thorough, for example by evaluating with other preference datasets. I would also appreciate further analysis or explanation on the gap between DPO and RLHF in the right most plot of Figure 3 as this contradicts my intuition."}, "questions": {"value": "(Does not impact my score)\nThe authors pose the question “What key property enables a (surrogate) reward model to subsequently help learn good policies?” and then answer this question with Eq. 3. I do not quite follow the connection, can you clarify?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xiqU742q7I", "forum": "G1R8Ns8n38", "replyto": "G1R8Ns8n38", "signatures": ["ICLR.cc/2026/Conference/Submission13213/Reviewer_sYS3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13213/Reviewer_sYS3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772722126, "cdate": 1761772722126, "tmdate": 1762923903583, "mdate": 1762923903583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical comparison between RLHF and DPO under model mis-specification.\nIt formalizes their performance difference using a KL-regularized bandit value function and decomposes it into representation and statistical gaps.\nUnder exact optimization, RLHF or DPO can outperform each other depending on whether the reward or policy model is more expressive.\nUnder finite-sample approximation, RLHF enjoys better statistical efficiency through reward sparsity.\nHowever, the analysis relies on strong assumptions — realizability of the ground-truth reward, linear/tabular parameterization, and static function classes — which are unrealistic for large-scale LLM alignment.\nThe study also models preference learning as a single-step bandit problem, neglecting long-horizon, sequence-level dependencies in real LLMs.\nExperiments on the PKU-SafeRLHF dataset qualitatively support the theory but are limited in scope and benchmark diversity.\nOverall, the paper offers conceptual clarity on when RLHF or DPO might be preferable, but lacks practical validation under realistic LLM settings.\nIts strength lies in theoretical framing; its weakness lies in empirical and modeling real-world."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main strengths lie in its conceptual clarity and theoretical framing of alignment methods.\n\n1. It presents a unified mathematical framework comparing RLHF and DPO via a KL-regularized value formulation, clarifying their equivalence and divergence under model mis-specification.\n2. The analysis introduces a fine-grained decomposition of performance gaps—representation vs. statistical efficiency—which helps articulate the trade-offs between reward-first and direct optimization approaches.\n3. Controlled experiments on the PKU-SafeRLHF dataset qualitatively support the theory, showing alignment between the proposed theoretical taxonomy and empirical behavior."}, "weaknesses": {"value": "Despite its conceptual contributions, the work suffers from several limitations in practical realism and empirical grounding.\n\n1. The analysis relies heavily on strong and unrealistic assumptions such as realizability, linear parameterization, and static function classes, which do not reflect modern LLM alignment settings.\n2. Its sample-efficiency comparison is based on an artificial sparse-linear toy model (DTSP), limiting its external validity to large-scale, non-linear sequence models.\n3. Empirical validation is narrow—restricted to a single dataset and small-scale models—without evaluation on standard alignment benchmarks or real-world LLM deployments.\n4.  The writing is dense and not well organized, with frequent notational jumps and unclear figure references that obscure the main intuition behind the theoretical claims."}, "questions": {"value": "The paper presents intuitively interesting findings on how RLHF and DPO differ under model mis-specification, but the theoretical and empirical support currently feels limited.\n\n1. Could the authors explain why the experiments were conducted only on the PKU-SafeRLHF dataset? Was this choice due to computational constraints, or do the authors believe it sufficiently represents broader alignment scenarios?\n2. Are there any additional experimental results (e.g., on different preference datasets or model scales) that could further validate the proposed theoretical claims?\n3. The paper would benefit from clearer figures or visualizations that directly illustrate the theoretical insights (e.g., representation gap or sample-efficiency separation). Do the authors plan to provide additional figures or analyses that better connect the theory with experimental outcomes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "98pA8Fgzo6", "forum": "G1R8Ns8n38", "replyto": "G1R8Ns8n38", "signatures": ["ICLR.cc/2026/Conference/Submission13213/Reviewer_JrNc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13213/Reviewer_JrNc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999284326, "cdate": 1761999284326, "tmdate": 1762923903301, "mdate": 1762923903301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a comprehensive theoretical analysis of Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) under various settings, including both exact and approximate optimization. The authors compare RLHF and DPO under different types of misspecification, showing that RLHF is optimal under policy misspecification, while DPO is optimal under reward model misspecification. They further present empirical results to support and validate their theoretical findings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "a. This paper provides a theoretical analysis comparing RLHF and DPO under different model misspecification settings, a topic that has been rarely explored in the existing literature. The analysis offers valuable insights into the performance gap between RLHF and DPO.\n\nb. The paper also discusses two optimization scenarios, including both the ideal (exact) and practical (approximate) optimization settings. The authors provide a statistical analysis for both RLHF and DPO, contributing to a more comprehensive understanding."}, "weaknesses": {"value": "a. The paper appears to have been written somewhat hastily, and the presentation could be improved. For instance, in Section 2, the authors mention that RLHF proceeds in two stages but only describe the first (reward learning) stage, omitting the second (policy learning) stage. As a result, $\\pi_{\\textrm{RLHF}}$ is never formally defined, which makes it more difficult to follow the subsequent theorems.\n\nb. The experimental evaluation is limited to a relatively small model (GPT-2-774M). Given that current practice relies on much larger models, the validity and generality of the experimental verification are questionable.\n\nc. Although the paper discusses different misspecification settings, it does not analyze suboptimality with respect to the misspecification error. This weakens the understanding of the performance gap between RLHF and DPO.\n\nd. In the more important approximate optimization setting, the authors do not provide a theoretical analysis of the performance gap as they do in the exact optimization case, but instead rely only on an empirical proxy. A more concrete statistical characterization of the performance under approximate optimization would make the results stronger."}, "questions": {"value": "Please refer to the weaknesses section. I am open to increasing my score if the concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "guWh40tEgv", "forum": "G1R8Ns8n38", "replyto": "G1R8Ns8n38", "signatures": ["ICLR.cc/2026/Conference/Submission13213/Reviewer_uT9m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13213/Reviewer_uT9m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163745247, "cdate": 1762163745247, "tmdate": 1762923902840, "mdate": 1762923902840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}