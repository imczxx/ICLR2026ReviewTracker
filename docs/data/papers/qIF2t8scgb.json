{"id": "qIF2t8scgb", "number": 19410, "cdate": 1758296003261, "mdate": 1759897040570, "content": {"title": "Beyond Softmax: A Natural Parameterization for Categorical Random Variables", "abstract": "Latent categorical variables are frequently found in deep learning architectures. They can model actions in discrete reinforcement-learning environments, represent  categories in latent-variable models, or express relations in graph neural networks. Despite their widespread use, their discrete nature poses significant challenges to gradient-descent learning algorithms. While a substantial body of work has offered improved gradient estimation techniques, we take a complementary approach. Specifically, we: 1) revisit the ubiquitous _softmax_ function and demonstrate its limitations from an information-geometric perspective; 2) replace the _softmax_ with the _catnat_ function, a function composed by a sequence of hierarchical binary splits; we prove that this choice offers significant advantages to gradient descent due to the resulting diagonal Fisher Information Matrix. A rich set of experiments — including graph structure learning, variational autoencoders, and reinforcement learning — empirically show that the proposed function improves the learning efficiency and yields models characterized by consistently higher test performance. _Catnat_ is simple to implement and seamlessly integrates into existing codebases. Moreover, it remains compatible with standard training stabilization techniques and, as such, offers a better alternative to the _softmax_ function.", "tldr": "Motivated by information geometry, we propose catnat, a hierarchical alternative to softmax for parameterizing categorical random variables in ML. We demonstrate its effectiveness through both theoretical results and extensive empirical evidence.", "keywords": ["Categorical Random Variables", "Latent Random Variables", "Probabilistic Methods"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b50f69c0edcb47f7fcce69867523a80fdb1b66e4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The article studies the problem of learning latent representations of categorical variables. In particular, it examines the role of the softmax function, which maps a score vector to a valid categorical probability vector. The paper shows that this function leads to a dense Fisher Information Matrix (FIM) for a categorical random variable and argues that such dense matrices can introduce geometric distortions that may hinder gradient-based optimization.\n\nTo address this, the authors propose a new parameterized function, catnat, for learning categorical probability vectors. This function is designed to yield a diagonal FIM, potentially mitigating the aforementioned issues. Experimental results on three different tasks are presented to illustrate the performance of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of the paper are:\n1. The paper examines the widely used softmax function from an Information Geometry perspective, which provides an interesting theoretical viewpoint.\n\n2. A novel parameterized function (catnat) is proposed and analyzed as an alternative to softmax.\n\n3. The experimental results appear to demonstrate improved performance compared to the softmax baseline."}, "weaknesses": {"value": "1. The proposed function introduces additional hyperparameters, which may complicate training and tuning.\n\n2. The advantages and underlying mechanisms of the new function are not clearly articulated.\n\n3. The numerical results are difficult to interpret and do not strongly support the claimed benefits."}, "questions": {"value": "Overall, the paper is well written and offers an interesting new perspective on the softmax function. However, several aspects of the current version need clarification and improvement.\n\n1. Practical Relevance of the Approach:\n\nThe paper focuses on the softmax function and proposes a new parameterization for categorical random variables in the context of Information Geometry and Natural Gradient Descent. However, most modern machine learning and AI methods rely on standard gradient descent or stochastic gradient descent (SGD) due to the large scale of neural networks, where computing or inverting the FIM is infeasible. It is therefore unclear whether the proposed analysis and the new function are practically useful.\n\nHow does the catnat function perform when used with standard GD or SGD rather than natural gradients?\n\n2. Justification of Claims about FIM and Geometry:\n\nThe paper claims that a dense FIM introduces geometric distortions, but it does not adequately explain why this occurs or what these distortions entail. The rationale behind why a diagonal FIM would alleviate this issue is also unclear.\n\nFurthermore, since the activation functions a() and ν() used in catnat can take zero values, the resulting FIM could be singular.\nOverall, the motivation for the new function is not fully convincing.\n\n3. Hyperparameters and Computational Cost:\n\nSoftmax is simple, widely adopted, and free of hyperparameters. The proposed catnat function introduces parameters A and C, which must be tuned.\n\nThe paper should discuss the sensitivity of results to these hyperparameters and the computational cost compared to softmax, especially for large numbers of categories.\n\nThe computation of each probability entry in catnat involves multiple products of powers of activation functions, which could be expensive. A complexity analysis would be helpful.\n\n4. Experimental Details and Interpretation:\n\nSeveral details are missing or unclear in the numerical experiments:\n\n(i) In Table 2, information about the graph sizes and GNN architectures is missing.\n\n(ii) In Table 3, the reported negative log-likelihood values (e.g., 97.8 ± 0.2 vs. 97.5 ± 0.3) are very close, with overlapping error ranges. It is unclear whether such differences are statistically or practically significant.\n\nSimilarly, in Table 4, results such as 406 ± 34 vs. 398 ± 25 do not convincingly demonstrate improvement.\n\nOverall, the empirical results do not clearly establish that the proposed function provides a meaningful advantage over softmax, especially given the added complexity and tuning effort."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RgIlBGzROm", "forum": "qIF2t8scgb", "replyto": "qIF2t8scgb", "signatures": ["ICLR.cc/2026/Conference/Submission19410/Reviewer_qbc1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19410/Reviewer_qbc1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947315559, "cdate": 1761947315559, "tmdate": 1762931326876, "mdate": 1762931326876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors suggest an alternative to the Gumbel-softmax trick for the stochastic gradient estimator of the latent categorical random variables. The proposed catnat is an alternative parameterization of the softmax function, consists of sequential hierarchical binary splits, based on the concept called information geometry and natural gradient. The authors provide theoretical backup for the proposed method, and conduct wide range of experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The algorithmically proposed methodology makes sense.\n- The theoretical basis was also appropriately presented."}, "weaknesses": {"value": "- With the current shape of the paper, it is hard to understand how to apply the suggested method practically. Please provide the pseudo-code or algorithm of the proposed method.\n- The experiments are wide, but very shallow. It is very hard to tell whether the suggested method is superior to the baselines. Thorough experiments against various stochastic gradient estimators for the categorical random variables should be conducted."}, "questions": {"value": "- How can we implement the proposed method when num_category $\\neq 2^n$?\n- What is the computational complexity for the suggested method?\n- Also, the computation cost should be compared against various baselines. Why are all the baselines missing?\n- The following is missing baselines [1,2,3,4], at least for the VAE experiment.\n- How does the variance of the stochastic gradient differ from the baselines?\n\n[1] Mnih et al., Neural variational inference and learning in belief networks\n[2] Gu et al., Muprop: Unbiased backpropagation for stochastic neural networks\n[3] Tucker et al., REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models\n[4] Grathwohl et al., Backpropagation through the void: Optimizing control variates for black-box gradient estimation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ofSoS5CGQz", "forum": "qIF2t8scgb", "replyto": "qIF2t8scgb", "signatures": ["ICLR.cc/2026/Conference/Submission19410/Reviewer_4C1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19410/Reviewer_4C1Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762086894034, "cdate": 1762086894034, "tmdate": 1762931326413, "mdate": 1762931326413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a parameterisation of Categorical random variables aimed at inducing a diagonal Fisher information matrix, which, the paper argues, is poised to address many of the pitfalls of the typical softmax parameterisation. This works by parameterising the categorical probabilities as the result of a binary branching process (somewhat akin a stick breaking procedure).\n\nThe idea is tested on some latent variable modelling problems, though in rather simple settings of graph structure learning, categorical VAE, and an RL setting. Through different performance metrics in these tasks, the paper argues that the proposed approach (catnat) is better suited than softmax."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. I find the parameterisation elegant\n2. The paper is mostly rather clear"}, "weaknesses": {"value": "1. The paper argues that the proposed technique addresses challenges that countless other papers addressed by various other means, yet none of those, it seems, are relevant for the empirical section. I find this a big omission. Catnat is a novel parameterisation and, as far as the reader can tell, there's so much to be established: stability, computational tradeoffs, performance against other parametrisation aimed at similar issues, performance against reasonable gradient estimators, etc.\n\n2. The paper misses an entire body of literature on sparse parameterisations (I'm thinking of the sparsemax family) for which a lot of work on inducing structure has been published (e.g., entmax, sparsemap, etc.). I'm listing only a fraction of the relevant papers in this line of work below: \n\n- https://proceedings.mlr.press/v48/martins16.html\n- http://papers.neurips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf\n- https://proceedings.mlr.press/v80/niculae18a.html\n\nIt also misses other sparse parameterisations, for example those based on 'stretch and rectify' principle: \n\n- https://openreview.net/forum?id=H1Y8hhg0b\n- https://openreview.net/forum?id=WAid50QschI\n\nAre there worthwhile connections between these techniques and catnat? Do they address similar issues? Are they addressed by fundamentally different things? How do they fare in comparison, etc. The reader cannot tell.\n\n3. The paper claims the proposed parameterisation is poised to address problems of the softmax parameterisation, and that the essence of the solution is that the Fisher information matrix is diagonal, but the paper does not discuss why this is expected to matter for the stability of the techniques used to train LVMs in this paper (ie, policy gradient, PPO, and the gumbel-softmax relaxation). I don't think this can be left for the reader to infer, the paper has to spell out the arguments clearly.\n\nFor the reasons above my judgment of soundness is as is."}, "questions": {"value": "Other than the weaknesses above, I have a small question. \n\nDoes something like the stick breaking construction, which the paper also fails to acknowledge, induce a Fisher information matrix somewhat like catnat's?\n\nhttps://openreview.net/pdf?id=S1jmAotxg"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2bU6eIXkqu", "forum": "qIF2t8scgb", "replyto": "qIF2t8scgb", "signatures": ["ICLR.cc/2026/Conference/Submission19410/Reviewer_DU4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19410/Reviewer_DU4o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159707927, "cdate": 1762159707927, "tmdate": 1762931325923, "mdate": 1762931325923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}