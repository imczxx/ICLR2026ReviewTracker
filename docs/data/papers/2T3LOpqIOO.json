{"id": "2T3LOpqIOO", "number": 9907, "cdate": 1758148338295, "mdate": 1759897687703, "content": {"title": "Distributionally Robust Cooperative Multi-agent Reinforcement Learning with Value Factorization", "abstract": "Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution, where value-factorization methods enforce the individual-global-maximum (IGM) principle so that decentralized greedy actions recover the team-optimal joint action. However, the reliability of this recipe in real-world settings remains uncertain due to environmental uncertainties arising from the sim-to-real gap, model mismatch, system noise. We address this gap by introducing Distributionally robust IGM (DrIGM), a principle that requires each agent's robust greedy action to align with the robust team-optimal joint action. We show that DrIGM holds for a novel definition of robust individual action values, which is compatible with decentralized greedy execution and yields a provable robustness guarantee for the whole system. Building on this foundation, we derive DrIGM-compliant robust variants of existing value-factorization architectures (e.g., VDN/QMIX/QTRAN) that (i) train on robust Q-targets, (ii) preserve scalability, and (iii) integrate seamlessly with existing codebases without bespoke per-agent reward shaping. Empirically, on high-fidelity SustainGym simulators, our methods consistently improve out-of-distribution performances.", "tldr": "We develop distributionally robust cooperative MARL algorithms based on a novel value factorization principle that accounts for environmental uncertainty.", "keywords": ["distributionally robust RL", "cooperative multi-agent RL; Centralized training decentralized execution"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6bd320fae54e4a818696bbfa062567bd71a94af9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Distributionally Robust IGM (DrIGM) — a principled framework extending the classical Individual–Global–Max (IGM) principle in cooperative multi-agent reinforcement learning (MARL) to environments with distributional uncertainty.\nThe authors formalize DrIGM within the Dec-POMDP framework, proving that robust individual action-value functions can align with the robust joint action-value function under specific rectangular uncertainty sets.\nThey then instantiate DrIGM-based variants of VDN, QMIX, and QTRAN, design robust Bellman operators under ρ-contamination and total variation (TV) uncertainty models, and empirically evaluate the methods on the SustainGym HVAC control benchmark.\nResults show modest but consistent improvements over non-robust baselines and the existing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper formally defines DrIGM and provides a proof, logically connecting robustness with value factorization.\n\n2. DrIGM can be directly embedded into mainstream architectures such as VDN, QMIX, and QTRAN, allowing for the reuse of existing code in engineering projects."}, "weaknesses": {"value": "1. DrIGM is essentially equivalent to \"performing IGM under a global worst-case model.\" In other words, the paper does not propose any new algorithmic ideas fundamentally different from existing Distributionally robust RL or MARL value decomposition methods; it simply incorporates the \"worst-case model\" into the IGM framework. This point needs to be explicitly acknowledged in the manuscript, along with a discussion of its limitations and added value (e.g., in which situations is this worst-case model approach more advantageous than other robustness strategies).\n\n2. Example 1's presentation is particularly critical: the current description of the \"selection process for individual optimal actions and joint optimal actions\" is not detailed enough, making it difficult for readers to trace why taking the worst value for each agent individually leads to inconsistencies with the joint worst value.  The symbols and variables are quite messy. It is recommended to provide intuitive annotations or diagrams where they first appear. η(s, a) first appears on line 312, but is not explained until line 375, which disrupts readability and understanding of its role in the proposed framework.\n\n3. Only the SustainGym HVAC environment is used; no tests on SMAC, MPE, or adversarial settings.Baselines are too few (only GroupDR and non-robust variants).Reported gains are small (2–5%) and lack significance tests or ablation analysis (e.g., sensitivity to ρ)."}, "questions": {"value": "1. In line 189. The optimal joint action value should not produce eight values.Given there are only two scenarios, there should be two corresponding joint Q-values, not eight.\n\n2. In line 203. My understanding is that DrIGM should ensure that, under the worst-case environment, the optimal individual actions equal the globally optimal joint action.However, Example 1 instead shows that the worst individual actions do not equal the globally worst joint action, which corresponds to a different principle (worst-vs-worst, not best-under-worst).\n\n3. DrIGM is essentially equivalent to IGM under the worst-case model?\n\n\n4. Could you supplement this by reproducing it on at least one standard MARL benchmark (such as SMAC) and performing sensitivity/ablation analysis on ρ and providing a significance test?\n\n5. How does the DrIGM method compare to the non-robust version in terms of training time, sample complexity, and memory overhead? Please provide training curves and statistical cost (or complexity estimate)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "im5rmrCVvX", "forum": "2T3LOpqIOO", "replyto": "2T3LOpqIOO", "signatures": ["ICLR.cc/2026/Conference/Submission9907/Reviewer_pUrR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9907/Reviewer_pUrR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884785061, "cdate": 1761884785061, "tmdate": 1762921364575, "mdate": 1762921364575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical, often overlooked challenge in cooperative Multi-Agent Reinforcement Learning (MARL): ensuring reliable decentralized execution when the deployment environment deviates significantly from the training environment. The core paradigm in cooperative MARL is centralized training with decentralized execution (CTDE), relying on value factorization methods (like VDN/QMIX/QTRAN) and the Individual-Global-Maximum (IGM) principle. The authors demonstrate that naive robustification of individual Q-functions, similar to approaches in single-agent Distributionally Robust RL (DR-RL), breaks the IGM alignment needed for decentralized execution.\nTo fix this, the paper introduces the Distributionally Robust IGM (DrIGM) principle. DrIGM requires that robust individual greedy actions align with the robust team-optimal joint action. Theoretically, this is achieved by defining robust individual action values based on the global worst-case model ($P_{worst}$) for the joint value function, rather than assuming independent per-agent worst cases. Building on this robust factorization theory (Theorems 1, 2), the authors derive DrIGM-compliant algorithms compatible with VDN, QMIX, and QTRAN, training them using robust Bellman operators based on standard uncertainty sets (ρ-contamination and Total Variation). Experiments on high-fidelity SustainGym simulators, modeling HVAC control under climatic and seasonal shifts, show that the DrIGM-based methods consistently mitigate out-of-distribution performance degradation compared to non-robust and baseline robust MARL algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Originality\n\nPrevious value factorization methods rely entirely on the classical IGM principle to ensure that decentralized greedy actions recover the team-optimal joint action. The authors correctly identify that the reliability of this recipe is uncertain in real-world settings plagued by environmental uncertainties like the sim-to-real gap or system noise. Extending DR-RL techniques to cooperative MARL is fundamentally non-trivial because individual agents act on local histories but share a single, team-coupled reward.\n\nCrucially, the authors provide a concrete counterexample (Example 1) showing that simply adopting robust per-agent action value formulations from the single-agent DR-RL literature breaks the decentralized alignment required by IGM in the multi-agent cooperative setting. This realization is a key theoretical novelty. The derivation that DrIGM is guaranteed when robust individual value functions are defined with respect to the global worst-case joint action-value function is a principled solution to this misalignment problem. This approach as depicted in Eq. 5, ensures robustness while maintaining the decentralized execution structure of CTDE. Furthermore, the work successfully derives DrIGM-compliant robust variants of VDN, QMIX, and QTRAN, showing the broad applicability of the new principle to established architectures. This systematic framework for model uncertainty in the CTDE regime under partial observability is a substantial theoretical advance over related work which often targets Nash solutions, assumes full observability, or requires individual rewards.\n\n\n- Quality\n\nThe foundation of the work rests on solid theoretical results, specifically Theorems 1, 2, and 3. Theorem 1 formally establishes the sufficient condition under which DrIGM holds, tying the robust individual Q-function to the global worst-case model. Theorem 2 further proves that this DrIGM condition is compatible with the structural constraints of all three canonical value factorization methods: VDN (additive factorization), QMIX (monotonic factorization), and QTRAN (consistency constraints). This demonstrates the generality and foundational nature of DrIGM.\n\nThe use of Distributionally Robust Optimization (DRO) techniques is appropriate for modeling environmental uncertainty. The paper successfully derives corresponding robust Bellman operators for two standard uncertainty sets, ρ-contamination and Total Variation (TV), which are well-studied in the single-agent DR-RL literature. These operators are then integrated into the practical TD-loss formulation (Eqs. 14, 15) for training deep recurrent Q-networks (DRQN-style networks).\n\nEmpirically, the evaluation is strong, moving beyond typical simplified MARL benchmarks. The authors use the SustainGym benchmark focused on multi-agent HVAC control, which intrinsically involves stochastic dynamics, partial observability, and inter-agent coupling, making it a high-fidelity environment suitable for testing robustness. The evaluation protocol is designed explicitly to measure generalization under distribution shift, simulating realistic deployment scenarios where unseen configurations arise. The consistency of the results across multiple factorization architectures (VDN, QMIX, QTRAN) and both uncertainty sets (ρ-contamination and TV) provides compelling evidence that the DrIGM framework effectively mitigates performance degradation under climatic and seasonal shifts.\n\n\n- Clarity\n\nThe problem definition is clear. The introduction of DrIGM (Definition 2) is logically built upon the classical IGM principle (Definition 1), clearly showing the extension to the robust setting. The immediate inclusion of Example 1 serves as a powerful illustration, preventing ambiguity by clearly showing why naive application of single-agent DR-RL techniques fails in the cooperative domain. This preemptive clarification is helpful in understanding the rest of the paper.\n\n\n- Significance\n\nThe practical utility of MARL policies is often hampered by environmental uncertainty, which can cascade into coordination failures due to partial observability and inter-agent coupling. By introducing DrIGM, the paper provides a crucial theoretical mechanism to maintain decentralized execution reliability even under distribution shifts. The resulting algorithms integrate seamlessly with existing CTDE codebases (VDN, QMIX, QTRAN) without requiring bespoke per-agent reward shaping or changes to the decentralized execution mechanism. This makes DrIGM a readily adoptable approach for improving robustness in many existing CTDE systems."}, "weaknesses": {"value": "- Originality\n\nWhile the formulation of DrIGM is novel for cooperative MARL, the theoretical and algorithmic tools leveraged to derive the robust Bellman operators are essentially direct adaptations of established single-agent Distributionally Robust RL (DR-RL) techniques. Specifically, the paper relies on ρ-contamination and Total Variation (TV) uncertainty sets, which are standard in the robust MDP literature. The extension relies fundamentally on the assumption of a history-action rectangular uncertainty set P (Eq. 1). This rectangularity assumption is known to simplify the robust Bellman operator to maximize over actions before minimizing over the transition probability P. Although this is necessary for the current theoretical construction, relying on this rectangular structure limits the sophistication of the uncertainty models that can be handled by DrIGM.\n\n\n- Clarity\n\nthe precise implementation of the core DrIGM concept needs improvement in clarity for the algorithms section. The definition of DrIGM relies on the robust individual action value $Q_i^{rob}$ being derived from the value function under $P_{worst}(h, \\overline{a})$, where  $\\overline{a}$ is the robust optimal joint action. However, the resulting robust Bellman operators (Eqs. 7 and 9) replace the term $max_{a′} Q_{tot}^P (h′, a′)$ with $Q_{tot}^P (h′, \\overline{a}′)$, where  $\\overline{a}'$ is derived from the greedy robust individual actions $ \\overline{a}'_i  = argmax a'_i Q_i^{rob} (h'_i, a'_i)$. This is a crucial step that relies on the DrIGM principle to align execution."}, "questions": {"value": "1.  The experimental environment (HVAC control) uses continuous actions. Since the DrIGM principle (Definition 2) and the derived robust Bellman operators (Eqs. 7, 9) rely fundamentally on the argmax operation over the action space, how exactly was the robust joint greedy action calculated in the continuous action setting for VDN, QMIX, and QTRAN? Was the action space discretized, or were alternative continuous control techniques (like optimization or actor-critic methods) employed for the argmax step, and if so, how does that impact the theoretical guarantee of DrIGM?\n\n2. Theorem 1 relies on the existence of individual Q-functions under $P_{worst} (h, \\overline{a})$. In the robust TD loss for both ρ-contamination and TV uncertainty, the expectation is calculated only over the nominal transition model $P^0$ (or implicitly factored through η in the TV case). Can you elaborate on how the implicit selection of $P_{worst}$ is handled during the training updates via function approximation, especially given that $P_{worst}$ itself depends on the optimal robust joint action $\\overline{a}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8Dz0DALeSL", "forum": "2T3LOpqIOO", "replyto": "2T3LOpqIOO", "signatures": ["ICLR.cc/2026/Conference/Submission9907/Reviewer_Nb62"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9907/Reviewer_Nb62"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179202439, "cdate": 1762179202439, "tmdate": 1762921364295, "mdate": 1762921364295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to achieve robust multi-agent reinforcement learning value factorization under environmental transition kernel uncertainty. It proposes a novel history-based DrIGM principle and theoretically demonstrates that the proposed optimal robust joint action-value and the robust individual action-value satisfy the DrIGM principle. Accordingly, two new loss functions are designed to learn these robust action-values under two different uncertainty sets. The proposed approach is integrated with three typical value factorization methods, and its effectiveness is validated through experiments in high-fidelity SustainGym simulators."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Addressing environmental uncertainty is crucial for improving policy robustness in real-world applications. \n2. This paper introduces the DrIGM principle, a novel concept in cooperative Multi-Agent Reinforcement Learning.\n3. To realize the DrIGM principle, this paper proposes two loss functions that can be easily integrated into various value factorization algorithms."}, "weaknesses": {"value": "1. To achieve provable robustness guarantees, the test environment must be included within the uncertainty set. This requirement may limit the method's performance on unseen environmental models.\n2. There is a lack of sensitivity analysis regarding the algorithm's performance across different ranges of uncertainty. It remains unclear to what extent of uncertainty the algorithm can effectively handle.\n3. The experiments lack comparisons with important baseline algorithms, such as ERNIE [1], which addresses changing transition dynamics by adversarial regularization.\n\n[1] Robust multi-agent reinforcement learning via adversarial regularization: Theoretical foundation and stable algorithms. NIPS 2023"}, "questions": {"value": "1. As indicated in Reference [2], there exists a bias between Q(h,a) and Q(s,h,a). Could this bias compromise the validity of GrIGM?  \n2. Can the proposed algorithm effectively resolve the problem presented in Example 1?\n\n[2] On Stateful Value Factorization in Multi-Agent Reinforcement Learning. arXiv 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YIT0Fxxfk7", "forum": "2T3LOpqIOO", "replyto": "2T3LOpqIOO", "signatures": ["ICLR.cc/2026/Conference/Submission9907/Reviewer_FAoh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9907/Reviewer_FAoh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762401850107, "cdate": 1762401850107, "tmdate": 1762921363948, "mdate": 1762921363948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}