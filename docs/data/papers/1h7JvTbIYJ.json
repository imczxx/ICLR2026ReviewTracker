{"id": "1h7JvTbIYJ", "number": 9084, "cdate": 1758110026631, "mdate": 1759897744616, "content": {"title": "Context is the Key: Backdoor Attacks for In-Context Learning with Vision Transformers", "abstract": "Due to the high cost of training, practitioners often rely on pretrained large models (LMs) from untrusted sources, exposing them to backdoor risks. In-context learning enables LMs to perform tasks based on prompts, introducing new attack surfaces for dynamic and flexible backdoor attacks. We study backdoor attacks against Vision Transformers (ViTs) under in-context learning for image-to-image tasks. We demonstrate that ViTs trained with masked image modeling can be poisoned to exhibit highly flexible malicious behaviors. Our analysis combines different trigger injection methods (BadNets, WaNet, and Blended), malicious objectives (Denial of Service, identity mapping, and black-and-white conversion), attacker goals (source-specific vs. source-agnostic), and stealthiness variations, i.e., parameter space stealthiness. \nWe achieve significant attack effectiveness: up to $13\\times$ performance degradation in DoS tasks and high similarity scores on identity-mapping and conversion tasks. Using a parameter space attacks further improves the attack performance while grating stealthiness in both input and parameter spaces. In-context learning grants attackers diverse possibilities for injecting backdoors and launching malicious tasks, even with data distributions absent from training. We evaluate standard mitigation strategies, including prompt engineering, fine-tuning, and fine-pruning. These defenses are largely ineffective, e.g., fine-tuning only reduces performance degradation from 89.90\\% to 73.46\\%, or fine-pruning reduces the attack performance by 4\\% in cost of 28.5\\% clean performance degradation.\n\\footnote{Our code is available at~\\url{https://anonymous.4open.science/r/Inpainting-Backdoor}.}", "tldr": "ViTs can be backdoored to behave maliciously during in-context learning for image tasks (even unseen tasks). Attackers can poison pretrained models to cause up to 13× performance drops or unwanted image transformations when triggered.", "keywords": ["backdoor attacks", "in-context learning", "vision transformers"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/052d8cae6c3f998167b377cd1bc2bf00dca7c313.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies backdoor attacks on Vision Transformers (ViTs) used in in-context visual learning (MIM-style “generalist painter” setup). It adapts classic trigger mechanisms (BadNets, WaNet, Blended) to image-to-image tasks and defines two regimes: task-specific and task-agnostic backdoors. Malicious objectives include DoS (e.g., output a flat color), identity mapping, and black-and-white conversion. On a multi-task ViT pretrained across depth/segmentation/deraining/denoising/low-light, the authors report large degradations under attack (e.g., ≈13× for DoS, and big drops in mIoU for segmentation), while clean performance is only mildly affected in some settings. They also discuss a “parameter-space stealthiness” variant that prunes attention heads (via Lipschitz screening) to make TAC features less distinguishable, and they evaluate straightforward defenses (prompt tweaks, fine-tuning, fine-pruning), finding them largely ineffective (fine-tuning reduces a segmentation backdoor from ~89.8% to ~73.5% degradation)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- In-context learning opens a real attack surface for vision models; positioning backdoors in this setting (beyond standard classification) is worthwhile. \n\n- Clear taxonomy across trigger types, objectives (DoS / identity / grayscale), and activation scope (task-specific vs task-agnostic). The study also probes training order for multi-task poisoning and shows generalization to an out-of-domain task (FSS-1000).\n\n- Tables for clean/attack performance across several vision tasks and datasets; the “TASR” proxy (SSIM-based) is a reasonable attempt to score non-classification attacks."}, "weaknesses": {"value": "- The work is valuable empirically, but the core ideas are incremental; the paper lacks a new attack formulation, training objective, or detection/defense method to anchor novelty.\n- The “attacks” reuse standard trigger families (BadNets, WaNet, Blended) and objectives (DoS / identity / B&W) with no new optimization or injection mechanism—essentially porting known backdoors to image-to-image ICL\n- The task-specific vs. task-agnostic split is a taxonomy, not a technical contribution; it doesn’t yield a new algorithm beyond training with existing triggers on different task sets. \n- The “novel metrics” claim largely reduces to SSIM/PSNR-style scoring (TASR), which are conventional for continuous outputs; the paper doesn’t introduce a fundamentally new evaluation framework.\n- Only simple baselines are tried; no certified/detection-style defenses, no retraining-from-scratch baselines, no model ensembling or input randomization at inference, and no task-conditioned sanitization. The conclusion that defenses are “largely ineffective” is thus limited in generality. \n- The main assumption is white-box access to a generalist ViT followed by redistribution (e.g., via a model hub). There’s limited discussion of black-box or limited-access settings (e.g., poisoning only via small-scale fine-tuning, adapter-level attacks, or parameter-editing without full retraining). The practicality for typical downstream users of MIM-style “painter” models is under-motivated.\n- \\citet and \\citep are not properly used."}, "questions": {"value": "See weakness as above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pPOEW3O1z6", "forum": "1h7JvTbIYJ", "replyto": "1h7JvTbIYJ", "signatures": ["ICLR.cc/2026/Conference/Submission9084/Reviewer_KJ79"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9084/Reviewer_KJ79"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630416920, "cdate": 1761630416920, "tmdate": 1762920789469, "mdate": 1762920789469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on evaluating the effectiveness of training-time backdoor injection adversarial attacks on in-context learning of ViTs. The authors evaluate a variety of existing backdoor attacks, paired with different desired behaviors to be elicited from the model. These attacks are then used during training of in-context learning for various tasks, such as segmentation and denoising, in different combinations. An extensive set of experiments is conducted, evaluating the success of the attacks under each scenario, alongside a discussion of the results and hypotheses about the underlying causes."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The scenario of using backdoor poisoning attacks for in-context learning of ViTs is novel and worth exploring\n- The experiments are thorough, in line with the expectations of an investigation into a new application of existing methods"}, "weaknesses": {"value": "- The clarity of the writing and the organization of the paper could be significantly improved upon. I understand that the authors conducted extensive experiments and had much to discuss; however, the majority of the methodology of the paper is only described in the appendix. The main text consists mostly of lengthy sentences and very little mathematical notation, which would be instrumental in efficiently and accurately delivering concepts. If it is not possible to reorganize the paper, I believe a paper with such dense experimental results, such that the methodology does not fit within the main text, is better suited for a journal with a longer format.\n- A lesser weakness is that the novelty of the paper comes from the investigated scenario, and no significantly novel methodologies are introduced."}, "questions": {"value": "- As in line with the clarity concerns, my understanding is that throughout the experiments the authors first continue the training of the pretrained model while injecting backdoor attacks, then evaluate the model performance a): before training with the backdoor attacks b): after training with the backdoor attacks, but without inserting the triggers during evaluation, and c): while inserting the triggers. Is this correct? Regardless of the answer, I believe this should be succinctly mentioned at the beginning of the experiment section.\n- Continuing on the first point, many tables essentially have the same description (e.g., 1 & 2 or 3 & 4) but show different scenarios (I believe scenarios a) and b) in the first and c) in the second). The table descriptions should be updated to accurately reflect the presented results.\n- Why use the notation $\\Delta_{Acc}$ while it does not represent accuracy, but rather the percentage change? I believe this notation to be confusing, especially since the description is only provided in the appendix.\n- The format and contents of tables 7 & 8 are not properly explained; table 7 is not referred to in the text.\n- Line 69 states: “Backdoors can generalize across unrelated tasks—poisoning one task during training can affect different tasks at inference time, **a phenomenon impossible in traditional computer vision backdoors.**” As this is a substantial claim, authors should provide evidence or citations to works containing evidence of this claim. Otherwise, I believe it should be removed from the paper.\n- Line 801: Shouldn’t the mask in the loss function be inverted? For example, we want $\\mathcal{L}(\\tilde{x}\\cdot(1 - M), x\\cdot(1-M))$, as we want the pixels which were **masked in the input** to be **present in the output**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GGNKsc0WVG", "forum": "1h7JvTbIYJ", "replyto": "1h7JvTbIYJ", "signatures": ["ICLR.cc/2026/Conference/Submission9084/Reviewer_6VTZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9084/Reviewer_6VTZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787294586, "cdate": 1761787294586, "tmdate": 1762920789060, "mdate": 1762920789060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates backdoor attacks on ViTs trained with Masked Image Modeling and used for in-context learning in I2I tasks. The threat model exploits ViT's contextual adaptation to deploy malicious behaviors (DoS, identity mapping) on both seen and unseen tasks by adapting classical triggers (BadNets, WaNet). The backdoors achieve high attack success rates (up to 13x degradation) and are shown to be largely effective against existing defenses like fine-tuning and fine-pruning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is the first to study backdoor attacks on ViTs trained with MIM, and provides a good background on the problem setup"}, "weaknesses": {"value": "- Although the paper is the first to study backdoor attacks for in-context learning, it relies heavily on existing attack methods and does not offer any novel theoretical insights.\n- The writing needs to be thoroughly refined, as several core details (methods, metrics, and how they're formulated, background on MIM attacks, etc.) are deferred to the appendix, with a lot of redundant discussion in the main paper (mainly the results section). The methods and experiment sections talk about $\\alpha$, $\\delta$ and $\\epsilon$ without really explaining what they denote. The main paper should be sufficiently self-contained, with only extra experiments and details provided in the appendix.\n- (Minor) Most citations are formatted incorrectly."}, "questions": {"value": "- Why and how the contextual adaptation capabilities of ViTs make them susceptible to these attacks? Rather than simply showing that the vulnerability exists, it is more valuable to examine the source"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1jawy0oNY6", "forum": "1h7JvTbIYJ", "replyto": "1h7JvTbIYJ", "signatures": ["ICLR.cc/2026/Conference/Submission9084/Reviewer_EKY3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9084/Reviewer_EKY3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866145240, "cdate": 1761866145240, "tmdate": 1762920788615, "mdate": 1762920788615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that ViTs trained with masked-image-modeling and prompted via in-context learning can be backdoored by data poisoning so that a specific context (with a trigger) flips the model’s behavior."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper makes an interesting contribution by linking backdoor vulnerabilities to in-context learning mechanisms in vision transformers\n2. The experiments cover multiple vision tasks and settings"}, "weaknesses": {"value": "1. The paper does not clearly formalize its in-context learning setup. It also remains ambiguous whether the “context” involves any language components or purely visual cues, which weakens the conceptual grounding.\n\n2. Weak connection between MIM and in-context learning. The proposed link between masked image modeling (MIM) pretraining and in-context learning is unclear. It is difficult to see how model providers could realistically train a vision transformer using such an in-context input format.\n\n3. The experiments are confined to a single, large Vision Transformer and a particular multi-task MIM training recipe. It is unclear whether the proposed attack generalizes to other architectures or training regimes.\n\n4. Only prompt engineering and short fine-tuning are explored as defenses. No data sanitization, trigger synthesis search, activation clustering, spectral analysis, or parameter-space defense methods are examined. Hence, the conclusions about the difficulty of defense remain preliminary and incomplete.\n\n5. The organization and exposition are difficult to follow, with key methodological steps scattered and insufficiently motivated. This makes the overall argumentation hard to read and interpret."}, "questions": {"value": "Please refer to the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tz5XbBfUrZ", "forum": "1h7JvTbIYJ", "replyto": "1h7JvTbIYJ", "signatures": ["ICLR.cc/2026/Conference/Submission9084/Reviewer_q9yy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9084/Reviewer_q9yy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982500048, "cdate": 1761982500048, "tmdate": 1762920788075, "mdate": 1762920788075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}