{"id": "qBy7nYDgEa", "number": 25174, "cdate": 1758364964657, "mdate": 1759896731695, "content": {"title": "HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish", "abstract": "Fact-checking in code-mixed, low-resource languages such as Hinglish remains a significant and underexplored challenge in natural language processing. Existing fact-verification systems are primarily designed for high-resource, monolingual settings and fail to generalize to real-world political discourse in linguistically diverse regions like India. To address this gap, we introduce HiFACTMix, a novel\nbenchmark comprising approximately 1,500 real-world factual claims made by 28 Indian state Chief Ministers and several influential political leaders in Hinglish,each annotated with textual evidence and veracity labels (True, False, Partially True, Unverifiable). Building on this resource, we propose a Quantum-Enhanced Retrieval-Augmented Generation (RAG) framework that integrates code-mixed text encoding, evidence graph reasoning, and explanation generation. Experimental results show that HiFACTMix not only outperforms strong multilingual and code-mixed baselines (CM-BERT, VerT5erini, IndicBERT, mBERT) but also remains competitive against recent large language models, including GPT-4, LLaMA-2, and Mistral. Unlike generic LLMs that may generate fluent but weakly grounded outputs, HiFACTMix explanations are explicitly linked to retrieved evidence, ensuring both accuracy and transparency. This work opens a new direction for multilingual, quantum-assisted, and politically grounded fact verification, with implications for combating misinformation in low-resource, code-mixed environments.", "tldr": "We introduce HiFACT, a Hinglish political fact-checking benchmark and propose a quantum-enhanced RAG framework that improves accuracy and explanation quality in low-resource, code-mixed settings", "keywords": ["Hinglish", "Fact-checking", "Code-mixed languages", "Low-resource NLP", "Political discourse", "Quantum-enhanced RAG", "Evidence graph reasoning", "LLM explanations"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3d5cbd2326c0531ffbe51253f7a599c4d270f43.pdf", "supplementary_material": "/attachment/73807c834eb9a5841a4ebe41c0a2830c48efec85.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a code-mixed Hinglish fact checking dataset of 1500 political claims with evidence documents and veracity labels, as well as a new RAG framework for the task that outperforms various BERT and T5 baselines.\nPrevious automatic fact checking efforts do not specifically model code mixing, which is the core novelty of this paper."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Previous automatic fact checking efforts do not specifically model code mixing, which is the core novelty of this paper."}, "weaknesses": {"value": "- The dataset collection process is underspecified -- the paper states that \"claims were collected from diverse sources\", but the exact list of sources and how the claims were collected or selected is not specified. It is also not specified how the evidence documents are collected.\n- Similarly, the annotation process is underspecified -- the veracity labels and meta-data categories are listed, and it is stated that annotations are performed by multiple reviewers, but nothing related to annotator selection, training, annotation quality etc. is described\n- The method is only described in broad details -- Figure 2 shows the different components, and the various subsections of Section 4 provide references to methods which are re-used, but the description is not stand-alone, and crucial details such as how the models are trained are left out\n- The paper claims that the RAG method is \"quantum enhanced\", but it is not explained what that means and why it is specifically relevant for code mixing\n- There is an introduction of a simple user interface for a \"demonstration\" -- it is not clear what the relevance of this to the paper is. Is this being used to collect annotations? Have you evaluated it against other user interfaces?\n- Some references under \"recent LLMS\" in Section 5.3 are not resolving\n- The proposed model outperforms various BERT models and a T5 baseline, but not more recent models, though a core reason for that is likely because it is T5 based. Why did you not experiment with other LLMs are base models for your approach?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MdSi1XcxR7", "forum": "qBy7nYDgEa", "replyto": "qBy7nYDgEa", "signatures": ["ICLR.cc/2026/Conference/Submission25174/Reviewer_LJ8D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25174/Reviewer_LJ8D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830802143, "cdate": 1761830802143, "tmdate": 1762943351760, "mdate": 1762943351760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HiFACTMix, a benchmark dataset of approximately 1,500 annotated Hinglish (Hindi-English code-mixed) political claims, each paired with evidence and veracity labels. The authors propose a fact-checking pipeline that integrates code-mixed text encoding, graph-based reasoning, a 'quantum-enhanced' RAG module, and explanation generation. The system is evaluated against multilingual and code-mixed baselines as well as recent LLMs, reporting improvements in explanation quality and accuracy. The paper also discusses ethical considerations and provides a user-facing demo."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- This paper studies a unique problem, that is, fact-checking in code-mixed (Hinglish) political discourse.\n- This paper introduces a new annotated dataset (HiFACTMix) for Hinglish political claims, which could be useful."}, "weaknesses": {"value": "- The presentation quality is poor. For example, the authors copied and pasted Figure 1 from (Guo et al., 2022). Please consider making original figures and using them in the paper.\n- Key methodological details are missing or vague: the architecture, training pipeline, evidence graph construction, and integration of components are described only at a high level, with missing references and incomplete sections.\n- Annotation protocol and inter-annotator agreement statistics are not described, raising concerns about dataset quality.\n- Some references are speculative or incomplete, and several sections are unpolished (e.g., 'Section ??')."}, "questions": {"value": "- What are the annotation guidelines for the dataset, and what is the inter-annotator agreement for veracity and evidence alignment?\n- Can the authors provide a more robust evaluation of explanation quality (e.g., using factual consistency metrics or detailed human evaluation protocols)?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Figure 1 is directly copied from an existing paper (Guo et al., 2022), which is subject to copyright compliance."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KHPpL2bI9c", "forum": "qBy7nYDgEa", "replyto": "qBy7nYDgEa", "signatures": ["ICLR.cc/2026/Conference/Submission25174/Reviewer_YQJo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25174/Reviewer_YQJo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853914135, "cdate": 1761853914135, "tmdate": 1762943351557, "mdate": 1762943351557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HiFACT, a benchmark dataset of approximately 1,500 annotated Hinglish political claims from 28 Indian state Chief Ministers and other leaders (with veracity labels, evidence, and metadata), and proposes HiFACTMix, a graph-aware framework integrating quantum-enhanced RAG, code-mixed text encoding, and explanation generation.\nExperimental results show HiFACTMix outperforms multilingual/code-mixed baselines (e.g., CM-BERT, VerT5erini) and is competitive with LLMs like GPT-4, with more evidence-grounded explanations."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper fills the gap in fact-checking for code-mixed low-resource languages like Hinglish.\n- The HiFACTMix framework not only outperforms strong multilingual and code-mixed baselines (such as CM-BERT and VerT5erini) in both veracity prediction and explanation quality but also remains competitive with advanced LLMs like GPT-4"}, "weaknesses": {"value": "At least 7 citation errors exist (e.g., Section 5.3, Reproducibility Checklist, Reproducibility Statement).\n\nInsufficient dataset size (1.5k samples) and lack of clear description of annotation standards/quality.\n\nNo novelty in using widely-used RAG for empirical validation.\n\nThe HiFACTMix framework’s integration of graph-based reasoning and quantum-enhanced retrieval introduces substantial computational overhead that requires careful consideration.\n\nExperiments are only conducted on self-constructed data without out-of-distribution evaluation, leading to untrustworthy results (potential overfitting).\n\nMissing citations for baselines."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8qUFVJ33xp", "forum": "qBy7nYDgEa", "replyto": "qBy7nYDgEa", "signatures": ["ICLR.cc/2026/Conference/Submission25174/Reviewer_zeCe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25174/Reviewer_zeCe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923907582, "cdate": 1761923907582, "tmdate": 1762943351328, "mdate": 1762943351328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HiFACTMix, a Hinglish (Hindi–English) political fact-checking benchmark (~1,500 claims) with evidence and four-way veracity labels (True/False/Partially True/Unverifiable), and proposes a graph-aware, code-mixed fact-checking pipeline with a Quantum-Enhanced Retrieval-Augmented Generation (Quantum-RAG) module and an FLAN-T5–based explanation generator. The method combines a code-mixed encoder, an evidence graph with transformer-style reasoning, quantum-inspired re-ranking for retrieval, and natural-language justifications linked to retrieved evidence. Experiments compare against multilingual/code-mixed baselines (mBERT, IndicBERT, CM-BERT, VerT5erini) and recent LLMs (GPT-4, LLaMA-2, Mistral); the authors report stronger explanation quality and competitive overall results, plus ablations attributing gains to graph reasoning and Quantum-RAG."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe primary strength of this paper is the introduction of the HiFACTMix dataset. Fact-checking in low-resource, code-mixed environments is a critical research gap, and this dataset, sourced from real-world political discourse, provides a much-needed resource for the community.\n2.\tThe paper addresses a highly relevant and impactful problem. Hinglish is a dominant language in online political and social discourse in India, making it a key vector for misinformation. Developing tools for this specific context is a significant contribution.\n3.\tThe proposed model attempts to build a complete fact-checking pipeline, from claim encoding and evidence retrieval to veracity prediction and justification generation. This holistic approach is commendable.\n4.\tThe authors compare their model against a reasonable set of baselines, including those specific to code-mixed text (CM-BERT) and large-scale, general-purpose LLMs."}, "weaknesses": {"value": "1.\tThe \"Quantum-Enhanced RAG\" is presented as a novel contribution but is poorly motivated and explained. The paper provides no specific details on the algorithm used, how it is \"quantum-inspired,\" or why this approach is superior to established classical re-ranking methods (e.g., BM25, dense retrievers, or monoT5-based re-rankers). The ablation study merely shows that removing the component (i.e., using a simpler retrieval) hurts performance, but it provides no evidence that the \"quantum\" aspect itself adds any value over a standard, non-quantum-branded re-ranker.\n2.\tThe paper relies on ROUGE and BLEU to evaluate explanation quality. The authors rightly acknowledge this as a limitation, but it is a significant one. These metrics measure surface-level n-gram overlap and are poor proxies for factual correctness, faithfulness to the evidence, or logical coherence. Given that RAG systems are known to be vulnerable to \"fabricated content\" and that their outputs must be evaluated for \"Faithfulness\", this evaluation is too shallow.\n3.\tThe paper introduces a new static benchmark. However, a key challenge for RAG systems is robustness to \"polluted\" or adversarial knowledge bases. The evaluation does not test the model's resilience to \"adversarial distractors\"—documents that are semantically similar but factually incorrect.\n4.\tThe dataset includes \"Partially True\" claims. This implies that claims may contain multiple facts that need to be verified independently. The paper's methodology (Figure 2) depicts a direct end-to-end model. It does not discuss or seem to implement a \"Decompose-Then-Verify\" strategy, which is often necessary for handling such complex, multi-fact claims. It is unclear how the model differentiates between \"False\" and \"Partially True\" without an explicit decomposition step, a process which itself is a known challenge due to potential error propagation."}, "questions": {"value": "1. Can you provide a detailed technical explanation of the \"Quantum-Enhanced RAG\" module? What specific algorithm is being used, and how does it differ from a standard, SOTA classical re-ranker? Can you provide an ablation study comparing it to a strong classical re-ranker, not just a weaker baseline?\n\n2. How does your model architecture handle \"Partially True\" claims? Does it perform any internal claim decomposition (as discussed in the \"Decompose-Then-Verify\" paradigm)? If not, how does it learn to distinguish a \"Partially True\" claim from a \"False\" one?\n\n3. Given the known limitations of ROUGE/BLEU for this task, did you consider a human evaluation of explanation faithfulness (i.e., are the explanations factually grounded in the retrieved evidence)?\n\n4.  Code-mixed handling. How do you tokenize romanized Hindi and mixed scripts? Any transliteration/normalization steps? What proportion of claims/evidence are fully romanized vs Devanagari/English?\n\n5. Provide rater guidelines, agreement statistics, and a sample of evidence-linked explanations used in the study to verify faithfulness claims.\n\n6. Which nodes/edges are extracted automatically, and with what tools (NER/linking/temporal parsing)?\n\n7.  What is the size/domain/timeframe of the corpus?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fNlwJF3Rii", "forum": "qBy7nYDgEa", "replyto": "qBy7nYDgEa", "signatures": ["ICLR.cc/2026/Conference/Submission25174/Reviewer_4N1m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25174/Reviewer_4N1m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762087819959, "cdate": 1762087819959, "tmdate": 1762943351106, "mdate": 1762943351106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}