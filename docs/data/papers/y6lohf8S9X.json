{"id": "y6lohf8S9X", "number": 20786, "cdate": 1758310113401, "mdate": 1759896958701, "content": {"title": "Hybrid Minority Oversampling via LLM-Generated Seeds and SMOTE Expansion", "abstract": "Class imbalance poses a persistent challenge in machine learning, as classifiers often underperform on the minority class when trained on skewed data. Oversampling is a common solution, with methods such as Synthetic Minority Oversampling Technique (SMOTE) offering efficiency but limited representational power, since they rely solely on existing data points. Recent approaches that employ large language models (LLMs) for oversampling overcome this limitation by generating diverse synthetic samples informed by contextual knowledge. However, LLM-only methods are computationally expensive and often impractical at scale. To bridge this gap, we propose LLM-SMOTE Hybrid (LSH), a method that integrates the strengths of both paradigms. In LSH, an LLM acts as a Scout that generates contextually meaningful seed samples for the minority class, while SMOTE serves as the Surveyor that efficiently expands these seeds to generate new samples. This design reduces reliance on repeated LLM calls while preserving diversity and scalability. Extensive experiments on 60 imbalanced tabular datasets, across multiple classifiers and resampling strategies, reveal that LSH consistently outperforms SMOTE and LLM in highly imbalanced datasets, demonstrating particular effectiveness in few-shot and zero-shot scenarios where SMOTE fails. Robustness analysis further shows that LSH achieves stable generalization with lower variance compared to other methods. Finally, LSH provides a practical trade-off, achieving competitive performance to LLM-based methods at substantially lower computational cost. These findings position LSH as an efficient, robust, and broadly applicable oversampling strategy for imbalanced learning problems.", "tldr": "LSH (LLM-SMOTE Hybrid) combines the creativity of LLMs with the efficiency of SMOTE, delivering a robust and practical oversampling method that shines in highly imbalanced and extreme few-/zero-shot scenarios.", "keywords": ["Oversampling", "SMOTE", "LLM", "tabular data"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c316109c8035d2b3fd6089fb243368a20c5ca14b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes $\\mathrm{LSH}$, a hybrid minority oversampling method that combines an $\\mathrm{LLM}$ for generating minority “seed” samples with $\\mathrm{SMOTE}$ for subsequent densification. Across $60$ datasets, $\\mathrm{LSH}$ delivers small but consistent gains over $\\mathrm{SMOTE}$ and an $\\mathrm{LLM}$-only baseline, with clearer advantages under severe imbalance and in few-/zero-shot settings (where $\\mathrm{SMOTE}$ alone fails). In efficiency, $\\mathrm{LSH}$ invokes the $\\mathrm{LLM}$ once to create seeds and then expands via $\\mathrm{SMOTE}$ in near–constant time."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Simple, practical hybrid: “$\\mathrm{LLM}$ lays the landmarks $\\rightarrow$ $\\mathrm{SMOTE}$ densifies,” which is easier to operationalize than $\\mathrm{LLM}$-only augmentation.  \n- Broad evaluation: $60$ datasets, multiple classifiers and resampling ratios, with analyses including the Bayesian Sign Test ($\\mathrm{BST}$).  \n- Actionable insights: advantages grow with imbalance severity; clear runtime benefits.  \n- Code available, which is a good step toward reproducibility."}, "weaknesses": {"value": "- The paper appears to rely solely on GPT-4o-mini; it remains unclear whether the results are robust to the choice of model or would change across different LLMs.\n- There is insufficient analysis of which dataset characteristics favor the proposed method (i.e., where and why it performs best).\n- Datasets are listed only as “Data 1..60” without OpenML IDs/names, which makes reproduction practically impossible.\n- SMOTE implementation details—\n$k$, random seed, handling of mixed continuous/categorical features, etc, are missing. Many OpenML tabular datasets include categorical variables, yet preprocessing/encoding policies are not described.\n- There is no audit of the realism of zero-shot LLM-generated minority data (e.g., distributional similarity, constraint violations, statistical distances) and no quality checks beyond downstream performance. This is especially important for domains like healthcare/fraud, where plausible-but-false samples can be harmful.\n- The choice of the seed ratio (e.g., first generating to \n1:0.2 with the LLM) lacks justification and sensitivity analysis.\n- The concrete thresholds defining “more/mid/less” imbalance are not specified.\n- Multiple presentation issues remain; please see the items below and incorporate any fixes that are helpful."}, "questions": {"value": "- Are there prior hybrid augmentation approaches ($\\mathrm{LLM}$ $+$ classical/generative methods)? If so, please include them as baselines.  \n- Could techniques other than $\\mathrm{SMOTE}$ expand $\\mathrm{LLM}$ seeds effectively (e.g., Borderline-$\\mathrm{SMOTE}$, $\\mathrm{ADASYN}$, variational/GAN-based)? Why is $\\mathrm{SMOTE}$ preferable here?  \n- Given that public $\\mathrm{OpenML}$-style data may appear in $\\mathrm{LLM}$ pretraining, can $\\mathrm{LSH}$ work on data unlikely to be in the $\\mathrm{LLM}$’s training set?  \n- Please cite the sources for the $\\mathrm{LLM}$-based (LM) baseline and specify the exact model/version used.  \n- State at the start of the Experiments that the task is binary classification (currently first noted in the conclusion).  \n- Please clarify the primary metric.  \n- Precisely define how the “average margin” is computed and justify averaging margins across heterogeneous datasets.  \n- For Table $3$, justify why datasets A–D were selected.  \n- For Figure $4$, clarify “Resampling strategy (minority ratio target),” provide raw runtimes with $\\mu \\pm \\sigma$, and explain why some segments appear flat (e.g., $0.3 \\rightarrow 0.4$). Shouldn’t $\\mathrm{LLM}$-only grow with the number of calls?  \n- Provide $\\mathrm{OpenML}$ IDs/names for all $60$ datasets.\n\n(Other Comment)\n- Global: ensure parentheses around citations where appropriate.  \n- Add brief how-to-read guidance in figure/table captions.  \n- Figure $1$: explain what “$21, 27$” denote; if 2D reduction uses $\\mathrm{PCA}$, state it explicitly.  \n- Figure $1$ legend: standardize capitalization (Major/Minor).  \n- Line $252$: fix typo $\\text{ANLAYSIS} \\rightarrow \\text{ANALYSIS}$.  \n- Table $2$: avoid italics for $\\text{LM–SM} / \\text{LS–LM} / \\text{LS–SM}$.  \n- Figure $2$: report actual correlation coefficients (even if not significant) and clarify what each plot and the $y$-axis represent (e.g., average margin).  \n- Table $3$ vs. Figure $6$: unify dataset identifiers (A–D vs. indices such as $21, 27$) and list the IDs/names for datasets $1\\text{–}60$ in the appendix.  \n- Classifier inconsistency: the main text lists Random Forest, whereas Appendix A.$3$ (Table $4$) lists kNN—please reconcile."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ihhp2HvDcy", "forum": "y6lohf8S9X", "replyto": "y6lohf8S9X", "signatures": ["ICLR.cc/2026/Conference/Submission20786/Reviewer_paAB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20786/Reviewer_paAB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629961874, "cdate": 1761629961874, "tmdate": 1762935044136, "mdate": 1762935044136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LLM-SMOTE Hybrid (LSH), a hybrid oversampling method designed to address class imbalance in tabular data by combining the complementary strengths of Large Language Models (LLMs) and the Synthetic Minority Oversampling Technique (SMOTE). \nThe key contributions are: (1) extensive empirical evaluation on 60 datasets showing LSH's consistent superiority, especially in highly imbalanced, few-shot, and zero-shot scenarios where SMOTE fails; (2)demonstrated robustness with lower generalization variance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method exhibits greater robustness, defined by a lower variance in performance between validation and test sets.\n2. Unlike LLM-only methods that require repeated, costly model invocations for each resampling ratio, LSH invokes the LLM only once for initial seed generation."}, "weaknesses": {"value": "1. The paper lacks critical details necessary for full understanding, verification, and reproducibility. The most significant omission is the absence of a defined strategy for determining the number of seed samples the LLM (\"Scout\") should generate.\n2. The paper's choice to use a single LLM (GPT-4o-mini) and a highly complex, custom prompting strategy limits the generalizability of its findings."}, "questions": {"value": "1.\tThe number of seed samples is a fixed number? A percentage of the original minority class? Is it tuned per dataset?\n2.\tIt is unclear if the reported benefits stem from the hybrid concept itself or are an artifact of this specific model and prompt engineering. The claim that LSH is \"model-agnostic\" remains unsubstantiated. A stronger validation would involve testing with other LLMs (e.g., open-source models like Llama, or other APIs like Claude) to demonstrate the general applicability of the Scout-Surveyor paradigm.\n3.\tThe chosen LLM-only baseline is the same complex pipeline, a more rigorous and actionable comparison would be against a simpler, more direct LLM-based oversampling method to isolate the contribution of the hybrid design from the sophistication of the prompting technique. The gains of LSH over the LLM-only method are modest."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1ICCo0db0Q", "forum": "y6lohf8S9X", "replyto": "y6lohf8S9X", "signatures": ["ICLR.cc/2026/Conference/Submission20786/Reviewer_F81Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20786/Reviewer_F81Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902341997, "cdate": 1761902341997, "tmdate": 1762934962873, "mdate": 1762934962873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LSH, a hybrid oversampling pipeline where an LLM generates a few minority “seeds,” then SMOTE expands them. Experiments on 60 OpenML tabular datasets and a few extreme few/zero-shot settings report small average F1 gains over SMOTE and an LLM-only variant. The claimed advantage is practicality (one-time LLM cost, cheap SMOTE scaling) and robustness under severe imbalance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple idea: “LLM seeds + SMOTE expansion” is potentially practical when minority data are scarce.\n\n\n2. Wide dataset sweep: 60 datasets + few/zero-shot scenarios cover diverse conditions."}, "weaknesses": {"value": "**1. Lack of clarity and self-containment.**\n\nSeveral tables and figures are not clearly explained, which makes the paper difficult to follow and reproduce.\nFor example, in Table 6, the meanings of columns such as SM_v, SM_t, and SM_a are not explicitly defined.\nIn Table 3, the datasets labeled Data A/B/C/D are mentioned without description—readers cannot tell what domains these datasets belong to or what kinds of features they include.\nSimple statistics like imbalance ratio are not sufficient; it would help to know whether the data are from finance, biology, or other domains.\nOverall, the paper could be more self-contained, with clearer explanations of datasets, metrics, and table contents.\n\n**2. Lack of justification and validity discussion**\n\nThe paper also lacks a discussion on when and why LLM-based seed generation is valid.\n LLMs could easily produce inconsistent or even impossible feature combinations, especially for structured or domain-specific data.\n It is unclear how such invalid outputs are detected or filtered.\n Moreover, not all domains are equally suitable for generation via LLMs—medical, financial, or safety-critical datasets may pose ethical or factual risks.\n Without addressing these issues, the methodological justification for using LLMs as data generators remains weak; the reader is left uncertain about the boundaries and reliability of this approach.\n\n**3. Missing concrete examples of LLM-generated outputs**\n\nThe paper does not provide real examples or validation of the data generated by the LLM.\nIt is not clear how the authors ensure that the generated features are realistic or consistent.\nWhat if the LLM produces implausible feature combinations? How is this handled in practice?\n\n**4. Weak and unconvincing empirical performance.**\n\nThe reported performance gains are quite small—about +0.011 F1 on average compared to SMOTE and the LLM-only variant.\nBayesian sign tests show that results are mostly draws across 80–90% of datasets.\nIn several scenarios, especially with moderate or low imbalance, LSH performs similarly or even slightly worse than the baselines.\nAs a result, the experiments do not provide strong evidence that the proposed method offers clear benefits over existing approaches.\n\n\n**5. Limited comparison baselines.**\n\nThe experimental comparison is limited to SMOTE and a simple LLM-only variant.\n The paper does not compare with tabular generation SOTAs (HARMONIC, Llmovertab).\nWithout these comparisons, it is difficult to understand how the proposed hybrid approach stands relative to state-of-the-art alternatives."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AC007VDF22", "forum": "y6lohf8S9X", "replyto": "y6lohf8S9X", "signatures": ["ICLR.cc/2026/Conference/Submission20786/Reviewer_fuz8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20786/Reviewer_fuz8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949115130, "cdate": 1761949115130, "tmdate": 1762934940479, "mdate": 1762934940479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LLM-SMOTE Hybrid (LSH), a two-stage method that first uses a large language model (LLM) to generate a small set of seed samples for the minority class, and then expands them using SMOTE. The authors evaluate their approach on 60 imbalanced datasets across multiple classifiers, comparing it with standard SMOTE and LLM-only methods. Experimental results indicate that LSH achieves modest but consistent improvements over baseline methods, particularly in highly imbalanced settings, while maintaining computational efficiency relative to pure LLM-based approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed Scout and Surveyor method is intuitive and effectively addresses key limitations of traditional oversampling techniques and the high computational cost associated with LLM-only methods.\n\n2. The evaluation on 60 imbalanced tabular datasets, spanning multiple classifiers, resampling strategies, and few-/zero-shot settings, provides rich and comprehensive empirical evidence."}, "weaknesses": {"value": "1. Methodological imbalance: LSH makes a single LLM call to reach the 1:0.2 ratio and then scales using SMOTE, whereas the LLM baseline regenerates samples at every target ratio. This design advantage reduces stochastic variance and call overhead for LSH but not for the baseline, meaning that the observed performance gains may partly result from the setup rather than the hybrid mechanism itself.\n\n2. Limited performance gain: The reported average improvement is only about one percentage point, and Bayesian Sign Test results indicate that most comparisons are statistical draws across datasets.\n\n3. Single-model dependency: All minority seed samples are generated exclusively using GPT-4o-mini, with no exploration of alternative LLMs, decoding temperatures, or sampling schemes. This dependence raises questions about generalizability and potential model-specific biases."}, "questions": {"value": "1. Evaluate the baseline under identical conditions: Re-run the LLM baseline using the same cumulative protocol as LSH: generate once to a 1:0.2 ratio, then scale up without repeated regeneration (e.g., by appending LLM samples or expanding fixed seeds with SMOTE) and report whether it still performs worse than LSH.\n\n2. Analyze sensitivity to parameters: Vary the initial seed ratio (e.g., 1:0.1, 1:0.2, 1:0.3, 1:0.4) and explore different LLM models and decoding settings to assess the robustness of the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FJUIm4fEcO", "forum": "y6lohf8S9X", "replyto": "y6lohf8S9X", "signatures": ["ICLR.cc/2026/Conference/Submission20786/Reviewer_MEWy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20786/Reviewer_MEWy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952173345, "cdate": 1761952173345, "tmdate": 1762934894709, "mdate": 1762934894709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}