{"id": "wyauZpoqRK", "number": 6281, "cdate": 1757964344222, "mdate": 1759897925141, "content": {"title": "GENIE: Gaussian Encoding for Neural Radiance Field Interactive Editing", "abstract": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have recently transformed 3D scene representation and rendering. NeRF achieves high-fidelity novel view synthesis by learning volumetric representations through neural networks, but its implicit encoding makes editing and physical interaction challenging. In contrast, 3DGS represents scenes as explicit collections of Gaussian primitives, enabling real-time rendering, faster training, and more intuitive manipulation. This explicit structure has made 3DGS particularly well-suited for interactive editing and integration with physics-based simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural Radiance Fields Interactive Editing), a hybrid model that combines the photorealistic rendering quality of NeRF with the editable and structured representation of 3DGS. Instead of using spherical harmonics for appearance modeling, we assign each Gaussian a trainable feature embedding. These embeddings are used to condition a NeRF network based on the \n nearest Gaussians to each query point. To make this conditioning efficient, we introduce Ray-Traced Gaussian Proximity Search (RTGPS), a fast nearest Gaussian search based on a modified ray-tracing pipeline. We also integrate a multi-resolution hash grid to initialize and update Gaussian features. Together, these components enable real-time, locality-aware editing: as Gaussian primitives are repositioned or modified, their interpolated influence is immediately reflected in the rendered output. By combining the strengths of implicit and explicit representations, GENIE supports intuitive scene manipulation, dynamic interaction, and compatibility with physical simulation, bridging the gap between geometry-based editing and neural rendering.", "tldr": "his work introduces GENIE, a hybrid model that fuses NeRF’s photorealism with 3D Gaussian Splatting’s editability, enabling interactive 3D scene editing via Gaussian-based conditioning and fast nearest Gaussian search.", "keywords": ["Gaussian Splatting", "NeRF", "3D objects", "physical simulations"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9683e9431e57f996c107fc4e7f18a38fd6e208e7.pdf", "supplementary_material": "/attachment/b8f83c0e55c5d12a6c9fc74eec83e45ec06b0cd4.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to merge NeRF- and 3D Gaussian Splatting-style representations to improve editing capabilities. More specifically, the authors attempt to merge Gaussians into hash grids. Since hash grids are difficult to edit, the authors propose using Gaussians for feature interpolation instead. Because the Gaussians will not be directly used for rendering and will take the place of hash grid parameters, their colors are replaced with feature vectors. To sample features from these Gaussians, the authors propose a ray-traced Gaussian proximity search, along with pruning and densification stages. Throughout the experiments, the paper compares the proposed method with baseline methods and demonstrates physics-based editing results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The applications and necessity of an editable representation are well-presented with ample examples (Fig 3, 6, 7).\n- Merging Gaussians in place of hash grids for editing is an interesting approach."}, "weaknesses": {"value": "**Editability**: Simply using Gaussians instead of hash-based parameters is not sufficient to claim improved editability. There are numerous editing techniques, yet it is not well-documented how the proposed method addresses their limitations or where it provides tangible advantages. A broader comparison and clearer explanation of why this representation specifically improves editing would strengthen the claim.\n\n**Motivation for Gaussians**: The rationale for integrating Gaussians into the representation for better NeRF-based editing remains unclear. If the goal is to combine the “best of both worlds” (NeRF and Gaussian Splatting), the paper should include comparisons with methods from both domains, discuss trade-offs, and demonstrate how the proposed approach leverages their respective strengths.\n\n**Experimental performance**: Even if this representation is more editing-friendly, its quantitative performance currently lags behind the baselines, raising questions about its practical benefits."}, "questions": {"value": "- The equations are not numbered. Adding numbers would improve clarity for both the authors and reviewers.\n- The method section is somewhat difficult to understand. In Section 4, the paper states it uses Gaussians instead of grid parameters for \"Splash grid encoding,\" noting that hash grids are not editing-friendly. However, if Gaussians are only used for feature interpolation, what do the grid parameters $\\Psi$ represent? If the grid does use its own parameters, how does the paper resolve the contradiction that these parameters are \"not good for editing\" but are still being used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fPZPaxYe8V", "forum": "wyauZpoqRK", "replyto": "wyauZpoqRK", "signatures": ["ICLR.cc/2026/Conference/Submission6281/Reviewer_cWxi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6281/Reviewer_cWxi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761509862843, "cdate": 1761509862843, "tmdate": 1762918587969, "mdate": 1762918587969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a specific type of 3D scene editing: scene deformation with human-guided inputs. The proposed method is to repurpose a Gaussian splatting representation to support NeRF-like volume rendering through interpolation-based point inquiry. The results show that the proposed method supports various deformation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is straightforward and easy to understand.\n- Code is provided in supplementary materials."}, "weaknesses": {"value": "- All the quantitative results are just evaluating the reconstruction of static scenes, and the numbers of the proposed method are just comparable with the baselines.\n    - Given that the proposed method can be regarded as an extension of 3DGS, it is certain that the performance should be comparable to 3DGS. However, in Tab.2, the performance dropped a lot compared with 3DGS.\n    - NeuralEditor's NeRFSynthetic benchmark is used for qualitative evaluation in Fig.8. NeuralEditor has provided the ground truth of each deformed scene, and also reported the PSNR/SSIM metrics in their paper. However, this paper neither presents these metrics for its scene nor compares them with NeuralEditor's numbers.\n    - Given that NeuralEditor's benchmark is the only benchmark for scene deformation, it is crucial to provide these numbers. \n- The idea of this paper is a combination of multiple existing ideas. \n    - The idea of interpolation-based point query and then an MLP is already the pattern of PointNeRF and NeuralEditor. Though this method is named \"Splash Grid Encoding\", it does not actually use any of the grid vertices, but is only dependent on the kNNs of the query point. \n    - To find the kNNs for interpolation, the paper proposes RT-GPS. However, both PointNeRF and NeuralEditor propose their kNN finding methods. There is no discussion on how the proposed kNN method is better than the baselines.\n- The view-dependent effects are confusing in this paper in both the method design and the results.\n    - Given that the Gaussian parameters already consider view-dependency as SH factors, it is confusing why there is another \"camera direction\" input to the MLP F_GENIE in Fig.4.\n    - In Fig.8, the bottom of the gong is completely unseen in training views, and this is why the baselines show strange colors. Even if the top of the gong is seen, the bottom may still be affected by view-dependent effects and have different colors. However, the proposed method shows a similar color to the top, which seems to assume it is view-independent.\n- The actual rendering efficiency is not sufficient for the claimed \"real-time\" rendering. As shown in Appendix Tab.4, it takes at least 3.3 seconds to render a frame."}, "questions": {"value": "- How is the training efficiency? \n- Could you please provide the PSNR metrics for NeuralEditor's NeRFSynthetic benchmarks?\n- What are the crucial idea differences between the proposed method and PointNeRF/NeuralEditor, specifically in the interpolation and kNN finding?\n- While training-free for editing, NeuralEditor supports fine-tuning to improve the results if ground truth is given. Does the proposed method support this?\n- As shown on NeuralEditor's website, in \"Human-Guided Shape Deformation\", row 1, column 2, NeuralEditor supports editing like \"cutting a scene in the middle and rendering the two parts\". Does the proposed method support this deformation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Lv9VBq2uXv", "forum": "wyauZpoqRK", "replyto": "wyauZpoqRK", "signatures": ["ICLR.cc/2026/Conference/Submission6281/Reviewer_7BsQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6281/Reviewer_7BsQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887709635, "cdate": 1761887709635, "tmdate": 1762918587450, "mdate": 1762918587450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GENIE, a hybrid model that combines the high-quality rendering capabilities of NeRF with the editability of 3D Gaussian Splatting. The paper introduces the RT-GPS algorithm for fast nearest neighbor search and Splash Grid Encoding for multi-resolution feature encoding. The method supports real-time local editing and integration with physics-based simulation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The overall structure is clear, with Figures 1 and 4 providing excellent overviews of the methodology.\n\n2. Rich visualization of physics simulation results (Figures 2, 3, and 6)."}, "weaknesses": {"value": "1. The notation is sometimes inconsistent. For example, the definition of G_GENIE in Section 4 is overly complex, which compromises readability.\n\n2. Some key concepts lack clear explanation. For instance, the use of \"triangle soup\" (Section 7) is not friendly to readers without a graphics background.\n\n3. The baseline selection criteria for Tables 1 and 2 are not explained. Why are there no RIP-NeRF results on the Mip-NeRF 360 dataset? \n\n4. The number of comparison methods is insufficient.\n\n5. Figure 8 has low visual quality with text labels that are too small.\n\n6. The experiments are insufficient in scope.\n\n7. Table 2 shows that GENIE's PSNR is 5-7 dB lower than Mip-NeRF on Mip-NeRF 360, with even larger gaps in SSIM and LPIPS metrics. While the authors attribute this to Gaussian density, there lacks systematic analysis. The authors need to strengthen their justification.\n\n8. There is no comparison of training time."}, "questions": {"value": "1. What is the speedup ratio of RT-GPS compared to brute-force nearest neighbor search? Can you provide detailed time complexity analysis and experimental data?\n\n2. The ablation study in Table 3 shows that removing Splash Grid Encoding causes OOM in some scenes (Ship, Mic). Does this indicate a design flaw in this module?\n\n3. How do you ensure view consistency of rendering results after physics simulation? Are there artifacts from multiple viewpoints?\n\n4. Is the method applicable to dynamic scenes or time-varying illumination?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PhAKhMoJ6s", "forum": "wyauZpoqRK", "replyto": "wyauZpoqRK", "signatures": ["ICLR.cc/2026/Conference/Submission6281/Reviewer_g8yM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6281/Reviewer_g8yM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999547072, "cdate": 1761999547072, "tmdate": 1762918586959, "mdate": 1762918586959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GENIE, a hybrid scene representation that marries the editability of 3D Gaussian Splatting with the photorealistic rendering of NeRFs to enable scene editing and physics-driven manipulation. Each Gaussian consists of a learnable feature embedding that conditions a NeRF; nearby Gaussians are selected efficiently with Ray-Traced Gaussian Proximity Search (RT-GPS), and the system supports densification/pruning to balance quality and speed. This design allows direct primitive manipulation or mesh-driven deformations to immediately reflect in rendered views without fine-tuning. Experiments on NeRF-Synthetic show reconstruction on par with strong baselines and competitive with 3DGS while outperforming editable baselines like RIP-NeRF on most scenes; on Mip-NeRF 360 the method remains editable though PSNR trails the best static models. The authors provide code, configs, and reproduction details."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow\n2. code, configs, hardware/software details, and workflows provided.\n3. Novel hybrid architecture that meaningfully combines implicit and explicit representations, addressing a real limitation of pure NeRF methods (difficult editing)\n4. Competitive reconstruction on NeRF-Synthetic and better than editable baselines like RIP-NeRF on most scenes; rendering speed competitive given editability.\n5. Thorough ablations isolate the value of Splash Grid Encoding, RT-GPS, learnable means, and densification."}, "weaknesses": {"value": "1. On Mip-NeRF 360, the proposed method achieves 19.47 PSNR vs 25.25 for Gaussian Splatting on bicycle scene - a 5.78 dB drop. Similar gaps appear across all outdoor scenes (Table 2). Even on NeRF-Synthetic, it underperforms GS (34.67 vs 35.82) and LagHash. This quality sacrifice is substantial and undermines the hybrid approach's value proposition.\n2. While editing capabilities are a critical contribution of this work, there are no quantitative metrics for editing quality. Only qualitative comparison with PhysGaussian/GASP (Figure 7) are shown. Quantitative metrics, and potentially a user study would go a long way in emphasizing the editing capabilities of this method.\n3. The rendering speed (Table 4) is considerably slower than prior works.\n4. The system is extremely complex: there are many interacting components (RT-GPS with quantile Q, k neighbors, Splash Grid with multiple LoDs, pruning with confidence vectors, densification thresholds τ_s and τ_α). While ablation helps, the hyperparameter sensitivity and design choices aren't fully explored."}, "questions": {"value": "1. The paper claims that the method is real-time (L42): but Table 4 shows that rendering is not real time. What does this claim mean?\n2. Can you incorporate metrics for editing performance as well?\n3. Why not use pure Gaussian Splatting editing? Given that Gaussian Splatting achieves better quality and faster rendering, what specific advantages justify the NeRF hybrid beyond avoiding \"gaps\" during super-resolution?\n4. Can the method handle view dependent effects, such as specular highlights, as a result of editing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2ijLjCnNm6", "forum": "wyauZpoqRK", "replyto": "wyauZpoqRK", "signatures": ["ICLR.cc/2026/Conference/Submission6281/Reviewer_oxnh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6281/Reviewer_oxnh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762213735050, "cdate": 1762213735050, "tmdate": 1762918586578, "mdate": 1762918586578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}