{"id": "TuXDD0IWN7", "number": 11511, "cdate": 1758200690036, "mdate": 1759897571166, "content": {"title": "Graphlets as Building Blocks for Structural Vocabulary in Graph Foundation Models", "abstract": "Foundation models excel at language, where sentences become tokens, and vision, where images become pixels, because both reduce to discrete symbols on a shared, fixed grid. Knowledge Graphs share the discreteness, but not the geometry. Their entities and relations are discrete symbols, yet their arrangement is relational and lacks a common, fixed grid. Knowledge Graphs (KGs) share the discreteness, but not the geometry.\nThey form irregular, non-Euclidean topologies whose local neighborhoods differ from graph to graph. Therefore, Graph Foundation Models (GFMs) rely on identifying structural invariances to produce transferable representations. Without a universal token set, GFMs are limited in their ability to transfer representations across unseen KGs. We close this gap by treating graphlets, small connected graphs, as structural tokens that recur in heterogeneous KGs. In this paper, We introduce a model-agnostic framework based on a vocabulary of graphlets that mines a KG between relations via pattern matching. In particular, we considered closed and open 2- and 3-path, and star graphlets, to obtain robust invariances. The framework is evaluated on 51 KGs from a wide range of domains, for zero-shot inductive and transductive link prediction. Experiments show that adding simple graphlets to the vocabulary yields models that outperform prior GFMs.", "tldr": "This paper investigates Graphlets as structural vocabulary to improve transferability and performance of Knowledge Graph Foundation Models.", "keywords": ["Knowledge Graph", "Graph Foundation Model", "Knowledge Graph Representation Learning", "Graphlets"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a44061d17709526ce1ec60909c49e43e8e670ec2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ULTRA+, a knowledge graph foundation model that aims to improve ULTRA and MOTIF by replacing SPMM with pattern-matching with SPARQL and by reducing the high-arity pattern to positional-binary edges to use a single relation graph to compute the relation invariant. They further highlight the importance of open and closed paths when considered as graphlets, and with additional graphlets consideration, such as star-shaped graphs. Empirically, they report a marginal gain over ULTRA and MOTIF on zero-shot link prediction across 51 KGs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Pattern matching with SPARQL is a practical method that augments MOTIF and ULTRA, and it naturally supports open/closed distinctions, which is nice to see as this has been implemented beyond SPMM, which are quite limited.\n- Binary version and summarization with positional binary edges are novel and a clean engineering trick."}, "weaknesses": {"value": "**Major**:\n- **Limited theoretical contribution**: The theorems are stated without proofs, which makes it hard to assess the core claims. In particular, a formal treatment of separation power (e.g., a WL-style characterization as in [1]) would substantially strengthen the work. \n- **Open vs. closed paths claim**: MOTIF can theoretically distinguish open vs. closed paths if the motif family includes the relevant closed patterns (e.g., cycles). In that sense, the architectural difference emphasized for ULTRA+ appears to be a direct corollary to Theorem 6.4 in [1]. \n- **Runtime/efficiency claims**: The paper states that SPARQL-based construction is “computationally less demanding” than the SPMM kernel, but neither a complexity analysis nor an empirical runtime/memory study is provided.\n- **Scope of architectural novelty**: Beyond the relation-graph construction, ULTRA+ appears architecturally the same as ULTRA. \n- **Limited empirical evaluation** The Author does not empirically evaluate nor compare with TRIX [2], which can also implicitly count the homomorphisms. KG-ICL[3] is also not considered, which serves as a strong baseline. Additionally, it would be nice to see how each model variation can catch up with further fine-tuning, as the current gains over the existing model are small with no error bar or significance tests.\n- **Explanation over empirical evaluation is unsubstantiated**: Notice that in the zero-shot setting, there is technically no difference between transductive and inductive dataset splits since the KGFM model does not observe the relation types or their distribution, regardless (they only know from the pre-training mix). Thus, the explanation of why ULTRA+ is worse than MOTIF on the transductive dataset is not justified. The author should instead discuss what the actual differences between these classes of datasets are, e.g., regarding graph statistics, to further justify their claim.\n\n**Minor**: \n- **Small Bug in codebase**: During training, the author first computes the relation representation from the relation encoder and then applies edge dropout in the training mode. This edge dropout during training might potentially change the relation graph constructions and thus yield different relation representations. \n- **Presentation**: There are noticeable typos and inconsistencies in the paper; figures and tables are not properly adjusted.\n\n[1] Huang, Xingyue, et al. \"How Expressive are Knowledge Graph Foundation Models?.\" arXiv preprint arXiv:2502.13339 (2025). \n\n[2] Zhang, Yucheng, et al. \"TRIX: A more expressive model for zero-shot domain transfer in knowledge graphs.\" arXiv preprint arXiv:2502.19512 (2025). \n\n[3] Cui, Yuanning, Zequn Sun, and Wei Hu. \"A prompt-based knowledge graph foundation model for universal in-context reasoning.\" Advances in Neural Information Processing Systems 37 (2024): 7095-7124."}, "questions": {"value": "- How does the notion of graphlet in ULTRA+ differ precisely from motifs in MOTIF?\n\n- Can the author formally compare the separation power of positional-binary constructions with MOTIF or TRIX (e.g., via WL-style arguments or expressivity hierarchies)?\n\n- Will the author include proofs for the stated theorems?\n\n- Can the author provide runtime/memory analysis for SPARQL-based construction vs. SPMM (theoretical + empirical timing)?\n\n- Does the author have an ablation isolating the effect of counting on relation graphs?\n\n- Within the MOTIF framework, can the author add a triangle/cycle motif baseline to directly test the open vs. closed claim, and compare to ULTRA+ both empirically and theoretically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DnAB6TYgsW", "forum": "TuXDD0IWN7", "replyto": "TuXDD0IWN7", "signatures": ["ICLR.cc/2026/Conference/Submission11511/Reviewer_2ruj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11511/Reviewer_2ruj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760735459418, "cdate": 1760735459418, "tmdate": 1762922613311, "mdate": 1762922613311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose using graphlets as structural tokens to establish a shared vocabulary for Graph Foundation Models (GFMs).\nThen, the authors introduce a model-agnostic framework that extracts and encodes graphlets (2- and 3-paths, closed triangles, and star structures) to capture structural invariances across heterogeneous KGs. This graphlet-based vocabulary enables zero-shot generalization across unseen graphs. Evaluations on 51 diverse KGs show that incorporating graphlets as structural tokens significantly enhances performance for both inductive and transductive link prediction tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I like the idea of treating graphlets as a structural vocabulary that parallels the tokenization principle in language models, offering a clean conceptual bridge between discrete and relational domains. \nUsing graphlets provides interpretable subgraph-level structures that can be intuitively linked to semantic or relational motifs in KGs.\nThe framework directly targets a key limitation in current Graph Foundation Models - their difficulty in transferring across unseen graphs."}, "weaknesses": {"value": "- The use of graphlets as structural primitives is not entirely new; prior works in network science and graph representation learning have explored motif-based or subgraph-based encodings.\n\n- The paper does not discuss the expressive power of the proposed graphlet-based vocabulary in relation to established graph isomorphism tests, such as the Weisfeiler–Lehman (WL) hierarchy. It remains unclear whether incorporating graphlets enhances the representational capacity beyond standard GNNs.  \n\n- Extracting graphlets at scale (especially 3-paths or larger motifs) can be computationally expensive for large or dense graphs."}, "questions": {"value": "- Does the approach capture semantic relations beyond structural similarity? For instance, can similar structures with different relational meanings be disambiguated?\n\n- How does the proposed framework compare against motif-based GNNs or subgraph isomorphism-based methods in terms of both efficiency and generalization?\n\n- How does the proposed graphlet-based structural vocabulary relate to the Weisfeiler–Lehman (WL) in terms of expressive power?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9wbwqMJHaL", "forum": "TuXDD0IWN7", "replyto": "TuXDD0IWN7", "signatures": ["ICLR.cc/2026/Conference/Submission11511/Reviewer_kvqC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11511/Reviewer_kvqC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760783252705, "cdate": 1760783252705, "tmdate": 1762922612882, "mdate": 1762922612882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a model-agnostic framework Ultra+ that builds a vocabulary of small graphlets to address the challenge of creating transferable representations in Graph Foundation Models (GFMs) for Knowledge Graphs (KGs), which lack a universal geometric structure. This sounds like a meaningful and promising research question, both for GFMs and KGs. However, the paper still faces some issues that need further improvement and clarification."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem studied in this paper is highly important and offers insightful implications for applying graph foundation models to knowledge graphs. Apart from some minor details, the overall writing of the thesis is professional. The experiments appear sufficiently comprehensive and generally sound, both in terms of dataset selection and comparisons with baseline methods. This paper seems to have sufficient theoretical support. There are insights into the improvement of the Ultra framework. Such research is worthy of appreciation and recognition."}, "weaknesses": {"value": "From the perspective of graph foundation model frameworks, Ultra+ is an extension of the existing Ultra framework, and its novelty appears somewhat limited. I'm not very clear whether the graph foundation model framework used in knowledge graphs is relatively similar and unified, but it is clear that innovation at the model level is not the main contribution of this article. \n\nWhile the paper provides very detailed definitions, it only includes two theorems, which are insufficient to robustly support the core argument. It would be beneficial to rigorously demonstrate the superiority of Ultra+ from perspectives such as expressive power, similar to what was done in paper [1].\n\nTheoretically-driven improvements are certainly appreciated, and some theoretical ideas may already be integrated into the main text. However, the contributions should be emphasized more clearly. At present, it is difficult to fully grasp the theoretical innovations and the sources of Ultra+'s advantages as a new framework.\n\nSome details need to be improved and clarified:\n\nThe description of graph foundation models on line 114 is outdated. It is now recognized that large language models represent a key branch of graph foundation models, extending beyond the scope of pre-trained GNNs[2, 3].\n\nLines 165-166: The “product function” notation $\\eta \\cdot \\rho \\cdot \\eta$ is non-standard and can be confusing.\n\nAre the concepts of graphlets and motifs first introduced in this paper? If not, they should be appropriately cited.\n\nAs a theorem, Theorem 3.2 requires a proof or a citation to the original work where it was first proposed. Similarly for Theorem 4.3.\n\nSubfigures (d3) and (e3) in Figure 2 are identical.\n\n“the query triple q(h, ?)” on line 294 and “the query (h, q, ?)” on line 303 are inconsistent. This discrepancy is confusing. The notation for a query should be unified and clearly defined.\n\nA key claimed advantage of Ultra+ on lines 328-329 is its ability to discriminate between closed and open paths, unlike Motif. This point would be significantly strengthened by providing a concrete example illustrating this discrimination and linking it to theoretical results about expressive power.\n\n[1]Xingyue Huang et al. How Expressive are Knowledge Graph Foundation Models? ICML, 2025.\n\n[2]Jiawei Liu et al. Graph foundation models: Concepts, opportunities and challenges. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025.\n\n[3]Zehong Wang et al. Graph foundation models: A comprehensive survey. arXiv preprint arXiv:2505.15116, 2025."}, "questions": {"value": "\"Ultra+ extends this approach by incorporating a richer set of graphlet-based pattern,\" how large is the size of this Graphlets in the specific implementation? If it is less than 5 as shown in Figure 2, how to \"capture more complex and higher-order interactions between relations\"\n\nWill expanding the size of Graphlets expand the range of structural vocabulary and further enhance the generalization performance of GFMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vaypy3aUED", "forum": "TuXDD0IWN7", "replyto": "TuXDD0IWN7", "signatures": ["ICLR.cc/2026/Conference/Submission11511/Reviewer_R1zb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11511/Reviewer_R1zb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761556828384, "cdate": 1761556828384, "tmdate": 1762922612520, "mdate": 1762922612520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Graph Foundation Model (GFM) framework based on graphlet structural vocabulary, designed for knowledge graph reasoning and zero-shot link prediction tasks. The authors argue that existing GFMs (such as Ultra and Motif) are limited in their ability to capture complex structural patterns, particularly due to their neglect of closed paths and higher-order structures. To address this, Ultra+ introduces a rich graphlet-based vocabulary that includes 2-path, 3-path, and star-shaped motifs to capture more robust structural invariances. Experiments conducted on 51 knowledge graph datasets demonstrate that Ultra+ consistently outperforms Ultra and Motif in both inductive and zero-shot reasoning settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Overall, the paper demonstrates strong innovation, with rigorous theoretical definitions, comprehensive experimental design, and good reproducibility, offering a new perspective on the role of structural vocabulary in GFMs.\n\n**S1.** This paper presents an innovative framework Ultra+ which enhances graph structural modeling capability by introducing a graphlet-based structural vocabulary.\n\n\n**S2.** On methodology, Ultra+ adopts a two-stage message passing mechanism that decouples relation graph learning from entity embedding learning, thereby achieving strong inductive and zero-shot generalization capabilities on unseen entities and relations.\n\n\n**S3.** The experimental section covers more than 50 knowledge graph datasets, validating the model’s generality and superiority in inductive reasoning and zero-shot link prediction, and demonstrating consistent and robust improvements over models such as Ultra and Motif."}, "weaknesses": {"value": "**W1.**  The improvements over Ultra and Motif remain somewhat ambiguous, lacking a clear mechanistic explanation—particularly regarding why the introduction of closed paths and higher-order graphlets leads to theoretical and performance gains. Moreover, the paper does not clearly justify why only closed and open 2-paths, 3-paths, and star-shaped graphlets are considered to achieve robust invariance.\n\n**W2.**  GFMs should not be limited to KG; the paper lacks a discussion on the potential significance and generalizability of the Ultra+ framework when applied to other graph datasets, such as social networks or molecular graphs.\n\n**W3.** The paper does not clearly explain how the proposed structural vocabulary is sampled, nor does it provide the corresponding complexity analysis.\n\n**W4.** The paper lacks a notation table, and the numerous mathematical symbols used throughout, along with the unclear connections and roles between definitions and theorems, make the paper difficult to follow. It is recommended to include clarifying explanations or detailed proofs to improve readability.\n\n**W5.** The experiments are not sufficiently comprehensive and lack comparative evaluations with the latest Graph Foundation Model baselines."}, "questions": {"value": "See the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZLQZCPb8wa", "forum": "TuXDD0IWN7", "replyto": "TuXDD0IWN7", "signatures": ["ICLR.cc/2026/Conference/Submission11511/Reviewer_KyYd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11511/Reviewer_KyYd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832607341, "cdate": 1761832607341, "tmdate": 1762922611994, "mdate": 1762922611994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}