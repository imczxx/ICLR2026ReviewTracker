{"id": "qins7MdufH", "number": 10588, "cdate": 1758176684756, "mdate": 1759897641973, "content": {"title": "Severing the Link: A Unified Adversarial Attack on Image and Video MLLMs via Generative Disruption", "abstract": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable cross-modal reasoning, their core vision-language grounding mechanisms present critical vulnerabilities, particularly in complex video scenarios. \nWe introduce **CAVALRY**, a unified framework for generating powerful adversarial attacks against both image and video MLLMs. \nOur approach introduces two key innovations: **(i)** a paradigm shift from conventional classification-boundary attacks to directly disrupting the generative process, realized through a novel loss that maximizes the likelihood divergence of the ground-truth response and severs the visual-linguistic link; and **(ii)** an efficient, progressive generator trained to produce spatiotemporally coherent perturbations for both dynamic videos and static images.\nComprehensive evaluations on seven state-of-the-art MLLMs, including GPT-4.1, Gemini 2.0, and QwenVL-2.5, validate CAVALRY's superior performance. \nOur method outperforms the strongest baselines by an average of 22.8\\% on video understanding benchmarks and extends this advantage to static images, proving 34.4\\% more effective than prior work. \nThese results establish CAVALRY as a foundational framework for probing the adversarial robustness of the entire spectrum of modern MLLMs.", "tldr": "We propose a unified generator-based framework that crafts transferable adversarial attacks for both images and videos by directly targeting the language generation process of Multimodal LLMs", "keywords": ["Adversarial Attacks", "Multimodal Large Language Models (MLLMs)", "Vision-Language Models (VLMs)", "Transferability", "Generator-based Attacks", "Video Adversarial Attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a2acfdd7524f6aa3db91b237eb894df6efda3d96.pdf", "supplementary_material": "/attachment/aad7fe5d27493f63ec1be784fce0c837060eda1c.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces CAVALRY, a unified adversarial attack framework that disrupts the vision-language grounding in MLLMs. CAVALRY employs a two-stage training strategy to produce spatiotemporally coherent perturbations for both images and videos, achieving strong transferability across diverse MLLM architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear and the task is interesting. Experimental results on seven mainstream MLLMs show that CAVALRY improves the attack effectiveness by an average of 34.4% and 22.8% in image and video tasks respectively, verifying its practicality and wide applicability.\n\n2. The method framework is clearly introduced, while using visualization to help readers quickly understand the method."}, "weaknesses": {"value": "1. The introduction lacks motivation for using generators, and accordingly, the related work lacks citations for generator-based approaches.\n\n2. Among the losses used by the author, Lsem seems like the loss design of the white-box LVLM attack [1], Lvis is the loss design of FARE [2], according to the auxiliary model is ResNet-50, and Laux is the traditional adv attack loss design. Can the authors explain the core difference in Lsem and normally used LVLM attack loss?\n\n3. In Table 1, why are all the comparison methods migrating image attacks to video? Can existing video methods be compared?\n\n4. Is the \"LLaVA-Video\" in Table 2 a typo? It's in image benchmark.\n\n5. It is recommended to add comparisons with other attack methods in Figure 4. In addition, the authors only show the processing efficiency. However, judging from the difference in methods, compared with attack paradigms such as PGD, the authors' method adds pre-training and finetuning processes. The overall time of these methods should be compared with the overall time of adversarial attack paradigms such as AnyAttack to reflect a fair comparison of performance gains and time costs.\n\n[1] Schlarmann C, Hein M. On the adversarial robustness of multi-modal foundation models[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 3677-3685.\n\n[2] Schlarmann C, Singh N D, Croce F, et al. Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models[J]. arXiv preprint arXiv:2402.12336, 2024."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ELk9jQYTRW", "forum": "qins7MdufH", "replyto": "qins7MdufH", "signatures": ["ICLR.cc/2026/Conference/Submission10588/Reviewer_pStB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10588/Reviewer_pStB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761398949764, "cdate": 1761398949764, "tmdate": 1762921858313, "mdate": 1762921858313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CAVALRY, a unified adversarial framework for both image and video MLLMs. It trains a U-Net generator to produce adversarial perturbations for each input image by maximizing the negative log-likelihood of the ground-truth response, together with a regularization term that keeps the perturbed visual tokens close to the originals. Training is performed in two stages, and the authors evaluate the method on both closed-source and open-source models, reporting improved attack success rates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Overall, the paper is well written and clearly presented. \n\nThe paper provides both theoretical and empirical results, demonstrating the effectiveness of the proposed framework."}, "weaknesses": {"value": "1. The method’s only video-specific component is batching frames from the same video with identical questions and answers during fine-tuning. This implicitly encourages temporal consistency but does not explicitly model temporal dependencies. In principle, the same setup would work for unrelated images sharing the same QA pairs. Therefore, the paper’s claim of “simultaneously modeling cross-modal reasoning vulnerabilities and temporal dependencies” (line 108) is an overstatement.\n\n2. The proposed negative-likelihood objective is essentially equivalent to standard untargeted adversarial attacks, which also maximize the negative log-likelihood. The conceptual novelty over existing attack paradigms is limited.\n\n3. All reported results rely on LLM-based scores, which inherently has randomness. Moreover, the score differences are small, making the reported improvements potentially sensitive to randomness. The paper should include multiple runs and standard deviations to ensure the statistical significance and reproducibility of the results."}, "questions": {"value": "How can this method capture meaningful temporal dependencies merely by including frames from the same video within a single batch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6970tPEGik", "forum": "qins7MdufH", "replyto": "qins7MdufH", "signatures": ["ICLR.cc/2026/Conference/Submission10588/Reviewer_AP9G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10588/Reviewer_AP9G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761692337230, "cdate": 1761692337230, "tmdate": 1762921857970, "mdate": 1762921857970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents CAVALRY, a unified adversarial attack framework targeting both image and video MLLMs. CAVALRY employs a generator that produces adversarial perturbations for visual inputs, whether static images or videos. The generator is optimized through three complementary objectives: (1) generative likelihood divergence, (2) manipulation of visual representations, and (3) an auxiliary feature loss. To ensure cross-modal and temporal coherence, CAVALRY adopts a two-stage training strategy involving large-scale pretraining followed by fine-tuning. Experimental results demonstrate that CAVALRY achieves state-of-the-art performance on both open-source and commercial MLLMs across the MMBench-Video and MME benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Theorem 1 and its mathematical proof clearly justify the semantic loss objective, providing theoretical soundness.\n2. The paper demonstrates transferability by showing that the attack generalizes not only to open-source MLLMs but also to commercial ones.\n3. Instead of iteratively updating perturbations, the authors train a generator that can produce frame-wise perturbations with linear time complexity, proving the practical applicability of the approach."}, "weaknesses": {"value": "Major Weaknesses\n1. Both the output-level loss and vision-encoder-level loss for MLLMs have been extensively studied in prior MLLM adversarial and jailbreak attack works [1, 2, 3, 4]. Thus, the methodological novelty is somewhat limited.\n2. The evaluation is restricted to a single benchmark for image understanding and one for video understanding, using only one LLM as the judge. This narrow experimental scope limits the assessment of generalizability across broader image/video domains and diverse text prompts.\n3. The paper lacks sufficient analysis and motivation regarding MLLMs. In particular, the reason behind the proposed method’s high transferability is not well explained. While the authors claim that large-scale pretraining enables cross-architecture transferability, this explanation is not fully convincing. Large-scale data may help the generator generalize across a wide range of samples within the surrogate model’s domain, but it does not inherently guarantee transferability across different architectures.\n\n[1] Luo, Haochen, et al. “An Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on Vision-Language Models.” ICLR 2024.\n[2] Cui, Xuanming, et al. “On the Robustness of Large Multimodal Models Against Image Adversarial Attacks.” CVPR 2024.\n[3] Zhao, Yunqing, et al. “On Evaluating Adversarial Robustness of Large Vision-Language Models.” NeurIPS 2023.\n[4] Zhang, Jiaming, et al. “AnyAttack: Towards Large-Scale Self-Supervised Generation of Targeted Adversarial Examples for Vision-Language Models.” CVPR 2025."}, "questions": {"value": "Minor Weaknesses & Questions\n1. It is unclear why the authors used an adversarially trained ResNet as the auxiliary model instead of an adversarially trained transformer architecture, given that most MLLM vision encoders are transformer-based.\n2. The paper employs the SRR metric instead of the traditional ASR, but it is somewhat difficult to intuitively understand what level of attack strength the SRR represents."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PvfL0pYuaV", "forum": "qins7MdufH", "replyto": "qins7MdufH", "signatures": ["ICLR.cc/2026/Conference/Submission10588/Reviewer_Tbio"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10588/Reviewer_Tbio"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908710719, "cdate": 1761908710719, "tmdate": 1762921857226, "mdate": 1762921857226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies adversarial attacks and robustness of multimodal large language models (MLLMs) that answer vision-and-language queries over images and videos. The proposed method, CAVALRY, uses a U-Net-style generator trained in a two-stage framework to provide perturbed visual inputs that alter MLLM outputs. CAVALRY combines three different losses to target both vision-language connections and vision representation, and it is evaluated on both static (images) and temporal (video) inputs. Experiments indicate CAVALRY can substantially degrade MLLM performance across multiple models and settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The attack design is intuitive and well-motivated: combining multiple loss terms to target both representation-level and alignment-level failure modes make sense for attacking LLMs.\n- The evaluation covers both images and videos, demonstrating the generality of the framework across static and temporal inputs.\n- The paper presents a reasonably comprehensive set of experiments showing CAVALRY’s efficacy across several MLLMs."}, "weaknesses": {"value": "- The training strategy appears to treat all video frames independently, using the same QA supervision across frames rather than modeling temporal dependencies explicitly. This raises concerns that the attack might ignore inter-frame dynamics and overfit to per-frame perturbations rather than truly exploiting temporal vulnerabilities.\n- Aside from a few qualitative examples (e.g., Figure 2), the paper lacks quantitative analysis of visual distortion (e.g., PSNR, LPIPS, or L2 norms) and human perceptual thresholds. Some examples images look heavily corrupted; reporting distortion budgets and the number of frames required for a successful attack would clarify real-world plausibility.\n- The method combines three loss terms but does not fully isolate their contributions. An ablation study showing how each loss affects attack strength, perceptual quality, and transferability would strengthen understanding of why CAVALRY works.\n- Training with the same QA for all frames may lead to artifacts: attacks might exploit static shortcuts rather than disrupting temporal reasoning. Evaluation on short-version of videos that is necessary for the true answer would give further understanding on how MLLMs are attacked."}, "questions": {"value": "- Equation (5) targets likelihood divergence for the ground-truth answer. Do you have any theoretical or empirical evidence that optimizing this loss reliably causes semantically different outputs (not just low-confidence or truncated answers)?\n- How does attack performance vary with the fraction of frames you perturb in a video? Is perturbing a single frame (or a small subset) sufficient in typical cases, or do you need to perturb most/all frames to achieve high success?\n- Based on the nature characteristics of proposed training strategy, it seems like the model is not trained to consider the temporal relationships of different frames. That said, the first frame of the video should be used to attack MLLMs in a way that unknown future information is related. Are there any further analysis to ensure that CAVALRY truly considers the temporal characteristics of videos?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FmcgOxyxQ1", "forum": "qins7MdufH", "replyto": "qins7MdufH", "signatures": ["ICLR.cc/2026/Conference/Submission10588/Reviewer_wEJt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10588/Reviewer_wEJt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956030550, "cdate": 1761956030550, "tmdate": 1762921856717, "mdate": 1762921856717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CAVALRY, a unified adversarial attack framework targeting both image and video Multimodal Large Language Models (MLLMs). Instead of conventional boundary-based or feature-space attacks, the method disrupts the generative process of MLLMs through generative likelihood divergence maximization, effectively “severing” the link between visual perception and language generation. It employs a progressive two-stage generator trained to produce spatially and temporally coherent perturbations: large-scale pretraining on LAION-400M followed by fine-tuning on LLaVA-Instruct-150K and Video-MME for temporal coherence. Experiments on seven MLLMs, including GPT-4.1, Gemini 2.0, and QwenVL-2.5, show state-of-the-art performance, outperforming baselines by 22.8 % on video and 34.4 % on image benchmarks. The paper claims broad transferability, computational efficiency, and responsible release for AI-safety evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Integrates both image and video adversarial attack settings into a single formulation, demonstrating flexibility across modalities. Bridges vision-language and temporal reasoning vulnerabilities, a gap unaddressed by prior work. This work establishes a new class of generative-disruption attacks for MLLMs.\n- Operates on token-level autoregressive manipulation rather than feature logits, offering higher semantic fidelity. This paradigm shift yields measurable performance gains, and it is novel in this field.\n- Benchmarked on seven diverse MLLMs, including commercial and open-source systems demonstrated its effectiveness of the proposed method."}, "weaknesses": {"value": "- The SRR metric depends on GPT-4o-mini or GPT-4-turbo scorers, which may introduce bias. No cross-validation with other judges or independent human raters.\n- Equation (8) introduces $\\lambda_1$, $\\lambda_2$, $\\lambda_3$, weighting, but no individual ablation for each loss. Lack of sensitivity analysis limits understanding of the contribution balance.\n- No adversarial detection, adversarial training, or model-side robustness evaluation provided. It would be good to include some experiments on mitigation methods."}, "questions": {"value": "- Could the authors provide some comparison of the evaluation metric with other models?\n- Could authors provide some insights on how to tune $\\lambda_1$, $\\lambda_2$, $\\lambda_3$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9F9MtmXxEt", "forum": "qins7MdufH", "replyto": "qins7MdufH", "signatures": ["ICLR.cc/2026/Conference/Submission10588/Reviewer_1Yvi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10588/Reviewer_1Yvi"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073362716, "cdate": 1762073362716, "tmdate": 1762921856286, "mdate": 1762921856286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}