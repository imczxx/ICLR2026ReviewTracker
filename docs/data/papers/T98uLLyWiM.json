{"id": "T98uLLyWiM", "number": 19985, "cdate": 1758301216900, "mdate": 1759897008184, "content": {"title": "Adapting Reinforcement Learning for Path Planning in Constrained Parking Scenarios", "abstract": "Real-time path planning in constrained environments remains a fundamental challenge for autonomous systems. Traditional classical planners, while effective under perfect perception assumptions, are often sensitive to real-world perception constraints and rely on online search procedures that incur high computational costs. In complex surroundings, this renders real-time deployment prohibitive. To overcome these limitations, we introduce a Deep Reinforcement Learning (DRL) framework for real-time path planning in parking scenarios. In particular, we focus on challenging scenes with tight spaces that require a high number of reversal maneuvers and adjustments. Unlike classical planners, our solution does not require ideal and structured perception, and in principle, could avoid the need for additional modules such as localization and tracking, potentially resulting in a simpler and more practical implementation. Also, at test time, the policy generates actions through a single forward pass at each step, which is lightweight enough for real-time deployment. The task is formulated as a sequential decision-making problem grounded in a bicycle model dynamics, enabling the agent to directly learn navigation policies that respect vehicle kinematics and environmental constraints in the closed-loop setting. A new benchmark is developed to support both training and evaluation, capturing diverse and challenging scenarios. Our approach achieves state-of-the-art success rates and efficiency, surpassing classical planner baselines by +96\\% in success rate and +52\\% in efficiency. Furthermore, we release our benchmark as an open-source resource for the community to foster future research in autonomous systems.", "tldr": "We introduce a real-world benchmark for parking in constrained spaces and propose a reinforcement learning approach that surpasses classical planners.", "keywords": ["reinforcement learning", "path planning", "autonomous parking"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e96d41b5719b7aa892e410a78e3f0851a57eecdf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a reinforcement learning-based method for parking path planning using a bicycle kinematic model, demonstrating excellent performance in narrow and complex environments. \nTo address the challenges of long horizons, high-precision control, and sparse rewards, the approach introduces two key techniques: curriculum learning, which gradually progresses from simple to complex scenarios to improve training stability; and action chunking, which groups multiple primitive actions into macro-actions to balance exploration efficiency and maneuver precision. For fair comparison, the method uses the same inputs as Hybrid A: ego and target poses, and obstacle contours. A \"rollout-back\" mechanism is designed to generate feasible initial states, resolving infeasibility caused by sparse representation. Inputs are transformed into the ego frame, normalized, and constrained by a limited perception range to simulate real sensor limitations; a cross-attention mechanism enables the agent to focus on critical obstacles. \nExperiments are conducted on the authors' self-constructed ParkBench benchmark, showing that the proposed method significantly outperforms Hybrid A in success rate, planning time, and travel distance. Ablation studies validate the effectiveness of curriculum learning and action chunking, while attention visualizations demonstrate strong environmental understanding. Despite degraded performance in open spaces and reliance on manually designed curricula, this work highlights the great potential of RL for generating human-like trajectories in complex parking tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe ParkBench is comprehensively designed and highly representative of real-world parking scenarios, making a significant contribution to the evaluation of parking trajectory planning methods. We look forward to its future expansion to cover even more diverse maneuvers.\n2.\tThe paper effectively addresses a practical engineering challenge—initial pose infeasibility caused by sparse obstacle representation—through a well-designed roll-out mechanism that generates valid starting states, thereby improving training stability and realism.\n3.\tThe integration of curriculum learning and action chunking significantly enhances both success rate and computational efficiency, demonstrating clear advantages over classical planning approaches in complex, constrained environments."}, "weaknesses": {"value": "1.\tThe current curriculum is manually designed specifically for rear-in parking, which limits its transferability to other maneuvers such as parallel or angle parking. To enhance generalization and scalability, the authors are encouraged to explore automated curriculum learning methods that can adaptively generate training tasks across diverse scenarios.\n2.\tWhile the method performs exceptionally well in narrow and complex environments, its performance degrades in open or sparsely constrained spaces. Given that the current dataset primarily focuses on challenging narrow scenarios, it would be valuable to include more diverse environments—especially open layouts—in both training and evaluation, along with a detailed analysis of the model’s behavior across different scene types.\n3.\tAlthough the limited perception range simulates partial observability under real-world sensing constraints, all experiments are conducted in static environments. For a more realistic assessment, future work should evaluate the method’s robustness in dynamic settings involving moving obstacles such as pedestrians and vehicles, which are common in practical parking scenarios."}, "questions": {"value": "1.\tIn Table 1, the line of\"PPO (Ours) CL✓ Chunking✗\" shows a significantly higher number of pivot points (53.4) compared to Hybrid A (3.2). Does this indicate that, in the absence of action chunking, the policy tends to generate excessive and unnecessary forward-backward transitions? It would be helpful to provide visualizations of such trajectories and analyze the underlying causes of this oscillatory behavior—whether it stems from poor temporal credit assignment, suboptimal exploration, or instability in policy learning.\n2.\tThe 51 scenarios in ParkBench are extracted from real-world datasets. Could the authors provide more details on the distribution of these scenarios? Specifically, how many correspond to perpendicular parking, parallel parking, or other configurations? A breakdown of scene types would help assess the benchmark's diversity and representativeness and clarify the scope of the method’s evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vt4Yzg6pPQ", "forum": "T98uLLyWiM", "replyto": "T98uLLyWiM", "signatures": ["ICLR.cc/2026/Conference/Submission19985/Reviewer_JmMd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19985/Reviewer_JmMd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761610541133, "cdate": 1761610541133, "tmdate": 1762932888263, "mdate": 1762932888263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper employs a RL–based approach to replace the classical planner in path planning tasks. It empirically demonstrates the effectiveness of the proposed method in parking scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper shows the effectiveness of using the PPO algorithm as a planner in parking scenarios.\n\n2. It introduces a new benchmark, ParkBench, to facilitate research on path planning in parking environments."}, "weaknesses": {"value": "1. In the introduction, the authors claim that RL-based methods, as representatives of closed-loop approaches, remain underexplored in path planning. However, RL-based planners have been extensively studied in various path planning tasks that consider practical constraints across diverse real-world applications, including transportation, warehousing, and surgical robotics.\n\n2. Numerous studies have focused on developing RL-based planners in related domains. Although this paper centers on parking scenarios, this domain is closely connected to transportation and autonomous driving. The related work section does not sufficiently review these prior studies.\n\n3. In the empirical evaluation, the proposed method is compared only with one classical heuristic baseline. The authors do not include adequate baseline methods, especially those that also utilize RL techniques. As a result, the experimental results do not convincingly demonstrate the contribution of this work.\n\n4. The classical heuristic method used as the baseline has shown strong performance and generalization across other scenarios. To further validate the proposed method’s effectiveness and generalization, it would be beneficial to evaluate it on additional tasks.\n\n5. In the experimental section, the authors integrate only the PPO algorithm into the parking planner. A comparison with other standard RL algorithms would provide a more comprehensive understanding of the framework’s robustness and performance.\n\n6. The contribution of this paper is limited. It primarily applies an existing RL algorithm to a parking task, which is not a novel direction for the community, although the development of the ParkBench benchmark and consideration of practical constraints are appreciated."}, "questions": {"value": "1. Since this work demonstrates the practicality of an RL-based planner trained on ParkBench, is it possible to deploy this method in a real vehicle, similar to how the A* algorithm has been applied?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EHo29hhGzE", "forum": "T98uLLyWiM", "replyto": "T98uLLyWiM", "signatures": ["ICLR.cc/2026/Conference/Submission19985/Reviewer_683i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19985/Reviewer_683i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738265280, "cdate": 1761738265280, "tmdate": 1762932887265, "mdate": 1762932887265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new reinforcement learning based path planning system for parking problem in cluttered environments. The paper also introduces ParkBench, a new parking benchmark using bicycle model dynamic for simulating the environment with a mobile robot. The method demonstrates more than 96% success and 52% efficiency improvement compared to classical path planner."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* Strong empirical results: Achieves 92.2% success on ParkBench vs. 47.1% for Hybrid A*, and 2× improvement in time efficiency.\n* Benchmark contribution: ParkBench fills a benchmark gap in parking evaluation, providing 51 realistic layouts for reproducibility and comparison."}, "weaknesses": {"value": "* The RL system presented in this paper is fairly straight forward. The component includes a handcrafted curriculum for initial configuration and  motion primitive (action chunking). These components are well-established in the literature and the author does not demonstrate sufficient effort in integrating these components as a whole system.\n\n* There exists a lot of RL-based motion planning for mobile robot, many of them are trained in high-fidelity simulator, such as Gazebo and IsaacLab. The author does not fully address these prior work to demonstrate the novelty of the system.\nXu, Zifan, et al. \"Benchmarking reinforcement learning techniques for autonomous navigation.\" arXiv preprint arXiv:2210.04839 (2022).\nAkmandor, Neşet Ünver, et al. \"Deep reinforcement learning based robot navigation in dynamic environments using occupancy values of motion primitives.\" 2022 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2022.\n\n* Writing tone: Some claims (e.g., “potentially eliminates need for localization and tracking”) are speculative and should be framed more cautiously. The use of language is not precise and formal for a research paper."}, "questions": {"value": "Please see the weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A3SGxwE2HO", "forum": "T98uLLyWiM", "replyto": "T98uLLyWiM", "signatures": ["ICLR.cc/2026/Conference/Submission19985/Reviewer_efQf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19985/Reviewer_efQf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954418286, "cdate": 1761954418286, "tmdate": 1762932886542, "mdate": 1762932886542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present an approach for real-time path planning in tight spaces with Deep Reinforcement Learning. The paper shows how the approach outperforms an A* based baseline by a large margin. In addition to the paper they provide a benchmark for parking in tight spaces based on 2d coordinates, like lidar points showing the surroundings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The performance compared to the baseline is quite impressive. \n\nThe paper also contributes a benchmark.\n\nIncluding a bicycle model introduces an easily tunable component, if more complex vehicle models are needed, and that should ensure feasible trajectories.\n\nThe benchmark provides very simple lidar scans which may seem like a disadvantage but is an easily transferable representation that should be easy to provide in many contexts. It is also very easy to simulate as the paper shows.\n\nThe approach, and architecture, seems surprisingly simple."}, "weaknesses": {"value": "While having impressive results, they are on a novel self-created benchmark which did not give other authors opportunity to optimize on it. Therefore, results have to be taken with a grain of salt and if the benchmark is accepted in the domain time must tell how these results hold up.\n\nIt seems more direction changes are needed, compared to a simple A* algorithm. While other metrics indicate better performance it would be interesting how this scales, i.e. how many more pivot points, which should come with a more complex trajectory, are needed for what performance boost?\n\nThe approach only compares to Hybrid A* and no other approach. It would have been possible to compare with other non-deep learning methods like a Reeds-Shepp Curve planner or Dijkstra. \n\nFinally, this is a very good solution for this task but I am not completely convinced this is the right venue for it. Maybe IROS, IV or ITSC would have been more suitable. I leave that decision to the AC."}, "questions": {"value": "Given the many penalties which are not primary goals, such as the goal achievement and collision, what are local minimas that were observed during training. In other words, were there \"cheating\" behaviors where the model optimized not getting penalized for being idle e.g. by moving very slow. \n\nPlease note that concurrent work on the horizon should be compared with this work in the future. E.g. \"RAFT: Regularized Adversarial Fine-Tuning to Enhance Deep Reinforcement Learning for Self-Parking\n, Pighetti et al.\". This was published in August 2025 so it is irrelevant for judging this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "plZQCFWpkc", "forum": "T98uLLyWiM", "replyto": "T98uLLyWiM", "signatures": ["ICLR.cc/2026/Conference/Submission19985/Reviewer_cNCZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19985/Reviewer_cNCZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971345872, "cdate": 1761971345872, "tmdate": 1762932885939, "mdate": 1762932885939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}