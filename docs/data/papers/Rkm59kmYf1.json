{"id": "Rkm59kmYf1", "number": 22450, "cdate": 1758331207967, "mdate": 1759896865445, "content": {"title": "Causal Partial Identification with Data Augmentation", "abstract": "We provide a first analysis for using knowledge of symmetries in data generation via data augmentation (DA) transformations for sharpening bounds on causal effects derived from observational data. The causal effect of the treatment $X$ on outcome $Y$ is generally not identifiable from observational data alone if their common causes, also known as confounders, are unobserved. Partial identification (PI) entails estimating bounds on such treatment effects by solving a constrained optimization problem that encodes different assumptions imposed on data generation. PI has use in many application domains where such bounds are sufficient to inform policy decisions, even if the treatment effect itself is not identifiable. To this end, we propose that the cheap and ubuquitous tool of DA, which is otherwise used for mitigating estimation variance, can also be repurposed for sharpening bounds in PI. This is especially useful when the data is complex (i.e., continuous, high-dimensional), as imposing additional constraints becomes expensive compared to a simple pre-processing via DA.", "tldr": "We repurpose symmetry-based data augmentation as an interventional tool to provably sharpen the bounds on causal effects derived from partial identification.", "keywords": ["Causal Inference", "Partial Identification", "Invariance", "Data Augmentation"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bddf56d356025ebb2db4dc64e466ab45877d553d.pdf", "supplementary_material": "/attachment/b24ad72f60d12c88e68fbeaae3ae6142c0816c59.zip"}, "replies": [{"content": {"summary": {"value": "The paper seeks to provide theoretical bounds for using data augmentation when coupled with partial identifications. The bounds proposed give both best- and worst-case scenarios for partially identifying the causal averages they employ, with respect to predefined causal risks."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Although I am no expert in neither domain augmentation nor partial identification, it seems to me that combining these two ideas is quite novel. The quality of writing is not incredibly good (see Weaknesses). \n\nThe theorems appear to be sound and (without taking a close look at the proofs) the conclusions do seem natural enough. \n\nThe concepts presented are clearly explained and it is clear that the theorems are algebraic derivations based on these definitions."}, "weaknesses": {"value": "Adding an image to illustrate an observation of the dataset described in Section 6.2 would make the explanation more intelligible.\n\nThe abuse of notation in Section 3 (Line 202) was quite confusing at first. It seems to me that the notation employed could be formally introduced, to prevent confusions. \n\nThe first equation hints to a very general functional model, that is later replaced by a linear model. Possibly because the results of Vankadara et al., (2022) are only valid for linear models. Is this the only bottleneck that prevents extending the current framework to more general models?\n\n### Typos and errors: \nSeveral citations in the first pages (and possibly other parts of the text) should be citep, instead of citet. This makes reading of these pages quite painful and annoying.\n\n- Line 20: ubiquitous.\n- Line 273: extra 'e' in perturbs.\n- Line 281: I suspect there are some parentheses or mathematical notation missing for $f^T x \\in H_{da+pi}(x), H_{pi}(x)$.\n- Line 461: out -> our.\n- Line 464: identification.\n- Line 466: Rosenbaum is inconsistently-cited (no year).\n- Equation (5), the norms used are not clearly defined. The (very subtle) difference between $X$ and $\\mathbf{x}$ inside the do operator, makes these hard to distinguish on a first read."}, "questions": {"value": "The authors claim these bounds are valid for the infinite-data setting and naturally are only able to assess in a finite-data setting. However, it would be good to numerically assess how the derived bounds behave for different data-size regimes.\n\nOutside of the optical device data used in Section 6, it is hard for me to envision the type of causal questions where the proposed tools would be useful. Could the authors point out other settings where this could be useful? Note that I am not requesting more experiments with this question, but they would be welcome.\n\nIn Theorems 1 and 2, I assume that the '_equality iff_' would also correspond to having slack equal to zero. Is this intuition correct? If so, how does the slack increase for different degrees of independence? Since the models are linear (and possibly Gaussian), measuring this dependence could be easily done by correlations/covariances.\n\nOn that note, the Gaussian assumption (over $G, U, N_X$, and $N_Y$) in Example 1 is quite strong (and it later affects all the theoretical developments), in what key ways does your contribution depend on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RNeDlxUQ9g", "forum": "Rkm59kmYf1", "replyto": "Rkm59kmYf1", "signatures": ["ICLR.cc/2026/Conference/Submission22450/Reviewer_2Gcy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22450/Reviewer_2Gcy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760960635680, "cdate": 1760960635680, "tmdate": 1762942223032, "mdate": 1762942223032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigated whether outcome‑invariant data augmentation (DA) can sharpen partial identification (PI) bounds for causal effects under the presence of unmeasured confounders. The key idea lies in constructing an outcome-invariant data augmentation transformations G as a transformation intervention do(x=Gx). A very simple linear Gaussian case study is provided. By experiments, the authors showed that DA could help reduce the length for the pointwise interval width. For the theoretical analysis, the authors showed that DA lowers worst‑case causal excess risk over the identified set. This seems to be a flexible approach as the DA approach is model‑agnostic and compatible with many PI frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe conceptual idea is very clear and straightforward. Constructing a outcome-invariant DA as transformation intervation and then use it as a plug-in module in any PI pipeline.\n2.\tThe theoretical analysis is solid, especially for the and lower worst case excess risk (Theorem 2)\n3.\tThe proposal is a pre‑processing step that can be composed with existing PI methods without changing their solvers or constraint sets."}, "weaknesses": {"value": "1.\tThe linear–Gaussian SEM seems still restrictive with additive noise and a particular sensitivity model. Under other common cases, such as non-Gaussian, non-linear outcome model, many guarantees may not work.\n2.\tThe outcome-invariant assumption is hard to be testable, there is no diagnostic or stress test for misspecified augmentations.\n3.\tThe numerical results are not comprehensive enough. Only one real dataset (Optical Device) is used with fixed sample size (n=1000)."}, "questions": {"value": "1.\ton the Optical Device data, why should flips/rotations/Gaussian noise be outcome‑invariant for f Can you provide predictive‑invariance checks?\n2.\tCan any of the theoretical results be extended to non-Gaussian case? Or with some additional mild assumption?\n3.\tHow to choose G seems very important and tricky. A practical guideline for choosing G is very necessary.\n4.\tA comprehensive sensitivity analysis to mis‑specified DA is very important for understanding the role of DA.\n5.\tThere are many augmentation parameters, such as noise scale, angle, etc. How these parameters affect the PI and the prediction performance?\n6.\tThe authors should test DA in multiple PI framework to demonstrate compatibility and gains beyond the partial R^2 model.\n7.\tSince authors argue compatibility with IVs, a toy example where an IV is present, showing that DA preserves IV validity and may further tightens bounds, should be very convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pslsJgknlA", "forum": "Rkm59kmYf1", "replyto": "Rkm59kmYf1", "signatures": ["ICLR.cc/2026/Conference/Submission22450/Reviewer_dc4P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22450/Reviewer_dc4P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662979611, "cdate": 1761662979611, "tmdate": 1762942222841, "mdate": 1762942222841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method based on data augmentation to improve partial identification bounds in causal inference when unobserved confounding prevents point identification. The key idea is to use data augmentation, specifically, an outcome-invariant augmentation, as an auxiliary source of information that can act as a ``soft intervention.''\nIf we can transform data in ways that leave the outcome function unchanged (say, rotations of a picture in a classification task), these transformations can help tighten the partial id bounds.\nThe method requires background knowledge about the regression function, for instance the null space of the coefficients matrix in a linear regression model, which can be quite limiting in practice."}, "soundness": {"value": 4}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Even though the analysis is restricted to linear-Gaussian systems, I believe most of the results have straightforward generalizations to more complex models.\n\n---\n2. Synthetic and semi-synthetic experiments consistently show that DA+PI yields tighter bounds than baseline PI."}, "weaknesses": {"value": "1. Significant overlap with Akbar et al [1]:\nTaking a look at the first reference of the paper, one can notice immediately that this paper has a significant overlap with, and is essentially not adding much to Akbar et al. Surprisingly, a great deal of content is copied without even rewording. Ideas are copied from that paper: DA as soft intervention, Figure 2 of the paper with its caption, the running example of the paper, and most significantly, the theoretical results of this paper such as proposition 2, Lemma 3 and Lemma 4, all appear in Akbar et. al., and the rest of the results provided here are quite trivial or straightforward at best to prove given those.\nCompared to Akbar et al., I don't see much of a novel idea, novel proof, novel result, or novel presentation, begging the question what is the merit of this paper given that? Only bringing up the observation that DA can be used for partial identification too? Then I do not believe this paper is contributing enough to the literature.\n\n---\n2. The paper presents its main claims (“valid bounds”, “sharpened partial identification”, etc) early as if fairly general (Sections 1–3). But when you dig into Section 4, you find that the proofs are only in the linear Gaussian “Example 1” setting with heavy structure/assumptions. This can be quite misleading. The authors should either restrict their claims explicitly in the first few sections, or lift their proofs to more general settings (if they can). \n\n---\n3. The manuscript repeatedly introduces notation and small definitions close together; readers must hunt back and re-read definitions frequently. I believe this paper can be presented in a much better (readable, at the least) way if a bit of time is spent on it.\n\n---\n4. I am not convinced by the idea of using the background knowledge (say symmetries of f) in the way that this paper suggests. Read my question below too."}, "questions": {"value": "If you already have symmetry knowledge about f, why do DA instead of directly imposing constraints?\nIf the researcher truly knows those symmetries, you can (in many cases) impose them directly in the inference/optimization (e.g., enforce invariance in hypothesis class H, add equality constraints, or augment the objective with invariance penalties) rather than applying DA as a pre-processing step. The manuscript should either 1) clearly justify why DA is preferable to directly injecting invariance into the estimator (computational simplicity? easier to combine with off-the-shelf solvers? better finite-sample behavior?), and provide a brief theoretical or experimental comparison; or 2) explicitly treat DA as one practical way to operationalize symmetry knowledge and discuss tradeoffs (what you lose/gain compared to e.g. constraining H)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lq8Xkfn5Bz", "forum": "Rkm59kmYf1", "replyto": "Rkm59kmYf1", "signatures": ["ICLR.cc/2026/Conference/Submission22450/Reviewer_ba8S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22450/Reviewer_ba8S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928792262, "cdate": 1761928792262, "tmdate": 1762942222676, "mdate": 1762942222676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies partial identification bounds under known symmetries of the causal effect, i.e., under invariances of $f$ when $Y = f(X) + \\xi$ when $\\xi$ may be correlated with $X$, and hence acts as an unobserved confounder. Primarily focusing on the population setting, they define a hypothesis class $H_{pi}$ of possible causal effects functions (each $h \\in H_{pi}$ is a function from $x$ to $y$), which are consistent with the observational distribution $P_{X,Y}$ and satisfy the invariance constraints. For each $x$, this hypothesis class induces a set of possible causal effects $H_{pi}(x)$, which in turn can be used to define a worst-case excess risk. Assuming a multivariate Gaussian distribution over $(X, Y, \\xi)$, they show that the excess risk strictly decreases under these constraints, as long as the symmetries are not almost surely orthogonal to the expectation of $X$ given $\\xi$.\n\nIn practice, rather than strictly enforcing the invariances, they are captured using data augmentation. This approach is corroborated through a simulation experiment on a linear model, giving sharper identification bounds, and in real-world experiments on the Optical Device dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Originality and significance:** The use of known symmetries to sharpen partial identification bounds is an interesting and (to the best of my knowledge) novel direction. As the authors discuss, such symmetries are common in many applications, especially in scientific machine learning, where causal inference is quite important, so the combination of the two is a very good match.\n\n**Quality and clarity:** The work is well-executed, the motivation is clear, and the mathematical details are well-written."}, "weaknesses": {"value": "## Major weaknesses\n\n1. **Limitation to multivariate Gaussian setting:** To the best of my understanding, the results are limited to multivariate Gaussian distributions on $(X, Y, \\xi)$, and this limitation is not made as transparent as it should be. Based on the text after Assumption 1, I understand that the partial R-squared sensitivity model is not a necessary restriction, but I'm less certain about the Gaussianity assumption (or at least, a linearity assumption). For example, Proposition 1 invokes a Lebesgue measure over $H_{pi}$, which is initially defined as a function space, but in the proofs, $h$ is taken to be a vector (i.e., the coefficients of a linear function). Overall, this lack of transparency gives the feeling that the results are being oversold.\n2. **Not enough focus on the quantitative form of the results:** In connection to Weakness 1, I would be much more interested to see Lemma 5 in the main paper and a more quantitative discussion of *how much* the invariances sharpen the partial identification bounds. Theorem 1, Proposition 2, and Theorem 2 don't provide any intuition about how much the invariance sharpens the bounds, which I think would be the most interesting part.\n\n## Minor weakness\n3. **Overly focused on data augmentation:** I think a more logical way to present the results would be to focus on how known symmetries/invariances improve the partial identification bound, and afterwards connect these results to data augmentation. The results are really about the restriction of the hypothesis space, and would hold even when strictly enforcing the invariance - the approach of using data augmentation is more of a practical implementation detail.\n-"}, "questions": {"value": "Please address the Major Weaknesses, especially (1)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NJcG0eCPSM", "forum": "Rkm59kmYf1", "replyto": "Rkm59kmYf1", "signatures": ["ICLR.cc/2026/Conference/Submission22450/Reviewer_1PwZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22450/Reviewer_1PwZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939996182, "cdate": 1761939996182, "tmdate": 1762942222495, "mdate": 1762942222495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}