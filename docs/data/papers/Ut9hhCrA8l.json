{"id": "Ut9hhCrA8l", "number": 22303, "cdate": 1758329341282, "mdate": 1759896873590, "content": {"title": "Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation", "abstract": "The impressive capabilities of Large Language Models (LLMs) have fueled the\nnotion that synthetic agents can serve as substitutes for real participants in human-\nsubject research. To evaluate this claim, prior research has largely focused on\nwhether LLM-generated survey responses align with those produced by human\nrespondents whom the LLMs are prompted to represent. In contrast, we address\na more fundamental question: Do agents maintain internal consistency, retaining\nsimilar behaviors when examined under different experimental settings? To this\nend, we develop a study designed to (a) reveal the agent’s internal state and (b)\nexamine agent behavior in a conversational setting. This design enables us to\nexplore a set of behavioral hypotheses to assess whether an agent’s conversational\nbehavior is consistent with what we would expect from its revealed internal state.\nOur findings show significant internal inconsistencies in LLMs across model families\nand at differing model sizes. Most importantly, we find that, although agents may\ngenerate responses matching those of their human counterparts, they fail to be\ninternally consistent, representing a critical gap in their capabilities to accurately\nsubstitute for real participants in human-subject research.", "tldr": "", "keywords": ["LLM", "Agent", "Simulation"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/52a47b4039ca4548476d5ac9fa247c29894c675e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors attempt to test the \"internal consistency\" of the so-called LLM agent. To do this, the authors hand-selected a series of LLM agent profile combinations and selected a number of topics with varying degrees of controversy, had the LLM agents engage in conversations with each other in order to discuss these topics, and observed the performance of the LLM agents during the experiment as well as the responses of the LLM agents to the questions that were specifically designed to assess their \"internal state\" in order to summarize the conclusions of the experiment. The entire experiment ultimately gave the conclusion that the LLM agent could not replace human participants in human-subject research."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This paper is concerned with whether LLM agents can replace human participants in social simulations and tries to give an answer through a series of experimental designs."}, "weaknesses": {"value": "# The Critical weakness\nThis paper does not give a clear definition of the LLM agent it discusses. Since LLM is designed as a system that complements text based on the context of the input, the behavioral performance of the LLM agent will depend entirely on the design of its prompts. Therefore, the conclusions given by all the experiments in this paper can only represent the situation of the LLM agent they are designed for and are not generalizable.\n\n# Other More Intuitive weaknesses\n1. This paper does not use the ICLR 2026 LaTeX template.\n2. The main text does not contain any description of the technical points such as the design of the experimental framework, the design of the LLM agents, or the selection of topics. Please note that the appendices are not required reading and the text should be self contained.\n3. The formatting of Table 1 is incorrect; the top horizontal line is missing.\n4. Given the relatively short length of the LLM agent dialogues presented in this paper (API call fees should not be an issue), the decision to use small-sized LLMs for testing means that the evaluation conclusions cannot accurately represent the current state of LLM capability development."}, "questions": {"value": "Please follow the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CGc4FC0OuG", "forum": "Ut9hhCrA8l", "replyto": "Ut9hhCrA8l", "signatures": ["ICLR.cc/2026/Conference/Submission22303/Reviewer_aH5L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22303/Reviewer_aH5L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761455137191, "cdate": 1761455137191, "tmdate": 1762942160965, "mdate": 1762942160965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the notion of behavioral coherence in large language model-based agents, defined as the internal consistency between their latent attributes (for example, preferences and openness) and their observable conversational behavior. The authors design pairwise interaction experiments across nine topics to examine whether agents with similar preferences achieve higher agreement than those with opposing views. The paper reports that large language model agents often show surface-level agreement yet internal inconsistency across conditions, suggesting a lack of stable behavioral coherence."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The topic of examining internal behavioral consistency in large language model agents is timely and relevant to the broader discussion of using language model agents in social simulation.\n\n* The experimental setup is clearly described, and the paper is easy to follow.\n\n* The attempt to move beyond external alignment toward internal consistency evaluation is interesting."}, "weaknesses": {"value": "1. The definition of behavioral coherence is not rigorously derived from social science or cognitive theory. It is instead motivated by intuitive expectations such as “agents with similar preferences should agree more”. These expectations are plausible but remain heuristic rather than theoretically or empirically justified. Without a rigorous formulation, the concept of behavioral coherence risks being circular and untestable.\n\n2. The conclusion that language model agents lack internal behavioral coherence is not well supported. The observed pattern of “surface agreement but internal contradiction” could have many alternative explanations, including the conversational format, prompt framing, or model uncertainty. Moreover, humans themselves often show context-dependent inconsistency, so it is unclear whether these results truly reflect a structural limitation of large language model agents.\n\n3. The study does not include any human data or behavioral benchmark to anchor its conclusions. Without comparing model patterns to real human interactions, it is impossible to determine whether the observed inconsistencies are meaningful or simply reflect normal human variability. \n\n4. The methodology mainly depends on prompt-based simulation and descriptive statistical analysis. The contribution is therefore conceptual rather than technical, and the insights are mostly intuitive and qualitative."}, "questions": {"value": "* How is “behavioral coherence” theoretically grounded beyond intuitive expectations, such as the idea that agents with similar preferences should agree more?\n\n* Could the authors include or discuss a human baseline to support the findings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cdZI3fDZCs", "forum": "Ut9hhCrA8l", "replyto": "Ut9hhCrA8l", "signatures": ["ICLR.cc/2026/Conference/Submission22303/Reviewer_36tt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22303/Reviewer_36tt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811280507, "cdate": 1761811280507, "tmdate": 1762942160690, "mdate": 1762942160690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper asks whether LLM agents behave consistently enough to truly stand in for humans in social research. Instead of just checking if their answers sound human, the authors test whether agents act in line with their own stated preferences and openness during conversations. They find that while LLMs often appear reasonable, they’re internally inconsistent: rarely disagreeing even when they should, favoring positive views, and being overly influenced by topic sensitivity. These issues show up across models and sizes, suggesting a broader limitation. The authors argue that true behavioral coherence, not just surface realism, is essential before treating LLMs as human substitutes."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "•\tThe introduction is well written.\n\n•\tThe overall flow of the paper is clear.\n\n•\tI appreciate the fact that the authors tried to back the claims by statistical tests, which is often missing in ML papers."}, "weaknesses": {"value": "•\tThroughout the paper, the authors seem to confuse “sentiment” with “stance”, which are orthogonal concepts. This is an NLP 101 issue. Briefly here:  a positive stance (e.g., “I support the mayor candidate”) can be expressed with either a positive sentiment (“I support him because I love his character”) or a negative sentiment (“I support him because I hate the current mayor”.), and same for negative stance. Based on the context, what the authors want to say is “stance” rather than “sentiment”. Using “sentiment” to refer to “stance” causes confusion.\n\n•\tSection 2. Missing literature. Previous studies have shown that demographic information is usually insufficient to align LLM’s response with humans - the value it provides is little to none, and that one should instead include latent belief to ensure alignment [1]. Given this, you want to justify why you only include demographic information.\n\n\n•\tSection 3.1. You should formally define what “U” stands for.\n\n•\tSection 3.2. You should provide measures when using LLM as a judge, e.g., validated through human annotation.\n\n\n•\tSection 4.1 “We establish the other end of the spectrum, the disagreement side, by assuming it to be the inverse of agreement as shown in Table 1.” -> You didn’t justify why this assumption makes sense. The assumption is critical in calculating the expected probability in Table 1, Figure 4, and throughout this section.\n\n•\tAccount for model inherent bias? That may explain why “shared dislike consistently yields lower agreement”.\n\n\n•\tFigure 5 is hard to understand. The label on the right (0,1,2,3) should be better explained.\n\n•\tFinding #4 bears more interpretation and analysis. You should elaborate what roles the contentiousness level actually plays. The current writing only describes Figure 6 verbatim.\n\n\n•\tSection 4.2. You should elaborate how you control the openness level. \n\n\n•\tSection 4.3 and Table 2. Based on the statistical test, the findings you listed in section 4.1 are mostly unfounded, except the two surface-level findings. In that case, I think the majority of the findings and conclusions are not justified by the statistical tests.\n\nReferences\n\n\n[1] Chuang, Y. S., Nirunwiroj, K., Studdiford, Z., Goyal, A., Frigo, V., Yang, S., ... & Rogers, T. (2024, November). Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks. In Findings of the Association for Computational Linguistics: EMNLP 2024 (pp. 14010-14026)."}, "questions": {"value": "•\tHow do you decide the distribution over the demographic space? How sensitive is your result to this distribution?\n•\tHow do you decide the “controversy level”?\n•\tSection 4.3. Table 2. Are you concerned about the power of the statistical tests being too low? Did you do a priori any power analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TsHjGYjJNA", "forum": "Ut9hhCrA8l", "replyto": "Ut9hhCrA8l", "signatures": ["ICLR.cc/2026/Conference/Submission22303/Reviewer_AR41"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22303/Reviewer_AR41"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975035872, "cdate": 1761975035872, "tmdate": 1762942160440, "mdate": 1762942160440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "N/A"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "N/A"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "The paper appears to deviate from the official ICLR 2026 formatting requirements: it uses incorrect fonts, a wider abstract margin, omits the mandatory header line “Under review as a conference paper at ICLR 2026”, and lacks the required paragraph describing LLM usage.\n\nSuch formatting inconsistencies create an unfair situation for other authors who have invested time ensuring full compliance with the submission guidelines. Therefore, this paper should be rejected on formatting grounds."}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hUILCZOUGS", "forum": "Ut9hhCrA8l", "replyto": "Ut9hhCrA8l", "signatures": ["ICLR.cc/2026/Conference/Submission22303/Reviewer_Jjd4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22303/Reviewer_Jjd4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985324041, "cdate": 1761985324041, "tmdate": 1762942160141, "mdate": 1762942160141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}