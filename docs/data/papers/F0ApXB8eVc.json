{"id": "F0ApXB8eVc", "number": 8910, "cdate": 1758102058663, "mdate": 1759897754640, "content": {"title": "SKILL: Structural Knowledge Injection into Large Language Models for Inductive Knowledge Graph Reasoning", "abstract": "Knowledge Graph Reasoning (KGR) aims to predict missing (head, relation, tail) triples by inferring new facts from existing ones within a knowledge graph. While recent methods embed entities and relations into vectors or model multi-hop paths, they predominantly rely on statistical co-occurrence patterns, yielding logically inconsistent or semantically implausible paths that degrade prediction quality. We introduce SKILL, a new framework that revolutionizes KGR by injecting structural knowledge into large language models (LLMs) through inductive reasoning, thereby optimizing the reasoning process with LLMs' semantic understanding capabilities. Our novel rule-miner module extracts and semantically validates symbolic reasoning rules from closed paths using LLM-based one-shot prompting, effectively filtering out invalid patterns. This innovative rule injection fine-tunes LLMs with explicit symbolic guidance, leading to a comprehension of KG structures required for downstream reasoning. Extensive experiments on three standard inductive benchmarks show that SKILL surpasses competing baselines by up to 5 absolute Hit@1 points, establishing a new state of the art for inductive knowledge graph reasoning.", "tldr": "", "keywords": ["Knowledge Graph", "Inductive Reasoning", "Large Language Models"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de282a57bbc0717672cb8085c8712675d402be0f.pdf", "supplementary_material": "/attachment/5504d8dea50efbeab8649c39e55bb56cd6afaec1.zip"}, "replies": [{"content": {"summary": {"value": "This work introduces SKILL, a framework for inductive Knowledge Graph Reasoning (KGR) aiming to improve upon embedding/path-based methods and existing LLM integrations. The core idea is to inject validated structural knowledge into LLMs. It involves mining symbolic rules from the KG, using an LLM via one-shot prompting to semantically validate these rules, and then fine-tuning another LLM using prompts that combine the validated rules with pruned subgraph context related to the query triple. The authors argue that this provides explicit structural guidance, enhancing inductive reasoning over unseen entities. Experiments on inductive KGR benchmarks claim state-of-the-art performance, especially on Hits@1."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Problem Statement:** Addresses the challenging and important problem of inductive KGR.\n2. **String empirical results:** Achieves strong empirical results, particularly in Hits@1 accuracy, on (limited) standard inductive benchmarks.\n3. **Utilization of Foundation models:** Utilizes LLMs to replace legacy KG methods by integrating symbolic rules and subgraph context into LLM prompts for fine-tuning, attempting to provide explicit structural guidance."}, "weaknesses": {"value": "1. **Scalability:** Rule mining and LLM fine-tuning (even with LoRA) are computationally expensive and unlikely to scale to very large KGs. LLM-based validation adds another potentially costly step.\n2. **Reliability of LLM Validation:** The LLM validation step via one-shot prompting is heuristic and probably unreliable being sensitive to prompt design/instance selection. There is also a possibility of it inheriting LLM biases. The criteria \"Reasonableness\" and \"Usefulness\" are subjective, and hard, especially for a small model like Qwen2-7B.\n3. **Marginal Benefits of some components:** Ablation results suggest that the expensive LLM validation step provides only a small improvement over using raw (unvalidated) rules combined with subgraph context. This questions the practical value of the validation component. The same is with fine-tuning the model, maybe the improvement is not justified by the cost.\n4. **Interpretability Claims:** While rules can be interpretable, there is no evaluation to confirm whether the LLM's reasoning process actually follows these rules or how explanations could be extracted from the fine-tuned LLM."}, "questions": {"value": "1. Why are Llama 3.1-8B and Qwen2.5-7B worse than Qwen2-7B in Table 3? I find this odd, do the authors have any hypothesis for this?\n2. How computationally expensive is the LLM validation step in practice? What fraction of the total pipeline time does it consume? Does this cost justify the marginal performance gain observed over using raw rules in the ablation study?\n3. Table 4 shows a very high number of raw rules found by NCRL compared to SKILL's candidate rules (derived from BFS paths before validation?). Why is there such a large discrepancy in the number of initial candidate rules between methods? The authors mention redundancy in NCRL, but is there evidence to back this claim?\n4. Can you provide evidence for the robustness of the one-shot LLM validation? What happens if different instances are used for the same rule, or if different prompts are used? How was the risk of the LLM simply confirming patterns aligned with its pre-trained knowledge (rather than KG structure) mitigated?\n5. How does this compare with KG Foundation Models, like ULTRA [1], and more specifically, KGFMs like SEMMA [2], which leverage LLMs along with the purely structural pipeline?\n\n---\n\n_[1] Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, & Zhaocheng Zhu (2024). Towards Foundation Models for Knowledge Graph Reasoning. In The Twelfth International Conference on Learning Representations (ICLR)._\n\n_[2] Arvindh Arun, Sumit Kumar, Mojtaba Nayyeri, Bo Xiong, Ponnurangam Kumaraguru, Antonio Vergari, & Steffen Staab. (2025). SEMMA: A Semantic Aware Knowledge Graph Foundation Model. In The Thirtieth Conference on Empirical Methods in Natural Language Processing (EMNLP)._"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kvcy2WPnTc", "forum": "F0ApXB8eVc", "replyto": "F0ApXB8eVc", "signatures": ["ICLR.cc/2026/Conference/Submission8910/Reviewer_VAXT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8910/Reviewer_VAXT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479663987, "cdate": 1761479663987, "tmdate": 1762920661853, "mdate": 1762920661853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SKILL, a framework for inductive knowledge graph reasoning that integrates symbolic rules into large language models (LLMs). SKILL first mines multi-hop relational paths from a knowledge graph and converts them into candidate logical rules. These rules are then filtered for semantic plausibility using an LLM-based one-shot prompt, keeping only those deemed logically valid. The validated rules are injected into the LLM through instruction-style fine-tuning, enabling it to perform structured, interpretable reasoning over unseen entities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The use of an LLM to filter candidate rules (for “reasonableness” and “usefulness”) adds a self-reflective, semantic validation layer not seen in prior inductive reasoning work."}, "weaknesses": {"value": "1) The paper repeatedly claims to “inject structural knowledge into LLMs” as a novel paradigm. But this is now a standard LLM-KG based reasoning method. Many papers like ChatRule (Luo et al., 2025), Think on Graphs (Sun et al 2024), RoG (Luo et al., 2024), already explore LLM-mediated rule mining or KG-guided reasoning. The novelty of this paper is very low.\n\n2) The framework simply fine-tunes an instruction model on rule-augmented prompts, i.e., data-level conditioning. That is not structural integration; it’s dataset augmentation. Hence, the paper’s title (“Structural Knowledge Injection”) oversells what is essentially fine-tuning with extra textualized context.\n\n3) The inductive generalization claims rely on dataset splits with disjoint entities, but the method doesn’t explicitly model inductive transfer. The model still memorizes textual co-occurrence of rule templates; there’s no architectural or representational mechanism ensuring entity-agnostic reasoning.\n\n4) LLM validation is ungrounded. The LLM-based rule filtering is the central novelty, yet the authors never evaluate its correctness. There’s no evidence that the LLM’s “Yes/No” judgments correlate with ground truth logical validity or human reasoning.\n\n5) LLM is good at common-sense KG, but may performs worse on domain-specific KG. The authors should also test the method on domain-specific KGs like biomedical knowledge graphs."}, "questions": {"value": "1) How is SKILL fundamentally different from prior LLM rule-mining or LLM fine-tuning frameworks like ChatRule or KG-FIT?\n\n2) How reliable is the LLM-based rule validation, did you measure accuracy or consistency?\n\n3) How sensitive are the results to the specific prompt wording used for validation?\n\n4) Are the same LLMs used for rule validation and reasoning, and if so, how do you avoid circular supervision?\n\n5) What is the computational cost of the rule-mining and LLM validation stages?\n\n6) Are the reported gains statistically significant across runs?\n\n7) How do you ensure inductive generalization isn’t driven by lexical overlap of entity names?\n\n8) How scalable is SKILL to larger KGs beyond the benchmark datasets?\n\n9) How does your method perform on domain-specific knowledge graphs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Df2AABvWmq", "forum": "F0ApXB8eVc", "replyto": "F0ApXB8eVc", "signatures": ["ICLR.cc/2026/Conference/Submission8910/Reviewer_copZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8910/Reviewer_copZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756574007, "cdate": 1761756574007, "tmdate": 1762920661411, "mdate": 1762920661411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper summary: The paper proposes SKILL, a two-stage pipeline for inductive knowledge-graph reasoning (KGR) that (1) mines closed-path symbolic rules from a KG with BFS, filters them by support/confidence and an LLM one-shot “Yes/No” semantic check, then (2) injects the validated rules into an LLM via instruction-style fine-tuning and logic-aware subgraph prompting. A soft rule–path matching and confidence-weighted pruning pick the most relevant paths/rules for each query. Using LoRA on Qwen2-7B, SKILL reports state-of-the-art Hit@1 in inductive settings on FB15k-237 and NELL-995, and competitive transductive results; few-shot variants also improve strongly."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper treats the KG as a source of relational logic (not just embeddings) is well-motivated and nicely executed via rule mining + LLM semantic validation. The pipeline is modular and interpretable. Also, LLM filtering sharply reduces noisy rules while keeping useful ones; examples are human-readable (e.g., language/film rules), aiding transparency.\n\nIn general, this is a solid application of LLM on knowledge graph reasoning. To me, it is amazing that people can achieve over 0.7 Hit@1 on FB15k-237. Back to 2020, the best models are like RotatE, ComplEx, GPFL etc, which can achieve around 0.2-0.3 Hits@1 on the same dataset. I will say that LLM and in general large autoregressive (AR) model based on transformer tremendously push forward the performance on almost all research directions.\n\nAnyway, this paper applies LLM and produces new state of the art performance, which is solid."}, "weaknesses": {"value": "The paper evaluates with 1 positive + 49 negatives per query and appears to use reduced subgraphs of standard datasets (table stats are much smaller than canonical FB15k-237/WN18RR sizes). This makes cross-paper comparability to classical KGE work (which uses filtered ranking over all entities) unclear, and very high WN18RR scores likely reflect the sampled-candidate setting. Please report both sampled and full-ranking metrics, or justify the choice and ensure baselines are re-run under the same protocol.\n\nThe match×confidence heuristic (Eqs. 8–9) is sensible but fixed; no learning of rule/path weights or uncertainty modeling is presented. This could under-perform on longer dependencies or noisy KGs."}, "questions": {"value": "1. You evaluate with 1 positive + 49 negatives per query. How are negatives sampled (type-consistent? filtered for trivial heuristics), and can you also report full filtered ranking (over all entities) for comparability with KGE work?\n\n2. The reasoning subgraph for each query includes first-order neighborhoods of h and t plus closed paths (length ≤ k). At test time, does this subgraph draw strictly from the train graph (to avoid inductive leakage), and what exact edges are visible? Please detail the construction for transductive vs inductive splits."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ec1ldYzbKu", "forum": "F0ApXB8eVc", "replyto": "F0ApXB8eVc", "signatures": ["ICLR.cc/2026/Conference/Submission8910/Reviewer_GgbP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8910/Reviewer_GgbP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924721980, "cdate": 1761924721980, "tmdate": 1762920661119, "mdate": 1762920661119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a meaningful attempt to enhance LLM reasoning through explicit rule-based semantic verification. The restricted dataset, missing metrics, and lack of scalability discussion further reduce its impact. Addressing these issues—particularly by improving the theoretical foundation, expanding experiments to full datasets, and optimizing inference efficiency—could make the work more competitive in future iterations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments are relatively extensive, covering multiple datasets and metrics to validate the proposed framework.\n\n2. The proposed model explicitly integrates rule-based verification into the reasoning process of large language models (LLMs), which strengthens the interpretability and semantic correctness of generated reasoning chains."}, "weaknesses": {"value": "1. The proposed framework is complex and computationally expensive, causing poor scalability and practical inefficiency.\n\n2. In Figure 2, “subgraph pruning” is incorrectly written as “subgraph proning.”\n\n3. The experiments use only 49 negative samples, which follows an early inductive evaluation convention but lacks credibility and general applicability in modern settings. Furthermore, the datasets used are subsets of WN18RR and FB15k-237, rather than the full versions, reducing the strength of the evaluation.\n\n3. Only MRR and Hits@1 results are presented. Additional metrics such as Hits@3, Hits@10, or runtime comparisons should be included for a more comprehensive evaluation.\n\n4. The proposed model provides limited theoretical insight or methodological novelty. It mainly extends existing symbolic verification frameworks without introducing fundamentally new ideas or proofs.\n\n5. The paper lacks necessary explanations of the training procedure, inference workflow, and how rule verification is integrated with the LLM generation process."}, "questions": {"value": "When dealing with large-scale graphs, does the proposed model suffer from a rule explosion issue? If so, how is computational efficiency maintained or mitigated in such scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o5FOKZGkXv", "forum": "F0ApXB8eVc", "replyto": "F0ApXB8eVc", "signatures": ["ICLR.cc/2026/Conference/Submission8910/Reviewer_sb7E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8910/Reviewer_sb7E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929520770, "cdate": 1761929520770, "tmdate": 1762920660730, "mdate": 1762920660730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}