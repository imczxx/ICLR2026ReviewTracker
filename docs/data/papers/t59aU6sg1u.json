{"id": "t59aU6sg1u", "number": 4022, "cdate": 1757585337174, "mdate": 1763559398120, "content": {"title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation", "abstract": "The generative capabilities of Large Language Models (LLMs) are rapidly expanding from static code to dynamic, interactive visual artifacts. This progress is bottlenecked by a critical evaluation gap: established benchmarks focus on algorithmic correctness and largely overlook the visual fidelity and interactive integrity that define modern user experiences. To bridge this gap, we introduce ArtifactsBench, a benchmark and automated, multimodal evaluation paradigm for visual code generation. Our framework programmatically renders each generated artifact and captures its dynamic behavior via temporal (three-step) screenshots. This visual evidence, alongside the source code, is then assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a fine-grained, per-task checklist to ensure holistic and reproducible scoring. We curate 1,825 diverse tasks and evaluate over 30 leading LLMs. Our automated evaluation achieves 94.4% ranking consistency with WebDev Arena—a de facto gold standard for human preferences in web development—and up to 90.95% pairwise agreement with human experts. We open-source ArtifactsBench, including the benchmark, evaluation harness, and baseline results at https://anonymous.4open.science/r/ArtifactsBench-F7F9, to provide the community with a scalable and accurate tool to accelerate the development of user-centric generative models.", "tldr": "", "keywords": ["Artifacts", "ArtifactsBench", "MLLM", "Code"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c12625c7458c76026bb238fdf97948dbc223704.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a benchmark to evaluate LLMs' visual artifact generation capabilities, using multimodal LLMs as judges and task-specific checklists. The benchmark covers both static and dynamic tasks, along with an evaluation framework to judge both code and visual components. Separate studies are conducted to assess the alignment between the proposed evaluation and human experts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-structured, and the motivation is sound. \n- The benchmark covers a wide range of tasks.\n- The data creation and evaluation pipeline has sufficient human grounding, which improves (e.g., task filtering and checklist generation) and makes the benchmark robust (e.g., alignment study of evaluation scores on 280 instances with human experts).\n- A large number of open-source and closed-source LLMs are evaluated across different model scales and capabilities to provide a holistic overview of current models on artifact generation tasks."}, "weaknesses": {"value": "In general, the work lacks the following:\n- crucial details about the dataset creation pipeline, which either creates confusion or gives an incomplete picture\n- insights or takeaways on how to improve the current LLMs for these tasks (e.g., the paper only provides which tasks are challenging, but through the checklist, the discussion could be more detailed)\n\nThese weaknesses are supported by the questions below. I will be happy to increase the scores if my concerns are adequately addressed."}, "questions": {"value": "- The paper talks about reproducible scores - is it implicitly assumed because the MLLM-judge has to score using the checklist? It would be better to conduct a small study on the variation in scores, at least for the responses of strong-performing LLMs.\n- Can you describe, out of 1825 tasks, how many are static and how many are dynamic?\n- Although a high-level difference between some existing related benchmarks is provided (e.g., Table 2), individual differences are not clear:\n  - How is CHA measured in Table 2 for all benchmarks (in the paper, only ArtifactsBench and WebBench CHA are studied)\n  - Why is the CHA assigned only \"Mid\" and \"High\" scores?\n  - What are the benefits of ArtifactsBench over FullFront (the rows are almost identical in the Table)?\n  - Why is DOM alignment (can you also briefly describe this evaluation?) not sufficient for evaluation, e.g., WebBench?\n  - Is the output from the tasks in these previous benchmarks identical to the outputs expected from your tasks?\n- Why are three screenshots enough (in the text, it is described as before, during, and after, but why is just one screenshot for \"during\" enough)? Does it mean that each dynamic task implicitly assumes a single interaction/transition? If so, why can't simpler methods be designed for evaluating such dynamic tasks?\n- Is Hunyuan-TurboS open-sourced or accessible through API? \n- Within the dual-referee setup, the open-source and closed-source MLLM evaluations are not combined to give a final score. Is there any interaction between the two referees, or are the two referees used independently to provide aligned scores? If so, then the dual-referee setup is somewhat misleading.\n- Could Gemini-2.5-Pro being used as a referee lead to the responses from the same model family being scored higher?\n- **Formatting**: The captions of tables are inconsistently placed.\n- Is there an intuition about how alarming a drop-off of 1 or 2 points is, i.e., between achieving 65 and 64, how bad (or on what aspects) is the second model worse than the first model?\n- Is inclusion of visual and code-oriented scores beneficial? Figure 4 demonstrates a strong positive correlation between the two scores; it would have been interesting to see the need for both scores when one score is high, the other decreases, and highlights the deficiencies of the model; however, that not being the general case casts doubt on the checklist preparation and evaluation metric. In more detail, having only one of the two scores (e.g., code-oriented scores) would be enough, while a significant motivation of the benchmark and evaluation was to evaluate the visual component (e.g., visual-oriented scores).\n- Are there overlaps between the tasks of ArtifactsBench and other benchmarks?\n- Questions on benchmark creation pipeline:\n  - What was the human effort required during the task creation (e.g., in hours for contamination control, prompt rewriting, difficulty calibration, and checklist refinement)?\n  - In what aspect would the pipeline differ for different topics, e.g., for data science and multimedia editing?\n  - I wanted to take a look at some sample examples of tasks, but was unable to download or view the dataset (waited for 5 minutes) at https://anonymous.4open.science/r/ArtifactsBench-F7F9/dataset/artifacts_bench.json (is this also happening for other reviewers?). Can you maybe provide 1 or 2 examples per topic in the appendix?\n  - The input to the benchmark creation pipeline is not described, i.e., what does \"candidates from expert showcases\" mean? Can you provide examples?\n  - How do you extract \"prompts, checklists, and normalized DOM/CSS/JS\" from the above sources?\n  - Can you provide examples of what underspecification means within \"ambiguity repair\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "28fu2bWszj", "forum": "t59aU6sg1u", "replyto": "t59aU6sg1u", "signatures": ["ICLR.cc/2026/Conference/Submission4022/Reviewer_e67X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4022/Reviewer_e67X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761470858528, "cdate": 1761470858528, "tmdate": 1762917138708, "mdate": 1762917138708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for their time and thoughtful feedback. We have incorporated the rebuttal content into a revised version of the paper; all new or clarified text is highlighted in red in the manuscript. Below we briefly restate the main aim of **ArtifactsBench** and summarize the key improvements made during the rebuttal.\n\nArtifactsBench targets a setting that is increasingly important in practice: evaluating LLMs on **instruction-to-executable interactive visual artifacts**, including games, dashboards, management systems, SVG posters, and quick tools. Beyond static DOM or screenshot matching, our benchmark couples 1,825 executable tasks across 9 domains with a checklist-guided, multimodal evaluation pipeline that inspects code, visuals, and interaction via sandboxed execution, three staged screenshots, and a dual-referee MLLM-as-Judge setup. This design yields scores that are both fine-grained (10 vision/code dimensions, difficulty and interaction tiers) and strongly aligned with human judgment (up to 90.95% Pair ACC with experts and 94.4% rank consistency with WebDev Arena).\n\nDuring the rebuttal phase, we have made the following substantive updates, which we hope will make the contribution and evaluation protocol clearer and more reproducible:\n\n- Clearer task space and interaction coverage.\nThe main text now explicitly describes the range of interaction types (time-driven dynamics, event-driven UI, forms and validation, sliders/drag/zoom, multimedia, SVG/Canvas interactions, multi-step workflows, game mechanics) and reports the distribution over interaction levels: 396 / 117 / 536 / 776 tasks for Static Visual / Mild Dynamics / High Dynamics / Intensive Interactive. We also clarify that 1,429 tasks require non-trivial interaction beyond a single static view.\n  \n- More detailed and auditable data pipeline.\nThe dataset section and a new appendix subsection (“Detailed Introduction to Data Collection and Cleaning”) provide a step-by-step description of how tasks are sourced (expert showcases, course projects, blogs, code repositories, SVG and game examples, LLM visual-to-query), filtered, de-duplicated, rewritten, difficulty- and domain-tagged, assigned interaction levels, paired with 10-dimension checklists, and validated for solvability and ambiguity. We also give rough estimates of human effort for these stages.\n  \n- Additional evaluation studies and robustness checks.\nWe expand the human evaluation to report multi-rater agreement measures and score distributions, and we analyze the cost–accuracy trade-off of visual evidence (0 / 1 / 3 / 5 screenshots). New appendix sections study:\n(i) Score reproducibility under repeated referee runs;\n(ii) **Robustness to checklist phrasing and template variants**;\n(iii) **Programmatic HTML/DOM quality checks**, showing that static DOM metrics are nearly saturated and weakly discriminative; and\n(iv) Contamination detection using a prefix-completion protocol with ROUGE-L.\nTogether, these studies support that the scores are stable, robust, and not driven by superficial rubric wording.\n  \n- Clarified dual-referee design and practical reproducibility.\nWe explain more clearly that Gemini‑2.5‑Pro and Qwen2.5‑VL‑72B are used as **two independent referees**, each inducing its own leaderboard; we do not fuse their scores. New figures show highly consistent partial-order constraints between the referees and confirm version stability for Gemini‑2.5‑Pro. An additional appendix section lists the public API endpoints used for proprietary models, facilitating external reproduction of our baselines.\n  \n- Qualitative examples and sharper takeaways.\nA new appendix section (“Visualization Results”) presents representative rendered cases from several domains (SVG posters, web applications, games, management systems, data-science dashboards) to help readers gauge benchmark difficulty and diversity. The analysis section has been sharpened to emphasize where models still struggle most (especially Intensive Interactive and complex management UIs) and to highlight the consistent advantage of instruction-tuned generalist models over coder-only or VL-only specialists, suggesting concrete directions for future model development.\n  \nWe appreciate the reviewers’ feedback, which helped us strengthen both the clarity and the empirical foundation of ArtifactsBench, and we are happy to address any further questions."}}, "id": "AjxRsqL0Pg", "forum": "t59aU6sg1u", "replyto": "t59aU6sg1u", "signatures": ["ICLR.cc/2026/Conference/Submission4022/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4022/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4022/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763559595935, "cdate": 1763559595935, "tmdate": 1763559595935, "mdate": 1763559595935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ArtifactsBench, a large-scale benchmark and automated evaluation pipeline for interactive visual code generation. The benchmark contains 1,825 tasks across nine domains (e.g., web apps, SVG, games) with Easy/Medium/Hard stratification. Evaluation renders each artifact in a sandbox and captures three staged screenshots (before/during/after interaction). A checklist-guided MLLM-as-Judge (dual referees: Gemini-2.5-Pro and Qwen2.5-VL-72B) scores both vision and code facets.\n\nOn 30+ LLMs, ArtifactsBench reports 94.4% rank consistency with WebDev Arena and up to 90.95% pairwise agreement with human experts. Main results show proprietary multimodal models leading; performance scales with model size/deliberation; and generalist instruction-tuned models often beat specialist coder/VL models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a valuable resource with 1,825 executable tasks spanning 9 domains with difficulty tiers; supports fine-grained analysis beyond single static correctness.\n- Proposes an interactive evaluation design where three-step screenshots and sandboxed execution capture dynamics while keeping runs reproducible.\n- Evaluates an extensive suite of 30+ LLMs, spanning both open-source and proprietary models; evaluation results show high pairwise agreement (up to 90.95%) and 94.4% Footrule rank consistency vs. WebDev Arena.\n- Clear empirical takeaways: generalist models > specialists on this task class; detailed category breakdowns (games, SVG, simulations, management systems)."}, "weaknesses": {"value": "- Three screenshots may miss long-horizon workflows and nuanced physics/UX timing; authors acknowledge this. including richer scripted interactions or short videos may strengthen the evaluation.\n- Fixed 1024×768 and single-browser setting may underrepresent responsive/adaptive designs; consider multi-viewport evaluation.\n- Checklists are LLM-drafted then human-refined; potential leakage of judge priors and over-optimization to rubric specifics—worth stress-tests with diverse/adversarial prompt styles.\n- WebDev Arena alignment is strong, but additional human-preference datasets or task-specific user studies (e.g., for accessibility/UX) would bolster generality."}, "questions": {"value": "- How robust are rankings to alternative, non-LLM programmatic checks (e.g., DOM state assertions, event logs, mutation observers)? Any preliminary results?\n- Can you report inter-annotator reliability for the 280-instance expert study beyond Pair-ACC, and provide the distribution of disagreements?\n- How sensitive are the model judges to the checklist phrasing? Have you tried paraphrased/held-out rubrics or blinded rubrics to test robustness?\n- Could you expand on the procedure for contamination control audits?\n- When the two referees (Gemini and Qwen) yield divergent scores/rankings, what is your tie-breaking/aggregation protocol? Do you see any common trends or cases where different LLM judges diverge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cG4XHccWGK", "forum": "t59aU6sg1u", "replyto": "t59aU6sg1u", "signatures": ["ICLR.cc/2026/Conference/Submission4022/Reviewer_ANuP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4022/Reviewer_ANuP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533743818, "cdate": 1761533743818, "tmdate": 1762917138525, "mdate": 1762917138525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes ArtifactsBench, a benchmark for visual code generation. The benchmark includes 1800+ examples, and the MLLM-based evaluation results correlate well with human experts and WebDev Arena."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is a comprehensive benchmark with 1800+ tasks.\n2. More than 30 models are benchmarked."}, "weaknesses": {"value": "1. Benchmarking visual code generation is not a novel problem; there are many works in this direction. We have similar benchmarks for the website and SVG before, while this benchmark claims to extend the scope to Game, Simulation, Data Science, etc, the evaluation idea is largely similar: show screenshots to MLLM and ask for judgment. I don't think you can judge the quality of a game by screenshots with limited interaction. In general, I don't see many useful insights from this very broad benchmark without specific and reliable evaluations.\n2. The data collection process is only described at a high level.\n3. What is the human baseline of these tasks?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RaOk9sjF05", "forum": "t59aU6sg1u", "replyto": "t59aU6sg1u", "signatures": ["ICLR.cc/2026/Conference/Submission4022/Reviewer_Wsco"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4022/Reviewer_Wsco"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865111922, "cdate": 1761865111922, "tmdate": 1762917138335, "mdate": 1762917138335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds a new benchmark for visual code generation (e.g., generating the code to implement a website). The main difference with existing Design2Code type work is that the new ArtifactsBench captures the dynamic interaction. \n\nArtifactsBench is a large-scale benchmark and automated evaluation framework for assessing LLMs’ ability to generate interactive visual artifacts—that is, executable web widgets, games, visualizations, or apps combining code, visuals, and interaction.\nIt aims to close the evaluation gap between algorithmic correctness (e.g., HumanEval) and real-world user experience (visual fidelity + interaction quality).\nArtifactsBench evaluates 1,825 executable tasks and introduces an MLLM-as-Judge system using multimodal evidence (code + screenshots) with fine-grained, checklist-based scoring.\n\nFor evaluation: Each model’s generated artifact is executed and rendered.\nThe MLLM-as-Judge receives: the original prompt; the full model output (code); three temporal screenshots; and the task-specific 10-item checklist. The judge produces reproducible per-dimension (0–10) scores.\n\nThe authors use a dual-Referee setup for robustness: Gemini-2.5-Pro (closed-source, high-capacity) and Qwen2.5-VL-72B (open-source).\nBoth achieve >90% pairwise agreement with human experts; with 94.4% ranking consistency with WebDev Arena (human preference gold standard).\n\nThey benchmarked various models including Qwen2.5/3, DeepSeek, Gemma, GPT, Claude, Gemini, Seed, Hunyuan. Gemini-2.5-Pro is generally the best."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Having three temporal screenshots is an addition to existing evals that only look at one static screenshot. \n\n- It's great to get an extra benchmark for visual artifact generation."}, "weaknesses": {"value": "- Why do you not have any examples of the actual benchmark examples? Not even in the Appendix? It makes it much harder to judge the actual quality of the benchmark. \n\n- I'm not quite sure if shoving three screenshots of the interactions to the LLM judge is the best way to evaluate the functional correctness of the dynamic interaction? Do you have any sort of human evaluation that lets users try out the generated websites to perform some specified, realistic tasks? Would that correlate with your automatic metric?\n\n- The overall finding and contribution seem rather incremental compared to existing works like Design2Code."}, "questions": {"value": "- Missing citation: \"Design2Code: Benchmarking Multimodal Code Generation for Automated Front-End Engineering\", NAACL 2025 \n\n- What kind of interactions do you cover in the benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TyhIMqbRme", "forum": "t59aU6sg1u", "replyto": "t59aU6sg1u", "signatures": ["ICLR.cc/2026/Conference/Submission4022/Reviewer_9Zfz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4022/Reviewer_9Zfz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762063216942, "cdate": 1762063216942, "tmdate": 1762917138166, "mdate": 1762917138166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}