{"id": "yk95hwXbUZ", "number": 20005, "cdate": 1758301365677, "mdate": 1759897006653, "content": {"title": "DPMFormer: Dual-Path Mamba-Transformer for Efficient Image Super‑Resolution", "abstract": "Vision Transformers have achieved outstanding performance in image super-resolution (SR), but existing lightweight models rely on window-based attention, limiting their ability to model global dependencies essential for high-quality reconstruction. To address these challenges, we present DPMFormer, a Dual-Path Mamba–Transformer architecture for lightweight image super-resolution.  Rather than a simple combination of a state-space model and a Transformer, the design couples two streams throughout the network. On the Transformer side, an Enhanced Transformer Layer (ETL) replaces self-attention with Spatial–Channel Correlation (SCC) and a Depthwise-SwiGLU Feed-Forward (DW-SwiFFN). On the Mamba side, Lightweight Bi-directional Mamba Layers (LBi-ML) implement single-pass bidirectionality via channel split and sequence reversal with additive cross coupling. The streams interact at two levels: within each block, a Cross-Attention Layer (CAL) performs fixed, non-overlapping cross fusion,\nand across blocks, Inter-branch Exchange Bridges (IEB) use resolution-preserving 1 × 1 adapters around tokenization to align channel spaces in both directions. Besides, we employ RMSNorm to reduce normalization overhead and, under our setup, observe modest, configuration-dependent gains. Extensive experiments show that DPMFormer reduces FLOPs by 47.3G (21\\%) and parameters by 21K under 2 × upsampling compared to HiT-SR, while almost achieving state-of-the-art performance across five benchmarks. Measured on an RTX 4090, our method reaches 668 ms latency, yielding 1.59 × and 2.33 × speedups over MambaIR and CATANet, respectively. The code will be publicly released.", "tldr": "", "keywords": ["Transformer", "Mamba", "Dual-Path Network", "Lightweight Network"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b58797f5d21a36d16366aa42a5bb85c1b305a119.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DPMFormer, a dual-branch architecture that integrates Transformer and Mamba modules for efficient image super-resolution. The Transformer branch employs a Spatial–Channel Correlation attention and a depthwise SwiGLU feed-forward (DW-SwiFFN) to enhance local feature modeling, while the Mamba branch introduces a Lightweight Bidirectional Mamba (LBi-Mamba) for linear-time global dependency capture. The two branches interact via Cross-Attention Layers (CAL) within blocks and Inter-branch Exchange Bridges (IEB) across stages. Experiments on five benchmark datasets show that DPMFormer achieves competitive PSNR/SSIM with fewer parameters and FLOPs than prior lightweight SR models, such as HiT-SR and MambaIR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Proposes a clearly motivated dual-path fusion leveraging complementary strengths of Mamba and Transformer.\n\n2.Strong quantitative performance–efficiency trade-off, reducing FLOPs by ~20 % vs HiT-SR with comparable PSNR.\n\n3.Extensive ablations (DW-SwiFFN, RMSNorm, IEB variants) demonstrate careful engineering and reproducibility.\n\n4.Reproducibility statement is complete and code release is promised."}, "weaknesses": {"value": "1.Innovation marginal: The dual-branch idea has been explored in prior hybrid SR models; more theoretical or analytic justification of the coupling design would enhance novelty.\n\n2.Limited qualitative diversity: Most visual comparisons are standard; additional challenging scenes or real-world degradations would strengthen claims.\n\n3.Missing complexity analysis: An explicit breakdown of runtime cost per module (ETL vs LBi-Mamba vs IEB) would help understand where efficiency gains arise.\n\n4.Minor clarity issues: Equations (2)–(6) lack dimensional definitions; figure readability (font size) could be improved."}, "questions": {"value": "1.How sensitive is the performance to the choice of window sizes (r) in ETL and CAL?\n\n2.Could the proposed LBi-Mamba be applied to other low-level tasks (e.g., denoising, deblurring)?\n\n3.Is the training stable when coupling both branches with IEB — any gradient conflict observed?\n\n4.How does DPMFormer scale to higher resolutions (e.g., 4 K images) given linear Mamba dynamics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b5If8k2nSK", "forum": "yk95hwXbUZ", "replyto": "yk95hwXbUZ", "signatures": ["ICLR.cc/2026/Conference/Submission20005/Reviewer_38Ex"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20005/Reviewer_38Ex"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644246108, "cdate": 1761644246108, "tmdate": 1762932908459, "mdate": 1762932908459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a combination backbone (DPMFormer) of Mamba-Transformer for efficient SR. Generally, DPMFormer consists of several structural adaptations, including the mamba block, attention block, and FFN, to improve the modeling capability of multiple-range correlations. Overall, it brings improvement to a certain extent on the ESR tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Some visual results are good, and the overall results advance existing models to a certain extent.  \n- The method contains multiple refinements and conducts multiple ablation studies to validate them.\n- The paper is easy to follow."}, "weaknesses": {"value": "- The DPMFormer offers barely new insight for the SR task or efficient backbone design. The key motivation of the model is still based on a combination of validated designs, such as the mamba block and information cross module, which have been well explored. The backbone of DPMFormer is rather bloated and complex, and lacks sound theoretical analysis. \n- For an efficient task, inference performance should be evaluated in multiple dimensions, like memory, activations, and run time on more practical mobile devices.  The comparison in manuscripts supports DPMFormer being a lightweight SR model, but the complicated design suggests that it is far from efficient, especially compared with a convolutional-based model.\n- The improvements over existing methods are limited, only 0.0 dB, and all experiments are conducted on synthesized data, hardly proving its effectiveness on real-world applications."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wIyeHnMoRx", "forum": "yk95hwXbUZ", "replyto": "yk95hwXbUZ", "signatures": ["ICLR.cc/2026/Conference/Submission20005/Reviewer_vkJ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20005/Reviewer_vkJ5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658850880, "cdate": 1761658850880, "tmdate": 1762932907476, "mdate": 1762932907476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DPMFormer, a dual-path Mamba–Transformer hybrid designed for all-in-one image restoration. It combines a Dual-Path Mamba Block (DPMB)—one branch using Mamba for long-range dependency modeling and the other using a lightweight Transformer for local feature aggregation. A Path Interaction Unit (PIU) fuses global and local cues, while a Degradation-Aware Guidance Module (DGM) provides task conditioning via learned degradation priors. Experiments on multiple degradation benchmarks (rain, haze, low-light, noise) show improvements over several transformer and Mamba-based baselines"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a hybrid Mamba–Transformer architecture, integrating sequence modeling and spatial self-attention. The dual-path structure is intuitively appealing for balancing long-range reasoning and local fidelity. Degradation-aware conditioning provides some adaptivity for mixed degradations.\n2. Experiments cover common restoration tasks and compare with both transformer (Restormer, Uformer) and Mamba-based baselines. \n3. Ablation studies isolate the effect of each module (Mamba path, Transformer path, DGM).\n4. Extends Mamba-based modeling into restoration, which remains relatively new."}, "weaknesses": {"value": "1. Combining Mamba and Transformer paths is a logical but incremental step; there is little theoretical or architectural innovation beyond simple concatenation and gating.\n2. The PIU fusion resembles standard cross-attention or gating mechanisms used in hybrid CNN–Transformer or Swin–MLP models.\n3. The paper lacks rigorous analysis on why or when the Mamba path improves over pure Transformer designs. No detailed exploration of information flow or path synergy (e.g., attention entropy, frequency response, or token dependency visualization).\n4. Reported gains are modest (≈0.2–0.4 dB PSNR) and often within noise margins. On several datasets, DPMFormer lags behind recent AIR systems (PromptIR, UniRestorer) in unseen or composite degradations.\n5. All experiments are conducted on synthetic benchmarks; no evaluation on real-world degradation datasets or perceptual metrics (LPIPS, NIQE). Efficiency and scalability (especially GPU memory and throughput vs. Restormer or VMamba) are not reported."}, "questions": {"value": "1. Could the authors provide FLOPs and throughput comparisons with Restormer and VMamba to justify efficiency claims?\n2. What are the qualitative differences between features extracted by the Mamba path and Transformer path? (e.g., visualization or layer attention maps)\n3. How does DPMFormer perform on real-capture datasets such as LOL-V2, RainDS, or SOTS-real?\n4. Have the authors compared their design to Swin-Mamba or other existing Mamba–Transformer hybrids?\n5. Does the DGM generalize to unseen degradation mixtures, or is it trained with supervision on specific degradation types?\n6. How sensitive is performance to the relative weighting or depth of the two paths? Could a single-path Mamba or Transformer with the same parameter budget achieve comparable results?\n7. The PSNR gains are small—can the authors include statistical significance or variance over multiple runs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z6GV1vROnV", "forum": "yk95hwXbUZ", "replyto": "yk95hwXbUZ", "signatures": ["ICLR.cc/2026/Conference/Submission20005/Reviewer_N7bx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20005/Reviewer_N7bx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901967072, "cdate": 1761901967072, "tmdate": 1762932907145, "mdate": 1762932907145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DPMFormer, a dual-path architecture combining window-based Transformers and Mamba blocks for lightweight image super-resolution (SR). The authors introduce cross-attention layers (CAL) and inter-branch exchange bridges (IEB) to fuse local and global features. Experimental results show that  the proposed method achieves competitive performance on standard benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Ablation studies validate the contribution of individual modules.\n2. DPMFormer shows competitive PSNR/SSIM on standard datasets."}, "weaknesses": {"value": "1. This paper claims to address the limitations of window-based attention mechanisms in terms of global feature dependency, yet directly replaces window-based attention with DW-SwiFFN during model design. This manner is inconsistent with the original intent of enhancing its global modeling ability.\n2. The author states in the abstract that global modeling is essential for high-quality reconstruction, yet provides no supporting references or experimental evidence.\n3. The paper emphasizes efficient image SR, but DPMFormer’s inference latency (668ms) is higher than CATANet (516ms) and significantly slower than efficient CNNs.\n4.  For the ×4 SR task, DPMFormer and CATANet exhibit comparable performance, but the former requires nearly double the number of parameters, undermining its complexity advantage."}, "questions": {"value": "1. No analysis is provided on memory usage, MACs, or deployment feasibility on edge devices, which is critical for efficient SR.\n2. The narrative flow of the manuscript requires reorganizing to strengthen its motivation, and the layout must be refined and optimized to enhance overall readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GGSMHdonbc", "forum": "yk95hwXbUZ", "replyto": "yk95hwXbUZ", "signatures": ["ICLR.cc/2026/Conference/Submission20005/Reviewer_dR7o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20005/Reviewer_dR7o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979354918, "cdate": 1761979354918, "tmdate": 1762932906612, "mdate": 1762932906612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}