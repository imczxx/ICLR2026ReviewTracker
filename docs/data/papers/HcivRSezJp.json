{"id": "HcivRSezJp", "number": 4947, "cdate": 1757817453483, "mdate": 1763601287642, "content": {"title": "Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval", "abstract": "Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing.\nAlthough there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration.\nIn this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration:\ni) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions.\nii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant.\nWith these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency.\nSpecifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch.\nMoreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, \nwith the saliency pattern reused from their first recollection.\nWith negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68$\\times$ speedup on SAM2.1-L model with only 1.0\\% accuracy drop on SA-V test set, where SWR and SMR provide 1.83$\\times$ and 1.78$\\times$ speedups, respectively.", "tldr": "", "keywords": ["Segmemt Anything Model", "Efficient Deep Learning", "Model Acceleration"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0892857b677265a436f5568e0a0ee1af7f956cc.pdf", "supplementary_material": "/attachment/b3481d522f61dec0cb42900d807bcdca9c57c111.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the key motivation of resolving SAM2’s high computational burden that limits its real-time application in video object segmentation (VOS). The  proposed Efficient-SAM2, a post-training acceleration framework, has two core components: 1) Object-Aware Sparse Window Routing (SWR) for the image encoder, which leverages spatial-temporal consistency and perceptual saliency from the previous frame’s mask decoder to route background windows to a lightweight shortcut branch  and preserve full computation for object-relevant windows,2) Object-Aware Sparse Memory Retrieval (SMR) for memory attention, which caches each memory frame’s saliency pattern during its first recollection and reuses it in subsequent frames to only involve salient tokens in computation. Experimentally, Efficient-SAM2 achieves a 1.68× end-to-end speedup on the SAM2.1-L model with only a 1.0% accuracy drop on the SA-V test set."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  Efficient-SAM2 avoids expensive full-model retraining. It adds negligible parameters  and low training overhead, making it flexible for low computational deployment.\n2.  By aligning with SAM2’s natural sparse perception, it eliminates redundancy without compromising core functionality—unlike generic token-merging methods (e.g., ToMe) that cause severe accuracy drops.\n3.  SWR (image encoder) and SMR (memory attention) are independent modules, allowing separate optimization or integration with other SAM2 variants. Their design (window-level routing, cached saliency patterns) is intuitive and supported by qualitative analysis (e.g., Figure 6 shows SWR preserves object attention).\n4.  It maintains strong accuracy across diverse VOS datasets (SA-V, DAVIS 2017, MOSE) and scales to larger models (SAM2.1-L)."}, "weaknesses": {"value": "1. The claimed contribution 1 should be merge with contribution 2 as whole one.\n\n\n2. SWR relies on hyperparameters like the prediction confidence threshold (θₒᵦⱼ=0.5) and saliency threshold (τ=0.7), while SMR depends on the sparsity ratio (s=0.95). The paper does not explore how these parameters generalize to edge cases (e.g., highly cluttered scenes, fast-moving objects) or different datasets.\n\n\n3. The variable symbols cause confusions, especially for different A.\n\n\n4. The codes released as supplementary material fail to run according to SAM-2 environment. The clarity of the code is also limited."}, "questions": {"value": "1.What's the actual theory that the equation (11) can reflect the saliency of a window? It should be explained.\n   \n 2.How can the object-aware router adapt to the fast moving object since all the information of prediction and saliency are from the preceding frame?\n   \n 3.Mask decoder module seems to disappear from the figure 3. How is the segmentation mask produced?\n   \n 4.What is the alignment of  background feature processed by lightweight shortcut branch?\n   \n 5.How would Efficient-SAM2 adapt to dynamic prompts (e.g., user-added points in middle frames) that change the focus of attention?\n   \n 6.Can you provide statistical data (e.g., average CS across all frames in SA-V/DAVIS) to quantify how often saliency patterns remain consistent? What is the impact of inconsistent patterns on SMR’s accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QsIhiUrQni", "forum": "HcivRSezJp", "replyto": "HcivRSezJp", "signatures": ["ICLR.cc/2026/Conference/Submission4947/Reviewer_FsUz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4947/Reviewer_FsUz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930321205, "cdate": 1761930321205, "tmdate": 1762917787165, "mdate": 1762917787165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Efficient-SAM2, a post-training acceleration framework to address the significant computational bottleneck of SAM2 in real-time video object segmentation. The authors identify a mismatch between SAM2's dense computation and its inherently sparse perception, highlighting redundancy in the image encoder's background processing and in full-token memory retrieval. To exploit this, the framework introduces two key components: object-aware Sparse Window Routing (SWR) and object-aware Sparse Memory Retrieval (SMR). SWR dynamically routes irrelevant background windows in the encoder to a lightweight shortcut branch, guided by saliency and consistency cues from the previous frame's decoder. SMR leverages temporal consistency by identifying a sparse set of salient memory tokens during their first recollection and reusing this pattern for subsequent frames, drastically reducing memory attention computations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation for reducing SAM2's computational overhead is well-grounded and intuitive for video object segmentation\n2. The post-training approach is practical, enabling efficient adaptation by leveraging the generalized parameters of the pre-trained SAM2.\n3. The method achieves a good speed-performance trade-off, delivering a speedup of nearly 2x while incurring only a minimal and acceptable performance degradation of approximately 1%."}, "weaknesses": {"value": "1. The SWR component is heavily dependent on the previous frame's prediction and salient mask. I think this may cause challenges in some cases, such as rapid motion, abrupt scene cuts, or severe occlusions, where this temporal assumption would be violated.\n2. For a video domain paper, the qualitative results with static images are insufficient. Supplemental videos would be significantly stronger to properly demonstrate temporal consistency, failure modes (especially in scenarios mentioned in point 1), and the practical impact of the optimizations.\n3. I think an evaluation is needed to determine if the SMR module, which uses a cached saliency pattern, maintains its effectiveness in long video scenarios where significant appearance and context drift are likely.\n4. The paper lacks a dedicated discussion of its limitations. This omission leaves the impression that the proposed efficiencies might be confined to easy (or trained) scenarios."}, "questions": {"value": "While the paper presents a valuable approach to making SAM2 more efficient, the design of the SWR and SMR modules appears highly heuristic and is tightly coupled to assumptions about temporal continuity. This raises a significant question: Is SAM2's strong, general-purpose segmentation performance fully preserved? The evaluation is currently confined to standard VOS benchmarks. To truly validate that these heuristics do not compromise the model's robustness, I would ask the authors to provide evaluations on more diverse datasets, particularly on challenging \"in-the-wild\" videos, which would better test the limits of these heuristic assumptions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3gBwG9PwUq", "forum": "HcivRSezJp", "replyto": "HcivRSezJp", "signatures": ["ICLR.cc/2026/Conference/Submission4947/Reviewer_gSJw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4947/Reviewer_gSJw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975880754, "cdate": 1761975880754, "tmdate": 1762917786905, "mdate": 1762917786905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to reduce the high inference cost of SAM2 in video segmentation. Specifically, they propose two modules: Object-aware Sparse Window Routing (SWR), which skips background windows in the image encoder based on object masks and saliency, and Object-aware Sparse Memory Retrieval (SMR), which selects only salient memory tokens and reuses their mask across frames. Together, these modules accelerate SAM2 by up to 1.75× with minimal accuracy loss on benchmarks such as SA-V, DAVIS, and MOSE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper makes a solid technical contribution by streamlining the SAM2 model. The object-aware pruning of the image encoder and the introduction of a background shortcut for non-foreground patches are both clever ideas that substantially reduce computation.\n\n2. The ablation study is comprehensive. It not only analyzes the proposed components in isolation but also integrates other efficient methods (e.g., ToME) into their framework for comparison, which provides valuable insights."}, "weaknesses": {"value": "1. The proposed routing mechanism heavily relies on the assumption of temporal consistency in video streams, meaning that no significant camera shaking or viewpoint shift occurs. This limits the method’s applicability in real-world scenarios with dynamic motion. It would be interesting to see comparisons with SAM2 on datasets such as MOSEv2[1] and SeCVOS[2], which feature frequent viewpoint transitions.\n\n\n\n\n2. Performing grid search on only one benchmark is not sufficient to demonstrate robustness. It would strengthen the paper to include additional grid search curves across multiple benchmarks (in Figure 5).\n\n\n\n\n\n\n\n[1] MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes\n[2] SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction"}, "questions": {"value": "1. What is the specific layer index after which the router divides foreground and background tokens? How many subsequent modules benefit from reduced computation? It would be helpful to include an ablation varying the routing layer index to assess its impact.\n\n2. How is speedup defined in this paper? It would be clearer to disclose it in two aspects—FLOPs reduction and throughput improvement—to give a more complete view of efficiency.\n\n3. What is the intuition behind using two different temporal intervals (∆t =1,5) in the experiments? It seems the performance difference might largely stem from the frame rate (FPS) of the original benchmark. For high-FPS datasets like SA-V, increasing the interval could naturally yield greater gains since the memory bank contains more diverse frames.\n\n4. For SMR, have you evaluated a variant that selects memory frames only when object presence is confident (akin to SAM2Long’s strategy)? It would be informative to report the gain from this filtering."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "m9cEBVF85c", "forum": "HcivRSezJp", "replyto": "HcivRSezJp", "signatures": ["ICLR.cc/2026/Conference/Submission4947/Reviewer_DdRT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4947/Reviewer_DdRT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762115363286, "cdate": 1762115363286, "tmdate": 1762917786601, "mdate": 1762917786601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}