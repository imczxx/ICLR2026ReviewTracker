{"id": "S9iB7BA3Zq", "number": 6342, "cdate": 1757970657821, "mdate": 1759897920912, "content": {"title": "Bridging Non-Intrusive Tracing and Fine-Grained Cross-Layer Representations for LLM Inference Diagnosis", "abstract": "LLM inference spans the inference engine, compute backend, host operators, and GPU kernels, where asynchrony and concurrency make request-level end-to-end observability and diagnosis challenging. We present $\\textbf{Truffld}$, a non-intrusive and cross-layer framework that provides fine-grained representations for diagnosis in large-scale LLM inference. For data collection, $\\textbf{Truffld}$ activates NVTX markers and CUPTI callbacks to capture raw events from vertical (intra-node stack execution) and horizontal (cross-node communication) perspectives. We then propose a call-chain merging algorithm that aligns these events on a unified time base and reconstructs a per-request call-chain tree preserving both structural and temporal semantics. For anomaly detection, $\\textbf{Truffld}$ adopts a two-stage pipeline. A Gaussian Mixture Model models multi-modal normal behavior and produces calibrated numeric confidences, while a large language model applies structure- and context-aware reasoning to generate step-level decisions and operator-level localization. Experiments on a multi-node GPU cluster running Qwen3-8B inference with both online and offline workloads demonstrate near-perfect step-level detection and superior operator-level performance compared to multiple baselines, with low deployment overhead and no modification to binaries. $\\textbf{Truffld}$  provides a practical end-to-end solution for observability and diagnosis in large-scale LLM inference.", "tldr": "", "keywords": ["LLM profiling; Anomaly detection; Performance diagnosis; LLM observability; LLM tracing"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f09133730ba886d0cd05d9add8a8d15664084d7d.pdf", "supplementary_material": "/attachment/8964d93ffd40f145cdc23b126a2a680e0906a610.zip"}, "replies": [{"content": {"summary": {"value": "The core goal of this work is to provide an improved GPU diagnostic setup for tracing LLM inference events both within and across nodes. The system leverages NVTX and CUPTI to capture detailed execution traces across nodes. These traces are then analyzed using Gaussian models to detect anomalies in the running system."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Tracing systems like these are very useful in debugging event tracing issues in practice. It is very hard to analyze multi-node issues and events that could be occurring in more complex LLM serving setups\n2. The anomaly detection pipeline does look like an interesting use case/extension on existing NSight."}, "weaknesses": {"value": "1. The paper talks a lot about the importance of low overhead profiling. In appendix H, they display the overhead to be 10%. In practical serving system and real time serving cases this can be very high. While I appreciate it was provided, it does feel a bit hidden.\n2. While I understand it is difficult to acquire larger setups, I worry this system is hard to scale/debug when working on large cluster. The evaluation is on 6 GPUs on 2 nodes, but tracing systems like this have to deal with lot of data. The overhead could scale as cluster size increases."}, "questions": {"value": "1. Is it possible to run a scalability experiment with a lot more nodes while serving? Possibly via simulation\n2. is there anything that can be done to further reduce the overhead? Is the overhead unavoidable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lu6bNVhNM4", "forum": "S9iB7BA3Zq", "replyto": "S9iB7BA3Zq", "signatures": ["ICLR.cc/2026/Conference/Submission6342/Reviewer_rDRP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6342/Reviewer_rDRP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956446767, "cdate": 1761956446767, "tmdate": 1762918633493, "mdate": 1762918633493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TRUFFLE, a non-intrusive and fine-grained tracing framework fro diagnosing performance anomalies in large-scale LLM inference systems. TRUFFLD collects raw events through NVTX markers and CUPTI callbacks without modifying binaries, then constructs per-request call-chain trees that preserve both structural and temporal semantics across vertical (intra-node) and horizontal (inter-node) views with a two-stage anomaly detection pipeline. Evaluations on multi-node GPU clusters serving Qwen3-8B with online and offline workloads demonstrate step-level 0.9+ accuracy, better operator-level F1 scores, and low deployment overhead compared with several classical and modern baselines (e.g., Robustlog, LAnoBERT, MAD-GAN)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an interesting gap in LLM inference observability through a non-intrusive tracing framework. The use of NVTX and CUPTI enables production deployment without binary modification. \n2. The hybrid approach combining GMM-based probabilistic modeling with LLM reasoning is well-motivated. Dual-stage pipeline provides both calibrated numeric confidences modeling multi-modal normal behavior and combination of structural and semantic constraints. \n3. The evaluation of the paper show practical significance for operators of large-scale LLM serving systems, bridging a gap between low-level tracing tools and high-level diagnosis frameworks."}, "weaknesses": {"value": "1. The paper is limited by generalization scope and scalability analysis. The evaluation is restricted to a single model (Qwen-8B), inference engine (vLLM), and hardware configuration (dual-node, 6× A40 GPUs). This narrow scope limits claims about generalizability to other LLMs (different sizes, architectures), frameworks (TensorRT-LLM, TGI), or hardware platforms (H100s, TPUs, heterogeneous clusters). \n2. I hold doubt on how reliable the artificial anomalies are to evaluate TRUFFLE on naturally occurring production incidents or long-term operational traces. Authors should provide stronger evidence to show that TRUFFLD captures real-world failure modes. \n3. The key design decisions of the paper lack empirical justification. The GMM stage does not compare alternative density models (kernel density estimation, normalizing flows) or centrality definitions. The LLM stage employs hardcoded thresholds (S ≤ 1 for horizontal, 10% for vertical) without sensitivity analysis or ablation of model choices. No comparison against simpler rule-based fusion methods is provided, leaving unclear whether the added complexity and cost of LLM reasoning is justified over interpretable heuristics."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K7dTODf1Oy", "forum": "S9iB7BA3Zq", "replyto": "S9iB7BA3Zq", "signatures": ["ICLR.cc/2026/Conference/Submission6342/Reviewer_1hpH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6342/Reviewer_1hpH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961965599, "cdate": 1761961965599, "tmdate": 1762918633174, "mdate": 1762918633174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a non-intrusive tracing that has low overhead. Also, the paper claims that the approach has fine-grained cross-layer representations, where the cross-layer refers to the multiple layers in inference engine, compute backend, host operators, and device operators. Proposed work TRUFFLD uses NVTX and CUPTI to gather execution traces collecting events from all the above stacks. It merges \"within-node\" and \"coss-node\" events per-request call-chain trees. This makes it easier to understand both what and when the operators are running.\nFor anomaly detection the paper employs a two-stage approach. (1) Gaussian Mixture Model (GMM) and (2) LLM reasoning. (1) provides numeric confidence for each operator instance (with self-time, CUDA runtime/driver time and counts, kernel counts and totals, approximate bytes moved, stream–overlap ratios, and communication size and world size.) and (2) reads structured summaries of each step along with the semantic context to produce the final step-level abnormality decisions and operator-level localization.\nThe paper tests on top of vLLM serving Qwen-8B across 6-GPU dual-node cluster and uses a dataset of 3264 step-level traces from online and offline workloads. It compares against several classical, supervised and log-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Non-intrusive and Zero-modification seems like a good point from an engineering perspective. Also, that can really improve the overall debuggability in production.\n* Cross-layer observation is a good contribution considering the growing complexity of the LLM SW stack."}, "weaknesses": {"value": "* It seems that the work is only tested on a limited case. It is difficult to understand how well this can be generalized to other environments and models.\n* The paper is rather weak on detail about how it handles the log. I see that there are some examples in the appendix, but the description are still too shallow for the work to be either reproducible by others or to be built upon.\n* It is difficult to understand why the specific combination of GMM and LLM was used. The paper seems to be rather shallow on the insights it provides."}, "questions": {"value": "1. It seems that the work is only tested on a limited case. It is difficult to understand how well this can be generalized to other environments and models. What if the HW changes? What if the framework changes? What if the models change?\n2. How would this work if multiple models are being served simultaneously.\n\nMinor.\nIt would really help if the main paper content has some outline about the usecase and the demonstration of the input and output (at least in small scale). This would help the readers benefit more from the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rMwg25VftZ", "forum": "S9iB7BA3Zq", "replyto": "S9iB7BA3Zq", "signatures": ["ICLR.cc/2026/Conference/Submission6342/Reviewer_qS7K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6342/Reviewer_qS7K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968636451, "cdate": 1761968636451, "tmdate": 1762918632621, "mdate": 1762918632621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TRUFFLD, a non-intrusive, cross-layer tracing and diagnosis framework for large-scale LLM inference. It leverages NVTX markers and CUPTI callbacks to collect execution events across engine, backend, host, and device layers without modifying binaries. A call-chain merging algorithm reconstructs per-request trees aligned on a unified time base. For anomaly detection, TRUFFLD combines a Gaussian Mixture Model for numeric confidence scoring with an LLM-based reasoning stage for context-aware localization. Experiments on a multi-node GPU cluster serving Qwen-8B demonstrate near-perfect step-level detection and strong operator-level performance compared to classical and log-based baselines, with low overhead."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Clear motivation and relevance: Addresses a critical gap in request-level, end-to-end observability for LLM inference under high concurrency.\n\nNon-intrusive design: Uses NVTX and CUPTI without modifying binaries, minimizing deployment overhead.\nFine-grained representation: Reconstructs per-request call-chain trees that preserve structural and temporal semantics.\nTwo-stage anomaly detection: Combines statistical modeling with LLM reasoning for robust and interpretable diagnosis.\nStrong empirical results: Outperforms multiple baselines on both step-level and operator-level metrics; overhead analysis shows practical feasibility.\nComprehensive evaluation: Includes fault injection across software, CUDA, hardware, and communication layers."}, "weaknesses": {"value": "Combination of existing techniques: The approach is mainly based on a combination of existing GMM and LLM methods.\nLimited generalization evidence: Experiments focus on Qwen-8B with vLLM; applicability to other models or inference engines is not demonstrated.\nLLM reasoning details: Prompt design and schema enforcement are described, but robustness to unseen anomalies and cost implications of LLM inference could be discussed more thoroughly.\nScalability concerns: While overhead is reported, the impact on large clusters or multi-tenant environments is unclear.\nInterpretability trade-offs: The reasoning stage relies on textual context; failure cases or misdiagnoses are not deeply analyzed.\nAblation studies: Missing analysis of the contribution of each component (e.g., GMM vs. LLM reasoning) to overall performance."}, "questions": {"value": "What are new in the proposed use of GMM and LLM?\nHow much does each component contribute to the anomaly detection improvement?\nHow does TRUFFLD scale when deployed on clusters with hundreds of GPUs and thousands of concurrent requests?\nHow sensitive is the anomaly detection pipeline to the choice of GMM hyperparameters?\nHow sensitive is the anomaly detection pipeline to the choice of LLM?\nCould you provide examples of failure cases where TRUFFLD misdiagnoses anomalies and explain why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XHCzswExg7", "forum": "S9iB7BA3Zq", "replyto": "S9iB7BA3Zq", "signatures": ["ICLR.cc/2026/Conference/Submission6342/Reviewer_pE3f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6342/Reviewer_pE3f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762196028704, "cdate": 1762196028704, "tmdate": 1762918632256, "mdate": 1762918632256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}