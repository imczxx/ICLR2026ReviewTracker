{"id": "o1OEzcafwi", "number": 18700, "cdate": 1758290250895, "mdate": 1759897086679, "content": {"title": "Adaptive Training of INRs via Pruning and Densification", "abstract": "Encoding input coordinates with sinusoidal functions into multilayer perceptrons\n(MLPs) has proven effective for implicit neural representations (INRs) of low-\ndimensional signals, enabling the modeling of high-frequency details. However,\nselecting appropriate input frequencies and architectures while managing parameter\nredundancy remains an open challenge, often addressed through heuristics and\nheavy hyperparameter optimization schemes. In this paper, we introduce AIRe\n(**A**daptive **I**mplicit neural **Re**presentation), an adaptive training scheme that refines\nthe INR architecture over the course of optimization. Our method uses a neuron\npruning mechanism to avoid redundancy and input frequency densification to\nimprove representation capacity, leading to an improved trade-off between network\nsize and reconstruction quality. For pruning, we first identify less-contributory\nneurons and apply a targeted weight decay to transfer their information to the\nremaining neurons, followed by structured pruning. Next, the densification stage\nadds input frequencies to spectrum regions where the signal underfits, expanding\nthe representational basis. Through experiments on images and SDFs, we show\nthat AIRe reduces model size while preserving, or even improving, reconstruction\nquality. Code and pretrained models will be released for public use.", "tldr": "We present an adaptive training of low-dimension signals consisting of pruning and densifying the neurons of sinusoidal neural networks.", "keywords": ["implicit neural representation", "sinusoidal neural networks", "coordinate-based neural networks"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/896e04a14ed8b6b57d5f25d5c65a31ef9c3cd90e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AIRe (Adaptive Implicit neural Representation), a novel adaptive training framework designed to refine the architecture of Implicit Neural Representations (INRs), specifically those based on sinusoidal functions, during the optimization process. The primary goal is to address the challenge of parameter redundancy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces AIRe, a novel adaptive training framework that effectively combines neuron densification and pruning\n2. The numerical results consistently confirm the theoretical assumptions laid out for the AIRe framework"}, "weaknesses": {"value": "1. Lack of comparison with:\n- SPDER: Semiperiodic Damping-Enabled Object Representation\n- FreSh: Frequency Shifting for Accelerated Neural Representation Learning\n\n2. Authors do not commenton the  relation between pruning and densification in INR and Gaussian Splatting\n- 3D Gaussian Splatting for Real-Time Radiance Field Rendering\n\nwhich can be interesting for readers.\n\n3. In related works, authors should mention that Gaussian Components can also reconstruct 2D images\n\n- GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting\n- MiraGe: Editable 2D Images using Gaussian Splatting\n\n4. Since the method introduces multiple additional training phases (densification, TWD, fine-tuning), the real efficiency gain remains unclear.\n\n5. The experimental evaluation primarily contrasts AIRe with generic pruning algorithms such as RigL and DepGraph, which are not tailored to INRs.\n\n6. The proposed densification and pruning schedules rely on manually chosen hyperparameters (e.g., number of added neurons, pruning thresholds, or training epochs).\n\n7. The ablation studies show that the densification component offers little benefit for the FINER architecture."}, "questions": {"value": "1. How does AIRe compare quantitatively with recent adaptive INR methods such as SPDER or FreSh? \n\n2. Could the authors clarify the conceptual relation between pruning/densification in INRs and Gaussian Splatting techniques? \n\n3. Why are recent 2D Gaussian-based representations (e.g., GaussianImage, MiraGe) not discussed in Related Work? \n\n4. What are the actual training and inference time gains achieved by AIRe after including the extra adaptation phases? \n\n5. Why are only generic pruning baselines (RigL, DepGraph) considered instead of INR-specific adaptive models? \n\n6. Are the pruning and densification hyperparameters selected manually or adaptively during training? \n\n7. Why does the densification component provide little or no improvement for the FINER architecture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z1XA2q0Lba", "forum": "o1OEzcafwi", "replyto": "o1OEzcafwi", "signatures": ["ICLR.cc/2026/Conference/Submission18700/Reviewer_aRcV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18700/Reviewer_aRcV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761206408158, "cdate": 1761206408158, "tmdate": 1762928404173, "mdate": 1762928404173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AIRe (Adaptive Implicit Neural Representation), an adaptive training framework for INR, which primarily employs neuron pruning and densification strategies to adjust the network. The introduction of the TWD mechanism allows information from low-contributing neurons to be transferred to important neurons, theoretically ensuring the stability of pruning. Input frequency densification enhances the network’s representational capacity. Experiments across multiple tasks demonstrate that AIRe can reduce model size while maintaining or even improving reconstruction quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes pruning and densification strategies for INR, including the TWD mechanism to transfer information from low-contributing neurons. \n\n2. Experiments across multiple tasks comprehensively validate the approach, showing that it reduces model size while maintaining or sometimes improving reconstruction quality. \n\n3. The work has potential significance for pruning and densification in the INR domain."}, "weaknesses": {"value": "1.Some theoretical explanations are unclear. It is not specified how the 2ωj frequency is determined during densification and why this particular frequency is chosen.\n\n2.Discussion of pruning effects on input and hidden layers is limited.For the SDF task (Lines 324–334), hidden layers are pruned, while for the image fitting task (Lines 378–385), input neurons are pruned. The paper only mentions that pruning the input layer may harm reconstruction.\n\n3.The pruning threshold ϵ is not clearly defined; it is unclear whether it is fixed, layer-wise adaptive, or percentile-based. Clarification on how ϵ is chosen or tuned would be helpful.\n\n4.Experiments mainly report final PSNR or CD. Including spectral visualizations and convergence curves would better illustrate how pruning and densification affect optimization and frequency coverage.\n\n5.Experimental setup is somewhat unclear. In Table 1, it is not specified whether the reported “large” baseline is based on SIREN, FINER, or something else.\n\n6.Some figures (3, 4, 6) could be improved for clarity. Figures 3 and 4 could be redesigned to provide richer visual comparisons, and the right half of Figure 6 is somewhat confusing in the information it intends to convey."}, "questions": {"value": "1.Why is densification performed using 2ωj for new neurons rather than other frequency choices? Is there a theoretical justification or is it empirically determined?\n\n2.In the densify-before-prune schedule, are newly added neurons immediately considered low-contributing and pruned by TWD? \n\n3.What does the “large” baseline in Table 1 correspond to (SIREN, FINER, or their average)?\n\n4.What criteria guide the choice of pruning input versus hidden layers, and can more theoretical or experimental analysis be provided on the pruning effects on input and hidden layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1N01VoxZ51", "forum": "o1OEzcafwi", "replyto": "o1OEzcafwi", "signatures": ["ICLR.cc/2026/Conference/Submission18700/Reviewer_HQzk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18700/Reviewer_HQzk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555377868, "cdate": 1761555377868, "tmdate": 1762928403539, "mdate": 1762928403539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AIRe (Adaptive Implicit Neural Representation), a training framework that progressively adapts a potentially overparameterized INR to the target data through two complementary operations: pruning and densification of neurons. It provides a general \nframework for the adaptive training of INRs, driven by pruning and densification. In the theoretical side, it leverage a harmonic expansion of sinusoidal neural networks to derive principled densification schemes, and prove stability of our neural networks under magnitudebased pruning. The method was mainly applied to SIREN and FINER for the experiments. Experiments were conducted on images, SDFs, and NeRFs,"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The integration of pruning and frequency densification within INR training is innovative and addresses a key limitation—manual architecture tuning. In addition, the paper provides mathematical proofs (Theorem 1 and 2) explaining spectral densification and pruning stability, enhancing methodological rigor."}, "weaknesses": {"value": "[1] The method only tested on low-dimensional signals (2D images, SDFs, small NeRF scenes). Therefore, it should be tested on different kinds of datasets. For example,  PDEs.\n\n[2] One of the major drawbacks of INR is the long training time. By adding pruning and densification, will it increase the training time? An analysis of training time should be provided.\n\n[3[How about the GPU comsumption? Like the Gfloop\n\n[4]I understand that it was applied to SIREN and FINER and reports some results. However, some other baseline should also be added to the experiments.  For example, LosslessINR [r1] \nHan, Woo Kyoung, et al. \"Towards Lossless Implicit Neural Representation via Bit Plane Decomposition.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NNI85YjUcV", "forum": "o1OEzcafwi", "replyto": "o1OEzcafwi", "signatures": ["ICLR.cc/2026/Conference/Submission18700/Reviewer_Bn28"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18700/Reviewer_Bn28"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957212654, "cdate": 1761957212654, "tmdate": 1762928402860, "mdate": 1762928402860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenges of parameter redundancy and limited representational capacity in implicit neural representations (INRs). It proposes AIRe (Adaptive Implicit neural Representation), an adaptive training framework that alternates between neuron pruning to remove redundant units and frequency densification to introduce additional input frequencies where the signal underfits. This process yields compact networks that better balance efficiency and reconstruction quality.\n\nThe method is validated on several signal representation tasks, including image reconstruction, 3D surface fitting, and neural rendering (NeRF). AIRe consistently matches or surpasses larger overparameterised baselines in reconstruction accuracy while using substantially fewer parameters, and outperforms general-purpose adaptive and pruning methods  (RigL, DepGraph). However, the gains are less pronounced on complex, practically relevant tasks such as NeRF-based view synthesis, where improvements over smaller fixed models remain marginal and size reduction benefits limited. Future revisions could strengthen the work by discussing the intended applicability of the approach beyond signal fitting, analysing inference-time efficiency, and broadening or clarifying comparisons with existing INR-specific pruning and sparsity methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**S1. Exploration of INR-specific network pruning.** The paper tackles a timely and important problem—adapting pruning strategies to the unique characteristics of implicit neural representations (INRs). This direction is both compelling and relevant, as multilayer perceptrons (MLPs) remain a major computational bottleneck in tasks such as neural rendering. The proposed approach demonstrates clear benefits over general-purpose pruning methods (Table 2), showing consistent and INR-aware improvements in efficiency–accuracy trade-offs."}, "weaknesses": {"value": "**W1. Limited performance gains on relevant or real-world tasks.** The most extensive experiments (Table 8, supplementary) show only marginal improvements on the NeRF reconstruction task. The proposed method achieves roughly 20 % model-size reduction but delivers only minor accuracy gains over the same-size model trained from scratch. This raises questions about its effectiveness for complex, practically relevant scenarios such as neural rendering.\n\n**W2. Missing analysis of inference efficiency.** While the paper discusses reductions in model size and training time (Table 7, supplementary), it does not analyse the effect of pruning on inference time, a major bottleneck for INRs in applications like real-time rendering and novel-view synthesis. Understanding how architectural adaptation impacts forward-pass latency is essential for assessing practical utility.\n\n**W3. Incomplete comparison with prior INR pruning work.** The omission of Zell et al. (2022) from quantitative comparison is insufficiently justified. Although that method reduces model size only through input-layer pruning, it remains—by the authors’ own admission—“the only prior work exploring the pruning (or adaptation) of INRs” (l. 118–120) and should be included for completeness. Moreover, other relevant studies addressing sparsity or compression in INRs [1–2] are not discussed; establishing their relation to the proposed approach would clarify the work’s novelty and scope.\n\n\nReferences\n\n[1] Lee, J., Tack, J., Lee, N., & Shin, J. Meta-learning sparse implicit neural representations. NeurIPS 2021.\n\n[2] Jayasundara, D., Rajagopalan, S., Ranasinghe, Y., Tran, T. D., & Patel, V. M. (2025). SINR: Sparsity-Driven Compressed Implicit Neural Representations. CVPR 2025."}, "questions": {"value": "**Q1. Applicability to real-world INR tasks.** If the improvements on neural rendering (e.g., NeRF) remain minor, where do the authors envision this approach having the greatest practical impact? In which INR domains or signal types does adaptive pruning most clearly translate into meaningful efficiency or accuracy gains?\n\n**Q2. Inference efficiency.** What is the impact of pruning and architectural adaptation on inference time—particularly for forward-pass latency in real-time or high-resolution INR settings—beyond the reported reductions in model size and training time?\n\n**Q3. Comparison with prior INR pruning methods.** The paper would benefit from a deeper discussion of related INR pruning and sparsity works (e.g., Zell et al., 2022; Lee et al., 2021; Jayasundara et al., 2025). If feasible, these could be included as additional baselines; otherwise, it would be helpful to clarify why such comparisons were not performed and how the proposed approach conceptually differs from them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OFFCiFKJ1b", "forum": "o1OEzcafwi", "replyto": "o1OEzcafwi", "signatures": ["ICLR.cc/2026/Conference/Submission18700/Reviewer_Gj4a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18700/Reviewer_Gj4a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002600087, "cdate": 1762002600087, "tmdate": 1762928401584, "mdate": 1762928401584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}