{"id": "7g74cnF6BJ", "number": 3203, "cdate": 1757365367782, "mdate": 1763580039555, "content": {"title": "Should You Use Your LLM to Explore or Exploit?", "abstract": "We evaluate the ability of the current generation of large language models (LLMs) to help a decision-making agent facing an exploration-exploitation tradeoff. We use LLMs to explore and exploit in silos in various (contextual) bandit tasks. We find that while the current LLMs often struggle to exploit, in-context mitigations may be used to substantially improve performance for small-scale tasks. However even then, LLMs perform worse than a simple linear regression. On the other hand, we find that LLMs do help at exploring large action spaces with inherent semantics, by suggesting suitable candidates to explore.", "tldr": "", "keywords": ["bandits", "exploration", "exploitation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da8342b35c2d123e46ee79f4629c4ad4995f2a24.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies whether LLMs should be used for exploitation or exploration in bandit-style decision problems. In this  paper, \"exploitation\" means let LLMs directly pick the best action from histories, and  \"exploration\" means let LLMs propose actions worth trying. In multi-armed bandits and contextual bandits, LLMs consistently underperform linear regression at exploitation, even with COT and natural mitigations (history compression via k-nearest/k-means). For exploration in large textual action spaces, treating LLM as a candidate arm generator is effective. The paper's contribution is that it shows LLMs are weak at exploitation but can be useful at proposing candidate arms in textual action spaces."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper cleanly separates exploitation vs. exploration roles for LLMs in bandit-style decision making, then study separately. This framing is clear.\n2. This paper shows in-context history compression strategies are effective in the scenario of using LLMs for exploitation, which is a meaningful guidance.\n3. The results give a concrete role assignment for LLMs in decision systems: avoid using them as exploitation oracles, but leverage them as candidate generators in large textual action spaces.\n4. Clear task constructions and prompt templates make the work easy to understand."}, "weaknesses": {"value": "1. In brief, the conclusions are predictable and not useful. The two central takeaways (LLMs underperform simple regression for exploitation, and LLMs can generate reasonable textual candidates) are easy to anticipate. The former favors stable numeric estimation where classic methods excel; the latter is essentially semantic discretization where LLMs have strong language priors, so beating crude controls is unsurprising.\n2. Missing online evaluation of the exploration–exploitation trade-off. Bandits are about multi-round online balancing of exploration and exploitation and minimizing cumulative regret. This paper measures two isolated, one-shot oracles (pick an arm once; generate a candidate set once), leaving the key question unanswered: if an LLM actually participates in the decision loop—as a prior/uncertainty provider or an adaptive proposer—does it reduce regret, improve sample efficiency, and remain stable under fixed budgets? To answer this, this paper should evaluate LLM-guided bandit algorithms in sequence-decision scenarios, and report regret and token/latency budgets. Krishnamurthy et al. [1] have done some valuable work in this area, but this paper did not delve deeper into the exploration exploitation trade-off capability of LLMs.\n3. Overall, the paper’s framework (testing LLM exploitation and exploration in isolation) makes the experiments largely uninformative for bandits and yields near-zero contribution. First, in the MAB setting, the finding (“performance degrades (1) as history length T increases and (2) as the empirical gap decreases”) is not useful: for (1), modern LLMs can explicitly execute code and compute means/variances, so scaling to long histories is easy; for (2), of course the probability of picking the best arm drops as the task gets harder—this needs no experiment. Second, in the CB setting, this paper effectively shows that LLM supervised regression underperforms linear regression, which is unsurprising and misaligned with current practice where LLMs can readily call code/external tools; moreover, the paper offers no remedy for scaling d and K. Third, the LLMs as exploration oracles experiments merely demonstrate that LLMs can propose a few plausible text candidates as arms—an almost self-evident claim that hardly requires experimentation.\n4. Exploration baselines are too weak. The strongest comparator is “random candidates”, implemented by uniformly sampling K vectors in the sentence-embedding space and treating them as arms, with cosine similarity to a target vector as reward. This baseline is not even natural-language; the candidates are typically semantically meaningless and thus extremely weak. LLM has rich prior knowledge, so experiments using such a baseline cannot explain anything.\n\n[1] Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, and Aleksandrs Slivkins. Can\nlarge language models explore in-context? In NeurIPS, 2024."}, "questions": {"value": "1. Can you explain why do you ignore the exploration–exploitation trade-off and study the ability of LLMs to explore and exploit in-context in silos?\n2. Your random baseline samples vectors uniformly in embedding space (not on the natural-language manifold). Can you add some reasonable baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EeXv7PUsN2", "forum": "7g74cnF6BJ", "replyto": "7g74cnF6BJ", "signatures": ["ICLR.cc/2026/Conference/Submission3203/Reviewer_YRTL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3203/Reviewer_YRTL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760594315589, "cdate": 1760594315589, "tmdate": 1762916600483, "mdate": 1762916600483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies whether LLMs should be used for exploitation or exploration in bandit-style decision problems. In this  paper, \"exploitation\" means let LLMs directly pick the best action from histories, and  \"exploration\" means let LLMs propose actions worth trying. In multi-armed bandits and contextual bandits, LLMs consistently underperform linear regression at exploitation, even with COT and natural mitigations (history compression via k-nearest/k-means). For exploration in large textual action spaces, treating LLM as a candidate arm generator is effective. The paper's contribution is that it shows LLMs are weak at exploitation but can be useful at proposing candidate arms in textual action spaces."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper cleanly separates exploitation vs. exploration roles for LLMs in bandit-style decision making, then study separately. This framing is clear.\n2. This paper shows in-context history compression strategies are effective in the scenario of using LLMs for exploitation, which is a meaningful guidance.\n3. The results give a concrete role assignment for LLMs in decision systems: avoid using them as exploitation oracles, but leverage them as candidate generators in large textual action spaces.\n4. Clear task constructions and prompt templates make the work easy to understand."}, "weaknesses": {"value": "1. In brief, the conclusions are predictable and not useful. The two central takeaways (LLMs underperform simple regression for exploitation, and LLMs can generate reasonable textual candidates) are easy to anticipate. The former favors stable numeric estimation where classic methods excel; the latter is essentially semantic discretization where LLMs have strong language priors, so beating crude controls is unsurprising.\n2. Missing online evaluation of the exploration–exploitation trade-off. Bandits are about multi-round online balancing of exploration and exploitation and minimizing cumulative regret. This paper measures two isolated, one-shot oracles (pick an arm once; generate a candidate set once), leaving the key question unanswered: if an LLM actually participates in the decision loop—as a prior/uncertainty provider or an adaptive proposer—does it reduce regret, improve sample efficiency, and remain stable under fixed budgets? To answer this, this paper should evaluate LLM-guided bandit algorithms in sequence-decision scenarios, and report regret and token/latency budgets. Krishnamurthy et al. [1] have done some valuable work in this area, but this paper did not delve deeper into the exploration exploitation trade-off capability of LLMs.\n3. Overall, the paper’s framework (testing LLM exploitation and exploration in isolation) makes the experiments largely uninformative for bandits and yields near-zero contribution. First, in the MAB setting, the finding (“performance degrades (1) as history length T increases and (2) as the empirical gap decreases”) is not useful: for (1), modern LLMs can explicitly execute code and compute means/variances, so scaling to long histories is easy; for (2), of course the probability of picking the best arm drops as the task gets harder—this needs no experiment. Second, in the CB setting, this paper effectively shows that LLM supervised regression underperforms linear regression, which is unsurprising and misaligned with current practice where LLMs can readily call code/external tools; moreover, the paper offers no remedy for scaling d and K. Third, the LLMs as exploration oracles experiments merely demonstrate that LLMs can propose a few plausible text candidates as arms—an almost self-evident claim that hardly requires experimentation.\n4. Exploration baselines are too weak. The strongest comparator is “random candidates”, implemented by uniformly sampling K vectors in the sentence-embedding space and treating them as arms, with cosine similarity to a target vector as reward. This baseline is not even natural-language; the candidates are typically semantically meaningless and thus extremely weak. LLM has rich prior knowledge, so experiments using such a baseline cannot explain anything.\n\n[1] Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, and Aleksandrs Slivkins. Can\nlarge language models explore in-context? In NeurIPS, 2024."}, "questions": {"value": "1. Can you explain why do you ignore the exploration–exploitation trade-off and study the ability of LLMs to explore and exploit in-context in silos?\n2. Your random baseline samples vectors uniformly in embedding space (not on the natural-language manifold). Can you add some reasonable baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EeXv7PUsN2", "forum": "7g74cnF6BJ", "replyto": "7g74cnF6BJ", "signatures": ["ICLR.cc/2026/Conference/Submission3203/Reviewer_YRTL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3203/Reviewer_YRTL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760594315589, "cdate": 1760594315589, "tmdate": 1763715590223, "mdate": 1763715590223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates the capacity of current large language models (LLMs) to aid decision-making agents facing the exploration-exploitation tradeoff. The study assesses these two functions independently within various (contextual) bandit frameworks. The findings reveal that LLMs struggle significantly with exploitation; even when in-context mitigations like k-nearest or k-means summarization are applied to small-scale tasks, LLM performance remains inferior to a simple linear regression baseline. In contrast, the research demonstrates that LLMs are highly effective at exploration, particularly in large action spaces with inherent semantic structure. In this capacity, LLMs can propose a concise set of high-quality candidate actions, and employing these candidates within an off-the-shelf bandit algorithm yields performance far superior to random baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The paper introduces a clear and practical framework for combining Large Language Models with bandit algorithms to address optimization problems in vast, semantically-rich action spaces. It effectively decouples the problem by leveraging the LLM as an \"exploration oracle\" to generate a high-quality, reduced set of candidate actions.   This set is then efficiently processed by a standard bandit algorithm (e.g., UCB1) for optimization. The feasibility and superiority of this approach are well-supported by thorough experimentation.\n\n* The study is supported by a comprehensive suite of experiments that systematically evaluate LLM capabilities in sequential decision-making."}, "weaknesses": {"value": "* The paper's novelty is severely undermined by its extensive reliance on the experimental framework established by Krishnamurthy et al. (2024) [1]. The core research question, the background framing, the \"exploitation\" task design (including the MAB puzzles), and even the prompting strategies appear to be heavily borrowed, if not directly replicated, from [1]. While this paper offers a different conclusion by separating exploration and exploitation \"in silos,\" this distinction does not sufficiently justify re-using an almost identical research setup. This significant overlap in methodology and problem formulation makes the work appear derivative and substantially weakens its original contribution.\n\n* Using LLMs as an \"exploration oracle\"—appears to be effective only within a limited domain. The success of this method hinges on the action space being inherently and richly semantic (e.g., free-text answers, paper titles), requiring a strong text-understanding capability from the LLM. This raises significant concerns about the generalizability of the approach. It is unclear whether this framework can be extended to other high-dimensional decision-making tasks that are not purely text-based or where the semantic structure is less explicit for an LLM to leverage effectively.\n\n---\n[1] Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, and Aleksandrs Slivkins. Can large language models explore in-context? In *NeurIPS*, 2024."}, "questions": {"value": "* The paper's conclusion that LLMs struggle with exploitation is a significant claim. Given the rapid improvements in the reasoning and numerical processing capabilities of new LLMs (e.g., Gemini-2.5-Flash, Gemini-2.5-Pro), it is possible that newer, more advanced models might exhibit fundamentally different performance on these tasks. Can you show whether this negative conclusion regarding exploitation remains valid for the absolute latest generation of models?\n\n* The paper's primary positive finding relies on the LLM's role as an \"exploration oracle\" in tasks with inherently rich semantic action spaces (QA, arXiv titles). This raises a critical question about the method's broader applicability. Could this framework be adapted to more traditional, non-text-native bandit problems, such as classic recommender system benchmarks (e.g., the MovieLens dataset [1])? Specifically, if an LLM were used to propose a candidate set of items (e.g., movies) based on their metadata, could this approach  consistently outperform a pure contextual bandit algorithm?\n\n---\n[1] Harper, F. M. and Konstan, J. A. The movielens datasets: History and context. *Acm transactions on interactive intelligent systems (tiis)*, 5(4):1–19, 2015."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hsbq0I16n3", "forum": "7g74cnF6BJ", "replyto": "7g74cnF6BJ", "signatures": ["ICLR.cc/2026/Conference/Submission3203/Reviewer_9szX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3203/Reviewer_9szX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655197077, "cdate": 1761655197077, "tmdate": 1762916600038, "mdate": 1762916600038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper offers behavioral and empirical results on evaluating frontier LLMs in decision-making under uncertainty. The authors isolate the abilities of LLMs to act as exploitation oracles (making the best decision given current data) and exploration oracles (trying new options for long-term benefit)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper has very clear theoretical framing and uses RL theory to guide empirical investigations -- this is applaudable. \n\nI find the two testing situations interesting, especially the exploration case, where exploration in large action space has been very difficult for theory-driven algorithms and rely on aggressive assumptions to have efficient algorithms. \n\nDespite many issues and questions I have raised, this paper is technically sound and useful for expanding the discourse on LLM for exploration/exploitation. Most of my issues are not difficult to fix during rebuttal and I hope to engage with the authors in a constructive way to enhance this paper for a broader audience."}, "weaknesses": {"value": "**Lack of Real Domains**\n\nThe paper cited a few prior works -- in [1], they have expanded CB domain to include MovieLens, which is a common recommendation algorithm benchmark/dataset adapted for LLMs. The code has also been released.\n\nIt is understandable to do MAB experiments in synthetic/toy domain, but for CB, we should continue to incorporate realistic domains in healthcare, education, or recommendation systems. Understanding LLM's behavior in these domains is crucial, especially for a behavioral/empirical investigation like this.\n\nThe domain can be rather small, or the testing can be limited, but not incorporating them makes the paper less impactful and useful to the practitioners. \n\n**Lack of New Metrics**\n\nThe paper investigated `FracCorrect` and `rew` as two metrics. `FracCorrect` is already investigated in an earlier paper [1] but was called `OptFrac`. Also, [2] proposed `MinFrac`, which is an interesting metric to understand exploration behavior. Do you guys have some thoughts about what other metrics are interesting to track model's exploration and exploitation behaviors? Is it easy to redo some analysis on already collected data to yield more insights?\n\n**Misleading Names**\n\n`DeepSeek-R1-Distill-Qwen-32B` should not be named as `DeepSeek-R1`. A 32B model cannot be compared to a 685B model...this might be too misleading. Please name it DeepSeek-R1-Distill. In my practical experience, these two models are very different.\n\n\n[1] EVOLvE: Evaluating and Optimizing LLMs For Exploration\n[2] Can Large Language Models Explore In-Context?"}, "questions": {"value": "1. Any chance to add a realistic domain for CB (especially for the exploration oracle)? For the exploitation case, I think prior work (and your current investigation) is sufficient to show LLMs don't work that well. \n2. Prior works lack evaluation on thinking-models (DeepSeek-R1 -- full model, Qwen3, etc.) -- do you think you have time/ability to add some analysis on the full DeepSeek-R1, or re-use the data you saved from DeepSeek-R1-Distill-Qwen-32B and evaluating the quality of the chain of thoughts there (instead of doing CoT prompting, you can get the actual thinking tokens)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PJ7tPunInX", "forum": "7g74cnF6BJ", "replyto": "7g74cnF6BJ", "signatures": ["ICLR.cc/2026/Conference/Submission3203/Reviewer_dBmo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3203/Reviewer_dBmo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665115556, "cdate": 1761665115556, "tmdate": 1762916599524, "mdate": 1762916599524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "+ Summary & Contributions\n\t- Across a broad array of providers (GPT-4, GPT-4o, GPT-3.5, Qwen-2.5, Gemma-3, Mistral-7B, & DeepSeek-R1), the authors study the ability of LLMs to perform in-context exploration or exploitation, respectively, in contextual bandits as well as standard multi-armed bandit problems. \n\t- For exploitation, the authors study how well a particular LLM may examine the history of interaction thus far within a multi-armed/contextual bandit problem to determine a \"best action\" in a newly sampled context. The authors stratify their results across the action gap of the associated contextual bandit and observe that, as problem size or difficulty increases, LLMs struggle to discern an arm with highest empirical reward and fail to exploit.\n\t- For exploration, the authors consider regular multi-armed bandit problems with large textual action spaces and entertain an algorithmic approach that isolates focus onto a chosen subset of actions. The quality of LLM exploration is evaluated by how well a chosen subset of actions by the LLM can be passed to a downstream standard multi-armed bandit algorithm and facilitate good performance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "+ Quality\n\t- Strengths\n\t\t- The authors have done a good job of testing their hypotheses across a broad array of LLM providers.\n\t- Weaknesses\n\t\t* Major\n\t\t\t- I question the authors' choice of definition for the \"best arm given the current history\" (L132-145). It seems they have chosen to define this as the arm with highest empirical reward based on the history of observations observed thus far. Unless I've missed it, I don't believe the authors have specified how the T-step history was generated for these 5-armed bandits. Depending on the exploratory coverage of that history-generating process, this definition of \"best arm\" may or may not be reasonable. Was each action seen at least N times, for some reasonable value of N? More importantly, even if the history-generation was reasonable in its coverage, the task the authors are essentially evaluating LLMs on is the ability to compute averages and taking an argmax; why do we need LLMs to perform those arithmetic calculations for us when we have better tools for doing so? \n\t\t\t- While a bit of an aside, it does also seem slightly odd to not have proceeded in a Bayesian fashion by first taking some prior distribution over the underlying bandit environment/optimal arm (imposed either via prompting or through empirical estimation with the LLM of choice) and then inducing the well-defined posterior belief over the optimal arm given the history thus far (this is the distribution that, for example, Thompson Sampling uses for action selection via the probability-matching principle). The broader point is that it would have perhaps been more informative to have the LLMs generate a probability distribution representing a belief over the optimal arm, rather than a single guess.\n\t\t\t- The authors of [1] introduce clear performance metrics (suffix failure frequency and minimum action frequency) for the precise purpose of cleanly evaluating the exploitation and exploration capabilities of LLM agents in isolation. Why would the authors not capitalize on these metrics and leverage them to illustrate their points instead of leaning on accuracy (FracCorrect) and mean reward?\n\t\t\t- The authors orient their analysis of LLM exploration around the \"average expected reward\" of a downstream multi-armed bandit algorithm (UCB1) given a subset of the action space selected by a LLM. Why is this a good metric of exploration? Certainly, the Q/A and ArXiv tasks themselves address exploration by virtue of being bandits, but how does the performance of a traditional UCB bandit algorithm with a LLM-chosen action space give us a useful insight into the exploratory capabilities of the LLM? At best, it seems that the authors have established that a reasonably well-prompted LLM will generate a candidate set of actions that can lead to downstream bandit algorithm performance which is better than a randomly chosen set of actions. This seems like a rather low bar to clear and highly insufficient for concluding that \"the LLM does succeed as an exploration oracle\" (L419). One cherry-picked example does not really lend much confidence to the \"eye test\" (L420); perhaps this would carry more weight if a user study was conducted where lay people were tasked with judging the diversity of the chosen candidates against baselines. The expected reward variance is implicit in the visuals of Figure 7 and so it is unclear how a range of [0.25,0.55] in a reward function ranging from [-1,1] demonstrates good exploration.\n\t\t\t- The only reasonable conclusion that Figures 6 and 8 provide is that the authors mechanisms for LLM action candidate selection can lead to better performance with UCB1 than randomly chosen candidates. No conclusion beyond that is statistically significant given the severely overlapping confidence intervals.\n\t\t* Minor\n\t\t\t- The authors' jargon of \"exploit puzzles\" vs. \"exploit tasks\" is annoying to keep track of. \n\t\t\t- For a sampled exploit task, it's not clear why the authors needed to stratify their results by the empirical (action) gap, rather than simply computing the true action gap for the sampled multi-armed bandit problem.\n\t\t\t- Is the reason for footnote 4 (an unclear \"empirically best arm\" for a contextual bandit) due to the fact that the current context may have never been seen before? Again, trying to proceed in a more Bayesian fashion and eliciting the implicit prior a LLM may have over the rewards of each arm before observing any data might have been useful for avoiding this.\n\t\t\t- While the natural language bandit tasks used in the exploration analysis (Section 3) are nice, it would have been better to start with simpler, didactic examples of the same flavor as the 5-armed bandit in the preceding section.\n\t\t\t- The choice to only generate LLM candidate actions with a temperature of 0 or 1 (L108) is confusing. Were no other intermediate temperatures assessed? Depending on the problem, such extreme values may not always be the most sensible.\n\t\t\t- What does it mean to prompt a LLM for diversity in its responses \"with encouragement\"?\n\n+ Clarity\n\t- Strengths\n\t\t- Overall, the paper is clearly written and reasonably structured.\n\t- Weaknesses\n\t\t* Major\n\t\t\t- N/A\n\t\t* Minor\n\t\t\t- N/A\n\t\t\n\n+ Originality\n\t- Strengths\n\t\t- The proposed mitigations (L240-250) for exploitation in contextual bandit problems appear novel.\n\t- Weaknesses\n\t\t* Major\n\t\t\t- It's unclear what the novel insights of this paper are. In particular, it would seem that the overwhelming majority of insights communicated in this paper are either already shown or rendered moot through the convex hull of four existing works [1,2,3,4]. The first two papers (already properly acknowledged in the submission) lay out preliminary exposition on the in-context exploration capabilities and failures of LLMs. The latter two papers (not properly acknowledged in the submission) offer concrete avenues for addressing these deficiencies and provide positive empirical results for addressing exploration-exploitation trade-off in multi-armed bandits, contextual bandits, and MDPs.\n\t\t\t- The idea of using LLMs to propose candidate behaviors that can be subsequently scored for their propriety in decision-making is implicit in previous work such as SayCan [5]. The authors have essentially specialized the idea to multi-armed bandits specifically and opted to use UCB1 as the downstream mechanism for scoring the candidates.\n\t\t* Minor\n\t\t\t- The mitigation strategy for contextual bandit exploitation is reminiscent of the environment compression used in the theoretical study of [6]. While showcasing an empirical instantiation of the idea here is interesting, the core of the mitigation focuses on exploiting the contextual bandit problem rather than the LLM; this does little to advance our understanding of improving LLM proficiency in addressing the exploration-exploitation trade-off.\n\n+ Significance\n\t- Strengths\n\t\t- N/A\n\t- Weaknesses\n\t\t* Major\n\t\t\t- As far as I can tell, the authors have failed to report the total number of trials/random seeds uses in any of their experiments. Consequently, it's difficult to know how confident a reader ought to be in the presented 95% confidence intervals reported across all figures. In Figures 1-5 the right-hand side y-axes are labeled to show the \"Cumulative # Runs,\" but my understanding is that this is not the number of experiment trials but rather the cardinality of the task set S in FracCorrect(S).\n\t\t\t- The culminating findings (L180) from the authors' study of LLM exploitation (Section 2) are that LLMs fail to exploit successfully when faced with (1) longer histories and (2) harder bandit problems (as quantified by the action gap). Unfortunately, neither of these are novel findings. It is well understood that LLM recall diminishes as the context buffer increases. Additionally, it is well known that all bandit algorithms struggle to maintain performance as the action gap shrinks and corresponding bandit instances become more difficult. If the authors were using those negative results as a setup for a resolution, that would be one thing. Unfortunately, the negative results by themselves do not carry sufficient insight to inform a reader's next experiment with LLM agents.\n\t\t* Minor\n\t\t\t- N/A\n\t\t\t\n\t\t\n+ Final Remarks\n\t- I have identified severe issues on the axes of quality, originality, and significance. Putting issues with the experimental setup aside, there are considerable challenges with the core methodologies put forth in this paper (evaluating exploitation in LLMs by how accurately they can take a sum and argmax or evaluating exploration by the performance of a downstream, classic bandit algorithm). \n\n\n\n+ References\n\t1.  Krishnamurthy, Akshay, Keegan Harris, Dylan J. Foster, Cyril Zhang, and Aleksandrs Slivkins. \"Can large language models explore in-context?.\" Advances in Neural Information Processing Systems 37 (2024): 120124-120158.\n\t2. Nie, Allen, Yi Su, Bo Chang, Jonathan Lee, Ed H. Chi, Quoc V. Le, and Minmin Chen. \"EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration.\" (2025) In Forty-second International Conference on Machine Learning.\n\t3. Arumugam, Dilip, and Thomas L. Griffiths. \"Toward Efficient Exploration by Large Language Model Agents.\" In The Exploration in AI Today Workshop at ICML 2025.\n\t4. Monea, Giovanni, Antoine Bosselut, Kianté Brantley, and Yoav Artzi. \"Llms are in-context reinforcement learners.\" (2024).\n\t5. Ahn, Michael, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn et al. \"Do as i can, not as i say: Grounding language in robotic affordances.\" arXiv preprint arXiv:2204.01691 (2022).\n\t6. Dong, Shi, and Benjamin Van Roy. \"An information-theoretic analysis for thompson sampling with many actions.\" Advances in Neural Information Processing Systems 31 (2018)."}, "weaknesses": {"value": "Please see above"}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "97EN6gyLz2", "forum": "7g74cnF6BJ", "replyto": "7g74cnF6BJ", "signatures": ["ICLR.cc/2026/Conference/Submission3203/Reviewer_Y83g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3203/Reviewer_Y83g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877419417, "cdate": 1761877419417, "tmdate": 1762916599212, "mdate": 1762916599212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}