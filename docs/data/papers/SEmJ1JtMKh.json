{"id": "SEmJ1JtMKh", "number": 22834, "cdate": 1758336022309, "mdate": 1759896843843, "content": {"title": "Hierarchical Representational Transformations of Working Memory in Brains and Machines", "abstract": "Working memory (WM) maintains past inputs while processing new ones, yet how representations transform between encoding and retrieval remains unclear. Clarifying whether these representations are sustained through stable coding formats, dynamically updated subspaces, or their interplay is key to uncovering the mechanisms of WM. To address this, we combined high-resolution 7T fMRI from the Natural Scenes Dataset with recurrent neural networks (RNNs) trained on a naturalistic 1-back task. Using representational similarity, cross-decoding, and subspace geometry analyses, we directly compared rotational and non-rotational transformations between encoding and retrieval in brain regions and model layers. Our analyses revealed convergent evidence for a mixture mechanism of WM coding for encoding and retrieval information: early visual regions (V1–hV4) underwent large representational changes across encoding to retrieval phases, including both rotational and non-rotational transformations. Whereas higher-order regions in the prefrontal cortex (FEF, dlPFC) were more stable. Applying the same analyses to models showed partial alignment, but critically depended on the learning objective and the recurrent architecture. We examined two different encoder architectures, ResNet and Vision Transformer (ViT), each trained with supervised and self-supervised learning objectives. Supervised encoders preserved a hierarchical layer dissociation paralleling the cortical gradient in both rotational and non-rotational transformations, while self-supervised encoders diverged especially in reduced rotational alignment with the cortical hierarchy. Among recurrent architectures, gated architectures (GRU, LSTM) best reproduced the brain-like mixture of subspace rotational transformation. Taken together, these results established hierarchical shifts between flexibility and stability in WM representational transformation in both humans and machines, with supervised learning objectives combined with gated recurrent dynamics most closely resembling human WM mechanisms.", "tldr": "This work shows working memory (WM) in humans and machines follows a shift from dynamic to stable representational transformation along the hierarchy, with gated recurrent dynamics and supervised learning best capturing human-like WM mechanisms.", "keywords": ["working memory", "representational geometry", "cortical hierarchy", "learning objectives", "recurrent neural networks"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/71b0b71876cbcbbff9cb9e6cd93b575a99d8ce8f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work explores how working memory representations transform between encoding and retrieval phases in both human brains and artificial recurrent networks. Using 7T fMRI data from the Natural Scenes Dataset and RNNs trained on a 1-back task, the authors compare representational similarity, cross-decoding, and subspace rotation metrics to identify whether transformations are stable, rotational, or dynamic. They find that early visual regions show strong transformations while higher cortical regions are more stable, and that gated RNNs with supervised encoders best recapitulate these patterns."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The integration of high-field fMRI with modern deep networks to probe representational dynamics across biological and artificial systems is timely and relevant. The framing around WM encoding–retrieval transformations is novel and theoretically motivated.\n\nI liked the combination of RSA, cross-decoding, and geometric rotation analysis. All together these provide a multifaceted characterization of representational transformation.\n\nThe paper is clearly structured, includes well-labeled figures, and discusses limitations and reproducibility carefully.\n\nIt's an interesting use of the NSD dataset. However I was slighlty disappointed by the subselection of trials (one the main advantage of the NSD is his size, I understand keeping the experiments cheap and computationally feasible but I felt some potential is wasted here just by not using more data that are available)"}, "weaknesses": {"value": "While the analyses are well executed, the conceptual novelty is somewhat limited. The notion of mixed stable and dynamic subspaces in WM is well established (e.g., Stokes et al. 2013; Murray et al. 2017 and others). The contribution lies mainly in replicating this finding in 7T fMRI and showing partial correspondence with RNNs. \n\nOnly one participant from NSD is shown for visualization, and the main analyses appear averaged across subjects. Inter-subject variability, cross-validation robustness, or the reproducibility of representational gradients across individuals are not reported. Given the small number of participants (n=8), the statistical reliability of subspace angles and cross-decoding should be demonstrated per subject or with permutation testing.\n\nWhile RSA and rotation-angle analysis are intuitive, they reduce high-dimensional geometry to single scalar measures. Methods like Procrustes alignment, representational connectivity analysis, or canonical correlation analysis (CCA) could better capture transformations. The “top-1 angle” approach seems ad hoc and may not robustly separate rotational vs. non-rotational effects.\n\nThe encoders are frozen, which limits the capacity for task-specific representational adaptation that occurs in the human visual system during WM tasks. It was proven several times in audio (Brain-tuning) and during the Algonauts challenge 2025 (See CCN 2025) that fine-tuning these systems could improve the quality of learned representations, potentially strenghtening the results.\n\nRNNs are tipically used in neuroscience, but authors could maybe want to try an attention based model? It's a different type of bias and therefore a different assumption."}, "questions": {"value": "You report that supervised encoders and gated RNNs show “brain-like” hierarchical gradients of representational stability, but this comparison is largely qualitative. Could you quantify the correspondence between cortical ROIs and model layers—for example using cross-system RSA, centered-kernel alignment (CKA), or Procrustes distance—so that the claimed alignment can be statistically evaluated rather than inferred descriptively?\n\nThe rotation-angle analysis focuses on the top-1 or top-2 principal axes, that usually captures the first moments of the distribution. In other tasks such as image retrieval from brain activity or decoding it seems to be quite relevant to account for more PCs. How sensitive are these results to dimensionality choices and noise structure? Could you comment on that?\n\nSupervised encoders appear to match cortical gradients better than self-supervised ones, yet biological learning is unlikely to be explicitly supervised. On the other hand, for encoding, decoding or RSA between brain and models it was proven multiple times that unsupervised model reach better scores (See \"What can 5.17 billion regression fits tell us about artificial models of the human visual system?\" for example, or Algonauts 2023/2025 results or similar literature, this result is pretty consistent)\nCould you dissect what specific representational property of supervised training (e.g., category clustering, linear separability, or contrastive invariance loss) drives this alignment? Is the difference preserved after controlling for feature discriminability or category structure?\n\nHow do you expect your results to change if you use more data? This was a quite unexpected choice in your work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A2ryzCB5E7", "forum": "SEmJ1JtMKh", "replyto": "SEmJ1JtMKh", "signatures": ["ICLR.cc/2026/Conference/Submission22834/Reviewer_ts76"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22834/Reviewer_ts76"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760533538677, "cdate": 1760533538677, "tmdate": 1762942404984, "mdate": 1762942404984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how working memory (WM) representations transform between encoding and retrieval in both the human brain and recurrent neural networks. Using 7T fMRI data from the NSD-synthetic dataset and RNN models trained on a 1-back image task, the authors analyze representational similarity, cross-decoding, and geometric subspace rotations across cortical regions and model layers. They report that early visual regions undergo stronger transformations while higher-order prefrontal regions remain stable, and that gated architectures (LSTM, GRU) and supervised encoders best align with this hierarchical pattern."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This study bridges neuroscience and machine learning in a clear and structured way. The use of high-resolution fMRI with model-based analyses provides a rigorous framework for comparing representational dynamics. The combination of RSA, cross-decoding, and subspace-geometry analysis gives convergent evidence for hierarchical differences between dynamic and stable WM codes. The inclusion of both supervised and self-supervised visual encoders also adds an informative comparison for understanding how training objectives shape representational transformations."}, "weaknesses": {"value": "The main limitation is that both the behavioral and modeling aspects are confined to a single, highly specific task—the visual 1-back paradigm. This narrow scope restricts the generality of the conclusions about “working memory mechanisms” in either brains or machines. The framework does not address whether the same transformation principles would hold across different WM domains (e.g., auditory, spatial, or rule-based memory) or task complexities.\n\nMethodologically, the model design is constrained by strong assumptions: the frozen encoder architecture and fixed training regime prevent adaptive representational learning, making the observed “alignment” largely descriptive rather than mechanistic. The subspace rotation metric, while intuitive, simplifies nonlinear representational changes and risks over-interpreting correlations as shared mechanisms. Moreover, analyses such as RSA and cross-decoding rely on correlation-based similarity, which cannot disambiguate dimensional scaling, feature reweighting, or other geometric deformations.\n\nOverall, while the results are well presented, the modeling approach feels confirmatory—testing pre-defined correspondences on a single dataset—rather than probing how flexible task dynamics or alternative objectives might yield convergent or divergent brain-like transformations."}, "questions": {"value": "Could the authors test their claims on a broader range of WM tasks or stimuli to evaluate whether the observed hierarchical pattern generalizes beyond 1-back visual memory?\n\nHow much of the model–brain “alignment” arises from architectural choices (e.g., frozen encoders, layer normalization) versus learning dynamics?\n\nWould fine-tuning the encoder jointly with the recurrent module change the geometry of encoding–retrieval subspaces?\n\nThe rotation-based metric assumes linear subspace alignment — have the authors considered nonlinear manifold alignment or mutual-information–based measures to test representational overlap?\n\nSince the recurrent models are trained on synthetic data with limited variability, could the results be partly driven by dataset idiosyncrasies rather than genuine hierarchical transformation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fkho0FOQYn", "forum": "SEmJ1JtMKh", "replyto": "SEmJ1JtMKh", "signatures": ["ICLR.cc/2026/Conference/Submission22834/Reviewer_ppZL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22834/Reviewer_ppZL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867767464, "cdate": 1761867767464, "tmdate": 1762942404210, "mdate": 1762942404210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study investigates differences in representations across the encoding and retrieval phases during a 1-back working memory task. Using the natural scenes dataset, they perform a series of parallel analyses on fMRI activity from working memory associated regions of the human brain and activations from two visual encoders (ResNet & ViT) and layers of different recurrent neural network architectures (RNN, GRU, LSTM). They find that the encoding and retrieval phases of working memory have partially overlapping representations, but that they follow rotational and non-rotational transformations across the visual, mid-level, and frontal regions. These findings were somewhat replicated in the recurrent neural network models, with supervised training regimes and gating architectures capturing the most brain-like representations."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The analysis into WM representation dynamics of the 1-back task was of high depth (combined RSA, cross-decoding, and geometry) \n- They find an interesting contradiction to previous findings (https://pmc.ncbi.nlm.nih.gov/articles/PMC7826371/) on supervision and the learning of brain-like representations. \n- The methods and results of the paper were presented in a clear manner\n- They perform strong statistical tests to validate results"}, "weaknesses": {"value": "Related to Soundness: \n- I have a hard time imagining how the brain can solve any n-back task with a stable memory representation. Given that in any n-back task new stimuli are constantly coming in, I am struggling to see how any algorithm can solve this task without a dynamic neural code. In the absence of such candidate, I had to retreat to assume the possibility of other scenarios being at work that may explain the high RDM similarity in dlPFC. At the top of this list is that this area is either not involved in solving this task altogether or that only a portion of this region is involved. If some of these regions are not involved in solving this task, I would expect that the activity in those voxels to be random and as a result the SNR to be lower. A helpful way to give some insights into this would be to check the SNR within different ROIs. Related to this point, fig3 results shows that the E->E decoding accuracy is much lower in FEF and dlPFC compared to other areas. This may indicate that the measurement noise is generally higher in those areas. However, it could also indicate that the information is not encoded in these areas in a linearly decodable manner. \n\n- It is unclear whether multiple instances of each model were tested or not. I didn't notice any mention of having multiple seeds for each model. Error bars are non-existent in Fig1 plots and in other figures where they exist, it is unclear how they were computed. \n- An alternative view of the relatively high similarity in associational areas is that they may show relatively less variation to stimuli in general.  For example, the activity in some or many voxels may remain unchanged in response to changing stimuli because they are not being involved in processing 1-back task. This may be interpreted as evidence of a stable code while in reality parts of the network (i.e. voxels) may not be involved in performing the task altogether.  \n- Lei et al. 2024 had shown that the distribution of tasks on which a model is trained could have drastic effects on its learned representational geometry. This is an important factor that was not considered in the study and could potentially affect the interpretations drawn from the experiments. \n- The model consists of 2 RNN layers. It's unclear how this architecture is perceived to map onto the brain. Why were two layers selected? How does that choice in general affect biological similarity? Both anatomically and representation-wise? \n- why were only top-1 and top-2 axes used in fig4 analyses? I noticed a description of the Procrustes analysis in the extended methods. Is that used to perform the rotation analyses? Why wasn't it tried on all decoding directions and instead the angles of top1/2 principal angles are used? \n\nRelated to Presentation: \nThe paper is presented generally well; however, there are several areas that need improvements:\n- Lines 177-178: Remove redundant “in addition to the project layer.”\n- Lines 345-349: The statement “..the rotation angle between encoding and retrieval was smaller in the low-level regions (V2, V3) compared to the high-level frontal areas…” seems to contradict the findings for top-1 angles just mentioned above it “...reduction in rotations when moving from low-level visual areas to higher-level brain regions..”\n- which model type is shown in fig2? \n- what were the decoders in fig3 trained to do? It’s unclear what the decoding task is \n- line 321: while the difference between E-E and E-R accuracies is lower in these two areas, the E-E accuracy is substantially lower than other areas. It is misleading to interpret that as evidence of “better generalization”\n- line 324-325: I’m not sure which result is used to back this statement. Fig 3b shows a significant drop in performance between the E-E and E-R cases. \n\nRelated to Contribution: \n- The paper's contributions and scientific impact are diminished by the limited working memory task space (1-back object matching only). While the methods and results for this specific task are interesting, general conclusions about how working memory encoding and retrieval representations unfold across time cannot be drawn from them. Ideally, this kind of analysis would have covered a wider range of tasks, as the similar Lei et al. 2025 paper did (9 tasks), as mentioned in the introduction. While NSD is a limiting factor, and that no other broad visual working memory fMRI datasets exist yet; nonetheless, the contributions are still diminished as a result. \n\n- Including more modern architectures such as state-space models would have improved the breadth of the findings\n- A more direct way of comparing similarity between the model representations and the brain would have been to measure predictability of different brain ROIs from each model type using linear regression or similar methods."}, "questions": {"value": "Why was the use of only 1-task not mentioned in the limitations? Do the authors feel this task paradigm is sufficient to support the broad claims on the nature of encoding-retrieval representations during WM? If so, why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xDfsYgPRQI", "forum": "SEmJ1JtMKh", "replyto": "SEmJ1JtMKh", "signatures": ["ICLR.cc/2026/Conference/Submission22834/Reviewer_mji9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22834/Reviewer_mji9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060752084, "cdate": 1762060752084, "tmdate": 1762942403828, "mdate": 1762942403828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this submission, the authors employ representational similarity analysis to investigate how items are encoded in both artificial and biological visual working memory. They evaluate three competing hypotheses regarding the similarity of representations during encoding and retrieval phases. Their findings indicate a mixed coding mechanism, with representational stability across these phases increasing along the cortical hierarchy. To further support this, the authors assess decoding accuracy using mismatched encoding and retrieval representations, providing additional evidence for the mixed coding mechanism across cortical areas. Finally, they perform an ablation study on various learning objectives and architectural choices to determine which configurations yield neural networks most consistent with brain data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n- The fields of neuroscience and cognitive science have long tried to better understand how items are encoded in visual working memory with many competing theories proposed to support the process. The current submission uses tools from AI and data science to suggest that the particular encoding transformation in biological (and artificial) neural networks is a function of a hierarchical processing.\n- I personally find the decoding evidence in Sec 4.2.2 to be quite a creative way to show the varied encoding schemes employed by neural networks to store items in the working memory. \n- The paper is theoretically sound, and the reported statistical analyses are conducted with commendable rigor."}, "weaknesses": {"value": "- The current submission presents compelling evidence regarding how different cortical areas represent items in visual working memory. However, I am concerned that the observed enhanced stability in higher visual representations—a central aspect of the proposed mixed coding hypothesis—may be influenced by a greater bias in recordings from these higher areas. To address this potential confound, I recommend demonstrating that decoders trained and tested on retrieval phase data achieve higher accuracy than those trained on encoder phase data.\n- The analysis of how training objectives and architectural choices affect alignment with brain data is intriguing, especially as it contrasts with previous work suggesting that self-supervised learning (SSL) objectives yield more human-like visual representations [1]. I encourage the authors to expand this section by explicitly relating their findings to prior studies that claim SSL methods produce models more aligned with brain data than those trained with supervised learning objectives.\n\nReferences:\n1. Zhuang, C., Yan, S., Nayebi, A., Schrimpf, M., Frank, M. C., DiCarlo, J. J., & Yamins, D. L. (2021). Unsupervised neural network models of the ventral visual stream. Proceedings of the National Academy of Sciences, 118(3), e2014196118."}, "questions": {"value": "NA. Please refer to my review above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZMNYxqfxdM", "forum": "SEmJ1JtMKh", "replyto": "SEmJ1JtMKh", "signatures": ["ICLR.cc/2026/Conference/Submission22834/Reviewer_1p9M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22834/Reviewer_1p9M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762223476603, "cdate": 1762223476603, "tmdate": 1762942403316, "mdate": 1762942403316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}