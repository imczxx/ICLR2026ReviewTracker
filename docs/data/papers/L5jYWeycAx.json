{"id": "L5jYWeycAx", "number": 20337, "cdate": 1758304876879, "mdate": 1763285116252, "content": {"title": "Learning on a Razor’s Edge: Identifiability and Singularity of Polynomial Neural Networks", "abstract": "We study function spaces parametrized by neural networks, referred to as neuromanifolds. Specifically, we focus on deep Multi-Layer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs) with an activation function that is a sufficiently generic polynomial. First, we address the identifiability problem, showing that, for almost all functions in the neuromanifold of an MLP, there exist only finitely many parameter choices yielding that function. For CNNs, the parametrization is generically one-to-one. As a consequence, we compute the dimension of the neuromanifold. Second, we describe singular points of neuromanifolds. We characterize singularities completely for CNNs, and partially for MLPs. In both cases, they arise from sparse subnetworks. For MLPs, we prove that these singularities often correspond to critical points of the mean-squared error loss, which does not hold for CNNs. This provides a geometric explanation of the sparsity bias of MLPs. All of our results leverage tools from algebraic geometry.", "tldr": "We discuss identifiability of MLPs and CNNs with a generic polynomial activation, and relate the singularities of their neuromanifolds to subnetworks and sparsity bias.", "keywords": ["identifiability", "singularities", "critical points", "neuromanifolds", "polynomial activation", "algebraic geometry"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9484431b452e47be2091e35f9a7e8d16a74eef2b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the function spaces corresponding to CNNs and MLPs with polynomial activations using tools from algebraic geometry. Due to polynomial activations, these function spaces are manifolds. The authors show that almost everywhere, each function on this manifold corresponds to at-most finitely many parameter values of the network (the identifiability problem) and stronger result for CNNs (almost everywhere uniformly identifiable). The authors also characterize the singularities of the neuromanifolds. \n\n\nI want to note that I have no training in algebraic geometry and most of the results and ideas in this paper appeared opaque to me. I have set my confidence level to be low to reflect this fact."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles the important question about the class of functions represented by neural networks. While standard results with non-linear non-polynomial activation functions consider approximation properties, this considers a different of view: that of characterizing the set of functions exactly represented by neural network families. The paper answers some deep questions about this function space such as identifiability and singularity."}, "weaknesses": {"value": "- The authors claim that the identifiability results were previously only known for sigmoid and tanh activation functions. Given that they are more widespread in ML than polynomial activations, I would say that the current results are not as relevant to the community. \n\n- It does not directly give us relevant insights into practically relevant activation functions such as ReLU or sigmoid."}, "questions": {"value": "- Theorem 4.1 requires $r$ to be large enough. Is there a quantitative estimate about how large it should be? \n\n- In Theorem 4.2 it is not clear what \"enough coefficients of \\sigma are non-vanishing\" means. \n\n- The term \"generic polynomial\" is used throughout the paper, but I could not gather the meaning of this term. This could be because I am not trained in algebraic geometry."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "6tnlGsfcDD", "forum": "L5jYWeycAx", "replyto": "L5jYWeycAx", "signatures": ["ICLR.cc/2026/Conference/Submission20337/Reviewer_RBeS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20337/Reviewer_RBeS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761222799019, "cdate": 1761222799019, "tmdate": 1762933797141, "mdate": 1762933797141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the (dis)connectedness and the existence of singularities in the parameter space of ReLU neural networks whose architectures admit a DAG computational graph representation. The authors then leverage these results to study whether these elements can exert impact on the dynamics of gradient flow (GF) of standard training algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I generally like the paper. Here are several remarkable points:\n1. The conservative law for ReLU networks in the DAG case is very elegant. \n2. Theorem 1 makes a very nice connection to flow problem in graph theory.\n3. Theorem 2 - Proposition 5 to 7 provide a very clean picture on the dynamics of gradient flow in the existence of singularities"}, "weaknesses": {"value": "1. I have a hard time distinguishing what are the main contributions and what are already proved in the literature. Authors might want to re-organize the section 2, and credit properly all the results (theorems, propositions, definitions) if they are ever taken/inspired by previous works.\n\n2. Do the author forget to define the notion of stable by forward/backward edges in the announcement of Theorem 1? Otherwise, I believe that Theorem 1 needs rephrasing to be easier to understand."}, "questions": {"value": "1. Do Proposition 6 - 7 imply that singularities are truly rare? It seems to me that the limit of GF can still be a singularity (or a sparse subnetwork). If the GF dynamics does not bias towards sparse subnetworks, do you have any idea which points are preferable for the convergence of GF?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GuNPRK7d6S", "forum": "L5jYWeycAx", "replyto": "L5jYWeycAx", "signatures": ["ICLR.cc/2026/Conference/Submission20337/Reviewer_jF6i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20337/Reviewer_jF6i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735812937, "cdate": 1761735812937, "tmdate": 1762933796635, "mdate": 1762933796635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents their study on the function spaces parameterized by polynomial neural networks (i.e., those whose activation functions are polynomial). There are two main contributions: identifiability and singularity of functions in the neuromanifold (i.e., functions representable by neural networks). For the former, the authors show that for generic functions in neuromanifold, the set of parameters realizing these functions is at most finitely many or singleton, for Multi-Layer Perceptrons (MLP) and Convolutional Neural Networks (CNN) architectures respectively. For the latter, they characterize singularities as functions realized by sparse subnetworks and links this discovery to the sparsity bias of MLPs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper are generally well-written and the results are well-presented. While I do not dive into the proof, their results look sound to me. Two contributions are mathematically interesting and suggest further following work."}, "weaknesses": {"value": "Several points deserves to be further polished:\n1. Since most architectures use ReLU, I find that it is better to connect the current results to the ReLU cases (authors did admit this limitation in section 5).\n2. The bound on the degree of the activation in Theorem 4.1 is vacuous in the dimensions of the neural network architecture. Hence, I am not sure if this result reflects what we truly observe in practice.\n3. If I understand it correctly, the definition of critically exposed implies that there exists a positive probability that mappings\n4. admit a weight in a critically exposed set as critical points of the training dynamics (provided that we have sufficiently data). However, since we are unable to quantify this probability, they might be negligible and might vanish when dimension increases. I am not sure if we can use this notion to explain the so-called ``bias towards sparse subnetworks'' as in the paper."}, "questions": {"value": "1. In section 3.2, the link between optimization on the parameter space and on the neuromanifold is rather hand-waving. I wonder if there is a real relation between these two (under suitable conditions)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "GuNPRK7d6S", "forum": "L5jYWeycAx", "replyto": "L5jYWeycAx", "signatures": ["ICLR.cc/2026/Conference/Submission20337/Reviewer_jF6i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20337/Reviewer_jF6i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735812937, "cdate": 1761735812937, "tmdate": 1763044722651, "mdate": 1763044722651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the geometry of neural networks with polynomial activation functions through algebraic geometric tools. The authors claim three main contributions:\nIdentifiability results: For MLPs with generic polynomial activations, almost all functions have finitely many parameter representations; for CNNs, the parametrization is generically one-to-one.\nSingularity characterization: Sparse subnetworks constitute singular points of neuromanifolds for both MLPs and CNNs.\nCritical exposedness: Subnetworks of MLPs are \"critically exposed\" (contain critical points of the loss with positive probability), providing a geometric explanation for sparsity bias. CNNs do not exhibit this property."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Important theoretical questions: The paper addresses fundamental issues in deep learning theory - identifiability, singularities, and sparsity bias - that have significant implications for understanding optimization and generalization.\nNovel geometric perspective: Connecting sparsity bias to singularities of neuromanifolds is creative and could provide new insights into the lottery ticket hypothesis.\nArchitectural comparison: The distinction between MLP and CNN geometry regarding critical exposedness is novel and aligns with empirical observations about their different behaviors."}, "weaknesses": {"value": "The paper relies heavily on citations in a way that makes the intuition of the proofs difficult to follow . I would appreciate a more self contained mathematical exposition. I would be happy to improve my rating if more exposition was provided, as well as the following are addressed. \n\nIssues with specific proofs: \nTheorem 4.1 (MLP Identifiability)\n- The constraint β₁ > 6m² - 6m appears without much justification. Why this specific bound?\n- The dependence on this proof and a good bit of discussion in the paper on the Zariski topology necessitates a more thorough exposition/ explanation of this topological space, as it is a rather different topology than commonly used in Learning. \n\nTheorem 4.2 (MLP Singularities)\n- There should be more said about the dominance of σ ◦ fW′  . In particular it isnt clear that the image of σ ◦fW′ having a non-empty interior is sufficient to show Zariski density .\n\nTheorem 4.6 (CNN Singularities) \nMore said at all points."}, "questions": {"value": "Add concrete examples: Provide explicit small examples (e.g., 2-layer networks) where singularities and exposedness can be computed directly, if possible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j4sU5E1Vib", "forum": "L5jYWeycAx", "replyto": "L5jYWeycAx", "signatures": ["ICLR.cc/2026/Conference/Submission20337/Reviewer_WScj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20337/Reviewer_WScj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920734629, "cdate": 1761920734629, "tmdate": 1762933796184, "mdate": 1762933796184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the function spaces (“neuromanifolds”) of deep MLPs and CNNs with polynomial activations, using tools from algebraic geometry. The core results:\n\nIdentifiability. For MLPs with a generic sufficiently high-degree polynomial activation, the parametrization is generically finite-to-one; hence the dimension of the neuromanifold equals the number of parameters (Theorem 4.1). For CNNs, it is generically one-to-one and regular off the zero fiber (Theorem 4.4).\n\nSingularities. Subnetworks (deactivating neurons/filters) yield singular points: fully characterized for CNNs (Theorem 4.6) and partially for MLPs (Theorem 4.2).\n\nOptimization bias. The paper introduces “critically exposed” parameter sets. Strict subnetworks of MLPs are critically exposed for quadratic losses (Theorem 4.3), but not for CNNs (Proposition 4.5). This gives a geometric account of sparsity bias in MLPs."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Conceptual advancement. Provides a clean algebraic–geometric framework for identifiability and singularity in deep networks with polynomial activations, extending prior results beyond monomials.\n\nGenerality of MLP result. Finite identifiability for generic polynomials closes a dimension conjecture (dimension = #params) and generalizes known tanh/sigmoid cases to large-degree polynomials (Theorem 4.1).\n\nCNN result. The regularity and generic injectivity of CNN parametrizations off the zero fiber (Theorem 4.4) is technically strong and explains why CNN singularities are mild and do not create optimization equilibria.\n\nSparsity bias lens. The critical exposedness notion and proofs (Theorem 4.3 vs Prop. 4.5) give a principled account of why MLPs tend to collapse to sparse subnetworks whereas CNNs typically recover from near-zero initializations—matching known empirical phenomena."}, "weaknesses": {"value": "Activation assumptions: Many results require “generic” high-degree polynomials (often with σ(0)=0 and nonzero top coefficients). Practical popular activations (ReLU, GELU, tanh) are non-polynomial; while the authors argue approximation plausibility (Remark 4.1, §5), formal transfer to non-polynomial nets is not proven.\n\nOptimization link: While “critically exposed” is compelling and geometric, the paper doesn’t classify whether exposed subnetworks are local minima vs saddles, which matters for SGD dynamics and generalization."}, "questions": {"value": "1) Can you sketch the proof of  the limit argument suggested in Remark 4.1: under what conditions do MLP/CNN singularities persist under uniform polynomial approximation of non-polynomial activations?\n\n2) Are there settings where subnetwork critical points in MLPs are provably local minima with non-negligible measure?\n\n\n3) Can dimension counts for U_S (Eq. 8) yield measure bounds for how often subnetworks are equilibria?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5uzOMaIJpn", "forum": "L5jYWeycAx", "replyto": "L5jYWeycAx", "signatures": ["ICLR.cc/2026/Conference/Submission20337/Reviewer_ZhpW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20337/Reviewer_ZhpW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953762047, "cdate": 1761953762047, "tmdate": 1762933795713, "mdate": 1762933795713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the function spaces (neuromanifolds) parameterized by deep MLPs and CNNs with generic high-degree polynomial activations. It proves finite identifiability for MLPs (generic outputs come from finitely many parameters) and generic one-to-one identifiability for CNNs, hence the dimension equals the number of parameters in both settings. It then characterizes singular points of these neuromanifolds: (i) for MLPs, many subnetworks yield singularities and are shown to be critically exposed (they occur as critical points of squared-loss for a set of targets with nonempty interior); (ii) for CNNs, all singularities are precisely the subnetworks with edge zero-padding that satisfy an integrality constraint, and such sets are not critically exposed (away from the zero fiber). This geometric picture explains sparsity bias in MLPs but not in single-channel CNNs and resolves a long-standing dimension conjecture from prior work on polynomial networks. Figures 1--2 visualize how subnetworks create singular points and how MLP cuspidal-type vs. CNN nodal singularities differ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Originality: Moves from monomial/linear models to generic polynomial activations; formalizes critical exposure; delivers complete CNN singularity characterization. \n* Quality: Careful use of fiber-dimension, Vandermonde invertibility, and toric lattice ideals; clean separation between parametrization criticality and image singularity (Appendix A). Proof architecture is transparent via modular lemmas. \n* Clarity: Clear definitions of neuromanifolds/subnetworks; optimization setup with quadratic loss; intuitive figures (Fig. 1--2) and didactic examples (nodal vs. cuspidal curves in Fig. 3) to illustrate singular behaviors. \n* Significance: Fixes dimension = parameter count generically for both MLPs and CNNs; connects geometry to sparsity bias and to the presence/absence of spurious critical points across architectures."}, "weaknesses": {"value": "* Generality vs. practicality: Many results require very high polynomial degree and generic coefficients. Concrete degree thresholds are not explicit beyond \\(r \\gg 0\\) (depends on architecture). Giving quantitative bounds (even conservative ones) would improve applicability. \n* Model scope: CNN analysis is single-channel, 1D; multi-channel and higher-D details are asserted “similarly” but not proved. Because modern CNNs are multi-channel, a pathway or obstacles to generalization would be valuable. \n* Singularity coverage in MLPs: The paper shows many singularities arise from subnetworks but leaves open whether all singularities do (contrast with linear MLPs and with CNNs, where a full characterization is given). Clarifying non-subnetwork singularities would strengthen the picture. \n* Type of critical points: Criticality vs. local minima vs. saddles is not analyzed; given the optimization motivation (sparsity bias), even partial results or conjectures on stability types would be informative. \n* Beyond polynomial activations: While approximation arguments are discussed (Remark 4.1), formal extension to ReLU/Tanh/Softmax is left for future work; clarifying which parts port over under approximation limits would broaden impact."}, "questions": {"value": "Non-polynomial activations: Can the polynomial approximation idea in Remark 4.1 be made quantitative (e.g., stability of singularity types under uniform approximation on compact sets)? Which parts of identifiability/exposedness survive in the ReLU or tanh settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r5uQaRqOIk", "forum": "L5jYWeycAx", "replyto": "L5jYWeycAx", "signatures": ["ICLR.cc/2026/Conference/Submission20337/Reviewer_nwJE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20337/Reviewer_nwJE"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987691825, "cdate": 1761987691825, "tmdate": 1762933794878, "mdate": 1762933794878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Message to all Reviewers"}, "comment": {"value": "We thank all the reviewers for their comments. Following feedback, we have uploaded a new version of the manuscript, where we have improved the exposition, and incorporated new discussions. The new text is **highlighted in blue**. The main changes are: \n\n- **Polynomial approximation**: Several reviewers asked about the extension of our result beyond polynomials, which we had hinted towards in the original version. We have extended Remark 4.1 by discussing an informal sketch for an argument to extend our results to \"almost all\" smooth functions (in an appropriate sense, depending on the topology of the chosen function space) via polynomial approximation. However, we believe that a full formal argument would require plenty of details from functional analysis, probably resulting in a whole work on its own. \n- **Explicit bounds**: We have included explicit bounds for the degree of $\\sigma$ in Theorem 4.1 and Theorem 4.3. \n- **Type of critical points**: We have included a paragraph in Sec. 3.3 discussing the fact that our work focuses only on critical points, and motivating this from the perspective of stochastic dynamics. \n- **Exposition**: We have added a new section (Sec. 3.1) containing a friendly introduction to the Zariski topology and the notion of genericity. This section is referred to in several points throughout the paper, where the special properties of the Zariski topology are leveraged upon. \n- **Example**: We have added an example in the appendix (Sec. D) of a small network where we manually compute singularities and exposedness, corroborating the conjecture that subnetworks exhaust all the singularities. \n\nWe hope that these additions help to address the points raised in the reviews. Below, we reply individually to the reviewers, expanding on the above points."}}, "id": "q4sP1NoowU", "forum": "L5jYWeycAx", "replyto": "L5jYWeycAx", "signatures": ["ICLR.cc/2026/Conference/Submission20337/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20337/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission20337/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763285290554, "cdate": 1763285290554, "tmdate": 1763285290554, "mdate": 1763285290554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}