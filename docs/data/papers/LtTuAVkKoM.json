{"id": "LtTuAVkKoM", "number": 4502, "cdate": 1757690456749, "mdate": 1759898029502, "content": {"title": "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning", "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable success across diverse visual tasks, yet their performance degrades in complex visual environments. While existing enhancement approaches require additional training, rely on external segmentation tools, or operate at coarse-grained levels, they overlook the innate ability within VLMs. To bridge this gap, we investigate VLMs' attention patterns and discover that: (1) visual complexity strongly correlates with attention entropy, negatively impacting reasoning performance; (2) attention progressively refines from global scanning in shallow layers to focused convergence in deeper layers, with convergence degree determined by visual complexity. (3) Theoretically, we prove that the contrast of attention maps between general queries and task-specific queries enables the decomposition of visual signal into semantic signals and visual noise components. Building on these insights, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method that extracts task-relevant visual signals through attention contrasting at the pixel level. Extensive experiments demonstrate that CARVE consistently enhances performance, achieving up to 75% improvement on open-source models. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning with contrasting attention.", "tldr": "We propose Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method that extracts task-relevant visual signals through attention contrasting at the pixel level.", "keywords": ["Vision-Language Models", "Visual Reasoning", "Large Language Model", "LLM", "VLM", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7440cd60f5532cb4ca1ea76ec53c812b467d3a6e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an empirical analysis showing a positive correlation between visual complexity and attention entropy, and a negative correlation between attention entropy and visual reasoning accuracy. Building on these findings, it introduces a training-free method, CARVE, which contrasts attention maps from a “general instruction” and a “task-specific question,” normalizes them to obtain semantically relevant attention, and then performs pixel-level masking, cropping, and magnification to suppress visual noise and focus on task-relevant regions. The paper evaluates CARVE on Qwen2.5-VL and LLaVA models across several typical VQA benchmarks, reporting substantial gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper quantifies visual complexity via texture density and hue diversity, measures attention dispersion with Shannon entropy, and demonstrates a correlation chain linking complexity, attention entropy, and accuracy. A longitudinal inter-layer analysis further shows that deeper layers exhibit more concentrated attention but increased variance, reinforcing the method’s motivation.\n\n2. The approach requires only a single forward pass to extract two attention maps and take their contrast; no external segmenter or additional training is needed.\n\n3. Evidence spans models and datasets, includes ablations across layers and time steps, sensitivity studies on thresholds and the number of regions, and comparisons against SAM/YOLO/CLIP/ViCrop, which together provide a well-rounded empirical case."}, "weaknesses": {"value": "1. The paper instantiates complexity through two proxies: Canny edge detection and Hue distribution, but offers limited references and few independent validation studies. Moreover, in settings with comparable edge density and color statistics (e.g., Table VQA benchmarks), attributing failures to attention dispersion driven by these proxies remains debatable.\n\n2. In the exploration phase, the attention entropy H is computed on contrasted attention by default, yet the design choice is not thoroughly analysed. In the method, contrasted attention is further treated as an effective estimator of semantic localization to generate masks, but there is no alignment against external localization ground truth, nor a direct comparison with the native attention readily available from standard models.\n\n3. The use of contrasted attention assumes that attention under the general instruction primarily reflects task-agnostic visual factors like noise or background. The exploration section does not directly test or substantiate this premise."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "flHIGZGYTP", "forum": "LtTuAVkKoM", "replyto": "LtTuAVkKoM", "signatures": ["ICLR.cc/2026/Conference/Submission4502/Reviewer_EEGZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4502/Reviewer_EEGZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761651500220, "cdate": 1761651500220, "tmdate": 1762917407676, "mdate": 1762917407676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper defined a “visual complexity” concept including texture and color complexity, then explored how they affect Vision-Language Models' (VLMs) attention and discovered some phenomena. Building upon the findings, the authors proposed a training-free method called CARVE. Concretely, CARVE generated contrastive attention maps to mask visual noise, crop relevant regions, and magnify them for improved inference, thereby improving visual reasoning. Experimental results exhibited improvements across multiple VLMs and benchmarks, demonstrating the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper analyzed VLMs from perspectives of visual complexity and attention entropy, finding several intuitive phenomena. With the findings, the authors presented the CARVE method to enhance the visual reasoning capability of VLMs.\n\n2. The proposed CARVE is training-free, the empirical results demonstrated its effectiveness on different VLMs and test sets, and model scales range from 3B to 13B, making it feasible for various visual reasoning scenarios."}, "weaknesses": {"value": "1. Numerous previous studies have already investigated VLMs’ attention mechanism and its relationship with hallucination or visual reasoning, such as [Devils, CVPR 2025], [EAH, EMNLP 2025], [Farsight, CVPR 2025], [TAME, ICLR 2025], [FastV, ECCV 2024], [Clearsight, CVPR 2025], and [SEE WHAT YOU ARE TOLD, ICLR 2025]. More importantly, there have been long Chain-of-Thought reasoning VLMs since OpenAI o1 and DeepSeek-R1(-0528), such as VLM-R^3, LlamaV-o1, Vision-R1, and Visual-O1, which are all of strong visual reasoning capabilities on most of benchmarks. It is necessary for the authors to clarify the exclusive contribution of the proposed method, explain limitations of the prior works, and explicitly distinguish their proposed method from them.\n\n2. The “visual complexity” concept based solely on \"texture and color dimensions\" was poorly justified, there is no validation that these dimensions can capture relevant aspects, or discussion of why other visual aspects, such as object density, or lighting variations were ignored.\n\n3. The paper claimed to “prove” that contrastive attention scores enable decomposition of visual signals into semantic and noise components, but this claim lacked enough theoretical proofs. For example, the core theoretical claim in Definition 1 that attention decomposes as Eq. 4.1 lacked rigorous justification, and Appendix A.2 didn’t interpret that clearly. The authors assumed this decomposition exists, but didn't establish why the specific multiplicative decomposition should hold neither from theoretical proofs nor empirical validations. Apart from that, Eq. 4.3 and Eq. 4.4 are more like heuristic designs rather than theoretically grounded.\n\n4. The experimental evaluation of this work was limited. Results are primarily shown only for TextVQA dataset, with no comprehensive testing on standard benchmarks like GQA or ScienceQA. Also, the paper requires proper baselines to demonstrate the superiority of the proposed CRAVE, more comparisons need to be conducted, such as such as [Devils, CVPR 2025], [EAH, EMNLP 2025], [Farsight, CVPR 2025], [TAME, ICLR 2025], [FastV, ECCV 2024], [Clearsight, CVPR 2025], and [SEE WHAT YOU ARE TOLD, ICLR 2025]. \n\n5. The Abstract claimed “75% improvement” is misleading, even this paper adopted ratio metric to scale up the limited improvements, the results are up to 71.83% at the old Llava1.5-7B. Additionally, there was no statistical significance testing for empirical results, and no confidence or variance intervals, making the reported results random and unreliable."}, "questions": {"value": "1. Given the long Chain-of-Thought reasoning methods, such as OpenAI o1 and DeepSeek-R1(-0528) distilled LLMs and VLMs (e.g., LLaVA-o1, LlamaV-o1, VLM-R3), could the current short CoT models outperform these o1-like VLMs? If not, what is the meaningfulness of this research?\n\n2. Why could general queries capture noise while specific queries capture semantic information, is this assumption universally valid or theoretically grounded? And what if both general and specific queries produce similar attention patterns?\n\n3. All experiments were conducted with greedy decoding, how would the CRAVE perform under a common sampling setting?\n\n4. How could the proposed CARVE handle the circumstance where the correct reasoning demands integrating information from multiple disparate regions of an input image? Also, this work assumed general instructions induce uniform semantic signals, but does that always hold? Some images might naturally focus on certain regions regardless of instructions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D2FX53HFbh", "forum": "LtTuAVkKoM", "replyto": "LtTuAVkKoM", "signatures": ["ICLR.cc/2026/Conference/Submission4502/Reviewer_whRJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4502/Reviewer_whRJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705283068, "cdate": 1761705283068, "tmdate": 1762917407308, "mdate": 1762917407308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies why VLMs fail to “focus” in complex scenes and proposes a training-free enhancement, CARVE (Contrastive Attention Refinement for Visual Enhancement). Empirically, the authors show that (i) visual complexity (texture/color) correlates with higher attention entropy and lower accuracy; (ii) attention sharpens from shallow to deep layers; and (iii) contrasting attention from a general instruction vs. the task question yields a pixel-level mask that suppresses visual noise, improving VQA-style benchmarks across LLaVA-1.5 and Qwen2.5-VL models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A simple inference-time refinement method for VQA reasoning tasks that improves accuracy without additional training or heavy compute.\n2. Plug-and-play method applicable to transformer-based VLMs.\n3. Clear explanation of the problem."}, "weaknesses": {"value": "1. Potential degradation on reasoning tasks beyond VQA, especially those requiring global context.\n2. Limited evaluation scope: only four datasets were used for evaluation. Given that no training is involved, this is relatively small for VLM evaluation and for demonstrating the generalizability of the results.\n3. Relatively small gains given ~3× inference passes. The paper mentions early termination to reduce two of the three passes but the total overhead remains underexplored."}, "questions": {"value": "1. Following the weaknesses above, I am a bit concerned about the generalizability of the proposed method, beyond VQA reasoning tasks that rely on local context. Could you please elaborate on this? I would be interested in seeing negative/neutral cases where CARVE hurts or offers no benefit (e.g., wrong region kept; too small K; OCR-like questions).\n2. Related to the above question, could you please elaborate on how the model would perform on more general reasoning tasks, such as image captioning? In those cases, I would guess that the general and the question attention masks become very similar, which could be a failure point for the proposed approach.\n3. How much additional memory and compute does the method require at inference compared to the raw model without CARVE? According to the paper, the number of passes is tripled (once for the general instruction, once for the question, and once for the question with the masked image).\n4. What does “accuracy” exactly mean in Table 3, and how is it computed for models like SAM and YOLO?\n5. In Figure 5-b, both performance and entropy are plotted on a single y-axis; it is unclear what the y-axis represents and what the exact performance values are for each point. Could you please clarify Figure 5-b in more detail?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YMjxmj7iqA", "forum": "LtTuAVkKoM", "replyto": "LtTuAVkKoM", "signatures": ["ICLR.cc/2026/Conference/Submission4502/Reviewer_pgMe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4502/Reviewer_pgMe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905414028, "cdate": 1761905414028, "tmdate": 1762917405427, "mdate": 1762917405427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the degradation of VLMs performance in visually complex scenes by providing a quantitative analysis that establishes a clear link between visual complexity (defined by texture and color), increased attention entropy, and reduced model accuracy. Based on this finding, the paper proposes CARVE, a training-free method to improve VLM reasoning. The core idea is to contrast the attention maps generated from a task-specific question against those from a general, descriptive prompt. This contrast is designed to isolate task-relevant \"semantic signals\" from task-agnostic \"visual noise.\" Extensive experiments on several VLMs, demonstrate consistent performance improvements across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. While the phenomenon that \"complex visual information impairs performance\" is intuitive, the authors' contribution lies in rigorously and quantitatively analyzing this relationship. By defining concrete metrics for visual complexity and linking them empirically to attention entropy and downstream task performance (Figs. 4 & 5), the paper provides a solid, data-driven foundation for its claims. This analysis transforms an intuitive observation into a measurable scientific insight, which is a significant strength.\n\n2. Based on this finding, This paper demonstrates clear effectiveness as a training-free enhancement. The ability to improve performance on a strong, recent model like Qwen2.5-VL  is particularly impressive and suggests that even advanced VLMs suffer from attention dispersion and can benefit from this approach. The substantial performance gains observed on weaker models (e.g., up to 75% on LLaVA-1.5-13B) further underscore the utility of CARVE as a mechanism to compensate for inherent model limitations in focusing ability."}, "weaknesses": {"value": "1. A primary weakness of the proposed method is its sensitivity to hyperparameters, which is a significant concern for a training-free approach that aims for broad applicability. Figure 7 clearly shows that the performance of CARVE is highly dependent on the choice of the top-p threshold (p) and the maximum number of kept regions (K). For a method to be truly \"plug-and-play,\" it should be robust to such choices or provide a principled, automatic way to set them. The current need for manual tuning for optimal results somewhat undermines the convenience of its training-free nature.\n\n2. It is hoped that more models, especially more experiments on complex visual scene benchmarks, will be conducted to further enhance the effectiveness and persuasiveness of the method proposed in this paper.\n\nIf the robustness of the method and the performance gains on more benchmarks can be further demonstrated, I am willing to increase the score."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9r97pS7eKb", "forum": "LtTuAVkKoM", "replyto": "LtTuAVkKoM", "signatures": ["ICLR.cc/2026/Conference/Submission4502/Reviewer_fSTn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4502/Reviewer_fSTn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916358398, "cdate": 1761916358398, "tmdate": 1762917404836, "mdate": 1762917404836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CARVE, a training-free method that compares attention from a general instruction and with the task question, to create a pixel-level mask that removes visual noise and improves VLM reasoning focus on the core part. It shows that visual complexity increases attention entropy, and higher entropy reduces accuracy. They conduct experiments to show consistent gains using their attention entropy based training-free methods across models and datasets, with ablations on time steps, layers, and mask parameters."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. CARVE shows consistent improvements over the original models and external-tool baselines;\n2. The paper formalizes attention-entropy clearly, shows complexity -> entropy and entropy -> accuracy trends, and provides a closed-form contrasted attention used for masking.\n3. Abalation results are through across time steps and layers, plus sensitivity to top-p and region count K, with practical guidance. They also compare with SAM/YOLO/CLIP/ViCrop and gradient/attention variants."}, "weaknesses": {"value": "Refer to Questions section."}, "questions": {"value": "1. For SAM/YOLO/CLIP/ViCrop, were thresholds, confidence cutoffs, and post-processing tuned to near-optimal for each dataset & model  to ensure a fair comparison?\n2. The paper needs an error case analysis to clarify the limits of the method and guide usage. Does it still work for tasks with global-context or multi-object queries, or other VQA tasks involving more reasoning?\n3. Have you considered in-attention sharpening (no image masking) that directly sharpens the difference between question and general attention? For example: compute (A' <- (A^Q)/(A^G+\\lambda)) and use softmax(log A'/t) to renormalize or sharpen the cross-attention weights. It could work as a baseline very direct to the motivation.\n\nIf the authors answer my questions well and make these clarifications, I am happy to raise my review score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fe0598m4vt", "forum": "LtTuAVkKoM", "replyto": "LtTuAVkKoM", "signatures": ["ICLR.cc/2026/Conference/Submission4502/Reviewer_WNpL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4502/Reviewer_WNpL"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762705729404, "cdate": 1762705729404, "tmdate": 1762917404455, "mdate": 1762917404455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}