{"id": "FrMUrv2dWA", "number": 20192, "cdate": 1758303466539, "mdate": 1759896993125, "content": {"title": "Omni-Thinker: Scaling Multi-Task RL in LLMs with Hybrid Reward and Task Scheduling", "abstract": "The pursuit of general-purpose artificial intelligence depends on large language models (LLMs) that can handle both structured reasoning and open-ended generation. We present OMNI-THINKER, a unified reinforcement learning (RL) framework that scales LLMs across diverse tasks by combining hybrid rewards with backward-transfer–guided scheduling. Hybrid rewards integrate rule-based verifiable signals with preference-based evaluations from an LLM-as-a-Judge, enabling learning in both deterministic and subjective domains. Our scheduler orders tasks according to accuracy backward transfer (BWT), reducing forgetting and improving multi-task performance. Experiments across four domains show gains of $6.2\\%$ over joint training and $12.4\\%$ over model merging. Moreover, we demonstrate that simple assumptions on accuracy transfer yield accurate predictions of curriculum outcomes, with entropy dynamics explaining deviations due to generative tasks. These findings underscore the importance of BWT-aware scheduling and hybrid supervision for scaling RL-based post-training toward general-purpose LLMs.", "tldr": "We present Omni-Thinker, a unified multi-task RL framework that combines verifiable and generative rewards to train LLMs across diverse tasks, achieving strong overall performance through curriculum-guided optimization.", "keywords": ["Multi-Task Learning", "Large Language Model", "Reinforcement Learning", "Curriculum Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4919db864bdfeaddb929343777509363d5d76f9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Omni-Thinker, a unified reinforcement learning framework for training large language models across different task domains. The approach combines hybrid reward signals, integrating rule-based verifiable rewards with LLM-as-a-Judge preference evaluations, and employs a backward-transfer guided curriculum scheduler to order tasks and reduce catastrophic forgetting. The authors claim improvements over joint training and model merging, and propose that simple assumptions on accuracy transfer can predict curriculum outcomes, with entropy dynamics explaining deviations in generative tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The use of backward transfer matrices to guide curriculum ordering is principled and builds on established continual learning concepts.\n- The paper is well written and easy to follow."}, "weaknesses": {"value": "Please see my detailed questions and concerns below."}, "questions": {"value": "- Assumption 2 requires that seeing the full dataset once saturates task accuracy to the same level as training from initialization. How realistic is this for complex reasoning tasks? What happens when tasks require multiple epochs to converge?\n- Theorem 1 is stated without proof. A statement without proof should not be called a \"Theorem\" in mathematics.\n- For creative writing, you compare against a reference response from the dataset. Doesn't this bias the reward toward a specific style rather than encouraging diversity or creativity?\n- The paper claims that models \"learn to emulate lower or higher temperatures\" (lines 384-387). Are there any direct evidence for this mechanistic claim beyond just correlational observations?\n- Table 2 shows temperature ablations for only QA and Writing. Why not include Math and Coding?\n- The creative writing evaluation uses MT-Bench against GPT-4 from 2023. This is now quite outdated. How would results change against more recent models?\n- The proposed method requires computing the full BWT matrix upfront, which means training on all task pairs. For K tasks, it seems to require O(K^2) training runs. How does this scale computationally as K increases?\n- The paper focuses on post-training of already instruction-tuned models. Would your approach work for continued pretraining or for training from scratch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UMvfgfFKWi", "forum": "FrMUrv2dWA", "replyto": "FrMUrv2dWA", "signatures": ["ICLR.cc/2026/Conference/Submission20192/Reviewer_qhMJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20192/Reviewer_qhMJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552848924, "cdate": 1761552848924, "tmdate": 1762933697054, "mdate": 1762933697054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Omni-Thinker, a unified RL framework that scales LLMs across diverse tasks using hybrid rewards and backward-transfer-guided scheduling. The method integrates verifiable rewards for structured tasks with preference-based LLM-as-a-Judge evaluations for open-ended tasks. Experiments across four domains show average gains of 6.2% over joint training and 12.4% over model merging. The proposed method offers an effective solution for unified multi-task learning in both structured and open-ended domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This framework addresses the inconsistency in optimization direction across different tasks in the reinforcement learning process, integrating verifiable rule-based rewards and preference-based LLM evaluation into a unified reinforcement learning paradigm.\n\n2. The proposed BMT, by quantifying how learning a task influences the performance of previously learned tasks, provides a referable paradigm for the learning order in curriculum learning, mitigating the catastrophic forgetting problem to some extent.\n\n3. In experiments across four different domains, the proposed method demonstrates stable performance improvements, outperforming existing approaches to model merging and joint training."}, "weaknesses": {"value": "1. As mentioned in the article, the overhead of curriculum scheduling increases gradually with the increase in workload, and the scalability of the proposed method may be limited. Are there efficient strategies for real-world deployment?\n2. The paper presents results using Qwen2.5-7B as the base model for all experiments. Would the same backward-transfer-guided scheduling strategy remain optimal for significantly smaller or larger models?\n3. The overall framework, particularly the curriculum design and entropy analysis, appears somewhat heuristic and lacks tight integration with the core methodological contributions. To strengthen the contribution, could the insights from the entropy analysis be more formally integrated into the scheduling algorithm itself?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z5VTycK4j6", "forum": "FrMUrv2dWA", "replyto": "FrMUrv2dWA", "signatures": ["ICLR.cc/2026/Conference/Submission20192/Reviewer_Afu3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20192/Reviewer_Afu3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934609117, "cdate": 1761934609117, "tmdate": 1762933696561, "mdate": 1762933696561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies training curriculum for multi-task RL with both verifiable and preference-based tasks. It considers a multi-stage training setup, where the model is trained only on a single task at each stage and only enters the next stage after it finishes all training samples of this task. The proposed approach first measures the cross-task influence — the impact of training on each individual task on the performance of other tasks — and then determines an optimal task ordering by prioritizing the task that yields the highest average test accuracy across the remaining tasks. Experiments shows that this curriculum-based strategy outperforms no curriculum training, model merging, and SFT."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper proposes a simple framework for ordering tasks. The final ordering heuristics make intuitive sense."}, "weaknesses": {"value": "* This work relies on overly simplisitc assumptions (for both assumptions) and there are no sufficient evidence to justify them. Also see questions section.\n* Authors claim that the predicted accuracy using test set backward transfers are surprisingly precise, however, table 3 shows relatively low correlations between test and predicted accuracies."}, "questions": {"value": "If inter-task transfer effects are assumed to be constant (i.e., independent of starting ckpt), and we have task-wise saturation, then the cumulative effect of sequential training should be order-invariant. Could the authors explain this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fBdNwhF3vn", "forum": "FrMUrv2dWA", "replyto": "FrMUrv2dWA", "signatures": ["ICLR.cc/2026/Conference/Submission20192/Reviewer_XPqG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20192/Reviewer_XPqG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153897915, "cdate": 1762153897915, "tmdate": 1762933696010, "mdate": 1762933696010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}