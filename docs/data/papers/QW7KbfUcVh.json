{"id": "QW7KbfUcVh", "number": 5952, "cdate": 1757948368004, "mdate": 1759897942586, "content": {"title": "Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion", "abstract": "Machine unlearning in text-to-image diffusion models aims to remove targeted concepts while preserving overall utility. Prior diffusion unlearning methods typically rely on supervised weight edits or global penalties; reinforcement-learning (RL) approaches, while flexible, often optimize sparse end-of-trajectory rewards, yielding high-variance updates and weak credit assignment. We present a general RL framework for diffusion unlearning that treats denoising as a sequential decision process and introduces a timestep-aware critic with noisy-step rewards. Concretely, we train a CLIP-based reward predictor on noisy latents and use its per-step signal to compute advantage estimates for policy-gradient updates of the reverse diffusion kernel. Our algorithm is simple to implement, supports off-policy reuse, and plugs into standard text-to-image backbones. Across multiple concepts, the method achieves better or comparable forgetting to strong baselines while maintaining image quality and benign prompt fidelity; ablations show that (i) per-step critics and (ii) noisy-conditioned rewards are key to stability and effectiveness. We release code and evaluation scripts to facilitate reproducibility and future research on RL-based diffusion unlearning.", "tldr": "CGRU uses a timestep-aware critic to guide RL unlearning in diffusion models, achieving state-of-the-art forgetting (95.6% UA) on UnlearnCanvas", "keywords": ["reinforcement learning", "machine unlearning", "diffusion models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83dc0b900fea9f8c3aa8dbe0786004025f277977.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "CGRU is a machine unlearning method tailored for text-to-image diffusion models. Its core is to formulate the stepwise denoising process as a reinforcement learning sequential decision-making task. By introducing a timestep-aware critic, it predicts the terminal outcome and guides policy-gradient updates of the reverse diffusion kernel at each denoising step. The method achieves plug-and-play integration without modifying the core architecture of standard text-to-image backbones and supports off-policy reuse of historical training trajectories via importance weighting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Innovatively implements per-timestep criticism for individual diffusion steps, addressing the limitations of sparse end-of-trajectory rewards in prior RL-based diffusion methods.\n- Plug-and-play design: Requires no modifications to the core architecture of text-to-image models and can be directly embedded into existing frameworks such as Stable Diffusion."}, "weaknesses": {"value": "- Fails to evaluate concept erasure for celebrities, specific styles, or concepts fine-tuned via methods like DreamBooth.\n- Exhibits a significant trade-off between Unlearning Accuracy (UA) and In-domain Retain Accuracy (IRA), with inferior overall performance compared to methods like SalUn and no substantial effectiveness improvement.\n- Only validated on Stable Diffusion 1.5; its concept erasure performance on other state-of-the-art generative models remains unconfirmed."}, "questions": {"value": "As demonstrated in https://arxiv.org/abs/2503.10637, during the later timesteps of generation, the model can already generate the final result through single-step diffusion. In contrast, the earlier timesteps are dominated by meaningless noise. Why is it imperative to perform scoring and training at every individual generation step? What is the fundamental difference between this approach and directly generating the final step (x₀) from timestep t (as proposed in https://arxiv.org/abs/2304.05977) followed by terminal reward scoring?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LXt5l1VP8F", "forum": "QW7KbfUcVh", "replyto": "QW7KbfUcVh", "signatures": ["ICLR.cc/2026/Conference/Submission5952/Reviewer_kkaD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5952/Reviewer_kkaD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821515066, "cdate": 1761821515066, "tmdate": 1762918371459, "mdate": 1762918371459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Critic-Guided Reinforcement Unlearning CGRU, a timestep aware method for reinforcement unlearning in text-to-image diffusion models. ​The key idea is to interpret the reverse diffusion process as a policy and add a timestep aware critic trained to predict the final reward. Empirically, the method shows moderate improvements over DDPO on aesthetic-reward optimization and object unlearning, reporting 95.6% unlearning accuracy and 78% retention accuracy.​"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well motivated, coherent, with clean notations and algorithmic details. ​\n\n- The paper offers a new perspective on unlearning by reframing diffusion sampling as an actor-critic RL problem, and provides a formal connection between two active areas - diffusion alignment and machine unlearning - in a unified formalism.​\n\n- Introduction of a per-timestep critic for diffusion policy optimization is a clear algorithmic step forward, especially given the substantial instability and high variance of prior end-of-trajectory reward methods like DDPO. \n\n- The limited evaluation that is provided shows improved performance. In particular, Table 1 (Page 8) shows that CGRU achieves the top Unlearning Accuracy (UA = 95.55%) on the benchmark while remaining competitive in In-domain Retain Accuracy (IRA = 78.47%). \n\n- Fig. 1 visualization of the training dynamics is convincing, offering a direct performance comparison versus DDPO. CGRU displays consistently faster convergence and higher final rewards. Further, Fig. 2 illustrates clearly superior suppression of the “Cat” class compared to DDPO, with cleaner reward trajectory improvements.\n\n- Ablation and architectural choices are motivated and described, adding modular value for future work.\n\n- Detailed appendices and release of code and scripts promote reproducibility."}, "weaknesses": {"value": "- Scope of evaluations: The evaluation depth is limited. While the paper tests 20 object classes, it does not cover a single concept, keeping the scope narrow. The experiments focus only on one model (Stable Diffusion 1.5) and one dataset (UnlearnCanvas), concentrating on specific objects like \"Cats\" and \"Towers\" (Appendix D, Table 4). There are no results on more abstract or safety-critical tasks like style removal or identity erasure, which are mentioned as important reasons for unlearning in the introduction.  \n\n- Weak evidence: The paper lacks enough qualitative evidence to judge its generalization abilities. The only visual examples shown in Figure 3 are for the \"Cats\" class, making it unclear how the method performs on the other 19 concepts tested.  \n\n- Lower retention performance: The In-Domain Retain Accuracy (IRA) is significantly lower than in strong baselines, indicating that CGRU gives up a lot of utility for unlearning accuracy. According to Table 1, CGRU's IRA is 78.47%, while methods like SalUn and EDiff-UN achieve 96.35% and 94.03%, respectively. The authors do not discuss this critical trade-off enough; they only state in Section 7 that \"methods achieving high unlearning accuracy tend to exhibit lower retain accuracy\" without any detailed analysis or solutions."}, "questions": {"value": "- Can the method be demonstrated on a larger scope, as detailed in the weaknesses section?\n- Can the authors provide further qualitative evidence to judge generalization abilities?\n- Can the authors further discuss/analyze the tradeoff between retention and utility?\n- Can the critic be reused across different concepts, or must it be retrained per target?​"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZA5wBLqFVi", "forum": "QW7KbfUcVh", "replyto": "QW7KbfUcVh", "signatures": ["ICLR.cc/2026/Conference/Submission5952/Reviewer_xCq1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5952/Reviewer_xCq1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918588306, "cdate": 1761918588306, "tmdate": 1762918371029, "mdate": 1762918371029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Critic-Guided Reinforcement Unlearning (CGRU), a method for removing specific concepts from text-to-image diffusion models by training a aper-timestep critic that evaluates noisy intermediate latents to predict the final outcome. Further RL on top of it shows effective results on object removal which is superior to methods relying only on the sparse reward."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The timestep-aware critic addresses the high-variance problem of sparse rewards in prior RL-for-diffusion methods, leading to more stable training and better credit assignment.\n\n2. The method achieves state-of-the-art unlearning accuracy on object removing tasks."}, "weaknesses": {"value": "1. As the target of this paper is for machine unlearning, I didn’t see any specific designs for machine unlearning. The proposed critic seems to be the same as the value function in normal policy gradient for variance reduction techniques. As a result it is more like an RL for diffusion method applied to a specific domain.\n\n2. The value function is also used in methods like DPOK and the proposed method just seems to be more fine-grained such that it is also dependent on the timestep. But there’s no ablation study on whether the value function is dependent on the timestep. \n\n3. While unlearning accuracy is improved, the model's retain accuracy for related, benign concepts is middling, suggesting the method might be overly aggressive."}, "questions": {"value": "The machine unlearning accuracy is improved at a cost to retain accuracy. Is this an inherent trade-off in the framework, or could the reward function be designed to better preserve utility for non-target concepts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mB7k92Syww", "forum": "QW7KbfUcVh", "replyto": "QW7KbfUcVh", "signatures": ["ICLR.cc/2026/Conference/Submission5952/Reviewer_6uFk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5952/Reviewer_6uFk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996647289, "cdate": 1761996647289, "tmdate": 1762918369973, "mdate": 1762918369973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a value function baseline into policy gradient RL training of diffusion models. They do this by fine-tuning a CLIP-based value model that takes in noisy latents and predicts the achieved reward. This should help with reducing variance in RL training and providing better credit assignment.\n\nThe paper compares their new technique, dubbed CGRU, against an old baseline DDPO, showing better performance with consistently higher aesthetic scores and better unlearning performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "For many legal and safety reasons, managing proper unlearning techniques through diffusion models is important, and currently not in a perfect state. So exploring new techniques is significant for the progress of the field. \n\nI'm not very familiar with the most recent related work in this field, but it sounds like training value model baselines for diffusion model RL is novel (though a very straightforward application of a common RL technique).\n\nThe method is straightforward, makes sense, and is presented clearly. Additionally, the initial results look promising – CGRU seems to train more stably and with better end results than DDPO, and achieves a strong balance of unlearning to in-domain retention accuracy."}, "weaknesses": {"value": "Largely, more experimental results would contribute significantly to the point of the paper. \n\n- RL training is notoriously unstable, so having at least 3 seeds with error bars in figures 1 and 2 would make me more confident the performance improvement is not just luck.\n- Include more non-cherry-picked image grids of generated image examples for the different methods.\n(these two are my largest critiques, my score would likely rise if these were addressed)\n\n- In Table 1 I recommend running a few seeds and adding standard deviations. Furthermore I'd suggest making this a graph instead so it's clearer that there' s and IRA/UA tradeoff and your method is on the pareto frontier. It also seems slightly odd to clall CGRU's IRA \"competitive\" when it seem like a significant decrease compare to the other methods. Maybe this is an ok tradeoff, but I think it would be worth directly talking about how SalUn dominates on IRA and which you'd prefer just depends on where on the pareto frontier you want to be.\n\nThere are also some components that are somewhat lacking in clarity. It is unclear from the paper where specific results are taken from, why specific baselines were chosen, or exactly what experiments were run (more on these in the questions section). \n\nThe introduction mentions “ablations show that (i) per-step critics and (ii) noisy-conditioned rewards are key to stability and effectiveness,” but this \"stability\" argument is never mentioned again in the paper."}, "questions": {"value": "Why was DDPO chosen as the comparison example? Table 1 shows metrics for many other techniques but not for DDPO, why was DDPO the chosen comparison metric? What are the metrics for UA and IRA for DDPO? \n\nDid the model unlearn each of the 20 object classes? If so, why are only the results for cats shown? It would be useful to see performance on multiple different classes if all 20 were unlearned, or a chart in the appendix of a summary comparison metric for how well CGRU and DDPO unlearned each class. Within classes, more visual examples from the model generation during training would make the point stronger. \n\nSome questions regarding Table 1:\n“We evaluate CGRU’s performance using established metrics from the UnlearnCanvas benchmark Zhang et al. (2024b)”\nIn that paper, the results shown for UA differ for the models, and no metrics for IRA are shown. Some are similar, like ESD (92.15% in this paper, 91.42% in the other paper), but others are pretty different, like UCE (94.31% in this paper, 75.97% in the other paper). Where are these metrics coming from? Were these techniques used to retrain the model and recompute the metrics? If not, is it a fair comparison?\nThe cited paper for UnlearnCanvas also cites three other metrics, SC, OC, and UP. Are these useful metrics that are worth including?\n\nNot necessary to implement, but I wonder if you need to train a separate critic model per-dataset? Or could you train e.g. a generic aesthetics critic and apply it to all aesthetics finetuning jobs going forward?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qIwi3DFJpx", "forum": "QW7KbfUcVh", "replyto": "QW7KbfUcVh", "signatures": ["ICLR.cc/2026/Conference/Submission5952/Reviewer_NsKY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5952/Reviewer_NsKY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762189183699, "cdate": 1762189183699, "tmdate": 1762918369566, "mdate": 1762918369566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}