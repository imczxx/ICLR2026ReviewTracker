{"id": "Q8f4N71DEK", "number": 8684, "cdate": 1758094992990, "mdate": 1759897769771, "content": {"title": "Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling", "abstract": "Transformers have achieved remarkable success in time series modeling, yet their internal mechanisms remain opaque. This work demystifies the Transformer encoder by establishing its fundamental equivalence to a Graph Convolutional Network (GCN). We show that in the forward pass, the attention distribution matrix serves as a dynamic adjacency matrix, and its composition with subsequent transformations performs computations analogous to graph convolution. Moreover, we demonstrate that in the backward pass, the update dynamics of value and feed-forward projections mirror those of GCN parameters. Building on this unified theoretical reinterpretation, we propose **Fighter** (Flexible Graph Convolutional Transformer), a streamlined architecture that removes redundant linear projections and incorporates multi-hop graph aggregation. This perspective yields an explicit and interpretable representation of temporal dependencies across different scales, naturally expressed as graph edges. Experiments on standard forecasting benchmarks confirm that Fighter achieves competitive performance while providing clearer mechanistic interpretability of its predictions.", "tldr": "", "keywords": ["Graph Convolutional Nature of Transformers", "Interpretability", "Time Series Modeling"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4586aaa4cb5ff28c1e76c7541448ead11be7fb75.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper explores analogies between Transformers and GNNs in the context of time series forecasting, drawing parallels between the attention mechanism and the message-passing framework. However, the paper has very limited technical novelty, some claims are overstated, and the empirical analysis has significant issues."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* Understanding attention-based architectures in the context of time series forecasting is an important and timely research problem."}, "weaknesses": {"value": "* **Limited technical novelty.**  The technical novelty of the paper is very limited.\n    - The paper’s main premise , i.e., that Transformers can be seen as a specific implementation of a GNN operating on a fully connected graph with data-dependent edge weights, is an obvious and well-known fact, which has been known and discussed in the community for years (see, e.g., [1] and the popular 2020 blog post [2]).  \n    - The MLP layer used between attention blocks in Transformer architectures increases model capacity and transforms the representations extracted by the attention block. The paper argues that this transformation is redundant since it is not used in vanilla GNN architectures (see, e.g., lines 199–201). However, there is no logical connection between the fact that a layer is not used in vanilla GNNs and it being generally unnecessary.  \n    - Given that the information propagation mechanism is the same, the equivalence observed when analyzing derivatives (Section 4.2) appears obvious and does not provide any novel insight.  \n    - The paper claims that, inspired by the analogy between GNNs and Transformers, one can use powers of the learned adjacency matrix to propagate information. However, since the graph is fully connected and the weighting mechanism data-adaptive, it is unclear what advantage this offers over standard multi-head attention.\n\n* **Poor empirical evaluation.**  There are several issues in the empirical evaluation\n    - The proposed approach is evaluated only against outdated baselines, on a small number of datasets, and with no indication of variability (e.g., standard deviations across runs). Moreover, the paper does not clearly describe how baselines were trained and tuned. This, combined with the absence of released code, makes the results difficult to reproduce.  \n    - The reported performance gains appear unreasonable (e.g., nearly 100× on some datasets) given the minor architectural modifications introduced. Furthermore, based on the sensitivity analysis, one would expect Fighter with a single hop to perform similarly to a Transformer, as the architectures are essentially equivalent. However, the reported performance on Weather is ~100× better even for a single hop, compared to the Transformer results in the table - while the Transformer performance shown in the plot (red line) appears more in line with Fighter. What is happening here? Are you sure the results were reported on the correct scale?  In addition, forecast accuracy on the Electricity dataset seems to improve with longer forecasting horizons, which is quite implausible since the task becomes more difficult.\n\nEven disregarding the empirical issues, the very limited technical novelty alone would prevent me from recommending acceptance.\n\n\nReferences\n\n[1] Dwivedi et al., \"A Generalization of Transformer Networks to Graphs\", arxiv 2020  \n\n[2] Chaitanya Joshi, \"Transformers are Graph Neural Networks*\", 2020 blog post: https://graphdeeplearning.github.io/post/transformers-are-gnns/ -- (arxiv 2025)"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lyw8xwGquq", "forum": "Q8f4N71DEK", "replyto": "Q8f4N71DEK", "signatures": ["ICLR.cc/2026/Conference/Submission8684/Reviewer_RRhp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8684/Reviewer_RRhp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760987984196, "cdate": 1760987984196, "tmdate": 1762920496574, "mdate": 1762920496574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal in preparation – will post in the coming days"}, "comment": {"value": "Dear Area Chair and Reviewers,\n\nThank you for your valuable feedback. We are actively preparing a detailed rebuttal and will post it in the next few days.\n\nBest regards,\n\nThe Authors"}}, "id": "mL7b1UDFaV", "forum": "Q8f4N71DEK", "replyto": "Q8f4N71DEK", "signatures": ["ICLR.cc/2026/Conference/Submission8684/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8684/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8684/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763723481258, "cdate": 1763723481258, "tmdate": 1763723481258, "mdate": 1763723481258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work focuses on proving the equivalence between a Transformer encoder and a flexible graph convolution network. \nThe authors also propose a flexible graph convolution transformer called Fighter, which removes redundant linear projections to improve efficiency and claims to have higher expressivity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is written clearly, and theoretical results are presented briefly in the main paper with further details deferred to the appendix. I also like the illustrations to aid the readers."}, "weaknesses": {"value": "I find the paper interesting and well-written overall. However, the experimental results section could be strengthened. I have included several questions and suggestions in that regard. I'm happy to improve the rating if my questions are answered."}, "questions": {"value": "1. What is the difference in the attention matrices of the Transformer and the GCN? Could you please report the norm of the difference? Basically, Table 5 shown in numbers.\n2. In Table 1, why is there a huge drop in MSE/MAE in the forecasting tasks, but comparatively a minimal improvement in the classification accuracy?\n3. Can the authors please provide a table comparing the number of parameters of different baseline models with Fighter, and also their per-epoch training time?\n4. Can the authors please provide confidence intervals for all the numerical results?\n5. Commenting on the better performance of Fighter over longer sequences just by comparing two datasets is not wise. To make such a claim, the trend must be observed over a larger number of data points (in this case, datasets and prediction length).\n6. Fig 4(b) is not clear. Could the authors please zoom in or adjust the scale so that the order over different $\\kappa$ values can be observed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a2wr8fEmLg", "forum": "Q8f4N71DEK", "replyto": "Q8f4N71DEK", "signatures": ["ICLR.cc/2026/Conference/Submission8684/Reviewer_SazB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8684/Reviewer_SazB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752412958, "cdate": 1761752412958, "tmdate": 1762920496234, "mdate": 1762920496234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors focus on time series forecasting and introduce a connection between the transformer encoder and the graph convolutional network, by mapping the attention distribution matrix to a dynamic adjacency matrix, the layer transformations to graph convolution, and the updated attention values and projection coefficients to the GCN parameters. Finally, based on this analogy, the Flexible Graph Convolutional Transformer, so-called Fighter, is proposed, excluding unnecessary linear projections and considering multi-hop graph aggregation, representing multi-scale temporal dependencies by graph edges."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Originality:** The authors make an analogy between the graph convolution and the transformer encoders in time series forecasting, and mathematically prove the connection under specific circumstances, which is a novel approach bringing two hot modeling aspects in modern time series frameworks.\n2. **Quality and Clarity:** The methodological section (and additional details in the appendix) is easy to follow, despite being heavy in formulas.\n3. **Significance:** On the few showcased datasets, the forecasting performance of the proposed Fighter model significantly outperforms transformer-based baselines."}, "weaknesses": {"value": "- **W1-Significance (Motivation of Proposed Contribution):** The motivation behind the proposed idea is not clearly showcased. The authors aim to connect graph convolution representations with transformer attention for time series forecasting, yet it is not clearly explained why existing approaches are inadequate. In particular, several transformer variants addressing optimization issues in time series forecasting have already been proposed, but the paper does not clarify whether these methods are conceptually related to the proposed approach or whether the limitations they face are effectively addressed by the new method.\n- **W2-Clarity (Positioning against Related Works):** The authors do not discuss related work in graph representation learning for time series, where methods often distinguish between sparse [1,3] vs. fully-connected adjacencies [4] and static vs. dynamic dependencies [4,5]. The Introduction and Related Work sections should be updated to position the proposed Fighter model in the context of existing graph-based time series approaches. Importantly, the work seems conceptually similar to StemGNN [5], where the adjacency is learned based on a latent correlation attention layer on the whole sequence.\n- **W3-Quality and Significance (Experimental Evaluation):** The set of baselines considered is limited to older transformer-based approaches and does not include more recent improvements, alternative architectures, or graph-based methods (see references and TimeMixer, TimeXer, iTransformer, and PatchTST from https://github.com/thuml/Time-Series-Library). Additionally, the datasets used do not cover the full benchmark in the forecasting community (e.g., several standard datasets are not evaluated, and few horizon lengths are considered). The inclusion of the text classification dataset is unclear, as it is not standard in the time series community and does not directly relate to the forecasting task. If additional tasks are to be considered, they should be standard in the time series field, such as classification and anomaly detection, to ensure comparability with existing literature.\n- **W4-Clarity and Significance (Computational Complexity):** Although the authors refer to computational improvements enabled by their proposed method, they do not provide a computational analysis or experimental comparison for time cost and memory complexity compared to baselines or variants of the model. The fully-connected design of the adjacency, combined with message passing, is in general computationally expensive and should be compared to baselines to justify the claims for computational improvement.\nW5-Significance (Reproducibility): Although a substantial explanation of the experimental setup is given, the code implementation of the proposed method is not available in the submission; therefore, direct reproducibility of the experimental results is not possible.\n\n**References:**\n1. Shang C, Chen J, Bi J. Discrete graph structure learning for forecasting multiple time series. arXiv preprint arXiv:2101.06861. 2021 Jan 18.\n2. Bai L, Yao L, Li C, Wang X, Wang C. Adaptive graph convolutional recurrent network for traffic forecasting. Advances in neural information processing systems. 2020;33:17804-15.\n3. Wu Z, Pan S, Long G, Jiang J, Chang X, Zhang C. Connecting the dots: Multivariate time series forecasting with graph neural networks. InProceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining 2020 Aug 23 (pp. 753-763).\n4. Yi K, Zhang Q, Fan W, He H, Hu L, Wang P, An N, Cao L, Niu Z. FourierGNN: Rethinking multivariate time series forecasting from a pure graph perspective. Advances in neural information processing systems. 2023 Dec 15;36:69638-60.\n5. Cao D, Wang Y, Duan J, Zhang C, Zhu X, Huang C, Tong Y, Xu B, Bai J, Tong J, Zhang Q. Spectral temporal graph neural network for multivariate time-series forecasting. Advances in neural information processing systems. 2020;33:17766-78."}, "questions": {"value": "1. Can the authors showcase with examples/illustrations how the proposed design solves long-term problems in the time series community, e.g., correlations, dynamic dependencies, distribution shift, or other?\n2. Can the authors position clearly the proposed method against related works in temporal modeling and graph-based modeling, including justifications for the architectural choices with respect to common challenges (see Q1)?\n3. The experimental comparisons should be extended to more relevant baselines and datasets or more time series tasks to improve the significance of the contribution.\n4. What are the computational aspects of the proposed architecture and how do these compare to baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vueQ3b4eIU", "forum": "Q8f4N71DEK", "replyto": "Q8f4N71DEK", "signatures": ["ICLR.cc/2026/Conference/Submission8684/Reviewer_rpL3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8684/Reviewer_rpL3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924085444, "cdate": 1761924085444, "tmdate": 1762920495928, "mdate": 1762920495928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FIGHTER (Flexible Graph Convolutional Transformer), a novel framework that reinterprets Transformers as Graph Convolutional Networks (GCNs) for time series modeling. The authors provide a unified theoretical analysis showing that the Transformer’s attention distribution acts as a dynamic adjacency matrix, while its value and feed-forward updates mimic GCN feature propagation. Building on this equivalence, they propose FIGHTER, a simplified Transformer variant that removes redundant projections and introduces multi-hop graph aggregation, enabling explicit, interpretable temporal dependencies. Experiments on standard forecasting datasets (Electricity, Weather) and text classification (AG News) show that FIGHTER achieves state-of-the-art or superior results with dramatically lower errors and improved interpretability through graph-based attention visualizations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes an important conceptual contribution by rigorously connecting the Transformer and GCN formulations, offering both theoretical insight and architectural innovation. The idea of treating attention as a learnable adjacency matrix provides a fresh and unifying lens for understanding sequence modeling. The derivations are detailed and mathematically grounded, bridging the forward and backward pass analysis of both architectures. Empirically, FIGHTER achieves substantial performance gains and offers clear interpretability via graph-based attention visualization. The design is also elegant—removing redundant projections and adding multi-hop aggregation leads to improved efficiency and better long-range dependency capture."}, "weaknesses": {"value": "The experiments, though diverse, are somewhat limited in dataset variety and task complexity; \n\nAdditional real-world datasets or ablations on larger Transformer variants would strengthen the empirical foundation. \n\nMoreover, the mathematical exposition is dense and sometimes overly formal, which might obscure intuition for non-theoretical readers. \n\nThe practical computational trade-offs of the multi-hop attention (in memory and speed) are not deeply discussed. \n\nFinally, interpretability is primarily visual and qualitative; a more quantitative assessment of interpretive fidelity would improve credibility."}, "questions": {"value": "How does FIGHTER scale computationally with increasing hop parameter κ, especially in long sequences?\n\nCan the proposed equivalence framework generalize to multi-head or cross-attention Transformers (e.g., encoder-decoder setups)?\n\nAre there scenarios where the dynamic adjacency interpretation breaks down—for instance, in sparse attention or masked modeling settings?\n\nHow stable are the gradients and convergence behavior compared to standard Transformers during long training runs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VfacMR7Kch", "forum": "Q8f4N71DEK", "replyto": "Q8f4N71DEK", "signatures": ["ICLR.cc/2026/Conference/Submission8684/Reviewer_HwzJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8684/Reviewer_HwzJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030895383, "cdate": 1762030895383, "tmdate": 1762920495617, "mdate": 1762920495617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}