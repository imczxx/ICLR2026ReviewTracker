{"id": "avdPTUXdPG", "number": 19156, "cdate": 1758293945199, "mdate": 1763097423484, "content": {"title": "Dissecting Demystifying Region-Based Representations in MLLMs", "abstract": "Multimodal Large Language Models (MLLMs) typically process visual information as a flat sequence of image patch tokens, which is computationally expensive and lacks explicit semantic structure. This paper provides a systematic, vision-centric analysis of region-based representations, which group patches into semantically meaningful regions, as a more efficient and interpretable alternative. Our investigation is grounded in a key finding: MLLM performance is surprisingly robust to the input order of patch tokens, as the visual encoder already encode spatial information within the patches. This insight provides a foundational justification for reorganizing patches into semantically coherent regions. We further identify that the success of region-based methods depends on the quality of the visual features, particularly their smoothness and locality. We systematically evaluate how to enhance these properties through vision backbone selection, feature normalization, and hybrid partitioning strategies. Through comprehensive evaluations, we demonstrate that optimized region-based representations are a competitive alternative to patch-based ones, offering a compelling path towards more efficient, interpretable, and performant MLLMs.", "tldr": "Dissecting Demystifying Region-Based Representations in MLLMs", "keywords": ["Vision Language Models", "Multimodal Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/71ad0d1d643e7db671b7b2aaecee51049e5ea0de.pdf", "supplementary_material": "/attachment/ba4cfd074610f2dbe66db3d8901ef094f41de8f9.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on the effect of aggregating token-level visual representations into region-level visual representations in vision-language models on general multimodal tasks. Authors show how region-level representations improve the performance of MLLM across various multimodal tasks. The authors also conducted a comprehensive discussion and experiments on several related points: (1) how order non-sensitivity of MLLM guarantees that region-level representation won’t collapse the model, (2) how the quality of visual representations influences the effectiveness of region-level representation, (3) how normalization helps region-level representations, (4) how different approaches of forming the region and pooling features influence the effect of region-level representations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper transitions from region-based representations’ usage for vision-only tasks to multimodal tasks. The authors ask an interesting question related to “why region-based representations work on MLLM”.  They use an analysis of how the order of the visual token influences the performance of MLLM."}, "weaknesses": {"value": "1. The explanation about the drop in OCR performance needs to be further justified by experiments, e.g., if one creates the region for each independent character, will the performance of region-level representation improve the performance on OCR tasks?\n2. The lack of sufficient explanation or label for particular figures makes some of the conclusions less convincing: \n    - There is no explanation about the difference between “random order (trained)” and “random order (w/o training)” in Table 3. Does that mean the model is further fine-tuned after reordering the input tokens? Why are these two methods distinguished and compared only in patch-based RADIOv2.5, whereas for CLIP and region-based RADIOv2.5, there is only a single “random order” condition? \n    - Why not test the pre-shuffle condition on CLIP and region-based RADIO v2.5?\n    - Is there any explanation about why some of the entries are empty in Table 3?\n    - Figure 3 does not have any label indicating which image belongs to which model, making it hard to tell anything from the figure here.\n3. As discussed in the paper, agglomerative visual encoders could offer better visual representation. To make the conclusion more solid,  I would like to see more results from different agglomerative visual encoders and compare with the traditional visual encoder rather than only RADIOv2.5.\n4. The results from Table 4 are not sufficient enough to support the point that RMSNorm helps the region-based representation, as there is only an improvement on MMStar. Also, the format of the plot here is confusing, as there are three models for patch-based representation but only one model for region-based representation. Are these the results for RADIOv2.5? How does RMSNorm work on the region-based CLIP and SigLIP2?\n\nMinor:\n1. The location of the “G” letter in figure (b) is different from the “G” in ( c) and (d), also those three “G”s seem to have different luminance.\n2. Line 256, the link to the table is not working correctly (Table ??)\n3. Figure 6 in the appendix is not centered."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TGX4Vgu3uT", "forum": "avdPTUXdPG", "replyto": "avdPTUXdPG", "signatures": ["ICLR.cc/2026/Conference/Submission19156/Reviewer_8ZZf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19156/Reviewer_8ZZf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797031456, "cdate": 1761797031456, "tmdate": 1762931167406, "mdate": 1762931167406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Cm1cgeyjzt", "forum": "avdPTUXdPG", "replyto": "avdPTUXdPG", "signatures": ["ICLR.cc/2026/Conference/Submission19156/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19156/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763097422094, "cdate": 1763097422094, "tmdate": 1763097422094, "mdate": 1763097422094, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores region-based representations as an efficient and interpretable alternative to patch-based representations. It builds on the observation that MLLM performance remains robust to the input order of visual tokens, implying that spatial information is already embedded within patch features. This insight motivates reorganizing patches into semantically coherent regions. The paper also provides comprehensive experimental evaluation and in-depth analysis to support its findings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper offers a comprehensive and systematic analysis of region-based representations. It identifies feature smoothness as a key factor underlying their effectiveness and proposes concrete strategies to leverage this property. The visualization and attention analyses are clear and compelling, demonstrating how region-based methods produce more structured and interpretable attention maps while significantly reducing the number of tokens."}, "weaknesses": {"value": "While the paper provides valuable analysis, its methodological novelty is limited. The work primarily examines existing components—such as segmentation, clustering, and normalization—rather than introducing new architectures or learning mechanisms. The performance improvements from region-based representations are moderate, and the exploration of aggregation strategies remains incomplete. In particular, the proposed cross-attention-based aggregation yields marginal gains, indicating that a more sophisticated design may be required. Additionally, the study relies on frozen visual encoders, which restricts insight into how region-based representations might interact with end-to-end optimization or benefit from joint training."}, "questions": {"value": "1. Would unfreezing the visual encoder during fine-tuning enhance feature coherence and potentially reduce the reliance on post-hoc normalization?\n2. How sensitive are the results to the number and granularity of regions? Could an adaptive region selection mechanism based on image complexity further improve performance?\n3. Is it feasible to integrate the hybrid segmentation–clustering approach into the training process itself, rather than using it solely as a preprocessing step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "89esP1N32A", "forum": "avdPTUXdPG", "replyto": "avdPTUXdPG", "signatures": ["ICLR.cc/2026/Conference/Submission19156/Reviewer_Hmy2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19156/Reviewer_Hmy2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798387963, "cdate": 1761798387963, "tmdate": 1762931166879, "mdate": 1762931166879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper study the region-based visual representation for MLLM input instead of traditional patch-based representations that have high computational costs (quadratic token growth) and lack semantic structure\nThe paper find that MLLM performance is robust to patch token order, as visual encoders encode spatial info into patch features—providing a key basis for region reorganization. It identifies raw visual feature incoherence as the main challenge for region-based representations aggregation . To tackle this, it proposes strategies: using agglomerative backbones (e.g., RADIOv2.5), adding normalization (e.g., RMSNorm), and hybrid regions (SAM segmentation + DBSCAN clustering) .\nExperiments show optimized region-based MLLM match patch-based MLLM in performance, while cutting visual tokens for efficiency and boosting interpretability via focused attention ."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It is meaningful to investigate how to construct a region-based visual representation suitable for MLLM input—one that can facilitate LLM understanding while reducing training overhead.\n- The paper is highly accessible, featuring a well-organized structure that allows readers to easily grasp its core ideas.\n- Extensive experiments and visualizations provide solid support for the research findings. The paper conducts in-depth analyses and deduces/validates each finding through systematic reasoning."}, "weaknesses": {"value": "- The paper mainly discusses the region feature aggregation in the vision part. However, how the LLM attends to the patch features and region features remains under exploration.  \n- The paper proposes a simple way to obtain the region-based representation, which is a post-processing step of the patch vision features. Yet, it does not study how to learn region features (suitable for MLLMs) within the ViT.  \n- Lack of efficiency discussion: The paper proposes using SAM and clustering methods to extract region-based vision features, but it fails to analyze whether the use of SAM and clustering brings additional computational overhead compared to the patch-based method.  \n- Missing results in several experiments raise doubts about the correctness of the findings. While Table 1 evaluates 7 benchmarks, Tables 2, 3, and 4 only evaluate on 2 or 4 benchmarks, which may cause confusion for readers.\n- Table 3: Configurations E, F, and G are missing results for POPE, OCRBench, and CV-Bench.  \n- Figure 3 does not indicate which models the visualizations correspond to.  \n- Typos: Line 256 has a missing reference (marked as `Table ??`)."}, "questions": {"value": "- Visualization of the attention mask when altering the order of vision tokens is required. I am curious whether vision tokens in random order exhibit the same positional attention patterns as those in sequential order.  \n- Many papers on token reduction or token pruning indicate that dropping 75% of tokens even above only slightly impacts performance. Does this mean LLMs do not need to capture all vision tokens and only need to attend to a few vital vision tokens?\n- The paper obtains the region-based representation based on SAM mask and token merging. It should compare to other token merging and token pruning methods, like PyramidDrop, (CVPR25), SparseVLM (ICML25), FasterVLM (ICCV25), VisionZip(CVPR2025)\n- Each region might be an object or the background with the same concept. Does the region representation for one element correspond to a single token or a set of tokens? Additionally, does the position of regions in the input sequence matter for LLMs?  \n- As mentioned in the paper, DINOv2 exhibits better feature coherence due to its self-supervised learning (SSL) training. How about the performance of using the clustering results of DINOv2 features to aggregate CLIP or SigLIP features?  \n- In Table 5, what does `768x` denote? If it refers to resolution, why does this setting differ from those in other ablation experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gIYq8bfUVa", "forum": "avdPTUXdPG", "replyto": "avdPTUXdPG", "signatures": ["ICLR.cc/2026/Conference/Submission19156/Reviewer_Y2ak"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19156/Reviewer_Y2ak"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934799308, "cdate": 1761934799308, "tmdate": 1762931945074, "mdate": 1762931945074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to dissect the region-based representation in the mul-timodal large language models. The paper shows that region-based representations are robust to patch token order, and their eﬀectiveness depends on smooth, localized visual features. The proposed in-sights are straightforward and easy to follow, though somewhat lacking in depth. The experiments and visualizations are clearly designed to illustrate the ﬁndings, though they sometimes lack quantitative support or deeper analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The insights are straightforward and easy to follow."}, "weaknesses": {"value": "-  Intuitive but Shallow Conclusions: The biggest strength of this paper is also its main weakness. The conclusions are intuitive and easy to follow, but their usefulness for guiding practical applications or informing further theoretical analysis may be limited. They lack underlying theoretical explanations, which makes the paper feel more like a report of experimental observations rather than a thorough dissection.\n\n- Lack of Quantitative Experimental Support: Some of the reasoning would be more convincing if supported by quantitative metrics. For example, in Finding 3, the authors mention that feature non-smoothness poses a challenge for region-based methods. Intuitively, the authors should provide a quantitative analysis of feature non-smoothness and establish its relationship with performance. The absence of such quantitative analyses reduces the depth and the inspirational value of the paper."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PY4oNc5j0b", "forum": "avdPTUXdPG", "replyto": "avdPTUXdPG", "signatures": ["ICLR.cc/2026/Conference/Submission19156/Reviewer_xyPG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19156/Reviewer_xyPG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983001102, "cdate": 1761983001102, "tmdate": 1762931166157, "mdate": 1762931166157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}