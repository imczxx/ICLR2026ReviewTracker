{"id": "UVhAIvnmbz", "number": 18206, "cdate": 1758285135068, "mdate": 1759897119608, "content": {"title": "SpatioTemporal-GRPO: Post-Training Large Multimodal Models for Video QA", "abstract": "We introduce SpatioTemporal-GRPO (ST-GRPO), a novel extension of the GRPO algorithm for video question answering. ST-GRPO addresses a limitation of standard GRPO: when all responses in a group have similar correctness, the low reward variance gives the model an uninformative signal for improvement. Our method overcomes this by generating multiple spatiotemporal variants of a video to serve as complementary inputs. Unlike standard GRPO, which only groups textual responses, ST-GRPO forms groups across both textual and spatiotemporal variants. This increases reward variance within each group, providing a more informative signal for learning. To ensure these visual variations are meaningful, we propose an importance-based grouping strategy. This approach computes per-frame relevance scores using cross-modal embeddings, prioritizing frames that carry higher semantic weight relative to the question. This question-aware method ensures our spatiotemporal groups are informed by the relevant visual cues for each query. Our experiments demonstrate consistent improvements across six challenging video understanding benchmarks, including VideoMME, TempCompass, VideoMMMU, MMVU, VSI-Bench, and PerceptionTest, showing that incorporating structured visual diversity into reinforcement learning provides a more effective approach for learning from spatiotemporal cues in video question answering.", "tldr": "", "keywords": ["Video Question Answering", "Large Multimodal Models", "Post-Training"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f08af00877b1f7935b435b9b2ec41c89911f80b4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SpatioTemporal-GRPO (ST-GRPO), an extension of GRPO for video question answering (VQA) that incorporates spatiotemporal diversity into reinforcement learning. ST-GRPO generates multiple video variants and forms groups across textual and visual inputs to increase reward variance, addressing GRPO’s low-variance limitation. An importance-based grouping strategy selects semantically relevant frames guided by the question, ensuring meaningful visual diversity. Experiments across six challenging VQA benchmarks demonstrate consistent performance gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is original, proposing the GRPO extension to video-language domains with spatiotemporal variants. The methodology is well-motivated and technically sound, addressing a known limitation of low reward variance in GRPO. The importance-based grouping ensures semantically meaningful visual learning, enhancing model robustness. Experiments across multiple benchmarks show consistent improvements, demonstrating the approach’s practical significance and generality."}, "weaknesses": {"value": "The paper’s novelty is limited, as ST-GRPO primarily adds video data augmentation without fundamental methodological changes. Experiments are restricted to a single base model (Qwen2.5-VL-7B), raising questions about generality. Comparisons with other methods may be unfair, as performance gains could stem from visual diversity rather than the algorithm itself. The work lacks analysis on alternative baselines or ablations to substantiate the claimed contributions."}, "questions": {"value": "1. The idea is very limited; aside from adding video data augmentation, ST-GRPO does not differ fundamentally from standard GRPO.\n\n2. While many baseline methods are presented, ST-GRPO is only applied to Qwen2.5-VL-7B, making the relevance of other comparisons unclear.\n\n3. The authors do not analyze how their method performs with other base LLMs.\n\n4. The observed performance improvement mainly comes from increased data diversity (video resampling), making comparisons with GRPO potentially unfair. It is unclear whether adding noise rewards to standard GRPO would yield similar gains. Overall, the method does not present a particularly novel idea."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D3HIQMCPhJ", "forum": "UVhAIvnmbz", "replyto": "UVhAIvnmbz", "signatures": ["ICLR.cc/2026/Conference/Submission18206/Reviewer_uKih"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18206/Reviewer_uKih"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760966495794, "cdate": 1760966495794, "tmdate": 1762927951918, "mdate": 1762927951918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends GRPO for Video QA by proposing a spatiotemporal grouping strategy. The method, ST-GRPO, aims to solve the low reward variance problem in standard GRPO by creating groups across different spatiotemporal video variants, which is shown to stabilize training and improve performance on six VQA benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed ST-GRPO method achieves consistent, albeit modest, performance gains over the standard GRPO baseline across all six evaluated benchmarks.\n\n2. The method's effectiveness is validated across a wide range of six different video understanding benchmarks."}, "weaknesses": {"value": "1. The performance improvement from ST-GRPO is limited.\n\n2. The method's novelty is somewhat incremental, as it combines existing techniques like GRPO, video augmentation, and question-aware frame relevance scoring.\n\n3. The paper's core claims lack sufficient experimental support; for instance, the \"Importance-Based\" grouping performs almost identically to \"Stochastic\" grouping, questioning its specific contribution."}, "questions": {"value": "1. There appears to be a notational gap; how does the \"importance-based grouping\" strategy, which generates video variants, connect to the final GxM responses used in the objective function?\n\n2. The paper hypothesizes that \"importance-based\" sampling improves temporal reasoning, yet experimental results show it performs negligibly better than \"stochastic\" sampling, questioning its unique benefit.\n\n3. The ST-GRPO objective function does not seem to include any explicit reward for spatio-temporal correctness, so how does the model learn this, as suggested in the qualitative examples?\n\n4. The reward analysis only plots the reward standard deviation; could the authors also provide the mean reward curve over training steps to better understand the learning dynamics?\n\n5. Does the proposed spatio-temporal transformation (e.g., temporal cropping or importance sampling) risk omitting critical frames, leading to a mismatch where the video variant no longer contains the answer to the question?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VvESkgcGwv", "forum": "UVhAIvnmbz", "replyto": "UVhAIvnmbz", "signatures": ["ICLR.cc/2026/Conference/Submission18206/Reviewer_PXxo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18206/Reviewer_PXxo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761594550015, "cdate": 1761594550015, "tmdate": 1762927951294, "mdate": 1762927951294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key limitation of the GRPO post-training algorithm for Video QA: low reward variance (e.g., when all sampled responses are incorrect) provides an uninformative learning signal. To address this, the paper proposes SpatioTemporal-GRPO (ST-GRPO), which extends GRPO grouping to the visual domain. The method generates multiple spatiotemporal video variants—using data augmentation and a question-aware importance-based frame selection—to serve as complementary inputs. This \"dual grouping\" strategy (across both textual responses and visual variants) aims to increase reward variance and improve policy optimization. Experiments on six video benchmarks show improvements over standard GRPO and other RL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies a clear and valid limitation in applying GRPO to video QA: low reward variance when all responses are similarly correct or incorrect. The core idea of using input (spatiotemporal) variations, not just output (text) variations, to increase this variance is a well-motivated conceptual extension.\n2. The paper provides strong empirical validation for its central hypothesis. Figure 3 directly visualizes that ST-GRPO successfully maintains a significantly higher reward standard deviation throughout training compared to GRPO and DR.GRPO, which suffer from signal decay."}, "weaknesses": {"value": "1. Formatting Issues: The manuscript suffers from formatting errors. In Table 1, there is an unnecessary vertical line on the right border while the left border is missing, creating an unbalanced and unprofessional appearance. In addition, all table captions (Tables 1-5) are placed below the tables. The ICLR style guide explicitly mandates that table captions should be placed above the table.\n2. Baseline Discrepancy: The paper's primary results are built on a Qwen2.5-VL-7B baseline, which is reported in Table 1 as achieving 56.6 on VideoMME (wo sub). This result is lower than the 65.1 score reported in the official Qwen2.5-VL technical report. The authors should explain and address this discrepancy.\n3. Missing ablation on hyperparameter $K$: The best-performing \"Importance-Based Grouping\" method relies on partitioning the video into $K$ segments and selecting one frame from each. $K$ is randomly sampled from [2, 6] during training. This hyperparameter directly controls the number of frames fed to the model. However, the paper provides no ablation study to justify this range. It is unclear how performance is affected by different values or ranges of $K$ (e.g., a fixed $K=8$, or sampling from [4, 8])."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P5bDxVPhAv", "forum": "UVhAIvnmbz", "replyto": "UVhAIvnmbz", "signatures": ["ICLR.cc/2026/Conference/Submission18206/Reviewer_bWqw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18206/Reviewer_bWqw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922815425, "cdate": 1761922815425, "tmdate": 1762927950807, "mdate": 1762927950807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SpatioTemporal-GRPO (ST-GRPO), a new method for improving large multimodal models in video question answering. It addresses a key limitation of the standard GRPO algorithm: when a model's answers to a question are all similarly correct or incorrect, the low \"reward variance\" provides a poor signal for learning. ST-GRPO solves this by creating multiple spatiotemporal (ST) variations of the input video and grouping them alongside different text-based responses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "They explore the diverse video transformations for GRPO."}, "weaknesses": {"value": "**Lack of Motivation & Analysis**: \n\nThe paper would be strengthened by providing a clearer motivation for the approach (why it is necessary) and a more detailed analysis of its benefits (why it is effective).\n\n**Lack of Novelty**\n\nCould you clarify how the method of using input augmentation to increase reward variance differs from DeepVideo-R1? The approaches currently appear very similar. Even though the author provided different approaches to augment the video, it seems to lack novelty.\n\n**Outdated Baselines**\n\nSome baselines, such as LLaVA-OneVision, may be highly outdated.\n\n**Miss-computed average score**\n\nIn Table 1, please double-check the GRPO avg. score. I believe it should be 59.0.\n\n**Contribution Claim**\n\nThe claim in the introduction about this being the \"first extension of GRPO to video-language domains\" may need revision, as this does not appear to be the case.\n\n**Citation (DeepVideo-R1)**\n\nThere seems to be a reference mismatch or an incorrect author listed for DeepVideo-R1.\n\n**Code** \n\nProviding the code would be beneficial for reproducibility."}, "questions": {"value": "See the weakness part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hFIC1klD9V", "forum": "UVhAIvnmbz", "replyto": "UVhAIvnmbz", "signatures": ["ICLR.cc/2026/Conference/Submission18206/Reviewer_gfr6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18206/Reviewer_gfr6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996219998, "cdate": 1761996219998, "tmdate": 1762927950285, "mdate": 1762927950285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}