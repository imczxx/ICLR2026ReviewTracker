{"id": "r8hzDA3pUY", "number": 8182, "cdate": 1758072595876, "mdate": 1759897801399, "content": {"title": "$\\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning", "abstract": "Active reasoning requires large language models (LLMs) to interact with external sources and strategically gather information to solve problems. Central to this process is belief tracking: maintaining a coherent understanding of the problem state and the missing information toward the solution. However, due to limited reasoning capabilities, LLM-based agents often suffer from belief deviation: they struggle to correctly model beliefs, lose track of problem states, and fall into uninformative or repetitive actions. Once this happens, errors compound and reinforcement learning (RL) training fails to properly credit the crucial exploratory steps.  To address this issue, we propose to track the deviation of model beliefs and develop $\\mathbf{T^3}$, a simple yet effective method that detects excessive belief deviation and truncates trajectories during training to remove uninformative tails. By preserving credit for informative prefixes, $\\mathbf{T^3}$ systematically improves policy optimization. Across 5 challenging tasks, $\\mathbf{T^3}$ consistently enhances training stability, token efficiency, and final performance, achieving up to 30\\% gains while cutting rollout tokens by roughly 25\\%. These results highlight belief control as a key principle for developing robust and generalizable LLM-based active reasoners.", "tldr": "", "keywords": ["Large language models", "LLM reasoning", "Agentic multi-turn reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f7e46fc0c479d32662e5fe542ba61b7d1c6dca3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel method, T3 (Truncating Belief-Trapped Trajectories), to mitigate belief deviation in LLM-based agents during multi-turn active reasoning under reinforcement learning (RL). The core idea is to detect when an agent enters a “Belief Trap Region” (BTR) where further reasoning becomes uninformative and truncate the trajectory early to preserve credit assignment for the earlier informative steps. Theoretical analysis establishes the adverse effect of BTR on gradient estimation, and experiments on five tasks (from AR-Bench and Multi-Turn Puzzles) demonstrate significant gains in stability, efficiency, and performance across multiple RL methods (PPO, GRPO, GSPO)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novel Theoretical Insight: Identifies belief deviation and BTR as key bottlenecks in multi-turn RL for reasoning.\n\nSimple & Effective Method: T3 integrates seamlessly into existing RL algorithms with minimal changes.\n\nRigorous Theory: Theorems formally show how uninformative trajectory tails can invert gradients and harm exploration.\n\nImpressive Results: Up to 30% performance gain, and 34% reduction in token usage across various benchmarks.\n\nRobustness: T3 generalizes well to OOD settings, LLM scales (3B–14B), and architectures (Qwen, LLaMA, DeepSeek)."}, "weaknesses": {"value": "(W1) Truncation proxy design remains task-specific and heuristic :While the theoretical formulation of T3 is grounded in the notion of epistemic stalling (i.e., halted belief progress), the practical implementation relies on task-specific heuristic proxies such as:\nrepeated “unknown” feedbacks in Situation Puzzles,\ninvalid guesses outside the hypothesis set in Guess Numbers,\nsimilarity drops in Movie Recommendation.\nThese conditions work well for the selected benchmarks, but lack generality. In tasks with ill-defined or open-ended hypothesis spaces, such proxies may fail to accurately detect BTR (Belief Trap Region) entry, leading to either missed truncation or premature stopping.\nmy suggestion: The authors could explore more general-purpose or learnable truncation detectors, such as classifiers over internal LLM hidden states or CoT consistency metrics, to broaden T3’s applicability across domains\n (W2) Limited to sparse outcome-based reward setting:\nT3 is explicitly designed for environments where only the final step yields a non-zero reward. While this aligns with many current RLHF-style settings, it raises concerns regarding:tasks with dense or shaped intermediate rewards, environments where step-wise feedback is available or desirable..\nIn such scenarios, truncating trajectories could remove valid informative signals from later steps, inadvertently harming credit propagation rather than helping it. T3’s compatibility with these more general reward formulations remains unexplored. Suggestion: Discuss how T3 would behave in dense-reward or hybrid reward settings, or propose adaptations for preserving mid-trajectory feedback signals.\n(W4) Lack of analysis on false positive truncation and its impact.While the authors provide some ablations on hyperparameters (e.g., window size k), they do not explicitly model the cost of mis-triggered truncation or propose mitigation strategies.\nWhat if the truncation proxy fires prematurely (false positive)?\nCould this discard useful exploratory steps that would have led to reward?\nDoes it increase variance in gradient estimation during early-stage training?"}, "questions": {"value": "How does T3 behave under dense reward or shaped intermediate feedback?\nHave you considered extending T3 beyond outcome-only reward settings?\n\nCan the proxy condition for BTR be learned, rather than hand-crafted?\nFor instance, via classification over hidden states or CoT traces?\n\nWhat is the impact of false-positive truncation in early training stages?\nDoes it increase gradient variance or harm exploration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ttp1Px6bmk", "forum": "r8hzDA3pUY", "replyto": "r8hzDA3pUY", "signatures": ["ICLR.cc/2026/Conference/Submission8182/Reviewer_4y4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8182/Reviewer_4y4o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760473595820, "cdate": 1760473595820, "tmdate": 1762920141063, "mdate": 1762920141063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method to provide early stop during the RL if the hypothesis space did not shrink by a meaningful amount. The paper proposes $T^3$ (Truncating Belief-Trapped Trajectories), a mechanism that uses task-specific proxy signals to detect and halt trajectories entering the \"Belief Trap Region (BTR)\". Experiments across five diverse reasoning tasks demonstrate that $T^3$ consistently improves varied RL algorithms (PPO, GRPO, GSPO) in both final performance and token efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a strong theoretical grounding for why RL fails in long-horizon active reasoning, formally characterizing the BTR and proving how it inverts expected advantage in GAE, thereby corrupting gradient estimates.\n2. $T^3$ is a straightforward, drop-in enhancement for standard RL algorithms (PPO, GRPO, GSPO) that yields significant gains with small change.\n3. The authors include thorough analyses, including out-of-distribution generalization tests (where $T^3$ maintains robustness) and ablations of different proxy truncation conditions, confirming the importance of detecting BTR entry rather than indiscriminate truncation."}, "weaknesses": {"value": "1. Dependency on Task-Specific Proxies: While the theoretical $T^3$ condition is general, its practical implementation relies on manually designing task-specific proxies (e.g., repetitive queries for Situation Puzzles, failure to reduce hypothesis space for Guess Numbers)999999999. Finding these proxies for novel, less-structured tasks might be challenging."}, "questions": {"value": "1. In Figure 3, the first and fourth figures have different y-axis label than the second and third figures, why?\n2. If we want to extend this to more real world cases, how would $T^3$ fit in these use cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GpqueP9K2G", "forum": "r8hzDA3pUY", "replyto": "r8hzDA3pUY", "signatures": ["ICLR.cc/2026/Conference/Submission8182/Reviewer_DC2y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8182/Reviewer_DC2y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884719250, "cdate": 1761884719250, "tmdate": 1762920140685, "mdate": 1762920140685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper formalizes belief deviation in multi‑turn, active reasoning and defines a Belief‑Trap Region (BTR) where epistemic progress stalls. It proves that long, uninformative “tails” can flip the sign of early‑step advantages (advantage inversion), then proposes T³, an early‑truncation rule triggered by simple proxy tests of stalled progress in the reasoning trace. T³ is a drop‑in wrapper for PPO/GRPO/GSPO and shows consistent gains across five interactive tasks, smoother learning, and shorter responses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Crisp failure‑mode lens. Clear POMDP framing with a truth‑anchored potential Ψ and a precise BTR definition; theory links tail length to advantage inversion (Theorem 2) \n\nSimple, general mechanism. T³ uses observable stalling proxies (e.g., non‑shrinking hypothesis sets, repeated Unknown judgments) so it can integrate with standard RL without changing optimizers \n\nThe proposed method shows broad, consistent empirical gains. Improvements on CircuitDetection (CD), SituationPuzzles (SP), GuessNumbers (GN), PreferenceEstimation (PE), MovieRecommendation (MR) with 14/18 metric wins; OOD robustness; works across model sizes/types.\n\nStability & efficiency: Learning curves are smoother and responses shorter, implying fewer wasted tokens."}, "weaknesses": {"value": "Theory–practice gap. Assumptions (e.g., value calibration to truth probability; linear update‑error growth) are strong and not empirically validated against measured beliefs/values. \n\nPrivileged proxies. PE/MR use oracle preference similarity to trigger truncation, limiting deployability without ground truth. \n“Shorter is better” confound. Random truncation sometimes helps (Table 3), so more direct evidence of advantage inversion would strengthen the causal story. \n\nTuning guidance. Heuristics for window size/thresholds are task‑specific; no adaptive rule is provided. \nComparisons & scope. Limited real‑world tasks and reliance on an LLM judge in SP; baselines could be more apples‑to‑apples vs. open, strong models trained under similar budgets."}, "questions": {"value": "No‑oracle proxies: How would you instantiate T³ on PE/MR‑like tasks without access to the true preference vector (e.g., entropy/disagreement/self‑consistency signals)? \nDirect test of Theorem 2: Can you measure early‑token advantages with/without truncation and show the predicted sign flips as tail length grows? \n\nAdaptive T³: Any online procedure to target a truncation‑ratio band (or validation return) and auto‑tune window/thresholds? Sensitivity to the SP judge identity/temperature? \n\nActionable feedback\n\nAdd a one‑page algorithm box for T³ (inputs, trigger, where truncation enters GAE/PPO).\nProvide equal‑length ablations to disentangle truncation vs. better credit assignment; include γ/λ/KL sensitivity.\nAdd a small calibration check: value vs. correctness reliability plots. Report token & wall‑clock to fixed reward thresholds."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "plufwDcsF2", "forum": "r8hzDA3pUY", "replyto": "r8hzDA3pUY", "signatures": ["ICLR.cc/2026/Conference/Submission8182/Reviewer_K1QM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8182/Reviewer_K1QM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905701107, "cdate": 1761905701107, "tmdate": 1762920140253, "mdate": 1762920140253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets multi-turn active reasoning with LLM agents, where belief tracking often drifts, which produces redundant actions that poison RL credit assignment. It proposes T3 - early truncation of rollouts once belief-trap are detected via simple proxies (e.g., non-shrinking hypothesis sets, repeated “unknown” feedback, off-manifold guesses, or declining preference similarity). T3 is claimed to be a drop-in rule for PPO/GRPO/GSPO that preserves credit for informative prefixes and removes noisy tails. Across five tasks, it improves stability and OOD robustness, increasing performance gains with fewer rollout tokens."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "the $T^3$ framework provides a useful lens to connect information gain with progress in active reasoning (though its usefulness is a bit questionable, see more below), and the proxy-based T3 is simple to implement and, empirically, shows consistent gains across tasks/algorithms"}, "weaknesses": {"value": "- while the BTR lens motivates truncation, Def. 2’s T3 condition does not give actionable guidance on setting $ k, \\alpha, \\beta $ beyond \"detect stalling\". The task-specific proxies in Sec. 3.1 are essentially heuristic and could be understood without Section 2. In other words, the T3 condition does not concretely guide how to pick the metric $d(\\cdot,\\cdot)$, the window size $k$, or the threshold $\\Delta_{\\min}$, nor does it yield principled settings for the concrete proxies (\"unknown\" counts in SP, hypothesis-set shrinkage in CD/GN, similarity trends in PE/MR).There is no quantitative link between $ \\Delta \\Psi$ and $ d (H_\\tau, H_{\\tau +1} ) $; can the authors articulate decision rules (e.g., choose $k$ to control an estimated upper bound on false truncations via concentration of an estimator of $\\Delta \\Psi$)?\n\n- as to the two theorems established\n  - assumption 1 appears a bit strong: it posits systematic error growth with uncertainty across all actions/observations, yet the experiments do not validate (or estimate) $m_\\theta, c_0, U_0$.\n   - the extra constants/conditions ($\\eta$, $L_\\pi$-Lipschitz policy, “non-degenerate observations”) appear task/model-dependent and unverified; the bound $U$ seems not computable from observable quantities.\n  - theorem 2 motivates truncation but does not establish when actual policy gradients are meaningfully biased on the reported tasks.\n  - the sufficient condition shows the possibility of a sign flip, but does not quantify when it occurs for real critics/discounts. \n\n- as to training dynamics/stability: figures show improved stability in several cases, but GRPO+T3 still collapses later in SP. The rationale for pairing PPO (CD/PE), GRPO (SP), GSPO (GN) is not explained; differences in optimizer/entropy/clip may confound stability claims. If T3 is “algorithm-agnostic,” a matched comparison across all tasks/algorithms would strengthen its stabilization role.\n\n- as to OOD analysis consistency, CD OOD uses Qwen-14B (Table 2) while the main CD uses 7B; can the authors justify the switch? The narrative “too many references induce redundancy $\\Rightarrow$ more BTR” in PE is plausible but unsubstantiated; no proxy trajectory statistics (e.g., stall frequency, hypothesis-shrinkage rates) are shown to support the causal link.\n\n- authors need to be explicit about what default $\\alpha$ and $\\beta$, $k$ values are used for all 5 reasoning tasks. \n\n- regarding ratio of early-truncation vs. performance, the trends are confusing: authors report SP with $\\alpha=0.9$, where the truncation ratio converges to 1 and performance increases, whereas PE with $\\beta=0.8$ also $\\rightarrow 1$ but performance decreases. Similarly, for CD, higher ratios ($k=1,2$) appear to hurt exploration, unlike SP. Is there actually a consistent correlation between performance and the truncation ratio? Readers need the rationale and a principled conclusion, not case-by-case explanations. A controlled sweep reporting correlation between ratio and performance, conditioned on proxy type, would clarify when \"more truncation\" helps vs. harms.\n\n- in addition, regarding the summary claim in lines 418-420, can authors elaborate on this alignment: what theoretical quantities map to the chosen proxy thresholds and settings, and under what observable conditions (e.g., provable relation between $|H_t|$-shrinkage and $\\Psi$-decrease in CD/GN)?\n\n- some claims are casual: \"This suggests that distillation can effectively boost the belief-tracking capability under finite state spaces\"; that size/type differences “can be attributed to $m_\\theta$\" (lines 450-452) is speculative with current evidence. The first statement is based on one distilled LLaMA baseline on one task. It should be labelled as a hypothesis; broader study across tasks/models is needed.\n\n- $m_\\theta$ is introduced in Assumption 1 as a slope in the belief-update error lower bound, but later used to explain size/type effects qualitatively. can authors elaborate on it? How can we connect $m_\\theta$ to measurable quantities (e.g., proxy stall rates) and estimate it empirically?\n\n- minor presentation issues\n\n  - authors need to list default ($k, \\alpha, \\beta$) per task and how you tuned them\n\n  - define the “ratio of early truncation” in text (denominator, per-episode vs. per-step?)\n\n  - in Fig. 3a,d the y-axis should read “Response length”\n\n  - Table 1 should be placed near Sec. 3.3.1 analysis"}, "questions": {"value": "- is the ratio of early truncation ever defined/formalized in the text?\n- what are variables $b, a, o$ in fig 1?\n- How would section 2 choose ($k, \\alpha, \\beta$)? For instance, can we map $d(H_\\tau, H_{\\tau + 1})$ to an empirical estimator of $\\Delta \\Psi$ and pick $k$ so that the false-truncate probability is < δ (with a concentration bound)?\n\n- Why does GRPO+T3 still collapse later in SP? Is this due to proxy false-truncation starving exploration, or value-function drift? Any mitigation (pro\n\n- for binary similarity threshold 0.88 (PE): why 0.88?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qRBlDfEAHd", "forum": "r8hzDA3pUY", "replyto": "r8hzDA3pUY", "signatures": ["ICLR.cc/2026/Conference/Submission8182/Reviewer_nLrS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8182/Reviewer_nLrS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762093650867, "cdate": 1762093650867, "tmdate": 1762920139868, "mdate": 1762920139868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}