{"id": "9h5MCXYu3y", "number": 9059, "cdate": 1758108888966, "mdate": 1762932011664, "content": {"title": "Exploring Conditions for Diffusion models in Robotic Control", "abstract": "While pre-trained visual representations have significantly advanced imitation learning, they are often task-agnostic as they remain frozen during policy learning. In this work, we explore leveraging pre-trained text-to-image diffusion models to obtain task-adaptive visual representations for robotic control, without fine-tuning the model itself. However, we find that naively applying textual conditions—a successful strategy in other vision domains—yields minimal or even negative gains in control tasks. We attribute this to the domain gap between the diffusion model's training data and robotic control environments, leading us to argue for conditions that consider the specific, dynamic visual information required for control. To this end, we propose CoRoCo, which introduces learnable task prompts that adapt to the control environment and visual prompts that capture fine-grained, frame-specific details. Through facilitating task-adaptive representations with our newly devised conditions, our approach achieves state-of-the-art performance on various robotic control benchmarks, significantly surpassing prior methods.", "tldr": "", "keywords": ["diffusion models", "imitation learning", "foundation model", "robotic control"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/21975633ae963665da1ae458f1b7421e75dffe52.pdf", "supplementary_material": "/attachment/4d141204fc3c56a460cc5f78904ee6583fa55abc.zip"}, "replies": [{"content": {"summary": {"value": "CoRoCo leverages pre-trained text-to-image diffusion models to derive task-adaptive visual representations for robotic control without fine-tuning the backbone. Recognizing that naive textual conditioning underperforms due to domain gaps, CoRoCo introduces learnable task prompts and frame-specific visual prompts that capture dynamic, fine-grained control-relevant information, yielding state-of-the-art results across multiple robotic benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes a novel robot control strategy that uses learnable conditions to bridge the gap between vision pre-training and robotic environments. The motivation is sound, the method appears technically solid, and the experiments are fairly comprehensive. The paper is well written, the method is clearly presented, and the figures/tables are complete and easy to read."}, "weaknesses": {"value": "The proposed approach adapts only a learnable condition token while freezing the remaining modules. This design is attractive because it substantially lowers training cost and retains the strengths of the pre-trained backbone. However, a key concern is whether such limited adaptation can guarantee downstream effectiveness and generalization. The current evaluations are concentrated on simulated locomotion and Meta-World manipulation—domains that are short-horizon and comparatively insensitive to language and rich visual semantics (indeed, many Meta-World tasks can be solved with images alone, without language). Demonstrating results on more complex, language- and vision-sensitive scenarios (e.g., longer-horizon, multi-object, compositional instructions, o.o.d. visuals) would better substantiate the generality of the approach."}, "questions": {"value": "See above in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9wOTMblV3G", "forum": "9h5MCXYu3y", "replyto": "9h5MCXYu3y", "signatures": ["ICLR.cc/2026/Conference/Submission9059/Reviewer_bZ1v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9059/Reviewer_bZ1v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761492751505, "cdate": 1761492751505, "tmdate": 1762920768713, "mdate": 1762920768713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We appreciate the comments from the reviewers, and look forward to improve our paper based on the constructive feedback."}}, "id": "BnsYelAh00", "forum": "9h5MCXYu3y", "replyto": "9h5MCXYu3y", "signatures": ["ICLR.cc/2026/Conference/Submission9059/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9059/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762931907711, "cdate": 1762931907711, "tmdate": 1762931907711, "mdate": 1762931907711, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents CoRoCo, a diffusion-based framework that introduces learnable task and visual prompts to guide the generation of evolving actions for robotic control. After a dual-encoder architecture fuses textual and visual inputs into a shared SVD representation, a policy network maps the resulting visual features to the action space. CoRoCo is evaluated across multiple simulated environments against strong baselines, demonstrating superior success rates on various manipulation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Using learnable tasks and visual prompts to replace the original diffusion model text condition, which highlights various regions in detail.\n- The method builds upon an SVD backbone pre-trained on large-scale video datasets, while keeping the U-Net parameters frozen. This design preserves the model’s foundational generative ability but raises questions about the extent of adaptability to new domains.\n- Visualization of experimental results and verification including different simulation environments."}, "weaknesses": {"value": "- **Limited discussion with related work**  \n  The paper provides insufficient discussion and comparison with closely related approaches. For instance, *Vidman* [1] and *VPP* [2] similarly employ video diffusion models for single-step denoising as text–image encoders without freezing the visual representation. The primary difference between *CoRoCo* and these methods appears to be the modification of diffusion conditions through additional visual prompts.\n\n- **Inadequate ablation analysis**  \n  The ablation experiments are weak and do not clearly demonstrate the contribution of freezing the SVD backbone to action learning. Intuitively, directly fine-tuning the SVD U-Net could also address the task-agnostic issue. Additional ablation studies are required to validate that the proposed learnable visual prompt mechanism outperforms full-parameter fine-tuning or LoRA alternatives.\n\n- **Relatively weak experiment**  \n  The evaluation benchmarks (e.g., DMC and Metaworld) are not sufficiently challenging for visual–language–action tasks. For example, Metaworld tasks typically do not rely on textual inputs and can be solved effectively using visual information alone. Evaluations on more complex and realistic environments (e.g., *Robotwin* [3], *CALVIN* [4]) would strengthen the empirical claims.\nBesides，when integrating textual information into policy learning, comparisons with recent Vision-Language-Action (VLA) frameworks—such as *π₀* [5]—would provide a more convincing validation of the proposed method’s advantage.\n\n[1] Wen, Youpeng, et al. \"Vidman: Exploiting implicit dynamics from video diffusion model for effective robot manipulation.\" Advances in Neural Information Processing Systems 37 (2024): 41051-41075.\n\n[2] Hu, Yucheng, et al. \"Video prediction policy: A generalist robot policy with predictive visual representations.\" arXiv preprint arXiv:2412.14803 (2024).\n\n[3] Mu, Yao, et al. \"Robotwin: Dual-arm robot benchmark with generative digital twins (early version).\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.\n\n[4] Mees, Oier, et al. \"Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks.\" IEEE Robotics and Automation Letters 7.3 (2022): 7327-7334.\n\n[5] Black, Kevin, et al. \"$\\pi_0 $: A Vision-Language-Action Flow Model for General Robot Control.\" arXiv preprint arXiv:2410.24164 (2024)."}, "questions": {"value": "1. Is it better to freeze SVD and use learnable visual prompts than to fine-tune U-Net in SVD directly? Please provide some relevant experiments.\n2. Considering comparison with VPP and π₀, it is recommended to choose a benchmark that is more suitable for multimodal evaluation (such as Calvin)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Please see above weaknesses"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2Q6PEwwwew", "forum": "9h5MCXYu3y", "replyto": "9h5MCXYu3y", "signatures": ["ICLR.cc/2026/Conference/Submission9059/Reviewer_adaB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9059/Reviewer_adaB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558148809, "cdate": 1761558148809, "tmdate": 1762920768350, "mdate": 1762920768350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how to condition pre-trained text-to-image diffusion models so their internal features become task-adaptive visual representations for imitation/control, without fine-tuning the diffusion model itself. They show that off-the-shelf text prompts (captions) give inconsistent or negative gains in control because of grounding failures and the domain gap; they therefore propose CoRoCo: learnable task prompts (shared tokens) plus visual prompts (dense features from a frozen vision encoder) that together form the condition to a frozen Stable Diffusion U-Net; the U-Net’s intermediate features are passed to a policy and everything except the diffusion model (and mostly the vision encoder) is trained end-to-end with behavior cloning. Empirically CoRoCo improves across 12 tasks on DMC/MetaWorld/Adroit."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The presentation is clear and the paper presents a range of experimental results (visualization, ablations) to give the reader more insight."}, "weaknesses": {"value": "1) My main issue is that I’m not convinced that this work has potential for impact in the area/problem being studied. The method is very similar to SCR which is compared to in the results section but the paper does not attribute to it all of the design choices related to feature extraction that are taken directly from that work (even the analysis and ablations present some very similar results for eg. impact of textual conditioning etc) - so it is unclear what the real contribution of this work is.\n\n2) Architecture seems very arbitrary and the authors have not explained their choices. For eg. Why did they need dino to get visual conditioning tokens, shouldn’t the U-net itself have captured visual features (and only the task conditioning tokens should have been needed?)\n\n3) The diffusion model architecture has also changed a lot since stable diffusion and the authors should have tried using a DiT [1] based architecture.\n\n4) The experiments are on synthetic and saturated benchmarks where I‘m not sure it is very meaningful to show minor improvements. On Metaworld there is only 1 task (button press) where there is actually a meaningfully improvement. The approach proposed is just generally overkill on such simple tasks.\n\n5) The proposed approach has so many more parameters, making it an unfair comparison to baselines. To show the need of foundation model backbones, the authors should show results across diverse and hard tasks (e.g. tasks that require zero/few-shot generalisation) to justify the spend on compute.\n\n[1] Peebles, William, and Saining Xie. \"Scalable diffusion models with transformers.\" Proceedings of the IEEE/CVF international conference on computer vision. 2023."}, "questions": {"value": "* Why does the paper not report SOTA manipulation baselines (like R3M [1] and more that might have come afterwards).\n\n[1] Nair, Suraj, et al. \"R3m: A universal visual representation for robot manipulation.\" arXiv preprint arXiv:2203.12601 (2022)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qJKW7YHfP8", "forum": "9h5MCXYu3y", "replyto": "9h5MCXYu3y", "signatures": ["ICLR.cc/2026/Conference/Submission9059/Reviewer_n6on"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9059/Reviewer_n6on"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051693644, "cdate": 1762051693644, "tmdate": 1762920767843, "mdate": 1762920767843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}