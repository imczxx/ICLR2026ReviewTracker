{"id": "ph9Pq45KLN", "number": 19792, "cdate": 1758299384586, "mdate": 1759897019085, "content": {"title": "Same model, better performance: the impact of shuffling on DNA Language Models benchmarking", "abstract": "Seemingly minor implementation details can significantly compromise benchmark validity. We demonstrate this through BEND (Benchmarking DNA Language Models), where hardware-dependent hyperparameters -- number of data loading workers and buffer sizes -- create spurious performance variations of up to 4\\% for identical models.\nThe problem stems from inadequate data shuffling interacting with domain specific data characteristics. \nExperiments with three DNA language models (HyenaDNA, DNABERT-2, ResNet-LM) show these artifacts affect both absolute performance and relative model rankings.\nWe propose a simple solution: pre-shuffling data before storage eliminates hardware dependencies while maintaining efficiency. This work highlights how standard ML practices can interact unexpectedly with domain-specific data characteristics, with broader implications for benchmark design in specialized domains.", "tldr": "Seemingly minor implementation details can significantly compromise benchmark validity.", "keywords": ["Deep Learnining", "gLMs", "BEND", "benchmark", "WebDataset", "data shuffling"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6be3d9cc4c0f4f7a0bacde4212e82eb5e770d358.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper audits BEND, a benchmark for DNA language models, and argues that seemingly innocuous dataloader choices (number of workers, buffer size) leak into evaluation by weakening shuffling and amplifying genomic autocorrelation. Empirically, the authors show that: \n\n(i) fewer workers and small buffers reduce AUROC/MCC in several BEND tasks for the same model;\n\n(ii) a simple “pre‑shuffling” of training annotations (before writing WebDataset shards) removes the hardware–hyperparameter dependence and usually improves scores; \n\nand (iii) the model ranking on CpG methylation can flip after proper shuffling.The remedy is to shuffle annotations prior to storage so that within‑batch variety no longer depends on workers/buffer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The author demonstrate the phenomenon across different backbones, proving that the problem exists widely and is caused by the dataloader instead of model difference.\n\n2. The paper quantifies sequence overlap per task, explaining why CpG methylation is most sensitive. This explaination further explain the phenonemon.\n\n3. The pre‑shuffling restores within‑batch diversity irrespective of worker count and buffer size is simple enough to demonstrate a raise in the BEND benchmark."}, "weaknesses": {"value": "1. Insufficient statistical rigor. No seeds, variances, or statistical tests. Even training curves lack shading for variability. Given small absolute gains, this could be the possible reason for the nuiance difference of the models.\n\n2. Narrow novelty. The fix is a well‑known practice in general domain in learning thoery. Without a more systematic characterization (e.g., formal mixing analysis or a generic recipe for streaming datasets with spatial autocorrelation), the contribution feels more like a benchmark note than a full conference paper."}, "questions": {"value": "1. Variance and replicability. Report mean with std over 3 seeds for all entries in Tables 1–2 and the Fig. 5 task panel. How stable are the gains and the ranking changes?\n\n2. Broader applicability. Evaluate at least one non‑BEND genomics dataset (or a synthetic autocorrelated dataset) to support the claim that this is a general issue in specialized domains, not a BEND‑specific implementation artifact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X5rMxm8SSh", "forum": "ph9Pq45KLN", "replyto": "ph9Pq45KLN", "signatures": ["ICLR.cc/2026/Conference/Submission19792/Reviewer_6TxJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19792/Reviewer_6TxJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760643275613, "cdate": 1760643275613, "tmdate": 1762931642518, "mdate": 1762931642518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical implementation flaw in the BEND (Benchmarking DNA Language Models) framework, where hardware-dependent hyperparameters (number of data loading workers and buffer sizes) create spurious performance variations of up to 4% for identical models. The authors demonstrate that inadequate data shuffling, when combined with genomic data characteristics (high sequence overlap and spatial autocorrelation), leads to biased training dynamics. They propose pre-shuffling data before storage as a solution and validate this across three DNA language models (HyenaDNA, DNABERT-2, ResNet-LM)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper identifies a real implementation problem in an existing benchmark that could affect reproducibility and fair model comparison, which is valuable for the genomics ML community.\n2. The authors provide concrete evidence showing how hyperparameters affect performance across multiple models and tasks, with particularly compelling results on the CpG methylation task (4% improvement)"}, "weaknesses": {"value": "1. This work is essentially a bug report and fix for a specific benchmark implementation rather than a novel research contribution. While valuable for practitioners, it lacks the theoretical depth, methodological innovation, or generalizable insights expected of a venue like ICLR. The core finding that data should be properly shuffled is a well-established ML best practice, not a research finding.\n2. Only evaluates on BEND tasks; no exploration of whether similar issues exist in other genomic or biological sequence benchmarks\n3. The main body of the paper contains only 6 pages of actual content, which clearly falls short of the research depth and comprehensiveness expected for an ICLR proceedings paper."}, "questions": {"value": "1. Why not compare against simply using standard PyTorch DataLoaders with proper shuffling instead of WebDataset"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4izCcPqbrh", "forum": "ph9Pq45KLN", "replyto": "ph9Pq45KLN", "signatures": ["ICLR.cc/2026/Conference/Submission19792/Reviewer_TSpt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19792/Reviewer_TSpt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891619165, "cdate": 1761891619165, "tmdate": 1762931640511, "mdate": 1762931640511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript addresses the issue of high batch homogeneity that arises when running tasks from the BEND benchmark, stemming from the collision of specific technical choices in the Webdataset dataloader provided by the original authors and autocorrelation in DNA sequences stored in genomic order. This issue adversely affects model performance, and the authors convincingly demonstrate that introducing a simple shuffling step substantially improves results across multiple architectures. This finding is particularly noteworthy, specifically for the DNA modelling space, as such effects may not be immediately evident when working with DNA data, given its inherent complexity and high dimensionality."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe issue identified with the WebDataset dataloader is described in thorough technical detail and effectively connected to theory\n2.\tThe authors present a well-designed experiment with clear, reproducible results demonstrated across multiple models.\n3.\tThe observed performance gains are substantial enough to potentially affect existing benchmarks and the relative ranking of models, with one example provided by the authors."}, "weaknesses": {"value": "I do not want the authors to be discouraged, this is a clever piece of work that highlights an important issue, one that even prompted me to revisit aspects of my own work. Its relevance is clear. However, I do not believe the paper has yet reached the threshold for a main track conference publication although I do think they should contact the authors of the original BEND benchmark. As it stands, the manuscript focuses on a subset of one benchmark, which limits its scope. If extended to other benchmarks that do not employ WebDataset, I would expect a reasonably skilled ML practitioner to shuffle their data by default. In the case of the Webdataset I suspect everybody is using it in the same way If the authors believe this assumption does not hold, I would have appreciated further commentary or evidence addressing whether this is likely to be wider spread. I would also suggest to the authors to focus on enhancing clarity in their plots and captions (e.g. Figure 2 is hard to read, and I believe the caption talking about the top and bottom row is misleading), and also revisit some sections from their manuscript (for example non-coding variants are mentioned in 2.1 but I could not find results for it)"}, "questions": {"value": "1. Can the authors highlight other benchmarks or tasks where this might be an issue? I do suspect there will be other DNA modelling benchmarks where these issues may arise\n2. DNABERT-2, HyenaDNA and ResNet-LM involve substantially different modelling choices (convolutions vs attention, BPE tokenization vs single nucleotide). While the autocorrelation is probably affecting all types of models I suspect they are not all affected equally as the results suggest. An in-depth treatment of these issue can substantially increase the scope of the manuscript.\n3. Would be expect these issues to arise in a self-supervised setting/foundation model training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QTmmdYTlbX", "forum": "ph9Pq45KLN", "replyto": "ph9Pq45KLN", "signatures": ["ICLR.cc/2026/Conference/Submission19792/Reviewer_1AvC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19792/Reviewer_1AvC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918667335, "cdate": 1761918667335, "tmdate": 1762931639458, "mdate": 1762931639458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper highlights some issues with the data shuffling of the BEND benchmark due to the implementation of it. Briefly, the BEND dataset on WebDataset is stored in genomic order (per shard), and then loading and shuffling the data the dataset is not shuffled globally, leading to issues downstream due to autocorrelation between samples within shards (due to being from similar genomic regions). This paper highlights these issues, and presents a simple fix (shuffle the dataset before you store it)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper identifies a real tangible problem and poses a very simple fix for it. I'd sincerely like to thank the authors for identifying and correcting this. Issues with datasets exactly like this are known to cause problems, and even though the fix is quite simple, actually fixing issues like this are of great interest to our field."}, "weaknesses": {"value": "The impact of shuffling (or lack thereof) is a known problem in ML so the results are unsurprising. While identifying and fixing issues such like is of paramount importance, there is no real academic novelty and further I don't believe this is in scope for publication in ICLR."}, "questions": {"value": "N/A, I believe most of the salient points were presented clearly. If the authors can provide some compelling evidence that their work is actually in scope, I would be willing to revise this review."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dPyNlFSrB6", "forum": "ph9Pq45KLN", "replyto": "ph9Pq45KLN", "signatures": ["ICLR.cc/2026/Conference/Submission19792/Reviewer_c9hF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19792/Reviewer_c9hF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969292674, "cdate": 1761969292674, "tmdate": 1762931638637, "mdate": 1762931638637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}