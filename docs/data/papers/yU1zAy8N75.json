{"id": "yU1zAy8N75", "number": 465, "cdate": 1756741231646, "mdate": 1759898259385, "content": {"title": "Query Routing over Multimodal Knowledge Bases for Retrieval-Augmented Reasoning", "abstract": "Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in mitigating hallucinations in Multimodal Large Language Models (MLLMs) by incorporating external knowledge during generation. Existing MRAG methods typically adopt a static retrieval pipeline that fetches relevant information from multiple Knowledge Bases (KBs), followed by a refinement step. However, these approaches overlook the reasoning and planning capabilities of MLLMs to dynamically determine how to interact with different KBs during the reasoning process.\nTo address this limitation, we propose R1-Router, a novel MRAG framework that learns to decide ***when*** and ***where*** to retrieve knowledge based on the evolving reasoning state. Specifically, R1-Router can generate follow-up queries according to the current reasoning step, routing these intermediate queries to the most suitable KB, and integrating external knowledge into a coherent reasoning trajectory to answer the original query. Furthermore, we introduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored reinforcement learning algorithm that assigns step-specific rewards to optimize the reasoning behavior of MLLMs.\nExperimental results on various open-domain QA benchmarks across multiple modalities demonstrate that R1-Router outperforms baseline models by over 7\\%. Further analysis shows that R1-Router can adaptively and effectively leverage diverse KBs, reducing unnecessary retrievals and improving efficiency and accuracy.", "tldr": "", "keywords": ["multimodal retrieval-augmented generation; nlp"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d4844485a3fa50a96c0dfc5ce61b9fd5dfab27e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces R1-Router, a framework that performs step-by-step retrieval and routing over multimodal knowledge bases (including text, images, and tables). By applying Step-GRPO, it provides fine-grained rewards at each intermediate reasoning step. The authors claim that the method achieves an average improvement of around 7% over various RAG and routing baselines on multiple QA benchmarks, and further present analyses on routing preferences as well as comparisons of reasoning depth and computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper proposes a new perspective on multimodal RAG, introducing a step-by-step routing and retrieval framework that enhances reasoning over text, images, and tables.\n\n* The training objective is well designed — by integrating step-wise advantage estimation and two types of rewards (query/routing and answer) on top of GRPO, the method effectively mitigates sparse reward issues.\n\n* The empirical evaluation is comprehensive, covering multiple QA scenarios (Text / Visual / Table) and providing clear analyses of routing behavior evolution and computational cost curves, with an average performance gain of around 7% over strong baselines."}, "weaknesses": {"value": "* Novelty: The proposed method builds on an iterative plan–retrieve–reason paradigm and agent-based subproblem decomposition with multimodal routing, which have already been explored in prior work such as OmniSearch[1]. The core difference mainly lies in the step-wise reward and advantage normalization of Step-GRPO, but this appears more like an engineering refinement of GRPO (e.g., grouped normalization, format reward, step-level query/routing reward) rather than a substantial innovation with theoretical support.\n\n* Single evaluation metric: he experiments rely solely on F1-Recall (token-overlap) across all tasks, which does not fully capture the objectives of multimodal, tabular, or extraction-based QA. The lack of comparisons with more appropriate metrics, such as Rouge-L, Retrieval Hit Rate@k, or other task-specific measures.\n\n* Supervision for multiple valid routes: When multiple gold-standard routing paths exist for the same question, it is unclear how r_route handles supervision. Using a single label may be insufficient, and a set-based matching or multi-path reward strategy would be more appropriate."}, "questions": {"value": "Please refer to the weakness section. There are two more questions:\n\n- How do group size,  $\\epsilon$, and in Step-GRPO affect performance and stability? Have the authors examined whether reward collapse or variance explosion occurs under different settings?\n\n- How was the upper bound of $n \\leq 3$ reasoning steps determined? How does the model perform on datasets that require longer reasoning chains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "veuiTIpkQa", "forum": "yU1zAy8N75", "replyto": "yU1zAy8N75", "signatures": ["ICLR.cc/2026/Conference/Submission465/Reviewer_nSWT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission465/Reviewer_nSWT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761693235668, "cdate": 1761693235668, "tmdate": 1762915526084, "mdate": 1762915526084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes R1-Router, a framework for Multimodal Retrieval-Augmented Generation (MRAG) that dynamically decides when and where to retrieve external knowledge during reasoning. Unlike prior static or heuristic retrieval pipelines, R1-Router employs a reasoning-driven query routing mechanism, combined with a reinforcement learning algorithm called Stepwise Group Relative Policy Optimization (Step-GRPO). The introduction motivates this need by arguing that current MRAG systems treat retrieval as a one-shot process, neglecting dynamic reasoning and adaptive querying across heterogeneous multimodal knowledge bases (text, image, table)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Propose R1-Router and Step-GRPO, a reasoning-aware MRAG controller that adaptively queries multiple modalities.\n- Extensive experiments across six benchmarks demonstrate +7% improvement and robust generalization."}, "weaknesses": {"value": "- From the method perspective, Step-GRPO relies heavily on ground-truth reasoning trajectories that are constructed using large teacher models (specifically R1-Distill-Qwen-32B and Qwen2.5-VL-7B). While this provides a strong supervisory signal during training, it also introduces potential distillation bias. Since the teacher-generated trajectories already embed the reasoning style and retrieval preferences of these large models, the student (R1-Router) may simply imitate the teachers’ patterns rather than discovering genuinely new reasoning behaviors. This dependency limits the generality and autonomy of Step-GRPO.\n\n- In the method design, the Step-GRPO and R1-router introduce lots of hyperparameters, such as different reward designs. A thorough analysis or ablation of these parameters is missing. \n\n- Related work on advanced agentic search is missing."}, "questions": {"value": "Could retrieval stopping be learned instead of fixed n≤3?\n\nIs retrieval redundancy or cost considered in the reward function?\n\nDoes Step-GRPO scale to larger KBs or unseen modalities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tllgE5IpNf", "forum": "yU1zAy8N75", "replyto": "yU1zAy8N75", "signatures": ["ICLR.cc/2026/Conference/Submission465/Reviewer_BqxX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission465/Reviewer_BqxX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957481770, "cdate": 1761957481770, "tmdate": 1762915525965, "mdate": 1762915525965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes R1-Router, a Multimodal Retrieval-Augmented Generation (MRAG) framework enabling Large Multimodal Language Models (MLLMs) to dynamically decide when and where to retrieve knowledge during reasoning. The model introduces Stepwise Group Relative Policy Optimization (Step-GRPO), i.e., a reinforcement-learning variant assigning step-specific rewards to intermediate reasoning stages. Experiments across 6 QA datasets show promising performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clear. Dynamic query routing over heterogeneous multimodal KBs is necessary to break through the limit of language only.\n\n- Step-GRPO extends GRPO with fine-grained stepwise rewards for multi-stage reasoning.\n\n- Dataset selection is extensive, which covers text, vision, and tables.\n\n- The paper is well-organized with appendix details on retrievers and training configs."}, "weaknesses": {"value": "- Figure 1 is not very self-illustrative; it is not very easy to distinguish the main breaking novelty or uniqueness of the proposed method.\n\n- The theoretical contribution of the paper is not extensive. Step-GRPO is currently empirical; a convergence or variance analysis versus standard GRPO would add credibility and novelty to the paper.\n\n- Training cost is not very clearly shown in the paper, which may be a concern. \n\n- The design of Eq (9) can be extended, especially the relationship between 'ask' and 'route', the need for balancing, whether they are contributing or competing with each other."}, "questions": {"value": "Beyond weakness, there are also some questions below.\n\n- How sensitive is Step-GRPO to $\\alpha$ and $\\beta$?\n\n- What percentage of the generated reasoning trajectories are filtered out as incorrect during training (Appendix A.3)?\n\n- How would R1-Router perform if KB modalities contain noisy or partially missing entries (e.g., imperfect captions)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XxEyksOAmc", "forum": "yU1zAy8N75", "replyto": "yU1zAy8N75", "signatures": ["ICLR.cc/2026/Conference/Submission465/Reviewer_pcKT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission465/Reviewer_pcKT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978644610, "cdate": 1761978644610, "tmdate": 1762915525766, "mdate": 1762915525766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "I found the core contribution, the Step-GRPO algorithm, to be a significant strength and a clear leap beyond static RAG. One of my issue is the model's dependence on golden trajectories from a teacher, which limits it to imitation rather than true discovery. I also question the claim of improved efficiency, as the accuracy gains appear to come at a substantial cost in inference time. Finally, the reported performance for some of the baseline is drastically different from its original paper, which undermines my confidence in the evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The Step-GRPO algorithm is a major highlight. By designing fine-grained rewards for each decision point in the reasoning process (sub-question generation, KB routing, intermediate answer generation), it effectively addresses the reward sparsity and credit assignment challenges faced by traditional RL in long-sequence, multi-action tasks. The paper is clearly written and well-structured. The method decides not only \"what\" to retrieve but also \"when\" and \"from where,\" representing a significant leap beyond the traditional static RAG paradigm."}, "weaknesses": {"value": "1. Dependence on \"Golden Reasoning Trajectories\": The Training relies on \"golden reasoning trajectories\" generated by a more powerful teacher model. This means the model's performance ceiling might be constrained by the quality of this synthetic data. The model may be biased towards imitating the teacher's specific reasoning patterns rather than discovering novel, potentially superior, paths.\n2. As stated in the end of the abstract “…enhance both efficiency and accuracy”, improved accuracy can be seen in Sec. 5, however, the improved efficiency lacks detailed illustration. Why does “2s additional per-step latency align well with performance-computational-cost-trade-off” (Ling 410)? E.g., compared to CogPlanner, the F1-Recall increase around 25% while the inference time increases around 60%."}, "questions": {"value": "1. As reported in [OmniSearch paper](https://arxiv.org/pdf/2411.02937) Table 4, OmniSearch obtains 41.20 on Dyn-VQA. However, in this paper, OmniSearch obtains 18.94 (Table 1). What causes this significant gap? (Did I miss something?) If KB routing degrades the model, what is the performance of OmniSearch (or other baselines) w/o KB routing?\n2. Why does random routing with Step-GRPO obtain comparable performance (Table 2, 52.50 & 55.93)? Could you please elaborate on “random routing”? E.g., we may adopt Text Retrieval (selected by random) even if the router predicts to use Text-Image Retrieval?\n3. Figure 2c: how many steps do other vanilla baseline methods use (E.g., CogPlanner and OmniSearch)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JiojJNSrso", "forum": "yU1zAy8N75", "replyto": "yU1zAy8N75", "signatures": ["ICLR.cc/2026/Conference/Submission465/Reviewer_gM5A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission465/Reviewer_gM5A"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050430495, "cdate": 1762050430495, "tmdate": 1762915525611, "mdate": 1762915525611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}