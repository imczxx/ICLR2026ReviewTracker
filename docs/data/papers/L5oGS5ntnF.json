{"id": "L5oGS5ntnF", "number": 13446, "cdate": 1758218021176, "mdate": 1763058217372, "content": {"title": "Towards Explainable Bilingual Multimodal Misinformation Detection and Localization", "abstract": "The increasing realism of multimodal content has made misinformation more subtle and harder to detect, especially in news media where images are frequently paired with bilingual (e.g., Chinese-English) subtitles. Such content often includes localized image edits and cross-lingual inconsistencies that jointly distort meaning while remaining superficially plausible. We introduce BiMi, a bilingual multimodal framework that jointly performs region-level localization, cross-modal and cross-lingual consistency detection, and natural language explanation for misinformation analysis. To support generalization, BiMi integrates an online retrieval module that supplements model reasoning with up-to-date external context. We further release BiMiBench, a large-scale and comprehensive benchmark constructed by systematically editing real news images and subtitles, comprising 104,000 samples with realistic manipulations across visual and linguistic modalities. To enhance interpretability, we apply Group Relative Policy Optimization (GRPO) to improve explanation quality, marking the first use of GRPO in this domain. Extensive experiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in classification accuracy, +15.9 in localization accuracy, and +2.5 in explanation BERTScore, advancing state-of-the-art performance in realistic, bilingual misinformation detection. Code, models, and datasets will be released.", "tldr": "We propose BiMi,a bilingual framework for misinformation detection with localization and cross-lingual reasoning,and introduce BiMiBench,a 104K-sample benchmark built from real news with diverse multimodal manipulations.", "keywords": ["Misinformation", "Multimodal", "LLM", "GRPO"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/74266371da64ecc008ae0fb6eef2c48e49a8f309.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BiMi, a bilingual multimodal misinformation detection system capable of region-level localization, cross-modal and cross-lingual inconsistency detection, and explanation generation. It further constructs BiMiBench, a 104K benchmark featuring realistic manipulations over images and Chinese–English subtitles. The approach achieves state-of-the-art performance in classification, localization, and explanation metrics compared with strong VLM baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. BiMi achieves strong performance gains over competitive baselines on BiMiBench, demonstrating the effectiveness of the proposed framework. \n\n2. The benchmark is large-scale with fine-grained manipulation categories and bilingual subtitle inconsistencies, enabling challenging evaluation beyond prior datasets. \n\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. The manipulations are created by prompts and automatic translation, which may not fully reflect how misinformation appears in real news or social media, where edits and cross-language differences happen in much more diverse and unpredictable ways. \n\n2. The paper claims retrieval helps generalization, but the only evaluation they provide is on 100 real posts, where retrieval only helped in 9 cases. For the remaining 91%, retrieval either did not help or did nothing. So the evidence is too limited to prove that retrieval is a reliable part of the system.\n\n3. The paper uses GPT-4o generated pseudo-references to evaluate explanation quality, so it is unclear whether the explanations truly reflect the model’s own reasoning rather than just matching GPT-4o’s writing style."}, "questions": {"value": "1. In Table 3, some strong VLMs such as Qwen3-8B have much lower F1 than accuracy, so could the authors explain what types of misleading samples these models fail on the most.\n\n2. Also in Table 3, some baselines like Gemma3 perform poorly on BiMiBench (accuracy only 17.48), could the authors clarify whether the bilingual setting is the main reason for these failures. \n\n3. In Table 5, GRPO improves both accuracy and BERTScore, so could the authors run a small ablation to show which reward component (classification reward, localization reward, or format reward) is actually responsible for the improvements in localization and explanation performance.\n\n4. In Table 9, Chinese subtitles suffer larger drops from OCR errors compared to English subtitles, it would be helpful to provide more examples of what OCR mistakes mainly cause wrong predictions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1yXXx60I2h", "forum": "L5oGS5ntnF", "replyto": "L5oGS5ntnF", "signatures": ["ICLR.cc/2026/Conference/Submission13446/Reviewer_vgWD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13446/Reviewer_vgWD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495646485, "cdate": 1761495646485, "tmdate": 1762924069238, "mdate": 1762924069238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "6Bgd2xDxtb", "forum": "L5oGS5ntnF", "replyto": "L5oGS5ntnF", "signatures": ["ICLR.cc/2026/Conference/Submission13446/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13446/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763058216697, "cdate": 1763058216697, "tmdate": 1763058216697, "mdate": 1763058216697, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BIMI (Bimodal Misinformation Interpreter), an explainable multimodal misinformation detection framework designed to improve both accuracy and interpretability of reasoning in multimodal misinformation tasks. The model consists of three main stages: Multimodal Alignment Pretraining, Explanation-Guided Fine-Tuning and GRPO-Based Reasoning Optimization. The authors evaluate BIMI on several benchmarks, including MMFakeBench, NewsCLIPpings, and their in-house dataset ExplainFake, showing that the model improves both accuracy and explanation faithfulness compared to baseline multimodal misinformation detectors and LMMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an underexplored problem—how to make multimodal misinformation detection not only accurate but also explainable.\n2. The adoption of GRPO for refining reasoning is novel and well-motivated. By optimizing explanation quality through a learned reward model, the approach introduces a meaningful alternative to conventional supervised or PPO-based fine-tuning."}, "weaknesses": {"value": "1.\tInsufficient dataset statistics and analysis. The paper does not report clear dataset statistics—such as the ratio of real vs. fake samples, textual length distribution, image diversity (scene/object categories), or modality correlation scores. These statistics are critical for understanding the coverage and bias of the dataset used for both pretraining and fine-tuning.\n2.\tUnclear source and validation of “Explanation” annotations. The paper states that explanations are used as supervision during Stage 2, but it does not specify whether these explanations are human-written, GPT-generated, or mixed, nor how their factual correctness was verified. If they are model-generated, some level of human curation or filtering should be reported.\n3.\tLack of comparison with proprietary models. In Table R3, the paper compares BIMI primarily against open-source baselines. However, it omits comparison with proprietary LMMs such as GPT-4o, which are now standard models in multimodal reasoning. Without showing whether BIMI achieves competitive results against such models, it is difficult to assess its real-world performance or cost-effectiveness.\n4.\tLimited details about the reward model in GRPO optimization. The “Stage 3” section only briefly describes the GRPO procedure but lacks essential information: What data was used to train the reward model (human-scored explanations? automatically scored outputs?)"}, "questions": {"value": "5.\tInterpretability evaluation remains partly subjective. While the paper mentions human evaluation of explanations, it lacks a rigorous description of the evaluation protocol (e.g., number of annotators, criteria such as factuality vs. coherence, scoring rubric)."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Kmi6ho7OJ", "forum": "L5oGS5ntnF", "replyto": "L5oGS5ntnF", "signatures": ["ICLR.cc/2026/Conference/Submission13446/Reviewer_dCnv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13446/Reviewer_dCnv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574896104, "cdate": 1761574896104, "tmdate": 1762924068847, "mdate": 1762924068847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BiMi, a novel framework for bilingual multimodal misinformation detection with an emphasis on explainability. The model is designed to detect manipulated content in images overlaid with Chinese-English bilingual subtitle. The authors propose a three-stage post-training pipeline built upon the GEMMA-3 vision-language model: domain alignment, supervised fine-tuning (SFT), and a custom GRPO-based reinforcement learning phase that optimizes a unified reward combining formatting correctness, classification accuracy, and localization IoU. The framework is evaluated on two benchmarks, demonstrating strong performance against state-of-the-art models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The focus on bilingual (Chinese-English) subtitles reflects real-world content on major platforms such as Bilibili and YouTube, addressing a gap in existing multimodal misinformation detection research.\n\n2. The proposed three-stage training pipeline (domain alignment → SFT → GRPO) is methodologically sound and effectively integrates multiple objectives into a unified framework.\n\n3. Unlike many models that only classify misinformation, BiMi explicitly generates interpretable, step-by-step natural language explanations, enhancing trust and usability for human users."}, "weaknesses": {"value": "1 The bilingual (Chinese–English) subtitles in the dataset are exact translations of each other, with no divergent or culturally nuanced content across languages. This raises questions about the necessity of the bilingual setting, as similar functionality could be achieved by translating existing monolingual datasets. Consequently, the contribution of constructing a new dataset appears incremental.\n\n2 Insufficient Evaluation of Generalization: The model’s performance is evaluated only on in-domain, Chinese–English data, without testing on other language pairs or out-of-distribution (OOD) benchmarks. This limits claims about its cross-lingual applicability or robustness to unseen domains, leaving the generalization capability largely unverified.\n\n3 Although BERTScore is used, the explanation quality analysis lacks rigorous human evaluation or automated metrics specifically designed for factual consistency and logical coherence in explanations."}, "questions": {"value": "1 How does BiMiBench contribute beyond existing datasets, given its synthetic and aligned bilingual text?\n\n2 Has BiMi been tested on other language pairs or out-of-distribution data to validate cross-lingual generalization?\n\n3 Did the authors conduct any human evaluation to assess the faithfulness, coherence, or interpretability of the generated explanations?\n\n4 Is the bilingual input actually leveraged for reasoning, or is one language sufficient? Is there evidence of cross-lingual benefit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bWdhRye2vZ", "forum": "L5oGS5ntnF", "replyto": "L5oGS5ntnF", "signatures": ["ICLR.cc/2026/Conference/Submission13446/Reviewer_q93M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13446/Reviewer_q93M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912103484, "cdate": 1761912103484, "tmdate": 1762924068589, "mdate": 1762924068589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BiMi, a bilingual multimodal framework for misinformation detection and localization, providing natural language explanations. Detecting misinformation becomes more complex with the combination of images and bilingual subtitles. BiMi also integrates an online retrieval module to enhance reasoning and releases BiMiBench, a dataset with 104,000 real news samples. Extensive experiments show that BiMi outperforms existing methods in classification accuracy, localization, and explanation quality."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Innovative Multimodal Framework: The introduction of the BiMi framework effectively combines bilingual and multimodal aspects, allowing for more nuanced analysis of misinformation that involves both images and subtitles in different languages.\n\nNatural Language Explanations: The ability to generate natural language explanations makes the model’s decision-making process more transparent and interpretable, enhancing user trust and understanding of the model’s outputs.\n\nComprehensive Dataset: The release of BiMiBench, with 104,000 systematically edited samples, provides a substantial resource for future research in the field, facilitating the development and evaluation of new methods in misinformation detection.\n\n\nStrong Experimental Results: The paper presents extensive experiments demonstrating that BiMi significantly outperforms traditional methods in terms of classification accuracy, localization accuracy, and explanation quality, highlighting the effectiveness of the proposed approach."}, "weaknesses": {"value": "Dependence on Data Quality: The performance of the BiMi framework heavily relies on the quality and accuracy of the input data (i.e., images and subtitles). In cases of low-quality or misleading inputs, the system's effectiveness may diminish.\n\n\nInterpretability Beyond Explanations: While the model provides natural language explanations, the underlying decision-making process and how different modalities interact might still lack transparency, making deeper interpretability a challenge."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dc9gnPhplH", "forum": "L5oGS5ntnF", "replyto": "L5oGS5ntnF", "signatures": ["ICLR.cc/2026/Conference/Submission13446/Reviewer_byRV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13446/Reviewer_byRV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996405492, "cdate": 1761996405492, "tmdate": 1762924067871, "mdate": 1762924067871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}