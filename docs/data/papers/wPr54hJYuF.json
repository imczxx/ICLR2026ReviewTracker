{"id": "wPr54hJYuF", "number": 24786, "cdate": 1758360356051, "mdate": 1759896748752, "content": {"title": "MA-EgoQA: Question Answering over Egocentric Videos  from Multiple Embodied Agents", "abstract": "As embodied models become powerful, humans will collaborate with multiple embodied AI agents at their workplace or home in the future. To ensure better communication between human users and the multi-agent system, it is crucial to interpret incoming information from agents in parallel and refer to the appropriate context for each query. Existing challenges are to effectively compress and communicate high volumes of individual sensory inputs in the form of video and to correctly aggregate multiple egocentric videos to construct system-level memory. In this work, we first formally define a novel problem of understanding multiple long-horizon egocentric videos simultaneously collected from embodied agents. To facilitate research in this direction, we introduce MultiAgent-EgoQA (MA-EgoQA), a benchmark designed to systemically evaluate existing models in our scenario. MA-EgoQA provides 1.7k questions unique to multiple egocentric streams, spanning five categories: social interaction, task coordination, theory-of-mind, temporal reasoning, and environmental interaction. We further propose a simple baseline model for MA-EgoQA named EgoMAS, which leverages shared memory across embodied agents and agent-wise dynamic retrieval. Through comprehensive evaluation across diverse baselines and EgoMAS on MA-EgoQA, we find that current approaches are unable to effectively handle multiple egocentric streams, highlighting the need for future advances in this direction.", "tldr": "", "keywords": ["Egocentric Video Understanding", "Multi-Agent System"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/efaf08bb93fdee7943bddae368a8443ed2739c87.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MA-EgoQA, a benchmark dataset for question answering over long-duration, multi-agent egocentric videos. The benchmark is built on the EgoLife dataset, which features 266 hours of video from 6 agents interacting in a shared house over 7 days. The authors' core contribution is a set of 1.7k question-answer pairs specifically designed to be answerable only by aggregating information from multiple agents' video streams. The benchmark spans five challenging, multi-agent categories: Social Interaction, Task Coordination, Theory-of-Mind (ToM), Temporal Reasoning, and Environmental Interaction.\n\nThe paper uses a \"single-agent filtering\" step to remove any questions solvable by one agent's perspective alone. The authors also propose EgoMAS, a training-free baseline model that uses an event-based shared memory and agent-wise dynamic retrieval.\n\nExperimental results, testing over 16 baselines (including SOTA LLMs and Video LLMs), show that MA-EgoQA is challenging. The top model, Gemini-2.5-flash, achieves only 36.93% accuracy (vs. 20% random chance), and all models struggle with the Theory-of-Mind category."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a new and interesting topic of multi-agent egocentric videos, where video streams are captured continuously during operation.\n\n- The data is verified and selected by human annotators."}, "weaknesses": {"value": "- Not enough qualitative analysis. Can the author show and analyze more benchmark samples? One sample is not enough to help audience understand the scope and quality of the benchmark data.\n\n- The performance gain from adding video frames in the EgoMAS (Text+Video) variant over the EgoMAS (Text) variant is very modest (35.96% vs. 35.55%). This suggests either that the text is sufficient for most questions or that the model's method of incorporating video (sampling 8 frames) is not sophisticated enough to be impactful."}, "questions": {"value": "- More qualitative analysis \n\n- How do the VLM models handle long videos in the evaluation of Table 2? \n\n- What is the distribution of video lengths in the benchmark? How does this influence the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BoC54EjeA1", "forum": "wPr54hJYuF", "replyto": "wPr54hJYuF", "signatures": ["ICLR.cc/2026/Conference/Submission24786/Reviewer_oPM5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24786/Reviewer_oPM5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893350640, "cdate": 1761893350640, "tmdate": 1762943196903, "mdate": 1762943196903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel MA-EgoQA benchmark designed for multi-agent egocentric video question answering. It aims to evaluate collaborative understanding and reasoning among multiple embodied agents. Each question in the dataset is designed to depend on multiple agents’visual observations, temporal relations, and mental states. The authors further propose a training-free multi-agent framework with shared memory and dynamic retrieval to perform system-level reasoning over multi-agent video data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. MA-EgoQA is the first benchmark to target multi-agent collaborative reasoning in egocentric settings, which extends beyond single-agent VQA and VideoQA tasks.\n2. The dataset integrates multiple synchronized first-person videos and corresponding QA pairs that require cross-agent temporal and social reasoning.\n3. It provides a unified evaluation platform for multi-agent perception, memory sharing, and cross-view reasoning, a meaningful direction for embodied AI research.\n4. The proposed EgoMAS baseline introduces a reasonable training-free design based on shared memory and agent-wise retrieval, showing competitive performance with larger commercial models."}, "weaknesses": {"value": "1. Current experiments mainly compare different models’overall performance but do not disentangle the sources of task difficulty (e.g., long-horizon temporal reasoning, multi-agent information fusion, or multi-modal dependency).\n2. Although the paper claims that ToM questions rely on dialogue and semantic context, it provides no quantitative evidence showing the impact of transcribed speech versus visual-only inputs.\n3. There is no sensitivity analysis on the number of agents used. It remains unclear how performance changes when reasoning over fewer or more viewpoints, leaving the true impact of multi-agent data unverified.\n4. The shared-memory and dynamic-retrieval modules are not compared with simpler baselines (e.g., uniform sampling, or event-based summarization), which weakens the argument for their necessity.\n5. The design of ToM questions likely depends heavily on ASR transcripts, given the absence of raw audio or gaze data. The extent of this dependency is not analyzed, raising uncertainty about whether these tasks truly reflect visuo-cognitive reasoning or are predominantly text-driven.\n6. The following datasets are missing from the background or comparisons:\n\n[R1] Mm-ego: Towards building egocentric multimodal LLMs, ICLR, 2025.\n\n[R2] Egotextvqa: Towards egocentric scene-text aware video question answering, CVPR, 2025.\n\n[R3] Assistq: Affordance-centric question-driven task completion for egocentric assistant, ECCV, 2022."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2TPfT4eATE", "forum": "wPr54hJYuF", "replyto": "wPr54hJYuF", "signatures": ["ICLR.cc/2026/Conference/Submission24786/Reviewer_8MQJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24786/Reviewer_8MQJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902359859, "cdate": 1761902359859, "tmdate": 1762943196655, "mdate": 1762943196655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **MA-EgoQA**, a multi-agent egocentric VideoQA benchmark designed to advance research on multi-agent collaboration and human–robot communication. The dataset is built upon **EgoLife**, featuring six actors (agents) and seven days of continuous life-log recordings. The question–answer pairs are automatically generated to cover six aspects: social interaction, task coordination, theory of mind, temporal reasoning, and environmental interaction. To address this task, the authors propose the **EgoMAS** framework, which incorporates a shared memory module and a system-to-individual retrieval mechanism. Experimental results demonstrate that EgoMAS consistently outperforms all baseline methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.\tMA-EgoQA: The first multi-agent egocentric VideoQA benchmark covering multiple multi-agent relevant QA tasks.\n2.\tEgoMAS: a simple but reasonable solution framework that achieves superior performance than strong baselines. \n3.\tComprehensive baseline analyses, with well-structured presentation."}, "weaknesses": {"value": "1.\tThe biggest concern in my side is that the questions seem to favor text-based information for answering, as indicated in Table 2. For example, EgoMAS, which utilizes both text and video inputs, only achieves a 0.4% improvement over its text-only counterpart. I also observe that the QA pairs are generated from captions and transcripts. Such text sources inherently lack fine-grained visual details and 3D spatial cues that are critical for embodied agents to understand and navigate indoor environments. I recommend the authors discuss this limitation in the paper.\n\n2.\tExcept for its egocentric aspect, both multi-agent theory-of-mind [1], social interaction and temporal reasoning [2], environmental interaction [3] are featured in previous datasets. There is a lack of related discussion and comparison in the paper—what are the differences (new challenges) of such questions in MA-EgoQA and in previous datasets?\n\n[1] Shi H, Ye S, Fang X, et al. Muma-tom: Multi-modal multi-agent theory of mind[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2025, 39(2): 1510-1519.\n\n[2] Xiao J, Shang X, Yao A, et al. Next-qa: Next phase of question-answering to explaining temporal actions[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 9777-9786.\n\n[3] Patraucean V, Smaira L, Gupta A, et al. Perception test: A diagnostic benchmark for multimodal video models[J]. Advances in Neural Information Processing Systems, 2023, 36: 42748-42761."}, "questions": {"value": "Will the dataset be released?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EwOgPfRKEW", "forum": "wPr54hJYuF", "replyto": "wPr54hJYuF", "signatures": ["ICLR.cc/2026/Conference/Submission24786/Reviewer_AiKH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24786/Reviewer_AiKH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762182021201, "cdate": 1762182021201, "tmdate": 1762943196459, "mdate": 1762943196459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed MA-EgoQA, a benchmark for multi-agent egocentric video QA of embodied agent scenarios. It constructs QA datasets grounded in long-horizon video, and categorizes them in 5 categories. It evaluates multiple LLMs and agents, and proposed a new method, EgoMAS."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. the benchmark fills a recognized gap in existing datasets\n2. the dataset construction pipeline and quality control is thorough and clearly described\n3. the evaluation is comprehensive, containing most of the open-source LLMs, close-source LLMs, and other agents"}, "weaknesses": {"value": "1. the dataset is generated based on EgoLife, it would be better if the author could include more other scenarios rather than only EgoLife\n2. the proposed EgoMAS is simple, and does not exhibit a significant improvement, the presence of this method is not so meaningful\n3. the windowing strategies and retrieval granularities may not be optimized equivalently across these methods\n4. the ablation study should verify the contribution of submodules 4W1H and BM25\n5. the efficiency of shared memory, which might be the most important part when this method is applied in real-world, is not discussed in this paper"}, "questions": {"value": "1. Why ToM is the hardest task, did you conduct any thorough analysis on the possible reasons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cbdJrhKgbP", "forum": "wPr54hJYuF", "replyto": "wPr54hJYuF", "signatures": ["ICLR.cc/2026/Conference/Submission24786/Reviewer_tkxf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24786/Reviewer_tkxf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762784740244, "cdate": 1762784740244, "tmdate": 1762943196235, "mdate": 1762943196235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}