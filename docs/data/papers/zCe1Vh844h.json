{"id": "zCe1Vh844h", "number": 17420, "cdate": 1758275811440, "mdate": 1759897176415, "content": {"title": "Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models.", "abstract": "The Fisher information is a fundamental concept for characterizing the sensitivity of parameters in neural networks. However, leveraging the full observed Fisher information is too expensive for large models, so most methods rely on simple diagonal approximations. While efficient, this approach ignores parameter correlations, often resulting in reduced performance on downstream tasks. In this work, we mitigate these limitations and propose Generalized Fisher-Weighted SVD (GFWSVD) — a post-training LLM compression technique that accounts for both diagonal and off-diagonal elements of the Fisher information matrix, providing a more accurate reflection of parameter importance. To make the method tractable, we introduce a scalable adaptation of the Kronecker-factored approximation algorithm for the observed Fisher information. We demonstrate the effectiveness of our method on LLM compression, showing improvements over existing compression baselines. For example, at a 20% compression rate on the MMLU benchmark, our method outperforms FWSVD, which is based on a diagonal approximation of the Fisher information, by 5%, SVD-LLM by 3%, and ASVD by 6%.", "tldr": "This work introduces GFWSVD, a post-training LLM compression method that improves over prior approaches by using a scalable Kronecker-factored approximation of the full Fisher information matrix, capturing both diagonal and off-diagonal terms.", "keywords": ["Efficient Inference Methods", "Matrix and Tensor Factorization", "Natural Language Processing", "Optimization for Deep Networks", "Model Selection and Structure Learning", "Efficient Training Methods Unsupervised Learning"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9934c73fbb340e69929fc597ddae8aae4814a557.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Generalized Fisher-Weighted SVD (GFWSVD), a post-training compression method for LLMs. It addresses the limitations of prior work like FWSVD, which uses only diagonal approximations of the Fisher Information Matrix (FIM) and ignores parameter correlations. GFWSVD leverages a Kronecker-factored approximation ($A \\otimes B$) of the FIM to capture both row and column dependencies, integrating these factors into a generalized SVD framework. The authors provide a scalable algorithm to compute these factors efficiently and demonstrate empirically that GFWSVD outperforms existing gradient- and activation-based compression baselines (FWSVD, ASVD, SVD-LLM) on BERT and LLaMA-2 models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and clearly structured.\n2. The proposed method GFWSVD is novel and  backed with theoretical grounding and practical evaluations, effectively generalizing FWSVD."}, "weaknesses": {"value": "1. Evaluations on modern LLMs are lacking. Modern LLMs like Llama 3.1/Qwen 3 are known to be overtrained and harder to compress. It is important to provide evaluations on such models for the proposed method to become practical.\n2. The paper can benefit from a more detailed literature review. More recent SVD-based LLM compression approaches can achieve much higher compression ratios on more recent LLMs; for example, BitStack[1] matches quantization results and far surpasses the baselines in the paper. I understand this paper is more analytical, but a section with a more detailed discussion of the practicality of GFWSVD and more recent approaches is needed.\n\n[1] BitStack: Any-Size Compression of Large Language Models in Variable Memory Environments"}, "questions": {"value": "See Weaknesses. I’ll increase the score from 4 to 6/8 if the authors can provide the requested experiments and discussions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k0SUdAnY8h", "forum": "zCe1Vh844h", "replyto": "zCe1Vh844h", "signatures": ["ICLR.cc/2026/Conference/Submission17420/Reviewer_CCmb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17420/Reviewer_CCmb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642895161, "cdate": 1761642895161, "tmdate": 1762927314448, "mdate": 1762927314448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes Generalized Fisher-Weighted SVD (GFWSVD), a post-training LLM compression technique. The method uses the Fisher information to approximate the Hessian, which is then further approximated using a Kronecker product.\nSince leveraging the full Fisher information is computationally expensive for large-scale models, existing approaches typically rely on diagonal approximations. This work tightens that approximation by proposing a more general method. The authors develop theory to motivate their approach, assuming the model's dense weight matrices are drawn from a matrix-variate normal distribution. Empirically, they demonstrate that their method achieves better perplexity and accuracy compared to (some) existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The strength of this paper is that it generally seems to outperform the baselines in terms of perplexity and accuracy. This can be observed in Tables 3 and 4 of the main paper, along with Table 8 in the appendix. It also tightens the diagonal approximation of the Fisher information used in other works, albeit under the assumption that the weight matrices are drawn from a matrix-variate normal distribution."}, "weaknesses": {"value": "I think one of the major weaknesses of this paper is the clarity in presentation, in that it is often missing key details to either experiments or is not clear to understand. For example:\n- In Theorem 1, what is the \"task loss function\"? I don't think the \"task\" is clearly defined. I assume that the authors are referring to the problem in Equation (5), but then this should be referenced.  In fact, is this first condition (or perhaps the second condition) redundant? Based on lines 154-156 of the manuscript, it seems to me that 1 and 3 already imply 2, or 2 and 3 already imply 1. I think the authors can frame this a bit more clearly.\n- Table 1 is difficult to follow. Is this the time it takes to perform the decomposition of all of the matrices in the corresponding model (e.g., LLaMa-2-7B) or is it just a few matrices of the model? If it is a few, then which matrices are chosen? What are the dimensions of these matrices? Or is this the time taken to approximate the actual Fisher information matrix? \n- What is the number of batches $D$ used to approximate the Fisher information in your experiments?\n- Separating Tables 2 and 3 is a bit confusing to me; it seems to me that you can include Table 2 as a row in Table 3. \n\nI'm also concerned at the compression rates used in the experiments; as far as I am aware, a 20% compression rate as done in Table 4 is pretty low and existing works compress at a far more aggressive rate and have good performance -- see [1] as an example.\n\n---\n\n[1] \"BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep Neural Network Inference\". Changwoo Lee, Soo Min Kwon, Qing Qu, Hun-Seok Kim. NeurIPS 2024."}, "questions": {"value": "I have listed a few questions in the weaknesses section. Here are just a few more:\n\n- Why is assuming that the dense weight matrices follow a matrix-variate normal distribution a fair assumption? As far as I can tell, this generalized decomposition only holds under this assumption. Can you reference existing works that also make this assumption, or is there any evidence you can provide? \n- In the limitations section, it says that often a constant $\\alpha$ is used to ensure positive semidefiniteness. What is the constant used for experiments? Does FWSVD suffer from this issue as well?\n\nI apologize in advance if these questions are already answered in the manuscript and I missed them. If so, I would appreciate it if the authors could kindly point me to the relevant sections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9VJKtow2IE", "forum": "zCe1Vh844h", "replyto": "zCe1Vh844h", "signatures": ["ICLR.cc/2026/Conference/Submission17420/Reviewer_kykp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17420/Reviewer_kykp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749032342, "cdate": 1761749032342, "tmdate": 1762927314106, "mdate": 1762927314106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GFWSVD: a post-training LLM compression method that uses a Kronecker-factored approximation of the full Fisher Information Matrix (FIM) to perform a generalized weighted SVD on linear layers. The authors prove that FWSVD is a special case of their framework, derive a closed-form 2-factor decomposition (Theorem 1), and provide a rank-1 SVD algorithm to estimate Kronecker factors efficiently. Experiments on BERT/GLUE and LLaMA-2-7B (MMLU, WikiText-2/PTB) show consistent improvements over SVD, ASVD and FWSVD at comparable compression rates."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear generalization & theory. Establishes MVN–Fisher–GSVD connection and proves FWSVD as a special case; gives a closed-form solution for low-rank factors (Eq. 7, 10)\n2. Scalable factor estimation. Rank-1 SVD on a permuted empirical Fisher enables practical Kronecker factorization with matrix–vector products (Alg. 1; complexity discussion + empirical runtimes)."}, "weaknesses": {"value": "1. The paper uses opposite meanings of “Compression Rate” in different sections. Table 2 maps ranks to “compression rate” as removed % (e.g., r=600 ≈ 1%, r=50 ≈ 36%), whereas Table 8 and Fig. 3 (LLM/MMLU) treat it as retained % (e.g., “Compression Rate 99% (r=600)”, x-axis 80–95%, and Full model=100%). This contradicts the BERT side and makes cross-figure/table reading ambiguous. \n2. The LLM side evaluates only LLaMA-2-7B-chat, lacking newer families like Llama-3 and Qwen-2.5/Qwen-3 (ideally multiple sizes) to support claims of architecture- and scale-robustness. The baselines should also include recent SVD-based methods—SVD-LLM v2, Basis Sharing, and Dobi-SVD—under a unified setup, and report a 50%–90% compression-ratio sweep (e.g., 50/60/70/80/90%) to enable apples-to-apples quality-vs-compression comparisons. \n**SVD-LLM v2** https://arxiv.org/abs/2503.12340 \n**Basis Sharing** https://openreview.net/pdf?id=gp32jvUquq\n **Dobi-SVD**  https://openreview.net/pdf?id=kws76i5XB8\n3. The paper targets better post-training compression via Kronecker-factored FIM and generalized weighted SVD, but it reports quality only and omits end-to-end time-to-truncated-model versus SVD-LLM.\n4. **References & in-text citations are not ICLR-compliant**"}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "knHtQk71TT", "forum": "zCe1Vh844h", "replyto": "zCe1Vh844h", "signatures": ["ICLR.cc/2026/Conference/Submission17420/Reviewer_iYrs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17420/Reviewer_iYrs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829006624, "cdate": 1761829006624, "tmdate": 1762927313743, "mdate": 1762927313743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced the Generalized Fisher Weighted SVD (GFWSVD), a post-training, dual pipeline\ncompression technique that leverages the Kronecker-factored approximation for the full empirical Fisher\nInformation Matrix (FIM) to drive optimal compression for dense weight matrices of diverse large\nlanguage models. Specifically, the method introduced, at first, a scalable rank-1 Kronecker decomposition\nalgorithm that reduces FIM factorization cost from $\\mathcal O((mn)^3)$ to $\\mathcal O(m^3 + n^3)$,\nthen it proposed an efficient Kronecker decomposition based Singular Value Decomposition for dense\nlayer compressions. The paper theoretically shows (in Theorem 1) that under MVN + Kronecker\nassumptions, this method yields the optimal weighted low-rank approximation in expectation. Empirically\ndemonstrates improvements over vanilla SVD, ASVD, SVD-LLM and diagonal FWSVD on BERT\n(GLUE) and LLaMA-2 (MMLU, perplexity) across a range of compression rates.\nStrengths"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduced a generalized, architecture agnostic compression method that applies to any linear (or\nKronecker-structured) layer in an LLM. By factoring the full empirical Fisher Information into two small\nsensitivity matrices and plugging them into a weighted SVD, GFWSVD ensures efficient compression\nwith respect to parameter interactions, while reducing the factorization cost from $\\mathcal O((mn)^3)$\nto $\\mathcal O(m^3 + n^3)$.\n2. Theoretical optimality (Theorem 1) follows from the MVN/Laplace approximation around an MLE\nsolution, where assumptions that mirror those commonly used in second-order optimization (e.g. K-FAC)\nand Bayesian posteriors. This alignment with well-understood curvature approximations ensured\nGFWSVD both sound and readily applicable in real-world settings.\n3. GFWSVD outperforms prior works on recognized baselines and benchmarks across a range of\ncompression rates."}, "weaknesses": {"value": "1. While the paper details the theoretical and empirical cost of the offline Kronecker decomposition\n(compression) step, there is insufficient experiments for performance on computation side. Since one of\nthe main appeals of low-rank methods is reduced FLOPs and wall-clock latency at runtime, the absence\nof end-to-end benchmarks (e.g. on LLaMA-7B-chat-bf across different ranks) makes it hard to judge realworld\nbenefit compared to ASVD, SVD-LLM, or Dobi-SVD. Furthermore, the time complexity analysis\nwas primarily focused on offline model decomposition step, which there is no theoretical analysis for\ninference side analysis. Therefore, a thorough theoretical and quantitative analysis for computational\nperformance is expected for an acceptance (Major concern, will consider raising score if properly\naddressed).\n2. The GFWSVD exhibits a relatively low compression ceiling, with only about a 40 % reduction at rank\n1, whereas pure SVD methods like SVD-LLM can achieve over 60 % reduction under comparable\naccuracy constraints when accuracy drop is tolerable. Without strategies to extend beyond this “shallow”\nbound (e.g. multi-term Kronecker sums or hybrid pruning), GFWSVD’s standalone compression\nadvantage appears limited. (If not properly solved, the maximum score will be a weak accept)\n3. The evaluation covers ASVD, SVD-LLM, and the original FWSVD, but omits more recent or stronger\nmethods such as SVD-LLM v2 and Dobi-SVD. Including those would clarify where GFWSVD stands\nagainst the current state of the art and strengthen its performance."}, "questions": {"value": "1. The paper proposed only a single Kron-term (rank-1) FIM factorization and varies only the model’s\nrank $r$ when compressing weights. It remains unclear how choosing different ranks for the Fisher\ndecomposition itself (or using multiple Kronecker terms) impacts end-task performance—and whether the\nFisher-based weighting truly outperforms unweighted or diagonal-weighted SVD across ranks. An\nablation study over both FIM rank and model rank is expected to clarify how sensitive accuracy is to\nthose choices and quantify the standalone benefit of the Fisher weighting.\n2. GFWSVD also relies heavily on Cholesky factorizations deriving FIM blocks ($A = L_A L_A^\\top$,\n$B = L_B L_B^\\top$). However, it is not uncommon that, in practice, finite-batch Fisher estimates can be\nnear-singular or even indefinite. The paper does not report on how often Cholesky fails, what damping is\napplied, or how numerical issues affect the compressed model’s performance. Therefore, an empirical\nanalysis of these edge cases and corresponding regularization strategies to tackle the edges are expected\nfor robustness consideration.\nFew other tips:\n1. Typo in Appendix D, Table 4: The ASVD entry “–0.03” at 64 % compression appears to report the\nchange in STS-B score (compressed minus original) rather than the absolute correlation. All other\nmethods list absolute STS-B values (about 0.7–0.9), so ASVD’s “–0.03” seems to be a transcription error.\n2. In Alg 1 step 1: “IF $\\leftarrow$ $\\frac{|D|}\\sum g_i g_i^T$”, where $g_i g_i^T$ is with quadratic\ncomplexity and was never materialized in practice. The author should clarify that in practice full matrix\nwas never formed but accumulated only $G_iG_i^T$ and $G_i^T G_i$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s0MUSX1vpt", "forum": "zCe1Vh844h", "replyto": "zCe1Vh844h", "signatures": ["ICLR.cc/2026/Conference/Submission17420/Reviewer_CAau"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17420/Reviewer_CAau"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886781152, "cdate": 1761886781152, "tmdate": 1762927313201, "mdate": 1762927313201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}