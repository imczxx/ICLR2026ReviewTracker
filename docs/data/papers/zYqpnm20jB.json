{"id": "zYqpnm20jB", "number": 5733, "cdate": 1757930192419, "mdate": 1759897957632, "content": {"title": "E$^3$-Pruner: Towards Efficient, Economical, and Effective Layer Pruning for Large Language Models", "abstract": "With the increasing size of large language models, layer pruning has gained increased attention as a hardware-friendly approach for model compression. However, existing layer pruning methods struggle to simultaneously address key practical deployment challenges, including performance degradation, high training costs, and limited acceleration. To overcome these limitations, we propose \\name, a task-\\underline{E}ffective, training-\\underline{E}conomical and inference-\\underline{E}fficient layer pruning framework. \\namespace introduces two key innovations: (1) a differentiable mask optimization method using a Gumbel-TopK sampler, enabling efficient and precise pruning mask search; and (2) an entropy-aware adaptive knowledge distillation strategy that enhances task performance. Extensive experiments over  diverse model architectures and benchmarks demonstrate the superiority of our method over state-of-the-art approaches. Notably, \\namespace achieves 96\\% accuracy, a mere 0.8\\% drop from the original model (96.8\\%) on MATH-500 when pruning 25\\% layers of Qwen3-32B, outperforming existing SOTA (95\\%), with a 1.33$\\times$ inference speedup by consuming merely 0.5B tokens (0.5\\% of the post-training data volume).", "tldr": "", "keywords": ["Large language model", "model compression", "layer pruning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c6e1cf3cd3d8da07a69f7c5dd50febad3e83d4f5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a layer pruning framework utilizing differentiable Gumbel-TopK sampler for and entropy-weighted knowledge distillation (KD) strategy for accuracy recovery. Experiments across different models show empirically better performance retention over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Efficient LLM pruning while addressing all three axes (accuracy, cost, speed) is of high importance. The experiments were well-designed in general and the results look promising."}, "weaknesses": {"value": "+ The core technical contributions are straightforward heuristics adapted from existing differentiable pruning methods (Gumbel-Softmax) and weighting schemes. They offer negligible conceptual advance and are presented without necessary theoretical justification.\n\n+ Lacking analysis of convergence and other key characteristics.\n\n+ No wall-clock latency analysis.\n\n+ The comparison restricts competitive baselines like DarwinLM.\n\nThe model optimization is presented as a black box, without any interpretability analysis of the resulting layer redundancy profile."}, "questions": {"value": "+ Provide a rigorous hardware-level latency analysis on a standard GPU. Quantify the real-world inference throughput after pruning.\n\n+ Explain why the Gumbel-TopK sampler should converge to a better mask than a simpler method, and show the sensitivity of the final mask composition to the annealing temperature (τ) schedule. Would a random search for layer indices yield a mask that performs comparably after distillation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1P3ebkHRK2", "forum": "zYqpnm20jB", "replyto": "zYqpnm20jB", "signatures": ["ICLR.cc/2026/Conference/Submission5733/Reviewer_6ac5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5733/Reviewer_6ac5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761378662899, "cdate": 1761378662899, "tmdate": 1762918227543, "mdate": 1762918227543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "$E^{3}$-PRUNER is a layer pruning framework for large language models. This approach combines a differentiable search strategy using a Gumbel-TopK sampler with adaptive knowledge distillation techniques to find and train the optimal layer pruning configuration in an end-to-end manner. Extensive experiments on multiple models demonstrate the superiority of it over state-of-the-art approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The verification experiment is done quite thoroughly, spanning from 7B to 671B.\n- The paper is easy to follow.\n- The Gumbel-TopK sampler is introduced to replace the discrete TopK selection, achieving end-to-end differentiable optimization of layer pruning masks."}, "weaknesses": {"value": "- As shown in Table 1, the model accuracy remains above 80% after pruning from 6.7B to 2.7B, which is extremely unusual in the field of LLM layer pruning. I am skeptical of this result and hope that the authors can provide a reproducible model checkpoint. In addition, since the proposed method can achieve such a high compression rate for LLaMA-2-7B, why is the compression rate so low for Qwen3-32B and DeepSeek-R1? In theory, the more model parameters there are, the more parts that can be compressed.\n- Regarding the performance recovery of the pruned model, this paper uses adaptive knowledge distillation. Is it to fine-tune the entire model or only some parameters?\n- Compared with traditional LoRA fine-tuning, how much performance improvement does the proposed adaptive knowledge distillation have, and how much difference is there in computational cost between these two?\n- How much performance improvement would occur if other pruning methods were used with Adaptive Knowledge Distillation?\n- No comparison with more advanced layer pruning methods."}, "questions": {"value": "Given that you used knowledge distillation to restore the pruned DeepSeek-R1 model, how many GPUs do you use for the experiment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I2mscZzRti", "forum": "zYqpnm20jB", "replyto": "zYqpnm20jB", "signatures": ["ICLR.cc/2026/Conference/Submission5733/Reviewer_Kz3N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5733/Reviewer_Kz3N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631255114, "cdate": 1761631255114, "tmdate": 1762918227004, "mdate": 1762918227004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes E3-PRUNER, which is a layer-pruning framework for large language models that learns which layers to keep or remove through a differentiable mask optimized with a Gumbel-TopK sampler. It further uses entropy-aware adaptive knowledge distillation to retain performance. Experiments show it achieves up to 2.18× speedup with minimal accuracy loss, offering an efficient, economical, and effective solution for LLM compression."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a differentiable mask learning framework (Gumbel-TopK) for layer pruning, enabling efficient gradient-based layer selection.\n2. The method achieves a good pruned model performance, outperforming prior pruning methods.\n3. The paper further introduces entropy-aware adaptive knowledge distillation, effectively preserving key reasoning tokens.\n4. The experiments demonstrate consistent and superior results across multiple LLMs with minimal accuracy loss."}, "weaknesses": {"value": "1. The paper provides a limited theoretical explanation of why the Gumbel-TopK mask search is able to identify the optimal layers.\n2. The paper does not clarify whether the performance gain comes from the layer pruning method or the Adaptive Knowledge Distillation. It would be better to compare the zero-shot performance of the pruned model without fine-tuning or apply Adaptive KD to baseline pruning methods to evaluate their relative effects."}, "questions": {"value": "This is the first academic paper I have seen that fine-tunes the 671B DeepSeek-R1 model. How many GPUs do you use? This information could be included in the Settings section of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wKNLQbfVrW", "forum": "zYqpnm20jB", "replyto": "zYqpnm20jB", "signatures": ["ICLR.cc/2026/Conference/Submission5733/Reviewer_iaGe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5733/Reviewer_iaGe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094316363, "cdate": 1762094316363, "tmdate": 1762918226701, "mdate": 1762918226701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes $E^3$-Pruner, a layer pruning framework for large language models that integrates a differentiable Gumbel-TopK sampler for efficient mask optimization and an entropy-aware adaptive knowledge distillation strategy to address the limitations of existing methods in performance preservation, training cost, and inference acceleration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper combines a differentiable Gumbel-TopK sampler, for efficient and accurate pruning mask search with an entropy-aware adaptive knowledge distillation strategy, for enhanced knowledge transfer with reduced computational cost.\n2. This paper executes extensive experiments on across diverse LLMs with different sizes and architectures, and evaluates on multiple benchmarks, demonstrating the generalization and practicality of the proposed $E^3$-Pruner framework."}, "weaknesses": {"value": "1. **Limited novelty:** Gumbel-TopK sampling for pruning has been explored in prior works [1-2] for model compression, and the progressive layer pruning strategy is a common approach (e.g., SLEB [3]) with no significant innovation here.\n2. **Incomplete baselines:** Heuristic layer pruning methods like SLEB [3] and Shortened LLaMA [4] should be added as baselines to enable more comprehensive comparison and better highlight the proposed method’s advantages.\n3. **Unfair comparison design:** The paper fails to clarify whether baseline methods update parameters (for performance recovery after pruning). Comparing parameter-updating and non-updating methods (e.g., training-free ShortGPT’s pruned model vs. $E^3$-Pruner’s fine-tuned model) is inappropriate; training-free baselines should be compared with $E^3$-Pruner’s post-search model.\n4. **Insufficient evidence for training economy:** Only training token counts are reported. Actual training time and computational resources (e.g., GPU hours) during pruning should be provided to substantiate the claim of training efficiency.\n5. **Lack of ablation study on layer importance initialization:** Comparing different initialization methods (e.g., random initialization, ShortGPT’s layer importance metric) would clarify how initialization impacts final performance.\n\n[1] Gonzalez-Carabarin, et al. Dynamic Probabilistic Pruning: A General Framework for Hardware-Constrained Pruning at Different Granularities. TNNLS 2022.\n\n[2] Tan, et al. Mutually-aware Sub-Graphs Differentiable Architecture Search. arXiv 2021.\n\n[3] Song, et al. SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks. ICML 2024.\n\n[4] Kim, et al. Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods. arXiv 2024."}, "questions": {"value": "1. Minitron proposed iterative prune+distill for model pruning. How many prune+distill iterations were conducted in your experiments? If multiple, please provide the number of iterations and detailed settings; if only one, explain the rationale for choosing single over multiple iterations.\n2. How does the performance of ShortGPT-pruned models fine-tuned with $E^3$-Pruner’s adaptive knowledge distillation compare to pruned models after $E^3$-Pruner's two stages process? This comparison would help isolate the effects of the pruning method and distillation strategy.\n3. Could you provide qualitative examples of outputs from the original and pruned models on specific tasks (e.g. MMLU using Qwen2.5-14B-Instruct)? This would illustrate the practical impact of pruning on model performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pUsbZtPViH", "forum": "zYqpnm20jB", "replyto": "zYqpnm20jB", "signatures": ["ICLR.cc/2026/Conference/Submission5733/Reviewer_Gw2L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5733/Reviewer_Gw2L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762099113152, "cdate": 1762099113152, "tmdate": 1762918226269, "mdate": 1762918226269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed E³-PRUNER, a two-stage layer-wise pruning framework tailored for LLMs. In the mask search stage, it learns a binary layer mask with a Gumbel-TopK sampler and STE backward, warmed up by KL-based layer-importance initialization and a curriculum schedule that gradually increases the pruning ratio. In the recovery stage, it applies adaptive knowledge distillation, using offline teacher Top-K logits and token-wise entropy weighting on the per-token loss. In the experiments, the method preserves accuracy while reducing depth and improving latency under modest training budgets across models and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem statement and challenge are clear and significant. Section 2.2 provides a detailed analysis of why existing training-free, differentiable, and NAS-based approaches each fall short (e.g., accuracy drop, high token budgets, irregular speedups), setting up a precise target for improvement.\n2. This paper is fairly written. The narrative is well-structured and easy to follow: motivation, formulation, differentiable mask search, and recovery via adaptive KD. Figures and algorithms (Fig. 3; Algs. 1–2; Eqs. (4)–(6)) make the procedure executable and reduce ambiguity about forward/backward behavior and training schedules.\n3. Clear real-world deployment evidence supports the author’s claim on effectiveness and inference efficiency gain. Beyond accuracy tables, the paper reports wall-clock improvements for large-scale models"}, "weaknesses": {"value": "1. The behavioral consistency metric is weakly defined. The paper measures consistency as average accuracy on a small mixed set rather than teacher–student agreement (e.g., output match rate, output distributions, or log-prob correlations). This undermines the claim that KD better preserves behavior.\n\n2. Storage/IO for offline KD is unquantified. The method relies on offline Top-K logits and asserts “minor storage,” using Top-10 in all configs, but provides no concrete footprint or bandwidth numbers under long contexts and 0.5B-token training budgets. This limits reproducibility and deployability assessments.\n\n3. Fairness of the comparison. ShortGPT is training-free, so its lower accuracy vs. trained methods is expected; using it as a headline accuracy comparator can be misleading. A fair alternative is ShortGPT with the same recovery budget (SFT/KD) to normalize token use.\n\n4. Missing comparisons to recent depth-pruning baselines. The paper does not compare to other layer/depth pruners that replace blocks via KD (e.g., LLM-Streamline), which target similar deployment goals."}, "questions": {"value": "1. Authors assert “minor storage,” set Top-10 in all configs, but provide no concrete footprint/bandwidth numbers. To better understand the practicality of the Top-10–logits setup, could you share results, such as the approximate on-disk size per token when storing Top-10 logits and the total footprint for 0.5B tokens under typical context lengths?\n\n2. Table 3 shows the ablation of the searching budget. Any clue why adding the search budget would even decrease performance?\n\n3. How does E³-PRUNER compare with other LLM depth pruners? For example, LLM-Streamline[1], which identifies less important blocks and substitutes them with a light-weight block obtained via KD.\n\n[1]Streamlining Redundant Layers to Compress Large Language Models, iCLR'25"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2aiPLO07kt", "forum": "zYqpnm20jB", "replyto": "zYqpnm20jB", "signatures": ["ICLR.cc/2026/Conference/Submission5733/Reviewer_gvMZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5733/Reviewer_gvMZ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission5733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181317853, "cdate": 1762181317853, "tmdate": 1762918225733, "mdate": 1762918225733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}