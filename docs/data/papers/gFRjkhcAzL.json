{"id": "gFRjkhcAzL", "number": 9345, "cdate": 1758119552321, "mdate": 1758923410944, "content": {"title": "When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity", "abstract": "LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional, ground-truth–based benchmarks. We argue that, without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. \\emph{Schematic adherence} quantifies how much of a judge’s overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric.  \\emph{Psychometric validity} aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: e.g., unexplained variance exceeding 90\\% for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md", "tldr": "LLM-judged benchmarks such as Arena-Hard Auto contain severe, unseen design flaws -- this leads to an illusion of a meaningful ranking that is in fact largely noise.", "keywords": ["foundation models", "llm judges", "benchmarking", "evaluation", "metrics", "meta-analysis"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c99661be60264db32f866208fecb1e676237e88b.pdf", "supplementary_material": ""}, "replies": [], "withdrawn": true}