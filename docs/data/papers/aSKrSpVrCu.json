{"id": "aSKrSpVrCu", "number": 24810, "cdate": 1758360585650, "mdate": 1759896747297, "content": {"title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling", "abstract": "Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. \nVector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. \nSince a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. \nHowever, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. \nFor instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. \nThis gap even increases with the reduction of clustering centers. \nFurther, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. \nThis property is contrary to the above finding, unavoidably leading to larger quantization errors. \nTo these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.\n2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. \nExperimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.", "tldr": "", "keywords": ["Large Language Models", "Vector Quantization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0d3be6edd72fca98f8be21a12a4879d387e2407.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes PCDVQ (Polar Coordinate Decoupled Vector Quantization), a novel post-training quantization (PTQ) framework for compressing large language models (LLMs).\nThe key insight is that a vector’s direction is more sensitive to quantization errors than its magnitude, yet most existing vector quantization (VQ) methods couple them together and use Euclidean distance, which overemphasizes magnitude errors.\n\nTo address this, the authors:\n1) Propose Polar Coordinate Decoupling (PCD) — representing weights in polar form and independently quantizing direction and magnitude, allocating more bits to direction.\n2)Introduce Distribution-Aligned Codebook Construction (DACC) — building codebooks aligned with theoretical distributions: E8 lattice-based greedy sampling for direction and Lloyd-Max quantization for magnitude.\n\nExtensive experiments on multiple LLMs (LLaMA-2/3, Mistral) show consistent improvements in 2-bit quantization performance, without introducing extra inference cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Mathematical rigor: The decomposition of quantization error and the codebook derivations are theoretically justified.\n\nStrong empirical validation: Broad experiments on LLaMA-2/3 and Mistral confirm robustness and generality.\n\nPractical efficiency: PCDVQ maintains inference speed while achieving higher accuracy and compression."}, "weaknesses": {"value": "Limited discussion on scalability: While the method performs well at 2–2.25 bits, it remains unclear whether benefits persist at moderate bitwidths (e.g., 3–4 bits) or in activation quantization.\n\nDependency on Gaussian regularization: The approach assumes weights approximate a standard Gaussian distribution after the randomized Hadamard transform; it would be useful to test models with non-Gaussian weight distributions.\n\nOverlap with QuIP#: Many technical components (e.g., E8 lattice codebook and fine-tuning scheme) are similar to QuIP#. The paper does not sufficiently clarify the conceptual distinction and the essential novelty beyond reinterpreting QuIP# in polar coordinates.\n\nReproducibility and code release: The paper does not explicitly mention whether code and trained quantization configurations will be made publicly available, which is important for validation and adoption."}, "questions": {"value": "Q1:How does the method perform at moderate bitwidths (e.g., 3–4 bits) and for activation quantization?\n\nQ2:How sensitive is PCDVQ to the Gaussian regularization step? What happens if it is omitted or replaced with another normalization?\n\nQ3:What is the essential novelty beyond QuIP#? How does the method conceptually differ from QuIP# despite using similar components like the E8 lattice codebook and fine-tuning scheme?\n\nQ4:Will the authors release code, pretrained codebooks, and fine-tuning scripts to ensure reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper does not need ethics review."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jKdRfs6G02", "forum": "aSKrSpVrCu", "replyto": "aSKrSpVrCu", "signatures": ["ICLR.cc/2026/Conference/Submission24810/Reviewer_3ksg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24810/Reviewer_3ksg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761022251600, "cdate": 1761022251600, "tmdate": 1762943205483, "mdate": 1762943205483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Polar Coordinate Decoupling Vector Quantization (PCDVQ), a method designed to improve the accuracy of low-bit quantization for Large Language Models (LLMs). The core objective is to mitigate the substantial performance degradation faced when compressing LLMs to extremely low bitrates (e.g., $\\leq 2.5$ bits). PCDVQ addresses this by observing that a vector's direction is significantly more sensitive to quantization error than its magnitude. It thus proposes to decouple the vector into polar coordinates (direction and magnitude) for independent quantization. Empirically, PCDVQ demonstrates superior accuracy retention compared to existing quantization methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Novel and Well-Motivated Decoupling Mechanism:** The fundamental insight that vector direction and magnitude exhibit different quantization sensitivities is novel for LLM quantization and provides a strong, intuitive justification for the methodological decoupling. Quantizing these components separately via polar coordinates is a sound solution to preserve the crucial directional information."}, "weaknesses": {"value": "**1. Lack of Robust Experimental Validation on Difficult Benchmarks:** The experimental evaluation is limited to zero-shot multiple choice benchmarks. To fully validate the method's contribution, the paper must be evaluated on more difficult and diverse reasoning benchmarks like MMLU and GSM8K, where small quantization errors often lead to catastrophic failure. The reported accuracy improvement also appears marginal on the limited set of reported tasks, necessitating further validation.\n\n**2. Missing System-Level Inference Evaluation and Comparison:** The inference speed (or throughput) of the quantized model is a crucial component for any quantization algorithm. The paper currently lacks a systemized analysis and comparison of the PCDVQ inference latency against competitors. This absence makes it impossible to assess the practical, end-to-end efficiency trade-off of the proposed method.\n\n**3. Unclear Experimental Consistency in Fine-tuning:** The paper does not clearly articulate whether the same post-quantization fine-tuning methods (if any were used) were applied across all compared baselines (e.g., GPTQ, AQLM, GPTVQ) and PCDVQ. Without explicitly confirming that all methods were compared under the same training/fine-tuning regime, the claimed accuracy improvements may be due to differences in the fine-tuning processes rather than the core PCDVQ mechanism."}, "questions": {"value": "1. Could you provide a detailed analysis of the inference speed of PCDVC? I want to check if it slows down the model’s inference speed compared to the existing method.\n2. Could you provide experimental results on MMLU and GSM8K, or other considerable benchmarks?\n3. Could you clarify the fine-tuning recipe for all competitors? For example, did you perform fine-tuning after quantizing GPTQ or GTPVQ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gpAlBAQCV7", "forum": "aSKrSpVrCu", "replyto": "aSKrSpVrCu", "signatures": ["ICLR.cc/2026/Conference/Submission24810/Reviewer_ywNG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24810/Reviewer_ywNG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747238151, "cdate": 1761747238151, "tmdate": 1762943204578, "mdate": 1762943204578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Preserving direction is more important than preserving magnitude in vector quantization. However, current VQ methods emphasize reducing the magnitude error. PCDVQ utilizes polar coordinates to enhance the expressivity of codebook for directional information. PCDVQ also shares codebook for entire model to minimize the memory consumption by regularize all weights to follow the same Gaussian distribution. Experimental results show that PCDVQ achieves the state-of-the-art performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of decomposing the expressive power of the codebook into fine-grained components (direction and magnitude in PCDVQ) is innovative.\n2. Regularizing the distribution of weights to share the codebook sounds solid and effective."}, "weaknesses": {"value": "1. The paper lacks a theoretical analysis on why directional information is more important than magnitude information.\n2. The efficiency should be compared not only with the full-precision model but also with methods such as VPTQ.\n3. (minor) Citation format seems inappropriate. Should have used \\citep instead of \\cite."}, "questions": {"value": "1. How are the direction and magnitude individually quantized in Figure 1(a)?\n2. How does PCDVQ determine bit widths $a$ and $b$ for direction and magnitude?\n3. It appears that using a polar coordinate representation requires additional computation during dequantization. Does this make the method slower than VPTQ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hHnDPHRhix", "forum": "aSKrSpVrCu", "replyto": "aSKrSpVrCu", "signatures": ["ICLR.cc/2026/Conference/Submission24810/Reviewer_m2Nb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24810/Reviewer_m2Nb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969443952, "cdate": 1761969443952, "tmdate": 1762943203825, "mdate": 1762943203825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work is motivated by the observation that in vector quantization, the directional component is more sensitive to quantization errors than the magnitude component. Existing Euclidean distance based quantization methods primarily focus on minimizing magnitude errors, which contradicts this finding and consequently leads to larger overall quantization errors. To address this issue, the paper proposes a polar decoupled vector quantization framework, which achieves satisfactory results across multiple experimental settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introducing polar decoupled vector quantization is an interesting and novel attempt.\n\n2. The overall writing is clear and easy to follow.\n\n3. The method demonstrates superior performance on several large language models, including LLaMA-2/3 and Mistral, achieving better zero-shot accuracy and perplexity at the 2-bit weight quantization level compared with existing state-of-the-art quantization approaches, which validates its effectiveness."}, "weaknesses": {"value": "1. The PCDVQ framework introduces additional computational steps, including polar coordinate conversion, two independent codebook searches using cosine similarity and Euclidean distance respectively, and possible inverse conversion. The paper reports improved throughput mainly due to reduced memory bandwidth, but it does not quantify the impact of these added operations on single inference latency. This is important because on many edge devices compute cost is more critical than memory bandwidth. The authors should provide a detailed latency breakdown, including wall clock time per layer for conversion, codebook lookup, and inverse mapping, measured on representative CPU and low-power GPU hardware, and compare end-to-end latency and energy consumption with baseline methods.\n\n2. The effectiveness of the DACC module relies on the assumption that weight vectors, after a random Hadamard transform, follow an approximate standard Gaussian distribution. It is unclear whether this approximation holds uniformly across all layers and architectures (for example, different LLaMA and Mistral variants). It would be better to include empirical diagnostics showing distributional statistics (mean/variance/skewness/kurtosis) of transformed weights per layer and per model, and discuss cases where the Gaussian approximation breaks down and how that affects quantization error.\n\n3. All experiments are conducted on decoder-only Transformer large language models and evaluated on a limited set of zero shot tasks and language modeling benchmarks. It remains an open question whether the direction magnitude decoupling idea transfers to encoder models such as BERT, Vision Transformer models, or multimodal models. These models have different activation and weight statistics and different sensitivity to quantization. \n\n4. For tasks that require stronger reasoning ability, such as mathematical reasoning or code generation, it is important to know how PCDVQ affects fine-grained semantic fidelity. The current evaluation set does not cover these demanding reasoning tasks. The paper would be stronger if it reported results on a suite of hard reasoning benchmarks and provided error analyses that reveal whether performance degradation (if any) is systematic and whether it is attributable to directional quantization errors or to capacity limits of the codebooks."}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7Dpgr4ERKB", "forum": "aSKrSpVrCu", "replyto": "aSKrSpVrCu", "signatures": ["ICLR.cc/2026/Conference/Submission24810/Reviewer_3yCH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24810/Reviewer_3yCH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975546831, "cdate": 1761975546831, "tmdate": 1762943203444, "mdate": 1762943203444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PCDVQ, a post-training weight-only quantization framework for LLMs that operates in polar coordinates: each weight vector is decomposed into direction and magnitude, which are quantized independently with a larger bit budget for direction. To reduce distortion, the method builds distribution-aligned codebooks. Across multiple LLMs and benchmarks in low-bit settings, PCDVQ consistently outperforms strong VQ and SQ baselines, indicating that decoupling and aligning to componentwise distributions yields better accuracy at very low precision."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper shows direction is markedly more sensitive to quantization than magnitude, and analyzes why Euclidean MSE emphasizes magnitude errors more strongly, supporting the decoupling design. The motivation of the work is clear and reasonable.\n\n2.This work provides a clear and comprehensive theoretical foundation for polar coordinate decoupling, demonstrating strong depth and theoretical rigor.\n\n3.Across multiple LLM families and standard zero-shot benchmarks, the main results tables show that PCDVQ generally matches or surpasses strong low-bit VQ/SQ baselines."}, "weaknesses": {"value": "1.The choice to allocate more bits to direction is well supported by experiments, but the paper offers no formal analysis to guide the split or to select an optimal allocation under different conditions.\n\n2. The method adopts a fixed vector dimension and borrows several settings from prior work, but it remains unclear how to adapt the dimension or the direction–magnitude bit split across model sizes, layer types, or differing weight statistics. Robustness to these design choices is not systematically examined."}, "questions": {"value": "1.How sensitive is PCDVQ to design choices such as the direction similarity metric, codebook size, and the vector dimension, and are there general guidelines for setting these across models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vGKsDhxJrq", "forum": "aSKrSpVrCu", "replyto": "aSKrSpVrCu", "signatures": ["ICLR.cc/2026/Conference/Submission24810/Reviewer_ipFv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24810/Reviewer_ipFv"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994881440, "cdate": 1761994881440, "tmdate": 1762943203180, "mdate": 1762943203180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to decouple magnitude and direction quantization when performing vector quantization in large language models (LLMs). The proposed pipeline transforms vectors of weight matrices into polar coordinates and quantizes the magnitude and direction separately, taking into account their distinct statistical distributions. The paper presents a persuasive comparison with other scalar-based and vector-based quantization methods across a range of model sizes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides an insightful observation about vector-based quantization: the difference in the approximation behavior of direction and magnitude.\n- The idea of decoupling magnitude and direction components and handling their different distributions is creative and conceptually elegant.\n- The proposed method achieves strong performance compared to established baselines."}, "weaknesses": {"value": "- The experiments primarily focus on a range of LLaMA models, with only a single Mistral experiment included. Broader evaluation across different architectures would strengthen the paper.\n- The comparison with scalar-based quantization methods is limited and could be expanded for a fairer assessment.\n- While the idea is simple and well-motivated, its conceptual simplicity raises questions about whether it is substantial enough for a full-length scientific paper."}, "questions": {"value": "-Is there a difference in inference speed between scalar-based and vector-based quantization methods? If so, wouldn’t it be fair to include that in the comparison?\n- Throughout the paper, you mention quantizing model weights one-by-one. Did you mean layer-by-layer?\n- Were the results in Table 1 and Table 2 obtained without fine-tuning?\n- Why was QuaRot not included in the experimental comparison, despite being mentioned in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xDLgoeuQgr", "forum": "aSKrSpVrCu", "replyto": "aSKrSpVrCu", "signatures": ["ICLR.cc/2026/Conference/Submission24810/Reviewer_eGVU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24810/Reviewer_eGVU"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission24810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997608159, "cdate": 1761997608159, "tmdate": 1762943202919, "mdate": 1762943202919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}