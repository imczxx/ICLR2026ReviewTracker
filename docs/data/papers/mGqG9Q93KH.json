{"id": "mGqG9Q93KH", "number": 2030, "cdate": 1756978077557, "mdate": 1763004794682, "content": {"title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training", "abstract": "In this work, we propose \\textit{DiT360}, a DiT-based framework that performs hybrid training on perspective and panoramic data for panoramic image generation.\nFor the issues of maintaining geometric fidelity and photorealism in generation quality, we attribute the main reason to the lack of large-scale, high-quality, real-world panoramic data, where such a data-centric view differs from prior methods that focus on model design.\nBasically, \\textit{DiT360} has several key modules for inter-domain transformation and intra-domain augmentation, applied at both the pre-VAE image level and the post-VAE token level.\nAt the image level, we incorporate cross-domain knowledge through perspective image guidance and panoramic refinement, which enhance perceptual quality while regularizing diversity and photorealism.\nAt the token level, hybrid supervision is applied across multiple modules, which include circular padding for boundary continuity, yaw loss for rotational robustness, and cube loss for distortion awareness.\nExtensive experiments on text-to-panorama, inpainting, and outpainting tasks demonstrate that our method achieves better boundary consistency and image fidelity across eleven quantitative metrics.\nOur code, trained models, and curated data will be available.", "tldr": "A framework for high-resolution and photorealistic panoramic image generation with hybrid training.", "keywords": ["image generation", "panoramic image generation", "hybrid training"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/2cf9103a7c18b539a55b2cbdd16b24209b0f63b9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a framework for panoramic image generation by leveraging a hybrid training scheme on perspective and panoramic data. It introduces some data augmentation and loss to enhance the model's performance. However, the designs need more justification and evaluation is insufficient."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Extensive experiments have been conducted, and with some extended experiments like inpainting and outpainting task;\n\n- Some interesting loss functions have been proposed."}, "weaknesses": {"value": "- Regarding the \"Perspective image guidance\" mentioned in the paper, the approach appears to involve simply incorporating certain masks into the loss function. This raises the question of whether the dit model has already been exposed to and computed attention for black regions in data constructed under such conditions. Relying solely on such a mask-based loss is insufficiently justified and does not make sense. More details should be discussed.\n\n- About the propesed \"Rotation-consistent Yaw Loss\". This loss resembles a form of data augmentation through rotation operations applied in the noise domain, which has been widely applied in previous works. The paper would benefit from a more rigorous theoretical or empirical argument explaining why this form of regularization is particularly effective, especially compared to standard augmentation in image level, and how it fundamentally improves latent space organization or generation quality.\n\n- The \"Position-aware Circular Padding\" seems a known trick in previous panorama generation paper to improve image boundaries. Any difference here? To better validate this contribution, the authors should include quantitative or qualitative results that explicitly demonstrate the method's performance in handling boundary inconsistency.\n\n- Insufficient evaluation. There is only evaluation for text-to-panorama generation. Why not evaluate image-to-panorama generation? As claimed by authors (and also shown in Fig. 1), the model can perform panorama outpainting, which is exactly the task of image-to-panorama. As many baseline models and the proposed method can perform image-to-panorama gen, more evaluation are needed. \n\n- Which benchmark did you use for quantitative evaluation in table 1? This is not clear and the results of some baselines in table 1 do not make sense. for example, hunyuan world clearly outperforms layerpano3d and worldgen in text-to-panorama in many papers'evaluation. Also lack of CLIP score to evaluate input-output alignment?\n\n- generalization. The visual results shown in paper only involve photorealsitic images. how about stylized images? also, stylized images are used in evaluation in table 1? this is very unclear and unreproducable."}, "questions": {"value": "see above. the evaluation is extremely insufficient with unclear settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VfqUT6BR5R", "forum": "mGqG9Q93KH", "replyto": "mGqG9Q93KH", "signatures": ["ICLR.cc/2026/Conference/Submission2030/Reviewer_Zwpe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2030/Reviewer_Zwpe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761297131548, "cdate": 1761297131548, "tmdate": 1762915997301, "mdate": 1762915997301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "yrGuDnn9aj", "forum": "mGqG9Q93KH", "replyto": "mGqG9Q93KH", "signatures": ["ICLR.cc/2026/Conference/Submission2030/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2030/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763004793673, "cdate": 1763004793673, "tmdate": 1763004793673, "mdate": 1763004793673, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DiT360, a hybrid training strategy for panoramic image generation using the DiT model. Since panoramic data are scarce and differ from normal image data, training with a pure DiT model results in poor generation quality. To address this issue, the authors introduce image-level regularization and token-level supervision. In image-level regularization, panoramic image refinement inpaints the blurry polar regions in the Matterport3D dataset, and perspective image guidance uses MSE loss on the ERP representation of perspective images during training. In token-level supervision, position-aware circular padding ensures avoidance of seams between the left and right sides of the image, and cube and yaw losses are used to reduce distortion. Experiments demonstrate the effectiveness of the proposed training framework for panoramic image generation, both quantitatively and qualitatively."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. In the case of panoramic image refinement, the performance was improved with solid motivation and a simple approach.\n\n2. Circular padding also has a clear motivation that previous works have not addressed, and it was solved by leveraging the characteristics of Flux.1-dev.\n\n3. Two different types of data were trained to improve performance using learning techniques specific to panoramic images.\n\n4. Each of the proposed modules created synergy, achieving a greater performance improvement compared to basic Flux + LoRA training."}, "weaknesses": {"value": "Overall, the proposed method demonstrates notable performance improvements. However, certain descriptions in the paper convey inaccurate or unclear information, and several key experimental details are missing, which hinders full reproducibility and proper evaluation of the results. I am willing to raise the rating if the items below are resolved:\n\n1. In L185-196, the pure DiT doesn’t have RoPE and doesn’t use flow-based scheduler. If the model uses a flow-based scheduler, Eq. 1 must contain the signal-to-noise ratio $\\lambda$ and the time-dependent weighting parameter $\\omega$. I think the authors are trying to introduce Flux, but the current writing is confusing. Please specify the model name and describe its characteristics correctly.\n\n2. Although it is not a major part of this paper, I think it is a small contribution to identify defects in the Matterport3D dataset, perform pre-processing (inpainting using Flux.1 Kontext), and improve data quality. Then, did the evaluated baseline models also learn from the pre-processed data for comparison? If not, it does not seem to be a fair comparison with the baseline (Tab. 1).\n\n3. The ablation of panoramic image refinement is missing. Data quality is a very important factor in learning, so please include ablation for this.\n\n4. The paper should explicitly state the exact model version used (I believe it is Flux.1-dev). The current description leaves this ambiguous, forcing readers to infer details that should be clearly specified.\n\n5. Please also clarify the training configuration. Specifically, whether only the additional LoRA layers were trained or if both the base Flux model and the LoRA layers were jointly trained.\n\n6. In L788-789, the authors mention collecting 40K Internet data samples. Since these data are presumably non-panoramic, it is unclear how they are utilized within the proposed framework. Is only the Perspective Branch trained in this case? If so, please clarify the training procedure and its integration with the overall model."}, "questions": {"value": "1. How much time and GPU memory are required to generate a single 1024 $\\times$ 2048 image?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ueV6M3ZBoV", "forum": "mGqG9Q93KH", "replyto": "mGqG9Q93KH", "signatures": ["ICLR.cc/2026/Conference/Submission2030/Reviewer_oajz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2030/Reviewer_oajz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827005513, "cdate": 1761827005513, "tmdate": 1762915996848, "mdate": 1762915996848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DiT360, a framework for panoramic image generation that leverages hybrid training on both perspective and panoramic data. The method augments a DiT-based diffusion model with several modules—circular padding, yaw loss, and cube loss—to improve geometric fidelity and photorealism. The approach is evaluated on text-to-panorama, inpainting, and outpainting tasks, showing improvements over prior baselines in both quantitative metrics and user studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The hybrid training strategy is well-motivated. While not inherently novel, the design and execution of the framework is well done, and the results show this.\n- The pipeline is well thought out, with clear ablations showing the impact of each module (this is nicely done). The paper is clear and easy to follow.\n- The method demonstrates strong qualitative results, particularly in boundary consistency and reduction of polar artifacts.\n- The inclusion of a user study and a broad set of evaluation metrics is appreciated."}, "weaknesses": {"value": "- The generalization to domains beyond indoor panoramas (e.g., outdoor, dynamic scenes) is not demonstrated.\n- Lack of ablations/insight into $λ_1$ and $λ_2$\n- A primary use case, particularly in AR/VR, is high-frequency details, human faces, etc. This method does not address these areas."}, "questions": {"value": "- How robust is the method to domains outside Matterport3D, such as outdoor or highly dynamic scenes?\n- Can the authors provide more quantitative analysis of failure cases, especially for high-frequency details and human faces?\n- How sensitive is the method to the choice of loss weights (λ1, λ2) in the hybrid loss design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "01egveVpXm", "forum": "mGqG9Q93KH", "replyto": "mGqG9Q93KH", "signatures": ["ICLR.cc/2026/Conference/Submission2030/Reviewer_oktw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2030/Reviewer_oktw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964285729, "cdate": 1761964285729, "tmdate": 1762915996244, "mdate": 1762915996244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}