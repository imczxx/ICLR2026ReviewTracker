{"id": "fScCawsZ77", "number": 18251, "cdate": 1758285634117, "mdate": 1759897116473, "content": {"title": "Accelerating Data Generation for Nonlinear temporal PDEs via homologous perturbation in solution space", "abstract": "Data-driven deep learning methods like neural operators have advanced in solving nonlinear temporal partial differential equations (PDEs).\nHowever, these methods require large quantities of solution pairs—the solution functions and right-hand sides (RHS) of the equations. \nThese pairs are typically generated via traditional numerical methods, which need thousands of time steps iterations far more than the dozens required for training, creating heavy computational and temporal overheads.\nTo address these challenges, we propose a novel data generation algorithm, called HOmologous Perturbation in Solution Space (HOPSS), which directly generates training datasets with fewer time steps rather than following the traditional approach of generating large time steps datasets. This algorithm simultaneously accelerates dataset generation and preserves the approximate precision required for model training.\nSpecifically, we first obtain a set of base solution functions from a reliable solver, usually with thousands of time steps, and then align them in time steps with training datasets by downsampling. Subsequently, we propose a \"homologous perturbation\" approach: by combining two solution functions (one as the primary function, the other as a homologous perturbation term scaled by a small scalar) with random noise, we efficiently generate comparable-precision PDE data points.  Finally, using these data points, we compute the variation in the original equation’s RHS to form new solution pairs.\nTheoretical and experimental results show HOPSS lowers time complexity. For example, on the Navier-Stokes equation, it generates 10,000 samples in approximately 10\\% of traditional methods’ time, with comparable model training performance.", "tldr": "", "keywords": ["Data generation", "AI4PDE", "Nonlinear temporal PDEs"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/291bce9f98f001d5166e12bfce4c89817e882c4c.pdf", "supplementary_material": "/attachment/ec24948e6ca9db1980912fced07d3697b93b65f6.zip"}, "replies": [{"content": {"summary": {"value": "The paper propose a simple way to augment PDE datasets for time-depende problem where the RHS is part of the input, called homologous perturbation. Given a base dataset, it randomly combine two samples as \n\n$$u_{new} = u_1 + a u_2 + \\epsilon$$\n\nand then compute the new RHS.\n\nNumerical experiments show the homologous perturbation augmentation generally improves performance and generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper studies an important but often overlooked probably on efficient data generation for neural operators.\n- The method is simple but seemingly effective."}, "weaknesses": {"value": "- It is not very clear how the new RHS is computed, especially new non-linear term $u u_x$. Is it just approximate by the finite difference method?\n- Please provides some error analysis on the non-linear term in Sec 4, and how it is related to the perturbation parameter.\n- The method is not applicable for many problem where RHS is not part of the input.\n\nMinor: please avoid abusing notations such as \n- - line 166: $O(Nn)$, $N$ was the nonlinear term, $n$ was the time index. \n- - line 271: $\\Delta f$  (look like laplacian),"}, "questions": {"value": "It will be helpful to add some discussion on how the RHS is computed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZXUp558a2W", "forum": "fScCawsZ77", "replyto": "fScCawsZ77", "signatures": ["ICLR.cc/2026/Conference/Submission18251/Reviewer_EjcR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18251/Reviewer_EjcR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861698246, "cdate": 1761861698246, "tmdate": 1762927979036, "mdate": 1762927979036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a particular data augmentation technique suitable for a particular family of nonlinear temporal PDEs with forcing term. Suppose that PDE $\\mathcal{N}(u(x, t)) = f(x, t)$ defines evolution of physical field $u(x, t)$ and we want to learn a mapping $u(x, 0), f(x, t)\\rightarrow u(x, t)$ for $t \\in [0, T]$. Authors propose to collect dataset $u_i(x, t), i=1,\\dots,N$ and augment it with additional solutions and forcing terms $\\widetilde{u}_j(x, t),\\widetilde{f}_j(x, t),\\,j=1,\\dots,M$ constructed as follows:\n1. Select distinct $k\\_1, k\\_2\\in \\left(1,\\dots, N\\right)$\n2. Select $\\mu\\ll 1$ and small random perturbation $\\xi(x)$, $\\left\\|\\xi\\right\\|\\ll\\left\\|u_{k_1}\\right\\|$\n3. Compute $\\widetilde{u}(x, t) = u_{k_1}(x, t) + \\mu u_{k_2}(x, t) + \\xi(x, t)$ and $\\widetilde{f}(x, t)$ (not explained how this is done in case of general nonlinearity)\n4. Add new points to the dataset $\\widetilde{u}(x, t), \\widetilde{f}(x, t)$.\n\nAuthors propose to apply the scheme above along with a temporal coarsening to make augmentation computationally cheaper.\n\nThe approach is validated on three equations: Burgers, Navier-Stokes, KdV."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Data generation presents a main bottleneck for learning neural PDE solvers. The aim of research is to alleviate this problem, so it is well motivated.\n2. The approach authors propose is lightweight and can be used to generate a large number of additional solutions with minimal overhead."}, "weaknesses": {"value": "1. The method authors suggest is not completely specified.\n2. Numerical experiments are not convincing.\n\nI provide more details in the section below."}, "questions": {"value": "**The method is not completely specified**\n\nAuthors explain how their method works in Section 4. The main confusing part is Section 4.3. where authors show how to compute forcing terms for a particular equation only. How this technique can be extended to the general case of PDE given in equations (4) - (6) is not explained. The problem is Burgers equation has a very specific form of nonlinearity, and for other equations one will need to perform linearisation explicitly. Can the authors supply a missing general description of their method?\n\n**Numerical experiments are not convincing**\n\nIn my view authors provide little evidence that their method works as intended. To assess the quality of augmentation one needs to train a neural network with and without augmentation on the same data and compare test errors. This needs to be done for different numbers of samples in the train set.\n\nMore specifically, I suggest the following comparison protocol (number of samples below is adjustable):\n1. For selected equations generate a dataset by standard means with $N=1100$ sample.\n2. Split dataset on $1000$ train samples and $100$ test samples\n3. For $N_{s}$ in $[100, 200, 300, \\dots]$ do\n   a. Select $N_s$ samples from $1000$ train samples\n   b. Train neural network on these samples and record test error (on $100$ hold out samples)\n   c. Enrich dataset with augmented samples; train second neural network and record test error\n4. Report $E_{\\text{test}}^{(s)}\\pm \\text{std}$ with and without augmentation.\n\nIf in the experiment above one can gain substantial improvement in test accuracy by training on augmented dataset, this provides evidence that augmentation is useful. Authors did not perform this kind of test, so it is impossible to tell whether their augmentation works or not.\n\nFrom the data present one can not make any useful conclusion. For example, data from Table 1 shows that for augmented dataset accuracy drops, but only slightly. What does it imply? Maybe, it simply implies that the dataset is not challenging enough, and with $100$ samples one can already have decent accuracy.\n\nThis conclusion is partially supported by Table 2, where one can see that for small perturbation level accuracy drops but only slightly. Essentially, for perturbation $10^{-5}$ we almost have unperturbed data. This may suggest $100$ samples of unperturbed data is enough to reach mean relative test error $\\simeq 8\\%$.\n\n**Other questions:**\n1. What if modelled PDE does not have a source term?\n2. What to do when there are other parameters, e.g., diffusion coefficient $k(x)$ in Burgers equation $\\frac{\\partial u(x, t)}{\\partial t} + u(x, t)\\frac{\\partial u(x, t)}{\\partial x} = \\frac{\\partial}{\\partial x} k(x)\\frac{\\partial u(x, t)}{\\partial x}$? How augmentation is applied in this case?\n3. How the augmentation approach by author compares with other augmentation techniques for PDEs, e.g., https://arxiv.org/abs/2202.07643, https://arxiv.org/abs/2301.12730, https://arxiv.org/abs/2501.14604, etc\n4. Why do authors call the experiment in Section 6.5. an ablation study? In Section 6.5. authors compare their method HOPSS with another method Mixup, that they introduce in the Appendix. Mixup clearly generates out of distribution data and is superficially related to HOPSS. Typically, ablation studies include some form of \"perturbation\" of the proposed approach, e.g., \"we remove layers not from the architecture and report accuracy\". Can the authors explain why Section 6.5. constitutes ablation study?\n5. In many places when citation is added authors forgot to add whitespace characters. Examples: line 49, line 54, line 57, line 93, line 94, etc. Can the authors please correct that?\n6. I find the description on lines 111-121 confusing. For evolution PDEs, the standard way to describe numerical solutions is as follows. The discretisation is first done for spatial differential operators (method of lines). After that we are left with a system of ordinary differential equations that is solved with matching techniques, e.g., RK4, trapezoidal rule (=Crank-Nicolson), etc. Besides, it is not true that nonlinear terms always require linearisation. As the authors pointed below, there are IMEX schemes.\n7. In equation (3) authors divide by $(1 - \\frac{L}{2}\\Delta t)$. I see two problematic parts: (i) $L$ is a matrix, so in place of $1$ one should use identity matrix (the same is valid for the numerator); (ii) for inverse matrix a typical notation is $(I - \\frac{L}{2}\\Delta t)^{-1}$, it is not customary to represent inverse matrix as fraction.\n8. Complexity specified in lines 164-167 is doubtful. Authors did explain how spatial discretisation is performed and without that it is hard to estimate the cost of matrix inversion. Can the authors provide more details how they compute computation complexity? What is the number of data points $N$? What is a dimension of spatial discretisation $n$? What is the number of physical dimensions authors consider?\n9. Lines 193-194: \"In existing algorithms, we typically generate initial conditions and RHS of nonlinear temporal PDEs randomly, ...\" This is not the case. There is a standard statistical formulation for operator learning, see, e.g., https://arxiv.org/abs/2010.08895. We generate data from the specified distribution, because this is a setup of an operator learning problem. The distribution is chosen based on the application: it encodes a subset of interesting physical parameters."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k65ae6v8yg", "forum": "fScCawsZ77", "replyto": "fScCawsZ77", "signatures": ["ICLR.cc/2026/Conference/Submission18251/Reviewer_ked8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18251/Reviewer_ked8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913712120, "cdate": 1761913712120, "tmdate": 1762927978725, "mdate": 1762927978725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel data generation algorithm called HOmologous Perturbation in Solution Space (HOPSS) to accelerate the creation of large datasets for training deep learning-based PDE solvers, such as Neural Operators (NOs). The authors address the very important problem of generating a sufficient number of high-fidelity solution-RHS pairs for nonlinear PDEs. The authors demonstrate that HOPSS can significantly accelerate the data generation process (e.g., by $10 \\times$ for the Navier-Stokes equation) compared to traditional high-fidelity numerical solvers, while maintaining comparable performance when the data is used to train NOs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors consider the most significant bottleneck to scaling deep learning-based PDE solvers (NOs): the enormous computational cost and time required to generate large, high-difelity solutions-rhs datasets using traditional numerical solvers. \n2. The proposed results show that the HOPSS method achieves a substantial acceleration (e.g., $10 \\times$) while maintaining competitive accuracy for nonlinear equations (Burgers, KdV, Navier-Stokes)."}, "weaknesses": {"value": "1. The mathematical expression of the proposed process is:\n$$u_{\\text{new}} = u_i + \\mu \\cdot u_j + \\xi$$\n\nThis methodology assumes that perturbing a solution $u_i$ with another solution $u_j$ yields a physically related state. However, for nonlinear PDEs, where the operator is $\\mathcal{A}(u) = f$, the principle of superposition is violated, making it clear that $\\mathcal{A}(u_i + u_j) \\not\\equiv \\mathcal{A}(u_i) + \\mathcal{A}(u_j)$. Despite this, the entire method hinges on calculating the new source term $f_{\\text{new}}$ from this non-physical superposition.\n\n2. The authors must quantify this deviation, $\\varepsilon = \\mathcal{A}(u_{\\text{new}}) - f_{\\text{new}}$, or rigorously justify why the generated pair $(u_{\\text{new}}, f_{\\text{new}})$ remains \"physics-consistent\" despite violating the fundamental structure of the governing nonlinear PDEs.\n3. The methodology states that base solutions are initially generated at thousands of time steps and then aggressively downsampled to the dozens of time steps required for training. For highly transient or turbulent nonlinear systems, this downsampling step is a major source of information loss that occurs before the HOPSS perturbation.\nSpecifically, the following downsampling ratios were applied:\n\t1. **Navier-Stokes**. Initial resolution: grid size $= 128$, $\\Delta t = 1 \\cdot 10^{-3}$, downsampled resolution for training NO: grid size $=64$, $\\Delta t = 0.5$.\n\t2. **Burgers**. Initial resolution:  grid size $= 1024$, $\\Delta t = 5 \\cdot 10^{-3}$, downsampled resolution for training NO: grid size $=64$ and $\\Delta t = 5 \\cdot 10^{-2}$ .\n\t3. **KdV**. Initial resolution:  grid size $= 512$ with $10000$ time steps, downsampled resolution for training NO: grid size $=64$, $20$ time steps.\n4. The authors should compare the HOPSS method against similar, relevant methods that perform augmentation to properly contextualize its novelty and value."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DOjFqqpvMg", "forum": "fScCawsZ77", "replyto": "fScCawsZ77", "signatures": ["ICLR.cc/2026/Conference/Submission18251/Reviewer_n4Qs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18251/Reviewer_n4Qs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949647921, "cdate": 1761949647921, "tmdate": 1762927978409, "mdate": 1762927978409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HOPSS (Homologous Perturbation in Solution Space), a new data-generation framework designed to accelerate dataset creation for training neural operators that solve nonlinear temporal PDEs.\nConventional approaches require thousands of full numerical simulations to produce pairs of solutions and right-hand sides (RHS), resulting in severe computational bottlenecks.\nHOPSS mitigates this by generating new high-quality training data from a small set of base solutions using linear combinations and perturbations directly in the solution space. Specifically, two base solutions are randomly selected—one as a primary function and the other as a homologous perturbation term scaled by a small coefficient. By adding mild random noise and recomputing the corresponding RHS, HOPSS rapidly produces new valid PDE solution pairs.\nExperimental results show that this method reduces data generation time by roughly an order of magnitude while maintaining comparable accuracy when training neural operators such as FNOs on the generated datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper’s primary strength lies in addressing a critical bottleneck in scientific machine learning: the cost of generating training data for neural PDE solvers.\nHOPSS offers a conceptually simple yet powerful approach to drastically accelerate data creation without degrading data quality.\nEmpirical evaluations, especially on the Navier–Stokes equation, demonstrate 10× faster dataset generation while preserving comparable model accuracy. In some configurations, models trained with HOPSS-generated data even outperform those trained on conventional datasets, confirming the practical value of the proposed method.\nThe approach is also general and lightweight—it does not depend on any specific PDE form and can be applied to a wide range of nonlinear temporal systems. The idea of operating directly in the solution space rather than parameter or residual space represents an elegant perspective shift that could inspire further research.\nThe writing is clear, the algorithm is easy to understand and implement, and the reported results convincingly support the claimed advantages."}, "weaknesses": {"value": "Despite its promise, several aspects of the method remain underdeveloped.\nFirst, HOPSS heavily depends on the quality and diversity of the base solutions used to seed the perturbation process. If the initial base set poorly represents the global solution manifold, the synthesized samples may propagate its bias, reducing generalization to unseen PDE dynamics. The paper would benefit from a clearer strategy or criterion for selecting representative base solutions.\nSecond, the algorithm introduces new hyperparameters—the number of base solutions $N_b$, the perturbation strength $\\mu$, and the noise magnitude $\\xi$—which require manual tuning. The sensitivity analysis in the experiments shows that $\\mu$ has a strong influence on model accuracy, suggesting that additional tuning may be necessary for new PDEs, partially offsetting the claimed simplicity.\nThird, while recomputing the RHS ensures PDE consistency, there is no theoretical justification that the linear combination of two physically valid solutions remains within a physically meaningful manifold. The lack of a theoretical analysis on physical validity or statistical representativeness limits confidence in generalizing HOPSS-generated data to real-world systems.\nFinally, most experiments are limited to a few PDE types and specific temporal resolutions; testing HOPSS under different equations and time-step settings would help demonstrate robustness."}, "questions": {"value": "- How were the base solutions selected in your experiments? Do they represent diverse dynamics of the PDE, and how does their selection influence downstream generalization?\n- Since the perturbation level $\\mu$ significantly affects training outcomes, what guided your choice of $\\mu$, and is there a principled way to set or adapt it automatically?\n- Could the reliance on combinations of a limited set of base solutions lead to overfitting, causing the neural operator to specialize in those patterns rather than learning the underlying PDE operator?\n- Is there empirical evidence (e.g., spectral analysis or diversity metrics) showing that HOPSS-generated data adequately covers the true solution space of the PDE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "trvXpRw8Lk", "forum": "fScCawsZ77", "replyto": "fScCawsZ77", "signatures": ["ICLR.cc/2026/Conference/Submission18251/Reviewer_T3GZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18251/Reviewer_T3GZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970503262, "cdate": 1761970503262, "tmdate": 1762927977920, "mdate": 1762927977920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}