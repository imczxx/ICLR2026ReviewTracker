{"id": "xAH3nnjre6", "number": 5241, "cdate": 1757875074016, "mdate": 1763090916408, "content": {"title": "IPCP: Interpreter, Planner, Checker, and Painter Dialogue for Compositional Text-to-Image Generation", "abstract": "Text-to-image generation has advanced rapidly, but existing models still struggle with faithfully composing multiple objects and preserving their attributes in complex scenes. We propose IPCP, an interactive multi-agent dialogue framework with four specialized agents: Interpreter, Planner, Checker, and Painter that collaborate to improve compositional generation. The Interpreter adaptively decides between a direct text-to-image pathway and a layout-aware multi-agent process. In the layout-aware mode, it parses the prompt into attribute-rich object descriptors, ranks them by semantic salience, and groups objects with the same semantic priority level for joint generation. \nGuided by the Interpreter, the Planner adopts a divide-and-conquer strategy, incrementally proposing layouts for objects with the same semantic priority level while grounding decisions in the evolving visual context of the canvas.\nThe Checker introduces an explicit error-correction mechanism by validating spatial consistency and attribute alignment, and refining the layouts before they are rendered. \nFinally, the Painter synthesizes the image step by step, incorporating newly planned objects into the canvas to provide richer context for subsequent iterations. Together, these agents address three key challenges: reducing layout complexity, grounding planning in visual context, and enabling explicit error correction. Extensive experiments on compositional benchmarks GenEval and DPG-Bench demonstrate that IPCP substantially improves text–image alignment, spatial accuracy, and attribute binding compared to existing methods.", "tldr": "Multi-Agent system Compositional Text-to-Image Generation", "keywords": ["Text-to-Image Generation", "Multi Agent System"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/907edb5d6e95d169ca1520c31aa76c9d7078fc2d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper propose​ a novel multi-agent framework IPCP designed to tackle the challenge of ​​compositional T2I generation​​. IPCP consists of a divide-and-conquer strategy​​, grounding layout decisions in visual context​​, and an ​​explicit error correction process to achieve better T2I in a multi-agent manner."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper clearly points out the key limitations of existing methods like quadratic relational complexity, lack of visual grounding, and lack of error correction. The IPCP framework is elegantly designed to address each one directly.\n- The roles of the agents designed in IPCP are well-defined and logically complementary."}, "weaknesses": {"value": "- Each generation process requires multiple calls to an MLLM (for the Interpreter, Planner, and Checker) and iterative calls to the Painter. The total latency compared to a single-pass model is likely significantly higher. A discussion on the efficiency and inference time would be valuable for this paper. Is the performance gain balanced with the inference latency?\n- The paper uses powerful MLLMs like GPT-5 and Flux but provides limited information on the details of the prompts used in this process, hyperparameters, or the exact architecture of the MLLM agents.\n- While qualitative results are strong, a more detailed discussion of the IPCP's limitations and typical failure cases (for exmaple, what happens when the Interpreter misranks object priority? When the Checker fails to correct a major error?) would strengthen the paper.\n\nI am not very familiar with the topic (Text-to-Image) of this paper. Therefore, I hope authors, ACs, SACs, PCs could carefully consider my review comments and reduce its weight in the final decision."}, "questions": {"value": "- For the results in DPG-Bench, why IPCP outperforms baselines like OmniGen2 in the overall performance but underperforms in most of other metrics? I found that in specific metrics, IPCP performs worse than OmniGen2 in 4 out of 5 metrics, yet it significantly outperforms it in overall performance. Is this evaluation method reasonable? Are there any inconsistencies or incoherencies here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TB8CyhVv6t", "forum": "xAH3nnjre6", "replyto": "xAH3nnjre6", "signatures": ["ICLR.cc/2026/Conference/Submission5241/Reviewer_X5tM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5241/Reviewer_X5tM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761466255124, "cdate": 1761466255124, "tmdate": 1762917969151, "mdate": 1762917969151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "V7XCre9IP1", "forum": "xAH3nnjre6", "replyto": "xAH3nnjre6", "signatures": ["ICLR.cc/2026/Conference/Submission5241/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5241/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763090915020, "cdate": 1763090915020, "tmdate": 1763090915020, "mdate": 1763090915020, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents the IPCP framework, in which four intelligent agents collaborate to generate images: 1) Interpreter, which adaptively selects between direct text-to-image generation and a layout-driven workflow; 2) Planner, which designs the overall layout for the image; 3) Checker, which verifies and refines spatial and attribute consistency; and 4) Painter, which progressively renders each instance onto the final image. This approach demonstrates strong performance on text-to-image benchmarks; however, the paper omits several crucial technical details and contains certain ambiguities, leaving the overall presentation incomplete."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The results show that the proposed method performs strongly and is logically coherent."}, "weaknesses": {"value": "The paper omits several crucial technical details and contains certain ambiguities, leaving the overall presentation incomplete. For specific issues, please refer to the Questions section."}, "questions": {"value": "My main concerns are as follows. If these issues are addressed and my doubts clarified, **I would consider raising the score**:\n\nQuestion 1: The paper states that it employs “3DIS: A 3D-aware diffusion model for instance-level synthesis” for layout-to-image generation. However, I could not find any work with this exact title, which makes it unclear what specific method was actually used. This ambiguity is likely to confuse readers.\n\nQuestion 2: The collaboration between the Planner and the Painter requires iteratively generating each object. The paper does not explain how this is implemented. Is this achieved using image editing techniques? I believe this is a critical technical detail that should be explicitly described.\n\nQuestion 3: The proposed method follows an iterative generation paradigm, rendering one object at a time. How does this approach differ from conventional layout-to-image methods that generate multiple objects in a single step? This comparison is important for understanding the contribution.\n\nQuestion 4: The paper should present examples showing the intermediate results of the full pipeline execution. Could you illustrate how multiple agents collaborate step by step? For instance, how does the Painter progressively render each object, and how does the Checker detect and correct errors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UK93ESUfa7", "forum": "xAH3nnjre6", "replyto": "xAH3nnjre6", "signatures": ["ICLR.cc/2026/Conference/Submission5241/Reviewer_n2fr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5241/Reviewer_n2fr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761492972875, "cdate": 1761492972875, "tmdate": 1762917968792, "mdate": 1762917968792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents IPCP, an interactive multi-agent dialogue framework for text-to-image generation, comprising four specialized agents: Interpreter, Planner, Checker, and Painter. The method demonstrates strong performance on GenEval and DPG-Bench for compositional scene generation, multi-object layouts, and attribute alignment. The work is novel, well-motivated, and experimentally thorough."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper has a clear and well-structured organization.\n\n2. The proposed method is reasonable and feasible.\n\n3. The experiments demonstrate that their approach is effective to some extent."}, "weaknesses": {"value": "1.The experiments are somewhat limited, as comparisons are made only with T2I methods.\n\n2.The explanations of each component could be clearer—for example, by providing more visualizations or showing the multi-level generation step by step.\n\n3. Some references in the paper are incorrect.\n\n4. See in questions."}, "questions": {"value": "1. It would be more convincing if the Planner could generate multiple layouts at once and compare them using the L2I model, or if comparisons with existing image-generation agents were provided.\n\n2. Are there examples that demonstrate very complex scenes? Or could the paper show a full end-to-end generation process?\n\n3. When generating layouts at different priority levels, can the Planner observe objects from other levels to arrange positions more reasonably? For example, if the most important object is accidentally placed at the far right, a less important object may not have space to be placed to its right—does this happen, and can the Checker correct such cases?\n\n4. In Table 4, introducing the layout-aware mode causes a drop in the Global metric, and adding Visual context leads to a significant drop in the Other metric—could the authors explain the reasons for this?\n\nTypo: Some references in the paper are incorrect, e.g., 3DIS is cited incorrectly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CuUJRkcbDf", "forum": "xAH3nnjre6", "replyto": "xAH3nnjre6", "signatures": ["ICLR.cc/2026/Conference/Submission5241/Reviewer_Csij"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5241/Reviewer_Csij"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553893694, "cdate": 1761553893694, "tmdate": 1762917968418, "mdate": 1762917968418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes IPCP, an interactive multi-agent dialogue framework for compositional text-to-image generation.\nFour agents—Interpreter, Planner, Checker, and Painter—collaborate in an iterative loop. \nExperiments on GenEval and DPG-Bench show large gains over strong baselines such as T2I-Copilot, GoT, and GPT Image, achieving new SOTA performance in compositional fidelity, spatial accuracy, and attribute binding. Ablation studies confirm the importance of visual grounding and the error-correction mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The four-agent IPCP pipeline (Interpreter–Planner–Checker–Painter) introduces modular collaboration rather than a fixed pipeline, enabling flexible adaptation between direct and layout-aware modes.\n2. The paper clearly articulates three pain points—layout complexity, missing visual grounding, and lack of explicit correction—and ties each to a corresponding agent.\n3. The paper is easy to follow."}, "weaknesses": {"value": "1. The authors claim that the proposed method is designed to handle multiple objects and preserve their attributes in complex scenes. However, based on the visualizations provided, the generated examples involve rather simple scenes with only around five objects on average. There is a lack of evaluation on genuinely complex or densely populated scenes that would better test the robustness of the proposed layout-planning mechanism. In such scenarios, would the number of agent calls increase significantly, thereby introducing substantial time and resource costs? A quantitative analysis of scalability under complex settings is needed.\n\n2. Although the automatic metrics (GenEval and DPG-Bench) are comprehensive, a small-scale human evaluation would be valuable to better assess the perceived compositional quality and visual faithfulness of the generated images.\n\n3. Regarding the agent design, the paper introduces four fixed roles (Interpreter, Planner, Checker, and Painter), each with a predefined workflow. However, it remains unclear whether such fine-grained role separation is truly necessary. Could a simpler configuration, such as a creator and a feedback provider, achieve similar effects? Overall, the innovation appears somewhat limited; the design feels more system-driven than principle-driven, without a deep analysis of why an agentic paradigm is inherently suitable for text-to-image generation or how each role contributes indispensably to the overall performance. As it stands, the contribution seems largely engineering-oriented, with modest conceptual novelty."}, "questions": {"value": "1. How sensitive is IPCP to the quality of the underlying T2I/L2I model? For instance, if FLUX is replaced with SDXL, does performance degrade proportionally?\n\n2. Can the Checker handle non-rigid or abstract relations (e.g., “a shadow beneath the cup”)?\n\n3. Did you experiment with different ordering heuristics besides semantic salience for prioritizing objects?\n\n4. How robust is VCoT when the partial canvas contains hallucinated geometry? Is error propagation a problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JKNi45K6HJ", "forum": "xAH3nnjre6", "replyto": "xAH3nnjre6", "signatures": ["ICLR.cc/2026/Conference/Submission5241/Reviewer_sWXY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5241/Reviewer_sWXY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918533897, "cdate": 1761918533897, "tmdate": 1762917968214, "mdate": 1762917968214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}