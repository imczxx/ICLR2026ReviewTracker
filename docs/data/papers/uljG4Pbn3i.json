{"id": "uljG4Pbn3i", "number": 20877, "cdate": 1758311290701, "mdate": 1759896954108, "content": {"title": "Benchmarking Mitigations For Covert Misuse", "abstract": "Existing language model safety evaluations focus on overt attacks and low-stakes tasks. In reality, an attacker can easily subvert existing safeguards by requesting help on small, benign-seeming tasks across many independent queries. Because individual queries do not appear harmful, the attack is hard to detect. However, when combined, these fragments *uplift misuse* by helping the attacker complete hard and dangerous tasks. Toward identifying defenses against such strategies, we develop *Benchmarks for Stateful Defenses* (BSD), a data generation pipeline that automates evaluations of covert attacks and corresponding defenses. Using this pipeline, we curate two new datasets that are consistently refused by frontier models and are too difficult for weaker open-weight models. This enables us to evaluate decomposition attacks, which are found to be effective misuse enablers, and to highlight stateful defenses as both a promising and necessary countermeasure.", "tldr": "Evaluation for realistic covert/decomposition attacks for misuse, and benchmarking stateful defenses.", "keywords": ["AI safety", "LLM misuse", "adversarial attacks", "evaluation", "adversarial training"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a5115851d7472d6b89dbc643719fb780d52015a0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the limitation of existing LLM safety evaluations that focus only on overt misuse, by systematically studying covert decomposition attacks—where attackers split a harmful goal into multiple benign-looking sub-queries to evade refusal and still achieve malicious outcomes. The authors introduce BSD (Benchmarks for Stateful Defenses), a new evaluation framework that automatically generates biosecurity and cybersecurity questions which are difficult, consistently refused by safety-trained models, yet solvable by unaligned models. BSD enables measurement of both misuse uplift (how much strong models inadvertently help attackers) and detectability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles an underexplored yet realistic threat, covert multi-query misuse, shifting the focus from single-prompt safety to stateful, multi-turn risk assessment.\n\nThe proposed BSD pipeline systematically generates difficult, refused, and answerable misuse questions, filling a major gap in current LLM safety evaluation."}, "weaknesses": {"value": "The BSD dataset is not publicly released, which significantly hinders reproducibility and independent validation of the results.\n\nThe evaluation focuses mainly on biosecurity and cybersecurity; other plausible misuse domains such as misinformation or social manipulation remain untested, limiting generalizability.\n\nThe study does not deeply examine adaptive attacker strategies (e.g., cross-account evasion, prompt obfuscation)."}, "questions": {"value": "How do you ensure that the BSD-generated tasks remain representative of real-world covert misuse scenarios rather than artificial examples tailored to model refusal behavior?\n\nHave you tested whether decomposition attacks still succeed when the “strong” and “weak” models are replaced with different architectures or model families (e.g., Claude vs Gemini)?\n\nCould adaptive attackers deliberately design sub-queries to mimic benign user behavior or cross multiple accounts to evade the buffer defense, and how might you detect this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0FJVxCvgWQ", "forum": "uljG4Pbn3i", "replyto": "uljG4Pbn3i", "signatures": ["ICLR.cc/2026/Conference/Submission20877/Reviewer_fhmj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20877/Reviewer_fhmj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760768967684, "cdate": 1760768967684, "tmdate": 1762937474584, "mdate": 1762937474584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The paper introduces Benchmarks for Stateful Defenses (BSD), a pipeline to evaluate model safety against decomposition attacks, where an attacker breaks a harmful task into multiple benign queries.\n\n2. The paper argues that existing single-prompt defenses are insufficient and highlight stateful defenses, which keep track of a user's query history, as a necessary and promising countermeasure."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly explains the concept of misuse uplift, which is the incremental advantage a strong, guarded model provides over a weak, unsafe model when solving a harmful task. \n\n2. The authors present an automated data generation pipeline (BSD) that successfully curates datasets in biosecurity and cybersecurity, filtered to be difficult for weak models and answerable but consistently refused by strong, safe models.\n\n3. The paper evaluates defenses, showing that adversarially training a prompt-level detector offers some benefit, and proposes a novel stateful buffer defense that tracks suspicious queries over time."}, "weaknesses": {"value": "1. The final curated benchmark only consists of 50 biology questions and 15 cybersecurity questions. This limited size may not be comprehensive enough to draw broad conclusions about model safety.\n\n2. ⁠The entire decomposition attack chain relies on a critical assumption: that a \"weak, unsafe model W\" (e.g., Qwen2.5-7B) is capable of intelligently decomposing a difficult, harmful task (X) that it cannot solve itself. This decomposition is itself a highly advanced planning and reasoning task. The paper does not sufficiently justify why a model that is \"weak\" at solving a problem would simultaneously be \"strong\" at planning its decomposition. If the weak model's decomposition quality is poor, the entire attack chain would fail.\n\n3. I think task decomposition is nothing new in LLM literature. This makes the paper's novelty very limited."}, "questions": {"value": "See weaknesses and here are some additional questions:\n\n1. Figure 3 indicates that 2815 generated questions were filtered out because they were not refused by strong, safe models. Does this high number of answered harmful questions itself represent a significant finding?\n\n2. While state-based defences can protect against a user who decomposes a task into many individual queries, could an attacker simply bypass this defense by distributing their decomposed queries across multiple, hard-to-link accounts. Does this limit the defense's real-world viability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lmbrpa7cIR", "forum": "uljG4Pbn3i", "replyto": "uljG4Pbn3i", "signatures": ["ICLR.cc/2026/Conference/Submission20877/Reviewer_EwCY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20877/Reviewer_EwCY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955352653, "cdate": 1761955352653, "tmdate": 1762937473419, "mdate": 1762937473419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a benchmark to address a critical gap in AI safety evaluations: existing benchmarks fail to capture realistic misuse patterns where attackers can decompose harmful tasks into benign-seeming queries (decomposition attacks). The authors introduce BSD (Benchmarks for Stateful Defenses), a dataset generation pipeline that produces 50 biology and 15 cybersecurity questions that are (1) difficult for weak models, (2) refused by safety-trained models, and (3) answerable by helpful-only strong models. They evaluate decomposition attacks and find that such achieve significant misuse uplift while evading prompt-level detection. The paper proposes stateful defenses that maintain buffers of suspicious queries across user sessions, showing promise but remaining imperfect against adaptive attacks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The threat model is well-motivated by actual incidents. The observation that attackers use benign-looking queries in practice, not direct harmful requests, is crucial and underexplored.\n- Comparing decomposition with 5 other attack types (Adaptive, PAIR, Adversarial Reasoning, Crescendo, improved decomposition) under equal query budgets (12 calls) is methodologically sound and provides useful baselines.\n- The finding that decomposition attacks are much harder to detect (AUC 0.60) than jailbreaks (AUC 0.71) even with adversarial training is significant and actionable for defenders."}, "weaknesses": {"value": "- Since the uplift is defined as difference in accuracy between a weak and a strong model, and Qwen2.5-7B is used as the weak models in most experiments in the paper, the reported \"uplift\" numbers are inflated. For clarity in the paper, the metric should be called \"relative uplift\" so that it is clear to a reader who is skimming that the uplift numbers are relative to Qwen2.5-7B. Reporting the relative uplift numbers against multiple open-weights models would also be insightful. \n-  The paper states in Section 4.1 that \"each attack is run for five epochs\" but still never reports variance. Table 1 doesn't have any confidence intervals. Figure 7 (left) also misses error bars which makes it difficult to refute that the scaling trend is within noise (no significance tests). This is particularly problematic since the benchmark has only 50 questions which could result in significant variance.\n- Some of the defender assumptions in the threat model are not validated\n    - Assumes defender can reliably track users across sessions (trivial to circumvent with multiple accounts/IPs)\n    - No analysis of false positive rates on benign users doing legitimate multi-step research\n- Figure 3 flow diagram is confusing (numbers don't clearly show what each filter removes)\n- Heavy appendix reliance (key results like Appendix G relegated to supplementary)\n- \"Restricted release\" of dataset is understandable but limits reproducibility.\n \nWeak baseline and missing error bars and significance tests leads me to weak reject the paper in the current form. Addressing these should make the paper stronger."}, "questions": {"value": "- What do intermediate helpful-only models score on BSD? Please add a table showing direct query accuracy for:\n  - Qwen2.5-7B (current baseline)\n  - Qwen2.5-72B (no safety training)\n  - Llama-3.1-405B (no safety training)\n  - Any 400B+ uncensored model you can access\n  \n  This is essential to validate that uplift is real and not inflated by weak baseline choice.\n- Why not test decomposition + jailbreak hybrid in main paper? Appendix G shows this is effective (87% vs 84%) - this seems like the realistic threat model.\n- What fraction of legitimate users doing multi-step research get flagged by your stateful defense at various thresholds?\n- Have biosecurity or cybersecurity experts validated that your questions are genuinely harmful and your generated answers would facilitate misuse? A dataset that claims to measure uplift should consult with experts on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1UsMUOwpxu", "forum": "uljG4Pbn3i", "replyto": "uljG4Pbn3i", "signatures": ["ICLR.cc/2026/Conference/Submission20877/Reviewer_jAQG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20877/Reviewer_jAQG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762447412138, "cdate": 1762447412138, "tmdate": 1762937471858, "mdate": 1762937471858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles an important gap in AI safety evaluations by introducing a framework to measure \"misuse uplift\" through decomposition attacks. The authors argue that existing safety benchmarks are too easy and don't capture real-world threat scenarios where attackers break harmful tasks into benign-looking queries. They develop BSD (Benchmarks for Stateful Defenses), a data generation pipeline producing questions that are difficult for weak models yet consistently refused by frontier models. The evaluation reveals that decomposition attacks can significantly increase misuse rates while evading prompt-level defenses, and proposes stateful defenses that track user query histories as a countermeasure.\nThe core contribution is formalizing the misuse uplift concept (Δ = r_attack - r_weak) and creating evaluation infrastructure that properly measures it. The paper includes extensive experiments across multiple frontier models, various attack methods, and defense mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The threat model is grounded in reality. The Las Vegas attack example and the employment fraud case study effectively motivate why we need to move beyond simple refusal-based evaluations. The distinction between \"can this model do harm\" versus \"does this model provide incremental advantage over alternatives\" is genuinely useful for thinking about deployment risks.\n2. The filtering pipeline is well designed as it requires unanimous agreement from multiple strong models for ground truth, combined with consistent refusal from safety-trained models and difficulty for weak models, creates questions that meet clear criteria. The correlation analysis showing BSD performance tracks biological reasoning ability (ρ=0.94) while WMDP doesn't (ρ=0.11) is convincing evidence the questions measure genuine capability rather than memorized facts.\n3. The stateful defense direction is novel for LLMs. While stateful defenses exist in computer vision, adapting them to track suspicious patterns across user sessions is an interesting contribution. The buffer-based approach that maintains the top-m most suspicious queries is practical and shows measurable improvements over naive rolling windows."}, "weaknesses": {"value": "1. The decomposition attack improvements feel incremental. Section 6 presents fine-tuning the decomposer on 700 benign MMLU examples as a major contribution, but the gains are modest (Table 1). For instance, on Claude 3.5 the improvement is 41.6% to 46.0%, and the ablation in Table 3 shows minimal difference when no strong model is involved. This suggests the technique is more about optimizing prompt engineering than discovering a fundamental new attack vector. The claim that this represents \"state-of-the-art\" performance needs more justification given the limited scope.\n2. Dataset release strategy undermines reproducibility. I understand the safety concerns, but committing to \"restricted release under controlled access only\" without specifying concrete access criteria makes it difficult to assess how useful BSD will be for the research community. The paper would benefit from either a clearer access policy or a compelling argument for why this approach is necessary when WMDP, which faces similar concerns, has been publicly released. The cybersecurity subset has only 15 questions, which seems insufficient for drawing strong conclusions about that domain.\n3. Stateful defense evaluation lacks key details. Section 5.2 introduces the buffer defense but several important aspects remain unclear. How is the \"harmfulness score\" for individual prompts computed? The paper mentions using \"adversarially trained Llama-Guard\" but doesn't specify whether this uses the same 20% train split or different data. The choice of m=30 for buffer size appears arbitrary. Most critically, the false positive rate discussion is limited. Figure 6 shows precision dropping substantially as the fraction of harmful queries decreases, but there's no analysis of how this would affect legitimate power users who make many technical queries.\n4. The weak/strong model distinction relies on fragile assumptions. Appendix B argues extensively that attackers will have access to both weak helpful-only models and strong safety-trained models. While currently plausible, this assumption could break down quickly. The paper acknowledges open-weight models are catching up (they cite DeepSeek and note the 6-month gap is closing) but doesn't adequately address how this affects the threat model's validity. If Qwen3-235B or similar models reach GPT-4 level performance, the entire misuse uplift framework becomes less meaningful because the attacker wouldn't need the strong model at all.\n5. Statistical rigor is limited. The paper reports single accuracy numbers without confidence intervals or significance tests. Given that BSD has only 50 biology questions, and some models answer stochastically, uncertainty quantification would strengthen the claims. The paper mentions running decomposition \"for five epochs\" and \"at least 4 out of 5 runs\" but doesn't consistently report variance across runs. This makes it hard to assess whether observed differences are meaningful or within noise.\n6. The relationship between detectability and utility is underexplored. The paper demonstrates that decomposition attacks are harder to detect than jailbreaks (Figure 5, AUC 0.60 vs 0.71) and that adversarial training helps (AUC increases to 0.76). However, there's no discussion of the tradeoff between detection accuracy and false positives in production settings. What happens to a legitimate bioinformatics researcher asking detailed questions about viral mechanisms? The precision-recall curves in Figure 6 show concerning drops at low base rates, but this isn't connected back to deployment recommendations."}, "questions": {"value": "1. You filter down from 4800 candidates to 50 final questions (about 1% retention rate). This seems extremely aggressive. What are the most common failure modes? Do questions fail primarily at the \"strong model agreement\" stage, the \"safety-trained model refusal\" stage, or the \"weak model difficulty\" stage? Understanding the bottleneck would help assess whether the pipeline generalizes to other domains beyond biology and cyber.\n\n2. The paper focuses on decomposition attacks because they're effective and hard to detect. But Section 4.1 shows that for some models (o3-mini, GPT-4o), traditional jailbreaks like adversarial reasoning achieve similar or better misuse rates (Table 1). Under what conditions should we expect decomposition to be the preferred attack strategy versus just investing more effort in jailbreaking? The paper would benefit from a decision tree or framework for when each attack class is most effective.\n\n3. How does the buffer handle concept drift over time? If a user's interests legitimately shift toward more technical topics, could their older benign queries dilute the suspiciousness score? Conversely, what prevents an attacker from gaming the system by front-loading thousands of benign queries before attempting misuse? The adaptive attack in Section 5.2 touches on this but assumes random interleaving. A more strategic attacker might optimize the temporal pattern.\n\n4. You compare against several jailbreak methods but not against other decomposition approaches. Glukhov et al. 2024 also proposes decomposition attacks. How does your improved decomposition method (fine-tuned decomposer + variable n) compare quantitatively to their approach on the same tasks? This would help establish whether the gains come from better evaluation infrastructure or genuinely better attacks.\n\n5. The biology questions achieve strong results but cybersecurity shows more modest uplift (Figure 8). You attribute this to lower baseline refusal rates but don't deeply investigate. Are cybersecurity tasks fundamentally different in ways that make decomposition less effective? Or is this about BSD-cyber having easier questions? Testing the same pipeline on additional domains (chemistry, general engineering, persuasion) would clarify whether this is a principled evaluation framework or one tailored to specific biology hazards.\n\n6. You fine-tune on MMLU-auxiliary decompositions from o3-mini with n varying from 3 to 12. Why this particular distribution? Did you experiment with other training data compositions (pure n=12, or weighted toward higher n)? Also, the ablation shows minimal improvement without a strong model, but have you verified the fine-tuned decomposer doesn't leak information about the answer by generating leading sub-questions?\n\n7. The stateful defense requires tracking user history across sessions and maintaining buffers. What's the computational overhead? How does this scale to millions of users? More importantly, how do you handle shared accounts or VPNs where multiple distinct users might appear as one \"user\" to the tracking system? These practical concerns seem important for real deployment but aren't addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3SR0nQGEYi", "forum": "uljG4Pbn3i", "replyto": "uljG4Pbn3i", "signatures": ["ICLR.cc/2026/Conference/Submission20877/Reviewer_wohN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20877/Reviewer_wohN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762486601674, "cdate": 1762486601674, "tmdate": 1762937470598, "mdate": 1762937470598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}