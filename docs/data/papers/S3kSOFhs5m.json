{"id": "S3kSOFhs5m", "number": 11757, "cdate": 1758203552001, "mdate": 1763699477477, "content": {"title": "Quantifying Cross-Attention Interaction in Transformers for Interpreting TCR-pMHC Binding", "abstract": "CD8+ “killer” T cells and CD4+ “helper” T cells play a central role in the adaptive immune system by recognizing antigens presented by Major Histocompatibility Complex (pMHC) molecules via T Cell Receptors (TCRs). Modeling binding between T cells and the pMHC complex is fundamental to understanding basic mechanisms of human immune response as well as in developing therapies. While transformer-based  models such as TULIP have achieved impressive performance in this domain, their black-box nature precludes  interpretability and thus limits a deeper mechanistic understanding of T cell response. \nMost existing post-hoc explainable AI (xAI) methods are confined to encoder-only, co-attention, or model-specific architectures and cannot handle encoder-decoder transformers used in TCR-pMHC modeling. To address this gap, we propose Quantifying Cross-Attention Interaction (QCAI), a new post-hoc method designed to interpret the cross-attention mechanisms in transformer decoders. Quantitative evaluation is a challenge for XAI methods; we have compiled TCR-XAI, a benchmark consisting of 274 experimentally determined TCR-pMHC structures to serve as ground truth for binding. Using these structures we compute physical distances between relevant amino acid residues in the TCR-pMHC interaction region and evaluate how well our method and others estimate the importance of residues in this region across the dataset. We show that QCAI achieves state-of-the-art performance on both interpretability and prediction accuracy under the TCR-XAI benchmark.", "tldr": "We introduce QCAI, a post-hoc method that incorporates cross-attention into explanations for encoder-decoder transformers; we apply it to analyze TCR–pMHC binding and show that it can successfully interpret experimentally observed interactions.", "keywords": ["CD4+ T cell response", "epitope prediction", "explainable AI", "multi-modal learning", "transformer models", "deep learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/926e9fa3790b7fbc5386a6aa6c59bf6825b54252.pdf", "supplementary_material": "/attachment/3050b416ff8d4a5157d9040e7d4fcc258c05e575.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed QCAI to interpret encoder-decoder transformer models for the TCR-pMHC binding task, where existing explainability (XAI) methods are inadequate for the asymmetric nature of cross-attention. It combines gradient-based scoring with a Moore-Penrose pseudoinverse decomposition and element-wise score aggregation to attribute quantitative contributions back to the distinct query and key input tokens. The paper establishes the TCR-XAI benchmark, a curated dataset of 274 experimentally determined protein structures that utilizes atomic distance as a ground truth for interaction importance. Also  included is direct-comparison metric, the Binding Region Hit Rate (BRHR), to confirm QCAI's ability to generate quantitativeuperior and biologically plausible insights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method, which interprets the complex cross-attention mechanism, effectively addresses the application gap between the existing post-hoc explainable AI methods and scientific research problems conceptually.\n\n3. The development of the TCR-XAI benchmark is useful for evaluation, grounding XAI performance in objective, physical atomic distances rather than subjective metrics.   \n\n4. A comprehensive validation framework is used that employs a suite of metrics, including ground-truth correspondence, perturbation influence, and the novel Binding Region Hit Rate."}, "weaknesses": {"value": "1. **Ambiguity in Cross-Attention Inputs:** The paper does not clearly define the roles of the input sequences (CDR and Peptide) within the TULIP model's cross-attention mechanism (Figure 1). It is ambiguous which sequence provides the query vectors and which provides the key and value vectors. Given that the TULIP model is autoregressive and can predict any one sequence conditioned on the others, these roles may be dynamic. A detailed visualization of this process, **explicitly tagging each component with its corresponding input entity (e.g., CDR3α as query, Peptide as key)** , would be highly beneficial for interpreting the model's mechanics and the application of the QCAI method.\n\n2. **Clarification of the Core Methodological Distinction:** The description of the gradient-based importance score does not clearly articulate how its computation differs for cross-attention versus self-attention. In particular, the manuscript does not say what is **unique or nontrivial about handling cross-attention compared to standard self-attention**. A more explicit treatment of these differences would help clarify the specific methodological contribution of QCAI, beyond applying generic gradient-weighted attention analysis.\n\n3. **Experimental Setting around Attention Modules:** In the current experiments, the baseline methods are evaluated only on self-attention layers, with cross-attention omitted, whereas QCAI is evaluated on both self-attention and cross-attention. This raises two concerns. First, part of QCAI’s performance gain may simply come from exploiting additional cross-attention information that the baselines are not allowed to use, which makes the comparison potentially unfair. Second, because QCAI is only reported in its combined form, it is not possible to disentangle how much of the improvement is due to self-attention versus cross-attention. It would substantially strengthen the empirical analysis to report ROC-AUC, LOdds, and AOPC for **(i) QCAI applied only to self-attention and (ii) QCAI applied only to cross-attention**, in order to quantify the individual contribution of each attention component."}, "questions": {"value": "Although transformer models offer promising results, its positional encoding scheme is not suited for modelling the positional relations in interacting sequences, because (1) relative distance between residues across sequences are unknown and have to be inferred first; (2) existing positional encoding vectors are not physically meaningful in the first place. Therefore, while it is useful to provide explanations on a complex, data-driven black-box model, it could be more significant to devise a model that itself accord with first principles (or at least provide physically meaningful intermediate results). The author may refer to the recent work entitled \"Sliding-attention transformer neural architecture for predicting T cell receptor-antigen-human leucocyte antigen binding\" in Nature Machine Intelligence, 2024, 6(10): 1216-1230. \n\nTypos:\nLines 197–198: The phrase ‘denotes calculate max among feature dimension’ is ungrammatical.\nLines 407–408: The phrase ‘critical contacts with peptide peptides’ contains a redundant repetition of ‘peptide.’\nLine 805: The text ‘was proposedAbnar & Zuidema’ is missing both a space and the preposition ‘by’."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v83Loke7B6", "forum": "S3kSOFhs5m", "replyto": "S3kSOFhs5m", "signatures": ["ICLR.cc/2026/Conference/Submission11757/Reviewer_dbL7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11757/Reviewer_dbL7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744279172, "cdate": 1761744279172, "tmdate": 1762922784573, "mdate": 1762922784573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank the reviewers for their detailed comments. We have addressed all concerns with additional experiments, clarification, or discussion. Below is an initial overview of our responses to the major points that we identified in reviewer comments. **Detailed, point-by-point replies for each reviewer questions will follow in the remained of the rebuttal period.** References such as `L#` and `Fig.#` indicate changes in the revised manuscript, with updates highlighted in blue.\n## Major Comments\nThese are the major concerns we identified from reviewer feedback. We have addressed them by clarifying the questions and/or adding experiments to support our responses.\n\n1. **TCR-XAI Benchmark**  \nAs requested by the reviewer, we clarify that the TCR-XAI benchmark is not curated from the TULIP training dataset. It includes 176 distinct epitopes, none appearing in more than 3.3% of samples (`Appendix A.9`, `L1194`). We also compared BRHR across samples grouped by Levenshtein distance to the TULIP training set (`Appendix Table 5`), showing that QCAI's performance remains stable even as samples diverge.\n\n2. **Application of QCAI to Other Domains**  \nWhile a full demonstration of QCAI in another domain would require a substantial time commitment, we did implement a proof-of-concept applicaiton in which QCAI is applied to analyze the grounding performance of CLIP with cross-attention, a vision-language model. As shown in `Appendix Figure 15` and `Appendix A.12`, QCAI can identify interactions between the text and image modalities in cross-attention and highlight the relative importance of the image and text for a given classification label. This is similar to identify the binding elements across two inputs. \n\n3. **Utilizing Energy Function as a Proxy for Ground-Truth Importance**  \nThe reviewer poses an interesting idea as residue-level energetics incorporate physicochemical information that may provide additional detail about binding contributions. We attempted a simple implementation using PyRosetta but could not obtain the necessary inter-unit energy terms for an appropriate implementation. We will add some discussion about this idea in discussion of future work.\n\n4. **Relationship Between TULIP Prediction Confidence and Importance**  \nAs requested by the reviewer, we investigated the correlation between prediction confidence and importance, as discussed in `Appendix A.8` (`L1152`). Using both MixTCRpred and TULIP, we found that samples with lower prediction confidence exhibit reduced explanation quality compared to high-confidence samples.\n\n5. **QCAI on Cross- vs. Self-Attention**  \nAs requested by the reviewer, we conducted an ablation study by applying QCAI only to cross- and self-attention, as reported in `Appendix A.11`. The results indicate that QCAI applied to cross-attention is the primary contributor to the final explanation and cross-attention plays a significant role in transformers that incorporate cross-attention. We will add this discussion to Results section.\n\n6. **Nontrivial Differences Between Handling Self- and Cross-Attention**  \nAs requested by the reviewer, we have emphasized this distinction in `Section 2.1` (`L128`), highlighting two main points: (1) the attention matrix of cross-attention fuses information from two modalities, and (2) it is no longer a square matrix, making direct measurement infeasible.\n\n7. **Role of Input Sequences in TULIP**  \nAs requested by the reviewer, we have clarified the roles of the epitope peptide, CDR3a, and CDR3b in TULIP, as detailed in `Appendix A.4` (`L892`). Specifically, peptide features serve as the query, while CDR3a and CDR3b features function as the keys and values in the cross-attention mechanism.\n\n8. **Random Performance of Competitors**  \nIn response to the reviewer's question regarding why some competitors perform worse than ``random,'' we clarify the following. First, this is a challenging task, as the models have no access to structural information. Second, unlike binary TCR-pMHC binding prediction, this task requires identifying contacting residues, where random guessing can naturally yield worse-than-random performance. In addition, competing methods cannot access cross-attention and therefore miss critical information. As an evidence, GradCAM, applied only to the last layer, benefits from direct gradient information and consequently performs better than other baselines."}}, "id": "GVskJ5A9EJ", "forum": "S3kSOFhs5m", "replyto": "S3kSOFhs5m", "signatures": ["ICLR.cc/2026/Conference/Submission11757/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11757/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11757/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763699787044, "cdate": 1763699787044, "tmdate": 1763699787044, "mdate": 1763699787044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a method for the interpretation of TCR-pMHC binding prediction models with encoder-decoder structure and cross attention mechanisms, as well as a benchmark dataset of ground truth interacting residues defined by residue distance.. Specifically, the proposed method, quantifying cross-attention interaction (QCAI), combines a GradCAM-style intrinsic importance and a relevance score derived from the attention matrix. The authors demonstrated superior performance of the proposed importance in the task of binding site retrieval by log-odds and area under perturbation metrics."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1\\. This work offers insights into the important problem of interaction interpretation in immune-proteins. Considering the scarcity of data and limited prediction performance of existing models, a reliable interpretation method would allow maximal use of available data and models. The method and findings could potentially guide rational design of TCRs and respective immunotherapy.\n\n2\\. The authors provide solid theoretical justifications of the methodology as well as practical insights.\n\n3\\. The work offers a useful benchmark dataset for future work.\n\n4\\. Case studies with visualization clearly demonstrate the biochemical implications of the proposed importance scores."}, "weaknesses": {"value": "Despite the sound problem setup and results, my major concern is the limited application both within and beyond the domain of TCR-pMHC. Specifically:\n\n1\\. The scope of the defined task and the respective benchmark dataset is somewhat limited. Distance is not the only indicator of interaction and only weakly indicates \"importance\" overall, considering residue contributions to TCR-pMHC interactions are somewhat additive (smaller, weaker interactions than dominating hotspots). Though that may be hard to analyze with the current tools, it should be briefly discussed.\n\n2\\. The method relies on TCR-pMHC structures, which is rather rare compared to other interaction types.\n\n3\\. TCR-pMHC is an important and complex subject, the proposed method could be widely applied to many other interaction types. Generalizability needs to be shown, or at least discussed."}, "questions": {"value": "1\\. Since the experiments are only performed on known, \"positive\" TCR-pMHC pairs (those already with a stable structure), what bias might that introduce? Please discuss.\n\n2\\. Could the proposed method be applied to downstream prediction tasks such as impacts of sequence or structure perturbations, or other protein-protein interaction types?  Please discuss.\n\n3\\. Since the method requires 3D structure of the interaction complex, how would predicted structures (such as by AlphaFold base model or fine-tuned on TCR-pMHC) be used when structures are not available? From another perspective, is it possible to use the method for the evaluation of predicted structures or unknown interaction pairs?\n\n4\\. Does the range and scale of the residue importance show any relationship to the TULIP model's confidence of prediction?\n\n5\\. Since some samples in PDB are close neighbours (differing by only one or two residues), such as 2PXY and 2Z3, does the importance score show any different or similar patterns between the different residues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "q08AXOOYwN", "forum": "S3kSOFhs5m", "replyto": "S3kSOFhs5m", "signatures": ["ICLR.cc/2026/Conference/Submission11757/Reviewer_XjQm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11757/Reviewer_XjQm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800523400, "cdate": 1761800523400, "tmdate": 1762922784015, "mdate": 1762922784015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Transformer-based models, while effective for predicting TCR-pMHC binding, are \"black boxes\" because existing explainable AI methods are not designed for their encoder-decoder architectures. This paper introduces Quantifying Cross-Attention Interaction, a post-hoc explanation method specifically designed to interpret the cross-attention mechanisms in the decoder part of the transformer. To quantitatively evaluate this new method, the authors also compiled TCR-XAI, a benchmark of 274 TCR-PMHC crystal structures derived from STCRDab and TCR3d 2.0 that uses physical residue distance as a ground truth for binding interaction. The authors demonstrate that QCAI achieves good performance on this benchmark across multiple metrics, including ROC-AUC, perturbation studies (LOdds and AOPC), and a new metric called Binding Region Hit Rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The primary strength is providing an explainable AI method for encoder-decoder transformers, like TULIP, which current methods designed for encoder-only models cannot adequately interpret.\n- QCAI's performance is validated against a suite of competing methods (e.g., AttnLRP, TokenTM, Rollout) using ROC analysis, two different perturbation metrics (AOPC and LOdds) , and the authors' own BRHR metric, demonstrating SOTA results across most of them."}, "weaknesses": {"value": "- The paper explicitly states that QCAI is up to 50x slower per sample than other methods due to the necessary pseudo-inverse operations.\n- The new TCR-XAI benchmark is heavily skewed towards MHC-I samples, which may limit the generalizability of the findings for MHC-II complexes.\n- The benchmark relies on atomic distance as a \"proxy for ground-truth importance\". An assessment based on an energy function would have been more appropriate"}, "questions": {"value": "- In the introduction, TULIP is mentioned as an example of a BERT-style model, but as mentioned in other parts of the paper, TULIP is an autoregressive encoder-decoder model, not an encoder-only BERT-like model.\n- The 2017 transformer paper should be cited in section 2\n- Did the author consider evaluating their method on a transformer trained on natural language?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LKQ537Kp44", "forum": "S3kSOFhs5m", "replyto": "S3kSOFhs5m", "signatures": ["ICLR.cc/2026/Conference/Submission11757/Reviewer_rFG4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11757/Reviewer_rFG4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970054363, "cdate": 1761970054363, "tmdate": 1762922783533, "mdate": 1762922783533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel method to interpret the features learned by an encoder-decoder transformer with cross attention. The method relies on the evaluation of attention scores for query and key values, and aggregates them across the layers of the decoder. The method is then tested against others on the task of predicting contact residues from models trained on protein binding (TCR-pMHC), showing increase in performance. In order to do so, the authors provide an annotated benchmark dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Methods to explain cross-attention in transformers are needed to go beyond their use as “black boxes”. The tested case is timely and interesting for the community working in molecular biology. The benchmark dataset curated by the authors can also be useful."}, "weaknesses": {"value": "If I understand correctly, the task in which the authors test their method is rather hard: all the competitors perform in a way that is comparable or worse than random chance (Fig. 2), showing biased assessment. For a method which is “potential to be applied to other fields”, as the authors claim, other applications, where explainable methods work more reliably, should be tested."}, "questions": {"value": "- I understand the the method proposed outperforms the others, but how should I read the overall low AUCs in Fig. 2? Are most methods performing worse than random chance? Can the authors comment on why they consider an AUC of .6 as “demonstrating strong alignment” between importance scores and binding interactions?\n- Can the authors test their method on tasks where explainable approaches work more convincingly?\n- The method is applied to TULIP, a state-of-the-art model for TCR-pHMC binding prediction. Are the benchmark data curated by the authors in the training set of TULIP, or not?\n \nMinor:\n\nThe notation for element-wise multiplication is not consistent throughout the paper ($\\odot$ vs. $\\cdot$)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HZ5M6yxCi0", "forum": "S3kSOFhs5m", "replyto": "S3kSOFhs5m", "signatures": ["ICLR.cc/2026/Conference/Submission11757/Reviewer_iex5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11757/Reviewer_iex5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984881579, "cdate": 1761984881579, "tmdate": 1762922782924, "mdate": 1762922782924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}