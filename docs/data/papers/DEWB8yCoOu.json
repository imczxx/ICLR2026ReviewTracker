{"id": "DEWB8yCoOu", "number": 3738, "cdate": 1757508870144, "mdate": 1759898072472, "content": {"title": "XTalker: Turn, Smile, and Speak in Controllable Talking Portrait Animation", "abstract": "Audio-driven portrait animation enables lifelike talking faces from a static image and audio input, and has become a key task for creating realistic digital humans. Existing methods mainly follow either latent- or parameter-based paradigms, yet they either suffer from limited controllability, high computational cost, or lack broader expressivity (emotion and head diversity) beyond lip synchronization. To address these limitations, we present qualitative and quantitative analyses of parameter representations, focusing on the disentanglement of head keypoints, and discover that facial representation can be decomposed into three interpretable subspaces: lip-phoneme synchronization governed by audio envelope dynamics, emotion modulation conditioned on semantic features and reference labels, and head motion controlled by user-defined curves. Building on this insight, we propose XTalker, a fast and controllable generation framework based on flow matching. It employs a unified MM-DiT backbone to jointly encode portrait and audio signals, followed by three lightweight heads: a talking head for accurate lip movement driven by audio envelope, an emotion head for synthesizing facial expressions from emotion labels, and a pose head for animating diverse head trajectories. Extensive experiments show that XTalker runs in real-time and achieves competitive lip-sync accuracy while improving emotion and motion expressivity, making it well-suited for controllable portrait animation applications.", "tldr": "", "keywords": ["Portrait Animation", "Parameter Repersentation", "Controllability"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8b72cf245e583a5de01863aede2ffba4c469497c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents XTalker, a framework for audio-driven talking portrait animation. It focuses on disentangling facial dynamics into interpretable subspaces using parameter representations. The method decomposes motion into three components: lip-phoneme synchronization, emotional expression, and head motion. Lip movements are driven by the audio envelope's temporal dynamics. Emotion is modulated through semantic features and labels. Head motion is controlled by user-defined curves. XTalker uses a unified MM-DiT backbone with three lightweight prediction heads for each component. It is designed for fine-grained control and expressivity in low-data settings. The model runs in real-time and achieves competitive lip-sync accuracy. Experimental results show improved expressivity in emotion and motion compared to prior methods."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Disentangled Control: The framework explicitly decomposes facial animation into three interpretable components (lip synchronization, emotion, and head pose) using dedicated lightweight heads, enabling separate control over each aspect.\n* Improved Expressivity: Experimental results show that XTalker generates more diverse and expressive head motions and facial emotions compared to several baseline methods, as evidenced by higher variance and emotion accuracy scores.\n* Real-time Performance: The model achieves real-time inference speeds (28.21 FPS on an RTX 4090), making it suitable for practical applications requiring low latency."}, "weaknesses": {"value": "* Insufficient experimental results: As a video generation task, this work does not provide generated videos to demonstrate the performance of the proposed method, making it difficult for reviewers to comprehensively assess its performance. Moreover, in the presented qualitative comparison images, all samples have relatively simple backgrounds (either solid-colored or blurred), which fails to adequately reflect the model's overall capabilities. Regarding qualitative evaluation, the method achieves only moderate performance on lip-sync metrics, ranking fourth on Sync–C and second on Sync–D, suggesting potential shortcomings in audio-driven synchronization. In terms of baseline comparisons, this work lacks comparison with more recent approaches such as Ditto[1], FantasyTalking[2] and MultiTalk[3]. Additionally, for emotion control, it does not compare with methods capable of explicit emotion control, such as EAT[4], EDTalk[5], DreamTalk[6], and DICE-Talk[7], making it difficult to evaluate the true performance of this work in emotion control.\n* Limited novelty: Audio-driven methods based on parameter representations have been extensively studied; representative prior works include SadTalker[8] and VASA-1[9]. This work builds upon LivePortrait[10] by introducing a DiT network to condition the motion features extracted by LivePortrait on audio, emotion, and pose. This overall approach is highly similar to prior works such as VASA-1, JoyVASA[11], Playmate[12], and Takin-ADA[13], all of which are based on an image animation method (e.g., MegaPortraits[14] for VASA-1, LivePortrait for JoyVASA and Playmate, FaceVid2Vid[15] for Takin-ADA) and employ a DiT network to incorporate various conditioning signals into the features extracted by the animation model. Furthermore, the way this work incorporates emotion conditioning—via a one-hot emotion vector injected through fully connected layers or attention mechanisms—is very similar to that of Playmate and DICE-Talk. Overall, the proposed method does not significantly differ from prior approaches, and its contribution to the community appears limited.\n* Missing technical details: The paper does not clearly specify which components of the framework are trainable and which are adopted from existing methods. This lack of clarity makes it difficult to understand the training procedure and implementation details, posing challenges for reproducibility and comprehension.\n\n[1]Li, Tianqi, et al. \"Ditto: Motion-space diffusion for controllable realtime talking head synthesis.\" arXiv preprint arXiv:2411.19509 (2024).\n\n[2]Wang, Mengchao, et al. \"Fantasytalking: Realistic talking portrait generation via coherent motion synthesis.\" arXiv preprint arXiv:2504.04842 (2025).\n\n[3]Kong, Zhe, et al. \"Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation.\" arXiv preprint arXiv:2505.22647 (2025).\n\n[4]Gan, Yuan, et al. \"Efficient emotional adaptation for audio-driven talking-head generation.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[5]Tan, Shuai, et al. \"Edtalk: Efficient disentanglement for emotional talking head synthesis.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.\n\n[6]Ma, Yifeng, et al. \"DreamTalk: When Emotional Talking Head Generation Meets Diffusion Probabilistic Models.\" arXiv preprint arXiv:2312.09767 (2023).\n\n[7]Tan, Weipeng, et al. \"Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation.\" arXiv preprint arXiv:2504.18087 (2025).\n\n[8]Zhang, Wenxuan, et al. \"Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.\n\n[9]Xu, Sicheng, et al. \"Vasa-1: Lifelike audio-driven talking faces generated in real time.\" Advances in Neural Information Processing Systems 37 (2024): 660-684.\n\n[10]Guo, Jianzhu, et al. \"Liveportrait: Efficient portrait animation with stitching and retargeting control.\" arXiv preprint arXiv:2407.03168 (2024).\n\n[11]Cao, Xuyang, et al. \"JoyVASA: portrait and animal image animation with diffusion-based audio-driven facial dynamics and head motion generation.\" arXiv preprint arXiv:2411.09209 (2024).\n\n[12]Ma, Xingpei, et al. \"Playmate: Flexible Control of Portrait Animation via 3D-Implicit Space Guided Diffusion.\" Forty-second International Conference on Machine Learning.\n\n[13]Lin, Bin, et al. \"Takin-ADA: Emotion Controllable Audio-Driven Animation with Canonical and Landmark Loss Optimization.\" arXiv preprint arXiv:2410.14283 (2024).\n\n[14]Drobyshev, Nikita, et al. \"Megaportraits: One-shot megapixel neural head avatars.\" Proceedings of the 30th ACM International Conference on Multimedia. 2022.\n\n[15]Wang, Ting-Chun, Arun Mallya, and Ming-Yu Liu. \"One-shot free-view neural talking-head synthesis for video conferencing.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021."}, "questions": {"value": "* Could the authors provide additional implementation details, such as the training pipeline, which modules are trained end-to-end versus those fixed or adopted from pre-trained models, and the step-by-step training procedure?\n* Would it be possible to provide video inference results via an anonymous url? This would help reviewers better assess the actual performance of the model.\n* In certain cases, LivePortrait[1] tends to produce artifacts in the mouth region, particularly when the subject is pursing their lips. Several existing methods that build upon LivePortrait for audio-driven animation exhibit similar issues. Does the proposed method encounter the same problem? If so, could the authors provide a discussion on this limitation?\n\n[1]Guo, Jianzhu, et al. \"Liveportrait: Efficient portrait animation with stitching and retargeting control.\" arXiv preprint arXiv:2407.03168 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jo2V50vkMS", "forum": "DEWB8yCoOu", "replyto": "DEWB8yCoOu", "signatures": ["ICLR.cc/2026/Conference/Submission3738/Reviewer_7ymS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3738/Reviewer_7ymS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761183854221, "cdate": 1761183854221, "tmdate": 1762916954829, "mdate": 1762916954829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes XTalker, a fast and controllable audio-driven portrait animation framework that generates talking faces from a single image and audio. It decomposes facial motion into three interpretable subspaces—lip synchronization, emotion modulation, and head pose control—and employs a unified MM-DiT backbone with three lightweight heads for each component."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Decomposes facial dynamics into three components: lip synchronization, emotion, and head motion.\n2. Designs a lightweight framework with three dedicated branches to synthesize lip movements, emotional expressions, and head motion."}, "weaknesses": {"value": "1. Missing supplementary video. This is a talking-head generation method, yet no supplementary video is provided. Without videos, it is difficult to assess the temporal consistency and synchronization quality of the generated results.\n2. Unclear disentanglement supervision. The authors claim that expression, emotion, and head pose are disentangled, but no explicit supervision (e.g., cycle consistency loss) is applied. Moreover, the final results fuse the outputs of three heads together, making it unclear whether these factors remain disentangled in practice.\n3. Limited improvement over existing work. The LivePortrait implementation already supports independent control of lip, eye, and head movements. The proposed method appears to offer only marginal improvements over LivePortrait.\n4. Potential metric error. In Table 1, the reported synchronization score computed with SyncNet ranges between 0–1, whereas most existing works report Sync-C scores typically within 3–8. This discrepancy raises concerns about possible inconsistencies in metric computation or reporting."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "This technology could be misused to create realistic talking-face deepfakes without the consent of the individuals involved. Such misuse poses potential risks to privacy, personal reputation, and public trust in digital media. Given the increasing accessibility of generative models, the authors should discuss safeguards or watermarking mechanisms to prevent malicious applications."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "otoOFTdkQa", "forum": "DEWB8yCoOu", "replyto": "DEWB8yCoOu", "signatures": ["ICLR.cc/2026/Conference/Submission3738/Reviewer_akiD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3738/Reviewer_akiD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577406186, "cdate": 1761577406186, "tmdate": 1762916954535, "mdate": 1762916954535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "XTalker is a method for turning a single portrait photo into a talking video where you can control how the face moves and expresses emotion. Given an input image and an audio clip, the system makes the person in the photo speak in sync with the audio, and it allows the user to specify things like emotion or a head turn."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The approach explicitly breaks down facial animation into three interpretable components: lip synchronization, emotional expression, and head motion. By isolating these subspaces, the system can finetune lip movements, expressions, and head pose separately.\n- XTalker’s architecture is efficiently designed with a unified diffusion transformer backbone (MM-DiT) and three control heads (one each for lips, emotion, and pose). This design means the model encodes the audio and image together, then applies specialized small networks for each type of motion.\n- The system runs in real time (tested on a single RTX 4090 GPU) while still producing expressive results.The paper reports that XTalker achieves competitive lip–audio synchronization and delivers more varied emotions and head movements than previous methods."}, "weaknesses": {"value": "- While XTalker’s lip-sync is good, it is not the SOTA on every metric. In the quantitative comparison (Table 1), its lip synchronization scores are behind the top baseline on some measures.\n- The framework assumes the user will specify an emotion label and a head motion trajectory curve for each animation: it does not automatically infer emotion or head movements from the audio (eg. Speech-Driven Emotional Disentanglement for 3D Face Animation ICCV2023, Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion CVPR2024). It could be seen as a limitation, if no emotion/pose input is given, the system wouldn’t add those aspects by itself.\n- The multi-task nature of XTalker (synchronizing lips while also handling expression and motion) makes training requiring careful loss balancing. The authors employ a dynamic weighting strategy to balance the different objectives. This indicates that the system required significant tuning to prevent one aspect (such as head motion) from degrading another (like lip accuracy or video stability). Such complexity might be harder to reproduce."}, "questions": {"value": "- The paper uses a Large Language Model to generate predefined head motion curves. What kind of prompts or data were given to this LLM, and how do we know the resulting motion curves look natural?\n- The training pipeline includes an Emotion Expression Transformer that creates target facial keypoint embeddings for a desired emotion. How confident can we be that these generated “happy” or “angry” expressions truly correspond to those emotions? For example, did the authors perform any validation (maybe via an emotion classifier or human judgment) to ensure the transformer’s outputs match genuine expressions?\n- XTalker relies on user-provided head motion curves to animate head turns. What happens if a user does not supply a custom head trajectory? Does the system default to no head movement, or could it infer a reasonable head motion from the audio or learned patterns? Some previous works generate subtle head movements from audio alone for realism. It would be interesting to know if the authors considered an automatic mode for head motion (and emotion) when explicit control inputs are not given."}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "awfQHs7bHz", "forum": "DEWB8yCoOu", "replyto": "DEWB8yCoOu", "signatures": ["ICLR.cc/2026/Conference/Submission3738/Reviewer_mAg6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3738/Reviewer_mAg6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879719117, "cdate": 1761879719117, "tmdate": 1762916954171, "mdate": 1762916954171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes XTalker, a controllable, real-time, audio-driven talking portrait animation framework.\nIt aims to improve beyond conventional parameterized systems that mainly focus on lip synchronization by introducing explicit control over emotion diversity and head motion.\n\nXTalker is built upon a flow-matching diffusion architecture (MM-DiT backbone) and decomposes facial motion into three interpretable subspaces:\n\t1.\tLip–phoneme synchronization - driven by the temporal dynamics of the audio envelope.\n\t2.\tEmotional diversity - modulated via emotion labels mapped to facial expressions.\n\t3.\tHead motion controllability - achieved through user-defined motion curves and pose parameters.\n\nThe model claims to achieve competitive lip-sync accuracy, improved emotion expressivity, and real-time performance (~28 FPS)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-structured and addresses a relevant problem: adding controllable emotional and head-motion diversity to speech-driven portrait animation.\n* The decomposition into three interpretable subspaces (lip, emotion, pose) is conceptually sound and aligns with the broader goal of human-centered controllable animation."}, "weaknesses": {"value": "1) Missing or overstated claims\n\t•\tThe paper claims real-time performance, but no supplemental video, demo, or timing benchmark is provided to verify runtime or visual smoothness.\n\t•\tThe statement that the model achieves “precise lip–phoneme synchronization” using amplitude envelope dynamics is overstated.\nWhile the envelope correlates with rhythm and intensity, it does not encode phoneme-level articulatory information.\nThe observed correlation mainly reflects speech energy, not phonetic content. To substantiate this claim, the authors should report phoneme-level metrics such as SyncNet confidence, viseme accuracy, or phoneme alignment error.\n\n2)  Methodological clarity\n\t•\tThe notion of “user-defined head trajectories” is not clearly defined. how are these curves created, parameterized, or constrained? Are they realistic, or manually drawn?\n\t•\tThe emotional control relies on mapped labels, but the paper provides little detail on how these emotion embeddings are extracted or how robust they are to noisy or synthetic emotion inputs.\n\n3) Conceptual issues\n\t•\tThe audio envelope is treated as a main driver of lip synchronization, but this assumption neglects the contextual and phonetic structure of speech.\nThe envelope may help timing but cannot enforce accurate articulation.\n\t•\tThe claimed decomposition (lip sync, emotion, pose) is presented as a “contribution,” but it is more of a design choice or model architecture feature than a novel methodological advance.\n\n4) Lack of qualitative validation\n\t•\tThe paper provides no supplemental videos or qualitative figures showing motion smoothness, expressivity, or emotion diversity.\nGiven that portrait animation quality is perceptual, the absence of visual evidence significantly weakens the paper’s impact."}, "questions": {"value": "1.\tHow are user-defined motion curves obtained or parameterized?\n2.\tCan phoneme-level SyncNet or viseme metrics be reported to validate lip-sync?\n3.\tCan the authors share runtime measurements or real-time demo videos to support their performance claim?\n4.    How would the model generalize to out-of-distribution subjects or in-the-wild data ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FSEuJ57oby", "forum": "DEWB8yCoOu", "replyto": "DEWB8yCoOu", "signatures": ["ICLR.cc/2026/Conference/Submission3738/Reviewer_sJRq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3738/Reviewer_sJRq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023959494, "cdate": 1762023959494, "tmdate": 1762916953784, "mdate": 1762916953784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}