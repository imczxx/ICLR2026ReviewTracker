{"id": "qKmXz1UR0Z", "number": 10908, "cdate": 1758184545364, "mdate": 1759897621509, "content": {"title": "Evolution of Concepts in Language Model Pre-Training", "abstract": "Language models obtain extensive capabilities through pre-training. However, the pre-training process remains a black box. In this paper, we track linear interpretable feature evolution across pre-training snapshots using a sparse dictionary learning method called crosscoders. We find that most features begin to form around a specific point, while more complex patterns emerge in later training stages. Feature attribution analyses reveal causal connections between feature evolution and downstream performance. Our feature-level observations are highly consistent with previous findings on Transformer's two-stage learning process, which we term a statistical learning phase and a feature learning phase. Our work opens up the possibility to track fine-grained representation progress during language model learning dynamics.", "tldr": "We track linear interpretable feature evolution across pre-training snapshots using a sparse dictionary learning method called crosscoders.", "keywords": ["Large Language Model; Pre-Training; Mechanistic Interpretability; Training Dynamics; Crosscoder"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2caa47e92fff979847daf006801f6bfb927b00a7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper separates the pretraining process of large language models (LLMs) into two distinct phases: a statistical learning phase and a feature learning phase. The authors argue that downstream performance improvements are closely tied to the emergence of features during the latter phase. To quantify this, they introduce a “crosscoder” network that detects features in activations with respect to a so-called decoder norm and track the development of this norm during pretraining. The paper presents empirical results linking these phases to downstream task performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The idea of analyzing the emergence of features during pretraining is interesting and aligns with the current interest in the mechanistic interpretability of LLMs.\n\n2) The paper attempts to link internal feature emergence in the model to downstream generalization, which is a valuable direction."}, "weaknesses": {"value": "1) The decoder norm is part of the training objective, and the weights in the norm are (obviously) part of the model that constructs the feature.\nI find this quantity borderline to abstract to serve as a metric that should verify the central concepts of this paper.\n\n2) Figures 4–6 are central to the main claims but are difficult to interpret, as the x-axis is on a logarithmic scale representing training steps.\nThe authors claim that two distinct learning phases appear, but on a linear scale, this might resemble a single continuous (albeit noisy) learning process. The choice of log-scale seems to visually exaggerate the phase transition; this requires at least discussion and justification. As a remark: on a linear scale, the \"statistical phase\" is less than 1% of training time.\n\n3) Figure 5 introduces an additional LLM for the interpretation of features, which adds another layer of complexity and potential confounding effects. For me, the interpretation of LLMs should ideally come without LLMs to avoid a circle.\n\n4) Minor remark: Including a figure before the abstract is unconventional."}, "questions": {"value": "1) Why did you choose a log-scale for presenting all results?\n\n2) As all experiments use Pythia, I would like to see a discussion of the training algorithm used there. How many warm-up steps were used? When did the learning rate change?   There is a (small) chance that the seen effects may be trivial if they can be related to, for example, a learning rate change.\n\n3) The y-axis states a \"normalized decoder norm\". What is normalized exactly? It appears to me that each feature reaches one at some point, which may again make the results less surprising, as each feature must either be present at the start or emerge at some point if one is eventually reached."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8kciQ5mzLi", "forum": "qKmXz1UR0Z", "replyto": "qKmXz1UR0Z", "signatures": ["ICLR.cc/2026/Conference/Submission10908/Reviewer_effr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10908/Reviewer_effr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760695526139, "cdate": 1760695526139, "tmdate": 1762922112178, "mdate": 1762922112178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how internal features in large language models emerge and evolve during pre-training. Using cross-snapshot crosscoders—a variant of sparse autoencoders adapted to align activations across training checkpoints—the authors track interpretable features throughout the training process of Pythia models. They identify initialization and emergent features, quantify their persistence, and show that feature complexity increases with training progress. Attribution-based analyses link these microscopic feature dynamics to downstream task improvements. The study further reveals a clear two-phase transition in pre-training, from an early statistical learning phase dominated by token-level regularities to a later feature learning phase characterized by sparse, semantically rich representations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a technically novel adaptation of crosscoders for temporal analysis, offering the first fine-grained view of feature evolution across pre-training snapshots. The experiments are extensive and carefully controlled, combining mechanistic interpretability with learning dynamics. The empirical discovery of a statistical-to-feature-learning transition provides a coherent mechanistic complement to theoretical frameworks such as the information bottleneck and singular learning theory. The work stands out for its methodological clarity, strong empirical validation, and relevance to both interpretability and representation learning research."}, "weaknesses": {"value": "The study provides interesting descriptive evidence, but the causal claims remain largely correlational—attribution patching is still a heuristic rather than proof of necessity.\nThe analysis is limited to Pythia checkpoints and relatively simple syntactic tasks, so generalization to larger or more complex models is unclear.\nSome methodological choices (e.g., decoder-norm as feature-strength proxy, selection of snapshots) lack formal justification or ablation.\nThe paper is also quite engineering-heavy, and clarity could improve in conveying what new conceptual insight the cross-coder adds beyond existing SAE frameworks."}, "questions": {"value": "How sensitive are the results to the number or spacing of snapshots?\n\nCould decoder-norm scaling artifacts or feature-splitting affect the interpretation of “feature emergence”?\n\nDoes attribution patching preserve causal validity when features are highly correlated?\n\nHave the authors verified that the same cross-coder features generalize across layers or different datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "yroK4K5tSc", "forum": "qKmXz1UR0Z", "replyto": "qKmXz1UR0Z", "signatures": ["ICLR.cc/2026/Conference/Submission10908/Reviewer_QXzd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10908/Reviewer_QXzd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708193899, "cdate": 1761708193899, "tmdate": 1762922111553, "mdate": 1762922111553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the evolution of features during LLM training using crosscoder SAEs. They first study the emergence, persistence of features in LLMs, showing that many features persist for long periods and that their emergence time varies. They then consider feature complexity, showing a moderate correlation between feature complexity and the time at which the feature's emergence peaks. After analyzing feature dynamics, they study the causal relationship between features and task performance, using attribution patching to assess causal relationships for subject-verb agreement, indirect object identification, and induction tasks. Finally, they show that the initial part of training learns statistical features of token distributions and study the changes in feature dimensionality during training."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The authors introduce a novel crosscoder method for studying feature dynamics in LLMs. They analysis of feature emergence and causal study is clear and thorough."}, "weaknesses": {"value": "Figure 3c is a bit difficult to interpret, since the analysis references the number of training steps but the figure is labeled in terms of snapshot number. Since there is a stratified sampling approach to selecting snapshots (and since the Pythia models use log-spaced snapshots + snapshots every 1000 training steps), it's not obvious which training step corresponds to which snapshot in the figure and hard to verify the accuracy of the claims. \n\nThe argument about feature dimensionality and compression is a bit unclear with an unclear takeaway.\n\nOnly relying on one family of models reduces the potential impact."}, "questions": {"value": "How do the results in 6b and 6c compare with a similar plot produced by ablating all but/ablating $k$ random features?\n\nCould you produce similar analysis with additional models with available checkpoints, such as Stanford CRFM GPT-2 or Olmo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lnRBQikkA4", "forum": "qKmXz1UR0Z", "replyto": "qKmXz1UR0Z", "signatures": ["ICLR.cc/2026/Conference/Submission10908/Reviewer_uCjx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10908/Reviewer_uCjx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939785625, "cdate": 1761939785625, "tmdate": 1762922111012, "mdate": 1762922111012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper claims to explore how meaningful features emerge as language models learn.  Using crosscoders to align representations across training snapshots, the authors trace training dynamics over time. They find a clear turning point early in pretraining, where models move from simple statistical patterns to more structured, feature-based understanding. Through attribution and patching analyses, these evolving features are linked to linguistic behaviors such as agreement and induction."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper touches on an interesting and relevant question, how internal features evolve during language model pretraining. \n\n- It attempts to provide a structured view using crosscoders. \n\n- The motivation to study model dynamics rather than static checkpoints is reasonable and aligns with growing interest in understanding training trajectories.\n\n- The work could serve as a preliminary exploration toward more rigorous analyses of concept formation in neural networks."}, "weaknesses": {"value": "- Novelty. The paper offers little substantive novelty beyond prior work on crosscoders (Lindsay et al., 2024). The method follows existing frameworks closely, with minimal theoretical or heuristic grounding for its design choices. For instance, \n    - 1) The selection of the SlimPajama dataset is not clearly justified, nor is the limited set of base models sufficient to generalize the findings.\n    - 2) An ablation study on activation aggregation in Eq. (1) would help clarify whether summing activations is the optimal or even appropriate choice.\n\n- Contribution. The contribution remains narrow, more a descriptive report than a step toward formal interpretability. The reliance on visualizations such as heatmaps and curves, without clear quantitative or linguistic interpretation, makes it hard to connect the observed patterns to interpretable model behaviors. \n\n- Rigorousness. Several findings, particularly those regarding early n-gram learning, largely lack of contextualization with known results from prior work (e.g., Svete & Cotterell, 2024; Nguyen, 2024, Chen, 2024). \n\n- Writing. The writing quality is also uneven. The paper reads more like a lab report than a cohesive scientific paper. For instance\n    - 1) Lack of grounded formalization for key concepts. For example, can we define \"initial features\" and \"emergent features\" (line 202-line207) formally using the notations in the paper? \n    - 2) Some phrasing is imprecise. \"the pre-training process remains largely a black box.\" The sentence is a bit unclear. The process of pretraining is generally gradient descent on large-scale of datasets. I guess what the authors mean here is \"the pretraining dynamic\" instead of \"process\".\n\nOverall, the work lacks the theoretical grounding, novelty, and clarity needed to make a strong contribution. That said, it is definitely worthwhile for presenting at a workshop after revision.\n\n\n``References``\n- Anej Svete and Ryan Cotterell. Transformers can represent n-gram language models. arXiv preprint\narXiv:2404.14994, 2024.\n- Timothy Nguyen. Understanding transformers via n-gram statistics. arXiv preprint\narXiv:2407.12034, 2024.\n- Chen et al. Jet expansions of residual computation  arXiv preprint 2410.06024, 2024."}, "questions": {"value": "1. Could the authors clarify the rationale behind choosing SlimPajama as the pretraining dataset? Were other datasets considered, and if so, why were they excluded?\n2. In Eq. (1), how were activations aggregated across snapshots? Why was summing chosen, and how might alternative strategies (e.g., averaging, normalization, or selective sampling) affect the results?\n3. Can “initial features” and “emergent features” be defined more formally within the paper’s notation? A clearer definition would make the results more interpretable and reproducible.\n4. Have the authors compared their findings with recent analyses of early-stage n-gram learning (e.g., Svete & Cotterell, 2024; Nguyen, 2024, Chen et al, 2024)? What new insights, if any, extend beyond those studies?\n5. How stable are the observed feature dynamics across different base models or seeds? Would larger or differently initialized models exhibit the same “turning point” behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8Eu9ctl0eY", "forum": "qKmXz1UR0Z", "replyto": "qKmXz1UR0Z", "signatures": ["ICLR.cc/2026/Conference/Submission10908/Reviewer_zNuD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10908/Reviewer_zNuD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998051520, "cdate": 1761998051520, "tmdate": 1762922110618, "mdate": 1762922110618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}