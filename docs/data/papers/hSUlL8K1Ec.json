{"id": "hSUlL8K1Ec", "number": 4188, "cdate": 1757627005906, "mdate": 1763086824187, "content": {"title": "Make it NoisEasier: Boosting Text-to-Video Generation with Direct Noise Optimization", "abstract": "Diffusion models have significantly advanced text-to-video (T2V) generation, yet they still struggle with complex prompts involving intricate object interactions and precise attribute binding. While reward-based fine-tuning can improve compositional alignment, it is computationally costly and prone to reward hacking. In this work, we propose **NoisEasier**, a test-time training framework that improves T2V generation by directly optimizing latent noise with differentiable rewards during inference. To make this practical, we leverage fast video consistency models, enabling full gradient backpropagation within just 4 denoising steps. To mitigate reward hacking, we integrate multiple reward objectives that balance semantic alignment and motion quality, and propose a novel negative-aware reward calibration strategy that uses LLM-generated distractors to provide fine-grained compositional feedback. Experiments on VBench and T2V-CompBench show that NoisEasier consistently improves strong baselines, achieving over 10% gains in several dimensions and even surpassing commercial models like Gen-3 and Kling. Notably, these improvements are achieved within 25 optimization steps, requiring only 45 seconds per sample on two RTX 6000 Ada GPUs. Under the same wall-clock time budget, NoisEasier achieves human preference win rates exceeding CogVideoX-2B by 18.8% and Wan2.1-1.3B by 6.8%, demonstrating a competitive trade-off between performance and efficiency.", "tldr": "Optimizing latent noise for fast T2V models with calibrated rewards significantly boosts compositional alignment.", "keywords": ["Text-to-Video Generation", "Diffusion Models", "Test-Time Optimization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c158a44fd911458d53e73ba1a7c0515533570366.pdf", "supplementary_material": "/attachment/5b26e1888c7400af6f5e7993f9d517d75b04b3c8.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose NoisEasier, a test-time training framework for optimizing video generation results.\nNoisEasier optimizes latent noise directly using differentiable rewards during inference, leveraging fast video consistency models (e.g., T2V-Turbo) to enable full gradient backpropagation. It integrates multiple reward objectives to balance semantic alignment and motion quality, and introduces a negative-aware reward calibration strategy using LLM-generated distractors for fine-grained compositional feedback.\nExperiments on VBench and T2V-CompBench show that NoisEasier consistently improves baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Unlike traditional reward-based fine-tuning that updates model parameters (computationally expensive and inflexible) or fixed-model inference (poor adaptability), NoisEasier optimizes latent noise directly during inference. This avoids redundant model parameter updates and can dynamically adapt to different text prompts."}, "weaknesses": {"value": "1. The paper assigns weights of 2 to HPSv2 and ViCLIP, and 1 to ImageReward and motion reward. What is the theoretical basis for this weight setting? Has a sensitivity analysis been conducted (e.g., testing weights such as 0.5, 1, 2, 3) \n2. Has the framework been validated on other video consistency models (e.g., VideoLCM), and what is its adaptability to different backbones?\n3 Negative samples are generated using GPT-4o, a closed-source LLM. If researchers only have access to open-source LLMs (e.g., Llama 3, Qwen), can these models generate high-quality \"hard negatives\" (with appropriate semantic perturbations) to ensure the effect of reward calibration? \n4. For closed-source models (e.g., Gen-3, Kling) marked with \"*\" (reproduction results), what are the specific hyperparameters used in the reproduction (e.g., denoising steps, text encoder weights)? For open-source models like CogVideoX-2B, were the official recommended optimization strategies (e.g., dynamic prompt enhancement) enabled during testing? In addition, what GPUs were used to test comparative models such as CogVideoX-2B and Wan2.1-1.3B? What is the memory usage of NoisEasier during operation? \n5. he qualitative examples and test prompts in the paper (e.g., \"a dog firefighter rescues kittens\", \"six robots dance rhythmically\") are mostly \"low-dynamic, multi-object static scenes\". Has the paper tested high-dynamic scenes (e.g., \"a basketball game with collisions\") to verify the robustness of the motion reward?"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VRqtNgKWgs", "forum": "hSUlL8K1Ec", "replyto": "hSUlL8K1Ec", "signatures": ["ICLR.cc/2026/Conference/Submission4188/Reviewer_Fu3g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4188/Reviewer_Fu3g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760776487751, "cdate": 1760776487751, "tmdate": 1762917220123, "mdate": 1762917220123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "xC0lnMMgMZ", "forum": "hSUlL8K1Ec", "replyto": "hSUlL8K1Ec", "signatures": ["ICLR.cc/2026/Conference/Submission4188/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4188/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763086822941, "cdate": 1763086822941, "tmdate": 1763086822941, "mdate": 1763086822941, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a test time training framework to enhance T2V generation. The primary contribution is a method that directly optimizes the latent noise during inference using a combination of differentiable rewards. This approach is made computationally efficient by leveraging fast video consistency models. The paper also introduces a \"negative-aware reward calibration\" strategy that uses generated  negative prompts for contrastive reward calculation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and easy to follow.\n\n- The paper conducts extensive ablation studies to validate the effectiveness of each proposed component."}, "weaknesses": {"value": "- The paper's contribution is somewhat incremental. It combines existing techniques, including mixed reward feedbacks (e.g. T2V-Turbo) and test time optimization of the input noise (e.g. Initno). The paper claims the \"negative-aware reward calibration\" as a major contribution, but its effectiveness appears to be limited according to Figure 6 (human preference rate 51.7 vs 48.3).\n\n- The paper lacks a critical baseline that optimizes the entire model with the same mixed reward model. Though the base models (T2V-Turbo) have already utilized HPSv2 and ViCLIP for reward-based model distillation, it is difficult to directly assess the necessity of optimizing the noise, as the reward and inference setups are different.\n\n- The paper relies on a distilled 4-step consistency model for effective test time training. However, the test time training makes the method much slower than the distilled model, diminishing the advantage of the consistency model. Figure 5 provides a Quality-Cost Pareto analysis trying to demonstrate the proposed model is more efficient than CogVideoX-2B, but this comparison to an arbitrary model does not sufficiently prove the method's efficiency. For example, a distilled version of CogVideoX-2B might be more effective than the proposed method."}, "questions": {"value": "- In Table 1 (Quantitative Evaluation on VBench), some open-source models have better scores than leading commercial models. For example, the average score of Open-Sora 2.0 is higher than Sora, and CogVideoX-2B is very close to Kling. Is there any explanation for this?\n\n- Directly optimizing with the reward gradient is usually considered highly susceptible to reward hacking. Did the authors observe related failure cases during experiments (beyond the ablations on single-reward optimization)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hk2IYQE4T5", "forum": "hSUlL8K1Ec", "replyto": "hSUlL8K1Ec", "signatures": ["ICLR.cc/2026/Conference/Submission4188/Reviewer_mPiR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4188/Reviewer_mPiR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810323724, "cdate": 1761810323724, "tmdate": 1762917219516, "mdate": 1762917219516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes NoisEasier, a test-time training framework for improving text-to-video (T2V) diffusion models by optimizing latent noise directly using differentiable reward signals. Rather than fine-tuning model weights (as in reward-based RLHF or preference optimization), NoisEasier refines the input noise during inference, leveraging video consistency models that allow full gradient backpropagation in only 4–8 denoising steps. Empirical Results show that NoisEasier delivers consistent and significant improvements over strong baselines on VBench and T2V-CompBench, with high efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. While noise optimization has been explored in text-to-image and motion domains, this work is the first to successfully adapt it to video generation.\n\n2. The authors show significant performance improvement over the T2V-Turbo base model and other strong models.\n\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. Although results are strong on VBench and T2V-CompBench, both benchmarks focus primarily on short, synthetically structured prompts. The method’s effectiveness on longer, narrative-style prompts remains untested. More realistic evaluation is need to understand the generalizability of the method.\n\n2. The framework heavily relies on existing image- and video-based reward models, which were trained on specific distributions and may not generalize. This might contribute to my previous point."}, "questions": {"value": "See weeknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1o9kUNGBgV", "forum": "hSUlL8K1Ec", "replyto": "hSUlL8K1Ec", "signatures": ["ICLR.cc/2026/Conference/Submission4188/Reviewer_Hbw5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4188/Reviewer_Hbw5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062314051, "cdate": 1762062314051, "tmdate": 1762917219118, "mdate": 1762917219118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}