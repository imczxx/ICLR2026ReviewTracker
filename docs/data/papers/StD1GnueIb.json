{"id": "StD1GnueIb", "number": 7488, "cdate": 1758024275021, "mdate": 1759897849830, "content": {"title": "GeoReasoning: Structured Semantic Reasoning for Image-to-Map Localization", "abstract": "We introduce *reasoning localization*, a new paradigm for self-localization that leverages multimodal large language models (MLLMs) to interpret spatial context from 2D maps and first-person images. Unlike traditional approaches that depend on LiDAR, odometry, or engineered markers, reasoning localization emulates how humans orient by aligning visual cues with map structure. To address this new self-localization problem, we present **GeoReasoning**, a zero-shot framework that decomposes image-to-map grounding into *structured semantic reasoning* followed by *geometric verification*. Instead of directly predicting coordinates, GeoReasoning (i) identifies map-visible landmarks, (ii) grounds them as anchors via promptable segmentation, (iii) estimates coarse distances through language-based reasoning, and (iv) solves a robust triangulation program to recover the pose. This design separates high-level semantic reasoning from metric optimization, yielding interpretable rationales, verifiable intermediate outputs, and resilience against map symmetries. To support this task, we release the first benchmark for reasoning localization, spanning diverse indoor maps, image-map pairs, and candidate poses, along with diagnostic metrics such as rationale consistency, mean/median localization error, and success@$r$ for $r\\in{0.1,0.5,1,3}$ m. Experiments with state-of-the-art MLLMs demonstrate that GeoReasoning significantly improves localization accuracy over direct prediction baselines, while exposing open challenges in symmetry disambiguation and monocular scale estimation. Our results highlight structured reasoning--geometry integration as a promising path toward scalable, human-like localization in GPS-denied settings.", "tldr": "", "keywords": ["Reasoning localization", "multimodal large language models", "LLM reasoning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/12c89a0b5205e6ae618b93cb3a610b21a4d802e7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GeoReasoning, a training-free framework for indoor reasoning localization that leverages large language models. The authors also present a novel indoor benchmark specifically designed for this task. The core of the proposed solution is a zero-shot framework that decomposes the complex localization problem into two stages: structured semantic reasoning followed by geometric verification. This approach explicitly models anchor (landmark) selection, verifies cross-view consistency to handle ambiguity, and uses robust trilateration to solve for the final pose. As demonstrated in the evaluations, the GeoReasoning framework significantly outperforms direct-prediction baselines, which often fail at this task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tOverall, The paper is well-written, clearly articulating the reasoning localization concept. The proposed GeoReasoning framework is presented logically, and its two-stage (Reason & Ground, Constrain & Solve) methodology is easy to follow.\n2.\tA key contribution is the proposal of a new indoor localization benchmark, which is derived from an existing dataset. The experimental validation further confirms that the proposed method achieves better performance over the baselines"}, "weaknesses": {"value": "1. The claimed reasoning localization paradigm is said to move beyond geometry-first methods. However, it still appears to be closely related to retrieval-based localization. The system essentially searches for geometric landmarks, employs large language models and segmentation to identify potential landmarks, expands the candidate set, and then applies additional rule-based reasoning (e.g., object associations) to refine localization.\n2. The authors argue that traditional SLAM-based localization pipelines, while effective, suffer from sensor noise, calibration drift, dynamic scenes, and appearance changes, and require careful tuning for long-term deployment. However, I do not find convincing evidence in this paper that the proposed method actually mitigates these issues. In fact, similar performance improvements can also be achieved by simply expanding the candidate set and introducing additional rules, such as re-ranking strategies.\n3. The experiments only compare different language models within the proposed framework, without benchmarking against other representative localization approaches such as LalaLoc or SceneGraphLoc[1] and etc. As a result, it is difficult to substantiate the claimed advantages of this “new paradigm.”\n4. The method is evaluated on a newly introduced dataset rather than on existing benchmarks. This raises the concern that the approach may rely on specific data conditions or annotations, which could limit its general applicability.\n5. The authors further claim that the proposed method generalizes well to outdoor environments. I remain skeptical of this assertion, as no experimental evidence or quantitative validation is provided to support it.\n\n[1] Scenegraphloc: Cross-modal coarse visual localization on 3d scene graphs. ECCV2024."}, "questions": {"value": "please seek weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zSf3jp8dQT", "forum": "StD1GnueIb", "replyto": "StD1GnueIb", "signatures": ["ICLR.cc/2026/Conference/Submission7488/Reviewer_YuzH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7488/Reviewer_YuzH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704143486, "cdate": 1761704143486, "tmdate": 1762919604174, "mdate": 1762919604174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel localization paradigm, which leverages the reasoning power of MLLMs to handle indoor-localization tasks. Without training, it decomposes the indoor-localization into the semantic reasoning and geometric trilateration stages. A new benchmark is released for this reasoning localization paradigm.  Compared with the direct-prompt baseline, the proposed GeoReasoning gains obvious improvements in the zero-shot setting."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed reasoning localization framework contains two stages. The first extract localization anchors (landmarks) through MLLM captioning and segmenter prompting, and it is more time&memory efficient compared to the sophisticated retrieval used in relocalization and SLAM systems. The second stage constrains the reasoning results with geometric trilateration, which prevents fragile matching and makes it more robust when facing scene clutter, repetition and symmetry."}, "weaknesses": {"value": "The application of the proposed egocentric-floor localization is limited: indoor objects are often moved or replaced in daily life. The framework localizes itself by reasoning and triangulating objects both observed in the egocentric image and the floor plan,  which is unreliable compared with matching (contains more anchor points and excludes irrelevant parts from RANSAC). Moreover,  it is hard for users to update the floor plan without other mapping or rendering techniques, but users can easily update scenes by uploading new egocentric images in traditional localization pipelines. \n\nThe proposed method lacks comparison with traditional localization and direct-prompt methods, partly due to the egocentric-floor localization settings, which hinder the demonstration of the contributions mentioned in Strengths."}, "questions": {"value": "Distance reasoning from a single image is inaccurate for SOTA MLLMs, which would be the bottleneck of the localization error under S@1. The authors could leverage monocular depth estimators like Moge-2 or more related DepthLM to get more accurate depth(and camera model) priors.\n\nIt would be better to add more visual analysis for failure cases in Line 432-436, such as a pie chart.\n\nEquations are not strictly labeled.  In equations under Line 283，$T$ and $R$ are not mentioned before, and $M$ is duplicated with the symbol of the RGB image."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YosLpsVYBb", "forum": "StD1GnueIb", "replyto": "StD1GnueIb", "signatures": ["ICLR.cc/2026/Conference/Submission7488/Reviewer_PKvn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7488/Reviewer_PKvn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920423654, "cdate": 1761920423654, "tmdate": 1762919603705, "mdate": 1762919603705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for localization in a camera setting through its use of landmarks, done a bit differently from SLAM like approaches. They also claim to release a new dataset for indoor scenes to support the research effort. The procedure can be used as an addon to generic MLLMs and improve its performance. \n\nThe main idea is to reconcile a camera view with landmarks obtained from a 'map' - a top down view of the scene. It is a two stage process, first using intelligent prompting to get an intermediate output and then refining it further through optimization.\n\nThe map can generate anchors of interest, prompted by a multimodal LLM - a SAM like apparatus. Once they are obtained, one sets up an optimization objective through a trilateralization procedure described. \n\n$ L = \\sum w_k \\varphi (||p - a_k|| - \\rho_k) $\n\nHere, we know $a_k$ from the landmarks (global solve), and $\\rho_k$ is the distance in local camera. So from this we can get $p$ and thus localize. In essence we obtain p that minimizes the residual contained within.\n\nA fairly convincing set of evaluations is provided. The gist of it is that the fittings - querying with a prompt for landmarks, and the optimization are able to significantly improve vanilla MLLM performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Intuitive way to connect indoor localization pieces \n+ Camera + map makes sense - also applicable in settings like autonomous driving with BEV. \n+ Principled constraint and prompting machinery. \n+ Results demonstrate the correctness of the approach. We see improved results with their fittings in nearly all the cases and models. To drill down, ID switches are reduced, localization accuracy is improved."}, "weaknesses": {"value": "- Please take this with a grain of salt. I am pretty sure that similar ideas abound in localization (for instance, in my research in BEV modelling), where we can carry out the task given a map and images by correlating them (with attention, or other means). However, the main novelty in this work is to use it with multi-modal LLMs. So to this end, I (very weakly) question the novelty of the approach. \n- Temporal modelling. It would have been nice if the authors could have extended the analysis with temporal modelling. I would gather that this extension is straight forward given the methods we have today.\n- More failure cases would be helpful."}, "questions": {"value": "I am curious about what would happen if the map were erroneous. These happen in outdoor driving scenes like construction zones. \nOn similar lines, I am curious about scaling errors, and generally about miscalibrated maps."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Pva4x5h98P", "forum": "StD1GnueIb", "replyto": "StD1GnueIb", "signatures": ["ICLR.cc/2026/Conference/Submission7488/Reviewer_g4Z3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7488/Reviewer_g4Z3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762207420874, "cdate": 1762207420874, "tmdate": 1762919603204, "mdate": 1762919603204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for localization in a camera setting through its use of landmarks, done a bit differently from SLAM like approaches. They also claim to release a new dataset for indoor scenes to support the research effort. The procedure can be used as an addon to generic MLLMs and improve its performance. \n\nThe main idea is to reconcile a camera view with landmarks obtained from a 'map' - a top down view of the scene. It is a two stage process, first using intelligent prompting to get an intermediate output and then refining it further through optimization.\n\nThe map can generate anchors of interest, prompted by a multimodal LLM - a SAM like apparatus. Once they are obtained, one sets up an optimization objective through a trilateralization procedure described. \n\n$ L = \\sum w_k \\varphi (||p - a_k|| - \\rho_k) $\n\nHere, we know $a_k$ from the landmarks (global solve), and $\\rho_k$ is the distance in local camera. So from this we can get $p$ and thus localize. In essence we obtain p that minimizes the residual contained within.\n\nA fairly convincing set of evaluations is provided. The gist of it is that the fittings - querying with a prompt for landmarks, and the optimization are able to significantly improve vanilla MLLM performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Intuitive way to connect indoor localization pieces \n+ Camera + map makes sense - also applicable in settings like autonomous driving with BEV. \n+ Principled constraint and prompting machinery. \n+ Results demonstrate the correctness of the approach. We see improved results with their fittings in nearly all the cases and models. To drill down, ID switches are reduced, localization accuracy is improved."}, "weaknesses": {"value": "- Please take this with a grain of salt. I am pretty sure that similar ideas abound in localization (for instance, in my research in BEV modelling), where we can carry out the task given a map and images by correlating them (with attention, or other means). However, the main novelty in this work is to use it with multi-modal LLMs. So to this end, I (very weakly) question the novelty of the approach. \n- Temporal modelling. It would have been nice if the authors could have extended the analysis with temporal modelling. I would gather that this extension is straight forward given the methods we have today.\n- More failure cases would be helpful."}, "questions": {"value": "I am curious about what would happen if the map were erroneous. These happen in outdoor driving scenes like construction zones. \nOn similar lines, I am curious about scaling errors, and generally about miscalibrated maps."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pva4x5h98P", "forum": "StD1GnueIb", "replyto": "StD1GnueIb", "signatures": ["ICLR.cc/2026/Conference/Submission7488/Reviewer_g4Z3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7488/Reviewer_g4Z3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762207420874, "cdate": 1762207420874, "tmdate": 1763047811970, "mdate": 1763047811970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GeoReasoning, a new zero-shot framework for self-localization that requires no sensors or environment-specific training. It uses multimodal large language models (MLLMs) to interpret maps and visual observations. The method works by first identifying landmarks with an MLLM, locating them in an image, estimating their distance through textual reasoning, and then using trilateration to find the user's position. The paper also introduces Loc-Bench, a new evaluation dataset, on which GeoReasoning outperformed direct MLLMs prompting methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The framework is interpretable and modular, separating semantic reasoning from geometric computation. Using semantic anchors followed by trilateration mirrors human reasoning and makes the multimodal localization process more transparent and analyzable."}, "weaknesses": {"value": "Main Weakness:\n1. The author states that they create Loc-Bench that differs from previous work by focusing on egocentric-to-allocentric transformation, while VSI-Bench[1] have included a 2D cognitive map and evaluates visual-spatial relationship from egocentric videos, espatially testing on relational reasoning\nand egocentric-allocentric transformation.\n2. The baselines used for comparison are limited. There are more SOTA open-sourced VLM that could be compared to, e.g. InternVL2.5 and LLaVA, which would better contextualize the improvements. I also suggest the authors to provide baselines for (1) always selecting the most frequent answer; (2) a random selection strategy, which would help quantify the task’s inherent difficulty. Besides, a human-level performance would establish an intuitive upper bound and clarify the practical significance of the results.\n3. The ablation study does not evaluate each component's strength. Key design choices (e.g. anchor reasoning, verification and refinement) are not individually quantified, leaving it unclear which parts actually drive performance. A more fine-grained ablation would clarify whether the observed improvements arise from the overall pipeline design or a few dominant modules.\n\n\n\nMinor:\n1. Only one equation is indexed; all others are not. Please ensure consistent numbering.\n2. In the equation on line 250, what does $\\mathcal{D}$ represent? Is it a typo for $G$? Also, the use $\\mathbb{R} > 0$ is not formal.\n3. Some notations are undefined, e.g., the $L$ in Equation (1). Define all symbols on first use.\n\n\n[1]Yang, Jihan, et al. \"Thinking in space: How multimodal large language models see, remember, and recall spaces.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025."}, "questions": {"value": "In lines 220–238, how does the cross-view semantic verifier handle repeated object categories (e.g., multiple bathrooms, beds, or tables), given that prompt-based segmentation on $M$ may highlight only one instance among several plausible candidates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Y1vm0KQKd", "forum": "StD1GnueIb", "replyto": "StD1GnueIb", "signatures": ["ICLR.cc/2026/Conference/Submission7488/Reviewer_kFu9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7488/Reviewer_kFu9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762397351527, "cdate": 1762397351527, "tmdate": 1762919602744, "mdate": 1762919602744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}