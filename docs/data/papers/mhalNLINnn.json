{"id": "mhalNLINnn", "number": 14891, "cdate": 1758245163648, "mdate": 1759897343314, "content": {"title": "Dynamic Optimizations of LLM Ensembles with Two-Stage Reinforcement Learning Agents", "abstract": "The advancement of LLMs and their accessibility have triggered renewed interest in multi-agent reinforcement learning as robust and adaptive frameworks for dynamically changing environments. This paper introduces RL-Focal, a two-stage RL agent framework that routes and ensembles LLMs. \\textit{First}, we develop the Decider RL-agent, which learns to dynamically select an ensemble of small size ($m_i$) among $N$ LLMs ($m_i \\ll N$) for incoming queries from a user-defined downstream task $i$, by maximizing both error-diversity and reasoning-performance of the selected ensemble through iterative updates of task-adaptive rewards and policy. \\textit{Second}, to enable effective fusion of dynamically selected LLMs, we develop the stage-2 Fusion RL-agent, which learns to resolve reasoning conflicts from different LLMs and dynamically adapts to different ensemble teams composed by the Decider Agent for different downstream tasks. {\\em Third}, we introduce the focal diversity metric to better model the error correlations among multiple LLMs further improving the generalization performance of the Decider Agent, which actively prunes the ensemble combinations. By focal diversity, we enhance performance across tasks by effectively promoting reward-aware and policy-adaptive ensemble selection and inference fusion. \nExtensive evaluations on five benchmarks show that RL-Focal achieves the performance improvement of 8.48\\% with an ensemble of small size \ncompared to the best individual LLM in a pool and offers stronger robustness. Code is available at  \\url{https://anonymous.4open.science/r/rl-focal-8DCF/}", "tldr": "", "keywords": ["ensemble methods", "LLMs", "reinforcement learning", "diversity", "meta learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/116173b899f9968f2ce17e37944554ab2cb52045.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces RL-Focal, a novel two-stage Reinforcement Learning (RL) agent framework designed for robust and adaptive multi-agent systems built upon existing LLMs. Extensive evaluations on five benchmarks demonstrate that RL-Focal achieves an 8.48% performance improvement with a small ensemble compared to the best individual LLM, while also offering stronger robustness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Well motivated.\n* Improvement on BBH seems significant.\n* Methods are described in detail"}, "weaknesses": {"value": "The manuscript has substantial room for improvement, particularly in the representation and experimental design.\n* The manuscript's structure seems unbalanced. Only two of the nine pages are dedicated to describing experimental results. Given the apparent lack of a theoretical contribution, the content devoted to methods and general descriptions should be significantly compressed to allow for a deeper discussion of the findings and ablations.\n* The current experiments and ablations are limited. I recommend performing a detailed analysis of model selection and usage. Specifically the activation frequency or utilization ratio of each individual model within the agent system across different tasks.\n* Since models like Llama 3 and Mixtral show strong inherent performance (e.g., on MMLU), are these models frequently selected for the ensemble within the proposed agent system?\n\nTo better understand the system's core capabilities, please consider the following baselines:\n* Evaluate the ensemble performance of only the top two or three best-performing models on a specific task to establish a powerful baseline.\n* Evaluate the baseline that combining the top $k$ outputs (e.g., $k=5$ outputs) from the best models (best of N).\n* Computational cost and improvement compared to these baselines."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M0rme01ZZ0", "forum": "mhalNLINnn", "replyto": "mhalNLINnn", "signatures": ["ICLR.cc/2026/Conference/Submission14891/Reviewer_FG51"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14891/Reviewer_FG51"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761117511015, "cdate": 1761117511015, "tmdate": 1762925234994, "mdate": 1762925234994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RL-Focal, a two-stage multi-agent RL framework that dynamically routes and ensembles LLMs. The stage-1 Decider agent learns to select a small subset of models per query by optimizing both error diversity and reasoning performance with task-adaptive rewards and policies. The stage-2 Fusion agent learns to resolve conflicts and fuse the selected models outputs, and a new focal diversity metric models error correlations to improve generalization in selection and fusion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tTackles an important, practical problem: adaptive, query-wise ensembling/routing among LLMs rather than static majority voting.\n2.\tAblations and sensitivity analyses help understand behavior."}, "weaknesses": {"value": "1.\t**Poor writing/formatting.**  1) Lines 50–51 contain content that should not appear in the paper; please remove or rewrite appropriately. 2) The captions/layout for Figure 3 and Figure 4 have almost no spacing, which hurts readability. Please increase the vertical spacing and ensure consistent caption styling.\n2.\t**Overstated novelty in the problem formulation.** The paper claims to be the first to formulate LLM ensembling as a POMDP, yet prior work (e.g., RLAE[A], DER [B]) already models LLM-ensemble reasoning as an MDP. Please clarify the substantive differences between your method and prior work.\n3.\t**Incomplete reporting in Table 1.** Several entries are missing (marked “–”), preventing a complete comparison across datasets. Please fill in the absent results or justify why they are unavailable.\n4.\t**Lack of same-setting SOTA baselines in the main table.** Table 1 compares RL-Focal primarily against base models; strong ensemble/router baselines are not included there under the same pool and evaluation protocol. For fairness, include leading SOTA methods in Table 1 (or provide a unified main results table) under identical settings.\n5.\t**Metric inconsistency for Qwen2.5-72B on MMLU.** The paper reports 75.01, whereas widely cited numbers are around 86.1 [C].\n6.\t**Minor issues.** Occasionally inconsistent capitalization (“LLama” vs “LLaMA”).\n\n\n[A] RLAE: Reinforcement Learning-Assisted Ensemble for LLMs, arxiv 2025.\n\n[B] Efficient Dynamic Ensembling for Multiple LLM Experts, IJCAI 2025.\n\n[B] Qwen2.5 Technical Report, 2024."}, "questions": {"value": "see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sm9VJzNPMl", "forum": "mhalNLINnn", "replyto": "mhalNLINnn", "signatures": ["ICLR.cc/2026/Conference/Submission14891/Reviewer_LM4H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14891/Reviewer_LM4H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533470725, "cdate": 1761533470725, "tmdate": 1762925234530, "mdate": 1762925234530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Introduces RL-Focal. This uses RL to route queries to the best subset of LLMs from a pool. Then another agent fuses the ensembles responses together. They also introduce a new focal diversity metric to improve pruning performance. The paper demonstrates an effective performance increase against popular benchmarks and baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Interesting two-stage formulation separating selection (Decider) and combination (Fusion), with a multi-agent RL formulation and a centralized critic to stabilize training. Algorithms and training loops are clearly described (Algorithm 1 and 2). Furthermore, the paper attempts cost accounting and shows wall-clock/param comparisons in Appendix E (encouraging effort to quantify cost)."}, "weaknesses": {"value": "There are some similar RL ensemble approaches which limit the novelty (i.e. RLAE can in effect prune LLMs by lowering weights near zero), although they are formulated differently. The paper motivates RL via online adaptivity, but an explicit demonstration of that advantage would clarify necessity. Furthermore, training which uses two RL policies and a centralized critic adds significant computational overhead over supervised learning methods, though this is perhaps offset by the lower inference cost. It would be nice to have some further details regarding the impact of warm starting as well as experiments task distribution shifts (i.e. start with maths, end with reasoning to highlight the strengths of RL). Furthermore, I am surprised such small networks are able learn complexities of routing and combining LLM outputs. Lastly, it would be nice to see the RL training curves (performance over episodes)."}, "questions": {"value": "Do the advantages of online RL here justify the training costs of over supervised or simpler RL frameworks? What differences in approaches leads to the difference in performance between LLM-TOPLA and RL-Focal on GSM8k?\nDo you have any results for performance of the difference in performance between the warm start and final RL tuned model?\nDid you experiment with higher parameter counts? Are there any intuitions behind why it was effective with so few?\nAre there any patterns or reasons why certain LLMs are chosen by the policy? Does this change over training time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SvZnT8B47V", "forum": "mhalNLINnn", "replyto": "mhalNLINnn", "signatures": ["ICLR.cc/2026/Conference/Submission14891/Reviewer_twgk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14891/Reviewer_twgk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857521149, "cdate": 1761857521149, "tmdate": 1762925233958, "mdate": 1762925233958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}