{"id": "hs9lwjH1bJ", "number": 22739, "cdate": 1758334889161, "mdate": 1759896849507, "content": {"title": "GRACE: Generative Representation Learning via Contrastive Policy Optimization", "abstract": "Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black-box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce \\GRACE{} (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy $\\pi_\\theta$ that produces explicit, human-interpretable rationales—structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query--positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross-category gains: averaged over four backbones, the supervised setting improves overall score by 11.5\\% over base models, and the unsupervised variant adds 6.9\\%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent decision traces.", "tldr": "GRACE reimagines contrastive learning as reward-guided generative reasoning, turning LLMs into interpretable embedders that generate explicit rationale traces. It boosts MTEB performance by up to 11.5% while preserving general capabilities.", "keywords": ["Large Language Models", "Text Representation", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d762182799f2033a389f22ced90d3b472d84039.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work proposes a new framework for training Large Language Models (LLMs) as interpretable text encoders by reinterpreting contrastive objectives as rewards rather than losses. Instead of producing static embeddings, GRACE trains LLMs as policy models that generate explicit rationales before deriving embeddings from those rationales. Using policy-gradient optimization based on GRPO, GRACE aligns query-positive similarity and discourages query-negative similarity through a multi-component reward incorporating contrastive alignment, consistency, and hard negative mining. Experiments on the MTEB benchmark show strong cross-task gains over base models while preserving general reasoning, math, and coding capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- GRACE delivers consistent and cross-category improvements on the MTEB benchmark, with a significant gain over baselines.\n- The introduction clearly articulates the fundamental contradiction in existing embedding models and motivates GRACE as a conceptually sound fix.\n- GRACE is architecture-agnostic. It can be applied to any instruction-tuned LLM without modifying backbone structure."}, "weaknesses": {"value": "- By optimizing for rationale quality and semantic contrast simultaneously, GRACE might bias representations toward linguistic fluency or stylistic patterns rather than true semantic alignment.\n- The framework’s reliance on policy optimization with sampled rationales makes it computationally heavier and more memory-intensive than deterministic encoders.\n- All evaluations are centered on MTEB, which is known to correlate strongly with models optimized for textual similarity tasks. There is no evidence that GRACE embeddings generalize beyond MTEB-style benchmarks. Meanwhile, the reward formulation (contrastive alignment + consistency + hard negative mining) implicitly encodes assumptions about semantic similarity that may not generalize across domains (e.g., legal or medical text). There’s also a risk that reward weights are tuned for specific datasets, leading to domain overfitting or poor generalization."}, "questions": {"value": "- How did the authors define and evaluate “human-readable” rationales? Are they consistent with ground-truth semantics or just stylistically coherent?\n- During training, did the authors observe training instability or reward collapse during GRPO updates? How did the authors tune the reward weights for contrastive, consistency, and negative components?\n- Since GRACE encourages rationale generation, models may produce verbose or generic explanations that sound fluent but carry little semantic value. How did the authors regularize or constrain rationale length and content to ensure concise yet meaningful reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3UMEdoE3Sy", "forum": "hs9lwjH1bJ", "replyto": "hs9lwjH1bJ", "signatures": ["ICLR.cc/2026/Conference/Submission22739/Reviewer_eAv2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22739/Reviewer_eAv2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724822984, "cdate": 1761724822984, "tmdate": 1762942366002, "mdate": 1762942366002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new method that improves embeddings based on generative models. The method is more interpretable than the traditional contrastive learning set up. Authors demonstrate performance gains compared to base models and in general domain tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- New method utilizing generative model capabilities to create an interpretable reward signal. The method shows clear improvements over the baselines. \n\n- Authors conduct detailed ablation studies.\n\n- Performance on general domain tasks is preserved."}, "weaknesses": {"value": "- Lack of statistical significance reporting. To ensure the improvement was not a result of noise, it would be appropriate to report confidence intervals or p-values.\n\n- Authors claim their method is more interpretable but interpretability is not evaluated. Examples of the rationales are also not provided, at least in the main text."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "GHygIJu3aQ", "forum": "hs9lwjH1bJ", "replyto": "hs9lwjH1bJ", "signatures": ["ICLR.cc/2026/Conference/Submission22739/Reviewer_uisq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22739/Reviewer_uisq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935183819, "cdate": 1761935183819, "tmdate": 1762942365748, "mdate": 1762942365748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GRACE, a novel framework that reimagines how large language models (LLMs) can be trained as text encoders.  GRACE treats contrastive signals as rewards that guide a generative policy. The LLM acts as a policy that generates explicit, human-readable rationales for its semantic understanding. These rationales are then pooled into embeddings. Through policy gradient optimization, the model is trained to maximize similarity between query-positive pairs and minimize similarity with negatives. The method is evaluated on the MTEB benchmark and shows significant improvements."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Unifies generative reasoning and embedding learning, boosting representation quality while preserving general capabilities.\n- Extensive MTEB evaluation shows consistent gains in both supervised  and unsupervised settings across multiple reinforcement learning algorithms.\n- The appendix provides additional theoretical and empirical analysis. This enhances the paper's reproducibility"}, "weaknesses": {"value": "- The paper would benefit from an evaluation of whether the generated rationales faithfully explain the embedding process. Currently, the examples seem to paraphrase or extend the original content, rather than justifying the model's semantic decisions.\n- High inference latency due to autoregressive generation limits practical deployment. There are existing, highly efficient encoder-only models that provide strong embedding performance."}, "questions": {"value": "- Could you discuss specific application scenarios or use cases where it is particularly advantageous to have a single model that performs both tasks, as opposed to using separate, specialized models for generation and embedding?\n- In Table 3, , what is the specific baseline model to which the \"Grace-3B w/ [RL Algorithm]\" results are compared? Additionally, there appears to be a typo in the text reference on line 402, which reads \"Table 3.4.2\".\n- How is $\\mathcal{R}_{final}^{(i,k)}$ in Equation 10 precisely defined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0heT6L5tMk", "forum": "hs9lwjH1bJ", "replyto": "hs9lwjH1bJ", "signatures": ["ICLR.cc/2026/Conference/Submission22739/Reviewer_vZAz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22739/Reviewer_vZAz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762175830628, "cdate": 1762175830628, "tmdate": 1762942365505, "mdate": 1762942365505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents GRACE (Generative Representation Learning via Contrastive Policy Optimization), a framework that reframes contrastive objectives as rewards for large language models. Instead of minimizing contrastive losses, GRACE applies policy-gradient optimization (e.g., GRPO) to maximize semantic alignment between queries and positives while discouraging hard negatives. The LLM generates textual rationales, from which embeddings are derived via mean pooling. Experiments on MTEB benchmarks across several instruction-tuned backbones show 6–11% average improvements in embedding quality, with minimal loss in general generative ability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Method: Elegant reformulation of contrastive learning into a reinforcement learning (RL) framework using explicit rewards rather than losses.\n\n2. Interpretability: The introduction of “rationales” offers a clear path toward explainable embeddings.\n\n3. Broad empirical validation: Results are reported on both supervised and unsupervised settings with multiple backbones, and the ablations (λ₁, λ₂ grid, alternative RL algorithms) are systematically conducted.\n\n4. Practical impact: Demonstrates that reward-based generative alignment can mitigate the trade-off between embedding quality and generative capability."}, "weaknesses": {"value": "1.Theoretical contribution is incremental.The paper frames GRACE as a “generative reformulation” of contrastive learning but, in practice, the proposed reward is a simple contrastive score built from similarity differences. The paper provides only a high-level convergence discussion that bounds the gradient norm by the magnitude of its reward terms, without offering non-trivial guarantees or formal comparison to the optimization behavior of standard contrastive learning. Consequently, the claimed “unification of contrastive and RL paradigms” feels more like a notational reinterpretation than a fundamentally new theory.\n\n2.Ablation design does not isolate the effect of RL. The supervised comparison includes four settings (Base, Base w/ reasoning, CL, GRACE), which partially decouple the contribution of reasoning prompts and RL. However, a crucial baseline is missing — Contrastive Learning + Rationale Embeddings trained with the same rationale generation path but optimized via InfoNCE. Without this, it is difficult to cleanly disentangle whether GRACE’s improvements arise from the reinforcement update or simply from the generative representation pathway.\n\n3.The general-capability evaluation in Table 4 maybe unreliable. CL fine-tuning collapses performance to 0.0 on five of six tasks across all backbones (except FEVER), an implausible pattern for standard contrastive tuning. The paper provides no diagnostics or ablations to explain this, yet uses it to claim that “CL severely damages general-domain ability.” Without verification, variance reporting, or analysis of training setup, this conclusion may reflect issues in the CL setup rather than an inherent flaw of contrastive objectives."}, "questions": {"value": "1.Disentangling RL vs. generative representation: could you implement or report a baseline that uses rationale-conditioned embeddings trained with a standard InfoNCE loss (i.e., CL + rationale) to quantify how much of the improvement is due purely to the RL optimization step?\n\n2.Interpretability validation: the paper already include a few illustrative rationale examples in the appendix, but they remain anecdotal. To substantiate the strong interpretability claim, it would be very helpful to (i) provide more systematic qualitative comparisons across Base / CL / GRACE, and (ii) add a small-scale human or automatic evaluation of rationale quality (e.g., relevance/usefulness to the similarity decision, or faithfulness to the underlying embedding behavior)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vSCpbfjnyE", "forum": "hs9lwjH1bJ", "replyto": "hs9lwjH1bJ", "signatures": ["ICLR.cc/2026/Conference/Submission22739/Reviewer_Bk7x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22739/Reviewer_Bk7x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762215871937, "cdate": 1762215871937, "tmdate": 1762942365305, "mdate": 1762942365305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GRACE (Generative Representation Learning via Contrastive Policy Optimization) which uses contrastive learning objectives as a reward function to guide a generative policy (i.e. an LLM). The LLM policy generates human-interpretable rationales which are structured natural language explanations of its understanding. These rationales are mean-pooled to obtain embeddings. With this new reward function, existing LLMs can be finetuned with reinforcement learning to improve their embedding performance while retaining most of their generative performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is fairly easy to read and follow\n\n* GRACE can improve embedding performance without affecting the generative performance. And using the contrastive loss as a reward for finetuning LLMs with RL seems to be novel."}, "weaknesses": {"value": "* It is unclear what the rationale output is exactly. It would be good to add some qualitative examples for it. Because the paper claims (in the abstract for instance) that the LLM produces human-interpretable rationales and that the proposed method leads to transparent decision traces. But these claims are not explicitly evaluated. Ideally, there should be a human study to validate improvements in these aspects over the baseline LLM.\n\n* Fig. 3: It seems that performance is quite sensitive to the choice of the hard negative mining weight $\\lambda_2$. Even between the supervised and unsupervised paradigm, a different $\\lambda_2$ is needed for the best performance. Given that training is expensive, it would be good to have some heuristics to select good hyperparameters. Also it would be good to report if the $\\lambda_1, \\lambda_2$ hyperparameters also had to be tuned for the other base LLM experiments with GRACE.\n\n* It is also unclear if there could be any potential reward hacking [W1] with the proposed contrastive reward (e.g. if there are certain text patterns that yield high rewards). There needs to be some discussion on this (the earlier suggested human study could reveal if this problem exists).\n\n### References\n\n[W1] Pang et al., \"Reward Gaming in Conditional Text Generation\", ACL 2023"}, "questions": {"value": "Please address my comments/questions from the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5Xt56dBc0Q", "forum": "hs9lwjH1bJ", "replyto": "hs9lwjH1bJ", "signatures": ["ICLR.cc/2026/Conference/Submission22739/Reviewer_yvNK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22739/Reviewer_yvNK"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225021635, "cdate": 1762225021635, "tmdate": 1762942364931, "mdate": 1762942364931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}