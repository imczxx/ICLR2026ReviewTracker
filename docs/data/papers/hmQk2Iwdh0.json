{"id": "hmQk2Iwdh0", "number": 17771, "cdate": 1758280393634, "mdate": 1759897155048, "content": {"title": "Reliability-Adjusted Prioritized Experience Replay", "abstract": "Experience replay enables data-efficient learning from past experiences in online reinforcement learning agents. Traditionally, experiences were sampled uniformly from a replay buffer, regardless of differences in experience-specific learning potential. In an effort to sample more efficiently, researchers introduced Prioritized Experience Replay (PER). In this paper, we propose an extension to PER by introducing a novel measure of temporal difference error reliability. We theoretically show that the resulting transition selection algorithm, Reliability-adjusted Prioritized Experience Replay (ReaPER), enables more efficient learning than PER. We further present empirical results showing\nthat ReaPER outperforms both uniform experience replay and PER across a diverse set of traditional environments including several classic control environments and the Atari-10 benchmark, which approximates the median score across the Atari-57 benchmark within one percent of variance.", "tldr": "We present Reliability-adjusted Prioritized Experience Replay, which boosts data efficiency over Prioritized Experience Replay by weighting samples with a novel TD-error reliability measure, achieving superior results on control tasks and Atari.", "keywords": ["Deep Reinforcement Learning", "Temporal Difference Learning", "Experience Replay"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e6966916f86387e54922a8dd403d89260a1052c.pdf", "supplementary_material": "/attachment/0a0c41664d2bd0ea3d02afeb68b17e2efbd4b73d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Reliability-adjusted Prioritized Experience Replay (ReaPER), an enhanced sampling method for reinforcement learning that addresses a key flaw in standard Prioritized Experience Replay (PER). While PER prioritizes training on transitions with high Temporal Difference Errors (TDEs), this paper claims that it overlooks the fact that the target Q-value used to calculate this error may be unreliable, potentially misdirecting the learning process. ReaPER solves this by introducing **a reliability score**, which weights each transition's priority not only by its TDE but also by the stability of its target value, **defined as being inversely proportional to the sum of TDEs in subsequent states of the same trajectory.** The authors provide both theoretical proof that this reliability-adjusted approach leads to faster convergence and lower error , and empirical results in both classic control environments and the Atari-10 benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Providing some theoretical results to verify the effectiveness of this proposed method.\n\n2. A well-structured motivation"}, "weaknesses": {"value": "1. A limited set of experiments\n\n2. Further discussion is needed regarding the validity of this assumption."}, "questions": {"value": "I have the following questions.\n\n### 1. Insufficient Scope of Algorithmic Validation\n\nFirst and foremost, the **algorithmic scope of the experiments appears insufficient**. Deep Double Q-Network (DDQN) is now considered as a classical algorithm. Consequently, demonstrating the method's effectiveness exclusively within the DDQN framework raises questions regarding its general applicability and robustness. The paper would be significantly more compelling and the findings more rigorously validated if the method's efficacy were also demonstrated across a broader range of popular algorithms, such as **Deep Deterministic Policy Gradient (DDPG)**, **Soft Actor-Critic (SAC)**, or advanced value-based methods after **Rainbow**. Expanding the experimental scope is critical for establishing the generalizability of the proposed approach.\n\n---\n\n### 2. Justification Required for Assumption in Equation (9)\n\nFurther discussion and justification are required regarding the assumption presented in Equation (9). A counterexample, frequently observed in complex, multi-stage tasks (such as Go or Chess), suggests that Q-value estimation difficulty does not always monotonically decrease. Specifically, the difficulty of reliable Q-value estimation often peaks during the initial and mid-game phases due to high uncertainty and complex structure, while the end-game becomes computationally simpler. Therefore, the authors must provide a more detailed and robust justification for this assumption, addressing how it holds in the context of tasks exhibiting these non-monotonic difficulty dynamics. For instance, in [1], it looks like that the authors used model prediction components to handle these situations.\n\n[1] Oh, Youngmin, et al. \"Model-augmented prioritized experience replay.\" International Conference on Learning Representations. 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WnISNkzsEO", "forum": "hmQk2Iwdh0", "replyto": "hmQk2Iwdh0", "signatures": ["ICLR.cc/2026/Conference/Submission17771/Reviewer_bfXy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17771/Reviewer_bfXy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17771/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760800135410, "cdate": 1760800135410, "tmdate": 1762927610932, "mdate": 1762927610932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an extension of prioritized experience replay mechanism for off-policy reinforcement learning algorithms. Specifically, the paper first conducts formal analysis of bias-error interaction in prioritized sampling and its impact on Q-value estimation, then practically proposes the reliability-adjusted experience replay. The proposed method down‑weights TD errors using a trajectory‑wise “reliability” score derived from downstream absolute TD errors and provide theoretical guarantees through convergence hierarchy and variance reduction of the Q function update to show better sample efficiency over standard experience replay framework with uniform sampling. The empirical experiments on continuous control tasks and Atari shows outperforming performances over both prioritized experience reply and uniform sampling experience sampling within one percent of variance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed concept is well-motivated with theoretical analysis and easy to implement on top of prioritized experience replay framework.\n2. The empirical analysis on both continuous control and Atari-10 shows robust and superior performance against all the baselines.\n3. The writing is logically fluent and easy to follow."}, "weaknesses": {"value": "1. The assumption appears overly strong. Specifically, Assumption 3.4 relies on a bias bound that presumes near-optimal trajectories. During training, policies are far from optimal, so this bound rarely holds early on. Moreover, function approximation and bootstrapping introduce high variance and correlation in TD errors, making the downstream sum an unreliable proxy for bias. Especially, in partially observed environments, future TD errors may fluctuate unpredictably.\n\n2. The empirical experiment result. It is not clear how many random seed was involved for the Atari-10 experiment and there's no confidence interval in the Figure 3 Right. In addition, there is no runtime analysis, which cannot prove that whether ReaPER is practically efficient for training or just theoretically \"sample efficient\" for training. (i.e., Using less timestamp but each timestamp takes way longer runtime).\n\n3. The empirical experiment setting. The authors only demonstrate the ReaPER with DoubleDQN, which is not sufficient to support the claim of \"algorithm-agnostic and can be used within any off-policy RL algorithm\".\n\n4. Missing comparison with state-of-the-arts. Although the author mentions many related works on PER, none of them are being included into the comparison. This makes it hard to evaluate the contribution and novelty of the proposed method given there exists many variants of PER."}, "questions": {"value": "1. How often does Assumption 3.4 approximately hold during training? Have you observed that before? If so, can you provide empirical diagnostics or plots validating this assumption?\n2. How robust is the method to episode length variability and partial observability?\n3. Why not include stronger baselines (e.g., PSER, ERO, NERS)?\n4. Can the authors provide confidence intervals, statistical significance tests, and seed counts for Atari results?\n5. Why are results limited to DDQN? \n6. How does the method scale to large replay buffers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xGEyR7Lz4H", "forum": "hmQk2Iwdh0", "replyto": "hmQk2Iwdh0", "signatures": ["ICLR.cc/2026/Conference/Submission17771/Reviewer_aaDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17771/Reviewer_aaDF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17771/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797097770, "cdate": 1761797097770, "tmdate": 1762927609981, "mdate": 1762927609981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce Reliability-Adjusted Prioritized Experience Replay (ReaPER), an improvement over traditional Prioritized Experience Replay (PER) in reinforcement learning.  PER prioritizes transitions based on temporal-difference errors to enhance learning efficiency, but it can fail when TDE-based targets are unreliable, leading to inaccurate value estimates.  ReaPER addresses this issue by incorporating a reliability score derived from downstream TDEs, ensuring that transitions with more trustworthy targets are sampled more frequently. In simpler terms, ReaPER applies a backward-decaying weighting scheme from the episode’s end toward its beginning.\n\nThe paper provides theoretical analyses showing improved convergence and variance reduction under reasonable assumptions, supported by extensive experiments on CartPole, Acrobot, LunarLander, and the Atari-10 benchmark. Empirically, ReaPER consistently outperforms PER, achieving faster convergence and higher peak performance, while requiring minimal hyperparameter tuning and introducing only modest computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is **well-written and clearly structured**.\n- theoretical foundation linking reliability scores to TDE target biases.\n- Clear methodological improvements over standard PER.  \n- Algorithm is model-agnostic and straightforward to implement in existing off-policy methods."}, "weaknesses": {"value": "1. **Limited baseline comparisons.**  The paper primarily compares ReaPER only against PER. While the authors justify this by PER’s ongoing adoption in SOTA systems (e.g., Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update) would make the empirical evidence more convincing.\n2. ReaPER does **not robustly address episode-length variance**, limiting its generalizability. This introduces severe biases and undermines reliability scores, making them heavily dependent on arbitrary episode lengths rather than meaningful signal. This weakens the paper’s claims of broad applicability."}, "questions": {"value": "- Could you report **wall-clock training time per environment** for PER vs. ReaPER, and quantify the fraction of time spent updating cumulative TD sums? This would confirm that the additional computational cost is indeed negligible in practice.\n- How might the reliability computation be **extended to continuing tasks**? What pitfalls would you anticipate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AzTthaPHvw", "forum": "hmQk2Iwdh0", "replyto": "hmQk2Iwdh0", "signatures": ["ICLR.cc/2026/Conference/Submission17771/Reviewer_fBAN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17771/Reviewer_fBAN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17771/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988546005, "cdate": 1761988546005, "tmdate": 1762927608798, "mdate": 1762927608798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReaPER, an extension to Prioritized Experience Replay by incorporating a new measure of reliability of targets. By down weighting the priority of transitions with large TD error but low reliability, ReaPER is argued to benefit the sample efficiency improvements of PER while also mitigating the harmful updates caused by misleading TD errors. The main idea stems from the fact that the terminal transitions do not bootstrap and have more reliable targets. Target reliability is proposed to be inversely related to the sum of absolute values of TD errors of future transitions in the current episode.\n\nThe paper presents a suite of theoretical justification for why this proposed reliability measure controls the bias-error term in value error caused by bootstrapping. Further analyses discuss a sampling distribution based on this reliability measure that achieves lower value error than TD error-based prioritization and support the soundness of the ReaPER algorithm.\n\nFinally a variant of DDQN equipped with ReaPER is tested on two sets of RL problems: Three smaller classic control domains, and 10 larger Atari games. In classic control domains, ReaPER reaches a good evaluation performance faster than PER, while in Atari, ReaPER performs similar to PER in early learning but outperforms PER in terms of best-so-far checkpoint aggregated across 10 games."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This is a good paper and it should be accepted. Its writing is clear, coherent, and well organized. The ideas are easy to grasp and follow with good justification and explanation for most design choices. The authors use illustrative examples and theoretical justification to support their proposed reliability measure and its incorporation in to PER\n\n- The paper does not overreach in its claims and its conclusions mostly match the provided evidence. The theoretical results are presented with easy to follow language and simple notation (although I am not a theoretician). The experiment results mostly match the claims of the authors and I did not feel the authors used overly sensational or exaggerated language\n\n- The authors cover a range of related works that place their paper in the context of past research. There two other recent papers that directly\ninvestigate the limitations of PER and their inclusion in the introduction would better motivate this paper:\n  - Panahi, P.M., Patterson, A., White, M., & White, A. (2024). Investigating the Interplay of Prioritized Replay and Generalization. RLJ, 5, 2041-2058.\n  - Carrasco-Davis, R., Lee, S., Clopath, C., & Dabney, W. (2025). Uncertainty Prioritized Experience Replay. ArXiv, abs/2506.09270.\n\n- The background section is clear and comprehensive, describing the RL problem and relevant objects for this research: MDP, Q-function, TD-error, Experience Replay, Sampling distribution\n\n- The theory section is nicely written, and is easy to understand. The paper does a good job making the theory accessible to a wider audience with intuitive reasoning, leaving the proofs for the Appendix (I did not check the proofs in the Appendix)\n\n- The ReaPER algorithm is clearly described in Pseudocode and its design choices and hyper parameters explained\n\n- The experiment details and setup are fully characterized including environment and agent details, choice of hyper parameters, and evaluation scheme"}, "weaknesses": {"value": "- Most of the justification and intuition behind ReaPER and the proposed reliability score is presented in the tabular setting. Non-linear generalization can wildly change the q-values during learning especially when the data distribution is being modified. Including a discussion of how this reliability measure interacts with non-linear generalization (oversampling certain transitions, higher noise in predicted values, overgeneralization, etc.) would better support and justify the ReaPER algorithm\n\n- One issue with the background section is that only the tabular version of Q-learning is introduced whereas the experiments in this paper focus on DDQN. So concepts such as semi-gradient Q-learning, target network, and optimizers such as Adam are not introduced.\n\n- Regarding the classic control experiments\n  - Agents are run until they reach a certain performance threshold and reported performance metric is averaged over 100 evaluations. RL algorithms are known to have large variability in performance and achieving one instance of good performance does not mean the agent has successfully learned to perform well on a task. A better way to characterize performance in classic control domains would have been to show learning curves of performance during learning for a fixed time step budget aggregated over seeds.\n  - The reported values and shaded regions in Figure 2 are undefined. It seems to be a Box and Whisker plot which suggests the middle horizontal bar is the median score. It is also unclear how the % improvements reported in the text are calculated. \n\n- Regarding Atari experiments,\n  - It is somewhat unclear whether the experiment is run for 50 million steps (200 million frames) or 50 million frames. My guess is that it is 50 million steps (200 million frames).\n  - The learning curves are monotone which suggests that the reported metric is the best-so-far checkpoint. I think reporting performance over time aggregated over games is a better way to characterize performance of an RL algorithm in Atari."}, "questions": {"value": "1. Regarding the % improvements statements in the paper, is the comparison based on individual agents (per-seed) then aggregated or are performance scores first aggregated and then compared to each other?\n\n2. How did you decide the aggregation scheme for Atari? If this is based on prior work please cite it. Or point me to where you mentioned this design choice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a3EwKzJu5s", "forum": "hmQk2Iwdh0", "replyto": "hmQk2Iwdh0", "signatures": ["ICLR.cc/2026/Conference/Submission17771/Reviewer_QGxj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17771/Reviewer_QGxj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17771/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050277464, "cdate": 1762050277464, "tmdate": 1762927608092, "mdate": 1762927608092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}