{"id": "xRgtzijvgc", "number": 5404, "cdate": 1757907486593, "mdate": 1763630014517, "content": {"title": "Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization", "abstract": "Critic-free methods like GRPO reduce memory demands by estimating advantages from multiple rollouts but tend to converge slowly, as critical learning signals are diluted by an abundance of uninformative samples and tokens. To tackle this challenge, we propose the **Dynamic Dual-Level Down-Sampling (D$^3$S)** framework that prioritizes the most informative samples and tokens across groups to improve the efficiency of policy optimization. D$^3$S operates along two levels: (1) the sample-level, which selects a subset of rollouts to maximize advantage variance ($\\text{Var}(A)$). We theoretically proved that this selection is positively correlated with the upper bound of the policy gradient norms, yielding higher policy gradients. (2) the token-level, which prioritizes tokens with a high product of advantage magnitude and policy entropy ($|A_{i,t}|\\times H_{i,t}$), focusing updates on tokens where the policy is both uncertain and impactful. Moreover, to prevent overfitting to high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by curriculum learning. This schedule starts with aggressive down-sampling to accelerate early learning and gradually relaxes to promote robust generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that integrating D$^3$S into advanced RL algorithms achieves state-of-the-art performance with generalization while requiring fewer samples and tokens across diverse reasoning benchmarks.", "tldr": "We propose the D$^3$S framework, an approach by dynamically selecting only a small subset of the most informative samples and tokens during training to achieve faster convergence and SOTA performance with less computation.", "keywords": ["Reinforcement Learning", "Policy Optimization", "Down-Sampling"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7745067d064a9a5ca36ed5d6294b945cf45bcd14.pdf", "supplementary_material": "/attachment/6cb596aed7642b6bf2acf8c79f9549e5be77be4a.zip"}, "replies": [{"content": {"summary": {"value": "This study uncovers that in group-relative advantage-based RL algorithms (e.g., GRPO), the theoretical upper bound of the policy gradient norm scales with advantage variance. Building on this insight, the authors propose Dynamic Dual-level Down-Sampling (D3S), a novel training framework that improves efficiency and performance through two key mechanisms:\n\nSample-level: selects rollouts that maximize advantage variance to enrich signal diversity;\nToken-level: prioritizes tokens with high entropy × |advantage|, focusing updates on regions that are both uncertain and influential.\nTo prevent overfitting, D3S employs a curriculum-inspired dynamic down-sampling schedule, gradually easing selection criteria as training progresses. Experiments on Qwen2.5 and Llama3.1 demonstrate consistent gains over baselines across multiple reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The thesis is well-written\n(2) The theoretical proof is reasonable and supports the contribution of this paper\n(3) The proposed algorithm is relatively concise and has certain generalization potential\n(4) The experiment was conducted from multiple perspectives and achieved a good empirical analysis"}, "weaknesses": {"value": "Recently, there have been many works on trajectory screening based on entropy and advantage, and the methods are relatively homogeneous. Please distinguish the differences between this work and previous ones through detailed comparative experiments and discussions. \n\nIn my view, the amplitude of the advantage value represents the difficulty of the problem (like [3]), and the positive impact of high-entropy trajectories on the optimization of strategies has also been discussed in a large number of studies [1,2]. The combined influence of advantage and entropy was also discussed in the paper [4].\n\n[1]: Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning\n[2]: The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models\n[3]: Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay\n[4]: RETHINKING ENTROPY INTERVENTIONS IN RLVR: AN ENTROPY CHANGE PERSPECTIVE"}, "questions": {"value": "Please refute the above weaknesses through discussion or experiment. I will adjust the score based on the author's feedback and the opinions of other reviewers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SurbOZXYgV", "forum": "xRgtzijvgc", "replyto": "xRgtzijvgc", "signatures": ["ICLR.cc/2026/Conference/Submission5404/Reviewer_CB3f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5404/Reviewer_CB3f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761472263020, "cdate": 1761472263020, "tmdate": 1762918040092, "mdate": 1762918040092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework called Dynamic Dual-Level Down-Sampling ($D^{3}S$), aimed at making critic-free reinforcement learning algorithms such as GRPO more efficient. The core problem is that these methods are often slow to converge because critical learning signals are diluted by an abundance of uninformative samples and tokens.\nIts core contributions are:\n1) Theoretical Justification: It first theoretically proves that the upper bound of the policy gradient norm is positively correlated with the advantage variance ($Var(A)$) of the sampled subset. This justifies its strategy of selecting samples to maximize this advantage variance, rather than reward variance ($Var(R)$) as other methods do.\n\n2) Dual-Level Down-Sampling: The $D^{3}S$ framework operates at the sample-level and token-level to select only the most valuable data for updates.\n\n3) Dynamic Sampling Schedule: To prevent overfitting to this high-signal data, $D^{3}S$ employs a dynamic schedule inspired by curriculum learning. It starts with aggressive down-sampling (using fewer, high-signal samples) to accelerate early learning and then gradually relaxes this (using more data) to promote robust generalization.\n\nExperiments on models like Qwen2.5 and Llama3.1 show that $D^{3}S$ achieves state-of-the-art performance on reasoning benchmarks while using significantly fewer tokens and achieving major training speedups."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1) This paper clearly defines the problem and presents a clear methodology, proposing a three-part strategy ($D^{3}S$) within critic-free frameworks like GRPO/GSPO to focus on the most informative samples and tokens from the source.\n2) This paper provides sufficient theoretical proof, presenting the upper bound of the GRPO gradient norm (Proposition 1). It further proves that when maximizing advantage variance on a subset, this upper bound increases with the advantage variance (Proposition 2), which forms the theoretical basis for the sample-level selection. Subsequently, the dual selection levels are integrated into a PPO-style objective.\n3) The experimental validation is thorough, conducted on models including Qwen2.5-Math-7B/1.5B and Llama3.1-8B, across a total of 7 benchmarks."}, "weaknesses": {"value": "1) This paper introduces the Dynamic Down-Sampling Schedule, which in turn brings at least four key hyperparameters: $N_{init}$ (initial sample size), $N_{final}$ (final sample size), $K_{init}$ (initial token ratio), and $K_{final}$ (final token ratio). The paper demonstrates in Figure 3 that this dynamic schedule is crucial for preventing overfitting. This implies that the model's final performance may be highly dependent on the careful selection of these four new hyperparameters. However, the paper does not provide a sensitivity analysis for these hyperparameters. This could make the cost of reproduction and application to new tasks very high.\n\n2) In Figure 4, $D^{3}S$ lowers policy entropy in well-aligned models (Qwen, OpenMath2) but sharply increases policy entropy in the unaligned Llama model. The paper attributes both of these contradictory behaviors to the ability of $D^{3}S$ to effectively balance exploration and exploitation. Why does the exact same algorithmic mechanism lead to exploration in one model and exploitation in another? The underlying mechanism for this is not fully substantiated."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dJny9DNSVa", "forum": "xRgtzijvgc", "replyto": "xRgtzijvgc", "signatures": ["ICLR.cc/2026/Conference/Submission5404/Reviewer_Bn5B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5404/Reviewer_Bn5B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741428425, "cdate": 1761741428425, "tmdate": 1762918039756, "mdate": 1762918039756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on enhancing the learning efficiency of current LLM RL algorithms by training on subsets of rollouts with high advantage variance and tokens that exhibit both high advantage value and high entropy. Meanwhile, the algorithm is equipped with a dynamic schedule that includes more rollouts and tokens to prevent overfitting. An empirical comparison with GRPO, GSPO, and PODS is provided on multiple math reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The algorithm is intuitive and well-explained. \n\nThe paper presents a thorough ablation study, demonstrating the contribution of each component."}, "weaknesses": {"value": "PODS already proposed to downsample rollouts. The advantage of $D^3S$ over the previous model is not immediately apparent.\n\nThe analysis of token masking and dynamic scheduling is not thorough enough to understand their effects. (Check the Question section)\n\nWith the additional cost of hyperparameters, the performance improvement from PODS is not obvious.\n\nThe paper can be better prepared, for example, by adding a comparison to PODS for analysis and including y-axis labels for Figures 3 and"}, "questions": {"value": "1. How is the sample usage rate compared to PODS?\n2. Is there any analysis on which tokens are masked off? Do they contain less information?\n3. Is there any evidence that, without a downsampling schedule, the model is overfitting? As shown in Figure 2(a), the sample usage is low again. Why can such zero-advantage samples help if they do not modify the gradient?\n4. The KL result where $D^3S$ is closer to the reference model is interesting. Why does it happen? Does it contribute to the performance of $D^3S$?\n5. Usually, entropy decreases during RL training, but why does Figure 4 show even increasing entropies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "swsxY9mhto", "forum": "xRgtzijvgc", "replyto": "xRgtzijvgc", "signatures": ["ICLR.cc/2026/Conference/Submission5404/Reviewer_wBN3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5404/Reviewer_wBN3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962330296, "cdate": 1761962330296, "tmdate": 1762918039307, "mdate": 1762918039307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback, which are crucial for improving our work. \n\n**A key concern** raised commonly (by reviewers 5NNR, Bn5B, and wBN3) is the sensitivity of the dynamic scheduling hyperparameters ($N$ and $K$). In response to it, we conducted a thorough ablation study, and the corresponding results have been included in the revised manuscript (see Section 4.4 and Table 4). These results demonstrate that D$^3$S remains robust to variations in hyperparameters within the tested range and consistently outperforms the vanilla GRPO. \n\n- **Sample Size ($N$):** Whether using $N = 8 \\rightarrow 16$ or $N = 16 \\rightarrow 32$, the average performance (Avg Pass@1/Pass@8) aligns closely with our original setting ($N = 8 \\rightarrow 32$) in the paper. $N = 16 \\rightarrow 32$ achieves an Avg Pass@1/Pass@8 of 34.41/56.16, closely matching the original configuration's 34.84/56.76.\n\n- **Token Ratio ($K$):** We evaluated various $K$ configurations, and the Pass@8 performance consistently falls within a narrow range (54.20% to 56.76%). Interestingly, a broader range configuration ($K = 5\\\\% \\rightarrow 30\\\\%$) achieved a higher Pass@1 performance (36.34%), further validating our scheduling strategy of progressively increasing data diversity.\n\nThese ablation studies confirm that the effectiveness of D$^3$S is not tied to specific hyperparameter settings or complex tuning. The sampling strategy of D$^3$S inherently balances high-signal data with diversity. By adopting a straightforward and intuitive dynamic range (e.g., starting with smaller $N,K$ values and scaling up), significant performance improvements can be consistently achieved.\n\n**In response to other individual concerns**, we have also made detailed replies and necessary revisions in paper corrspondingly. We believe these revisions may address the reviewers' concerns and significantly strengthen the paper.\n\nIn favor of reviewers’ ease to check in discussion, ablation results of hyper-parameters are presented below. Baseline configuration of D$^3$S is $N=8\\rightarrow 32, K=5\\\\%\\rightarrow 20\\\\%$ with rollout number of GRPO as $G=32$.\n\n| **Configuration** | **AIME24** | **AIME25** | **AMC23** | **GSM8k** | **MATH** | **Minerva** | **Olympiad** | **Average** |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| GRPO | 13.2/37.6 | 5.5/21.6 | 47.0/83.5 | 64.9/94.3 | 48.5/70.2 | 19.8/45.0 | 9.7/19.8 | 29.8/53.1 |\n| **D$^3$S** | **20.3**/**48.2** | 7.9/25.8 | 54.4/**87.1** | 73.4/95.7 | 52.2/71.5 | **25.0**/**48.2** | 10.7/20.8 | 34.3/**56.8** |\n| $N=8\\rightarrow 16$ | 20.1/46.2 | 7.9/23.2 | 56.5/85.0 | 76.1/**95.9** | 55.5/**71.7** | 23.0/46.9 | 11.1/21.0 | 35.8/55.7 |\n| $N=16\\rightarrow 32$ | 18.4/46.7 | 7.6/25.2 | 55.0/85.5 | 73.6/95.8 | 53.1/71.4 | 22.5/47.6 | 10.7/20.9 | 34.4/56.2 |\n| $K=5\\\\%\\rightarrow 30\\\\%$ | 19.7/43.4 | **8.9**/25.6 | **57.4**/81.3 | **77.6**/95.7 | **55.9**/**71.7** | 23.6/47.6 | **11.3**/21.2 | **36.3**/55.2 |\n| $K=5\\\\%\\rightarrow 40\\\\%$ | 18.1/41.1 | 7.6/24.6 | 55.2/82.5 | 72.5/95.4 | 54.1/71.5 | 21.7/46.9 | 10.2/20.8 | 34.2/54.7 |\n| $K=5\\\\%\\rightarrow 50\\\\%$ | 17.5/44.5 | 7.6/25.8 | 52.0/82.8 | 74.4/95.6 | 54.2/**71.7** | 24.1/47.9 | **11.3**/**21.4** | 34.4/55.7 |\n| $K=10\\\\%\\rightarrow 20\\\\%$ | 16.8/42.3 | 7.5/23.8 | 56.0/83.6 | 74.3/95.6 | 54.6/71.6 | 21.7/46.4 | 10.5/20.5 | 34.5/54.8 |\n| $K=10\\\\%\\rightarrow 30\\\\%$ | 17.3/46.1 | 8.1/24.8 | 53.7/86.4 | 72.7/95.2 | 53.7/71.5 | 22.1/47.0 | 10.3/20.8 | 34.0/56.0 |\n| $K=10\\\\%\\rightarrow 40\\\\%$ | 18.6/45.0 | 7.7/26.1 | 52.5/83.2 | 70.2/95.6 | 52.2/71.4 | 22.5/**48.2** | 10.4/20.6 | 33.4/55.7 |\n| $K=10\\\\%\\rightarrow 50\\\\%$ | 17.9/44.5 | 8.0/**27.8** | 52.3/83.6 | 72.7/95.6 | 53.9/71.5 | 22.2/47.2 | 10.7/20.7 | 34.0/55.8 |"}}, "id": "kzhgRWFx2W", "forum": "xRgtzijvgc", "replyto": "xRgtzijvgc", "signatures": ["ICLR.cc/2026/Conference/Submission5404/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5404/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5404/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763608135046, "cdate": 1763608135046, "tmdate": 1763608135046, "mdate": 1763608135046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a down-sampling approach for fine tuning LLMs with RL to speed up learning and convergence called D3S. The authors argue that using all the available interaction data can slow down learning because critical learning signals are overshadowed by many uninformative transitions. The proposal is to down-sample training data at two levels: (1) select a small set of trajectories to have large advantage variance, and (2) within each trajectory select transitions whose advantage magnitude multiplied by policy entropy is large. Finally, to avoid overfitting to the sub-selected data, the sub-sampling is decreased over training to promote large updates in early learning, while maintaining diversity and generalization toward the end of training.\n\nThe contributions are as follows,\n1. Proposing a down-sampling framework to improve training efficiency of critic-free policy optimization algorithms for fine tuning LLMs.\n2. Theoretical analysis to prove that maximizing the advantage variance increases the upper bound of the policy gradient norm, which can speed up learning.\n3. Experiments show improved sample efficiency, and better final performance on benchmarks compared with previous methods\n4. Ablation studies show different components of the algorithm contribute to its improved performance"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I believe this paper is a good contribution to the RL and LLM literature and should be considered for acceptance. Down-sampling training data to improve training efficiency is not common in prior work and may be interesting to the community. Furthermore, maximizing advantage by sub-selecting trajectories is a novel idea and it is nicely investigated in this work.\n\n- The paper's overall quality is good. The writing and content organization is good. The exposition and motivation is clear. The paper mostly does a good job covering prior work and placing their contribution in the context of the broader literature. The proposed algorithm is supported by intuitive and theoretical justification and the experiments are accompanied by sufficient details and analysis."}, "weaknesses": {"value": "- The paper may benefit from further discussion of prior work such as coresets for machine learning. For example: Mirzasoleiman, B., Bilmes, J.A., & Leskovec, J. (2019). Coresets for Data-efficient Training of Machine Learning Models. International Conference on Machine Learning.\n- The paper may also benefit from some discussion of prioritizing transitions in RL. For example: Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2015). Prioritized Experience Replay. arXiv: Learning.\n\n- some typos in the abstract and introduction\n  - line 017 “efficient”\n  - line 019 “proven”\n  - line 078 “data pool is gradually expanded”"}, "questions": {"value": "- The paper claims state of the art generalization for D3S. I don't think this claim is supported by the evidence. Maybe this claim needs further elaboration on what is meant by generalization in this case.\n- D3S also introduces some hyperparameters such as N_init, K_init, N_final, K_final. Are these values hard to choose or require additional resources to find good values for? Are they sensitive to the task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "V4Fh9SgARG", "forum": "xRgtzijvgc", "replyto": "xRgtzijvgc", "signatures": ["ICLR.cc/2026/Conference/Submission5404/Reviewer_5NNR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5404/Reviewer_5NNR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762895909148, "cdate": 1762895909148, "tmdate": 1762918039014, "mdate": 1762918039014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}