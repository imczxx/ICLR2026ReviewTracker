{"id": "YgOY1QTEZj", "number": 22021, "cdate": 1758324980495, "mdate": 1759896890798, "content": {"title": "Language-Guided 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering", "abstract": "Dynamic rendering methods often prioritize photometric fidelity while lacking explicit semantic representations, which constrains their ability to perform semantically guided rendering. To this end, we introduce Language-Guided 4D Gaussian Splatting (L4DGS), a lightweight framework for real-time dynamic scene rendering that integrates natural language into semantically structured 4D Gaussian representations. Central to L4DGS is a Sparse Multi-Scale Attention (SMSA) mechanism that enables fine-grained, language-driven control by emphasizing semantically relevant regions across space and time. To enforce semantic fidelity and spatial coherence, we propose a static regularization that aligns language-guided features with rendered outputs and ensures consistent depth. To further ensure temporal consistency, A dynamic regularization penalizes abnormal variations in semantics and depth over consecutive unit time intervals. L4DGS achieves a 16.1% improvement in PSNR, reduces perceptual error by 58.8%, and increases rendering speed by over 50\\%. Experimental results demonstrate the superiority of our approach in both visual quality and computational efficiency.", "tldr": "", "keywords": ["Novel View Synthesis", "Dynamic Scene", "Gaussian Splatting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38a9a80c7f4a86fe7e15d8103f651c1bcd11626c.pdf", "supplementary_material": "/attachment/ba3223f15490906b5c41da8d1619a1416b1dabac.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces a language-guided regularization loss to enhance semantic consistency and rendering quality in 4D Gaussian Splatting (4DGS)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea of integrating language guidance into 4DGS is interesting. The reported metric is high."}, "weaknesses": {"value": "1. Unclear relevance of physical motion to language guidance\n\n (line 354 to line 369) \"To further ensure physically plausible motion and accurate structural evolution, we extend our formulation with a depth-based regularization that constrains temporal changes in the predicted geometry. We penalize excessive depth fluctuations in Gaussian primitives across consecutive unit time intervals:\" The connection between motion regularization and language semantics is not well explained.\n\n\n2. Inadequate evaluation of “Language-Guided Semantic Consistency”\n\nThe semantic consistency claim is mainly demonstrated through optical flow visualizations. This seems inappropriate because optical flow operates at the pixel level and does not directly measure semantic or language-level consistency. More suitable semantic metrics and visualziation should be considered to substantiate this claim.\n\n3. lack of context for Figure 3\nThe paper states that “Figure 3 further confirms the key advantages of L4DGS. It enables language-driven control for accurate and localized scene editing.” However, Figure 3 is not clearly connected to 4DGS or explained in sufficient context. The figure and its caption should better demonstrate how language guidance enables localized editing within the 4DGS framework.\n\n4. Missing analysis of computational overhead\nThe paper lacks an analysis of the computational cost introduced by additional regularization terms (e.g., depth-based and CLIP-based losses). It would be helpful to quantify the impact on training speed and memory usage compared to vanilla 4DGS."}, "questions": {"value": "1. Can the authors provide concrete examples or quantitative results showing “language-driven control for accurate and localized scene editing”? Ideally, please show examples on datasets of 4DGS\n\n2. Training details and runtime feasibility\nThe paper claims that all experiments were conducted on a single RTX 3090 GPU and cost 20~30 minutes. How was this runtime measured?\nIs the CLIP-based language regularization computed online during training for all 20,000 or 14000 iterations?\nWhat image resolution is used as input to the CLIP model during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jecy7YuPLe", "forum": "YgOY1QTEZj", "replyto": "YgOY1QTEZj", "signatures": ["ICLR.cc/2026/Conference/Submission22021/Reviewer_aJtV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22021/Reviewer_aJtV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944979753, "cdate": 1761944979753, "tmdate": 1762942022186, "mdate": 1762942022186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces language-guided 4D Gaussian Splatting (L4DGS), a lightweight framework that integrates natural language guidance into real-time 4D Gaussian splatting for dynamic scene rendering. The paper points out that although the existing 3DGS has improved the rendering efficiency, it still lacks semantic control capabilities. The dynamic scene expansion version also has problems such as motion blur and scene drift. For the above issues, this paper combines the sparse multi-scale Attention (SMSA) used for cross-modal feature fusion with static and dynamic regularization mechanisms to ensure semantic and temporal consistency. It significantly outperforms existing methods in terms of rendering fidelity and computational efficiency, and the qualitative results demonstrate its ability to guide scene editing through language (such as deleting target objects)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The current mainstream dynamic rendering methods mostly rely on visual features and lack explicit control at the semantic level, making it impossible to align human language intentions with dynamic rendering results. L4DGS innovatively constructs a language-guided 4DGS framework, deeply integrating the 4DGS representation of natural language understanding and semantic perception.\n2. The proposed Sparse multi-scale Attention (SMSA) effectively aligns language and visual patterns and uses the top-k sparse strategy to improve interpretability and efficiency. The dual regularization design (static + dynamic) ensures spatial consistency and temporal consistency, and resolves the long-standing semantic drift and flickering issues in dynamic NeRF systems.\n3. Compared with leading baselines such as MixVoxels, K-Planes and Deformable4DGS, there have been substantial improvements in PSNR, LPIPS and rendering speed. The training time is reduced to a few minutes while maintaining real-time rendering quality. Support for prompt semantic operations (\" delete car \", \"delete person\") highlights the potential of interactive applications (for example, VR/AR, robot perception, content creation)."}, "weaknesses": {"value": "1. Although the paper emphasizes \"first language-embedded real-time 4D rendering\", there have already been many works combining 4DGS with semantics, such as 4-LEGS[1], 4D LangSplat[2], DHO[3]. Is the core difference between L4DGS and these works in \"temporal consistency\" or \"dynamic semantic alignment\"? Moreover, the text does not demonstrate how the semantic control of the editing object is achieved. Is there any difference from other semantic embedding methods?\n2. Although the fusion of CLIP features and sparse attention is mentioned, there is a lack of quantification or visualization to verify language consistency (such as attention map visualization, semantic distribution similarity). For instance, qualitative alignment images of language prompts and rendering results, CLIP-score or image-Text retrieval accuracy and other metrics.\n3. Both static and dynamic regularization have multiple λ hyperparameters, but in this paper, only \"learnable hyperparameters\" are mentioned, without ablation or stability analysis. Moreover, the sensitivity of SMSA parameters was not analyzed either. The influence of the \"top-k value\" (such as k taking 10, 20, 50) on performance was not analyzed, nor was the adaptive selection strategy of k explained. It is suggested to add: sensitivity experiments of λ, comparison of top-k values, and robustness verification for scenarios with different motion intensities or semantic complexities.\n\n[1]Fiebelman G, Cohen T, Morgenstern A, et al. 4‐LEGS: 4D Language Embedded Gaussian Splatting[C]//Computer Graphics Forum. 2025: e70085.\n\n[2] Li W, Zhou R, Zhou J, et al. 4d langsplat: 4d language gaussian splatting via multimodal large language models[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 22001-22011.\n\n[3] Yan Z, Liang Y, Cai S, et al. Divide-and-Conquer: Dual-Hierarchical Optimization for Semantic 4D Gaussian Spatting[J]. arXiv preprint arXiv:2503.19332, 2025."}, "questions": {"value": "1. The comparison with existing semantic rendering methods is insufficient. The innovative positioning needs to be strengthened. What are the essential differences from some methods based on 4DGS combined with semantics? The paper does not clearly define the core advantages of L4DGS in \"4D dynamic support\", \"attention mechanism\", and \"regularization strategy\", only mentioning \"L4DGS supports dynamics\", and does not quantitatively compare the performance gap between the two in dynamic semantic rendering.\n2. How robust is the system to fuzzy prompts or combined prompts (for example, \"The red chair near the window\")?\n3. How do the lamda and top-k hyperparameters in the paper affect the model performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ehcko6tOsw", "forum": "YgOY1QTEZj", "replyto": "YgOY1QTEZj", "signatures": ["ICLR.cc/2026/Conference/Submission22021/Reviewer_ZLKE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22021/Reviewer_ZLKE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968615568, "cdate": 1761968615568, "tmdate": 1762942021847, "mdate": 1762942021847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **L4DGS**, a lightweight real-time dynamic scene rendering framework that integrates natural language into a semantically structured 4D Gaussian representation. The proposed **Sparse Multi-Scale Attention (SMSA)** mechanism emphasizes semantically relevant regions in space and time, enabling fine-grained language-driven control. Furthermore, the combination of static and dynamic regularization effectively resolves temporal and semantic inconsistencies. The experiments are comprehensive and convincingly demonstrate the superiority of the proposed method as well as the effectiveness of each component."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes the first real-time 4D rendering algorithm with embedded language, demonstrating both effectiveness and innovation.\n- By combining static and dynamic regularization, it effectively addresses issues such as semantic drift, flickering, and identity instability."}, "weaknesses": {"value": "- The paper repeatedly mentions memory efficiency; however, the experimental section seems to lack sufficient discussion or quantitative analysis on memory consumption. It would be helpful to include additional experiments or data in this regard.\n- The paper mentions CLIP-based features and language guidance but does not specify how textual prompts are used in training and testing. Are prompts fixed, varied per scene, or user-provided at inference? More examples ofprompt-to-rendering alignment would clarify practical usability."}, "questions": {"value": "- Since SMSA guides L4DGS to focus on semantically important spatial regions, does it lead to any degradation in rendering quality for background areas?\n- Could the proposed dynamic regularization cause loss of motion detail in fast-moving scenes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f8OHyPkT7F", "forum": "YgOY1QTEZj", "replyto": "YgOY1QTEZj", "signatures": ["ICLR.cc/2026/Conference/Submission22021/Reviewer_PGpH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22021/Reviewer_PGpH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762100766912, "cdate": 1762100766912, "tmdate": 1762942021583, "mdate": 1762942021583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the probable visual and linguistic guidance for dynamic scene rendering. The authors propose the Sparse Multi-Scale Attention to better fuse vison and language features. Then, the novel static and dynamic regularizations are used to provide 3DGS with visual and linguistic guidance. The experiments show an improvement caused by the proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The visual and linguistic guidance is probably useful to enhance dynamic scene rendering.\n\n2. The experiments might demonstrate the effectiveness of the proposed methods."}, "weaknesses": {"value": "1. This paper is not well-written. The pipeline is quite confusing. How do you obtain the language description for each scene to generate linguistic guidance? How do you get the rendered feature map F_rendered in Equation.3? What does the \\lambda_o in Line 376 mean, and why the hyperparameters can be learnable? Which model do you use for guidance generation, CLIP or others, and could you cite the paper? How do you get GT depth map in Equation 4 for supervision? \n\n2. There might have several mistakes in the Paper. In Equation 4, how do you calculate the cosine similarity for depth, which is not a vector?\n\n3. The motivation is not clear. Why visual and linguistic guidance is useful for dynamic scene rendering? I hope the authors could provide a deep insight explanation.\n\n4. The experiments lack qualitative comparison. I think the author should visualize more results on the D-NeRF, HyperNeRF, Nerfies, long-sequence and iphone datasets.\n\n5. Lack of comparison with SOTA methods. Deformable-3D-gs[1], SC-GS[2] and Grid4D[3] might have better rendering quality on the D-NeRF dataset.\n\n6. The average PSNR in the last row of Table 2 in the supplementary might be incorrect, which should be 37.00.\n\n7. I am confused about the training time of the proposed methods. As shown in Table 2, how do you realize extremely fast inference to get the visual and linguistic guidance from a large model while reducing the training time to 5min on the D-NeRF dataset? If possible, could the authors provide more details?\n[1] Yang et.al. Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction. CVPR 2024.\n[2] Huang et.al. SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes. CVPR 2024\n[3] Xu et.al. Grid4D: 4D Decomposed Hash Encoding for High-Fidelity Dynamic Gaussian Splatting. NeurIPS 2024."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r9KBlEQ4a1", "forum": "YgOY1QTEZj", "replyto": "YgOY1QTEZj", "signatures": ["ICLR.cc/2026/Conference/Submission22021/Reviewer_LsMU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22021/Reviewer_LsMU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131515500, "cdate": 1762131515500, "tmdate": 1762942021280, "mdate": 1762942021280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}