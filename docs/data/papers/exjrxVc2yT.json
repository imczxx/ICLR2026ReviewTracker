{"id": "exjrxVc2yT", "number": 17807, "cdate": 1758280686200, "mdate": 1759897152408, "content": {"title": "SiNGER: A Clearer Voice Distills Vision Transformers Further", "abstract": "Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality.\nWhen knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals.\nThe key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts.\nThen, the refined teacher's features are distilled to a student. \nWe implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification.\nExtensive experiments show that \\oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations.", "tldr": "We propose SiNGER, a nullspace-guided LoRA framework that suppresses artifacts in Vision Transformer distillation while preserving informative representations, achieving state-of-the-art student performance.", "keywords": ["Vision foundation models", "model compression", "knowledge distillation", "representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3f938976d4ffc7188f6db62836a313222a065a3.pdf", "supplementary_material": "/attachment/b6051edfc19a69ee8739faf88c47eefa0c74c189.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces SiNGER (Singular Nullspace-Guided Energy Reallocation), a knowledge distillation (KD) framework designed to improve the compression of Vision Transformers (ViTs) by addressing high-norm artifacts in teacher representations. These artifacts, arising from power-iteration-like accumulation in ViT layers, dominate the distillation objective, causing students to overfit to noise and underutilize informative signals. SiNGER refines teacher features using a lightweight LoRA-based adapter that applies nullspace-guided perturbations to suppress artifacts while preserving downstream-relevant information. The refined features are then distilled to smaller student models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Principled Solution to a Core Problem: Identifies a fundamental limitation in ViT KD (artifact dominance) and resolves the trade-off between suppression and information preservation via nullspace guidance, leading to more effective knowledge transfer.\n\nEfficient and Minimalist Design: The LoRA adapter requires little structural change, making it easy to integrate into existing KD pipelines while maintaining computational efficiency."}, "weaknesses": {"value": "ViT-Specific Focus: The method is tailored to ViT artifacts (e.g., from self-attention scaling); its generalizability to other architectures like CNNs or hybrids isn't explored.\n\nDependency on Teacher Quality: Assumes well-pre-trained ViT teachers; if artifacts are severe or pre-training is suboptimal, refinement might not fully compensate.\n\nScope Limitations: Primarily evaluated on downstream vision tasks; extensions to multimodal or non-vision transformers aren't discussed."}, "questions": {"value": "How do you select the nullspace dimension or perturbation strength in practice? Are there hyperparameters, and how sensitive is performance to them (e.g., ablation results)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CtPQadMcxf", "forum": "exjrxVc2yT", "replyto": "exjrxVc2yT", "signatures": ["ICLR.cc/2026/Conference/Submission17807/Reviewer_Mz5w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17807/Reviewer_Mz5w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831298227, "cdate": 1761831298227, "tmdate": 1762927651683, "mdate": 1762927651683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of high-norm artifacts in ViTs that degrade knowledge distillation effectiveness. The authors propose SiNGER, a distillation framework that refines teacher features by applying perturbations guided toward the nullspace of subsequent layers. The key innovation is using LoRA-based adapters initialized with approximate nullspace vectors to suppress artifacts while preserving informative signals. Experiments demonstrate improvements across multiple downstream tasks including classification, segmentation, and depth estimation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-Motivated Problem: The paper identifies a concrete limitation in ViT distillation—high-norm artifacts dominate gradient flow and harm student learning.\n2. Comprehensive Evaluation: The multi-task evaluation spanning 10 benchmarks is thorough. Consistent gains on ImageNet and ADE-20K"}, "weaknesses": {"value": "1. Approximation Gap: The core theoretical claim assumes linear blocks, but ViT blocks are highly nonlinear.\n2. Incomplete Comparison with ViTKD"}, "questions": {"value": "1. Nullspace Verification: \n2. Can you scale to Larger Models?\n3. Ablation on Linearization: What happens if you use the full nonlinear block's Jacobian"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "otzP1mfTGD", "forum": "exjrxVc2yT", "replyto": "exjrxVc2yT", "signatures": ["ICLR.cc/2026/Conference/Submission17807/Reviewer_CqPa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17807/Reviewer_CqPa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996058044, "cdate": 1761996058044, "tmdate": 1762927651265, "mdate": 1762927651265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles a well-observed problem in Vision Transformer (ViT) distillation: high-norm \"artifact\" tokens in large ViTs dominate feature-level KD losses, so the student ends up overfitting to these outliers instead of learning the informative inlier structure. To address this, the authors propose SiNGER (Singular Nullspace-Guided Energy Reallocation), a distillation framework that first refines the teacher’s features and only then distills them. The key idea is to add a LoRA-style low-rank adapter to the teacher and to constrain its perturbations to (an approximation of) the left nullspace of the next block, so that: (i) high-norm outliers are suppressed, and (ii) downstream information is preserved because the perturbation lies in a direction that doesn't change the next layer's output. They couple this with two auxiliary losses: an outlier suppression loss (to explicitly shrink high-percentile tokens) and an information-preservation / Gram-matching loss (to keep patchwise relations intact). Experiments on several downstream tasks (ImageNet-1K, ADE-20K, NYUd-v2, domain shift benchmarks, fine-grained datasets) show that SiNGER improves over FitNet and ViTKD when distilling a ViT-Large to a ViT-Tiny, and produces visually \"cleaner\" feature maps with more uniform patch norms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear and well motivated problem formulation. The paper starts from a real and now increasingly reported issue in ViTs: high-norm background/low-semantics tokens that get amplified across residual blocks and hurt both interpretability and distillation. The link they make (artifact dominance -> gradient bias in feature KD) is very sensible and well motivated. \n\n2. Principled mechanism. Instead of just masking teacher features (like ViTKD), they try to change the teacher supervision itself in a way that is theoretically tied to the next layer: perturb only in (approximate) nullspace so the next block’s output is preserved. This is a neat way to reconcile \"suppress artifacts\" vs \"don't destroy useful signals\".\n\n3. Low-intrusion design. Using a LoRA-based adapter to implement the refinement is pragmatic: it's lightweight, it doesn't require re-training the whole teacher, and it aligns with current practice on parameter-efficient tuning.\n\n4. Good ablations. The nullspace initialization vs random init, the presence/absence of the information-preservation loss, and the small hyperparameter sweep on rank r and percentile a all support the central claim.\n\n5. Multi-task evaluation. Even though the numbers in Table 1 are a bit hard to read, the intent is good: they try to show that better, less-artifact supervision gives students that transfer better to unrelated tasks (segmentation, depth), not just ImageNet linear eval."}, "weaknesses": {"value": "1. Central assumption relies on a rough linearization. The core trick (perturb in the left nullspace of the next block so the next block's output is unchanged) is only feasible because they linearize the non-linear transformer block using the FFN weights and then pick the smallest singular vectors. But real blocks have attention, non-linearities, layernorm, residual couplings, and cross-token interactions. The paper cites an appendix showing FFN is the dominant source of norm amplification, but in the main text this is still an approximation, not a guarantee. A reviewer could fairly ask: how often does the adapter actually stay in the true (nonlinear) null directions during training? The ablation in Table 4a helps, but it’s still empirical. \n\n2. Evaluation scope is narrow. All main results are ViT-Large to ViT-Tiny, canonical ViT, every-second-layer alignment. That is a single, quite specific distillation path. There is no evidence it works for: (i) teacher and student of different families (e.g., DINOv2 to DeiT-Small), (ii) CNN to ViT, or (iii) same-size or near-size distillation where artifacts might behave differently. This makes the generality claim (\"artifact-aware KD for VFMs\") weaker than it sounds.\n\n3. Choice of baselines. They compare to FitNet and ViTKD, but they do not compare to the most natural artifact-aware teacher-side fixes like register tokens (Darcet et al., 2024), or to SINDER / DINOv3-style cleaned teachers as sources for distillation--both of which are already in their related work. If the claim is \"we are the principled way to keep info while suppressing artifacts\", then we need to see: what if you just gave the student a cleaner teacher in the first place? Right now that experiment is missing. \n\n4. Table 1 is confusing / formatting issues. The version here has lines that appear interleaved or misaligned (e.g., \"SiNGER 70.59 8.16 21.76 3.03 ...\"), which makes it hard to confirm the exact gains on each benchmark. For a distillation paper, exact numbers matter. This hurts Presentation. \n\n5. Cost and practicality not fully discussed. Attaching adapters to multiple teacher layers and running SVD-style initialization on (linearized) FFNs is not free. The paper doesn’t quantify the extra compute / memory during distillation vs. plain feature KD. This is important for practitioners.\n\n6. Long-tail result potentially goes in the wrong direction. The iNat2019 result is weaker and is brushed off as “long-tail is hard,” but that’s precisely a setting where better supervision and less artifact noise should help the student pick up minority classes. A short analysis or per-class breakdown would strengthen the story. \n\n7. Causality of the improvement is still a bit entangled. Three things change at once: teacher refinement, outlier loss, and Gram matching. We do see an ablation on Linfo, but we don’t see, for example, \"plain LoRA adapter without nullspace init\" vs \"nullspace init\" vs \"only percentile shrinkage\" on all tasks, just on a subset. It’s still plausible that a well-tuned teacher-side LoRA denoiser would get close."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xujCeNOe6Q", "forum": "exjrxVc2yT", "replyto": "exjrxVc2yT", "signatures": ["ICLR.cc/2026/Conference/Submission17807/Reviewer_YFDi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17807/Reviewer_YFDi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762127089667, "cdate": 1762127089667, "tmdate": 1762927650884, "mdate": 1762927650884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}