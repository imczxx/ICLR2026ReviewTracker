{"id": "ZSDmfoVvCs", "number": 12903, "cdate": 1758211485622, "mdate": 1759897478260, "content": {"title": "ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork", "abstract": "Learning to collaborate with previously unseen partners is a fundamental generalization challenge in multi-agent learning, known as Ad Hoc Teamwork (AHT). \nExisting AHT approaches often adopt a two-stage pipeline, where first, a fixed population of teammates is generated with the idea that they should be representative of the teammates that will be seen at deployment time, and second, an AHT agent is trained to collaborate well with agents in the population. \nTo date, the research community has focused on designing separate algorithms for each stage. This separation has led to algorithms that generate teammates with limited coverage of possible behaviors, and that ignore whether the generated teammates are easy to learn from for the AHT agent. \nFurthermore, algorithms for training AHT agents typically treat the set of training teammates as static, thus attempting to generalize to previously unseen partner agents without assuming any control over the set of training teammates.\nThis paper presents a unified framework for AHT by reformulating the problem as an open-ended learning process between an AHT agent and an adversarial teammate generator. \nWe introduce ROTATE, a regret-driven, open-ended training algorithm that alternates between improving the AHT agent and generating teammates that probe its deficiencies. \nExperiments across diverse two-player environments demonstrate that ROTATE significantly outperforms baselines at generalizing to an unseen set of evaluation teammates, thus establishing a new standard for robust and generalizable teamwork.", "tldr": "Introduces a regret-driven, open-ended learning framework and algorithm for ad hoc teamwork", "keywords": ["ad hoc teamwork", "open-ended learning", "multi-agent reinforcement learning", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd27ce72e127a01d60e15031523baade8118a071.pdf", "supplementary_material": "/attachment/36de1555536a19191b8064606414f34cf0cd16dc.pdf"}, "replies": [{"content": {"summary": {"value": "The paper reframes ad‑hoc teamwork as an open‑ended min–max process: a teammate generator maximizes the ego agent’s cooperative regret while the ego minimizes it, alternating over training. The key idea is state‑wise regret, optimized over SP/XP/SXP state distributions to discourage sabotaging teammates by construction. A population buffer stabilizes training under a non‑stationary teammate distribution. On six two‑agent tasks across LBF and Overcooked, the method outperforms strong baselines on 5/6 tasks, and a toy “destructive matrix game” illustrates that state‑wise regret strongly suppresses sabotage."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear problem reframing. Maximizing cross‑play with unknown partners is cast as minimizing cooperative regret, yielding a natural open‑ended training framework that aligns well with UED principles.\n\nObjective‑level protection against sabotage. Jointly optimizing state‑wise regret on SP/XP/SXP exposes weaknesses while enforcing compatibility with a best‑response partner, which is more principled than adversarial diversity only from the initial state.\n\nReasonable empirical support. Diverse tasks, 9–13 “benign” evaluation partners, a normalized score (upper‑bounded by an estimated BR), and targeted ablations (trajectory vs. state‑wise regret; with/without population buffer)."}, "weaknesses": {"value": "1.Narrow experimental scope. Evidence is confined to two agents with full observability. The absence of results on larger multi‑agent and partially observable settings (e.g., SMAC, GRF)[1] weakens claims of scalability.\n\n2.Lack of theory and stopping criteria. “Open‑endedness” lacks a formal definition and a practical stopping rule; there are no guarantees on coverage, convergence, or regret bounds, limiting deployability.\n\n3.Comparative evaluation is incomplete. Empirical comparisons against LIPO[2], MACOP and rigorously budget‑matched versions of BRDiv/CoMeDi are missing; fairness requires equal interaction budgets/compute.\n\n4.Positioning vs. prior work needs precision. The relationship to MACOP and related methods should be spelled out at the level of objective functions, “benign partner” constraints, stopping rules, and network design, to avoid conceptual conflation.\n\n5.Evaluation and ablations need tightening (detail add‑ons).\n\nThe normalized metric depends on an estimated BR upper bound; report sensitivity to BR approximation error.\n\nProvide weight sensitivity for SP/XP/SXP and justify the SXP term’s necessity.\n\nUse more random seeds—open‑ended procedures can have high variance.\n\nClarify environmental assumptions (mid‑episode policy switching/state resets); give an approximation strategy when resets are unavailable.\n\nDiscuss computational complexity as the partner space expands, and what approximations to BR/regret are admissible with guarantees.\n\nRef:\n\n[1]A survey of progress on cooperative multi-agent reinforcement learning in open environment\n\n[2] Generating Diverse Cooperative Agents by Learning Incompatible Policies"}, "questions": {"value": "1.Scaling to many agents. How does the method avoid combinatorial blow‑up for 5–10 agents or more? Would centralized training with decentralized execution, hierarchical BR, population BR, or fictitious‑play‑style approximations be viable, and at what cost?\n\n2.Multi‑modal partner distributions. If the partner distribution is genuinely multi‑modal, is a single ego policy sufficient? Would mixture‑of‑experts, latent‑variable policies, or distributionally robust objectives (e.g., CVaR) be required to capture distinct partner modes?\n\n3.Formalizing open‑endedness. What is the precise criterion—coverage growth, novelty accumulation, or monotone regret reduction? Please provide operational metrics (coverage/novelty/regret) and a stopping rule, accompanied by evidence.\n\n4.Human–AI collaboration. Can the method transfer to real human partners (e.g., Overcooked‑human)? Would demonstrations, preference modeling, or safety constraints be needed to bound the teammate generator’s search space?\n\n5.Visualization and interpretability. Please include state‑level sabotage heatmaps, SXP vs. XP occupancy differences, teammate embedding visualizations, and term‑wise causal ablations to show where and why each component works.\n\n6.Embodied multi‑agent and LLM integration[1]. Can the framework extend to embodied, partially observable, continuous‑control domains? Could LLMs serve as a teammate generator or a language‑mediated coordination channel for richer partner diversity and policy decomposition?\n\nRef:\n\n[1] \t\nMulti-agent embodied ai: Advances and future directions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DonAmdSqUE", "forum": "ZSDmfoVvCs", "replyto": "ZSDmfoVvCs", "signatures": ["ICLR.cc/2026/Conference/Submission12903/Reviewer_1eRE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12903/Reviewer_1eRE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665685262, "cdate": 1761665685262, "tmdate": 1762923682197, "mdate": 1762923682197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discuss ad hoc teamwork as an open-ended partner co-learning problem and introduces ROTATE, which optimizes a per-state cooperative regret objective while encouraging competent, non-adversarial teammates. The approach alternates between training the ego policy and generating partner policies using state distributions from self-play, cross-play, and switched-play interactions, aided by a population buffer. Experiments across cooperative benchmarks report improved generalization to unseen partners."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper foregrounds the self-sabotage failure mode in open-ended partner generation, clearly articulating why partners that deliberately depress cross-play (XP) can inflate training signals yet harm zero-shot coordination; this diagnosis sharpens evaluation design (e.g., beyond average XP) and motivates principled mitigation objectives."}, "weaknesses": {"value": "- Eq. 10 employs a fixed 0.5/0.5 weighting with no analytical justification, and the experiments do not analyze this hyperparameter.\n\n- The method section has poor readability, with unclear logic and difficult-to-follow exposition.\n\n- Missing ZSC-side baselines, especially some open-ended methods like COLE [1] and E3T [2]."}, "questions": {"value": "- Relationship to [3] and [4]: The method currently appears very similar to these two paper—could the authors clarify whether, under certain conditions, it degenerates to XP-min? How do the weights in Eq. 10 relate to XP-min’s hyperparameter ?\n\n- Role of regret: The paper lacks analysis of regret’s actual effect. Do the generated partners indeed exhibit the property of “coordinating with a BR while exposing the ego’s weaknesses without engaging in self-sabotage”? Does Eq. 10 admit any theoretically provable guarantee that suppresses self-sabotage?\n\nReference \n\n[1] Li, Yang, et al. \"Cooperative open-ended learning framework for zero-shot coordination.\" International Conference on Machine Learning. PMLR, 2023.\n\n[2] Yan, Xue, et al. \"An efficient end-to-end training approach for zero-shot human-AI coordination.\" Advances in neural information processing systems 36 (2023): 2636-2658.\n\n[3] Charakorn, Rujikorn, Poramate Manoonpong, and Nat Dilokthanakul. \"Diversity is not all you need: Training a robust cooperative agent needs specialist partners.\" Advances in Neural Information Processing Systems 37 (2024): 56401-56423.\n\n[4]Sarkar, Bidipta, Andy Shih, and Dorsa Sadigh. \"Diverse conventions for human-AI collaboration.\" Advances in neural information processing systems 36 (2023): 23115-23139."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xq29uB8m3F", "forum": "ZSDmfoVvCs", "replyto": "ZSDmfoVvCs", "signatures": ["ICLR.cc/2026/Conference/Submission12903/Reviewer_EESm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12903/Reviewer_EESm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917185694, "cdate": 1761917185694, "tmdate": 1762923681667, "mdate": 1762923681667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ROTATE, a regret-driven open-ended training framework for ad hoc teamwork that reframes zero-shot coordination as minimizing worst-case cooperative regret, i.e., $\\min_{\\pi^{ego}}\\max_{\\pi^{-i}}\\mathbb{E}[\\mathrm{CR}(\\pi^{ego},\\pi^{-i})]$. It introduces a per-state regret objective coupled with an SXP term that maximizes payoff with a best-response partner to discourage sabotage, and alternates teammate generation with ego learning using a population buffer. On Overcooked and Level-Based Foraging, ROTATE outperforms UED and teammate-diversification baselines with unseen partners, and ablations attribute gains to the per-state objective and the buffer."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Comprehensive treatment of ZSC/ad hoc teamwork: the paper unifies teammate generation and ego learning via a cooperative-regret min–max objective $ \\min_{\\pi^{ego}}\\max_{\\pi^{-i}}\\mathbb{E}[\\mathrm{CR}]$, making assumptions and evaluation protocol explicit."}, "weaknesses": {"value": "- Clarity and exposition: the paper is difficult to follow; the core algorithmic loop (who updates when, how SP/XP/SXP are sampled/weighted, and how the BR is trained/used) is buried under notation, so the end-to-end procedure remains unclear even after multiple readings.\n- Mischaracterization of the gap: (a) the claim that most ZSC/AHT methods are two-stage is outdated—recent open-ended or end-to-end approaches already move beyond fixed teammate sets (e.g., COLE [1], E3T [2], TrajeDi [3]); (b) the comparison to current work is incomplete, especially where prior methods already leverage SP/XP and mixed-play/SXP-style rollouts (e.g., CoMeDi [4]), making the incremental novelty of the proposed per-state regret $J_{\\text{state}}$ hard to isolate.\n- Anti-sabotage rationale under-specified: the paper asserts that coupling per-state regret with an SXP best-response term mitigates sabotage, but offers little intuitive or theoretical support (e.g., no conditions under which maximizing SXP payoff with BR implies low sabotage against arbitrary partners, no analysis of bias induced by approximate BR or sampling); more formal justification or counterexample analysis is needed.\n\nReferences\n\n[1] Li, Y., Zhang, S., Sun, J., Du, Y., Wen, Y., Wang, X., and Pan, W. 2023. Cooperative Open-ended Learning Framework for Zero-Shot Coordination. In Proceedings of the 40th International Conference on Machine Learning (ICML 2023). Proceedings of Machine Learning Research, 202:20470–20484.\n\n[2] Yan, X., Guo, J., Lou, X., Wang, J., Zhang, H., and Du, Y. 2023. An Efficient End-to-End Training Approach for Zero-Shot Human-AI Coordination. In Proceedings of the Thirty-Seventh Conference on Neural Information Processing Systems (NeurIPS 2023).\n\n[3] Lupu, A., Cui, B., Hu, H., and Foerster, J. 2021. Trajectory Diversity for Zero-Shot Coordination. In Proceedings of the 38th International Conference on Machine Learning (ICML 2021). Proceedings of Machine Learning Research, 139:7204–7213.\n\n[4] Sarkar, B., Shih, A., and Sadigh, D. 2023. Diverse Conventions for Human-AI Collaboration. In Proceedings of the Thirty-Seventh Conference on Neural Information Processing Systems (NeurIPS 2023)."}, "questions": {"value": "- Does the combination of per-state regret on SP/XP and the SXP best-response payoff formally or intuitively guarantee reduced sabotage (i.e., lower probability of destructive actions), and under what assumptions on BR optimality and sampling?\n- Can an agent maximize $J_{\\text{state}}$ on SP/XP while keeping high SXP payoff yet still sabotage arbitrary non-BR partners (e.g., collusion with BR)? Is there any bound linking SXP payoff to sabotage rate against unseen partners?\n- How sensitive is the anti-sabotage effect to the weighting between SP/XP and SXP and to environments without reliable state resets/cut-ins? Please provide analysis or ablations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J4lHtJj1Gv", "forum": "ZSDmfoVvCs", "replyto": "ZSDmfoVvCs", "signatures": ["ICLR.cc/2026/Conference/Submission12903/Reviewer_NkUx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12903/Reviewer_NkUx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762085560913, "cdate": 1762085560913, "tmdate": 1762923681205, "mdate": 1762923681205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This tackles the problem of Ad-Hoc Teamwork through iterative training diverse teams.\n\nThe diffuclty in AHT is that your AHT agent might be robust with respect to some, apparently diverse population of teammates, but not others. The paper claims that this is due to a small traning set. Their idea is to use iterative training in order to minimise co-operative regret.\n\nThe iterative idea is obviously not new and the related work contains some algorithms which maintain a diverse set of opponents. ROTATE uses a minimax approach, similarly to e.g. Villein et al, but finds a worst-case policy,rather than a distribution, at each step. This makes me think that the authors have not looked at the related work in sufficient detail.\n\nI would have liked the algorithm to be more precisely defined in the main paper: Eq. 7 says that they use a minimax approach, but $\\Pi^{-i}$ is not defined until Section 6, and only really discussed in detail in the appendix. Since many other works use iterative training, I suppose that the real open-endedness is the generation of new partners, rather than the iterative nature of the training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Interesting notion of regret\n+ Good comparison with related work."}, "weaknesses": {"value": "- The authors could have done a better job of identifying which component is more important: the notion of regret, the way the teammates are generated, etc.\n- Unclear novelty.\n- Lack of clarity and theoretical discussion."}, "questions": {"value": "Can you explain exactly how you used the baselines? From my reading of the appendix, it seems that you only took some aspect of these approaches, and adapted them to your framework, rather than have done a direct comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rOxqpb3uu2", "forum": "ZSDmfoVvCs", "replyto": "ZSDmfoVvCs", "signatures": ["ICLR.cc/2026/Conference/Submission12903/Reviewer_xw5y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12903/Reviewer_xw5y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149466280, "cdate": 1762149466280, "tmdate": 1762923680854, "mdate": 1762923680854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}