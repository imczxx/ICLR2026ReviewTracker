{"id": "5E5sd3TWGD", "number": 1555, "cdate": 1756891396949, "mdate": 1759898202296, "content": {"title": "RD-HRL: Generating Reliable Sub-Goals for Long-Horizon Sparse-Reward Tasks", "abstract": "Long-horizon sparse-reward tasks, such as goal-conditioned or robot manipulation tasks, remain challenging in offline reinforcement learning due to the credit assignment problem. Hierarchical methods have been proposed to tackle this problem by introducing sub-goal planning guided by value functions, which in principle can shorten the effective planning horizon for both high-level and low-level planners, and thereby avoiding  the credit assignment problem. However, we demonstrate that the sub-goal selection mechanism is unreliable, as it relies on value functions suffering from generalization noise, which misguides value estimation and thus leads to sub-optimal sub-goals. In this work, to provide more reliable sub-goals, we novelly introduce a reliability-driven decision mechanism, and propose Reliability-Driven HRL (RD-HRL) as the solution. The reliability-driven decision mechanism provide decision-level targets from transition regions, thereby providing noise-immune decision spaces for high-level policy, ensuring the reliability of sub-goals (which are termed as action-level targets in this paper). Comprehensive experimental results demonstrate that our approach RD-HRL outperforms baseline methods across multiple benchmarks, highlighting the competitive advantages of RD-HRL. Our code is anonymously available at \\url{https://anonymous.4open.science/r/RD-HRL-243D }.", "tldr": "We propose RD-HRL, which introduces a reliability-driven decision mechanism to address the issue of sub-optimal sub-goals caused by generalization errors in existing HRL methods.", "keywords": ["Hierarchical Reinforcement Learning", "Sub-goal", "Key States", "Choice Learning", "Reinforcement Learning", "Goal-conditioned Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c6515166da141411d1ca2b344c46ff18d35ab2d9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Reliability-Driven HRL (RD-HRL) to address generalization noise in value functions when generating subgoals for hierarchical RL tasks. The method has three components: Transition Region Extraction (TRE), which identifies transition regions from the offline dataset; a Target Identification (TI) module, which selects targets from these regions; and a Target Evaluation (TE) module, which evaluates and refines the selected targets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1- Well-written paper and easy to read.\n2- Comparison with recent baselines.\n3- Experiments on both navigation and manipulation tasks."}, "weaknesses": {"value": "Please refer to the questions."}, "questions": {"value": "1- This approach proposes defining subgoals for offline RL using a dataset of training trajectories. Do you think it would also work for online RL, where the agent must collect its own experience? Would the early subgoals be of sufficient quality?\n\n2- By the definition of Transition Region Extraction (TRE), do all transition regions consist of reliable and optimal subgoals, as illustrated in Figure 2(a)?\n\n3- In Eq. 8, how do you account for the current state? In addition to the quality of a subgoal with respect to the final goal, we should also consider which subgoal is closer to—and more easily achievable from—the current state.\n\n4- Are all modules trained jointly or sequentially? If sequentially, in what order?\n\n5- Can you provide examples of transition regions in manipulation tasks as well?\n\n6- Could you elaborate on the statement: \"states $s_{t1}$ and $s_{t2}$ may originate from different trajectories, which enables direct cross-trajectory propagation of value signals rather than generalization, thereby overcoming generalization error”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ioMQJtOLFd", "forum": "5E5sd3TWGD", "replyto": "5E5sd3TWGD", "signatures": ["ICLR.cc/2026/Conference/Submission1555/Reviewer_XFsY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1555/Reviewer_XFsY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761455416978, "cdate": 1761455416978, "tmdate": 1762915810244, "mdate": 1762915810244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of unreliable sub-goal generation in Hierarchical RL(HRL) framework in long-horizon tasks. To address this issue, the author provides a reliability-driven HRL mechanism. This mechanism consists of three modules: Target Region Extraction (TRE) -- extracting target region from offline dataset, Target Identification (TI) -- selecting decision-level target guided by TE module, Target Evaluation (TE) -- evaluating target region candidates to help with TI module. This model, RD-HRL, achieved SOTA performance in several long-horizon RL tasks, including realistic datasets such as robotics tasks. Also, they provide an in-depth investigation of each component for their proposed mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novelty and Generalizable Approach.** The issue of value functions being biased toward offline trajectories is a well-known problem in offline RL. This paper offers a general solution by identifying transition regions and sampling sub-goals within local regions that exclude transition areas. The authors also introduce a novel metric for extracting transition regions, which they identify as the primary source of the problem.\n2. **Performance.** The proposed model achieves state-of-the-art performance on challenging long-horizon RL tasks. This is a non-trivial contribution to the field.\n3. **In-depth Analysis.** The paper provides a thorough ablation study examining each component of the framework, so that the following researchers can investigate more easily."}, "weaknesses": {"value": "1. **Unclear Definition of Unreliable Sub-goals.** The paper does not clearly explain what makes sub-goals \"unreliable.\" Adding intuitive examples would help. For instance, transition regions naturally contain diverse trajectories, which could affect value propagation and generalization. (If I understood correctly)\n2. **Limited Scope of Evaluation.** The approach is only tested with HIQL as the base model. While the hierarchical policy structure seems generally applicable, testing it on other hierarchical RL algorithms would strengthen the contribution. The choice of only IQL-based baselines seems limited, given the claimed generality of the approach."}, "questions": {"value": "1. Is there any analysis or reference that explains the unreliable sub-goals problem? The paper cites ContextFormer, but the explanation of this issue is unclear there. If I'm wrong, please point me and reflect their exact analysis result and implications in the paper. If there are not enough analyses for this, it would be best to include an analysis in this paper.\n2. I believe the high-level intuition of this paper's suggestion as a solution makes sense and is generalizable over HRL. I wonder if this can be extended to other base models, not just on HIQL. e.g., HAC, HGCBC, etc., if the impl or idea is too specific to HIQL. I might consider this a narrower contribution than I expected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W8WtvzDPjh", "forum": "5E5sd3TWGD", "replyto": "5E5sd3TWGD", "signatures": ["ICLR.cc/2026/Conference/Submission1555/Reviewer_5uMD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1555/Reviewer_5uMD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733070194, "cdate": 1761733070194, "tmdate": 1762915810041, "mdate": 1762915810041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper tackles the credit assignment problem in long-horizon, sparse-reward offline reinforcement learning (RL) tasks.\n- The authors argue that existing HRL methods are unreliable because the value functions used to select sub-goals suffer from generalization noise.\n- The paper proposes Reliability-Driven HRL (RD-HRL).\n- This method introduces a reliability-driven decision mechanism composed of three new components\n  1. Transition Region Extraction (TRE): K-Means clustering and a \"Future Diversity Index (FDI)\" to identify \"transition regions\"\n  2. Target Evaluation (TE): A value function trained only on states within these transition regions\n  3. Target Identification (TI): A policy that selects a long-term \"decision-level target\"\n- The authors claim this framework provides \"noise-immune\" guidance, leading to SOTA performance on benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Despite some flawed analysis (see \"Weaknesses\"), the paper presents good results on some tasks. For example, Antmaze-Ultra benchmarks appear to be statistically significant and state-of-the-art\n- The paper introduces a new 3-level hierarchy (TI $\\rightarrow \\pi^h \\rightarrow \\pi^l$) to tackle the problem of sub-goal generation HRL\n- The paper provides an extensive set of ablations (e.g., RD-HRL-TRE, RD-HRL-HP, RD-HRL-TE) that attempt to validate the necessity of each new component"}, "weaknesses": {"value": "- The analysis is flawed and contains multiple errors that inflate performance\n  - The claim of achieving \"the best performance across all [manipulation] benchmarks except for the kitchen-mixed task\" is false. Table 1 clearly shows PlanDQ (75.0) and DTAMP (74.4) are superior on kitchen-partial and kitchen-mixed, respectively\n  - The claim of a \"57%\" outperformance over HIQL on CALVIN  is an incorrect calculation and a cherry-picked comparison. The 25-point delta (68.8 vs 43.8) is not 57%, and the more relevant comparison is to the second-place DiffuserLite (52.1)\n- The paper's method for claiming statistical wins is non-standard and not well-justified\n  - The 3% rule is justified by citing papers that use different rules (e.g., 5% rule in Chen et al and Li et al or CIs in Badrinath et al (see Table 1 row 1 -- halfcheetah))\n- The \"Future Diversity Index\" (FDI) metric is not clearly justified\n- There is a typo on line 144: \"To improve the the action-level...\"\n- In Table 3, the CIs for RD-HRL and HIQL on CALVIN clearly overlap"}, "questions": {"value": "- The central claim about solving \"generalization noise\" is confusing. Can the authors provide a precise explanation for how the TE module's \"cross-trajectory propagation\" is fundamentally different from the \"generalized Bellman backup\" it claims is the problem? Why is one reliable and the other not, when both operate on the same transition regions?\n- The paper claims SOTA on manipulation tasks (except one), but Table 1 shows this is false. Please correct this claim or explain the discrepancy\n- Can the authors provide a stronger theoretical or empirical justification for the FDI metric? As written, it seems arbitrary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eha73TYK2p", "forum": "5E5sd3TWGD", "replyto": "5E5sd3TWGD", "signatures": ["ICLR.cc/2026/Conference/Submission1555/Reviewer_nnn4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1555/Reviewer_nnn4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971660161, "cdate": 1761971660161, "tmdate": 1762915809926, "mdate": 1762915809926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RD-HRL, a hierarchical offline reinforcement learning method that aims to make sub-goal selection more reliable in long-horizon sparse-reward problems. It first clusters states to extract “transition regions” with a Future Diversity Index, then trains a Target Evaluation (TE) module that estimates values only over these regions and a Target Identification (TI) module that selects a decision-level target (a transition region) given the current state and goal. At evaluation time, TI provides a decision-level target to the high-level policy, which outputs an action-level sub-goal for the low-level policy to execute. Experiments on antmaze, kitchen, and CALVIN benchmarks report strong average performance and ablations analyze the contribution of each component."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem motivation: The paper articulates how value-function generalization noise can mislead sub-goal selection in hierarchical methods and uses a didactic example to illustrate the issue.\n- Architectural novelty: Introducing a layer that constrains high-level decisions to transition regions/bottleneck states and learning a temporally abstracted value function over these regions is an interesting design choice. The split between decision-level targets (TI output) and action-level targets (from the high-level policy) is conceptually clean.\n- Broad set of experiments: The method is evaluated on standard and challenging long-horizon benchmarks (antmaze including ultra variants, kitchen, CALVIN), and reports strong results relative to recent HRL baselines such as HIQL (Park et al., 2024a), PlanDQ (Chen et al., 2024a), DTAMP (Hong et al., 2023), and DiffuserLite (Dong et al., 2024), with helpful ablations (RD-HRL-TE, -TRE, -HP, -CU) and a horizon-size comparison.\n- Useful ablations and analysis: The ablations disentangle the effects of transition regions, temporal abstraction, and cross-trajectory updates; the horizon-size study clarifies that performance gains are not merely from increasing $H$ in a standard HRL pipeline."}, "weaknesses": {"value": "Notation and parameter-role inconsistencies:\n  - Section 2.2 describes $\\beta$ as the discount factor even though $\\gamma$ is the discount factor throughout the paper. In AWR-style objectives $\\beta$ is typically a temperature; this should be corrected to avoid confusion with $\\gamma$.\n  - The TI objective (Eq. 10) uses $\\exp(\\beta^{d(s\\_t, s\\_z)} \\cdot A\\_{TI})$ together with text stating “raised to the power of $d(s\\_t, s\\_z)$.” This deviates from the standard AWR form $\\exp(\\beta \\cdot A)$ and from the high-/low-level objectives in Eq. 3 and 4. The paper should justify or correct this exponentiation vs. linear scaling.\n- Unclear writing. From a first read, it was very unclear whether $g\\_{TI}$ is a region identifier, a representative state from a region, or a learned embedding for the region. The paper would definitely benefit from a few additional iterations of editing/re-writing.\n- Cross-trajectory updates in Eq. (8) require a notion of temporal distance $d(s\\_{t1}, s\\_{t2})$ across trajectories. The approximation used can introduce bias or high variance when trajectories differ in speed/length. Some empirical sensitivity analysis or an uncertainty-aware alternative would strengthen the claim.\n- The reliance on K-Means clustering and a fixed FDI threshold may limit generality in high-dimensional or non-Euclidean state spaces. The procgen-500 pilot supports this concern: performance is poor when clustering raw pixels and improves only with external embeddings. This suggests that end-to-end learned representations or graph-based region discovery could be necessary for broader applicability."}, "questions": {"value": "1. Could the authors clarify the intended role of $\\beta$ in Eq. (10)? Is it a temperature parameter (as in AWR) or related to the discount factor $\\gamma$? If the exponentiation form $\\beta^d(s_t, s_z)$ is intentional, what is the theoretical or empirical justification for raising $\\beta$ to the power of distance?\n2. What exactly does $g_{TI}$ represent at evaluation time - a region index, a representative state, or a learned embedding of the region? How is $\\pi_h$ conditioned on this variable in practice?\n3. There are mismatches between text, pseudocode, and tables (e.g., Algorithm 1 labels, Table 1 vs. Table 3). Could the authors reconcile these discrepancies and clarify whether they affect experimental results?\n4. Since the method relies on K-Means and Euclidean distances, how well does it scale to high-dimensional or non-Euclidean spaces? Have the authors tried graph-based or density-based alternatives, and if so, how do results compare?\n5. In Eq. (8), temporal distance $d(s_{t1}, s_{t2})$ is approximated across trajectories. How sensitive are TE’s estimates to this approximation, and does it introduce bias or variance in value learning?\n6. The paper includes several ablations, but could the authors include one specifically isolating the impact of the FDI thresholding and clustering scheme to show its independent contribution to performance?\n7. How frequently does TI select each transition region during evaluation, and does this selection correlate with TE’s estimated reliability or downstream task success?\n8. Why do the authors claim the robotic tasks in CALVIN benchmark are non-euclidean? Could they clarify what makes these non-euclidean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TsOdoxmB4o", "forum": "5E5sd3TWGD", "replyto": "5E5sd3TWGD", "signatures": ["ICLR.cc/2026/Conference/Submission1555/Reviewer_DegB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1555/Reviewer_DegB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049113515, "cdate": 1762049113515, "tmdate": 1762915809782, "mdate": 1762915809782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}