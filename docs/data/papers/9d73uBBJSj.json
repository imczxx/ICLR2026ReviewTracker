{"id": "9d73uBBJSj", "number": 20727, "cdate": 1758309399222, "mdate": 1759896961494, "content": {"title": "EWC-Guided Diffusion Replay for Exemplar-Free Continual Learning", "abstract": "Continual learning for medical imaging must adapt to new tasks while preserving prior competence and avoiding retention of patient examples. We present EWC-guided Diffusion Replay, a hybrid framework that combines a single class conditional diffusion model for exemplar free replay with Elastic Weight Consolidation for parameter anchoring. To target replay where it is most needed, we introduce Fisher Scheduled Replay, which allocates synthetic samples using a mixture of Fisher saliency and recent loss drift at the class level. We further provide a concise decomposition of forgetting that links retention to divergence between real and replayed data and to Fisher weighted parameter drift, clarifying how replay fidelity and synaptic stability interact. In class incremental settings without task identities and without exemplars, the method attains competitive accuracy and lower forgetting on MedMNIST v2 in two and three dimensions and on CheXpert, outperforming strong regularisation and replay baselines under a matched memory budget. The unified conditional generator is used only during training, which reduces reliance on stored data while remaining architecture agnostic.", "tldr": "We propose EWC-guided diffusion replay, combining high-fidelity exemplar-free diffusion replay with Fisher-scheduled allocation and a theoretical forgetting bound for continual learning in medical imaging.", "keywords": ["Continual Learning", "Diffusion Models", "Elastic Weight Consolidation", "Fisher-Scheduled Replay", "Generative Replay", "Exemplar-Free Learning", "Medical Imaging", "Catastrophic Forgetting", "Theoretical Forgetting Bound"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a830692be8e69067b6a419177f589677fbfb2c6.pdf", "supplementary_material": "/attachment/fa120a51869cf058b5648459208535ddbc0c62b1.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a method for exemplar-free class-incremental learning in medical imaging that couples a single class-conditional diffusion model for generative replay with EWC for parameter anchoring, and adds Fisher-Scheduled Replay (FSR) to allocate synthetic samples to fragile classes. The paper motivates the design with a replay-vs-drift forgetting decomposition and reports gains over regularization and replay baselines on MedMNIST-2D/3D and CheXpert, using a shared ViT backbone, 100 MB budget, and 5-seed CIs. The paper falls short of top-tier exemplar-free CL standards in benchmarking scope, evaluation metrics, memory fairness, theoretical strength, and fidelity measurement. In its current form I recommend strong reject."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "+ Well-done ablation study that include “EWC-only”, “DDPM-only”, “w/o FSR”, and DDGR; five-seed paired t-tests are reported.\n+ A single shared conditional generator reduces parameters vs per-task generators; basic efficiency numbers (VRAM/params/train time/sampling) are tabulated."}, "weaknesses": {"value": "+ This paper does not meet top EFCIL benchmarking standards. Evidence is limited to MedMNIST-2D/3D and a custom 3-task CheXpert. There is no evaluation on standard CL suites (e.g., CIFAR-100, CORe50, DomainNet, ImageNet variants), no task-order robustness study, and no cross-site transfer—well below the breadth expected in exemplar-free continual learning papers at top venues.\n+ All methods are said to obey a 100 MB cap, but generative replay “stores only the generator checkpoint and samples on-the-fly,” while exemplar replay “stores raw images,” and replay storage is “counted as if synthetic samples were materialized.” This hybrid policy mixes storage and hypothetical sample bytes and risks under-counting the (large) generator; it is not aligned with common, strictly byte-accounted protocols.\n+ The “forgetting bound” relies on smoothness, Pinsker-style arguments, and diagonal-Fisher local quadraticity; it functions as an explanatory lens but lacks tight constants, verification of assumptions, or actionable guarantees. The paper’s own empirical “validation” is correlational.\n+ Replay fidelity measurement is unconvincing. Divergence is approximated via latent-space KDE/KL rather than more established generative metrics (FID/KID, precision–recall); conclusions about diffusion’s “lower drift” thus rest on a representation-dependent proxy.\n+ I find baseline coverage and experimental fairness highly insufficient. While DDGR, DER++, SPM, CoPE, EFT, and a dynamic expansion baseline (PMoE) are included, there is no broader comparison to recent exemplar-free adapters/regularizers under matched memory and compute, nor to strong non-diffusion generative alternatives with identical schedules. The ablations do not isolate FSR against simpler schedulers under budget-matched sampling. \n+ Metric breadth is below field norms. Top EFCIL papers report ACC, BWT/FWT, in-stream accuracy curves, and often calibration; here the focus is on final accuracy/AUROC and a scalar “forgetting,” with t-tests. Missing standard diagnostics hinder comparability\n+ I find a big mismatch between the problem framing and the broad claims. The medical-only scope (small MedMNIST; bespoke CheXpert split) and the absence of multi-site evaluation make external validity unclear. Conclusions about “privacy-sensitive settings” are not backed by deployment-style studies."}, "questions": {"value": "+ Can you add standard CL suites (CIFAR-100 class-inc. at minimum) and BWT/FWT + in-stream metrics to meet established EFCIL reporting norms?\n+ Please replace or complement latent-KDE KL with FID/KID and/or precision–recall for generative models, and relate fidelity metrics to forgetting reductions.\n+ Can you include budget- and schedule-matched ablations that isolate FSR from simpler allocators (uniform, class-prior, loss-only, Fisher-only) and from diffusion-only replay under identical sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KciA6ziFe1", "forum": "9d73uBBJSj", "replyto": "9d73uBBJSj", "signatures": ["ICLR.cc/2026/Conference/Submission20727/Reviewer_D8nW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20727/Reviewer_D8nW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774118627, "cdate": 1761774118627, "tmdate": 1762934142151, "mdate": 1762934142151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EWC-guided Diffusion Replay, a hybrid continual learning method for medical imaging that operates without storing patient data. It combines a class-conditional diffusion model for generating synthetic data (replay) with Elastic Weight Consolidation for parameter stability. A key innovation is Fisher Scheduled Replay, which strategically allocates replays based on parameter importance and recent loss."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes EWC-guided Diffusion Replay. It combines a class-conditional diffusion model for generating synthetic data (replay) with Elastic Weight Consolidation for parameter stability. A key innovation is Fisher Scheduled Replay, which strategically allocates replays based on parameter importance and recent loss. The method achieves competitive accuracy and reduced forgetting on medical image datasets, outperforming baselines."}, "weaknesses": {"value": "The innovation of this paper is limited. While the work combines EWC with replay, its main contribution appears to be the proposed Fisher Scheduled Replay. However, the authors fail to provide an in-depth analysis or explanation of its underlying principles, and even the specific mechanism for allocating replay samples remains unclear.\n\nFurthermore, the paper is poorly written, making it difficult to grasp the motivation and follow the logic. Significant redundancy exists between Sections 2 and 3, which could be consolidated. There is no dedicated related works section in the main text, it is only provided in the supplementary materials. Additionally, many equations in the paper are unnumbered.\n\nThe method uses a diffusion model to generate replay data, which in theory also requires continual learning. This critical point is not addressed in the main text and is only briefly mentioned in the supplementary materials. The effectiveness of the diffusion model's own continual learning directly impacts the overall results, and the authors should include relevant analysis in the experiments section, along with an evaluation of the method's computational efficiency.\n\nThere is also an error in the results labeling in Table 1, where the second-best performance is incorrectly marked."}, "questions": {"value": "What is the specific underlying principle of Fisher Scheduled Replay, and how does it operate concretely in the replay process?\nHow does the \"Theoretical Analysis of Forgetting\" provided in the paper motivate Fisher Scheduled Replay? What is the motivation behind this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BCwxl72MPt", "forum": "9d73uBBJSj", "replyto": "9d73uBBJSj", "signatures": ["ICLR.cc/2026/Conference/Submission20727/Reviewer_vKPg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20727/Reviewer_vKPg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832774954, "cdate": 1761832774954, "tmdate": 1762934141523, "mdate": 1762934141523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose EWC–Guided Diffusion Replay (EWC–DR) for exemplar-free, task-ID-free continual learning in medical imaging. The introduced method integrates: \n\n(i) A single class-conditional diffusion generator amortized across tasks for high-fidelity replay, \n\n(ii) Elastic Weight Consolidation (EWC) to anchor Fisher-salient parameters, and \n\n(iii) Fisher-Scheduled Replay (FSR) that allocates synthetic samples to fragile classes via a convex combination of Fisher saliency and recent loss drift. \n\nThe authors present a forgetting decomposition that upper-bounds forgetting by a sum of replay divergence (KL between real and replayed data) and Fisher-weighted parameter drift, motivating the hybrid design. On MedMNIST v2 (2D/3D) and CheXpert under a strict 100 MB memory cap, EWC–DR outperforms strong regularization and replay baselines (DER++, SPM, VAE+Replay) and a diffusion-only replay baseline (DDGR) on accuracy/AUROC and average forgetting, with ablations validating each component and statistical tests (paired t-tests) indicating significance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Principled objective decomposition (design). The forgetting bound clearly motivates diffusion replay (reduce KL) + EWC (constrain Fisher drift) + FSR (budget where it matters). The paper validates how both terms correlate with forgetting and shows additive gains. \n\n- Exemplar-free, task-ID-free CL with a single generator. Amortized class-conditional DDPM used only during training keeps inference light and privacy-friendly. It matched 100 MB budget ensures fair comparisons. \n\n- Consistent cross-benchmark improvements. EWC–DR improves Acc/AUROC and reduces forgetting vs. regularization, exemplar replay, and DDGR under identical settings. The paired t-tests indicate significance. \n\n- Solid engineering & reporting. Clear protocols, seeds, task-order robustness, ablations (w/o EWC, w/o FSR, diffusion-only), and efficiency analysis (VRAM/params/time/sampling)."}, "weaknesses": {"value": "- KL estimation and realism auditing. The KL term is approximated (latent KDE) and may correlate with aesthetics rather than diagnostic fidelity. A more per-class calibration/FID-like or feature-space distances could strengthen the link between replay quality and forgetting. \n\n- Limited baseline diversity. Baselines omit some strong class-incremental methods (e.g., ER-ACE/DER w/ balanced rehearsal, GCR, or prompt/adapters) and non-diffusion generative baselines beyond VAE. Including at least one recent coreset/adapter method would contextualize gains. \n\n- FSR specifics and stability. The scheduler mixes Fisher and loss-drift with a fixed γ. Shown sensitivity/robustness across streams, imbalance, and label-noise regimes is not deeply explored (CheXpert noise is mentioned, but targeted analyses would help). \n\n- Privacy & compliance discussion is brief. While exemplar-free helps, generative replay may still reveal training patterns. A short discussion of membership-inference/attribute-inference risks or DP variants would anticipate deployment concerns in clinical settings."}, "questions": {"value": "- FSR sensitivity: How sensitive are results to γ and the EMA parameters for loss-drift? Could the authors report a small grid (e.g.,  γ ∈ {0.25,0.5,0.75}, two EMA half-lives) and per-class replay counts vs. retention? \n\n- Replay quality probes: Beyond downstream metrics, do the authors observe improved feature-space fidelity (e.g., Inception/MedNet features) or per-class calibration when using diffusion replay? Is there any failure cases where replay harms calibration? \n\n- Adapters/prompts baseline: Could the authors add a lightweight adapter/prompt CL baseline (no exemplars) to clarify whether EWC–DR’s benefits persist when plasticity is routed through small modules? \n\n- 3D scaling: For 3D DDPM, what is the sampling throughput vs. 2D? How does this affect budgeted replay ratios? Any instability or mode-dropping on NoduleMNIST3D? \n\n- Bound calibration: Can the authors provide the joint regression coefficients a,b and R^2 per benchmark for the forgetting model \nF_j = a * \\hat(KL) + b * D_j + ϵ? This would make the decomposition’s predictive value more concrete."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A (public, de-identified datasets; exemplar-free training). I would recommend to consider a brief discussion about privacy auditing for generated replay and licensing/compliance of CheXpert/MedMNIST derivatives."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GPj7X2Nk5l", "forum": "9d73uBBJSj", "replyto": "9d73uBBJSj", "signatures": ["ICLR.cc/2026/Conference/Submission20727/Reviewer_asid"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20727/Reviewer_asid"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909663802, "cdate": 1761909663802, "tmdate": 1762934141123, "mdate": 1762934141123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes EWC DR, a continual learning method that combines a single class conditional diffusion generator for replay with Elastic Weight Consolidation on the classifier. It also introduces Fisher Scheduled Replay, which allocates synthetic samples to classes using a mixture of per class Fisher saliency and recent validation loss drift. The analysis gives a forgetting decomposition that upper bounds average forgetting by a sum of a replay divergence term and a Fisher weighted parameter drift term, which matches the design of the method. Experiments on MedMNIST 2D, MedMNIST 3D, and CheXpert report higher final performance and lower forgetting than regularization and replay baselines under a fixed memory budget."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The paper presents an interesting idea of generating targeted data based on the Fisher importance metric."}, "weaknesses": {"value": "**Major Concerns:**\n\n* The generator is pretrained on all future classes, which limits use when new classes appear. It cannot adapt without retraining the generator.  \n* Ablations are very limited. Each part of the method, such as diffusion replay, Fisher-based weighting, and the scheduling mechanism, could be replaced by different alternatives. However, the paper does not explore or compare these options. Section 6.5 only includes a written description of what happens when a component is removed, without showing quantitative results or a summary table. A more complete ablation study is needed to show the contribution and importance of each component.  \n* The method is general continual learning, yet all experiments are only on medical datasets.  \n* The consolidation relies on an older regularization idea while many newer methods exist, and the paper does not justify this choice.  \n* Benchmarks are small and MNIST like, which weakens the claims.  \n* The memory comparison appears unfair. Buffer-based and buffer-free methods are mixed, the 100 MB rule is vague, and what matters is the number of stored samples and buffer capacity. Experimental details are not sufficient to ensure fair comparisons.\n\n\n**Minor Concerns:**\n\n* The paper is unorganized. Method details and mathematical definitions appear after the experimental section, which makes it hard to follow.  \n* There is no related work section in the main paper, and the coverage of prior literature is too limited."}, "questions": {"value": "* I would appreciate if the authors could clarify why the EWC regularization method was specifically chosen for consolidation instead of exploring more recent or advanced regularization-based continual learning techniques.  \n* It would be helpful if the authors could provide clearer experimental details on how the 100 MB budget is applied and explain more precisely how the evaluation protocol ensures fairness between replay-based and non-replay-based continual learning methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "awxYmhUX4z", "forum": "9d73uBBJSj", "replyto": "9d73uBBJSj", "signatures": ["ICLR.cc/2026/Conference/Submission20727/Reviewer_DBRb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20727/Reviewer_DBRb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762191899432, "cdate": 1762191899432, "tmdate": 1762934140624, "mdate": 1762934140624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}