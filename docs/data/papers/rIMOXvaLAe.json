{"id": "rIMOXvaLAe", "number": 4271, "cdate": 1757651791229, "mdate": 1759898042428, "content": {"title": "OVSeg3R: Learn Open-vocabulary Instance Segmentation from 2D via 3D Reconstruction", "abstract": "In this paper, we propose a training scheme called OVSeg3R to learn open-vocabulary 3D instance segmentation from well-studied 2D perception models with the aid of 3D reconstruction. OVSeg3R directly adopts reconstructed scenes from 2D videos as input, avoiding costly manual adjustment while aligning input with real-world applications. By exploiting the 2D to 3D correspondences provided by 3D reconstruction models, OVSeg3R projects each view's 2D instance mask predictions, obtained from an open-vocabulary 2D model, onto 3D to generate annotations for the view's corresponding sub-scene. To avoid incorrectly introduced false positives as supervision due to partial annotations from 2D to 3D, we propose a View-wise Instance Partition algorithm, which partitions predictions to their respective views for supervision, stabilizing the training process. Furthermore, since 3D reconstruction models tend to over-smooth geometric details, clustering reconstructed points into representative super-points based solely on geometry, as commonly done in mainstream 3D segmentation methods, may overlook geometrically non-salient objects. We therefore introduce 2D Instance Boundary-aware Superpoint, which leverages 2D masks to constrain the superpoint clustering, preventing superpoints from violating instance boundaries. With these designs, OVSeg3R not only extends a state-of-the-art closed-vocabulary 3D instance segmentation model to open-vocabulary, but also substantially narrows the performance gap between tail and head classes, ultimately leading to an overall improvement of +2.3 mAP on the ScanNet200 benchmark. Furthermore, under the standard open-vocabulary setting, OVSeg3R surpasses previous methods by about +7.1 mAP on the novel classes, further validating its effectiveness.", "tldr": "", "keywords": ["3D Perception", "Open-vocabulary 3D Instance Segmentation", "Open-vocabulary 2D Instance Segmentation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d736a5cfa412475537ea2f28d43f87b2a017446f.pdf", "supplementary_material": "/attachment/be96e0611c7058c933ccedb7bd100d1687388e5a.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents OVSeg3R, a training scheme for open-vocabulary 3D instance segmentation that avoids manual 3D instance annotations. Instead, it uses two readily available sources: (1) video-based 3D reconstruction to obtain a point cloud and 2D–3D correspondences, and (2) strong open-vocabulary 2D instance segmentation to get per-frame instance masks and category names. These 2D masks are lifted into 3D and used as supervision.\n\nTo make this supervision stable, the authors introduce (i) View-wise Instance Partition (VIP), which matches each predicted 3D instance only to the visible portion in each view to avoid penalizing regions that were never observed, and (ii) 2D Instance Boundary-aware Superpoints (IBSp), which uses 2D instance boundaries to prevent small or low-relief objects from being merged into large planar superpoints.\n\nThey apply OVSeg3R to a strong recent 3D instance segmentation backbone (SegDINO3D), modifying only the classification head to score similarity with text embeddings for open-vocabulary recognition. Trained under OVSeg3R, this model outperforms both prior open-vocabulary 3D methods and a traditionally trained baseline, with especially large gains on long-tail and novel categories."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation. The paper tackles the real bottleneck for open-vocabulary 3D instance segmentation: 3D instance masks with textual labels are expensive to annotate, but videos + strong 2D open-vocabulary segmentors + modern 3D reconstruction are becoming cheap and ubiquitous. OVSeg3R leverages exactly those ingredients.\n- Significant gains on long-tail and novel categories. The method not only improves overall mAP but also drastically narrows the head–tail gap and boosts novel categories in the standard open-vocabulary setting. This indicates real progress toward robust, open-world 3D perception rather than just re-labeling known classes.\n- Benchmark thoroughness. The paper evaluates both in an open setting (mixing ScanNet200 with reconstructed ScanNet3R-* scenes) and in the widely used standard setting (only ScanNetv2’s 20 labeled classes for supervision, evaluate on all 200 classes and report base vs. novel performance). The setting of these benchmarks is sufficient."}, "weaknesses": {"value": "- Generalization to other backbones is not empirically shown. OVSeg3R is advertised as a training scheme, but all quantitative results are on a single architecture family (SegDINO3D to SegDINO3D-VL). The author does not display the results of Mask3D, OneFormer3D, ODIN, etc. retrained under OVSeg3R. This makes it hard to judge whether VIP and IBSp are truly architecture-agnostic or mainly tailored to SegDINO3D’s query-from-superpoints decoder.\n- Entangled factors in the main comparison. The baseline SegDINO3D-VL trained in the traditional way uses only ScanNet200’s manually aligned 3D annotations. The OVSeg3R model, in contrast, gets access to reconstructed ScanNet3R-* scenes plus lifted 2D pseudo-labels with potentially more and diversified supervision. It is therefore non-trivial to isolate how much of the gain comes from the proposed VIP/IBSp training mechanism itself vs. simply having more data. An ablation experiment that feeds SegDINO3D-VL the same data volume but without VIP/IBSp would clarify causality.\n- Presentation clarity. Because the paper names the final model “OVSeg3R (Ours),” it can be misunderstood as a new architecture, rather than “SegDINO3D-VL trained with our pseudo-labeling scheme (OVSeg3R) plus VIP+IBSp.” This blurs the line between the backbone and the training recipe, and makes it trickier to interpret fairness in the comparisons."}, "questions": {"value": "These are the major concerns that will be related to my final rating:\n- Backbone generality. Can you demonstrate OVSeg3R on other closed-vocabulary 3D instance segmentation backbones (e.g., Mask3D, OneFormer3D, ODIN)? Even a smaller-scale experiment would help establish that VIP and IBSp are not tightly coupled to SegDINO3D’s query-from-superpoints architecture. If not, please discuss what specific assumptions OVSeg3R makes about the backbone.\n- Ablation disentangling data volume vs. VIP/IBSp. In Table 1 / Table 3 comparisons, OVSeg3R benefits from additional *ScanNet3R-** reconstructed scenes and thus more pseudo-labels. Could you run a control where SegDINO3D-VL is exposed to the same reconstructed scenes and text-prompts but without VIP or IBSp?\n\nAnd here are some minor questions that may not influence the rating:\n- Can you further clarify how to reproduce your visualization with your ply files? It seems I need to manually adjust the scale of each point cloud after opening it in MeshLab.\n- Metrics beyond AP. Since the main story is *we drastically improve novel/long-tail categories*, can you report recalls on head / common / tail categories, or at least qualitative failure modes for novel classes? This would make the open-vocabulary claim even more convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XX73lBe4V2", "forum": "rIMOXvaLAe", "replyto": "rIMOXvaLAe", "signatures": ["ICLR.cc/2026/Conference/Submission4271/Reviewer_3KhS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4271/Reviewer_3KhS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552394129, "cdate": 1761552394129, "tmdate": 1762917269067, "mdate": 1762917269067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents OVSeg3R, a framework designed to tackle open-vocabulary 3D instance segmentation by leveraging the capabilities of existing 2D open-vocabulary segmentation models and 3D reconstruction models. The core idea is to use 3D reconstruction from videos to lift 2D instance predictions into the 3D space, generating large-scale, automatic supervision for a 3D segmentation network. The authors introduce two main technical contributions to stabilize this 2D-to-3D lifting process: 1) the View-wise Instance Partition (VIP) algorithm to partition the predicted 3D segmentation masks to their respective views for training supervision, and 2) the 2D Instance Boundary-aware Superpoint (IBSp) clustering, which leverages 2D mask boundaries to directly constrain 3D superpoint generation, thereby preventing over-smoothing and ensuring sufficient geometric distinction at instance boundaries that is often lost in 3D reconstruction outputs. The proposed method demonstrates strong performance, successfully extending closed-vocabulary 3D segmentation to the open-vocabulary regime."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. **Addressing a Critical Problem**: Open-vocabulary 3D segmentation is a crucial area for real-world applications (robotics, AR/VR), and this paper offers a scalable, data-efficient solution by exploiting available 2D models and 3D reconstruction pipelines.\n\n2. **Innovative Supervision Strategy**: The automatic generation of 3D supervision by projecting 2D open-vocabulary masks is a practical and creative approach to circumvent the prohibitive cost of manual 3D annotation.\n\n3. **Targeted Solutions**: The paper correctly identifies the two main issues of 2D-to-3D lifting—inconsistency/partiality in view projections and over-smoothing of 3D geometry—and proposes two respective solutions: VIP and IBSp, which effectively use 2D annotations from 2D segmentation models to refine and stabilize the 3D pseudo-labeling and feature grouping process.\n\n4. **Competitive Results**: The reported performance metrics suggest that OVSeg3R is an effective way to transfer knowledge from the 2D domain to 3D, achieving state-of-the-art results in the open-vocabulary 3D segmentation setting."}, "weaknesses": {"value": "1. **Clarity of Technical Details**: The description in lines 272-278 is confusing: concepts like \"the attention is masked by the mask prediction of the previous layer\", how \"the multi-layer decoder functions like K-means clustering\" to associate a superpoint with an entity, and \"pixels that describe an entity\" are expressed vaguely and require rephrasing for clarity. In addition, it is better to add captions for math symbols in Fig. 2\n\n2. **Reliance on a Single 2D Perception Model**: While the authors test the framework's robustness across different 3D reconstruction models (e.g., VGGT and MASt3R-SLAM), the core pipeline relies on a single 2D open-vocabulary segmentation model (DINO-X). Given that the quality of the 3D pseudo-labels is highly dependent on the 2D mask predictions and their boundaries, a sensitivity analysis or ablation study comparing results across different 2D perception models (e.g., models with different architectures or mask qualities) is missing. This limits the understanding of OVSeg3R's generalization capability beyond the specific chosen 2D backbone.\n\n3. **Limited Generalizability Testing**: The evaluation is conducted primarily on a single dataset, ScanNetV2. While the methodology is aimed at open-vocabulary generalization, testing its performance, robustness, and calibration on diverse 3D datasets (e.g., matterport3d, sunrgbd, or datasets with different scene characteristics, density, and scales) is crucial to verify generalizability."}, "questions": {"value": "1. **Domain Discrepancy (Train vs. Test Data)**: The method is trained on point clouds generated by 3D reconstruction models. During inference, point clouds may be obtained from different sensors (like LiDAR or consumer depth cameras), which produce data with different noise characteristics, density, and quality than reconstructed scenes. Does this discrepancy between the training data distribution (reconstructed) and potential inference data distribution (direct sensor capture) hinder the model's performance?\n\n2. **Failure Modes and Error Analysis**: Since this is a pseudo-labeling approach, what are the observed failure modes for OVSeg3R? Does the VIP mechanism also introduce a significant number of false negatives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2iuoj0LZeP", "forum": "rIMOXvaLAe", "replyto": "rIMOXvaLAe", "signatures": ["ICLR.cc/2026/Conference/Submission4271/Reviewer_G6nS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4271/Reviewer_G6nS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635815500, "cdate": 1761635815500, "tmdate": 1762917268691, "mdate": 1762917268691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes OVSeg3R, a training scheme that turns a closed-vocabulary 3D instance segmentor (SegDINO3D) into an open-vocabulary model by: (i) lifting per-view 2D open-vocabulary masks into 3D via reconstruction correspondences; (ii) a View-wise Instance Partition (VIP) procedure that partitions scene-level predictions to views to avoid false positives from partial labels; and (iii) a 2D Instance Boundary-aware Superpoint (IBSp) graph that prunes KNN edges crossing 2D instance boundaries before superpoint clustering. OVSeg3R reports +2.3 mAP overall on ScanNet200 (val) and +7.7 mAP on novel classes in the “standard” setting, with improved tail-class performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear training recipe that leverages modern 3D reconstruction and strong 2D open-vocabulary segmentors to create supervision without manual 3D labels; aligns with realistic inputs from videos/handheld sensors.\n\nIBSp is a practical fix to over-smoothed reconstructions that would otherwise merge thin/low-salience objects into planes; the pruning logic is transparent"}, "weaknesses": {"value": "The pipeline inherits any bias/error from the 2D segmentor (DINO-X) and reconstruction correspondences. There is no quantitative audit of mask noise, cross-view inconsistencies, or the error rate of the VIP view assignment/truncation (only a qualitative claim that it’s “highly reliable”).\n\nThe main ablation shows VIP matters more than IBSp on the evaluation set, but the paper itself argues IBSp is crucial when reconstructions are noisy; rigorous stress tests with controlled boundary noise are missing. \n\nThe paper concedes there’s no mathematical guarantee the partition always assigns to the “right” view; no statistics on misassignments or training-time instability (e.g., oscillations/false negatives) are provided."}, "questions": {"value": "Cost profile: Report FLOPs/params and p50/p90 latencies for SegDINO3D-VL forward, 2D sampling, IBSp graph build, and VIP,\n\nFailure cases: Where do VIP or IBSp harm performance (thin structures, reflective surfaces, severe drift)? Show qualitative counter-examples.\n\nInject controlled 2D mask noise (boundary dilation/erosion, label flips) and 3D correspondence noise (perturbed C) to quantify VIP/IBSp robustness. Report mAP deltas."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x4mRmrQXlV", "forum": "rIMOXvaLAe", "replyto": "rIMOXvaLAe", "signatures": ["ICLR.cc/2026/Conference/Submission4271/Reviewer_LW1e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4271/Reviewer_LW1e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930596460, "cdate": 1761930596460, "tmdate": 1762917268283, "mdate": 1762917268283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OVSeg3R, an approach to learn *open-vocabulary 3D instance segmentation* from 2D foundation models. To achieve this, it proposes an augmented version of SegDINO3D, namely SegDINO3D-VL, which replaces the categorical classifier with a CLIP-based feature predictor. To improve training stability, two additional components are introduced: (a) the View-wise Instance Partition (VIP) algorithm, which assigns each 3D mask prediction to the corresponding camera view to ensure consistent multi-view supervision, and (b) the 2D Instance Boundary-aware SuperPoint (IBSp) module, which modifies the traditional superpoint aggregation to account for 2D mask boundaries and prevent over-smoothing of 3D instance masks. In addition, the model is trained on point maps predicted either by Mast3r-SLAM or VGGT, and this makes it robust to noisy point clouds. Experiments on ScanNetV2 and ScanNet200 demonstrate that OVSeg3R achieves state-of-the-art performance, outperforming existing open-vocabulary approaches and even several fully supervised baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **SoTA performances on Open-Vocabulary Instance Segmentation.** By carefully extending SegDINO3D to the open vocabulary OVSeg3R achieves SoTA performances for the task. This makes the method valuable for the research community working on multimodal 3D understanding.\n2. **Evaluation of the design choices.** The paper includes ablation studies that analyze the impact of the VIP algorithm and the IBSp module, providing useful insights into their contribution to the overall method.\n3. **Robustness.** Training on point clouds produced by VGGT and Mast3rSLAM makes the method robust to reconstruction noise. This property is practically important, as it allows faster deployment without relying on possibly slow, high-accuracy 3D reconstruction pipelines."}, "weaknesses": {"value": "1. **Limited methodological novelty.** While the task is interesting and the results surpass existing baselines, I am not entirely convinced about the technical novelty of the method. The core idea of distilling 2D vision-language knowledge into 3D has already been explored in prior works such as OpenScene, OpenMask3D, etc. In addition, the two extensions proposed in the paper (i.e. VIP  algorithm and IBSp module) can be seen more as implementation choices than technical contributions."}, "questions": {"value": "1. In line 234 what does $s$ represents?\n2. How well does the model perform when given higher resolution reconstructions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kor8nWKNpp", "forum": "rIMOXvaLAe", "replyto": "rIMOXvaLAe", "signatures": ["ICLR.cc/2026/Conference/Submission4271/Reviewer_Q4jS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4271/Reviewer_Q4jS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941778354, "cdate": 1761941778354, "tmdate": 1762917268019, "mdate": 1762917268019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}