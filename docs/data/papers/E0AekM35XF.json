{"id": "E0AekM35XF", "number": 14849, "cdate": 1758244683703, "mdate": 1759897345618, "content": {"title": "CTIArena: Benchmarking LLM Knowledge and Reasoning across Heterogeneous Cyber Threat Intelligence", "abstract": "Cyber threat intelligence (CTI) is central to modern cybersecurity, providing critical insights for detecting and mitigating evolving threats. \nWith the natural language understanding and reasoning capabilities of large language models (LLMs), there is increasing interest in applying them to CTI, which calls for benchmarks that can rigorously evaluate their performance.\nSeveral early efforts have studied LLMs on some CTI tasks but remain limited: (i) they adopt only closed-book settings, relying on parametric knowledge without leveraging CTI knowledge bases; (ii) they cover only a narrow set of tasks, lacking a systematic view of the CTI landscape; and (iii) they restrict evaluation to single-source analysis, unlike realistic scenarios that require reasoning across multiple sources.\nTo fill these gaps, we present CTIArena, the first benchmark for evaluating LLM performance on heterogeneous, multi-source CTI under knowledge-augmented settings.\nCTIArena spans three categories, structured, unstructured, and hybrid, further divided into nine tasks that capture the breadth of CTI analysis in modern security operations.\nWe evaluate ten widely used LLMs and find that most struggle in closed-book setups but show noticeable gains when augmented with security-specific knowledge through our designed retrieval-augmented techniques.\nThese findings highlight the limitations of general-purpose LLMs and the need for domain-tailored techniques to fully unlock their potential for CTI.", "tldr": "We introduce CTIArena, the first benchmark for LLMs on heterogeneous, multi-source CTI.", "keywords": ["Large Language Models", "Cyber Threat Intelligence", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/158f2789d9127a80d7ce2119d3b39c69e31db567.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces CTIArena, a benchmark designed to evaluate LLMs on diverse CTI tasks under both closed-book and knowledge-augmented settings. It defines nine tasks across structured, unstructured, and hybrid categories: spanning mappings among CVE, CWE, CAPEC, and ATT&CK taxonomies. The authors evaluate ten state-of-the-art LLMs and demonstrate that while general-purpose models perform poorly in closed-book CTI reasoning, performance improves significantly with security-specific retrieval methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper offers limited strengths in terms of novelty, clarity, or significance. While it identifies several limitations of prior CTI benchmarks, such as CTIBench and SEvenLLM, including their narrow task coverage and lack of retrieval-augmented evaluation, it does not necessarily address these limitations. Although the authors aim to develop a more comprehensive benchmark for CTI reasoning, as discussed below, the resulting CTIArena framework still falls short of constituting a true “reasoning” benchmark for CTI contexts."}, "weaknesses": {"value": "The main limitation of this work is that CTIArena is not truly a benchmark for evaluating LLM reasoning. The included tasks primarily measure an LLM’s ability to extract or retrieve known knowledge, rather than perform any form of reasoning or inference. Consequently, the large performance gains observed with retrieval-augmented generation (RAG) are unsurprising, as once supporting documents are provided, the tasks essentially become lookup problems. For instance, in the RCM (Root Cause Mapping) structured task, the question simply asks for the CWE ID corresponding to a given CVE ID, without including any contextual information or description. Such a task can be trivially solved via a direct database or Google search, and since many CVEs postdate the training cutoff of the evaluated LLMs, it is expected that they would fail without retrieval access. This design does not test reasoning ability, nor is an LLM the appropriate tool for it. In contrast, CTIBench defines RCM using CVE descriptions rather than IDs, requiring models to infer the underlying weakness (CWE) from textual evidence, thus addressing a practical and reasoning-oriented use case for analyzing newly discovered vulnerabilities. CTIArena completely overlooks this reasoning aspect in its task design. \n\nMoreover, the paper makes several unsubstantiated claims, such as stating that CTIBench contains only 150 manually annotated queries, whereas in reality the dataset is substantially larger. For example, the RCM task alone includes around 1,000 questions, while CTIArena itself contains only 691 total questions. Moreover, the two hybrid tasks, CTI-VCA and CTI-ATA, are very similar in nature to the CTI-RCM and CTI-ATE tasks from CTIBench."}, "questions": {"value": "I would recommend that the authors either revise the task designs to better capture and evaluate the reasoning abilities of LLMs in CTI contexts by incorporating tasks that require inference, abstraction, or contextual understanding, rather than factual lookup or reframe the benchmark’s stated goal to focus explicitly on assessing whether LLMs can retrieve and align CTI concepts from heterogeneous knowledge sources."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "749W0FX2Tn", "forum": "E0AekM35XF", "replyto": "E0AekM35XF", "signatures": ["ICLR.cc/2026/Conference/Submission14849/Reviewer_oucB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14849/Reviewer_oucB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710072599, "cdate": 1761710072599, "tmdate": 1762925203995, "mdate": 1762925203995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work benchmarks how well LLMs understand and reason over CTI from different sources. The benchmark includes 9 tasks across structured, unstructured, and hybrid CTI settings, using 691 question–answer pairs. The authors evaluated 10 popular LLMs under both closed-book and knowledge-augmented settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work covers structured, unstructured, and hybrid CTI tasks.\n\n2. The tasks reflect what security analysts actually do (e.g., mapping CVEs to weaknesses, profiling threat actors)."}, "weaknesses": {"value": "1. The benchmark contains only 691 QA instances, which may not be enough to reflect the complex and ever-evolving nature of CTI. \n\n2. Much of the dataset is produced through LLM prompting with human filtering rather than entirely human-authored. It is thus unclear  about biases introduced during such a semi-automation process.\n\n3. Using fixed templates with predefined associations may lack the linguistic and structural diversity seen in real-world CTI queries. This may limit how well the benchmark tests genuine reasoning ability versus pattern-matching on fixed formats.\n\n4. The benchmark assumes that entities like CVEs, malware, or threat actor names can be cleanly mapped across sources using templates and matching rules. In reality, CTI is full of conflicts and ambiguous or conflicting evidence, which the benchmark largely ignores rather than tackles directly."}, "questions": {"value": "Please see the comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U9WjQnSaD0", "forum": "E0AekM35XF", "replyto": "E0AekM35XF", "signatures": ["ICLR.cc/2026/Conference/Submission14849/Reviewer_qfcD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14849/Reviewer_qfcD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964072056, "cdate": 1761964072056, "tmdate": 1762925203427, "mdate": 1762925203427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "(1) The paper introduces a benchmark for CTI reasoning. The benchmark is constructed through a 3-stage process:\n- stage 1: annotate correlation among different sources\n- stage 2: generating QA tasks \n- stage 3: human-curator \n\n(2) Empirical evaluation is performed by comparing several target models under different setups (closed-domain vs. knowledge-augmented), as shown in Table II.\n\n(3) Based on these experiments, the paper presents several observations about CTI reasoning performance and challenges."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed benchmark is timely and relevant, contributing to standardized evaluation of cybersecurity-related agent.\n\n- The proposed benchmark includes tasks that expand coverages of existing benchmarks."}, "weaknesses": {"value": "- The paper employs GPT-5 as an LLM-based judge to assess data quality during dataset construction and as an automatic evaluator to score model predictions against reference answers in the testing phase. However, the authors also include GPT-5 in the performance comparison (Tables 2, 3, and 4), emphasizing its superiority. This practice may introduce evaluation bias, as the same model is used both as a judge and as a participant in the comparison.\n\n- The evaluation process lacks detailed descriptions and does not include the evaluation of fine-tuned LLMs. In the Structured Tasks of Table 2, after injecting the related official entries of structured enumerations into the LLM, the performance improves significantly: from around 0 in the closed-book setting to around 1 with inference-time knowledge injection. There should be analysis to identify and verify the precise sources of the observed performance improvements.\n\n- Interpretation of results is unclear. It is not entirely evident how to interpret Table II on the proposed  benchmark’s overall validity.\n\n- Although Table I provides examples illustrating the benchmark’s coverage, there is no clear quantitative measure of how comprehensive or representative the benchmark is."}, "questions": {"value": "- Identify the source of the significant  jump of improvement as mentioned earlier. \n\n- A quantitative comparison with other benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ofQnNKLk0g", "forum": "E0AekM35XF", "replyto": "E0AekM35XF", "signatures": ["ICLR.cc/2026/Conference/Submission14849/Reviewer_6ADC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14849/Reviewer_6ADC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131852634, "cdate": 1762131852634, "tmdate": 1762925202823, "mdate": 1762925202823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}