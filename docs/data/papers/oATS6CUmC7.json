{"id": "oATS6CUmC7", "number": 5612, "cdate": 1757922781761, "mdate": 1763147284342, "content": {"title": "Audio-driven 3D Conversational Full-body Human Avatar Generation from a Single Image", "abstract": "Prior conversational 3D avatar systems require mapping audio to parametric poses and then pass through rendering pipeline. This forms a lossy bottleneck and introduces cumulative errors at the the pose–to–render interface, where quantization, retargeting, and per-frame tracking errors accumulate. As a result, they struggle to maintain tight audio–motion synchronization and to express micro-articulations crucial for conversational realism—bilabial closures, cheek inflation, nasolabial dynamics, eyelid blinks, and fine hand gestures—issues that are amplified under single-image personalization. We address these limitations with an end-to-end framework that constructs a full-body, photorealistic 3D conversational avatar from a single image and drives it directly from audio, bypassing intermediate pose prediction. The avatar is represented as a particle-based deformation field of 3D Gaussian primitives in a canonical space; an audio-conditioned dynamics module produces audio-synchronous per-particle trajectories for face, hands, and body, enabling localized, high-frequency control while preserving global coherence. A splat-based differentiable renderer maintains identity, texture, and multi-view realism, and we further enhance synchronization and natural expressivity by distilling priors from a large audio-driven video diffusion model using feature-level guidance and weak supervision from synthetic, audio-conditioned clips. End-to-end training lets photometric and temporal objectives jointly shape the audio-conditioned deformation and rendering. Across diverse speakers and conditions, our method improves lip–audio synchronization, fine-grained facial detail, and conversational gesture naturalness over pose-driven baselines, while preserving identity from a single photo and supporting photorealistic novel-view synthesis—advancing accessible, high-fidelity digital humans for telepresence, assistants, and mixed reality.", "tldr": "We present a method to generate a photorealistic 3D conversational avatar from a single image that is driven directly by audio for natural, synchronized speech and gestures.", "keywords": ["3D Computer Vision", "Neural Rendering", "3D Avatar Modeling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/348c714e7875b7d4772e6704a27c47039ec6ddfb.pdf", "supplementary_material": "/attachment/3af09f5a7a31181c3ee3de8a2ff583e55a058fe3.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an end-to-end framework for generating full-body, audio-driven 3D conversational avatars from a single image, bypassing traditional intermediate pose estimation. The method represents the avatar as a particle-based deformation field over 3D Gaussians, enabling direct audio-to-deformation control with fine-grained facial and hand articulation. It further leverages knowledge distillation from a large audio-driven video diffusion model to enhance motion naturalness and synchronization under the single-image setting. Experiments show improvements over pose-mediated 3D avatars and 2D diffusion baselines in metrics like lip-sync (SyncC/D), hand keypoint confidence (HKC), and video realism (FVD/FID).\n\nWhile the architecture is technically sound and the results visually compelling, the paper overstates its 3D novelty (novel-view synthesis is not rigorously validated), lacks human evaluation, and does not isolate the contribution of synthetic data or the renderer from the core deformation design. Motion quality—especially gesture diversity and audio–gesture semantic alignment—is assessed only via indirect proxies. These gaps prevent a confident assessment of the method’s true advance over existing audio-driven avatar systems."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**Better cloth animation:**\n\nThe method avoids mesh-based skinning (e.g., SMPL-X LBS), which often causes cloth sliding or unnatural deformation. By using a particle-based deformation field over 3D Gaussians, it preserves fine surface details and avoids topology constraints. This leads to more natural cloth motion, especially around shoulders and torso. The renderer also retains texture fidelity from the single reference image.\n\n**Good lip-sync compared to other full-body methods**\n\nQuantitative results (SyncC↑, SyncD↓) and visual comparisons show tighter audio–lip alignment than pose-mediated baselines like LHM or PERSONA. This stems from bypassing the audio-to-pose bottleneck and using end-to-end temporal rendering losses. However, lip-sync is still evaluated only via automated metrics—not perceptual studies.\n\n**End-to-end direct audio-to-deformation design**\n\nThe architecture eliminates intermediate pose estimation, reducing error accumulation and enabling micro-articulations (e.g., cheek puffing, eyelid blinks). The particle-based control allows localized high-frequency motion while maintaining global coherence. This is a meaningful architectural shift from template-driven pipelines."}, "weaknesses": {"value": "**Lack detailed gesture quality analysis**\n1. Lack of diversity analysis. Hand and body motions appear repetitive across utterances, lacking semantic variation (e.g., beat vs. iconic gestures). The method relies on distilled priors from a diffusion model, which may limit gesture vocabulary. No analysis is provided on gesture diversity, timing, or speech alignment.\n2. Lack of human evaluation & user study. Claims about “naturalness,” “expressivity,” and “conversational realism” rely solely on proxy metrics (FVD, HKC) and visual inspection. Without MOS or A/B preference studies, it’s unclear if users perceive the motion as more realistic. This weakens the paper’s core user-facing claims.\n3. Lack of analysis on audio–gesture relation. The paper does not test whether gestures are meaningfully tied to speech content (e.g., stress, semantics). A critical experiment—driving the avatar with mismatched audio—would reveal if gestures are audio-contingent or generic. Without this, gesture “naturalness” remains unverified.\n\n**Novel-view synthesis: only fronts are shown**\n\nAll qualitative results are near-frontal; the paper admits poor performance under large viewpoint shifts (Appendix D). If the method cannot render consistent side/back views, its 3D advantage over 2D diffusion models is questionable for applications requiring true 3D interaction. No multi-view metrics (e.g., view SSIM, depth consistency) are reported. Although the author mentioned this in the limitation, but I'm still wondering if multi-view consistency can be achieved much easier than pure 2D baselines."}, "questions": {"value": "**Can you handle complex cloths like long dresses?**\n\nThe current demos show only short sleeves or fitted clothing. Long, flowing garments involve complex dynamics (e.g., self-contact, wind interaction) that Gaussian primitives may not capture. It’s unclear if the particle density or deformation model scales to such cases.\n\n**How’s the throughput?**\n\nThe paper omits FPS, latency, or real-time capability—critical for telepresence or AR/VR. While 3DGS enables fast rendering, the per-frame audio-driven deformation and autoregressive SE(3) updates may bottleneck inference. Without timing data, practical deployment remains uncertain.\n\n**Other questions**\n1. Identity preservation under motion: Does facial identity degrade during extreme expressions or gestures?\n2. Audio input robustness: What happens with noisy audio, silence, or non-speech sounds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Kz3UY0k0f2", "forum": "oATS6CUmC7", "replyto": "oATS6CUmC7", "signatures": ["ICLR.cc/2026/Conference/Submission5612/Reviewer_6aRf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5612/Reviewer_6aRf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802165156, "cdate": 1761802165156, "tmdate": 1762918158879, "mdate": 1762918158879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We would like to formally withdraw our paper from the ICLR 2026 submission process. After careful and thorough consideration, we have decided to revise and further improve our work in light of new findings and the constructive feedback we have received. We believe that these revisions will allow us to make a stronger and more meaningful contribution in the future. \n\nWe sincerely appreciate the time, effort, and thoughtful evaluations provided by the reviewers and Area Chairs."}}, "id": "TkDSOb5COy", "forum": "oATS6CUmC7", "replyto": "oATS6CUmC7", "signatures": ["ICLR.cc/2026/Conference/Submission5612/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5612/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147283474, "cdate": 1763147283474, "tmdate": 1763147283474, "mdate": 1763147283474, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an end-to-end framework that constructs a full-body, realistic 3D avatar from a single image and drives it directly with audio. To address the loss of nuanced details (such as finger movements) common in template-based driving approaches, this work introduces 1) an end-to-end pipeline that maps audio directly to a deformation field inside a Gaussian renderer, circumventing error accumulation from intermediate pose estimation, 2) a particle-based representation for precise local control and globally coherent full-body motion, and 3) diffusion-distillation scheme that leverages priors for high-quality, synchronized animation with limited personalization data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Unlike prior methods that rely on intermediate pose templates, this work introduces an end-to-end framework that directly drives a full-body 3D avatar from raw audio, overcoming the limited expressiveness of template-based approaches.\n2.The adoption of a particle-based representation, which utilizes an implicit deformation field to model the motion of Gaussians, enhances the generation of high-frequency details in intricate areas such as the hands and face.\n3.The diffusion-distillation scheme enables data-efficient learning of realistic, synchronized avatar behavior through feature alignment and synthetic clips."}, "weaknesses": {"value": "1.The presented results show compelling performance on subjects with relatively simple textures and fitting clothing. It would be valuable to further explore the method's performance under more challenging conditions, such as complex patterns or loose garments, to understand its generalizability.\n2.For a fair comparison with the LHM baseline, it would be helpful to clarify whether it was fine-tuned on the conversational dataset. A comparison against a fine-tuned LHM would more directly highlight the benefits of the proposed architecture.\n3.The initial personalization stage involves fine-tuning the Hand++ Body-Head Transformer. It would be valuable to report the associated computational cost and time, as these are important factors for assessing the practical usability of the method."}, "questions": {"value": "While the limitations note challenges in novel view synthesis, I would be interested to see more concrete evidence of its current performance, such as qualitative examples or a quantitative metric (e.g., PSNR from a held-out view).\nI am also curious about whether fine-tuning the Gaussian decoder, compared to the original LHM, introduces a trade-off between enhancing front-view expressiveness and preserving the capability for accurate novel view synthesis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "20FHXq1wnx", "forum": "oATS6CUmC7", "replyto": "oATS6CUmC7", "signatures": ["ICLR.cc/2026/Conference/Submission5612/Reviewer_ms4y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5612/Reviewer_ms4y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884147738, "cdate": 1761884147738, "tmdate": 1762918158472, "mdate": 1762918158472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for audio-driven avatar generation from a single RGB  image.\n\nTo this end the authors leverage the pretrained LHM model, which can already predict 3D Gaussian attributes from a single RGB image, but is animated using SMPL pose conditions. The authors argue that using SMPL poses are missing important details to achieve realistically behaving conversation avatars, and that going from audio to SMPL-poses to avatar can accumulate errors. \nTherefore, the authors propose to directly learn deformations of Gaussians from the audio condition. \n\nIn order to obtain more stable training, the authors extend the typical set of losses (RGB, perceptual, regularization), using distillation losses from a recent pre-trained, audio-conditioned video diffusion model. To be specific video score distillation is used to guide predictions in the direction of the video diffusion model. Moreover, a keypoint loss between keypoint detections of the predicted images and video-diffusion generations put important constraints on human movement.\n\nTo validate their approach the authors evaluate against 3D baselines, which are conditioned on SMPL poses and require an audio-to-SMPL estimator. Secondly, the authors compare against audio-conditioned video-diffusion models. The proposed method outperforms all baselines, and additionally several important design decisions are ablated to be beneficial."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Overall the paper seems to be focused on obtaining high-quality visual results. To this end a large pipeline leveraging two large-scale human-specific models are successfully used, and the resulting excellent visual quality of the proposed method is one of its main strengths. \n\nDirectly conditioning on audio instead of SMPL-pose is an important aspect, which is novel for whole body avatars. In the face domain, such a concept has been proposed previously, e.g. in VASA-1. Nevertheless, it seems to be crucial addition to the whole body domain, although it has not been perfectly ablated (see below).\n\nWith recent improvements on video-diffusion models, the distillation strategy is a useful idea that can be also be applied to related domains. As such, it could become a significant part of future work, due to its simplicity and effectiveness.\n\nMost parts of the paper are well-written and easy to follow, except for the method section which is missing some details (see below)"}, "weaknesses": {"value": "Below I list some major and minor weaknesses in no particular order:\n- (A) Neither the paper nor the video seems to show renderings from novel views. This raises the question whether the model still produces correct 3D geometry, or whether the renderings only look good from the same viewpoint as the input image. If that would be the case, one arrives at the question what the the benefit of a enforcing a 3D representation is. Learning to generate image from the same perspective might be easier when directly operating in 2D space,  e.g. how would the method compare to a fine-tuned version of HunYuan-VideoAvatar? \n\n- (B) The evaluation seems to be heavily favoring the proposed method, since the training and evaluation is performed on the **same dataset** AND the **same identities**, i.e. only 10% of the frames are excluded. This heavily benefits the proposed model, since it can simply overfit the appearance of the identities, which constitutes large parts of the visual and perceptual metrics. But at least the hand gesture and synchronization metrics are less effected by this. Similarly, this also concerns the presented ablations, since it might favor methods with a larger representation capacity that can overfit better. Either I am somehow mistaken on this, or the evaluation on held-out IDs and ideally new datasets or in-the-wild examples (at least some qualitative comparisons) would be required to move the paper above the acceptance threshold. I am looking forward to hear the authors explanation for the chosen evaluation, and I am willing to upgrade my score based on it.\n\n\n- (C) The method is not perfectly clearly described, especially when it comes to prediction of movement, which is on of the core technical novelties. E.g. the method section is not self-contained when it comes to the movement of gaussians, e.g. it is not explained what the predicted $T_{m,t}$ are used for, neither is $\\mu^{\\prime}$ defined, altough it is used in eq. (7). This should be improved for the final version of the paper.\n\n- (D) On a semantic level, one of the papers core novelties is direct conditioning on audio compared to a combination of audio-to-SMPL and SMPL-conditioning. A proper ablation experiment for this would help the readers gauge the signifcance of this change. E.g. by using SMPL-based animation and predicting SMPL codes from audio."}, "questions": {"value": "- What does neural renderer mean, is it not regular 3DGS rasterization anymore?\n- Is the 3D consistency broken when training on this sort of data? This would be a core advantage compared to video-diffusion models, which are likely to scale better for audio conditioned 2D avatars.\n- How exactly does the deformation work? it seems that both the \"Particle Deformer\" and the \"Gaussian Head\" can predict some sort of transformation/offset to move around the Gaussians."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hJ4HCHBvKO", "forum": "oATS6CUmC7", "replyto": "oATS6CUmC7", "signatures": ["ICLR.cc/2026/Conference/Submission5612/Reviewer_TF4u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5612/Reviewer_TF4u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911558200, "cdate": 1761911558200, "tmdate": 1762918158201, "mdate": 1762918158201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an end-to-end framework for generating a full-body, photorealistic 3D conversational avatar from a single image, driven directly by audio. The method bypasses traditional audio-to-pose pipelines by introducing an audio-driven particle-based deformation field operating over 3D Gaussian primitives. It integrates a differentiable neural renderer and leverages video score distillation from large-scale audio-driven diffusion models to enhance synchronization and realism."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "End-to-end architecture: The proposed audio-driven particle deformer eliminates the lossy audio-to-pose bottleneck, a key limitation in previous avatar systems.\n\nSingle-image personalization: Good results given only one input photo, demonstrating robust identity preservation and realistic animation.\n\nIntegration with diffusion priors: The use of video score distillation from a large audio-driven diffusion teacher improves the quality of results from the baseline they start with."}, "weaknesses": {"value": "- The paper claims it needs a single image to get the final result, but in the experiment section, it says they take 80% of the videos as training data. \n\n- Scalability and efficiency: Although 3D Gaussian splatting improves speed, training and inference costs for full-body rendering are not clearly analyzed.\n\n- Data dependence: The framework relies on distillation from large diffusion models trained on massive datasets, yet the proposed method's lip synchronization results are better than the diffusion model itself. It's not clear how this is possible \n\n- Also, the paper claims in L185 that the video diffusion models struggle with preserving the identity, and yet use the same video model to distill knowledge to the proposed model, and claim better identity preservation. It's a bit of a contradictory loss."}, "questions": {"value": "- How robust is the model to out-of-domain audio, such as emotional or non-conversational speech?\n\n- Can the approach generalize to non-frontal or multi-person scenes?\n\n- How does the temporal stability behave in long sequences (beyond 30 seconds)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t0mRT2Jlho", "forum": "oATS6CUmC7", "replyto": "oATS6CUmC7", "signatures": ["ICLR.cc/2026/Conference/Submission5612/Reviewer_6N8z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5612/Reviewer_6N8z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995595366, "cdate": 1761995595366, "tmdate": 1762918157934, "mdate": 1762918157934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}