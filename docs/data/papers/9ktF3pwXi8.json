{"id": "9ktF3pwXi8", "number": 5376, "cdate": 1757905616541, "mdate": 1763650969595, "content": {"title": "From End-to-End to Step-by-Step: learning Composable Navigation Primitives for Vision-Language Navigation", "abstract": "Recent Vision-Language Navigation (VLN) research with Multi-modal Large Language Models (MLLMs) has broadly adopted end-to-end training on long-horizon instruction datasets. However, human navigation mainly relies on the sequential execution of simple primitives guided by immediate observations. Our analysis shows that, although VLN models reported achieving promising results on long-horizon instructions, they struggle with basic navigation primitives (e.g., move, change region). To the best of our knowledge, we are the first to point out this phenomenon. To address this, we propose a primitive-based paradigm that first learns core skills and then composes them into long-horizon behaviors. We design a unified data pipeline to construct Vision-Language-Move-Base (VLMB), the first controllable benchmark centered on the move-to primitive, covering 206 scenes and 873 object instances. Based on VLMB, we develop Move-to-Anything, a model equipped with a memory mechanism that balances historical context with current observations. Experiments demonstrate that existing VLN models achieve only a 43.8\\% success rate in MP3D; our approach reaches 60.6\\% in MP3D and 71.4\\% in HM3D, exhibiting substantially stronger compositional generalization. These results highlight the effectiveness of primitive-based learning for building robust and generalizable navigation agents.", "tldr": "Current VLN systems lack basic navigation skills; we propose a step-by-step training paradigm to equip MLLMs with reliable foundational capabilities for VLN tasks.", "keywords": ["embodied ai", "vision language navigation", "multimodal large language models"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/71247b5a5417d32a93e3810f2906478c6ff69404.pdf", "supplementary_material": "/attachment/3611a017d84ec4645b2cddae9cc77ec38d0d4809.pdf"}, "replies": [{"content": {"summary": {"value": "This paper identifies a phenomenon in current video-based navigation VLAs: although they perform well in long-horizon navigation, they exhibit poor performance in short-horizon navigation tasks. The authors believe the reason lies in the unbalanced training data of VLN-CE R2R and RxR. To address this, they propose the VLMB dataset, which incorporates diverse instruction horizons across both MP3D and HM3D environments. Moreover, the authors propose a video-based VLA that efficiently models both short-term and long-term history, achieving best performance on the VLMB benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The highlighted phenomenon is interesting and significant in current VLN-oriented VLA methods, as it reflects the data foundations underlying existing approaches.\n- The proposed dataset, VLMB, could benefit the research community by providing more diverse instructions and scenarios.\n- The Move-to-Anything framework is largely technically sound and demonstrates better performance than existing methods."}, "weaknesses": {"value": "Major weakness:\n- The paper stops at comparing short-horizon and long-horizon tasks. I would like to see a more in-depth analysis of the impact of instruction length (i.e., the number of sub-goals) on VLA models—such as the upper limit of sub-goal size that existing VLA models can handle, or the performance variance across different numbers of sub-goals.\n- The concept of using short-term and long-term frames is unclear to me. How are long-term observations maintained (e.g., are all frames kept, or is there a selection process)? Does the memory retain all tokens, or only those from chosen frames?\n\nMinor weakness:\n- The trajectories and instructions are based on off-the-shelf scenes and simulators, which diminishes the dataset's contribution. However, I believe this is acceptable since the proposed data are tailored for a specific purpose."}, "questions": {"value": "- All experiments are conducted in the VLMB environment, which raises the question of how the proposed method would perform on VLN-CE benchmarks such as R2R or RxR. Would training on VLMB data improve model performance in VLN-CE, for instance, through co-training with data from both datasets?\n- The comparison appears unfair, as StreamVLN and NaVid have not been fine-tuned on VLMB."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XnaxdY4vKt", "forum": "9ktF3pwXi8", "replyto": "9ktF3pwXi8", "signatures": ["ICLR.cc/2026/Conference/Submission5376/Reviewer_NPKL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5376/Reviewer_NPKL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576397569, "cdate": 1761576397569, "tmdate": 1762918029981, "mdate": 1762918029981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new framework called Move-to-Anything and a corresponding dataset named VLMB, aimed at improving Vision-and-Language Navigation by enabling models to execute general, step-by-step instructions in a more flexible and human-like manner. The authors argue that traditional VLN methods struggle with simple, generalizable commands due to their dependency on fixed instruction distributions. To address this, they reformulate VLN into a more modular and universal navigation task, where agents are trained to move toward any visual or linguistic target. The proposed method integrates spatial grounding and multimodal understanding, and the experiments show consistent performance gains over baseline models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear Presentation and Visualization. The paper is clearly written and well-organized. The figures and examples effectively illustrate the motivation, architecture, and qualitative results. The visualizations, especially those comparing instruction-following behaviors, help readers intuitively understand the model’s capabilities.\n2. Improved Performance in Experiments. The proposed Move-to-Anything demonstrates noticeable performance improvements over strong  VLN baselines across the proposed dataset. The results validate that the method is better at handling flexible, goal-driven instructions.\n3. Reframing VLN to a generalized primitive-based paradigm is an interesting idea. It pushes the direction of VLN research toward more open-ended and human-like reasoning rather than narrow, dataset-dependent training."}, "weaknesses": {"value": "1. Motivation. The biggest issue with this paper is the motivation. The authors claim that “existing models fail to execute these general but straightforward instructions effectively,” but this statement is not well supported by experiments. The results in Table 2 only compare two VLN methods—NaVid and StreamVLN—while many other existing approaches are not included. Without broader evidence, it’s hard to accept the claim that current models generally fail on such instructions.\n2. Dataset Distribution and Fairness. VLN models are known to be highly sensitive to the distribution of instructions, which is why we usually need to train a separate model for each dataset. For example, a model trained on R2R performs poorly on RxR, and I think this is quite similar to the situation described in the paper. The VLMB dataset seems to be another dataset with a different instruction distribution. So, it’s not surprising that the Move-to-Anything model, when trained on VLMB, performs much better than models that were never exposed to this type of data. I’m also curious how the proposed method would perform on R2R or RxR, which are also multi-step navigation tasks (as in Table 3).\n3. Novelty. The novelty of the paper is limited. The main challenge in VLN is to align language instructions with visual observations over long trajectories, while the proposed dataset seems to simplify the task instead of addressing this difficulty. In addition, the two new components added to the Move-to-Anything model are techniques already widely used in MLLMs and VLN research, so the technical contribution feels incremental."}, "questions": {"value": "1. Classification of Modular Methods. In the “Related Work” section, the term Modular Method VLN itself feels a bit odd. The paper describes these methods as ones that “decompose the navigation pipeline into distinct components such as perception, language understanding, mapping, planning, and control.” However, the cited examples—HAMT and DUET—are generally regarded as end-to-end approaches. I think the criteria for this classification need to be explained more clearly.\n2. Scene Selection Criteria. The paper does not mention the standard for selecting scenes from MP3D and HM3D. \n3. The authors should avoid using the term ObjNav to describe one type of generated instruction. It’s confusing because ObjNav is also used in the paper with a different meaning. \n4. The finding in lines 92-93 is not mentioned and explained in the latter part.\n5. Currently, the primitives are limited to \"MoveTo\", is there any other possible primitives or how could we use such primitives to handle object-centric tasks like REVERIE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QCnVWLOEnk", "forum": "9ktF3pwXi8", "replyto": "9ktF3pwXi8", "signatures": ["ICLR.cc/2026/Conference/Submission5376/Reviewer_XQBp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5376/Reviewer_XQBp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578327386, "cdate": 1761578327386, "tmdate": 1762918029656, "mdate": 1762918029656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the field of Vision-Language Navigation (VLN) and points out that although current end-to-end VLN models based on Multi-modal Large Language Models (MLLMs) perform reasonably well on long-horizon instruction tasks, they have significant shortcomings in basic navigation primitives (such as moving and changing regions). To address this issue, the paper proposes a primitive-based learning paradigm, which first learns core skills and then combines them into long-horizon navigation behaviors. It constructs the first controllable benchmark dataset, Vision-Language-Move-Base (VLMB), centered on the \"move-to\" primitive, covering 206 scenes and 873 object instances. Based on this dataset, the paper develops the Move-to-Anything model with a memory mechanism that balances historical context and current observations. Experiments show that this method achieves a success rate of 60.6% in the MP3D scene and 71.4% in the HM3D scene, significantly outperforming the 43.8% success rate of existing models, and demonstrating stronger compositional generalization ability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It identifies the performance shortcomings of end-to-end VLN models in basic navigation primitives, breaking the inherent perception that \"good performance on long-horizon tasks implies solid basic capabilities\".\n\n2.  A unified data pipeline is designed to build the VLMB dataset. Focusing on the \"move-to\" primitive, it integrates multi-scene resources, enhances spatial-semantic information, and ensures data quality through interactive verification. It fills the gap of existing datasets in training basic navigation skills and provides reliable data support for subsequent related research.\n\n3. The proposed hierarchical memory mechanism effectively balances recent detail fidelity and global context by decomposing historical observations into short-term and long-term components. At the same time, temporal and segment embeddings are introduced to improve the stability of long-horizon reasoning. \n\n4. From problem formulation, literature review of related work, to dataset construction, model design, and experimental verification, each part is coherently connected, and the argumentation progresses step by step. This allows readers to clearly understand the research ideas and technical paths, ensuring high readability."}, "weaknesses": {"value": "1. Although the paper verifies the model's performance in MP3D and HM3D scenes, it does not deeply explore the model's performance in more complex and diverse real physical environments (such as extreme weather and scenes with dense dynamic obstacles). It is recommended to supplement test results in such scenes, provide failure case analyses and feature visualization, to help readers clearly grasp the applicable scope and limitations of the method.\n\n2. Existing ablation experiments verify the effectiveness of dataset cross-validation, Hierarchical Memory (HM), and Temporal & Segment Embeddings (TSE). However, there is a lack of sensitivity analysis on parameters such as the short-term window size (W) and the number of long-term memory slots (M) in HM, as well as disassembly experiments on the respective contributions of temporal embeddings and segment embeddings in TSE. It is recommended to supplement such experiments to clarify the optimal settings of each parameter and the independent role of each component.\n\n3. The paper does not mention the comparison of the Move-to-Anything model with existing SOTA VLN models (such as NaVid and StreamVLN) in terms of parameter count, training duration, and inference speed. It is recommended to supplement quantitative analysis of model complexity and efficiency."}, "questions": {"value": "See weaknesses 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2I7w87h42T", "forum": "9ktF3pwXi8", "replyto": "9ktF3pwXi8", "signatures": ["ICLR.cc/2026/Conference/Submission5376/Reviewer_Nwdw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5376/Reviewer_Nwdw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796292736, "cdate": 1761796292736, "tmdate": 1762918029441, "mdate": 1762918029441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}