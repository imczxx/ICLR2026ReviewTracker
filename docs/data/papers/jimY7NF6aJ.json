{"id": "jimY7NF6aJ", "number": 5002, "cdate": 1757831345043, "mdate": 1759898000922, "content": {"title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding", "abstract": "Video temporal understanding is crucial for multimodal large language models (MLLMs) to reason over events in videos. Despite recent advances in general video understanding, current MLLMs still struggle with fine-grained temporal reasoning. While reinforcement learning (RL) has been explored to address this issue recently, existing RL approaches remain limited in performance on time-sensitive tasks. In this work, we propose **MUSEG**, a novel RL-based method that enhances temporal understanding by introducing timestamp-aware multi-segment grounding. MUSEG enables MLLMs to align queries with multiple relevant video segments, promoting more comprehensive temporal reasoning. To facilitate effective learning, we design a customized RL training recipe with phased rewards that progressively guides the model toward temporally grounded reasoning. Extensive experiments on temporal grounding and time-sensitive video question answering (QA) tasks demonstrate that \\methodname significantly outperforms existing methods and generalizes well across diverse temporal understanding scenarios.", "tldr": "We propose MUSEG, a novel RL-based framework that enhances temporal understanding by introducing timestamp-aware multi-segment grounding.", "keywords": ["Video Temporal Understanding", "Temporal Video Grounding", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/16f73c986f5c1e8681cc964466762263180b7673.pdf", "supplementary_material": "/attachment/8c0d22f7668e8fd28c5d9b4096b12982e596ea92.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MUSEG, a reinforcement learning (RL) method to improve the fine-grained temporal understanding of Multimodal Large Language Models (MLLMs) in videos.\n\nThe core idea is timestamp-aware multi-segment grounding, which enables the model to align a query with multiple relevant video segments. This is optimized through a customized RL training recipe using phased rewards that progressively guides the model's reasoning.\n\nExperiments show MUSEG significantly outperforms existing methods on temporal grounding and time-sensitive video QA tasks, indicating strong performance and generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "With clear motivation and intuition, the paper uses GRPO and introduces two new reward functions, allowing the model to learn how to correctly segment events within videos for better temporal understanding. Especially in the segment matching reward, the authors meticulously design a function that computes the IoU of the predicted timestamps compared to the ground truth, a sound and wise choice.   Finetuning models with this method demonstrates improvements on selected temporal benchmarks. The authors also conducted exhaustive ablations in an attempt to justify different training strategies and reward design."}, "weaknesses": {"value": "1. Though the authors presented promising results on 4 different benchmarks, it would be great if they could evaluate on more temporal benchmarks, such as TempCompass [1], TemporalBench, Vinoground [2]'s video score (which contains multiple segments), and so on, if time permits.\n2. In Section 4.2.2, the timestamp reward is designed to be 1 if all timestamps are correct, and 0 if any are wrong. This design is concerning to me because I do not understand why this reward is so strict; for segment matching the authors applied a very lenient strategy involving IoU, why won't one consider giving partial credit to the timestamp reward as well? I believe this might be the reason why the authors had to remove timestamp reward after 400 steps.\n3. In Section 6.2, the authors stated that \"the model can continue to freely explore more effective reasoning strategies\" by removing the timestamp reward mid-training. Though the improvements quantitatively are acknowledged, there is no qualitative result demonstrating the difference in reasoning between removing and not removing the reward.\n4. Accompanied by the above doubts about the reward function, which is the core contribution of this paper, I hate to say that I don't think the paper has enough novelty. But this idea is subject to change depending on the rebuttal for the above points.\n\n[1] Liu et al, 2024, TempCompass: Do Video LLMs Really Understand Videos?\n\n[2] Zhang et al, 2024, Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sWtp5Nz4XB", "forum": "jimY7NF6aJ", "replyto": "jimY7NF6aJ", "signatures": ["ICLR.cc/2026/Conference/Submission5002/Reviewer_5XGo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5002/Reviewer_5XGo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760551132304, "cdate": 1760551132304, "tmdate": 1762917816662, "mdate": 1762917816662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a RL framework called MUSEG. The proposed method improves MLLMs' ability to understand temporal events in videos. Specifically, this paper proposed the task \"multi-segment grounding\", and uses GRPO with \"segment matching reward\" and \"timestamp reward\" to align predicted with true video segments and encourage temporal reasoning. Experiments show that MUSEG significantly outperforms previous SFT- and RL-based models on multiple benchmarks, achieving stronger fine-grained temporal reasoning and better generalization to time-sensitive video tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper's proposed methodology has a very clear motivation. It performs a preliminary empirical study revealing why using single-segment grounding as the training task won't work. \n2. The proposed approach outperforms previous baseline models on multiple benchmarks with both 3B and 7B models. \n3. This paper provides comprehensive analysis into why the proposed approach would work, along with the strength (1), making the proposed approach more reasonable. \n4. The proposed method won't hurt the model's performance on general video QA task."}, "weaknesses": {"value": "1. The proposed method's performance improvement on general task is greatly limited (compared with the performance improvement on grounding task). On all benchmarks, the performance improvement is less than 2%. The result reveals that there's a great limitation on the method's generalizability. \n2. The idea of utilizing the multi-segment as task has already appeared in the related work [1], which appears on the Internet on Jun 23 2025. It is one month earlier than the ICLR's comparison cutoff July 24, 2025, therefore, I think the connection to this work needs to be discussed, and it's likely that this work[1] needs to be compared with the proposed approach. \n\n[1] Universal Video Temporal Grounding with Generative Multi-modal Large Language Models, NeurIPS 2025"}, "questions": {"value": "1. In Table 2, is the \"+vanilla GRPO\" based on \"+vanilla SFT\" (so it's + GRPO AND +SFT), or is based directly on the original \"Qwen2.5-VL-7B\"?\n2. Why \"seconds\" is chosen as unit in verbal reasoning, rather than frames, millioseconds or minutes. If this will cause issues for long video or extremely short video?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dovkesBx3g", "forum": "jimY7NF6aJ", "replyto": "jimY7NF6aJ", "signatures": ["ICLR.cc/2026/Conference/Submission5002/Reviewer_d1dA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5002/Reviewer_d1dA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447473092, "cdate": 1761447473092, "tmdate": 1762917816214, "mdate": 1762917816214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper design rewards for temporal reasoning to do RL, and experiment on 2 models: Qwen2.5VL 7B and 3B, which beats the baselines. The rewards they design are segment matching rewards and timestamp reward."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written\nThe reward design is interesting"}, "weaknesses": {"value": "1. it is clear that adding temporal related reward will improve the temporal related tasks, it will be cool to test on non-temporal video tasks\n2. the novelty of the paper is only on the reward design for temporal tasks, everything else is standard \n3. the RL training require a lot of hyperparameter tuning and manual engineering (e.g. 900 steps where first 400 with timestamp reward and 500 without)"}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aGYL7cIlXE", "forum": "jimY7NF6aJ", "replyto": "jimY7NF6aJ", "signatures": ["ICLR.cc/2026/Conference/Submission5002/Reviewer_YPLo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5002/Reviewer_YPLo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761521702279, "cdate": 1761521702279, "tmdate": 1762917815320, "mdate": 1762917815320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MUSEG, a RL-based approach to enhance temporal understanding by introducing timestamp-aware multi-segment grounding. It designs a RL training recipe with phased rewards that progressively guides the model toward temporally grounded reasoning. The experimental results show superior performance over previous methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an important problem and the paper is clearly written. Motivation is clear.\n2. Evaluations on in- and cross-domain tasks show good results over previous methods."}, "weaknesses": {"value": "1. To me, the segment matching reward and timestamp reward are likely targeting the same thing. And the reward formats are duplicating. I am wondering the effect of each component. I think there should be an ablation study on how each type of rewards matters to the performance.\n2. Since there are several video temporal grounding tasks in the literature, it would be beneficial if authors also evaluate on Ego4D-NLQ, TaCoS, ANet-Captions, QVHighlights. \n3. The paper does not have sufficient ablation studies."}, "questions": {"value": "1. How do you handle long and short videos in this setting? \n2. How do you make sure the timestamps and video frames are aligned to each other? \n3. Could you report results with only phase 1 training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "y5zbe4GQGh", "forum": "jimY7NF6aJ", "replyto": "jimY7NF6aJ", "signatures": ["ICLR.cc/2026/Conference/Submission5002/Reviewer_ogyi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5002/Reviewer_ogyi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025500350, "cdate": 1762025500350, "tmdate": 1762917814889, "mdate": 1762917814889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}