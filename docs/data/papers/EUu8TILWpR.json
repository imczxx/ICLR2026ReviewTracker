{"id": "EUu8TILWpR", "number": 8412, "cdate": 1758082368083, "mdate": 1759897785791, "content": {"title": "Context Learning for Multi-Agent Discussion", "abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency—LLMs fail to reach a coherent solution—due to the misalignment between their individual contexts. In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism. It enables LLMs to avoid premature convergence on “majority noise” and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20\\%--50\\%, while enjoying favorable transferability and computational efficiency.", "tldr": "We introduce a new context learning method for multi-LLM systems that can continually adjust LLMs' context based on the current state of discussion, enabling agents to effectively collaborate.", "keywords": ["Large Language Models", "Context Learning", "Multi-agent discussion"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b23c5100c28b0471ff3373d96a9a53a2f6cca81f.pdf", "supplementary_material": "/attachment/28677f93634ef1adfeeb3940e8de405814fb1fb3.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the issue of discussion inconsistency in Multi-Agent Discussion (MAD) systems, where multiple Large Language Models (LLMs) collaborate through structured dialogue but often fail to reach a coherent consensus due to context misalignment among agents. To tackle this, this paper proposes M2CL (Multi-LLM Context Learning) that learns a context generator for each agent, enabling dynamic, round-by-round generation of optimized context instructions through automatic information organization and refinement. By introducing a self-adaptive mechanism that balances context coherence and output diversity, M2CL prevents premature convergence to “majority noise” and guides agents toward the correct consensus."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper clearly diagnoses discussion inconsistency，a core limitation of existing Multi-Agent Discussion systems to reach an agreement on a coherent solution.\n\n2.The paper provides analytical understanding of how initial and evolving contexts influence discussion dynamics, grounding the method in a clear theoretical framework rather than purely empirical design.\n\n3.Based on theoretical insight, the paper proposes M2CL framework learn dynamic, per-agent context generators that evolve during discussion.\n\n4.The paper used dual gradient descent to automatically balance coherence and diversity, which is crucial for avoiding premature consensus and enabling more robust reasoning convergence.\n\n5.Extensive experiments are conducted across three domains and nine datasets, covering LLM reasoning, embodied agentic, and mobile GUI AndroidWorld. The results demonstrate substantial and consistent performance improvements over previous MAD methods in terms of the number of collaborating LLMs, model size, and other aspects."}, "weaknesses": {"value": "1.Lines 45–47: I have some concerns about whether the conclusion “fail to reach an agreement on a coherent solution, easily making the collaborative decision dominated by noise rather than principled reasoning” can be fully supported by Fig. 2. On the one hand, whether a higher level of discrepancy might actually be beneficial in some multi-agent discussion scenarios. Diverse responses could help explore different reasoning paths. Would it be helpful to check whether successful cases show lower final discrepancy than failed ones, to better support the claim that reducing discrepancy improves consensus?\n\n2.Line 85: The paper states “A straightforward solution is to manually adapt instructions in contexts as the discussion progresses.” However, an arguably more intuitive approach would be to assign each LLM an additional discussion-summary LLM responsible for dynamically aggregating intermediate discussion results and generating updated context instructions. I am curious whether any prior work has adopted such a setup, and if so, what specific limitations those approaches faced compared with the proposed M2CL framework.\n\n3.The derivation of Eq. (19) may contain a potential issue: the missing first term in the inequality could influence whether the inequality still holds. It would be helpful for the authors to clarify whether omitting this term affects the mathematical validity of the inequality, and if not, to explain why it can be safely ignored. Otherwise, this omission may undermine the soundness of subsequent theoretical conclusions that rely on Eq. (19).\na−ac=i=1Nωi∗aCib−ac≤i=1Nωi∗aCib−i=1Nωi∗aCit+i=1Nωi∗aCit−ac\n\n4.While the paper presents impressive empirical results and a novel mechanism for context initialization in multi-LLM coordination, the theoretical reasoning connecting Eq. (9)–Eq. (10) seems to rely on several implicit assumptions. Eq. (9) ensures that f(a([A;P]))≈vp, and Eq. (10) ensures that F([Iib;P])≈f(a([Iib;P])). However, there is no theoretical guarantee that f(a([A;P]))≈i∈SωiF([Iib;P]) or f(a([A;P]))≈i∈Sωif(a([Iib;P])). \nTherefore, it remains unclear whether the proposed procedure can truly achieve the “orthogonal basis selection” objective. I have some reservations about this logical gap, even though the empirical evidence is strong."}, "questions": {"value": "Refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9I6JUJJyw8", "forum": "EUu8TILWpR", "replyto": "EUu8TILWpR", "signatures": ["ICLR.cc/2026/Conference/Submission8412/Reviewer_g8ch"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8412/Reviewer_g8ch"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788249425, "cdate": 1761788249425, "tmdate": 1762920313025, "mdate": 1762920313025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes M2CL, a multi-round context collaboration framework for multi-LLM systems (MAD) that balances diversity vs. consistency across agents: diverse initialization broadens solution coverage, while temporal consistency and dual-driven scheduling steer the system toward agreement. The method reports strong gains across multiple datasets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper addresses the fundamental and practically important diversity–consistency trade-off in multi-agent LLM collaboration.\n2. This work presents a principled methodology: it secures diversity by solving for orthogonal, sufficiently covering initial profiles, fosters inter-agent consistency via temporal cross-round coherence, and employs a dual-driven scheduling mechanism to dynamically steer the trade-off between the two.\n3. It demonstrates consistent performance gains across multiple datasets."}, "weaknesses": {"value": "1. Although a lightweight distilled $F(\\cdot)$ is used, the cost may still grow combinatorially as the profile pool expands. \n2. How the contextual pool is constructed, whether it is shared across tasks, and whether it can be aligned with baselines."}, "questions": {"value": "1. Could you provide the exact input–output specification and examples. What patterns does it learn—tone control, attention to specific evidence?\n2. Are generators trained per dataset or jointly? Is there cross-dataset generalization? Could you report train/val/test sizes?\n3. Will you release checkpoints, data, the initial context pool ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Df6B91ZYvi", "forum": "EUu8TILWpR", "replyto": "EUu8TILWpR", "signatures": ["ICLR.cc/2026/Conference/Submission8412/Reviewer_xVos"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8412/Reviewer_xVos"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816001049, "cdate": 1761816001049, "tmdate": 1762920312658, "mdate": 1762920312658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper starts from the observation that discussion inconsistency in MAD prevents conclusions from converging, and proposes M2CL, a multi-LLM context learning method that learns a context generator for each agent to dynamically produce context instructions at every discussion round. Grounded in theoretical analysis, the method posits that agents’ activations should be as orthogonal as possible during initialization, and that subsequent evolution should reduce inter-agent activation discrepancies while controlling deviation from the initial context. During the initialization, the paper selects a near-orthogonal set of initial instructions from a predefined pool of multi-perspective prompts, minimizing the weighted reconstruction error of the selected instructions’ activations. After that, the paper proposes a round-level context evolution objective: on one hand, constraining the distance between the current round’s context and the initial context; on the other, aligning activations by encouraging the current round’s instruction to be consistent with the previous round’s own output, thereby indirectly reducing inter-agent divergence. The objective is further reformulated as a constrained problem with a dual variable $\\alpha$, and alternating updates with approximate dual gradients are used to achieve adaptive trade-off. The paper evaluates M2CL across academic reasoning, embodied tasks, and mobile control scenarios , demonstrating its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is theoretically driven and structurally clear.  The paper formalizes how to preserve diversity while fostering consensus, alleviating discussion inconsistency and non-convergence in MAD.\n2. During the initialization, M2CL selects near-orthogonal instructions from a multi-perspective prompt pool.  The round-level update objective further combines a distance-to-initial constraint with activation alignment, and employs a dual variable $\\alpha$ to dynamically balance diversity and consistency.\n3. Thorough experiments on nine benchmarks spanning academic reasoning, embodied tasks, and mobile control, demonstrates the effectiveness  of the proposed method ."}, "weaknesses": {"value": "1. The paper uses output activation alignment as a proxy for discussion consistency and contribution. This proxy is not the task ground truth, there may exist cases where embeddings are close yet the logic still conflicts, or semantics are diverse yet complementary.\n2. In initialization, one must select a near-orthogonal subset of initial instructions from a multi-perspective prompt pool and minimize the weighted reconstruction error. If the pool lacks coverage or contains heavy semantic redundancy, it is difficult in practice to pick a subset that balances diversity and reconstructability.\n3. Implementing initialization requires the activation mapping $a(\\cdot)$, the projection $f(\\cdot)$ and the distilled projection $F(\\cdot)$. Access to answer-side activations are needed and the stability of projection training may limit the initilization quality."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper raises no ethical concerns."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4TqROs0nNS", "forum": "EUu8TILWpR", "replyto": "EUu8TILWpR", "signatures": ["ICLR.cc/2026/Conference/Submission8412/Reviewer_uL5G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8412/Reviewer_uL5G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927972189, "cdate": 1761927972189, "tmdate": 1762920312342, "mdate": 1762920312342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of discussion inconsistency in Multi-Agent Discussion (MAD) systems, where multiple LLM instances fail to reach coherent solutions due to context misalignment. The authors propose M2CL, a two-stage method that (1) initializes diverse, approximately orthogonal instruction contexts, and (2) evolves these contexts across discussion rounds using learned generators with a self-adaptive balancing mechanism. Extensive experiments across 9 benchmarks (MMLU, MATH, GPQA, embodied tasks, GUI control) demonstrate 20-50% performance improvements over existing MAD methods with minimal computational overhead (~10%)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Clear empirical demonstration (Fig 1-2) that fixed contexts cause discussion inconsistency, with concrete examples showing misalignment\n- The dual gradient descent approach (Eq 16) for automatically tuning α is elegant and well-justified through Lagrangian duality (Appendix E)\n- Testing across diverse domains (academic reasoning, embodied agents, mobile GUI) with multiple model sizes (7B to 72B parameters) demonstrates broad applicability\n- Consistent 20-50% improvements are substantial, especially on complex tasks (e.g., +33% on GPQA with Qwen-72B)"}, "weaknesses": {"value": "Unclear notations and theoretical motivation.\n- In its proof and many following lemmas, a constant term is usually ignored. In addition The constant term, however, grows exponentially with the quantity $\\rho^2$. Can this constant term be ignored?\n- Many concepts are introduced without context or formal introduction, making the paper extremely hard to read. For example,\n  - Section 4:\n      - the notation $P, X, I$ appeared in line 171-180 is understood generally as texts or sequences or tokens. But they can be magically applied in Eq (5) with weight matrices before any description. There is no clue from the authors about whether those $I, X, P$s are vectors or matrices and how they are calculated.  Furthermore, In the Theorem 4.1, the \"activations of the correct answer\" $a_c$ also comes from nowhere, readers still do now know whether this is a vector or a matrix. From Eq (5), a general guess is that $a(C^t_i)$ is a matrix of the same shape as $P$ because it is the output of attention layer, and it should be of the shape $[\\text{number of layers (might be ignored), length of sequence, dimension of latent space}]$, if we guess $P$ is a matrix, and so as $a_c$. In the up coming Eq(8), the role of $a_c$ can be magically replaced by a sentence vector $v_P$ (this is first place the word vector appeared besides the weight vector $\\omega$). Only now, people can guess $P, X, I$ are vectors.\n  - Section 5.1\n    - The notation $i$ in line 233 is confusing, before, it is about the i-th of N LLMs, but now it is for the predefined pool of M Initial contexts. This makes the Equation (7) very hard to understand. We do now know whether the authors wants to find a subset $S$ of what domain, of the pool? or of the LLM agents? or the tuple of (llm, context)?\n    - The notation of the braces is confusing, line 184 $[I_i^t, X_i^{t-1}, P]$ uses $,$ and line 196/237 $[I_i^t; X_i^{t-1}; P]$ uses $;$, are they the same?\n    - The sentence vector $v_P$ represents the vector of question P, is it the same as the activation of P?\n    - Is $a([A;P])$ in Eq (9) computed from the Eq (5)? Are the parameters $W_V$, $W_K$, $W_Q$ shareable across all occurrence of $a$?\n    - The loss function (10) is strange, why don't we just use the composition of functions $f$ and $a$ as $\\mathcal{F}$?\n    - Consider Eq (11), if functions are optimized by the loss functions in Eq (9-10), I didn't see why the entire formulation in Section 5.1 encourages orthogonality, because each $\\mathcal{F}( ... )$ approximate $f( ... )$ and approximate $v_P$. And the critical input contains all information of $P$ , I think ideally, the function $F$ can collapse to a sentence embedding function to generate $v_P$. It is just a knowledge distillation of the sentence embedding network.\n  - Section 5.2\n    - Eq (12) is poorly written. In Eq (12), it tries to select the proper index j to maximize the objective. In its remark, the instruction $I$ magically appeared to be optimized. What is the optimization problem you actually want to define?\n    - Eq (16) is strange, how do you choose $\\beta$? If $\\beta$ is sufficiently large, the $\\alpha_i$ will just decrease to zero and another part of loss will only keep the first term, seems to only approximate $X_i^{t-1}$ using $\\mathcal{G}_{\\theta_i}$."}, "questions": {"value": "My major concerns remains about the confusing methodology part. I would like to discuss with the authors about the notations above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q77N3DoBIu", "forum": "EUu8TILWpR", "replyto": "EUu8TILWpR", "signatures": ["ICLR.cc/2026/Conference/Submission8412/Reviewer_sNuJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8412/Reviewer_sNuJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762267669536, "cdate": 1762267669536, "tmdate": 1762920311915, "mdate": 1762920311915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}