{"id": "1j0ormf8uI", "number": 10078, "cdate": 1758160043338, "mdate": 1759897675622, "content": {"title": "Conformalized Survival Counterfactuals Prediction for General Right-Censored Data", "abstract": "This paper aims to develop a lower prediction bound (LPB) for survival time across different treatments in the general right-censored setting. Although previous methods have utilized conformal prediction to construct the LPB, their resulting prediction sets provide only probably approximately correct (PAC)–type miscoverage guarantees rather than exact ones. To address this problem, we propose a new calibration procedure under the potential outcome framework. Under the strong ignorability assumption, we propose a reweighting scheme that can transform the problem into a weighted conformal inference problem, allowing an LPB to be obtained via quantile regression with an exact miscoverage guarantee. Furthermore, our procedure is doubly robust against model misspecification. Empirical evaluations on synthetic and real-world clinical data demonstrate the validity and informativeness of our constructed LPBs, which indicate the potential of our analytical benchmark for comparing and selecting personalized treatments.", "tldr": "We propose a conformalized survival counterfactuals prediction method under the potential outcome framework with exact miscoverage guarantee.", "keywords": ["Conformalized survival analysis", "Counterfactual inference", "General right-censored data"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/74b3a742bb8a32f556c3ada928a93c1f471575d1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a novel approach to do conformalized survival counterfactuals prediction, which mainly has two advantages: 1) provide valid LPB for counterfactual survival outcomes for general right-censored data; 2) achieve the marginal coverage with the double robustness property instead of the PAC-type guarantee, which should be more reliable when rare and extreme cases are present.\n\nThe author proposes a new non-conformity score based on the quantile regression and use the probability $P(V(X, \\tilde{T})\\leq c_{1-\\alpha}(\\tau))$ with the help of weight function $w(X)$ to learn the cut-off value, which allows for inference on general censored data with exact coverage guarantee compared to the use of empirical version for $\\alpha(\\tau)$ ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. transform the problem into a weighted conformal inference problem with exact coverage for the entire population.\n2. LPB optimization to get the highest LPB with the coverage guarantee.\n3. doubly robust with theoretical guarantee if the weight function is estimated rather well."}, "weaknesses": {"value": "* The definition of $c_{1-\\alpha}^{(w)}(\\tau)$ appears to be incorrect, as the right-hand side should also depend on $w$. For example, it should be a function of $V^{(w)}(X, T\\leq c)$.\n\n* It would be beneficial to provide the selected $\\tau$ in the LPB optimization. I suspect that the optimal $\\tau$ should be close to $0.5$, as this would allow the majority of the data to be leveraged for a more accurate estimation of $q_{\\tau}(x)$.\n\n* If the extreme cases occur with very small probability, achieving exact marginal coverage should intuitively inflate the conformalized interval (i.e., result in a lower LPB). I did not observe this effect in Figure 1. Is this because the data generation process for the synthetic datasets does not produce too extreme values, or is it an artifact of the LPB optimization step? I believe an ablation study on the LPB optimization would be a valuable addition to the paper, as it would experimentally highlight the importance of achieving exact coverage."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hliGl9zIPa", "forum": "1j0ormf8uI", "replyto": "1j0ormf8uI", "signatures": ["ICLR.cc/2026/Conference/Submission10078/Reviewer_dYEk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10078/Reviewer_dYEk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761610534670, "cdate": 1761610534670, "tmdate": 1762921468434, "mdate": 1762921468434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to estimate the lower prediction bound for the survival time of different treatments under general right-censored data. For that, the authors leverage ideas from weighted conformal prediction in the survival setting under standard assumptions of the potential outcomes framework. The authors provide theory for coverage guarantees and a double robustness property of their framework. Lastly, they perform experiments on synthetic data to compare their method against baselines, and provide results on a real-world medical dataset to show the applicability of their method in practice."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Uncertainty quantification for counterfactual prediction of survival data is an important problem for real-world medical treatment settings and using conformal prediction seems to be an useful research direction here. \n- The authors provide results on real world data sets and give insights and evaluation of the plausibility of their estimates."}, "weaknesses": {"value": "Some parts of the theoretical contribution seem unclear to me and the comparison to prior related work could be improved (however, I do not have further background knowledge on conformal prediction for survival data).\n- Especially the contribution over Candes (2023) is not clear to me (see also Questions).\n- E.g., in ll. 182, the authors state, unlike previous methods, their method would achieve exact marginal coverage. However, Theorem 4.1 shows that the method only achieves marginal coverage when $\\hat{w}(x) = w(x)$ if I understand it correctly? Could the authors elaborate on this? \n\nThe experimental results section could be improved especially with respect to clarity and presentation.\n- At least a short summary of the used datasets and a bit more details should be included in the main paper as this is relevant to check the robustness of their method (simple vs complex setup) and practical relevance (real-world applicability, and realistic setup?).\n- the baselines should be cited and described shortly again in the experiments to give intuition about their potential shortcomings.\n- The used metrics (empirical coverage, relative LPB) should be at least once defined formally. \n- The presentation of the results could be improved. In Figure 1, especially the lower row is not easy to interpret. The authors state in the caption of Fig. 1, a higher LPB would be better but write in the text uncab (with highest LPB) would produce a conservative LPB estimate, this seems inconsistent to me. I guess, overall, the authors would like to display a tradeoff between valid empirical coverage and less conservative/tighter LBP estimates? One possible way to show this would be to display both in one graph with coverage and LPB on the axis and show this for over different values for $\\alpha$ to show that their method dominates the others?\nNo code is provided for reproducibility.\n\nMinor:\n- citation style needs to be adjusted (direct vs indirect citations)\n- In the introduction the double robustness property is mentioned. It should be summarized once in the beginning what this means and double robustness with respect to which nuisance functions is meant."}, "questions": {"value": "- The authors mention “Candes et al. (2023) also provided survival counterfactuals prediction, but only used on cases with censoring time exceeding a specific threshold.” Could the authors elaborate on the difference to that and what the implications of this shortcoming are? That would help to stress their contribution more clearly.\n- For the real-world experiments, how can the empirical coverage given the different treatment strategies be evaluated without access to ground-truth counterfactuals/survival times?\n- For the real-world experiments, how do the results and findings of the authors’ method deviate or coincide with the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mLupXNjtnr", "forum": "1j0ormf8uI", "replyto": "1j0ormf8uI", "signatures": ["ICLR.cc/2026/Conference/Submission10078/Reviewer_xp44"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10078/Reviewer_xp44"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861995416, "cdate": 1761861995416, "tmdate": 1762921468201, "mdate": 1762921468201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to give lower prediction bounds (LPBs) for counterfactual survival time under different treatments with general right-censoring. While prior conformal survival methods gave PAC-type guarantees; this work targets marginal population coverage for counterfactual survival times, under the strong ignorability assumption.\n\nThe method adopts the weighted conformal prediction framework. Experiments on synthetic and a 541-patient NSCLC lung-cancer data show near-nominal coverage and informative bounds. \nTheory gives a finite-sample lower bound on marginal coverage that degrades with density-ratio error, and a doubly robust asymptotic result if either the weight estimator or the quantile estimator is consistent."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Important problem: The paper tackles a meaningful and practical challenge—making counterfactual survival time prediction reliable when data are right-censored. This is essential for treatment-effect estimation in medicine, where censoring is unavoidable.\n\n2. Solid theoretical foundation: The authors derive finite-sample marginal coverage guarantees and show that coverage error depends only on the estimation error of a single density ratio. \n\n3. Consistent empirical performance: The method performs reliably on six diverse synthetic setups and on a real high-dimensional clinical dataset, showing both strong coverage and clinically plausible counterfactual predictions."}, "weaknesses": {"value": "1. Coverage depends on ratio accuracy: The claim of “exact” coverage only holds when the density ratio is estimated perfectly. Theorem 4.1 actually provides a lower bound that subtracts an error term based on the ratio estimation error, so the language precision may be worth reviewing\n\n2. Limiting assumptions: The method assumes strong ignorability, meaning the potential outcomes are independent of both treatment and censoring once covariates are conditioned on. This can be a demanding assumption in practice.\n\n3. Narrow empirical validation: Experiments only test a single coverage level ($\\alpha=0.1$). Broader evaluation would make the empirical evidence more convincing."}, "questions": {"value": "1. Could you please provide some results for for other values of $\\alpha$ to show consistency of the performance?\n\n2. Compare to previous conformal survival works, is it true that one reason the theory successfully avoids the PAC-style guarantee is because of the survival counterfactuals setting. Could you please provide some intuition for why this joint setting enables that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pPkcIvLFvo", "forum": "1j0ormf8uI", "replyto": "1j0ormf8uI", "signatures": ["ICLR.cc/2026/Conference/Submission10078/Reviewer_Mprf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10078/Reviewer_Mprf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954499048, "cdate": 1761954499048, "tmdate": 1762921467963, "mdate": 1762921467963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed CSC method develops a weighted conformal calibration procedure under the potential outcome framework, producing exact marginal coverage and a doubly robust property against model misspecification. Theoretically, the authors derive formal coverage guarantees and empirically validate the approach on synthetic simulations and a real lung cancer dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors provide formal proofs for marginal validity and robustness.\n2. Introducing conformal calibration for counterfactual survival with exact coverage is a meaningful advance.\n3. Enhances credibility and robustness under imperfect model estimation."}, "weaknesses": {"value": "1. The experiments, though well organized, are limited to one real-world dataset (a single lung cancer cohort). This constrains the generalizability of the findings. Additional datasets from other clinical domains or public survival benchmarks (e.g., SUPPORT, METABRIC, MIMIC, SEER) would strengthen the empirical evidence.\nMoreover, ablation studies isolating the contributions of key components—such as the choice of quantile regression model (MLP vs. CQR forest), weighting scheme, and calibration size—are missing. Such ablations would help quantify where performance gains arise.\n\n2. The proposed method introduces several estimation layers: quantile regression, density ratio estimation ($\\hat{\\gamma}(x)$), and weighted conformal calibration. These can be computationally intensive and may suffer from instability when sample sizes are small or censoring rates are high. The paper does not analyze runtime complexity, convergence stability, or the effect of hyperparameter choices. In large-scale or high-dimensional settings, density ratio estimation can be error-prone and potentially undermine the theoretical guarantees if $\\hat{\\omega} (x)$ is poorly estimated.\n\n3. The paper mainly compares against a few conformal survival calibration baselines (uncalibrated, naive, focused, fused). However, it omits other relevant conformal-based approaches that are published in recent years. Inclusion of such baselines would clarify whether CSC’s advantages stem from conformalization or from model choice.\nAdditionally, it would be useful to report not only coverage and LPB but also metrics like mean absolute deviation, average interval width, and computational cost for a fuller comparison."}, "questions": {"value": "1. Could the authors provide runtime comparisons with existing conformal survival approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9rTtLWEzeZ", "forum": "1j0ormf8uI", "replyto": "1j0ormf8uI", "signatures": ["ICLR.cc/2026/Conference/Submission10078/Reviewer_rUA7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10078/Reviewer_rUA7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989103751, "cdate": 1761989103751, "tmdate": 1762921467743, "mdate": 1762921467743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new method to provide lower prediction bounds for counterfactual outcomes of binary treatments in right-censored survival data based on conformal prediction. In contrast to the former work, which only provides approximate guarantees, the proposed method provides exact marginal coverage guarantees. The core idea of the method is based on a transformation of the coverage probability into a reweighted expectation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an interesting and important topic: Finite sample uncertainty quantification in counterfactual prediction in time-to-event data.\n- The paper provides rigorous mathematical guarantees and derivations."}, "weaknesses": {"value": "- The paper lacks a proper related work discussion on conformal prediction for causal effects.\n- In the motivation, the paper states that it provides bounds for \"different treatments\". However, later on, the treatment is considered binary, rendering the former an overstatement.\n- The paper combines existing work in a straightforward manner. I do not see much novelty but rather a direct application of Lei and Candès (2021) and Candès et al (2023). \n- The work contains many wrong statements (potentially due to a last-minute submission). For example, line 188 states that the upper bound exactly equals alpha. This is not true. In line 74, the paper (presumably) confuses upper and lower bounds.\n- The paper requires the weight function to be well estimated. If it is not, the coverage guarantee could even be 0. This is not useful in practice.\n- The experimental evaluation is insufficient to assess the benefit of the proposed method over, e.g., the fused method by Davidov et al. (2025). Statements made on the comparison are not necessarily true. A significantly larger lower bound is not observable in the stated settings 3,4,5. \n- The paper is not yet ready for publication. Besides an often unstructured presentation with confusing mistakes and unexplained mathematical notation, this can be seen in the incorrect specification of the references, e.g., journal name \"find this article online\" or incomplete conference proceedings \"proceedings of the... conference.\""}, "questions": {"value": "- Lines 141 following: what is $\\tau$? The notation has not been introduced.\n- Counterfactual quantile regression (lines 168 following): What exactly is the relation between $\\tau$ and $\\alpha$?\n- Double robustness provides asymptotic guarantees in contrast to the finite-sample CP guarantees. Why does it make sense to assess the double robustness of the CP interval? What does it mean in practice? Which value does Theorem 4.2 provide for the CP intervals?\n- Experiments: how does the variance of the coverage and the lower bound compare to other methods? This is not observable from the current evaluation. How does the method perform over different levels of alpha? In comparison to the fused method, one can observe, on average, a larger variance in coverage and a lower bound. This does not support the effectiveness of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hep3upQQnz", "forum": "1j0ormf8uI", "replyto": "1j0ormf8uI", "signatures": ["ICLR.cc/2026/Conference/Submission10078/Reviewer_2PvE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10078/Reviewer_2PvE"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762352913542, "cdate": 1762352913542, "tmdate": 1762921467426, "mdate": 1762921467426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}