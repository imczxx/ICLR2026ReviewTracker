{"id": "1ifQzlETeG", "number": 9526, "cdate": 1758126134178, "mdate": 1759897714385, "content": {"title": "MindMix: A Multimodal Foundation Model for Auditory Perception Decoding via Deep Neural-Acoustic Alignment", "abstract": "Decoding complex auditory experiences from non-invasive EEG is a rapidly emerging field that holds significant promise for advancing both fundamental neuroscience and human-machine interaction technologies. Recent developments in EEG foundation models have yielded powerful neural representations that are promising for auditory decoding. However, the effectiveness of these models remains fundamentally constrained by their limited integration with acoustic stimulus information. Specifically, the lack of deep coupling between neural signals and auditory inputs hampers the models’ ability to generalize effectively across diverse auditory tasks. To bridge this gap, we introduce MindMix, the first multimodal foundation model specifically designed to learn deeply aligned neural-acoustic representations for supporting auditory decoding. MindMix employs a two-stage training strategy: first, a high-capacity EEG encoder is pre-trained on over 3,000 hours of EEG data to learn generalized EEG features that can transfer across tasks and subjects. Second, the model learns the neural-acoustic mapping using over 100 hours of paired data, facilitated by our novel Cross-Attention Low-Rank Alignment module, which facilitates fine-grained, cross-modal information integration. Experimental results demonstrate that MindMix substantially surpassing existing baselines across a range of auditory decoding tasks, including auditory attention decoding, auditory emotion recognition, and cross-modal retrieval. This work thus establishes a foundation for future research in multimodal brain decoding and auditory brain-computer interfaces. Our code is available at https://anonymous.4open.science/r/MindMix-654B/.", "tldr": "", "keywords": ["Electroencephalogram; Audio; Multimodal foundation model; Auditory decoding"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7395a273567c5a6746c9a0f290984a894d6d364d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes **MindMix**, a multimodal foundation model designed to achieve deep EEG–audio alignment for auditory perception decoding. The approach follows a three-stage training pipeline: (1) large-scale unimodal EEG pretraining, (2) neural–acoustic alignment using the proposed Cross-Attention Low-Rank Alignment (CALRA) module, and (3) fine-tuning on downstream auditory tasks including AAD, emotion recognition, and music retrieval. Experimental results show strong improvements over prior works."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper addresses a clear limitation in current EEG pretraining, which lack of grounding with auditory stimuli, and proposes a principled cross-modal alignment strategy.\n\n2) MindMix achieves substantial gains across diverse tasks and datasets, signaling strong generalization capabilities.\n\n3) The component-wise analysis provides reasonably convincing evidence for the effectiveness of CALRA and the training pipeline.\n\n4) Multiple datasets, tasks, and baseline categories are included, strengthening the validity of the comparison."}, "weaknesses": {"value": "1. The paper does not provide **parameter counts** for different model components (EEG encoder, CALRA, stage-wise variations). Without reporting efficiency and compute scaling trends, claims of “foundation model” scalability feel incomplete.\n\n2. The **three-stage pipeline** is resource-intensive, yet key details are missing (Pretraining duration GPU hours / computational cost Comparison to baselines under equal compute).\n\n3. The architecture appears **highly similar to LaBraM** (masked token prediction + quantized neural tokens). What specifically differentiates this encoder from LaBraM beyond being trained from scratch?\n\n4. The improvement (+ ~16% over strong baselines) is unusually large given DTU’s difficulty. Whether **within-trial only** evaluation inflates performance Need **cross-trial** and **cross-subject** results for consistency with prior work\n\n5. CALRA uses shared low-rank fusion. but no comparison to multimodal LoRA or other parameter-efficient alignment adapters is provided  (especially since LoRA is widely adopted in multimodal foundation models)"}, "questions": {"value": "1. Please provide **detailed parameter counts** for each major module and stage. How does model size relate to performance? Any preliminary scaling analysis?\n\n2.  Key details are missing (Pretraining duration GPU hours / computational cost Comparison to baselines under equal compute).\n\n3. Can you clarify **why training a LaBraM-style encoder from scratch** is better than using existing pretrained EEG encoders? What inductive bias or architectural novelty is gained?\n\n4. Can you provide **cross-trial** / **cross-subject** AAD performance on KUL/DTU/ESAA? The current within-trial only protocol does not sufficiently measure practical generalization.\n\n5. Have you compared CALRA with **multimodal LoRA** as an alignment bottleneck? Is there a reason LoRA-style low-rank adaptation was not considered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kzIRWWkggR", "forum": "1ifQzlETeG", "replyto": "1ifQzlETeG", "signatures": ["ICLR.cc/2026/Conference/Submission9526/Reviewer_NCy7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9526/Reviewer_NCy7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711390327, "cdate": 1761711390327, "tmdate": 1762921091758, "mdate": 1762921091758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MindMix, the first multimodal foundation model specifically designed for auditory perception decoding from non-invasive EEG signals through deep neural-acoustic alignment. The work addresses a critical limitation of existing EEG foundation models (such as EEGPT, LaBraM, and CBraMod), which are pre-trained exclusively on EEG signals without exposure to auditory stimuli, resulting in representations that are not optimized to align with acoustic structures and thus perform poorly on auditory decoding tasks. MindMix employs a two-stage training strategy: first, a high-capacity EEG encoder is pre-trained on over 3,500 hours of diverse EEG data using a novel multi-task self-supervised objective that combines masked token prediction and Fourier spectrum reconstruction, enabling the model to learn robust neural representations that can transfer across tasks and subjects. The encoder uses a heterogeneous patching strategy that integrates temporal and spatial encodings to handle varied electrode layouts, and quantizes patch embeddings into discrete neural tokens via a shared codebook for the masked prediction task. Second, the model learns neural-acoustic mapping on over 100 hours of paired EEG-audio data through a novel Cross-Attention Low-Rank Alignment (CALRA) module, which facilitates fine-grained, token-level cross-modal information integration. CALRA consists of three key components: a type-specific aligner that routes projections through learnable transformations conditioned on auditory type labels (speech vs. music), bi-directional cross-attention that enables each modality to retrieve complementary information from the other, and a shared low-rank alignment mechanism that enforces semantic consistency by projecting both modalities into a compact shared space, modeling their interaction via element-wise product, and integrating feedback through residual connections. The entire framework is optimized end-to-end via a symmetric contrastive learning objective (InfoNCE loss) that maximizes cosine similarity between true EEG-audio pairs while minimizing it for non-corresponding pairs.\n\nThe experimental evaluation demonstrates substantial improvements over existing methods across diverse auditory decoding tasks. MindMix is evaluated on six downstream tasks using strict subject-independent protocols with 70%/10%/20% train/validation/test splits: auditory attention decoding (AAD) on three datasets (KUL, DTU, ESAA), emotion analysis on two datasets (PME4, HR-EEG4EMO), and cross-modal music retrieval on MAD-EEG. Results show near-perfect performance on speech AAD (99.82% balanced accuracy on KUL, 99.93% on DTU, 100% on ESAA), dramatically outperforming both task-specific state-of-the-art models like DARNet (94.81% on KUL) and unimodal EEG foundation models like LaBraM (63.30% on KUL) and CBraMod (68.42% on KUL). On emotion analysis, MindMix achieves 72.56% on PME4 and 88.78% on HR-EEG4EMO, showing improvements of over 10 percentage points compared to the best baselines. Comprehensive ablation studies reveal that each component contributes significantly: removing CALRA entirely causes performance drops up to 4.58% in AAD accuracy, replacing the custom EEG encoder with LaBraM reduces performance by 2.38%, and within CALRA, removing cross-attention has the largest impact (5.58% drop), followed by shared low-rank alignment (2.40% drop) and type-specific aligner (1.29% drop). A critical analysis comparing the full model against its EEG-only counterpart quantifies a massive \"synergy gap\" ranging from 11.26% on ESAA to 36.32% on DTU, demonstrating that the performance gains stem from learning the deep relationship between neural signals and auditory stimuli rather than simply having better EEG representations. Qualitative visualization using pseudo-reconstruction of Mel spectrograms shows that MindMix achieves substantially higher Pearson correlation coefficients (0.88 on DTU, 0.91 on KUL) compared to the LaBraM variant (0.72, 0.85) and baseline methods (0.67, 0.61), with visual inspection confirming that MindMix faithfully captures fine-grained harmonic structures that are blurred or lost in other methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents a well-motivated and technically sound approach to auditory perception decoding from EEG signals. The core contribution—the CALRA module enabling deep token-level neural-acoustic alignment—represents a genuine architectural innovation beyond simple projection-based fusion methods. The experimental design is rigorous, employing strict subject-independent evaluation protocols across six diverse downstream tasks, and the results are compelling, with near-perfect AAD performance and consistent improvements of 10+ percentage points over strong baselines. The ablation studies systematically validate each component's contribution, while the quantified \"synergy gap\" analysis (11-36% performance gain) effectively demonstrates that gains stem from multimodal alignment rather than merely improved unimodal representations. The paper maintains appropriate scientific rigor with transparent limitations regarding data scarcity, clear implementation details supporting reproducibility, and thoughtful architectural choices such as the two-stage training strategy that pragmatically addresses limited paired data availability."}, "weaknesses": {"value": "The paper's experimental evaluation, while showing impressive performance gains, raises several methodological concerns that limit confidence in the generalizability of the findings. First, the near-perfect accuracy achieved on speech AAD tasks (99.82%-100%) is unusually high and deviates dramatically from established performance ranges in the literature, suggesting potential data leakage or overfitting issues despite the claimed subject-independent protocol. The authors do not provide sufficient detail about how subject independence was strictly enforced across the three training stages, and it remains unclear whether any subjects appearing in the downstream test sets were also present in the 100+ hours of multimodal alignment training data, which could artificially inflate performance. Second, the baseline comparisons appear somewhat unfair: unimodal foundation models like LaBraM and EEGPT are evaluated directly on auditory tasks for which they were never designed, while MindMix benefits from explicit auditory-specific pretraining. A more appropriate comparison would include these baselines after additional fine-tuning on auditory data, or alternatively, evaluate MindMix on non-auditory tasks where it received no specialized pretraining. The paper also lacks critical analyses such as cross-dataset generalization (training on one AAD dataset and testing on another without fine-tuning), performance degradation curves with varying amounts of training data, and computational cost comparisons, all of which are essential for assessing practical deployment feasibility. Additionally, the massive performance gap between MindMix and baselines raises questions about whether the improvements stem primarily from the proposed architecture or simply from having access to substantially more paired training data during the alignment stage.\nBeyond experimental concerns, the paper suffers from limited novelty in its core technical contributions and insufficient contextualization within the broader neuroscience literature. The CALRA module, while effective, essentially combines standard architectural components (cross-attention mechanisms and low-rank factorization) that are well-established in multimodal learning literature, with the primary novelty being their specific arrangement and the addition of type-specific routing. The paper does not adequately justify why this particular architecture is theoretically well-suited for EEG-audio alignment compared to alternatives, nor does it provide ablations comparing against other fusion strategies beyond simple co-attention. More critically, the work lacks neuroscientific grounding: there is no discussion of which brain regions or neural signatures the model learns to leverage, no analysis of temporal alignment between EEG and audio features (whether the model captures known neural lags in auditory processing), and no interpretation of what the learned embeddings represent in terms of underlying neural computations. The visualization of Mel spectrogram reconstructions is superficial and does not demonstrate that the model has learned physiologically plausible or neuroscientifically meaningful representations. The framing as \"the first multimodal foundation model\" for auditory decoding also overstates the contribution, as the distinction from existing multimodal EEG-audio methods appears to be primarily one of scale rather than fundamental approach. Yet the paper provides no systematic comparison of model scale effects, no evidence of zero-shot transfer capabilities to genuinely new auditory tasks, and no analysis of what makes this a \"foundation\" model beyond simply being larger and trained on more data."}, "questions": {"value": "Please see your weaknesses and I will adjust the final score based on your answers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "S2Zf7P7Ftl", "forum": "1ifQzlETeG", "replyto": "1ifQzlETeG", "signatures": ["ICLR.cc/2026/Conference/Submission9526/Reviewer_bKp3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9526/Reviewer_bKp3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748271021, "cdate": 1761748271021, "tmdate": 1762921091528, "mdate": 1762921091528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MindMix is a multimodal foundation model that learns a shared space between EEG and audio. It trains in stages: first, a large EEG encoder is pretrained on diverse, unlabeled EEG to capture robust temporal, spectral, and spatial patterns; next, paired EEG–audio data are used to align the two modalities with CALRA, a block that mixes type-aware routing, bidirectional cross-attention, and a shared low-rank fusion bottleneck under a contrastive objective; finally, the aligned model is fine-tuned for specific tasks.\n\nOn benchmarks, MindMix reports near-perfect speech auditory attention decoding and strong gains on affective recognition and music retrieval compared with task-specific and unimodal EEG baselines. Ablations indicate that both cross-modal alignment and the low-rank fusion are essential, and performance improves as pretraining and alignment data grow. The key contributions are a unified EEG–audio representation learning pipeline, the CALRA module for efficient coupling, and evidence that a single pretrained model can transfer across multiple auditory cognitive tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "MindMix offers a clear two-stage recipe: large-scale unimodal EEG pretraining to learn general neural patterns, followed by EEG–audio alignment with the CALRA block. CALRA’s type-aware routing plus bidirectional cross-attention through a shared low-rank bottleneck is a thoughtful way to couple modalities while keeping capacity focused.\n\nQuality:\nThe empirical study spans three task families (speech auditory attention decoding, emotion recognition, music identification) and shows consistent gains over both task-specific and unimodal EEG baselines. Ablations replace the EEG encoder and systematically modify CALRA, and the resulting performance shifts align with expectations, which supports the design choices.\n\nClarity:\nThe training pipeline is easy to follow: pretrain the EEG encoder on broad corpora, then align EEG and audio with CALRA, then fine-tune for tasks. The role of each CALRA component is explained through targeted ablations, which helps readers map architecture pieces to observed effects.\n\nSignificance:\nBy learning a shared EEG–audio space that transfers across multiple auditory cognition tasks, the work pushes toward a reusable foundation for brain–audio applications. The parameter-efficient cross-modal coupling suggests practical value for future BCI systems where compute and data are constrained."}, "weaknesses": {"value": "1 AAD results: Near-perfect scores (0.998–1.000) are not plausible without airtight leakage controls.\n\n2: The paper lacks concrete safeguards against content leakage (same story/music across splits), window overlap across splits, and other split-level confounds.\n\n3 Representation inconsistency: Section 3.2 pools to a single vector, while Section 3.3 treats those embeddings as sequences for cross-attention, leaving CALRA under-specified.\n\n4 The text claims a symmetric contrastive objective, but the equations show only EEG→audio.\n\n5 The type-specific aligner assumes access to stimulus type k at test time, which may leak dataset priors when audio is unavailable.\n\n6  Artifact removal (EOG/EMG), filtering, bad-channel policy, and whether z-score stats are fit only on training data remain unclear.\n\n7 Using 2-s windows with negatives drawn from the same recording can inflate performance; the negative sampling policy is unspecified."}, "questions": {"value": "1 Can authors provide runnable split manifests with fixed seeds that are subject-disjoint and content-disjoint, guarantee no temporal overlap across splits, and rerun AAD under this protocol? Include shuffled-stimulus and shuffled-subject controls. If results stay near perfect, pls show an error analysis.\n\n2 Please resolve the pooling-vs-sequence contradiction. Specify the tokenization for EEG and audio, sequence lengths, and the exact tensor shapes that CALRA consumes and emits.\n\n3 could authors define the negative policy? Ensure negatives come from different subjects and different stories. Report sensitivity to 1 s, 2 s, and 5 s windows, and report both raw window-level metrics and any aggregated or majority-vote metrics.\n\n4 When audio is unavailable, how is the type label k obtained? Report performance with known k, with a learned EEG-only predictor for k, and with no type routing. This will clarify reliance on dataset priors.\n\n5 could authors release a minimal repo with preprocessing scripts, split files, training configs, and eval scripts that reproduce a subset of the tables. Fix table typos, add statistical tests and confidence intervals, and report parameters, FLOPs, and inference latency alongside key baselines."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "Your study uses existing human EEG datasets, so the Ethics section should explicitly say so, cite IRBs and consent, and explain de-identification, data access controls, and how you mitigate re-identification risk."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RaNTMm0HyP", "forum": "1ifQzlETeG", "replyto": "1ifQzlETeG", "signatures": ["ICLR.cc/2026/Conference/Submission9526/Reviewer_gpHw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9526/Reviewer_gpHw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924214308, "cdate": 1761924214308, "tmdate": 1762921091095, "mdate": 1762921091095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MindMix, a multimodal EEG–audio foundation model designed to decode auditory perception via deep neural-acoustic alignment. The key architectural innovation is the Cross-Attention Low-Rank Alignment (CALRA) module, which enables fine-grained token-level interactions between EEG and audio embeddings. The training follows a two-stage process: (1) large-scale unimodal EEG pretraining (3,500+ hours) using masked token prediction and spectral reconstruction objectives, and (2) contrastive multimodal pretraining (100+ hours of EEG–audio pairs) using CLIP-style alignment. Experiments show strikingly high performance (up to 0.999 accuracy) on several auditory decoding benchmarks, reportedly surpassing both task-specific and unimodal EEG foundation model baselines.\n\nHowever, **empirical results are too strong to be credible without verification** (code link does not work). Combined with presentation ambiguities and overlapping ideas with prior cross-attention-based alignment methods, it is difficult to assess soundness and novelty with confidence.\n\nNevertheless, the direction is highly promising, and if the authors can:\n\n1. Provide working, verifiable code and confirm the data split strategy,\n2. Clarify the architectural details and novelty of CALRA, \n3. Justify the plausibility of the near-perfect accuracies, and \n4. Answer the questions regarding proper comparison with prior models,\n\nthen this work could become a significant contribution to multimodal brain decoding.\n\nAt the current stage, I remain cautiously skeptical—the ideas are intriguing, but the evidence and clarity are insufficient for confident acceptance. However, if the concerns outlined in the Weakness section are resolved, I am willing to raise the score."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Proposes a new **two-stage multimodal foundation model** framework for EEG–audio alignment.\n- Introduces a novel **CALRA module** that combines type-specific alignment, bidirectional cross-attention, and shared low-rank bottlenecks.\n- Achieves very high performance in auditory tasks."}, "weaknesses": {"value": "- **Reproducibility / Verification, with Scores maybe too good to be true**:\n    \n    The code link currently does not work. Reported accuracies are extremely high (approaching (>0.999) or reaching **1.0** with zero standard deviation on noisy, cross-subject EEG data), which raises doubts about correctness or possible data leakage (e.g., temporal overlap between training/test). Access to working code is essential to verify claims. Also please describe in more detail the data-split procedure in the Appendix if possible (beyond the “strict subject independent evaluation protocol with 70%, 10%, 20% split …”)\n    \n- **Architectural description clarity**:\n    \n    The description of the EEG-only pretraining (Sec. 3.2) is confusing:\n    \n    - Eq. (1) defines $E_{\\text{patch}} = X + T + E$, but Fig. 2 seems to show that quantization happens before addition (i.e. $E_{\\text{patch}} = \\text{quantizer}(X) + T + E$)—please clarify the correct order.\n    - Please clarify the “unique pretraining methodology” (line 153) of yours relative to **LaBraM** (which also uses masked patch prediction and spectral reconstruction). What, specifically, is new about this pretraining? Also please cite the relevant papers that motivated your architecture choice (e.g., LaBraM motivated your tokenizer usage?)\n    - Dimensional consistency is unclear: $S\\in C\\times T$ implies $E_{\\text{patch}}\\in C\\times N\\times D$ ($N$ being the number of temporal patches), yet $E’_{\\text{proj}}\\in N\\times D$ (lines 204–206). How is the channel dimension aggregated? (It seems $f_k$ is used? Can you elaborate on what exactly this is?) Also how does it handle varying number of channels during training?\n- **Missing implementation details**:\n    \n    Important training configurations (number of layers, embedding dimension, optimizer hyperparameters, fine-tuning LR, rank for CALRA, etc.) are missing. \n    \n- **Novelty concerns for CALRA**:\n    \n    I am not an expert in CLIP style training, but it seems the CALRA module resembles prior cross-attention alignment mechanisms used in multimodal CLIP-style works (e.g., *CARZero: Cross-Attention Alignment for Radiology Zero-Shot Classification*, *Multi-Granularity Cross-Modal Alignment*). Please differentiate CALRA from these or acknowledge conceptual overlap, and add a related works for CLIP modality alignment especially using CALRA like modules.\n    \n- **Justification and motivation**:\n    \n    Beyond performance gains, please explain in more detail *why* CALRA works with proper references to previous CLIP works if possible—e.g., what failure modes of simpler co-attention or projection methods it solves. In its current form, the architecture seems to have come out of nowhere with no context explained.\n    \n- **Presentation issues**:\n    - Define T and E in Eq. (1) explicitly (positional vs. channel embeddings? Is it cosine positional embedding?).\n    - Clarify Eq. (10): what is E_i? E_{\\text{aligned}} or E_{\\text{proj}}?\n    - Combine results from Table 2 and Figure 4 for easier comparison (currently scattered).\n    - Explain what “token-level interaction between modalities” (line 134) precisely means.\n    - In Table 3, “Ablation on EEG Encoder”, when the author mentions “w/ LaBraM”, does this mean using the pretrained weights of LaBRaM or using their architecture only initialized from scratch?\n- Miscellaneous but important points :\n    - The author claims in line 351 that “… these large models are often highly sensitive to the data format and preprocessing pipelines..”, but do not provide any reference or supporting experimental evidence, and whether their model is superior.\n    - The author **claims in line 417 that their EEG-only counterpart often outperforms SOTA**. However, when comparing Table 2 with Figure 4 we find that **CBraMod is beaten by the EEG-only model only in the ESAA task**. Please clarify this discrepancy.\n- Proper comparison with previous models.\n    - The model uses data that was preprocessed using a bandpass filter of **1–40 Hz**, whereas baselines you used such as LaBraM was pretrained on data bandpassed at **0.1–70 Hz**. Were preprocessing pipelines harmonized across comparisons? (Or preprocessing set differently based on the basemodel used?) **Differences here can cause distribution shifts that unfairly penalize other pretrained models.**\n    - The author claims in line 417 that their EEG-only counterpart often outperforms SOTA. It might beneficial to verify this with tasks other than auditory tasks. (More general tasks used in the previous models’ papers)\n    - Given that CBraMod still outperforms your EEG-only model (as pointed out by the \n    ”Miscellaneous but important points” I wrote above), have you tried CLIP-style pretraining on top of CBraMod to isolate CALRA’s contribution?"}, "questions": {"value": "All the questions are listed in the Weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MlANQGvPTu", "forum": "1ifQzlETeG", "replyto": "1ifQzlETeG", "signatures": ["ICLR.cc/2026/Conference/Submission9526/Reviewer_FBHr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9526/Reviewer_FBHr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981832904, "cdate": 1761981832904, "tmdate": 1762921090811, "mdate": 1762921090811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}