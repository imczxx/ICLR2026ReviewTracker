{"id": "PVarNsrX2v", "number": 15768, "cdate": 1758255065324, "mdate": 1759897283571, "content": {"title": "Red Teaming Program Repair Agents: When Correct Patches can Hide Vulnerabilities", "abstract": "LLM-based agents are increasingly deployed for software maintenance tasks such as automated program repair (APR). APR agents automatically fetch GitHub issues and use backend LLMs to generate patches that fix the reported bugs. However, existing work primarily focuses on the functional correctness of APR-generated patches—whether they pass hidden or regression tests—while largely ignoring\npotential security risks. Given the openness of platforms like GitHub, where any user can raise issues and participate in discussions, an important question arises: Can an adversarial user submit a valid issue on GitHub that misleads an LLM-based agent into generating a functionally correct but vulnerable patch? To answer this question, we propose SWExploit, which generates adversarial issue statements\ndesigned to make APR agents produce patches that are functionally correct yet vulnerable. SWExploit operates in three main steps: (1) Program analysis to identify potential injection points for vulnerable payloads. (2) Adversarial issue generation to provide misleading reproduction and error information while preserving the original issue semantics. (3) Iterative refinement of the adversarial issue statements\nbased on the outputs of the APR agents. Empirical evaluation on three agent pipelines and five backend LLMs shows that SWExploit can produce patches that are both functionally correct and vulnerable (the attack success rate on the correct patch could reach 0.91, whereas the baseline ASRs are all below 0.20). Based on our evaluation, we are the first to challenge the traditional assumption that a patch passing all tests is inherently reliable and secure, highlighting critical limitations in the current evaluation paradigm for APR agents. Our code is available at GitHub", "tldr": "We propose the first red-teaming work to test automatic program repair agents to generate functional correct but vulnerable patches.", "keywords": ["Software Engineering Agents", "Large Language Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88abd691204b7e48047fda132223b5974137bf19.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SWExploit, an agent that generates adversarial issue statements designed to make automated program repair (APR) agents produce patches that are functionally correct but contain security vulnerabilities. This task is novel and important, and this work is timely."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The research problem is novel and important: an attacker can craft issue reports that mislead APR agents into generating functionally correct patches that nevertheless introduce vulnerabilities.\n\n2. The proposed ideas—“trigger vulnerable code only with specific malicious inputs”, \"fake traceback entries\", and \"fake reproduce code\"—are creative and intriguing. These mechanisms are well-motivated and relevant to real-world attack scenarios."}, "weaknesses": {"value": "1. Line 254: possible typo — “four main steps” should likely be “three main steps”.\n\n2. Line 318: The paper mentions a baseline named Auto-Red but cites RedCodeAgent, in which Auto-Red is not described. The authors may have intended to refer to RedCodeAgent. Please clarify the citation and the corresponding method."}, "questions": {"value": "1. Please compare SWExploit with the work “When ‘Correct’ Is Not Safe: Can We Trust Functionally Correct Patches Generated by Code Agents?” [1].\n\n2. The base LLMs of SWExploit include Claude. Given Claude’s safety/guardrails, will Claude-based SWExploit refuse to assist in generating adversarial issue statements in red-teaming? How did you handle cases where a base model refused?\n\n\n[1] When ‘Correct’ Is Not Safe: Can We Trust Functionally Correct Patches Generated by Code Agents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1S6prP4Qyq", "forum": "PVarNsrX2v", "replyto": "PVarNsrX2v", "signatures": ["ICLR.cc/2026/Conference/Submission15768/Reviewer_vnPw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15768/Reviewer_vnPw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761456305457, "cdate": 1761456305457, "tmdate": 1762926002639, "mdate": 1762926002639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper posits the use of LLMs in an interesting threat model that involves automatic coding agents maintaining a Github repository. The question is whether another LLM can insert bug-fix requests to the coding agent of the repository in such a way that while the coding agent produces a functionally correct fix, the fix also contains a security vulnerability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I find the threat model credible, though I am less convinced about how easy it is to attack production systems. It is nonetheless an interesting perspective where AI coding meets open source."}, "weaknesses": {"value": "The main weakness seems to be that the flow is based on a LLM-based Github - which is not necessary the most popular of Github configurations yet. I would also imagine it would be based on how good the APR agent is and how tightly checks and quality controls are applied to its output and the request itself. It is already reported that current LLM generated code already contain lots of security bugs. It is unclear if SWExploit will need to add to that.\n\nThe main description outlines the steps of SWExploit but did not give an insights on what the APR agent did that enabled the attack. It seems the APR agent is naive and accepts fixes from any random source. Also, besides the two common defenses dismissed as ineffective, no insight was given on how the entire system can be re-engineered for better robustness."}, "questions": {"value": "1. What are the assumptions made on the APR agent?\n\n2. Is feedback of the APR agent taken into consideration for refining the attack?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vNuv3g3NJm", "forum": "PVarNsrX2v", "replyto": "PVarNsrX2v", "signatures": ["ICLR.cc/2026/Conference/Submission15768/Reviewer_X1Et"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15768/Reviewer_X1Et"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557219749, "cdate": 1761557219749, "tmdate": 1762926001991, "mdate": 1762926001991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores whether LLMs deployed as automated program repair (APR) agents can be exploited to insert patches with hidden vulnerabilities. The authors take valid GitHub-like issues and modify them to include instructions that will cause an (APR) to insert a hidden vulnerability"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses and important attack vector that can occur with the unbridled roll-out of LLM-based APR agents."}, "weaknesses": {"value": "The proposed attack makes unrealistic assumptions about how hard it is to detect the hidden vulnerability. By using the MAGIC STRING, they are making the hidden vulnerability very obvious and easy to detect. Some parts of how they get the APR to generate the exact patch based on their poisoned issues were not well described."}, "questions": {"value": "I think this paper doesn't have a very realistic threat model, that makes the problem too easy. The use of the MAGIC STRING makes it obvious to any reviewer that there is hidden functionality that will only be triggered under very specific circumstances. Since the MAGIC STRING is unlikely to match any test input (as it is high-entropy), then it is fairly obvious that it will not impact any functionality tests, so the modified patch will pass all normal tests. Thus, relying on the MAGIC STRING makes the solution trivial, but it at the same time makes it completely unstealthy and obvious to any human reviewer. I think assuming the patch will never be reviewed by a human is currently unrealistic.  CodeLLMs are well known to be unreliable in generating secure code [1], even without adversarial prompting, so it seems unlikely that current developers would use agentic patching without manual review.\n\nThe paper also makes very little contribution over the state of the art. The paper doesn't make clear what is the difference and why they  outperform baselines as (1) they are all prompt-only methods (2) it is unclear how they changed the baselines' prompts (what auxiliary model?)\n\n[1] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri. 2022. Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions. In 2022 IEEE Symposium on Security and Privacy"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "This paper proposes methods to attack APRs to inject harmful vulnerabilities. There are ethical concerns. They do not have a statement about these risks."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ya4Mj7EFXk", "forum": "PVarNsrX2v", "replyto": "PVarNsrX2v", "signatures": ["ICLR.cc/2026/Conference/Submission15768/Reviewer_YXhk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15768/Reviewer_YXhk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758812250, "cdate": 1761758812250, "tmdate": 1762926001301, "mdate": 1762926001301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SWExploit, a red-teaming tool designed to assess the security risks of LLM-based Automated Program Repair (APR) agents. SWExploit generates adversarial GitHub issue statements that mislead these agents into producing patches that fix the reported bug—passing standard CI/CD tests—while simultaneously injecting hidden vulnerabilities triggered by specific attacker inputs. The approach utilizes program analysis to find effective injection points and crafts fake tracebacks to ensure the vulnerable code is actively invoked by the application. Empirical evaluations show SWExploit achieves an attack success rate on functionally correct patches of up to 0.91, significantly outperforming baselines that often fail to maintain functionality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The attack uses standard GitHub issue statements as the entry point, reflecting the real-world attack surface where any user can submit issues to open-source projects.\n- SWExploit ensures generated patches remain functionally correct and pass regression tests, making them highly likely to be merged by maintainers. The use of a \"MAGIC STRING\" as a conditional gate hides vulnerabilities from normal testing while keeping them exploitable. \n- The method achieves a high Correct-ASR (0.91) compared to baselines (<0.20) and demonstrates strong transferability across different backend LLMs."}, "weaknesses": {"value": "- Please explicitly define 'CI/CD' within the specific context of the paper, which can improve reproducibility and clarity for a broader audience.\n- Specificity of the Trigger Mechanism: The attack heavily relies on an explicit trigger wrapper function guarded by a \"magic string\". This specific design pattern lacks generality, as it may be inapplicable to subtler vulnerability classes (such as complex logic errors) that cannot be easily gated by a simple input string comparison.\n- While the authors demonstrate bypassing basic static checkers and perplexity filters, these mitigation methods are somewhat outdated (from 2023). More advanced, security-aware assessment methods might detect these injected triggers (e.g., LLM judge, guardrail models)"}, "questions": {"value": "Regarding potential defenses, did the authors consider or evaluate implementing safety-aware system instructions directly within the APR agents? For example, explicitly prompting the LLM to verify that its generated patches do not introduce new security vulnerabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EQjX9LMNLN", "forum": "PVarNsrX2v", "replyto": "PVarNsrX2v", "signatures": ["ICLR.cc/2026/Conference/Submission15768/Reviewer_MBoB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15768/Reviewer_MBoB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184160333, "cdate": 1762184160333, "tmdate": 1762926000297, "mdate": 1762926000297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}