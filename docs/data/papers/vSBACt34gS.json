{"id": "vSBACt34gS", "number": 16854, "cdate": 1758269427498, "mdate": 1759897215671, "content": {"title": "The data-quality illusion: Rethinking Classifier-based Quality Filtering for LLM Pretraining", "abstract": "Large-scale models are pretrained on massive web-crawled datasets containing documents of mixed quality, making data filtering essential.\nA popular method is Classifier-based Quality Filtering (CQF), which trains a binary classifier to distinguish between pretraining data and a small, high-quality set. It assigns each pretraining document a quality score defined as the classifier's score and retains only the top-scoring ones.\nWe provide an in-depth analysis of CQF. \nWe show that while CQF improves downstream task performance, it does not necessarily enhance language modeling on the high-quality dataset.\nWe explain this paradox by the fact that CQF implicitly filters the high-quality dataset as well. \nWe further compare the behavior of models trained with CQF to those trained on synthetic data of increasing quality, obtained via random token permutations, and find starkly different trends. \nOur results challenge the view that CQF captures a meaningful notion of data quality.", "tldr": "", "keywords": ["Data filtering", "data selection", "large language models", "pretraining"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc51db328981b32d91ba5eb0caa17183286e4c7a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper critically examines classifier-based quality filtering, a widely used method for pretraining data selection. It shows that CQF improves downstream task performance but paradoxically doesn’t improve language modeling on the high-quality data. The authors argue that CQF implicitly filters the HQ set itself and captures stylistic similarity rather than true data quality. They introduce \"data conditioning\" as a new lens to evaluate whether filtering improves optimization dynamics."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Excellent empirical and theoretical dissection of a widely used but poorly understood technique.\n- Timely and relevant analysis of a core assumption in LLM data engineering.\n- Clear exposition of the paradox and insightful explanation linking CQF to implicit HQ filtering.\n- Strong experimental design and visualizations that clarify complex effects."}, "weaknesses": {"value": "- This work is largely diagnostic with limited actionable guidance on improving CQF beyond critique.\n- Data conditioning concept, while elegant, remains somewhat abstract and untested in real large-scale settings.\n- All experiments are conducted at modest scale which may not generalize.\n- Relies heavily on pretraining proxies (ARC, MMLU) rather than practical LLM evaluation."}, "questions": {"value": "- Can the proposed data conditioning principle be used to design better filtering methods?\n- How robust are the findings when using multilingual datasets?\n- Does the illusion persist when HQ sets are human-curated instruction data versus web data?\n- Could the implicit HQ filtering effect be leveraged deliberately (e.g., via adaptive weighting)?\n\nMissing citations:\n- When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale, Max Marion, Ahmet Üstün, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, Sara Hooker\n- Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs, Kyle O'Brien, Stephen Casper, Quentin Anthony, Tomek Korbak, Robert Kirk, Xander Davies, Ishan Mishra, Geoffrey Irving, Yarin Gal, Stella Biderman"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H8f03dn6tG", "forum": "vSBACt34gS", "replyto": "vSBACt34gS", "signatures": ["ICLR.cc/2026/Conference/Submission16854/Reviewer_5AvM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16854/Reviewer_5AvM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727440131, "cdate": 1761727440131, "tmdate": 1762926873935, "mdate": 1762926873935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that the perceived benefits of Classifier-based Quality Filtering (CQF) in large language model (LLM) pretraining are largely illusory.\n\nAlthough CQF improves downstream benchmark performance, it does not actually select data that better resemble “high-quality” corpora, nor does it improve language modeling performance on such data.\n\nInstead, CQF works primarily by removing obvious low-quality samples and by reweighting the pretraining distribution toward benchmark-style text."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Elegant theoretical framing (density-ratio view) that unifies prior empirical quirks.\n\n2. Strong empirical design across multiple HQ datasets.\n\n3. Clear visualizations demonstrating domain drift."}, "weaknesses": {"value": "1. The filtering process also reduces token count; loss differences may stem from fewer tokens, not “quality”.\n\n2. Circular validation: The “HQ-decile” analysis (Fig. 5) reuses the same CQF score to both partition and evaluate HQ samples, effectively validating the classifier with itself rather than demonstrating genuine quality correlation.\n\n\n2. The scaling-law fitting is not convincing.\n\n    - The experiments vary both data size and distribution with k, violating the assumption of a fixed task distribution.\n\n    - Only three data points (1 %, 10 %, 100 %) are used, with uncontrolled compute budgets, making the fitted β unreliable.\n\n    - No residuals or confidence intervals are reported.\n\nI recommend removing or reframing this section; at present, it does not provide meaningful evidence of scaling behavior."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "U3gsn7JTwY", "forum": "vSBACt34gS", "replyto": "vSBACt34gS", "signatures": ["ICLR.cc/2026/Conference/Submission16854/Reviewer_dHu5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16854/Reviewer_dHu5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969977182, "cdate": 1761969977182, "tmdate": 1762926873450, "mdate": 1762926873450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits Classifier-based Quality Filtering (CQF) — a standard data curation method in LLM pretraining. The authors find a paradox: CQF boosts downstream task performance but doesn’t improve language modeling on the high-quality (HQ) dataset. They explain this by showing CQF implicitly reweights the HQ set itself, emphasizing samples far from the low-quality (LQ) data. They also contrast CQF with importance sampling and introduce data conditioning as a new, optimization-based notion of data quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The writing is clear and easy to follow.\n\n\n- The paper identifies a previously overlooked paradox in CQF, showing that it may not improve language modeling on high-quality data even when it enhances downstream task performance — a perspective that challenges long-standing assumptions in data filtering.\n\n\n- Introducing data conditioning as an optimization-based definition of data quality represents a significant conceptual advance, offering a new lens to evaluate dataset usefulness beyond static classifier scores."}, "weaknesses": {"value": "- While the data conditioning notion is conceptually sound but too ideal, it is impractical for real-world data filtering — evaluating it requires repeated model training and loss computation across datasets, which is computationally expensive and cannot be efficiently estimated per sample. Thus, it serves more as a diagnostic concept rather than a usable filtering metric. From this perspective, the original CQF-style “remove-the-bad” metric can be viewed as a more conservative yet pragmatic strategy for large-scale data curation. Moreover, this paper fail to provide an alternative practical metric to \n\n\n- The experimental analysis, while extensive, is conducted on relatively limited model scales (≤1.3B) and specific datasets (RedPajama-V2, OpenOrca, KnowledgePile). It is unclear whether the same paradoxical behavior of CQF would persist under larger-scale models (such as 7B) or more diverse corpora with heterogeneous noise patterns."}, "questions": {"value": "1. The data conditioning notion is theoretically appealing but computationally impractical. How could it be approximated or operationalized for large-scale data filtering — e.g., via small-model proxies, gradient statistics, or early-training dynamics?\n\n2. The paper analyzes how the CQF selection fraction (k) affects downstream and HQ-set performance, showing that smaller k values emphasize samples farther from the LQ distribution. However, the analysis does not explicitly quantify how the relative size of the selected subset influences this implicit reweighting. When k becomes large—approaching the size of the HQ set itself—does CQF recover most HQ-like data, or does it still inevitably include LQ-like regions? Could the authors clarify how the composition (HQ-aligned vs LQ-aligned) of the filtered dataset evolves with k and whether this proportion can be estimated empirically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UvQ1OLJPA4", "forum": "vSBACt34gS", "replyto": "vSBACt34gS", "signatures": ["ICLR.cc/2026/Conference/Submission16854/Reviewer_vK32"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16854/Reviewer_vK32"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987059090, "cdate": 1761987059090, "tmdate": 1762926873049, "mdate": 1762926873049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to explore the mechanism behind classifier-based quality filtering (CQF) that is commonly used for large-scale pretraining data. The authors question whether CQF selects high-quality (HQ set) data. The paper finds that although CQF improved the downstream metrics, it does not necessarily lead to better language modeling on the HQ set. Finally, the authors propose a framework to \"evaluate\" CQF using \"data conditioning,\" where they find CQF captures properties more closely related to stylistic or domain similarities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. CQF is a commonly used method to curate a pre-training dataset for many state-of-the-art LLMs. Works aims to understand CQF is highly relevant and important. \n\n2. The paper includes a large set of experiments ranging over multiple datasets and settings."}, "weaknesses": {"value": "1. The paper challenges that CQF correlates with the data property of \"universal quality\"; however, it is unclear what \"universal quality\" means. There are no actionable findings pointed out from the paper, rather than showing that CQF works to better align with the downstream tasks. Is this not expected when HQ data is used from downstream targets?\n\nI would be interested in creating a universally high-quality dataset -- without using directly downstream task datasets, and maybe using human/LLM annotations -- and exploring CQF using this data. \n\n2. Evaluation tasks do not necessarily correlate with HQ data used in terms of the paper's analysis. Specifically, OpenOrca and OpenHermes datasets are instruction datasets, and OpenWebMath is a math domain data; however, there is no instruction-style or math evaluation. Supporting this, the downstream metric vs loss relationship is only different for ARC Eacy, where the evaluation and HQ set closely match. \n\n3. Training details (token budget, model size, hparams) are not present in the main pages of the paper, which is important because pretraining dynamics are highly dependent on these factors. I would like to see a baseline where the model is trained with the base dataset and see if it performs above random in downstream evaluations."}, "questions": {"value": "Please see `Weaknesses` for my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kBPyVL2or4", "forum": "vSBACt34gS", "replyto": "vSBACt34gS", "signatures": ["ICLR.cc/2026/Conference/Submission16854/Reviewer_kWia"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16854/Reviewer_kWia"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076586125, "cdate": 1762076586125, "tmdate": 1762926872704, "mdate": 1762926872704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}