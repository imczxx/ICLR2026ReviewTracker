{"id": "aBeIFDshvZ", "number": 10393, "cdate": 1758169740658, "mdate": 1763700242235, "content": {"title": "Diffusion Alignment as Variataional Expectation-Maximization", "abstract": "Diffusion alignment aims to optimize diffusion models for the downstream objective. While existing methods based on reinforcement learning or direct backpropagation achieve considerable success in maximizing rewards, they often suffer from reward over-optimization and mode collapse. We introduce Diffusion Alignment as Variational Expectation-Maximization (DAV), a framework that formulates diffusion alignment as an iterative process alternating between two complementary phases: the E-step and the M-step. In the E-step, we employ test-time search\nto generate diverse and reward-aligned samples. In the M-step, we refine the diffusion model using samples discovered by the E-step. We demonstrate that DAV can optimize reward while preserving diversity for both continuous and discrete tasks: text-to-image synthesis and DNA sequence design.", "tldr": "Diffusion Alignment as Variational Expectation-Maximization (DAV) alternates test-time search (E-step) and forward-KL distillation (M-step) to align continuous and discrete diffusion models.", "keywords": ["Diffusion Model", "Alignment", "RLHF", "Test time search"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/57694e8e72b13a75f0b61fdaf0f1a8163c601ee5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper develops an approach to “Diffusion Alignment as Variational Expectation-Maximization” (DAV) that alternates between two complementary phases: the E-step, which is essentially an exploration step that aims to discover diverse and high-reward samples from the variational posterior; and the M-step, “amortization”,  which refines the diffusion model using samples identified from the E-step."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The DAV approach is built on solid technical foundations as outlined in \\S4. For instance, the E-step uses gradient-based guidance and importance sample to enhance the exploration. The M-step minimizes a mode-covering objective that incentivizes the covering of all diverse modes generated in the E-step. \n\nThe combined E-M steps iteratively refine the model towards a multi-modal aligned distribution; and this overcomes problems like over-optimization and mode collapse that often arise in RL."}, "weaknesses": {"value": "The M-step involves two variations, in addition to the standard DAV objective in (10), there’s a variation,  DAV-KL in (11). From the experimental results in Table 1, it’s not clear which one to use and when, except perhaps the ad hoc trial and error."}, "questions": {"value": "Can the author(s) shed some light on how to choose the value of the KL-coefficient \\lambda in (11), which is meant to control “the trade-off between aligning with the expert policy and pre-serving the pretrained model”? In particular, is the value \\lambda robust or not with respect to downstream applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dJyAgmWkUr", "forum": "aBeIFDshvZ", "replyto": "aBeIFDshvZ", "signatures": ["ICLR.cc/2026/Conference/Submission10393/Reviewer_xurq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10393/Reviewer_xurq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761648532508, "cdate": 1761648532508, "tmdate": 1762921712432, "mdate": 1762921712432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DAV, a novel framework for fine-tuning pre-trained diffusion models. The authors motivate the work by claiming to address reward over-optimization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work formulates diffusion alignment as an iterative Variational Expectation-Maximization, which appears to be a new and interesting theoretical lens for diffusion fine-tuning.2. \n2. The proposed DAV framework enjoys broad applicability. It can accommodate both continuous and discrete settings.\n3. The presentation of this work is easy to follow.\n4. The empirical results show DAV and DAV-KL enjoy superiority over multiple strong baselines, such as DDPO, DRaFT."}, "weaknesses": {"value": "1.  The main comparison in Figure 3, which plots aesthetic reward against diversity/naturalness, is confusing and potentially incomplete. The reported performance of the RL-based if they are properly trained with \"suitable\" KL penalty (which might be non-trivial to choose). This raises questions about the optimal tuning of these baselines. \n2. Furthermore, the analysis in Figure 3 omits purely inference-time methods, which are often competitive in image experiments.\n3. As noticed by the authors, this method has non-negligible computation costs. The E-step involves substantial \"additional test-time computation\" through gradient-guided search. In large-scale diffusion model finetuning, DDPO already takes much time to converge (compared to the fastest direct propagation). It is important to quantitatively present the added training-time overhead of DAV relative to DDPO."}, "questions": {"value": "1. For the results presented in Figure 3, do the images for each algorithm come from a single training run, or are they gathered over multiple runs? If from a single run, please report the standard deviation rather than only the mean.\n2. For discrete finetuning, usually it's straightforward to test both DNA and RNA sequences. Can the authors provide explanations on why only the DNA enhancer is tested?\n3. See other questions above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2yPV7MeCuC", "forum": "aBeIFDshvZ", "replyto": "aBeIFDshvZ", "signatures": ["ICLR.cc/2026/Conference/Submission10393/Reviewer_KMQx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10393/Reviewer_KMQx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800176284, "cdate": 1761800176284, "tmdate": 1762921712120, "mdate": 1762921712120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose DAV, an alignment algorithm for diffusion models via variational expectation maximization. In the E-step, DAV employs test-time search algorithms to generate samples from the reward-weighted posterior distribution. In the M-step, the diffusion model is fine-tuned using the samples from the E-step. The authors demonstrate the effectiveness of DAV on both the continuous text-to-image generation task and the discrete DNA sequence design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea to formulate diffusion alignment as a variational expectation-maximization problem is interesting.\n- The paper is well-written, and the theory and method are well-motivated.\n- Experiments showcase the effectiveness of DAV."}, "weaknesses": {"value": "- The searching algorithms lead to computational overhead. Therefore, a fairer comparison with the baselines should also take the computational cost into account. For example, it would be helpful to compare model performance under the same computational budget and the performance scaling curve of TR2-D2 as the computation increases.\n- The value of 3-mer correlation for DNA sequence design is significantly lower than those reported by baselines, e.g., in DRAKES paper the value is 0.887, much higher than DAV (0.397), while in table 2 and figure 5, it is only 0.229. Also, the target and naturalness are two competing properties, and one can get a higher value of one property by sacrificing the other via hyperparam tuning or using different training epochs. Does DAV have Pareto optimal performance compared to baselines? \n- The E-step can lead to an inaccurate estimation of the posterior distribution, due to the limited sample size and the value estimation error in the test-time algorithms. How does this affect the M-step optimization, and is the model robust to a suboptimal posterior distribution (e.g., with fewer samples or inaccurate samples)?"}, "questions": {"value": "Please refer to the **weaknesses** section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3KCCBUntwA", "forum": "aBeIFDshvZ", "replyto": "aBeIFDshvZ", "signatures": ["ICLR.cc/2026/Conference/Submission10393/Reviewer_77LS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10393/Reviewer_77LS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979003614, "cdate": 1761979003614, "tmdate": 1762921711802, "mdate": 1762921711802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a diffusion alignment method (DAV) that alternates between test-time searches as Expectation steps and online refinement of the diffusion model as Maximization steps. Specifically, the diffusion alignment problem is formulated as a soft RL objective, whose evidence lower bound is optimized an EM algorithm. In the E-steps, DAV draws posterior samples given a reward-tilted distribution; while in the M-steps, DAV distills the sampled trajectories into the diffusion model. Experimental results show the effectiveness of DAV compared to existing RL and direct preference optimization methods for both continuous and discrete diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper offers a fresh perspective by aligning diffusion models with the EM algorithm. I especially appreciate this idea because the multi-round iterative alignment could potentially help in settings where the reward is costly or intractable to evaluate—for example, when it requires human evaluation or expensive wet-lab experiments.\n- The proposed method is accompanied by rigorous derivations and theoretical guarantees."}, "weaknesses": {"value": "- Experiments only include one example for continuous diffusion and one for discrete diffusion. The case for generality would be stronger with additional tasks (e.g., compressibility or prompt alignment as in DDPO). Moreover, some of the recent methods are not included or discussed as well, such as DSPO[1], DanceGRPO[2].\n- The EM algorithm may be substantially more expensive than the methods it is compared to (e.g., DRaFT or DDPO), given the test-time search required in each expectation step. However, there is currently no analysis on the runtime or convergence speed of DAV.\n\n[1] Zhu et al. \"DSPO: Direct Score Preference Optimization for Diffusion Model Alignment\", ICLR 2025.\n\n[2] Xue et al. \"DanceGRPO: Unleashing GRPO on Visual Generation\", arXiv: 2505.07818."}, "questions": {"value": "- For the discrete diffusion model alignment, I am curious of how DAV compare to test-time sampling algorithms such as [3,4], which also consider the same DNA enhancer task, and also alignment methods designed for discrete diffusion models (e.g., [5,6]).\n\n[3] Li et al. \"Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding\", arXiv: 2408.08252.\n\n[4] Chu et al. \"Split Gibbs Discrete Diffusion Posterior Sampling\", NeurIPS 2025.\n\n[5] Borso et al. \"Preference-Based Alignment of Discrete Diffusion Models\", ICLR 2025 Bi-Align Workshop.\n\n[6] Zhu et al. \"LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models\", arXiv: 2505.19223."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aqOIkbDs2Z", "forum": "aBeIFDshvZ", "replyto": "aBeIFDshvZ", "signatures": ["ICLR.cc/2026/Conference/Submission10393/Reviewer_8DWZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10393/Reviewer_8DWZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982010707, "cdate": 1761982010707, "tmdate": 1762921711399, "mdate": 1762921711399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank the review committee for their thoughtful and constructive feedback. We appreciate the recognition of our paper’s strengths, simultaneously emphasized by the reviewers: **Novelty** (8DWZ, KMQx), **Technically solid** (8DWZ, xurq), **Strong experimental results** (77LS, KMQx), and **Clear writing and motivation** (77LS, KMQz).\n\nWe also acknowledge the shared concern regarding computational expense raised by (M6wy, 77LS, KMQx). To address this concern, we now provide detailed computational costs analysis for optimizing (1) aesthetic score, (2) compressibility & incompressibility metrics, and (3) DNA enhancer activity.\n\n---\n**Computational costs analysis for aesthetic score**\n\nTable A reports the RTX 4090 GPU hours and performance of our method compared to DDPO, DRaFT, and their KL-regularized variants, evaluated following Eq. 18 of [1]. While DAV requires a substantial computational budget, its runtime remains comparable to the high-epoch DDPO and KL-regularized baselines. Crucially, DAV justifies this cost by achieving a superior trade-off: it attains the highest aesthetic scores while preserving LPIPS-A and ImageReward. In contrast, KL-regularized baselines suffer significant degradation in diversity and ImageReward even when consuming comparable or greater GPU hours.\n\n\n**Table A.** Comparison of computational cost and performance.\n| Method-(epochs)           | Aesthetic (↑) | LPIPS-A (↑) | ImageReward (↑) | GPU hours |\n|:----------:|:-------------:|:-----------:|-----------------|-----------|\n| Pretrained       | 5.40          | **0.65**       | 0.90 | -\n| DDPO-100   | 6.08          | 0.63        | 0.96            | 18.1      |\n| DDPO-200   | 6.44          | 0.57        | 0.85            | 36.1     |\n| DDPO-300   | 6.70          | 0.54        | 0.67            | 54.2      |\n| DDPO-400   | 6.84          | 0.48        | 0.28            | 72.2    |\n| DDPO-500   | 6.82          | 0.44        | -0.44           | 90.3     |\n| DDPO+KL-400 ($\\alpha$=0.3) | 6.93          | 0.47        | 0.47            |  82.7         |\n|DRaFT-42 |7.22|0.46|0.19| **1.7**|\n| DRaFT+KL-2000 ($\\alpha$=0.035)  | 6.78          | 0.59        | 0.23            | 220.0 |\n| DAV-100 (M=4)          | **8.04**          | 0.53        | 0.95            | 82.4|\n| DAV-KL-100 (M=2) | 7.11      | 0.58    | 1.11        | 91.2     |\n| DAV-KL-100 (M=4) | 6.99      | 0.58    | **1.13**        | 98.7     |\n\n\n---\n**Computational costs analysis for compressibility and incompressibility**\n\nWe note that the analysis of compressibility and incompressibility score [2] was already included in the original paper in Appendix J. As shown in Figure 10, DAV-KL trained for 6 epochs substantially outperforms the DDPO baseline trained for 100 epochs. In terms of compute, DAV-KL requires 14.3 GPU hours on an RTX 3090, which is roughly half the cost of DDPO at 28.7 GPU hours.\n\n---\n**Computational costs analysis for DNA sequence design**\n\nFor discrete diffusion model fine-tuning, we reproduce DDPO [2] and VIDD [3] using the [official codebases of VIDD](https://github.com/divelab/VIDD), and we reproduce DRAKES [4] following its [official implementation](https://github.com/ChenyuWang-Monica/DRAKES). All hyperparameters are set exactly as specified in the original papers. On a single RTX 3090 GPU, the training times are approximately: **14 hours for DDPO, 16 hours for VIDD, 43 hours for DRAKES, and 15 hours for DAV**. Notably, DAV achieves comparable training time while yielding higher reward and naturalness with preserved diversity as presented our paper.\n\n[1] Uehara, Masatoshi, et al. \"Understanding reinforcement learning-based fine-tuning of diffusion models: A tutorial and review.\" arXiv preprint arXiv:2407.13734 (2024).\n\n[2] Black, Kevin, et al. \"Training Diffusion Models with Reinforcement Learning.\" The Twelfth International Conference on Learning Representations.\n\n[3] Su, Xingyu, et al. \"Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design.\" arXiv preprint arXiv:2507.00445 (2025).\n\n[4] Wang, Chenyu, et al. \"Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design.\" The Thirteenth International Conference on Learning Representations."}}, "id": "UIXrK9pkzN", "forum": "aBeIFDshvZ", "replyto": "aBeIFDshvZ", "signatures": ["ICLR.cc/2026/Conference/Submission10393/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10393/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission10393/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763698202845, "cdate": 1763698202845, "tmdate": 1763698202845, "mdate": 1763698202845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision notification"}, "comment": {"value": "To address the reviewers’ feedback, we have revised our manuscript and highlighted the corresponding changes in red.\n\n- Figure 3: Following the clarity concern raised by KMQx, we revise the caption to provide a more precise explanation.\n- Appendix F.2: We add a discussion of recent GRPO and DPO-based approaches to better situate our work within the current literature.\n- Appendix H.2: We incorporated additional test-time search baselines for DNA sequence design.\n- Appendix K: We included a detailed computational cost analysis."}}, "id": "ZNbxH0DO6Z", "forum": "aBeIFDshvZ", "replyto": "aBeIFDshvZ", "signatures": ["ICLR.cc/2026/Conference/Submission10393/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10393/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission10393/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763698254650, "cdate": 1763698254650, "tmdate": 1763698254650, "mdate": 1763698254650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}