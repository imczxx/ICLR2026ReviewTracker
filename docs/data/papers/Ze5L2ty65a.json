{"id": "Ze5L2ty65a", "number": 15556, "cdate": 1758252652385, "mdate": 1759897299561, "content": {"title": "A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings", "abstract": "Knowledge tracing (KT) aims to estimate a student's evolving knowledge state and predict their performance on new exercises based on performance history. Many realistic classroom settings for KT are typically low-resource in data and require online updates as students' exercise history grows, which creates significant challenges for existing KT approaches. To restore strong performance under low-resource conditions, we revisit the hierarchical knowledge concept (KC) information, which is typically available in many classroom settings and can provide strong prior when data are sparse. We therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a probabilistic KT framework that models student understanding over a tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree Model. KT$^2$ estimates student mastery via an EM algorithm and supports personalized prediction through an incremental update mechanism as new responses arrive. Our experiments show that KT$^2$ consistently outperforms strong baselines in realistic online, low-resource settings.", "tldr": "We propose KT^2, a probabilistic knowledge tracing framework based on a knowledge concepts tree. It supports data efficienct, interpretible, and online student modeling by combining a Hidden Markov Tree with incremental EM updates.", "keywords": ["educational applications"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea2f21e8ccdf24d9b6cf44b83cb110b75bf5dbe9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes $\\mathrm{KT}^2$, a probabilistic KT method that models a student’s mastery over a hierarchical tree of knowledge concepts using a Hidden Markov Tree. \nIt learns a global model from a small class-wide burn-in, then performs one-step EM updates per student as new answers arrive. \nAcross two datasets, $\\mathrm{KT}^2$ has consistent advantages over baselines and also provides interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Nice figures. The method writing is clear, though the formality of the notations could be improved. \nThe motivation is valid, in proposing a personalized graph for each student and considering online learning in KT, which hasn’t been extensively explored yet."}, "weaknesses": {"value": "- I like the presentation overall; it is clear. However, I strongly recommend that the authors improve the writing. For example, 1) avoid overusing LLMs for paraphrasing; 2) I assume lines 98–102 belong to the same paragraph; 3) The subheadings in the related works section should either be formatted as subsection titles or end with a full stop.\n\n- What is gained beyond correlation graphs? Ths use of Hidden Markov Tree over KCs where each node’s mastery is a latent variable and has a hard entailment rule: if a parent is mastered, all children are mastered (Eq. 7). This is basically deterministic CPTs along the tree and a set of conditional independences that are observationally indistinguishable from certain non-tree graphical models when only passively observed correctness is available (no interventions). In other words, here adding latent variables is still giving you a correlational model. I am confused about where the extra modeling power comes from. Two main concerns: \n    - Missing comparisons to graph-based KT and non-hierarchical structure. To isolate what the tree (vs. a general graph or leaf-leaf links) buys you, please compare against graph-based KT baselines (e.g., GKT, SKT) fed the same induced structure. \n    - The hard constraint may affect the results and is risky especially when your trees are inferred by LLMs and may be imperfect. \n\n- This relates to the previous point. I think the work lacks analysis on why it works and when it fails. The qualitative Fig.3 is helpful, but there’s little error analysis (per-KC, per-item, or per-student). We don’t see calibration plots, learning curves beyond AUC, or examples where the hard hierarchy hurts.\n\n- Emissions rely on three difficulty bins with shared parameters plus a single guessing rate $\\varepsilon$(Eq. 9), and $\\varepsilon$ clipped to 0.3 during training (Appendix H). This is quite a strong inductive bias. \n\n- Only small LLMs (3B/7B) with a simple 10-shot protocol are tried; no retrieval, cot, or structure-aware prompting. This likely underestimates LLM performance. \n\n- Table 2 lacks confidence intervals or significance tests as several margins are modest. This matters because differences of ~0.02-0.03 AUC may not be robust."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xn107Z7Z8S", "forum": "Ze5L2ty65a", "replyto": "Ze5L2ty65a", "signatures": ["ICLR.cc/2026/Conference/Submission15556/Reviewer_uKF4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15556/Reviewer_uKF4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761518301688, "cdate": 1761518301688, "tmdate": 1762925831531, "mdate": 1762925831531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a tree-based method for knowledge tracing that they update using EM updates.\nThey show using 2 lesser known datasets that they outperform existing approaches, notably deep learning approaches (some of them refitted in an online way) or LLMs.The proposed approach is elegant and does not require heavy computation nor a GPU."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written. The proposed approach is elegant and does not require heavy computation nor a GPU. It seems to outperform existing approaches.\n\nI enjoyed reading the paper, didn't believe the results at first read, then I understood that the proposed approach results in extra runtime as there is refitting at test time."}, "weaknesses": {"value": "However, there is a large body of literature that seems missing from the paper. The authors mostly compare themselves to deep learning approaches and not simpler approaches. For example, this paper uses a hierarchical Bayesian network that is refitted on new observations, and matches the performance of DKT:\n\n\tWilson, Kevin H., et al. \"Back to the Basics: Bayesian Extensions of IRT Outperform Neural Networks for Proficiency Estimation.\" International Educational Data Mining Society (2016). https://www.educationaldatamining.org/EDM2016/proceedings/paper_145.pdf\n\nMore generally I am surprised that they didn't compare to cognitive diagnosis (DINA model, attribute hierarchy model) or recent approaches such as neural cognitive diagnosis:\n\n\tWang, Fei, et al. \"Neural cognitive diagnosis for intelligent education systems.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 04. 2020.\n\nThe link between cognitive diagnosis and knowledge tracing has been established:\n\n\tWang, Fei, et al. \"Dynamic cognitive diagnosis: An educational priors-enhanced deep knowledge tracing perspective.\" IEEE Transactions on Learning Technologies 16.3 (2023): 306-323."}, "questions": {"value": "The authors are aware of the pyKT benchmark, however it is surprising that the datasets used in the experiments are less encountered in the literature. Why didn't they compare themselves to more standard datasets such as ASSISTments?\n\nWhy didn't they compare to standard baselines such as NeuralCDM?\n\nWhat is called transition probabilities is different from the transition probability in BKT \"probability to acquire a skill\". It is more related to the dependency structure on the tree. In the proposed approach, nothing models the latent evolution of knowledge. If some student fails some exercise at first attempt and tries again the same exercise, will the proposed model predict that the student will fail again? (The proposed model may overfit the past data and not be resilient to changes in distribution i.e. learning.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zaxJAJBHkU", "forum": "Ze5L2ty65a", "replyto": "Ze5L2ty65a", "signatures": ["ICLR.cc/2026/Conference/Submission15556/Reviewer_VEQH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15556/Reviewer_VEQH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813362507, "cdate": 1761813362507, "tmdate": 1762925831045, "mdate": 1762925831045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes KT2, a probabilistic framework designed for low-resource and online classroom scenarios. It models hierarchical knowledge concepts via a Hidden Markov Tree (HMT), treating student mastery as latent variables and exercise correctness as observed variables. The model uses an EM algorithm for initial parameter estimation and performs one-step EM updates for each new student interaction to enable incremental learning. Experiments on simulated subsets of XES3G5M and MOOCRADAR datasets show that KT² outperforms deep learning KT baselines  and LLM-based methods under low-resource conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper addresses a practical problem: performing KT under low-resource and online settings.\n2. The model formulation is mathematically consistent and clearly presented, with interpretable structure.\n3. The use of a hierarchical KC tree adds intuitive interpretability compared to flat KT baselines."}, "weaknesses": {"value": "1. While the model is conceptually clear, some assumptions—such as deterministic parent-to-child mastery—might be too strong; relaxing them could further improve realism. The model assumes full entailment between parent and child KCs (“if parent mastered → all children mastered”), which is unrealistic and oversimplifies real learning dynamics.\n2. The current experiments rely on simulated subsets of existing datasets; validation on live classroom or streaming data would better demonstrate real-world applicability.\n3. The paper could discuss more deeply how KT² might integrate with future LLM-based methdos.\n4. The emission probability depends only on a fixed difficulty bin (easy/medium/hard), ignoring item discrimination and personalized learning rates.\n5. The guessing and slipping parameters are fixed globally, which undermines personalization despite the “incremental” claim."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "poQ2ZnxqH7", "forum": "Ze5L2ty65a", "replyto": "Ze5L2ty65a", "signatures": ["ICLR.cc/2026/Conference/Submission15556/Reviewer_qbuF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15556/Reviewer_qbuF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920532781, "cdate": 1761920532781, "tmdate": 1762925830572, "mdate": 1762925830572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}