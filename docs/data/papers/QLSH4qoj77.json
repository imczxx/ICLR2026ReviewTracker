{"id": "QLSH4qoj77", "number": 23180, "cdate": 1758340614275, "mdate": 1759896828284, "content": {"title": "Beyond Mean Scores: Factor Models for  Reliable and Efficient AI Evaluation", "abstract": "Generative AI evaluation relies heavily on benchmark leaderboards, which rank models according to their mean score on a given benchmark. In this paper, we show that these one-number metrics often obscure the multidimensional structure of model capabilities.  We propose a factor model approach that decomposes model-item performance into interpretable latent constructs. We apply the modeling approach to examine a novel data set constructed from the Huggingface Open LLM Leaderboard containing item responses from 4,416 language models evaluated across 21,176 questions from six benchmarks. Our analysis reveals two key findings. (i) First, benchmarks contain distinct, sometimes negatively correlated constructs that mean scores conflate---models with identical averages can excel at entirely different capabilities. This makes mean scores uninformative---or even misleading---measures of model capabilities.  We propose disaggregated alternatives based on the factor structure. (ii) Second, we demonstrate that the factor structure enables efficient estimation of full-benchmark and disaggregated factor-level mean scores. By identifying the most informative questions, we can reduce evaluation costs while preserving model rankings. These results establish factor models as a principled framework for understanding benchmark structure, diagnosing when aggregation obscures meaningful differences, and enabling adaptive evaluation that maximizes information per question.", "tldr": "Mean scores can obscure model abilities. Factor models reveal distinct capability clusters and enable more reliable and efficient LLM evaluation.", "keywords": ["AI evaluation", "Benchmarking", "factor models", "large language models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f025069f3d7527f25e4722ea9713d264844a8651.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes using factor models from psychometrics to analyze AI benchmark data at the item-response level, arguing that traditional leaderboard averages obscure the multidimensional structure of model capabilities. The study demonstrates that benchmarks often conflate distinct skills and that factor-based adaptive evaluation can achieve comparable reliability with far fewer items."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper introduces a psychometric perspective to AI evaluation, moving beyond aggregate benchmark scores toward construct-level analysis.\n\n2.  It uses large-scale, real leaderboard data from diverse benchmarks, offering empirical insight into hidden capability patterns.\n\n3. The adaptive testing framework demonstrates substantial efficiency gains (up to 94% item reduction) while preserving measurement accuracy."}, "weaknesses": {"value": "1. Lack of professionalism: E.g., Several figures—especially the heatmaps—lack clear color legends or scales, making them visually confusing and unprofessional. The paper dives directly into response matrices without clearly stating the evaluation objective, research questions, or intended outcomes of “AI assessment.”\n\n2. The core approach is essentially a multidimensional Item Response Theory (IRT) model; both the factor-analytic framing and adaptive item selection are well established in prior works (e.g., [1][2]). **The methodology in this paper is almost identical to previous works, yet it neither cites them nor provides any comparative analysis**.\n\n\n[1] Polo, Felipe Maia, et al. \"tinyBenchmarks: evaluating LLMs with fewer examples.\" International Conference on Machine Learning. PMLR, 2024.\n\n[2] Kipnis, Alex, et al. \"metabench--A Sparse Benchmark to Measure General Ability in Large Language Models.\" arXiv preprint arXiv:2407.12844 (2024)."}, "questions": {"value": "What is difference between your factor model and IRT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kIgpX0CQ7p", "forum": "QLSH4qoj77", "replyto": "QLSH4qoj77", "signatures": ["ICLR.cc/2026/Conference/Submission23180/Reviewer_MSHc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23180/Reviewer_MSHc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739861835, "cdate": 1761739861835, "tmdate": 1762942548523, "mdate": 1762942548523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the shortcomings of using mean scores to determine the performance of LLMs. Through factor model analysis, the authors are able to probe into different capability dimensions of the data, infer performance on held-out data-model evaluation pairs and also provide a framework for adaptive testing to reduce evaluation costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper does a great job with theoretical guarantees. For example, a factor model generally assumes continuous variables but this work assumes binary data which cannot be normally distributed. The way to adapt binary data to a factor model is with the tetrachoric correlation, which is detailed in the work.\n\n2. The scale of data points and models from OpenLLM Leaderboard is large-scale which gives validity to the authors' claims.\n\n3. Adaptive testing, while not completely novel, has been demonstrated here to provide significant reductions in evaluation costs."}, "weaknesses": {"value": "I am very inclined to increase my score to an 8, but need the following points addressed.\n\n1. There is no ground truth to validate that the factors extraxcted from the model correspond to useful capabilities. \n\n2. The assumption of binary(correct/incorrect) evaluations is not  accurate: there are datasets for LLMs such as WMT that are numeric floating point scores. Large benchmark collections such as HELM contain many samples of this metric. The authors should conduct an additional study to test the factor model under heterogeneity.\n\n3. Human experiment: there needs to be more analysis such as inter-annotator agreement and review guidelines.\n\n4. Confounders for \"models that perform well on one part of the benchmark are very likely to perform poorly on another\": some confounders that can affect this claim are data contamination (training on the test set), diversity in training objectives (base model vs instruction-tuned).\n\n5. I contest the conclusion from the MUSR example (L372-L403), specifically \"two items may appear textually similar, yet test takers\ncan exhibit sharply different response patterns\". I would argue that the factors are considering superficial reponse patterns if two very similar questions from the same narrative have different answers. I base mty argument on theory-of-mind reasoning [1]\n\n[1] Amirizaniani et al. Can LLMs Reason Like Humans? Assessing Theory of Mind Reasoning in LLMs for Open-Ended Questions, 2024"}, "questions": {"value": "1. Real-world data is noisy and missing evalutions are likely given the rapid proliferation of models and datasets. How would the authors treat missing entries in P before the train-test split? Interpolating the value should not be a scientific solution.\n\n2. What is the justification of error variance being fixed at $\\pi^2$/3?\n\n3. Does the promax rotation(oblique rotations) produce more interpretable factors? How does it compare to orthogonal rotations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m6AlkLPLeV", "forum": "QLSH4qoj77", "replyto": "QLSH4qoj77", "signatures": ["ICLR.cc/2026/Conference/Submission23180/Reviewer_NFUq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23180/Reviewer_NFUq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913438477, "cdate": 1761913438477, "tmdate": 1762942548216, "mdate": 1762942548216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper applies factor models to analyze 4,416 language models on 21,176 questions from six benchmarks. The work demonstrates that: (1) benchmarks contain distinct, sometimes negatively correlated constructs that mean scores conflate; (2) factor models enable adaptive testing that reduces evaluation costs, and (3) factors can be interpreted via clustering and LLM-based labeling. The paper positions this as bringing psychometric methodology to leaderboard-scale AI evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**S1: Strong empirical validation and scale.** The analysis of 4,416×21,176 item responses seems to be substantially larger than prior work. The factor models achieve good predictive performance.\n\n**S2: Compelling practical impact.** The adaptive testing results demonstrate 33.9-94.3% reduction in required questions while maintaining 95% reliability.\n\n**S3: Clear presentation.** The paper is well-written (even though one could probably reduce the equation-load without affecting understandability) with effective visualizations (especially Figure 1)."}, "weaknesses": {"value": "### W1: Overclaimed Novelty \n\nLine 104 claims: \"We present a first systematic attempt to interpret factors discovered by the model.\"\n\nThis is might not be completely true given existing work:\n- [**CAIMIRA (Gor et al.)**](https://arxiv.org/abs/2410.06524) already provides systematic interpretation of QA abilities using IRT with theory-driven constructs linked to cognitive science frameworks\n- **metabench** factor analysis (arxiv.org/abs/2407.12844v2) applies factor analysis to benchmark interpretation\n- Additional relevant work: arxiv.org/pdf/2501.17200, sciencedirect.com/science/article/pii/S0160289624000527\n\nThese papers are not cited. The interpretation contribution should be reframed as \"applying interpretation at leaderboard scale\" rather than claiming primacy.\nNo comparisons to those approaches are made, and only a mean baseline is used. \n\nThe motivation that mean scores are inadequate  is well-established (cited: Saxon et al. 2024, see also https://www.sciencedirect.com/science/article/pii/S0927025625003842); the contribution seems to be applying factor models at scale, not identifying this problem\n\n\n### W2: Weak Interpretation Methodology\n\nThe interpretation approach has no evidence that labels are stable, expert-validated, or predictive beyond the clusters they describe\n\nThis is notably weaker than theory-driven approaches in prior work like CAIMIRA."}, "questions": {"value": "1. How does your work contextualize with CAIMIRA, metabench, and co? \n2. Have you validated GPT-5 labels in some way, e.g., with domain experts?\n3. How do you ensure factors measure constructs rather than spurious artifacts (given the MUSR pattern-matching evidence)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KL1wnpbNsM", "forum": "QLSH4qoj77", "replyto": "QLSH4qoj77", "signatures": ["ICLR.cc/2026/Conference/Submission23180/Reviewer_wmHA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23180/Reviewer_wmHA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914067924, "cdate": 1761914067924, "tmdate": 1762942547775, "mdate": 1762942547775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work applies latent factor models to LLM benchmarking. The main claimed contributions are: (1) identifying that benchmarks are testing multiple underlying capabilities that can be discovered through fitting factor models, an argument against standard mean aggregation which hides tradeoffs between these capabilities, (2) showing how factor models can be used for efficient adaptive evaluation, with the main augmenting showing one can preserve evaluation reliability while cutting evaluation costs, and (3) a case study interpreting the latent factors on LLM benchmarks using clustering based on learned factors and prompting GPT 5 on clusters.\n\nTheir experiment involves taking 6 LLM benchmarks (e.g. MMLU Pro, BBH, IFeval, etc) and many LLM responses on those benchmarks sourced from the open LLM leaderboard. They fit a multidimensional factor model to each benchmark dataset, holding out some instances for testing."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work is a tutorial-like deep dive of factor analysis and how to use it on LM benchmarks to gain different insights. It covers a breadth of topics and analyses. Factor analysis methodology is sound."}, "weaknesses": {"value": "**Lacking proper dev set for tuning rank of factor models**\nSomething concerning to me is that their factor models lack a proper baseline for comparison. In L269, they describe how they split the data into train 80% and test 20% for each benchmark, and fit factor models that result in high AUC (mid-high 90%s) across benchmarks. The problem is they also tune the hyperparameter (rank size) of their factor models on this test set (Appendix C.1) rather than splitting off a proper development set from the train set. So these AUC scores are going to be artificially inflated. \n\n**Lacking baseline for factor models**\nThe only baseline reported in this work for proving the validity of the factor models is a super naive baseline that takes the mean performance of the chosen subset to predict the test instances. Indeed this is the “standard” practice in LM benchmarking today, but there are many works that already improve on this. For example, why not have results against Truong et al (2025)’s use of Rasch style models? If this work is arguing that single-trait assumption is limiting (L135), and multi latent factor models are superior, then one should show it empirically by comparing simple Rasch model or 2PL models in the same experimental setup and showing it has lower AUC than the factor model. It is the responsibility of the authors to pick reasonable baselines that can help readers understand the strengths/weaknesses of the proposed method.\n\n**Lacking baselines for analysis/insights on decomposed factors**\nOne of the big results of this paper is in Figure 3, showing how factor models can be used to produce interesting clusters with good block-diagonal membership diagnostics. The issue here is, again, there are lacking baselines that show that factor models are needed to produce these types of insights. Can one have achieved this with PCA? With some more naive embedding + clustering technique? Are there metadata tags on instances in these benchmarks that could’ve done better? It’s nice that factor models can reveal this type of structure, but one needs to show that this structure is non-trivial to recover using reasonable other methods.  Especially if this experiment is achieved with a rank 2 factor model; would this be achievable with the first two components of PCA, or even clustering on the two learned variables in a 2PL model?\n\nFurthermore, on the topic of decomposing benchmark tested capabilities, there is prior work such as “FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets (ICLR 2023)” that exactly tackled this problem of decomposing aggregate scores into finer-grained skills. Another work is “Unearthing Skill-Level Insights for Understanding Trade-Offs of Foundation Models (ICLR 2024)” that also performed this type of decomposition to discover subsets of benchmarks that test finer grained skills in LM evaluation. Indeed, they do not apply factor analysis, so this work is novel in that regard, but because there is prior work in this space, it is important for the submission authors to situation this work against those prior works. Missing also from related work is “Anchor Points: Benchmarking Models with Much Fewer Examples (Vivek et al 2024)” which also uses item clustering.\n\n**Lacking baselines for efficient LM evaluation**\nThis work shows an adaptive testing procedure for efficient LM evaluation (Sec 4.2). This is an incredibly short section, but again, it lacks baselines and situating in prior literature. There are many works on efficient benchmarking, some even using IRT which is related inspiration from psychometrics. There is no citation in this work for Anchor Points (Vivek et al 2024), Tiny Benchmarks (Polo et al 2024), Metabench (Kipnis et al 2025), and more. All of these build on ideas of selecting an informative subset of items because they reflect some “underlying” capabilities. The factor analysis approach is new, but there is so much work uncited & not baselined against in this paper.\n\n**Lacking related work and baselines for interpretable features**\nWith any sort of work that’s about using learned latent factors and assigning human interpretable meaning to these learned factors, there is already plenty of work in this space that is not cited or baseline against. This paper states “We view this as an initial\nstep toward a richer interpretive framework that future work can refine and extend” in L433. Unfortunately, this work is not the initial step at all.  “Qualeval: Qualitative evaluation for model improvement (Murahari et al 2024)” and “Unearthing skill-level in-\nsights for understanding trade-offs of foundation models (Moayeri et al 2024)” automatically categorize benchmark instances into capability groups. “Goal-driven explainable clustering via language descriptions (Wang et al 2023) and “ Explaining datasets in words:\nStatistical models with natural language parameters (Zhong et al 2024)” are also examples. There should be better related work discussion + baselining against them."}, "questions": {"value": "Sec 3 takes up a ton of space mostly explaining background on how factor analysis works. It’s not clear this is needed, and if anything could be shortened to make room for more important additions to this work, especially baseline experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tA5jDKLEE3", "forum": "QLSH4qoj77", "replyto": "QLSH4qoj77", "signatures": ["ICLR.cc/2026/Conference/Submission23180/Reviewer_55PF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23180/Reviewer_55PF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762121854477, "cdate": 1762121854477, "tmdate": 1762942547433, "mdate": 1762942547433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}