{"id": "cuN6DSCS8i", "number": 8991, "cdate": 1758106256442, "mdate": 1759897749570, "content": {"title": "Inference-time Unlearning via Adaptive Output Regulation", "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in memorizing vast amounts of knowledge across diverse domains. However, the ability to selectively forget specific knowledge is critical for ensuring the safety and compliance of deployed models. Existing unlearning efforts typically fine-tune the model with resources such as forget data, retain data, and a calibration model. These additional gradient steps blur the decision boundary between forget and retain knowledge, often resulting in degraded overall performance. To avoid the negative impact of fine-tuning, it would be better to achieve *approximate unlearning at inference time*, where the model is dynamically guarded against generating responses related to the forget target without retraining or damaging its fluency. Current training-free approaches, though avoiding retraining, often suffer from incomplete or superficial forgetting.  To this end, we introduce **GUARD**, an inference-time unlearning approach via adaptive output regulation to mitigate this problem without retraining or compromising fluency, which first employ a prompt classifier to detect unlearning targets and extract the corresponding forbidden tokens. We then dynamically penalize and filter candidate tokens during generation through a combination of token matching and semantic matching, thereby preventing the model from leaking the forgotten content.  Experimental results on copyright-content unlearning tasks over the Harry Potter dataset and the MUSE benchmark, as well as entity unlearning tasks on the TOFU dataset, demonstrate that **GUARD** achieves strong forget quality across various tasks while causing almost no degradation to the LLM’s general capabilities, striking an excellent trade-off between forgetting and utility.", "tldr": "", "keywords": ["Large Language Model", "LLM Unlearning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03fecc372363852d3d95e36dc0c91ac2eb4bd4fb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a framework called GUARD to address the selective forgetting problem in LLMs. The method follows a training-free paradigm, operates at inference time, and consists of three key steps:\n1. Prompt classification: A lightweight MLP classifier determines whether the input query is related to the content that should be forgotten.\n2. Forbidden token extraction: If the query is relevant, the system retrieves the original answer from pre-stored forgetting data and extracts key “forbidden tokens” that should no longer appear.\n3. Controlled generation: During decoding, by combining token-level hard matching with SBERT-based semantic soft matching, the method dynamically penalizes or filters candidate words that contain or are semantically close to the forbidden tokens, guiding the model to produce safe and compliant responses.\n\nThe authors conduct extensive experiments on several benchmarks and GUARD achieves high-quality forgetting while almost perfectly preserving the model’s original utility."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is important and practical: Selective forgetting in LLMs is one of the core challenges in AI safety and compliance, especially in the context of regulations such as GDPR. This study has clear real-world significance.\n2. The method is cleverly designed: It decomposes the complex task of modifying a model to “forget” into a controllable “output guarding” problem executed at inference time. This approach avoids the high computational cost of traditional fine-tuning and the risk of catastrophic forgetting.\n3. The experiments are thorough and effective: They cover three representative tasks—TOFU, MUSE, and Harry Potter—with rich baselines and evaluations across multiple models. The metrics are comprehensive, demonstrating a balance between forgetting quality and utility."}, "weaknesses": {"value": "1. The method may not satisfy the true nature and boundaries of \"unlearning\" or \"forgetting\": it performs output regulation/filtering rather than parameter-level causal removal of training influence, and is therefore not, strictly speaking, mechanism-level unlearning. For example, suppose A trains a model M_A using data provided by B. Later, after B discovers sensitive information in the data, B asks A to delete the sensitive information contained in M_A. In this scenario, training-based methods could theoretically fulfill B’s request, whereas the method in this paper cannot, because the sensitive information still resides in the model. Thus, if regulation or compliance requires “removal of training influence,” the legal and technical adequacy of this method remains unclear.\n2. The cost of inference latency is not clear: This is the method’s most significant shortcoming. Although it avoids training costs, GUARD shifts computational overhead to every relevant inference request. Classification, retrieval, and especially SBERT-based semantic matching at each decoding step will significantly increase the latency of generating responses. In real applications, efficiency issues such as inference latency, parallelism, and throughput are non-negligible; however, the authors did not provide experiments analyzing inference speed, which casts doubt on the method’s practicality.\n3. Limited “depth” of forgetting: GUARD is essentially behavioral-level forgetting—it achieves forgetting by preventing the model from “saying” what it should not, rather than truly erasing the information from the model’s internal knowledge representations. This means the model may still “know” the information but cannot directly express it. While this is effective for preventing direct content leakage, its effectiveness against more advanced privacy attacks that infer from the model’s internal states remains uncertain."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "luP2U7WDTh", "forum": "cuN6DSCS8i", "replyto": "cuN6DSCS8i", "signatures": ["ICLR.cc/2026/Conference/Submission8991/Reviewer_kHfe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8991/Reviewer_kHfe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810461847, "cdate": 1761810461847, "tmdate": 1762920721573, "mdate": 1762920721573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GUARD, a training-free unlearning method that aims to mitigate the fluency degradation seen in fine-tuning-based unlearning approaches. Instead of modifying model weights, GUARD intervenes directly in the decoding process. It first trains a classifier to determine whether a prompt belongs to the forget set, using the prompt embedding from the target LLM. Then, GUARD retrieves the most relevant answers from the forget set using SBERT and extracts *forbidden tokens*. During generation, these forbidden tokens are used (alongside SBERT similarity) to suppress the generation of unwanted information based on both token-level and semantic similarity. Experiments on several unlearning benchmarks (TOFU, MUSE, and Harry Potter) demonstrate GUARD’s ability to preserve model utility on retain data while reducing recall of the forget set."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- [S1] **Comprehensive experimentation.** The paper provides thorough experimental evaluation across multiple widely used unlearning benchmarks and compares GUARD against a broad range of baselines, including both fine-tuning-based and inference-time methods.\n- [S2] **Interesting methodological idea.** Using a classifier to detect whether a prompt belongs to the forget set is an underexplored direction. This approach can certainly help preserve model utility, since the original model remains untouched when the classifier correctly identifies prompts as outside the forget set."}, "weaknesses": {"value": "- [W1] **Misalignment with the core goal of unlearning.** The main issue is that GUARD does not align with the fundamental goal of machine unlearning, producing a model that behaves as if it were never trained on the forget data. Several components of GUARD undermine this goal:\n\n  - The prompt classification step relies on representations from the original (yet unlearned) model.\n  - The forbidden-token extraction requires continued access to the forget set $D_f$ even after deployment, whereas $D_f$ should ideally be discarded once unlearning is complete.\n  - Appendix C mentions that “further fine-tuning could potentially lead to even better performance,” which implies additional exposure of the model to the very data it should forget.\n\n  These design choices contradict the spirit of unlearning and would not meet the expectations of data deletion from a regulatory or user-trust standpoint.\n- [W2] **High inference-time computational cost.** As shown in Table 9 (Appendix G), GUARD introduces significant computational overhead during inference, an expected outcome for inference-time methods. Even with caching (which would increase memory usage), the method remains inefficient compared to fine-tuning-based unlearning.\n- [W3] **Limitation of token-wise filtering.** I'm not entirely convinced with the token-based suppression approach, which can produce brittle or undesirable behaviors depending on the query. For example, if asked “Who is Harry Potter?”, suppressing tokens like not only “wizard” and “J.K. Rowling”,  but also “Harry Potter” could result in blocking outputs that a truly un-knowing retain model could generate such as “Harry Potter was a 19th-century alchemist…”. Such inconsistency can make the model diverge from the retain model’s behavior, which deviate from the stated goal of approximating the retain model."}, "questions": {"value": "- [Q1] During forbidden-token extraction, why is the semantic similarity computed between the input prompt and answers rather than questions in the forget set?\n- [Q2] In the TOFU experiments, should being closer to the retain model be considered better for the F-RL and R-RL metrics? The use of downward arrows (“lower is better”) seems inconsistent with the bolded results, which appear closer to the retain model’s scores.\n- [Q3] In equation 3, the index $i$ does not seem to serve much useful purpose. The equation could be simplified just for a single prompt, in which case we no longer require the attention masks:\n$$\n\\mathbf{z} = \\dfrac{1}{L} \\sum_{j=1}^L h_{j}^{(\\ell)}\n$$\n\n### Typos\n- In the abstract, fine-tuning based unlearning methods are also *approximate* unlearning methods, so maybe italicizing only *at inference time* rather than *approximate unlearning at inference time* would make more sense?\n- Equation 3 has an unnecessary comma in the summation subscript.\n- Dataset notation should be made consistent. Currently both $D_f$ and $\\mathcal{D}_f$ are being used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YbnRTuTN18", "forum": "cuN6DSCS8i", "replyto": "cuN6DSCS8i", "signatures": ["ICLR.cc/2026/Conference/Submission8991/Reviewer_UnJo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8991/Reviewer_UnJo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961234077, "cdate": 1761961234077, "tmdate": 1762920721219, "mdate": 1762920721219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel inference-time unlearning method that removes the need for parameter updates in large language models (LLMs). The approach consists of three key steps: (1) detecting whether an input prompt is related to the forget set, (2) retrieving the relevant texts from the forget set, and (3) guiding the generation process to avoid reproducing those texts. Empirical evaluations on the MUSE and TOFU benchmarks demonstrate that the proposed method achieves state-of-the-art unlearning performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is structured well and has good clarify. It is easy to follow overall.\n2. The evaluation is comprehensive. The baseline methods involve popular existing methods as well as a category of inference-time unlearning methods. The evaluation is conducted with two popular benchmarks MUSE and TOFU.\n2. The results look promising. Through the evaluation, the proposed methods achieve the SOTA performance. Some reasonable ablations are conducted."}, "weaknesses": {"value": "1. The paper does not clearly describe how the dataset for training the MLP classifier is constructed. If the forget data are labeled as 1, how are the samples with label 0 selected? This design choice is crucial, as different negative sampling strategies could significantly affect the classifier’s behavior for certain query groups. Moreover, it is unclear how the method would adapt if the forget set were dynamically updated.\n2. The computation of $P_{sbert}$ involves generating semantic embeddings for sequences of tokens. Would performing this check at each decoding step introduce substantial additional time cost during inference? A discussion or quantitative analysis of this overhead would strengthen the evaluation.\n3. It is unclear which layer $l$ is used in Equation (3)"}, "questions": {"value": "Please check the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Yd1STHERYl", "forum": "cuN6DSCS8i", "replyto": "cuN6DSCS8i", "signatures": ["ICLR.cc/2026/Conference/Submission8991/Reviewer_PCwM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8991/Reviewer_PCwM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996574488, "cdate": 1761996574488, "tmdate": 1762920720835, "mdate": 1762920720835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GUARD (Generation-time Unlearning via Adaptive Restriction and Detection), a novel training-free framework designed to perform LLM unlearning entirely at inference time, thereby avoiding the catastrophic forgetting and utility degradation common in fine-tuning-based approaches. The proposed system operates via a three-stage pipeline: first, a lightweight MLP classifier identifies if an input prompt pertains to the forget set; second, for positive queries, SBERT-based retrieval identifies the original answer and extracts a corresponding set of \"forbidden tokens\"; finally, a core contribution of controlled generation dynamically regulates the output during beam search. This control mechanism is a hybrid, combining a Trie-based, token-level hard matching for exact sequence blocking with an SBERT-based soft semantic matching to penalize semantically similar evasions, applying a total penalty to the token's log-likelihood at each decoding step. Extensive experiments across TOFU, MUSE-News, and Harry Potter benchmarks demonstrate that GUARD effectively prevents outputting target information without compromising the model's general capabilities."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces GUARD, a highly original training-free framework that reframes unlearning as an inference-time output regulation problem. Its core technical novelty lies in the synthesis of a Trie-based hard-matching for exact sequence blocking and an SBERT-based soft semantic matching to penalize evasions. \n\n2. The most significant contribution is demonstrating that unlearning can be achieved without parameter modification, thereby completely avoiding the catastrophic forgetting and utility degradation that plague fine-tuning-based methods. Experiments across three benchmark datasets demonstrate that GUARD achieves a superior trade-off between forget quality and knowledge retention.\n\n3. The paper is well-written and easy to follow, supported by high-quality empirical validation. The authors conduct comprehensive baseline comparisons and detailed additional experiments to verify the effectiveness of the proposed modules."}, "weaknesses": {"value": "While this paper introduces a novel and practical approach to unlearning, there are critical weaknesses as below:\n\n1. The primary concern is the substantial computational overhead introduced \"at inference time.\" The GUARD framework requires executing both Trie matching and SBERT semantic similarity checks for every candidate token at every decoding step.\n- Appendix G (Table 8) confirms this is a critical bottleneck. Even after an SBERT-caching optimization, the method is slower in batch processing than standard beam search (and was slower pre-caching). This latency increase could be prohibitive for real-world, low-latency applications.\n- In this regard, the authors should analyze how inference latency scales as the number of \"forbidden tokens\" increases (e.g., from 100 to 10,000) to evaluate the method's scalability properly. Additionally, the authors need to compare the proposed method's inference time with other training-free methods.\n\n2. The proposed method is fundamentally a sophisticated output filter, not a true parameter-level \"unlearning\" mechanism. The knowledge remains fully intact within the model's weights.\n- The method blocks specific token sequences and their local semantic neighbors. It is not designed to prevent the model from leaking the same information via deep paraphrasing. It is also incapable of unlearning abstract concepts like bias or misconceptions, which are not tied to specific strings. Even though the authors demonstrate the method's robustness in this paraphrasing scenario, I believe there is still a limitation that prevents it from handling all kinds of paraphrasing in the real world.\n- Therefore, I think the authors should frame the work more precisely as \"inference-time content suppression\". The evaluation should be strengthened by testing against diverse adversarial prompts designed to elicit the core concept through complex rephrasing.\n\n3. Several questions arise from the reported experimental results regarding the baselines.\n- NPO (specifically NPO+RT) has been reported in prior work to achieve strong results on the TOFU dataset, particularly in challenging 5% and 10% settings. However, the NPO+RT results in this paper show a significant, unexplained discrepancy from those previously published figures.\n- Additionally, the performance of other baseline algorithms appears to be reported as substantially lower than in established literature.\n- The authors must thoroughly verify and confirm that these baseline algorithms were reproduced correctly. This includes a review of the experimental setup, hyperparameters, and resulting metrics to ensure a fair and accurate comparison."}, "questions": {"value": "I wrote my major weakness/questions in the main Weakness section. I wrote the remaining questions below.\n\n1. Are the experimental results reported in the tables (e.g., Tables 1, 2, 3) the product of a single experimental run or an average over multiple runs (i.e., multiple seeds)? If they are from a single seed, this limits the assessment of the method's stability. \n\n2. The paper's primary trade-off is sacrificing inference time (Weakness 1) to avoid the pitfalls of training. However, recent works (e.g., [1, 2]) have proposed cost-efficient, training-based unlearning using methods like LoRA. Especially, [2] reports that they achieve strong forget/retain trade-offs (for the TOFU dataset) without incurring any additional inference-time cost.\n- What do the authors view as the primary advantage of GUARD when compared to these efficient parameter-lite fine-tuning methods?\n- Can the authors provide a compelling argument or experimental comparison to demonstrate that their inference-time approach is superior to (or necessary in addition to) methods like those in [2]?\n\n\n[1] RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models, NeurIPS 2024\n\n[2] Towards robust and cost-efficient knowledge unlearning for large language models, ICLR 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E8p15Ztu7n", "forum": "cuN6DSCS8i", "replyto": "cuN6DSCS8i", "signatures": ["ICLR.cc/2026/Conference/Submission8991/Reviewer_kj3J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8991/Reviewer_kj3J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996917623, "cdate": 1761996917623, "tmdate": 1762920719468, "mdate": 1762920719468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}