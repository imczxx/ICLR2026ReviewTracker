{"id": "oWKJursYpW", "number": 20589, "cdate": 1758308035972, "mdate": 1759896969216, "content": {"title": "Membership Inference Attacks Against Fine-tuned Diffusion Language Models", "abstract": "Diffusion Language Models (DLMs) represent a promising alternative to autoregressive language models, using bidirectional masked token prediction. Yet their susceptibility to privacy leakage via Membership Inference Attacks (MIA) remains critically underexplored. This paper presents the first systematic investigation of MIA vulnerabilities in DLMs. Unlike the autoregressive models' single fixed prediction pattern, DLMs' multiple maskable configurations exponentially increase attack opportunities. This ability to probe many independent masks dramatically improves detection chances. To exploit this, we introduce SAMA (Subset-Aggregated Membership Attack), which addresses the sparse signal challenge through robust aggregation. SAMA samples masked subsets across progressive densities and applies sign-based statistics that remain effective despite heavy-tailed noise. Through inverse-weighted aggregation prioritizing sparse masks' cleaner signals, SAMA transforms sparse memorization detection into a robust voting mechanism. Experiments on nine datasets show SAMA achieves 30\\% relative AUC improvement over the best baseline, with up to 8$\\times$ improvement at low false positive rates. These findings reveal significant, previously unknown vulnerabilities in DLMs, necessitating the development of tailored privacy defenses.", "tldr": "SAMA: First membership inference attack exposing privacy vulnerabilities in Diffusion Language Models. Achieves 8× better detection than existing methods through robust sign-based aggregation of multiple mask configurations.", "keywords": ["Membership Inference Attack", "LLM", "AI Privacy"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7cb2656ba9e469ecb8bd2afb5ba84662229931f5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents SAMA, a new Membership Inference Attack method tailored to Diffusion Language Models. The authors argue that the iterative masking mechanism in DLMs creates unique opportunities for membership probing and propose an attack that aggregates membership signals across progressively sampled masking configurations using robust sign-based statistics. Experiments on multiple datasets and DLM architectures show that SAMA outperforms prior MIA methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies an emerging gap by studying MIAs on DLMs, which are indeed less explored compared to autoregressive models.\n\n- The formulation of the problem and discussion of bidirectional masking effects are insightful.\n\n- The use of sign-based aggregation to handle heavy-tailed loss noise is conceptually sound."}, "weaknesses": {"value": "- The attack assumes the adversary has grey-box access to both the fine-tuned DLM and its reference pre-trained model. In practice, access to the exact pre-trained model is often unrealistic, especially for proprietary commercial models. In Appendix D.5, we also see that performance drops significantly when the reference diverges. This raises doubts about the practical feasibility and real-world impact of the attack.\n\n- Each attack step involves querying the target model with multiple masking configurations across many subset samples (T×N queries), which could easily require thousands of model queries per instance. This not only makes the attack computationally expensive but also highly detectable in monitored API environments. Standard defenses such as API rate limiting, randomization, or output obfuscation would likely render SAMA ineffective. \n\n- The comparison with baselines seems unfair. SAMA requires orders of magnitude more queries than existing MIA approaches, yet the paper reports results without normalizing for query cost. The reported AUC gains may primarily reflect this increased probing power rather than a superior inference mechanism. A fair evaluation should constrain all methods to the same query budget or analyze performance vs. the number of queries.\n\n- The work assumes Diffusion Language Models are widely deployed in practice. However, as of now, most production LLMs are still autoregressive (GPT, Claude, Gemini, etc.), while DLMs are largely at the research stage. Without stronger evidence that DLMs are being fine-tuned and deployed at scale, the urgency and real-world relevance of the attack remain limited."}, "questions": {"value": "- How realistic is it for an adversary to obtain access to both the fine-tuned DLM and its corresponding pre-trained reference model, particularly for proprietary or commercial systems?\n\n\n- What is the total number of queries required per target sample, and how does the attack’s performance scale with reduced query budgets?\n\n- Could common API defenses such as rate limiting, randomized outputs, or noise injection effectively thwart SAMA?\n\n- How much of SAMA’s reported advantage is attributable to increased query opportunities rather than algorithmic superiority?\n\n- Can the authors provide evidence or examples of Diffusion Language Models being used in real-world production or fine-tuning scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qiqlTNGNsM", "forum": "oWKJursYpW", "replyto": "oWKJursYpW", "signatures": ["ICLR.cc/2026/Conference/Submission20589/Reviewer_9XsJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20589/Reviewer_9XsJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719930596, "cdate": 1761719930596, "tmdate": 1762934000514, "mdate": 1762934000514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SAMA (Subset-Aggregated Membership Attack), a novel membership inference attack tailored for diffusion-based language models (DLMs) in fine-tuning scenarios. Unlike traditional autoregressive models, DLMs generate text through iterative bidirectional masking, which exposes unique privacy vulnerabilities. SAMA exploits these characteristics by employing progressive masking and sign-based subset aggregation to amplify sparse membership signals that emerge under specific mask configurations. Through extensive experiments across multiple datasets and diffusion architectures, the authors show that SAMA achieves substantial improvements over established baselines (previously mainly designed for autoregressive models), such as loss-based and ZLIB attacks, particularly at low false-positive rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on a new problem by systematically investigating membership inference vulnerabilities in diffusion-based language models, a model family that differs fundamentally from autoregressive architectures.\n\n\n2. The proposed SAMA framework combines progressive masking and sign-based subset aggregation to effectively expose membership vulnerabilities.\n\n\n3. The experimental evaluation is extensive and well-controlled, spanning multiple datasets, and various MIA baselines.\n\n\n4. The ablation analysis is informative, isolating the contribution of each component in SAMA.\n\n5. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The proposed method is specialized for diffusion-based language models and depends on access to compatible reference models, limiting its generalizability to broader LLM architectures.\n\n\n2. The intuition behind progressive masking and subset aggregation remains somewhat abstract; a concrete example with simple proof or visualization would make the mechanism more interpretable.\n\n\n3. The method incurs high query complexity due to multiple mask configurations, raising scalability concerns for large models and long sequences (the $m$ can be large).\n\n\n4. The paper focuses primarily on empirical MIA results. Better to have more in-depth discussion on the concrete patterns or examples in the evaluation.\n\n\n5. It is better to discuss the potential defenses strategies for SAMA."}, "questions": {"value": "1. Could SAMA be generalized beyond diffusion-based architectures, for example, to masked or autoregressive language models, by adapting the masking or sampling strategy?\n\n\n2. What is the computational overhead of SAMA in practice? Since it involves repeated masking and querying, quantifying its runtime or query cost relative to traditional MIAs would help assess its feasibility in realistic attack settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WZE2MKP5bg", "forum": "oWKJursYpW", "replyto": "oWKJursYpW", "signatures": ["ICLR.cc/2026/Conference/Submission20589/Reviewer_7q5n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20589/Reviewer_7q5n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890171443, "cdate": 1761890171443, "tmdate": 1762934000043, "mdate": 1762934000043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first systematic investigation into the vulnerability of fine-tuned Diffusion Language Models (DLMs) to Membership Inference Attacks (MIAs). The authors identify that the bidirectional, multi-configuration masking mechanism of DLMs creates a fundamentally different and larger attack surface compared to autoregressive models (ARMs). To exploit this, they propose a novel attack framework named SAMA (Subset-Aggregated Membership Attack). SAMA leverages a progressive masking strategy to probe the model at multiple scales, combined with a robust subset aggregation method using sign-based statistics to handle sparse and noisy membership signals. Through extensive experiments on two state-of-the-art DLMs across nine datasets, the authors demonstrate that SAMA significantly outperforms a wide range of existing MIA baselines, revealing critical and previously overlooked privacy risks in this emerging class of models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-  The work is highly significant and original as it provides the first systematic study of Membership Inference Attacks against the emerging and important class of Diffusion Language Models. It tackles a critical and underexplored privacy problem.\n-  The quality of the work is high. The proposed SAMA attack is technically sound and specifically tailored to the unique properties of DLMs. The empirical evaluation is comprehensive, and the substantial performance gains over a wide range of baselines strongly support the paper's claims.\n-   The paper is written with clarity. The core concepts, especially the insightful comparison between the attack surfaces of ARMs and DLMs, are well-articulated, making the motivation for the proposed method easy to understand."}, "weaknesses": {"value": "- The grey-box access model assumed in Section 2.1 appears to be quite strong. It posits that the adversary can query the target model with arbitrary, custom partially masked sequences and receive detailed outputs like logits for specific token positions. This may not be realistic in many practical scenarios. For instance, with closed-source models, an attacker might only be able to access the final output sequence and its token probabilities via an API. Conversely, with open-source models, an attacker would have full white-box access, including parameters and gradients, which is an even stronger (and different) threat model. The assumed threat model sits in a potentially unrealistic middle ground. The authors should provide a stronger justification for its practicality or discuss how the attack's performance might change under more constrained, API-based settings.\n-   While the components of SAMA are intuitive and empirically effective, their design is largely heuristic and lacks theoretical guarantees. For example, the analogy drawn between progressive masking and multi-scale analysis techniques like wavelet decomposition (line 291) is an interesting intuition but is not rigorously justified; the mathematical mechanisms and application domains are fundamentally different.\n-  A significant concern after reading the methodology is the computational overhead of SAMA. A standard loss-based attack on an ARM requires roughly one forward pass per sample (complexity $O(L)$). In contrast, the proposed SAMA algorithm requires multiple forward passe, e.g. one for each of the $T$ progressive masking steps. This suggests a complexity that is at least $T$ times higher. While the impressive performance gains might justify this extra cost, the authors should discuss this directly. A theoretical analysis and an empirical comparison of the runtime/computational cost of SAMA versus the baselines are essential for evaluating the practical viability of the attack."}, "questions": {"value": "1. The grey-box assumption in Section 2.1 is my primary concern. Could the authors clarify if the cited works for this threat model (Zhai et al., 2024; Duan et al., 2023; etc.) truly operate under this specific assumption for language models (i.e., querying with custom masks to get intermediate losses)? Or is this a stronger variant? How would the attack perform in a more realistic API-based scenario where only the final generated sequence and its token probabilities are available?\n2. Section 3.1 provides an intuitive analysis that multiple masking configurations in DLMs create a larger attack surface. While Section 3.2 points to the main results in Section 4.2 for empirical validation, is there any more direct experimental data that can specifically isolate and demonstrate how varying mask configurations ($S$) impact the membership signal strength ($\\Delta_{DF}(x;S)$)?\n3. The attack seems to rely on sampling a variety of mask configurations with the hope of \"hitting\" one that reveals a strong signal. Is there an analysis of how many samples are needed to achieve good performance? Specifically, how does the attack's performance scale with the number of progressive steps ($T$) and the number of sampled subsets ($N$)?\n4. The subset aggregation method relies on hyperparameters $N$ (number of subsets) and $m$ (subset size). How were the values ($N=128, m=10$) chosen? Could the authors provide a sensitivity analysis showing how these parameters affect the trade-off between attack accuracy and computational cost?\n5. The analysis in Section 3.1 is compelling, but it is presented without direct empirical support within the section itself. Could the authors include a small, illustrative experiment to support the claim that different mask configurations can reveal stronger or weaker signals for member vs. non-member samples?\n6. A minor point on presentation: the manuscript uses a large number of em-dashes for sentence construction, which can hinder readability. I would recommend the authors polish the text to improve flow and clarity, which would help lower the cognitive load for readers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FMzeltOsy0", "forum": "oWKJursYpW", "replyto": "oWKJursYpW", "signatures": ["ICLR.cc/2026/Conference/Submission20589/Reviewer_VbcU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20589/Reviewer_VbcU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926568825, "cdate": 1761926568825, "tmdate": 1762933997021, "mdate": 1762933997021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first systematic investigation of Membership Inference Attack (MIA) vulnerabilities in fine-tuned Diffusion Language Models. It introduces SAMA, a novel attack framework that leverages progressive masking and robust sign-based aggregation to exploit the bidirectional dependencies unique to DLMs. The method demonstrates significant improvements over a wide range of baselines across multiple models and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed SAMA method is innovative since it is the first work to systematically study MIA risks for Diffusion Language Models.\n2. The experimental setup is comprehensive, evaluating SAMA on state-of-the-art DLMs across diverse datasets against multiple baselines. The results are compelling and the ablation studies effectively justify the design choices.\n3. The paper is written in a clear structure."}, "weaknesses": {"value": "1. The SAMA framework involves multiple components (e.g., progressive masking, subset aggregation, adaptive weighting) and hyperparameters. While effective, the method is somewhat complex. A more intuitive explanation and stronger justification for the necessity of each component could enhance clarity.\n2. It would be good to propose or evaluate more defensive strategies to improve practical applicability."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dfK9vVL3zb", "forum": "oWKJursYpW", "replyto": "oWKJursYpW", "signatures": ["ICLR.cc/2026/Conference/Submission20589/Reviewer_aFkU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20589/Reviewer_aFkU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973707690, "cdate": 1761973707690, "tmdate": 1762933996497, "mdate": 1762933996497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}