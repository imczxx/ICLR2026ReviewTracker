{"id": "x6bG2Hoqdf", "number": 14783, "cdate": 1758243613730, "mdate": 1759897349610, "content": {"title": "CALM: Co-evolution of Algorithms and Language Model for Automatic Heuristic Design", "abstract": "Tackling complex optimization problems often relies on expert-designed heuristics, typically crafted through extensive trial and error. Recent advances demonstrate that large language models (LLMs), when integrated into well-designed evolutionary search frameworks, can autonomously discover high-performing heuristics at a fraction of the traditional cost. However, existing approaches predominantly rely on verbal guidance, i.e., manipulating the prompt generation process, to steer the evolution of heuristics, without adapting the underlying LLM. We propose a hybrid framework that combines verbal and numerical guidance, the latter achieved by fine-tuning the LLM via reinforcement learning (RL) based on the quality of generated heuristics. This joint optimization allows the LLM to co-evolve with the search process. Our method outperforms state-of-the-art (SOTA) baselines across various optimization tasks, running locally on a single 24GB GPU using a 7B model with INT4 quantization. It surpasses methods that rely solely on verbal guidance, even when those use significantly more powerful API-based models.", "tldr": "", "keywords": ["LLM", "Algorithm Generation", "Reinforcement Learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43c37897b6b18e4b6b80b5d0cae1ae96f82b954c.pdf", "supplementary_material": "/attachment/98f971b4fccc54856986bb05a91a4abbd83d49ef.zip"}, "replies": [{"content": {"summary": {"value": "This manuscript proposed CALM to generate heuristics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**S1:**  This manuscript is well-structured.\n\n**S2:**  This paper fine-tunes LLMs to generate heuristics, which represents a novel contribution.\n\n**S3:**  The experiments demonstrate the effectiveness of each proposed method."}, "weaknesses": {"value": "**W1:**  This manuscript should introduce more LLM-based Heuristics Generation methods, such as [1,2].\n\n**W2:**  This manuscript lacks experimental comparisons between GRPO and fine-tuning strategies such as DPO.\n\n[1] Efficient heuristics generation for solving combinatorial optimization problems using large language models. KDD, 2025.\n\n[2] Multi-objective Evolution of Heuristic Using Large Language Model. AAAI, 2025."}, "questions": {"value": "**Q1:** The authors measure diversity by counting unique tokens in a heuristic’s idea. Would using a deep neural network to embed each heuristic idea and compute diversity in that embedding space be more effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3NG0MQIy4W", "forum": "x6bG2Hoqdf", "replyto": "x6bG2Hoqdf", "signatures": ["ICLR.cc/2026/Conference/Submission14783/Reviewer_SGF5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14783/Reviewer_SGF5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760962764256, "cdate": 1760962764256, "tmdate": 1762925136844, "mdate": 1762925136844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a GRPO-based automatic heuristic design framework which employs LLMs to iteratively modify optimization codes to generate better performing algorithms. Authors decompose mutation operators into injection and replacement. Diversity-aware crossover and collapse mechanism is introduced to enhance diversity, and a progressive reward function is designed to measure the code quality. Experimental results show that the proposed method surpasses heuristic design baselines on multiple optimization tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. CALM is presented as one of the first LLM-based AHD frameworks to incorporate GRPO\n\n2. This work proposes new heuristic design operators such as injection, replacement and collapse, which could provide insights for future works. The proof and discussion about the introduction of collapse is provided.\n\n3. Experimental results show that CALM outperforms LLM-based heuristic design baselines on multiple combinatorial optimization tasks."}, "weaknesses": {"value": "1. In line 206-207, the authors claim they use compact summaries instead of full code; however, the response examples in Appendix E show generated operators as exact code, like mutation operators in prior LLM-based AHD methods. Additionally, focusing on granular modifications might limit the search space and reduce code diversity.\n\n2. The proposed method uses seed heuristics to generate initial heuristics; however, the authors do not specify the methodology for selecting these seed heuristics. The specific seed heuristics used in experiments are not reported, and the impact of seed heuristics is not discussed. Since the subsequent heuristics are all derived from the seed heuristics, this selection could significantly affect the performance.\n\n3. In reward design, $r_{invalid} \\in (-1, 0)$, $\\alpha_1, \\alpha_2, \\Delta(\\cdot) \\in (0, 1)$ and $\\alpha_1 > \\alpha_2$, meaning $\\alpha_1 r_{invalid}$ (the first term in Eq. (4) ) is more negative than $\\alpha_2 r_{invalid} \\Delta(\\cdot)$ (the second term). This preference for worse steps over no-improvement steps appears questionable. Additionally, in Appendix H.1, authors propose 5 new reward parameters, $r_3 \\sim r_7$ (where are $r_1$ and $r_2$ ? ), which do not appear in Eq. (4). The detail of how they are used is absent. Furthermore, the values of $\\alpha_1$ and $\\alpha_2$ are not reported.\n\n4. In the experiments, baselines are compared under different base LLMs with different knowledge and capabilities, potentially introducing bias. The source of CALM's advantage remains unclear—would Qwen-based baselines outperform it?  For instance, the performance of o4-mini with CALM without GRPO in Table 9 dominates all baselines in both Table 1 and Table 9, potentially reduces the contribution of this paper.\n\n5. The presentation should be improved. For example, line 312 and 319 miss periods at the end of the sentences. Algorithm 1 abuses the equal sign which refers to both equations and assignments."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gHPEKICxNL", "forum": "x6bG2Hoqdf", "replyto": "x6bG2Hoqdf", "signatures": ["ICLR.cc/2026/Conference/Submission14783/Reviewer_ny5L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14783/Reviewer_ny5L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766626807, "cdate": 1761766626807, "tmdate": 1762925136330, "mdate": 1762925136330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CALM, an LLM-based Automatic Heuristic Design (AHD) framework that co-evolves the prompted search over heuristics with on-the-fly RL fine-tuning of the LLM via GRPO. Beyond standard “verbal gradients” (prompt-driven operators), CALM adds “numerical gradients” by rewarding generated heuristics based on feasibility/novelty and relative performance to parent heuristics. It further introduces fine-granularity mutation (injection/replacement), a diversity-aware crossover, and a probabilistic collapse mechanism to escape stagnation. Experiments show consistent gains over prior LLM-AHD baselines, often using only a quantized 7B model on a single 24 GB GPU; ablations indicate GRPO is the main contributor."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is easy to follow and well-written\n    \n2. The idea is good and has practical relevance\n    \n3. Emphasis on local models with low compute budget\n    \n4. Code is provided\n    \n5. Good results compared to some SOTA algorithms"}, "weaknesses": {"value": "1. Authors do not report performance on public benchmarkse.g. TSPLib, which would make it easier to compare CALM against other methods\n    \n2. The performance gain in non-GRPO settings is not too clear. For example, it seems that MCTS-AHD outperforms CALM without GRPO, which makes me wonder about why not finetune MCTS-AHD ‘s LLM too, and if s,o what the performance would be\n    \n3. The novelty of finetuning LLMs for AHD is not too much due to the existence of concurrent works in finetuning, although CALM uses a different algorithm (GRPS vs DPO)"}, "questions": {"value": "1. Why have hardcoded operators via sampling? Would using an LLM generate modifications instead be sufficient?\n    \n2. Convergence with fewer LLM queries appears to be worse than other methods (see Fig 2). How could you explain this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BKPIE1XZZy", "forum": "x6bG2Hoqdf", "replyto": "x6bG2Hoqdf", "signatures": ["ICLR.cc/2026/Conference/Submission14783/Reviewer_9Hpb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14783/Reviewer_9Hpb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910677286, "cdate": 1761910677286, "tmdate": 1762925135871, "mdate": 1762925135871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CALM (Co-evolution of Algorithms and Language Model), a framework for automatic heuristic design where large language models (LLMs) evolve jointly with the heuristics they generate. CALM combines evolutionary prompt search with reinforcement learning fine-tuning (GRPO) to adapt both the prompt and the model, introducing a probabilistic collapse mechanism and a relative reward function to guide stable co-evolution. Experiments on four combinatorial optimization tasks show that CALM outperforms state-of-the-art LLM-based baselines while running efficiently."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The related work section is clearly written and situates the method well within the context of prior research.\n\n2. The paper provides a clear mathematical definition of the task and notations, which are often missing or ambiguous in previous work.\n\n3. The interpretation of LLM-based evolutionary heuristic search as a general reinforcement learning process is insightful and conceptually interesting."}, "weaknesses": {"value": "1. The use of unique token overlap in the “idea” text as a diversity metric feels superficial. Simply counting different words doesn’t necessarily indicate whether two heuristics are algorithmically or behaviorally distinct, so the diversity might be overestimated. It’s efficient, but I think something like embedding-based similarity or even code-level structure comparison would better capture the real heuristic diversity.\n\n2. My biggest concern is still about the RL-based LLM fine-tuning. RL is known to be very sample-inefficient, and here each update needs running and scoring several heuristics, which is computationally expensive. This makes the approach hard to scale to larger problems or longer runs. Also, the performance improvement from fine-tuning (Table 4; Appendix I.2) looks quite moderate and might mainly come from other parts of the design. The fine-tuning seems to make the model more stable and consistent, but I’m not convinced it’s the main reason for the performance boost.\n\n3. Another issue is about the experiments that mix results from GPT and Qwen models. The actual fine-tuning is only done on the open-source Qwen2.5 model with Unsloth. Since only one baseline result is available for Qwen2.5, comparing it with the GPT-4o-mini results feels a bit unfair. I understand the limitation of using closed models, but it would be better to include more baselines tested under the same Qwen model for a fairer comparison."}, "questions": {"value": "1. The co-evolution of prompts and the LLM itself raises legitimate stability concerns, as simultaneous adaptation of two coupled components can yield non-stationary learning dynamics. CALM mitigates this through sequential update scheduling, conservative GRPO fine-tuning with KL regularization, limited LoRA updates, and a collapse mechanism that periodically resets the population (Sec. 4; Appendix C–G). These choices are reasonable and appear to prevent divergence—the training curves in Fig. 2 are smooth, and all tasks are completed successfully. However, the paper provides no explicit stability analysis or ablation contrasting co-evolution with prompt-only training, leaving the robustness of this dual-optimization setup empirically supported but theoretically uncharacterized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jsKakwuPAB", "forum": "x6bG2Hoqdf", "replyto": "x6bG2Hoqdf", "signatures": ["ICLR.cc/2026/Conference/Submission14783/Reviewer_STCg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14783/Reviewer_STCg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998386304, "cdate": 1761998386304, "tmdate": 1762925135475, "mdate": 1762925135475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}