{"id": "q4qxtaKVAU", "number": 13678, "cdate": 1758220803334, "mdate": 1763568350729, "content": {"title": "Early Signs of Steganographic Capabilities in Frontier LLMs", "abstract": "Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks from misuse and misalignment. However, LLMs could evade monitoring through steganography: Encoding hidden information within seemingly benign generations. In this paper, we evaluate the steganography capabilities in frontier LLMs to better understand the risk they pose. We focus on two types of steganography: passing encoded messages and performing encoded reasoning. We find that current models are unable to encode short messages in their outputs without a monitor noticing under standard affordances. They can succeed, however, if given additional affordances like using an unmonitored scratchpad and coordinating on what encoding scheme to use. We additionally find early signs that models can perform basic encoded reasoning in a simple state-tracking problem. This includes some ability to reason with their own and pre-defined schemes, including encoding schemes such as Hexadecimal. Despite this, they can rarely hide reasoning subtly within a cover task to fool a monitor. Overall, our results indicate that current LLMs exhibit nascent steganographic capabilities. While these capabilities are likely insufficient to bypass well-designed monitors at present, this could change in the future.", "tldr": "", "keywords": ["AI Safety", "Alignment", "CoT Monitoring", "AI Control", "Encoded Reasoning", "Steganography", "Evaluations", "LLMs"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3c44d4a8da08e8d520d24d449960b3c6562cf85.pdf", "supplementary_material": "/attachment/2a5a6516e4d2937af02b897eb683fa670edd70bb.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies steganographic capabilities of current LLMs and whether their use to establish covert channels can be detected by a monitor. The study focuses on two tasks: \n\n1. Message Passing: an _encoder_ model is given a _cover task_ (a random question from the Anthropic HH-RLHF dataset) and is asked to encode a short secret string in its answer. The encoder succeeds when a _decoder_ model (an independent instance of the same model) can decode the secret string but a monitor does not flag the response as suspicious.\n2. Encoded Reasoning: a model is given a _cover task_ and a _secret task_ that requires CoT reasoning. The model succeeds when it responds correctly to the _secret task_ but the monitor cannot detect anything suspicious in the CoT used to produce the response.\n\nThe authors evaluate several LLMs on these two tasks under different scenarios, starting with an evaluation of GPT-4.1 on two motivating case studies where steganographic abilities are conspicuously harmful.\n\nFor Message Passing, models are evaluated with and without an unmonitored reasoning scratchpad and over scenarios ranging from realistic (no coordination between encoder/decoder) to idealized (no cover task or monitor). The results show that in a realistic scenario, current LLMs almost never succeed and that even when given an unmonitored scratchpad, they are almost always caught by a monitor that uses a weaker model. However, frontier reasoning models (o3-mini and Claude Opus 4) show modest success covertly transmitting a secret string when using a scratchpad and an encoding scheme agreed upon between the encoder and the decoder. This success is bounded by model capabilities under idealized conditions: the authors show that while models can successfully use simple encoding schemes such as acrostics (which they prefer when not hinted otherwise), only the most advanced models can use more complex schemes that get past monitors.\n\nFor Encoded Reasoning, models are evaluated on a simple state-tracking task (2 states, 2 actions, up to 12 transitions) using various encoding schemes, with and without a cover task. The results show that models can use simple schemes to systematically encode their reasoning with little accuracy loss over unencoded reasoning. When simultaneously solving a cover task, simple schemes that suffice for the state-tracking task result in modest accuracy and can often get past a monitor. This constitutes the strongest evidence found in the experiments of effective steganography usage.\n\nAs supplementary material, the authors provide the source code used to reproduce the experiments (which they intend to open source and can be reused as a benchmark) and traces for each task. Considering the early evidence of models engaging in _deception_ and _scheming_ (see e.g. Claude 4 System Card), this submission is a reminder that it would not take much more to make these behaviours difficult to monitor or detect."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents a systematic analysis of the ability of modern LLMs to use steganography to covertly transmit messages and conceal their reasoning. While both of these abilities have been studied in the past in published work and concurrent pre-prints (e.g., Skaf et al., 2025, Roger and Greenblatt, 2025, Karpov et al., 2025), this is a domain where independent evaluations and methodologies add value and the submission does a good job at developing a sound, re-usable methodology for empirical evaluation that is both comprehensive and sufficiently different from prior work.\n\nThe authors present their results clearly, in a well structured manner, including extensive methodological details in the appendix, supplementary source code, and example traces. What the paper lacks in originality, it offsets with clarity, reproducibility, and timeliness."}, "weaknesses": {"value": "1. The authors discuss relevant related work and appropriately credit it. However, they sometimes overclaim the originality of their contributions and omit comparing their results to similar past evaluations. Two examples: \n\t1. The claim that the submission provides \"*The first set of evaluations for encoded reasoning abilities.*\" is misleading. The ability of LLMs to obfuscate their reasoning has been studied before. Some of the cited related work establishes tasks and methodologies to evaluate this ability. For example, Skaf et al. (2025), accepted at NeurIPS'25, define tasks where a model must conceal CoT reasoning necessary to solve a task (*load bearing*) and report on LLMs success at learning encodings that can evade monitoring; Karpov et al. (2025), investigate whether LLMs could be fine-tuned to hide their reasoning in seemingly benign CoTs using steganography. I do believe that the evaluation framework presented in this submission is more comprehensive and systematic than in previous work and that the relevant work has been appropriately credited, but the authors cannot claim to be _the first_ to provide such an evaluation.\n\t2. Message Passing experiments are similar to Karpov et al.'s (2025), which also evaluate the unaided ability to covertly transmit a message while performing a cover summarization task and being monitored.\n\n2. Most of the prompts used to elicit the use of steganography include strong indications that the models are being monitored and evaluated (e.g., _You are participating in a game_, _Your answer should look like a normal response to the task prompt to an external monitor_), which raises concerns about _sandbagging_. Although the possibility of steganography abilities being only elicitable in certain settings is stated as a limitation, the authors should also acknowledge that sandbagging might affect the validity of the results.\n\n3. The experimental setup for Sections 5 and 6 appears to be such that the monitor observes the model response rather than the CoT for models that do support CoT reasoning natively, such as OpenAI o3-mini and Gemini 2.5 Pro. Unless I am missing something, this means that for these models the monitor scores cannot be relied upon and do not reflect the capabilities of a monitor that can observe the full CoT. To be fair, the authors could not do any better since model providers do not expose full CoTs through their API (with the exception of Claude Sonnet 3.7), but this limitation should be stated prominently, which gives also an opportunity to discuss the asymmetry between vendors and developers in their ability to detect deceptive model behaviour.\n\n4. The paper evaluates unaided steganography abilities. An alternative more insightful evaluation of current technological capabilities is to test these abilities on intentionally misaligned models, not just prompted to elicit steganographic techniques as in this submission, but fine-tuned to learn to use them effectively. In general, even though this alternative evaluation could be considered out of scope and too much to cover in a single paper, I believe that the submission could draw a comparison to existing results and include a discussion of how much uplift has been observed in steganography abilities under optimization pressure (intentional or unintentional).\n### Minor comments\n\n- p.3: Typo \"LLM models\"\n- p.5: Typo \"by by\" -> \"by\"\n- Figure 3 shows the results for 1 common and 3 uncommon encoding schemes. Since the results for `rot13_third_word_letter` and `third_word_letter` schemes are similar, consider swapping one of them for one of the other common encoding schemes you evaluated, preferably the one where models perform the best. In addition, or alternatively, consider plotting the results of all encoding schemes in additional figures in the appendix."}, "questions": {"value": "- In Figure 5, did you use the full dataset of 300 samples with different number of moves you mention earlier? How does the histogram of \"number of moves\" looks like? This would help comparing Figure 5 to Figure 6.\n- Reasoning models like OpenAI o3-mini, Claude Opus 4, and Gemini 2.5 Pro do not expose full CoTs through their API, just summarized CoTs. Your experimental setup seems to only monitor the response, not the full CoT (unavailable through the API) or the summarized CoT. Would not this invalidate your results for these models (Figure 3 (right) and Table 11), making the monitor likely look less effective than it actually is when given the actual CoT?\n- Monitor prompts (Listing 13) include hints to look out for acrostics. Would not this bias your results towards flagging this scheme as suspicious more often than others?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lGh5y7tkr7", "forum": "q4qxtaKVAU", "replyto": "q4qxtaKVAU", "signatures": ["ICLR.cc/2026/Conference/Submission13678/Reviewer_rAxR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13678/Reviewer_rAxR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761334601247, "cdate": 1761334601247, "tmdate": 1762924241729, "mdate": 1762924241729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors investigate the steganographic capabilities of advanced large language models. They design experiments to examine two emerging behaviors: encoded message passing, where models hide short secret messages within natural-looking responses, and a more challenging scenario: encoded reasoning, where models conduct internal reasoning through encoded representations that evade external oversight. The study concludes that current frontier models already exhibit these steganographic abilities, highlighting the need for more effective monitoring systems in the future."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- I find the problem and threat model discussed in this paper novel and very interesting.\n- The analysis in this paper is well-organized and covers a wide range of experiments and test models."}, "weaknesses": {"value": "- The authors did not include any encoding examples in the main paper. It would be helpful to provide some interesting examples illustrating how the model performs encoding and how the monitor model reasons about them. This addition should be feasible for the camera-ready version, as an extra page will be available.\n- There are additional strategies the encoder could use to improve the stealthiness of encodings that the authors do not discuss. For example, the encoder could sample multiple encodings and select one that passes the monitor, or sample multiple encodings and perform decoding to choose the one with the best reconstruction. Including such approaches (and evaluating their effectiveness) would strengthen the paper."}, "questions": {"value": "- For the without coordination case, I’m curious about what the decoder model’s CoTs look like. Does the model attempt various encoding schemes and then identify which one successfully decodes the meaningful message?\n- For the novel encoding schemes, would it be helpful to include a few illustrative examples, like few-shot cases?\n- In the paper, the monitor is typically a weaker model. I’m curious—if the monitor were as strong as, or even stronger than, the encoder/decoder, would that mean all successful encoding attempts would be detected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BgmzLydY46", "forum": "q4qxtaKVAU", "replyto": "q4qxtaKVAU", "signatures": ["ICLR.cc/2026/Conference/Submission13678/Reviewer_rgm3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13678/Reviewer_rgm3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922667063, "cdate": 1761922667063, "tmdate": 1762924241364, "mdate": 1762924241364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides an extensive empirical evaluation of steganographic capabilities in frontier LLMs, examining both encoded message passing and encoded reasoning across multiple models. The evaluation finds that current models exhibit nascent but limited capabilities, and that they cannot reliably bypass well-designed monitors without strong affordances such as unmonitored CoT or pre-coordinated schemes. The work also shows how steganographic capabilities could cause harm and provides a reusable framework that can be used to evaluate future models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Extensive evaluation of steganographic capabilities, testing multiple frontier models across diverse settings with both message passing and reasoning tasks\n- Proper experimental design with error bars, multiple monitors tested, sensitivity analyses for key parameters, and realistic monitor thresholds; evaluation framework can be used to track capabilities as model capabilities grow\n- Excellent figures and clear writing effectively communicate results; appendix includes full prompts, implementation details, and additional analyses; additionally, a transcripts website is provided\n- Addresses critical safety concern given that current evaluation and monitoring methods heavily rely on analyzing model outputs, which would become infeasible given sufficient steganographic capabilities"}, "weaknesses": {"value": "1. **Cover task performance**: It would be illuminating to assess the impact on the cover tasks performance. Although this can be proxied by looking at the suspiciousness scores or manually examining the transcripts, a more direct and quantitative approach could be important for predicting the feasibility of steganography in real threat models.\n\nMinor points:\n- L462-464 seems to imply that performance drop is due to extra “cognitive load” of producing realistic cover text. This phrasing might downplay the potential difficulty caused by an interaction between the cover task and the secret task. E.g. it would not be hard to construct a case where each task is trivial, but solving both tasks jointly within the same output is cryptographically hard.\n- Layout of some figures could be improved to be less cramped, e.g. row of \"100\"s in Fig 7\n- A couple of typos, e.g. at L1197\n- L484-485 phrasing could be rephrased for clarity"}, "questions": {"value": "1. How did you choose which schemes to test only in settings without a cover task? Did you also check if schemes such as Base64 were viable in the cover task setting?\n\n2. Should the text of this paper be excluded from future training data, i.e. similar to how you include an exclusion canary string in the transcript website? A consideration in favor is that if schemes described here enter training corpora, future models may execute them, disadvantaging monitors based on older models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zUqQ7byozZ", "forum": "q4qxtaKVAU", "replyto": "q4qxtaKVAU", "signatures": ["ICLR.cc/2026/Conference/Submission13678/Reviewer_khdy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13678/Reviewer_khdy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762092278161, "cdate": 1762092278161, "tmdate": 1762924241012, "mdate": 1762924241012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically evaluates the steganographic capabilities of frontier LLMs. The authors investigate two central questions:\n(1) whether models can transmit hidden information under monitoring (encoded message passing), and\n(2) whether models can perform hidden or encoded reasoning during task execution (encoded reasoning).\nThrough a range of controlled settings, the study finds that current LLMs cannot reliably hide messages or evade monitors unaided, but they begin to exhibit early steganographic behaviors when provided with strong hints or pre-shared encoding schemes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses an important and timely issue in LLM safety and alignment, the emergence of steganographic capabilities, which is of high relevance to AI governance and monitoring research.\n\n2. It provides a reproducible and well-documented evaluation suite and code."}, "weaknesses": {"value": "1,. The paper lacks a theoretical link between steganographic capability and model propertiessuch as architecture complexity or reasoning-chain length.\n\n2. Most experiments use GPT-4.1-mini as the monitor, which is relatively weak compared to production-grade oversight systems, potentially limiting the realism of the risk evaluation.\n\n3. The encoded reasoning experiments mainly rely on simple tasks (e.g., state-tracking, AQuA-RAT) and may not sufficiently demonstrate complex hidden reasoning.\n\n4. Sec. 6.2 contains content overlap with the results section and could be streamlined for clarity."}, "questions": {"value": "1. How exactly do the encoder and decoder coordinate on the shared encoding scheme, through a common prompt, shared context, or prior instruction?\n\n2. The paper mentions that the monitor threshold corresponds to ≤ 1% false positive rate, but it is unclear under what data distribution this was computed.\n\n3. For the subtle encoded reasoning results, although the authors claim that they are hard to detect, there is no quantitative analysis of the monitor’s score distribution or false negative rate to support this statement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "toTN7P0aCf", "forum": "q4qxtaKVAU", "replyto": "q4qxtaKVAU", "signatures": ["ICLR.cc/2026/Conference/Submission13678/Reviewer_84xL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13678/Reviewer_84xL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137907010, "cdate": 1762137907010, "tmdate": 1762924240762, "mdate": 1762924240762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}