{"id": "scgWlsd9DW", "number": 2835, "cdate": 1757267527410, "mdate": 1759898124255, "content": {"title": "Adapting Rewards to the Agent Using Rational Activation Functions", "abstract": "Reinforcement learning agents vary in representation capacity and credit assignment, yet environment rewards are fixed, leading to miscalibrated gradients, instability, and wasted samples when signals exceed what an agent can effectively internalize. This paper introduces Rational Reward Shaping (RRS), a capacity-aware transformation that converts raw rewards into internal signals aligned with the agent’s learning bandwidth. By combining experience-normalized scaling with a monotone rational activation to reshape curvature and sensitivity while preserving order. The approach can adapt automatically to changing reward regimes and drops into standard actor–critic updates by simply replacing the immediate reward in the target, requiring minimal code changes and no task-specific reward engineering. Integrated into DDPG, TD3, and SAC across six MuJoCo benchmarks, RRS consistently improves average return and stability in both noiseless and perturbed-reward settings, with larger gains under noise. Altogether, RRS offers a simple, broadly applicable way to make reward feedback better calibrated to an agent’s learning dynamics, yielding stronger learning signals without modifying environment design.", "tldr": "Capacity-aware reward shaping that normalizes replay rewards and applies an adaptively tuned rational activation to improve stability.", "keywords": ["Reward Shape"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3cdd634915141d007b2ee748375c6dfd830e798.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- This paper addresses the mismatch between fixed environmental rewards and agent capabilities in deep reinforcement learning (DRL) by proposing a reward shaping scheme that combines empirical normalization (to match reward scales) and a monotonically decreasing rational activation function (to match learning sensitivity), which can be integrated into algorithms like DDPG, TD3, and SAC without modifying their core frameworks.\n\n- The proposed method is validated on 6 continuous control tasks in MuJoCo (covering both noiseless and noisy scenarios), showing improved average returns and training stability under most configurations—with more significant gains in noisy environments—thus demonstrating its effectiveness and anti-interference ability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addressing the issue of \"mismatch between fixed environmental rewards and agent capabilities\" in deep reinforcement learning (DRL), the proposed method directly tackles the problems of improper gradient calibration and unstable training caused by reward signals exceeding the agent’s internalization capacity. This is achieved through a design integrating \"empirical normalization for matching reward scales + adaptive rational activation for matching learning sensitivity\". Notably, in noisy environments with the reward-sensitive DDPG algorithm, it achieves a significant performance improvement, validating the effectiveness of the scheme.\n\n- The method is concise and easy to implement. The monotonic rational activation function dynamically adjusts reward curvature while preserving the reward order. Without modifying the core framework of existing algorithms, it can be integrated into DDPG, TD3, and SAC algorithms merely by replacing the original reward with the shaped reward in the target Q-value calculation—eliminating the need for task-specific reward engineering.\n\n- The experimental validation is comprehensive and robust, covering three dimensions: \"algorithms (DDPG, TD3, SAC) – environments (6 continuous control tasks in MuJoCo, ranging from the simple Ant to the complex Humanoid) – signals (noiseless and multiplicative interference noise scenarios)\". Under most configurations, it enhances both average return and training stability, with more prominent gains in noisy environments. This fully demonstrates the method’s generality and anti-interference capability."}, "weaknesses": {"value": "- The adaptive tuning of α suffers from a domain limitation. The paper assumes that α covers the interval [0.5, 1] via the scaled_sigmoid function; however, the actual input x is always greater than 0 (due to ξ>0), leading to the output of sigmoid(x) being consistently greater than 0.5. Consequently, the actual value range of scaled_sigmoid(x) is only (0.75, 1), violating the initial assumption that \"large α is required for sparse rewards and small α for dense rewards\". This weakens the method’s adaptability to scenarios with low-variance dense rewards, and the paper fails to explain this contradiction.\n\n- Key ablation experiments are absent. The individual effects of \"empirical normalization\" and \"rational activation function\" are not verified independently—neither the performance of using only rational activation (with empirical normalization removed) nor that of using only empirical normalization (with rational activation replaced by conventional activation functions) is tested. As a result, it is impossible to determine whether the performance improvement of RRS stems from reward scale optimization, reward curvature reshaping, or their synergy, which undermines the persuasiveness of the method.\n\n- In Section 3.2, this paper employs a monotonically decreasing rational activation function, which poses unresolved contradictions with the fundamental principles of reinforcement learning (RL). By its very nature, RL optimizes for the maximization of cumulative rewards; however, a monotonically decreasing transformation inverts this objective: maximizing the adjusted reward is effectively equivalent to minimizing the original reward.\n\n- The adaptive tuning of α depends only on reward statistics (inverse standard deviation) and is not linked to the agent’s actual capability indicators (e.g., state dimension). This makes it impossible to quantitatively explain how α matches agents with different capabilities, resulting in a lack of theoretical depth."}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j6toBpSXLW", "forum": "scgWlsd9DW", "replyto": "scgWlsd9DW", "signatures": ["ICLR.cc/2026/Conference/Submission2835/Reviewer_5sb8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2835/Reviewer_5sb8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639546688, "cdate": 1761639546688, "tmdate": 1762916398748, "mdate": 1762916398748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Rational Reward Shaping (RRS), which normalizes rewards and applies a “rational activation function” with an adaptive parameter α to align rewards with agent capacity.\nExperiments on DDPG, TD3, and SAC in MuJoCo show moderate improvements, especially under noisy rewards.\n\nThe paper reads as a heuristic reward transformation with unclear motivation.\n“Rational activation” and “capacity-aware shaping” are not conceptually substantiated.\nDespite clean writing and broad experiments, the work lacks theoretical grounding, internal consistency, and precise presentation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Clear structure and broad experimental coverage.\n- Implementation is simple and integrates easily.\n- Results show small but consistent gains across several environments."}, "weaknesses": {"value": "- While Eq. (2) cites prior work on rational activations, the paper does not explain how that reference conceptually relates to the proposed transformation; there is no discussion in Related Work or Preliminaries clarifying its theoretical basis.\n- The introduction section introduces capacity limitation (L40–L56) with no clear connection to reward shaping. Sentences like “the effectiveness of both exploration-exploitation balance and reward shaping is fundamentally shaped by the agent’s capacity” (L55-L56) are confusing.\n- “Capacity-aware” remains undefined and unmeasured; the method effectively acts as a heuristic rescaling.\n- The adaptive α-update rule lacks theoretical justification or formal analysis of its stability or convergence.\n- The meaning of “improvement” is unclear. It is not specified what baseline the values are computed against, and the meaning of averaging them across environments is questionable. In Table 2, improvements appear relative to the left baseline algorithm, whereas in Table 3 they seem measured against the noiseless setting, causing inconsistency and confusion.\n- Claims such as “higher noise robustness tend to benefits from higher values of α” (L398) are unsupported.\n- Several phrases (e.g., “unlimited” L50, “differences of potentials” L85) require clearer explanations, and multiple citation parentheses are formatted incorrectly throughout."}, "questions": {"value": "Covered within the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WirQ3blLOz", "forum": "scgWlsd9DW", "replyto": "scgWlsd9DW", "signatures": ["ICLR.cc/2026/Conference/Submission2835/Reviewer_6Bqw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2835/Reviewer_6Bqw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884482998, "cdate": 1761884482998, "tmdate": 1762916397722, "mdate": 1762916397722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Rational Reward Shaping (RRS): replace the environment reward in actor–critic targets with a capacity-aware, monotone transformation that combines (i) experience-based normalization from the replay buffer and (ii) a rational activation with an auto-tuned curvature parameter \\alpha driven by recent reward variability. The method is drop-in for DDPG/TD3/SAC and aims to stabilize gradients and improve sample-efficiency without hand-crafted task shaping. On six MuJoCo tasks, RRS variants often improve returns in both noiseless and perturbed-reward settings (with larger gains under noise), while adding little implementation overhead. The paper also analyses alpha’s evolution, reward variability, and runtime."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Simple, drop-in idea with low engineering overhead.\n- Broad coverage (DDPG/TD3/SAC) across several tasks.\n- Analyses of curvature parameter dynamics and reward variability add useful diagnostics.\n- Shows notable improvements in some settings, particularly with reward noise."}, "weaknesses": {"value": "- Transform outputs strictly positive rewards, potentially changing optimal policies (not policy-invariant shaping).\n- Limited ablations\n- Statistical evidence is underpowered for strong claims.\nWhile mean±std are reported, robustness claims would be stronger with more seeds, 95% confidence intervals, and paired significance tests per environment, following best practices (e.g., https://jmlr.org/papers/volume25/23-0183/23-0183.pdf). Seed-level violin plots would clarify variance and overlap."}, "questions": {"value": "- Can you characterise when your monotone, non-affine (and always-positive) transform preserves optimal policies? If not, please reframe as a surrogate objective and discuss bias.\n- Why claim boundedness for your normalisation? Would z-score or min–max over replay be more appropriate, and how would results change?\n- Did you retune or auto-tune SAC’s temperature under reward rescaling?\n\n- Typo to fix line 161 \"a a\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rQnp7kVToX", "forum": "scgWlsd9DW", "replyto": "scgWlsd9DW", "signatures": ["ICLR.cc/2026/Conference/Submission2835/Reviewer_aReJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2835/Reviewer_aReJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894846804, "cdate": 1761894846804, "tmdate": 1762916397362, "mdate": 1762916397362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}