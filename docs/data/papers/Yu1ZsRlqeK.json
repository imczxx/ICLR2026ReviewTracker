{"id": "Yu1ZsRlqeK", "number": 2742, "cdate": 1757230945980, "mdate": 1759898129972, "content": {"title": "MARFT: Multi-Agent Reinforcement Fine-Tuning", "abstract": "The rapid rise of LLM-based agents has led to the emergence of LLM-based Multi-Agent Systems (LaMAS), which show strong potential in complex, collaborative tasks such as presentation generation and even scientific research. While Reinforcement Learning is well-established in enhancing LLM-based agent performance, its success has largely focused on single-agent settings. In contrast, applying Multi-Agent Reinforcement Learning to LaMAS remains limited. This is due to fundamental mismatches between traditional MARL assumptions and the unique dynamics of LaMAS, including action asynchronicity, dynamic organization, characteristic profiles, etc., which present significant new challenges. To address these challenges, we first formalize LaMAS optimization as a Flex-MG, capturing agent heterogeneity and interdependence, and then propose a novel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT), introducing a new optimization framework for LaMAS. Two naive instantiations of MARFT are implemented on the action-level and token-level, and experiments on math problem-solving and coding environments and benchmarks demonstrate MARFT’s effectiveness in improving accuracy and efficiency, establishing it as a principled and generalizable approach for tuning LaMAS. As this work establishes a new paradigm, we conclude by highlighting the limitations of current research and pinpointing promising directions for future work.", "tldr": "", "keywords": ["Multi-Agent Systems", "Large Language Models", "Reinforcement Fine-Tuning", "Multi-Agent Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ada58a4ce34159f189571b87da3718ac70cd28df.pdf", "supplementary_material": "/attachment/b4d486cbdd82fca98fe5c5c10b7426e28316f394.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces **MARFT (Multi-Agent Reinforcement Fine-Tuning)**, a framework that aims to extend reinforcement learning (RL) principles to **LLM-based multi-agent systems (LaMAS)**. The authors first formalize a new game model, the **Flexible Markov Game (Flex-MG)**, which introduces a dynamic dependency function ( D(a_i, a_j) ) to capture conditional relations between agents. Building on this formulation, they propose two concrete implementations:\n\n* **MARFT-A** (action-level fine-tuning), where each agent’s policy is optimized with a PPO-like objective under sequential dependencies;\n* **MARFT-T** (token-level fine-tuning), which treats every token as an action and defines a token-level Bellman backup.\n\nThe authors evaluate MARFT on **mathematical problem-solving** (MATH, CMATH, GSM8K) and **coding** (CodeForces) environments using Qwen2.5 models in single-, dual-, and triple-agent configurations. Experimental results show modest accuracy improvements (e.g., +3 p.p. on MATH500 and +4 points on CodeForces) compared with vanilla multi-agent or single-agent baselines.\n\nThe paper positions MARFT as a new paradigm that unifies large-language-model fine-tuning and multi-agent reinforcement learning by leveraging heterogeneous, dynamically organized agents interacting through language."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses a timely problem, optimizing large language model-based multi-agent systems (LaMAS) through reinforcement learning, and presents a reasonably clear exposition of the proposed framework. The idea of introducing a Flexible Markov Game (Flex-MG) to capture dynamic dependencies among agents is conceptually interesting, and the implementation of two variants (MARFT-A and MARFT-T) demonstrates some engineering effort and reproducibility. The manuscript is well written and organized, with clear figures and experimental details that make the pipeline understandable. Overall, the paper is sound at a conceptual level and fair in its presentation quality, showing adequate awareness of related work and providing a structured attempt to formalize multi-agent fine-tuning for LLMs, even though the originality and practical contribution remain limited."}, "weaknesses": {"value": "The main weakness of this paper lies in its **lack of conceptual clarity and empirical justification**. Although the Flex-MG formulation introduces a dependency function $D(a_i, a_j)$, the paper does not clearly demonstrate how these dependencies are represented or learned in practice. No concrete examples or visualizations in experiment part are provided to help readers understand how one agent’s action influences another, leaving the proposed mechanism largely theoretical and disconnected from real implementation. The **experimental design is weak**, as all evaluations are conducted on mathematical and coding tasks that do not inherently require multi-agent coordination. The “Reasoner–Actor” and “Coder–Reviewer” setups appear artificial and fail to convincingly illustrate any genuine inter-agent interaction or dependency. Moreover, the **performance improvements are minor** and could easily stem from additional fine-tuning rather than from the multi-agent reinforcement framework itself. The paper also **fails to position MARFT relative to RLHF**: it uses standard PPO objectives without explaining the conceptual or methodological distinction from human-feedback-based reinforcement learning, which is currently the dominant paradigm for post-training LLMs. Overall, the work feels **premature and speculative**, with limited novelty beyond reinterpreting existing MARL ideas under LLM settings and insufficient empirical evidence to support its claimed contributions."}, "questions": {"value": "1. **Clarification on action dependency modeling:** Could the authors provide a concrete example or visualization showing how the dependency function ( D(a_i, a_j) ) is computed or updated during training? For instance, how does one agent’s output affect another’s input in the MARFT framework, and how is this handled across asynchronous timesteps?\n\n2. **Justification for multi-agent design in math and coding tasks:** Why are tasks like MATH and CodeForces appropriate for evaluating multi-agent reinforcement learning? Can the authors demonstrate any scenario where coordination between agents (e.g., Reasoner–Actor or Coder–Reviewer) is essential to solving the task, rather than just increasing model complexity?\n\n3. **Connection and difference with RLHF:** The paper repeatedly refers to “reinforcement fine-tuning,” but appears to use standard PPO without human feedback. Could the authors clearly articulate how MARFT differs from RLHF in terms of training signal, objective function, or supervision source?\n\n4. **Empirical evidence of coordination benefits:** Beyond modest accuracy gains, is there any qualitative or behavioral analysis (e.g., communication traces, dependency activations) showing that MARFT leads to more coordinated or efficient agent behaviors?\n\n5. **Scalability and practical feasibility:** The proposed setup requires separate GPUs for each agent and sequential rollouts. How would this approach scale to realistic multi-agent LLM systems with more than three agents or longer tasks? Are there plans for efficient parallelization or off-policy training to address this limitation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8CxpU22QtA", "forum": "Yu1ZsRlqeK", "replyto": "Yu1ZsRlqeK", "signatures": ["ICLR.cc/2026/Conference/Submission2742/Reviewer_xPkj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2742/Reviewer_xPkj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760860929787, "cdate": 1760860929787, "tmdate": 1762916354913, "mdate": 1762916354913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Multi-Agent Reinforcement Fine Tuning (MARFT), which is a framework for optimizing LLM-based multi-agent systems. The authors develop two algorithms: MARFT-A (action-level) and MARFT-T (token-level). The method is theoretically supported and empirically validated."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The formulation is original, and the idea to apply multi-agent RL for LLM agents is novel, to the best of my knowledge. \n- The use of Theorem 1 to justify the sequential nature of the system is sound. It provides a better understanding of the approach.\n- The empirical study is showing that the prorposed approach is effective."}, "weaknesses": {"value": "- If I understand the setting correctly, independent RL can also be applied.  Namely that each agent is not aware of the existence of the others. This can be a valid baseline to compare with but it is not shown in the experiments. Actually there is no baseline in the experiments other than the vanilla performance.\n\n- Following the previous point,  it is therefore hard to evaluate the proposed approach without baselines. So at this moment I cannot tell how useful it is to formulate it at a multi-agent level (rather than single agents)\n\n- Figure 3 contains way more information than is presented in the main contents. Concepts including Central Critic Head, Buffer, \"inst\" are not explained.  So I would suggest to either simplify the figure a lot, or provide more explanation in text.\n\n- Minor: certain notations are missing definition. The advantage function in Theorem 1 is not formally defined."}, "questions": {"value": "- How sensitive is MARFT-A to the order of agent updates? In some cases certain agents can running in parallel rather than sequentially. For example, in figure 1, calendar agent and location agent can run at the same time. Although we can still model it as a sequential problem, does agent ordering affect stability or final performance?\n- In section 5.5, do you mean the implementation is without vllm and sglang, and is it because they cannot be incorporated due to some technical issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u2pBapHjKy", "forum": "Yu1ZsRlqeK", "replyto": "Yu1ZsRlqeK", "signatures": ["ICLR.cc/2026/Conference/Submission2742/Reviewer_Dgkv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2742/Reviewer_Dgkv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618490599, "cdate": 1761618490599, "tmdate": 1762916354688, "mdate": 1762916354688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MARFT (Multi-Agent Reinforcement Fine-Tuning), a new paradigm for optimizing LLM-based Multi-Agent Systems (LaMAS) via reinforcement learning. The authors first formalize the Flexible Markov Game (Flex-MG) to account for agent heterogeneity, asynchronous execution, and dynamic organizational dependencies. Building upon this, they propose MARFT-A (action-level) and MARFT-T (token-level) instantiations, extending PPO-like optimization to multi-agent language systems. Experiments on math problem solving and coding tasks demonstrate consistent performance improvements over vanilla LaMAS baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper generalizes reinforcement fine-tuning from single-agent MARL to LaMAS systems, addressing an important theoretical gap.\n2. The proposed Flex-MG formulation is reasonable and effectively models dynamic dependencies among agents. \n3. The experiments on coding and math problem-solving tasks verify the effectiveness of the proposed method, particularly MARFT-A, which shows stable and consistent improvement."}, "weaknesses": {"value": "1. The method and experiments are limited to single-round tasks, which raises concerns about the framework’s scalability to more complex or multi-turn interactive environments. Could MARFT be extended to handle richer LaMAS settings (e.g., multi-turn reasoning or tool-use workflows)?\n2. The paper lacks comparison with relevant RL-based LaMAS works, such as MAPoRL[1]. \n[1] Park, Chanwoo, et al. \"Maporl: Multi-agent post-co-training for collaborative large language models with reinforcement learning.\" arXiv preprint arXiv:2502.18439 (2025)."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n3dD3PFY1A", "forum": "Yu1ZsRlqeK", "replyto": "Yu1ZsRlqeK", "signatures": ["ICLR.cc/2026/Conference/Submission2742/Reviewer_ojuV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2742/Reviewer_ojuV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890985337, "cdate": 1761890985337, "tmdate": 1762916354540, "mdate": 1762916354540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MARFT (Multi-Agent Reinforcement Fine-Tuning), a novel framework that extends reinforcement fine-tuning from single-agent language models to multi-agent LLM systems.The authors formalize a new theoretical model called Flexible Markov Game (Flex-MG) to represent asynchronous, dependency-driven agent interactions, and prove the Multi-Agent Advantage Decomposition Theorem, which allows global rewards to be decomposed into per-agent advantages.Two concrete instantiations are implemented and evaluated on reasoning (MATH, CMATH, GSM8K) and coding (CodeForces) environments. Results show that MARFT significantly improves multi-agent collaboration performance over standard supervised fine-tuning baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper systematically extends RL-based fine-tuning into the multi-agent LLM regime, which has not been thoroughly studied before.\n- The Advantage Decomposition Theorem provides a clean bridge between joint and sequential optimization, addressing credit assignment across agents.-\n- Well-written and well-organized"}, "weaknesses": {"value": "- Limited experimental scope. Experiments are restricted to relatively small-scale environments and specific LLMs (mainly Qwen-based), which raises questions about generalization to larger or open-ended LaMAS systems.\n- While MARFT is compared to SFT-based LaMAS baselines, it would be valuable to include multi-agent RL algorithms for a more comprehensive comparison, especially MARTI and MAPoRL, which were mentioned in the introduction."}, "questions": {"value": "- How sensitive is MARFT-A to the order of agents (since the framework assumes sequential dependencies)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JEkq8PMNvg", "forum": "Yu1ZsRlqeK", "replyto": "Yu1ZsRlqeK", "signatures": ["ICLR.cc/2026/Conference/Submission2742/Reviewer_AzVe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2742/Reviewer_AzVe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913283148, "cdate": 1761913283148, "tmdate": 1762916354396, "mdate": 1762916354396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}