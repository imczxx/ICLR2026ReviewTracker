{"id": "9ZrjgzlAuh", "number": 15696, "cdate": 1758254068680, "mdate": 1759897288207, "content": {"title": "HDR-4DGS: High Dynamic Range 4D Gaussian Splatting from Alternating-exposure Monocular Videos", "abstract": "We introduce HDR-4DGS, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR) videos captured with alternating exposures. To tackle such a challenging problem, we present a unified framework with two-stage optimization approach based on Gaussian Splatting. The first stage learns a video HDR Gaussian representation in orthographic camera coordinate space, eliminating the need for camera poses and enabling robust initial HDR video reconstruction. The second stage transforms video Gaussians into world space and jointly refines the world Gaussians with camera poses. Furthermore, we propose a temporal luminance regularization strategy to enhance the temporal consistency of the HDR appearance. Since our task has not been studied before, we construct a new evaluation benchmark using publicly available datasets for HDR video reconstruction. Extensive experiments demonstrate that HDR-4DGS significantly outperforms alternative solutions adapted from state-of-the-art methods in both rendering quality and speed.", "tldr": "", "keywords": ["High Dynamic Range", "Gaussian Splatting", "Monocular 4D Reconstrution"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab9b276662b43713380ba760a40a7125fbf22aa4.pdf", "supplementary_material": "/attachment/925ec4a42b21e9d9ff0ce727cd3fc0c51dc37c5e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a system for reconstructing 4D HDR scenes from alternating-exposure monocular LDR videos without known camera poses. The method builds on 3D Gaussian Splatting and proposes a two-stage optimization framework: \n- A video-space stage, which learns dynamic HDR Gaussians in orthographic camera coordinates, eliminating the need for camera poses.\n- A world-space stage, which transforms and refines these Gaussians jointly with camera poses using HDR photometric reprojection.\nAdditionally, a Temporal Luminance Regularization is introduced to ensure temporal consistency of HDR appearance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "-  it is the first to handle alternating-exposure monocular HDR reconstruction.\n\n- Well-designed two-stage optimization, effectively bridging unposed monocular input to HDR Gaussian representation.\n\n- Good experiment results, outperforming both 3DGS- and NeRF-based HDR methods in quality and speed, and comprehensive ablation studies, validating each componentâ€™s contribution."}, "weaknesses": {"value": "- Dependence on multiple vision foundation models (DepthCrafter, RAFT, etc.) makes the pipeline complex and may limit real-time applicability.\n\n- The approach assumes alternating exposure patterns; performance under random or adaptive exposure schedules is not analyzed.\n\n- Large novel view rendering is not demonstrated. It remains unclear whether the reconstructed HDR scenes maintain geometric and photometric consistency under wide view changes"}, "questions": {"value": "- How sensitive is the method to inaccuracies in the exposure timing or camera response function estimation?\n\n- Could the framework generalize to arbitrary (non-periodic) exposure sequences?\n\n- How large is the performance drop if vision priors (depth/flow) are noisy or absent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BQE3Lq53Jl", "forum": "9ZrjgzlAuh", "replyto": "9ZrjgzlAuh", "signatures": ["ICLR.cc/2026/Conference/Submission15696/Reviewer_vqUK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15696/Reviewer_vqUK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751393430, "cdate": 1761751393430, "tmdate": 1762925947267, "mdate": 1762925947267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HDR-4DGS, a two-stage framework for reconstructing HDR scenes from monocular alternating exposure videos. Unlike previous works that handle dynamic Gaussians implicitly, this method explicitly parameterizes the motion of Gaussians, enabling more accurate transformation of video Gaussians into the world coordinate system. Additionally, various optimization losses are introduced to ensure high-quality final rendering."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Explicitly parameterizing the motion of Gaussians not only improves rendering quality but also maintains a relatively fast rendering speed.\n2. The invariance of 2D Gaussian covariance serves as a simple yet effective tool introduced by the authors, which is validated through ablation studies in the paper.\n3. The entire paper is clear and easy to understand."}, "weaknesses": {"value": "1. The division between dynamic and static regions relies on epipolar error maps, so the final results are heavily influenced by them.\n2. The selection of dynamic Gaussians depends on threshold settings, which reduces the generalizability of the pipeline, as determining appropriate thresholds for each scene is not straightforward."}, "questions": {"value": "One of my concerns is that if there is a large viewpoint difference between two frames, then when warping from frame t-1 to frame t, some regions may appear black because certain content in frame t-1 is not visible from the viewpoint of frame t. Would these regions affect the supervision of the TLR loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m5lHjQwQe1", "forum": "9ZrjgzlAuh", "replyto": "9ZrjgzlAuh", "signatures": ["ICLR.cc/2026/Conference/Submission15696/Reviewer_J4n4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15696/Reviewer_J4n4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902671454, "cdate": 1761902671454, "tmdate": 1762925946668, "mdate": 1762925946668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "HDR-4DGS presents a well-motivated approach for reconstructing renderable 4D HDR scenes from unposed monocular LDR videos with alternating exposures. The authors propose a two-stage Gaussian Splatting optimization framework, where dynamic HDR video Gaussians are first learned in orthographic camera space and then transformed to world space for joint optimization with camera poses. The method is complemented by temporal luminance regularization, ensuring temporal consistency of HDR appearance. The experimental evaluation is thorough, including both synthetic and real-world datasets, and demonstrates that HDR-4DGS outperforms  state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper presents the first system to address 4D HDR reconstruction from unposed, single-camera, alternating-exposure LDR videos.\n\n2. The proposed two-stage optimization is effective.\n\n3. HDR-4DGS effectively handles varying brightness across frames, which would break conventional photometric reprojection losses.\n\n4. The paper constructs a new benchmark for HDR video reconstruction including real and synthetic scenes."}, "weaknesses": {"value": "1. Although HDR reconstruction is the core contribution, the paper primarily evaluates PSNR/SSIM on tone-mapped images. No HDR-specific metrics (e.g., PQ-PSNR, HDR-VDP).\n\n2. While quantitative results are extensive, the paper provides limited qualitative discussion on typical failure cases.\n\n3. The approach has not been evaluated on low-light, reflective, or transparent surfaces, which may limit applicability in certain real-world conditions."}, "questions": {"value": "I would like to know how the proposed method performs when dealing with scenes that involve extremely fast and complex motion, where motion blur and ambiguity are present."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6QugcjVlmj", "forum": "9ZrjgzlAuh", "replyto": "9ZrjgzlAuh", "signatures": ["ICLR.cc/2026/Conference/Submission15696/Reviewer_GHAo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15696/Reviewer_GHAo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925963764, "cdate": 1761925963764, "tmdate": 1762925946233, "mdate": 1762925946233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "HDR-4DGS tackles 4D HDR reconstruction from unposed monocular LDR videos with alternating exposures. This problem hasn't been tackled exactly before. Their temporal regularization and 2 stage training to model the world shows superior performance qualitatively and quantitatively over adapted baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem setting is well defined and motivated.\n2. The paper is well written and clear.\n3. The evaluations done are adequate, both quantitatively and qualitatively.\n4. The paper comprehensively ablates all the design features showing the importance/visual effect of each modification."}, "weaknesses": {"value": "1. How does the optmization/loss curves look like with that many losses? Would it be possible to show the curves?\n2. Are all scenes at 24-30fps? Have the authors tried any more challenging settings like faster motion (ex - moving cars for autonomous driving applications?), non-lambertian surfaces etc? Those would be nice to haves but not necessary of course."}, "questions": {"value": "1. Just curious as to why GaussHDR tends to remove the foreground object altogether. Any insights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jh6pOCXOmJ", "forum": "9ZrjgzlAuh", "replyto": "9ZrjgzlAuh", "signatures": ["ICLR.cc/2026/Conference/Submission15696/Reviewer_ww4e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15696/Reviewer_ww4e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928664317, "cdate": 1761928664317, "tmdate": 1762925945880, "mdate": 1762925945880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}