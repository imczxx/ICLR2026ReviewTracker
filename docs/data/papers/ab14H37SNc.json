{"id": "ab14H37SNc", "number": 20935, "cdate": 1758311807740, "mdate": 1759896950860, "content": {"title": "CoM-V2I: Communication-Efficient Multimodal Cooperative Perception via Codebook Pruning and Multiscale Fusion", "abstract": "Cooperative perception, which fuses sensory information from multiple agents to enhance individual agent's perception ability, has emerged as a promising approach to overcome the limitations of single-agent line-of-sight sensing. However, a significant challenge lies in economically deploying sensors across agents while minimizing communication costs and maintaining strong perception performance. To address this challenge, we propose CoM-V2I, a novel framework for Communication-efficient Multimodal Vehicle-to-Infrastructure (V2I) cooperative perception. In CoM-V2I, the road infrastructure is equipped with a high-resolution LiDAR sensor, while vehicles are fitted with cost-effective multi-view cameras to balance performance with economic feasibility. We introduce a residual vector quantization-based codebook representation method to improve communication efficiency by compressing bird's eye view (BEV) feature maps into lightweight indices before transmission. We also propose a codebook pruning method that reduces codebook size by removing low-importance code vectors and combining high-similarity ones, thereby decreasing communication costs with minimal impact on perception performance. Furthermore, we propose a multiscale fusion mechanism that progressively integrates multimodal BEV feature maps from the infrastructure and vehicles, which have different spatial resolutions in a coarse-to-fine manner. Experimental results on the V2X-Real and V2X-Sim datasets demonstrate that the proposed CoM-V2I framework outperforms existing baselines in terms of perception accuracy and communication efficiency.", "tldr": "", "keywords": ["Cooperative perception", "BEV representation", "Multimodality", "Vector quantization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0db042d846d38742e9f7ed07a76438d8eb147428.pdf", "supplementary_material": "/attachment/517411e02ac8d9370896a0c4068d9207aad51d3d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes CoM-V2I, a novel framework for Communication-efficient Multimodal Vehicle-to-Infrastructure (V2I) cooperative perception. It enhances communication efficiency through a residual vector quantization-based codebook representation method and further reduces costs with a codebook pruning technique. Experimental results demonstrate that CoM-V2I surpasses existing baselines in both perception performance and communication efficiency. However, the proposed method primarily constitutes engineering improvements on existing techniques, leading to a slight lack of novelty."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Improved Codebook Utilization: The proposed residual vector quantization-based codebook representation method leverages multiple smaller codebooks, effectively mitigating the issue of under-utilization commonly observed in the training of a single, large codebook.\n\n2.Reduced Communication Bandwidth: The codebook pruning technique further reduces communication bandwidth requirements by achieving a more compact feature representation using fewer indices.\n\n3.Good Performance-Communication Trade-off: CoM-V2I achieves good performance in balancing perception accuracy with communication bandwidth efficiency, which is crucial for practical deployment."}, "weaknesses": {"value": "1.Limited Novelty in Core Components: The residual vector quantization-based codebook representation and the codebook pruning methods presented in this paper appear to be engineering improvements on existing techniques, particularly drawing parallels with Codefilling, thus exhibiting a slight lack of innovation. Furthermore, the multiscale feature fusion module, which combines FPN with self-attention, is a standard design and does not seem to incorporate specific considerations for cooperative perception scenarios.\n\n2.Focus on Infrastructure-Centric Perception vs. Vehicle-Centric Needs: This work focuses on sharing features from connected vehicles to the infrastructure for feature fusion and environmental perception, with the infrastructure then broadcasting the perception results within a fixed range back to the vehicles. However, in practical autonomous driving scenarios, the environmental perception results around the ego vehicle are often of paramount concern, a point seemingly overlooked by this work. For instance, if vehicle agent i and agent j have significant overlapping perception ranges, but both have minimal overlap with infrastructure agent k, the proposed operational flow seems to have limited utility in enhancing agents i and agent j’s perception of their immediate surroundings. Therefore, it is recommended that the authors evaluate the network with vehicles as ego-agents to demonstrate the effectiveness of cooperative perception in practical deployment scenarios."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1YFMkM4oqA", "forum": "ab14H37SNc", "replyto": "ab14H37SNc", "signatures": ["ICLR.cc/2026/Conference/Submission20935/Reviewer_tJVz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20935/Reviewer_tJVz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654957162, "cdate": 1761654957162, "tmdate": 1762939034904, "mdate": 1762939034904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel communication-efficient collaborative perception framework. The main contributions include: (i) a vector quantization-based compression module with codebook pruning; and (ii) a multi-scale feature fusion module. Experimental results demonstrate that the proposed approach achieves a favorable trade-off between perception performance and communication bandwidth on both real-world and simulated datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized and clearly written.\n2. The evaluation is comprehensive, covering both real-world and simulated datasets as well as detection and segmentation tasks."}, "weaknesses": {"value": "1. Sensor generalization. The paper assumes that infrastructure nodes are equipped with high-resolution LiDAR sensors, while vehicles use cost-effective multi-view cameras. It remains unclear how the proposed method generalizes to other sensing modalities. Is the framework limited to this specific sensor configuration?\n2. Effectiveness of codebook pruning. Codebook pruning introduces a potentially lossy operation that modifies the code space. This may create a domain gap between training and inference.\n3. Weak connection between the two innovations. The proposed multi-scale fusion module appears to be weakly related to the codebook pruning mechanism and communication efficiency. The overall narrative might be strengthened by clarifying how these two components interact or complement each other.\n4. Lack of ablation on multi-scale fusion. The effectiveness of the multi-scale fusion design is not sufficiently validated through ablation studies."}, "questions": {"value": "1. Can you quantify any domain shift introduced by codebook pruning between training and inference—reporting accuracy vs. pruning ratio?\n2. What is the concrete interaction between the multi-scale fusion module and codebook pruning in delivering accuracy–communication gains? Please provide ablations isolating each component？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YGbeLgC4DG", "forum": "ab14H37SNc", "replyto": "ab14H37SNc", "signatures": ["ICLR.cc/2026/Conference/Submission20935/Reviewer_gAPv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20935/Reviewer_gAPv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826636680, "cdate": 1761826636680, "tmdate": 1762939034180, "mdate": 1762939034180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CoM-V2I, a communication-efficient heterogeneous cooperative perception framework. It introduces a residual vector quantization (RVQ)-based codebook representation method that compresses BEV feature maps into lightweight indices before transmission. A codebook pruning strategy is further proposed to reduce the codebook size by removing low-importance and merging high-similarity code vectors, thereby decreasing communication costs with minimal impact on perception performance. In addition, a multiscale fusion mechanism is designed to progressively integrate multimodal BEV features from both infrastructure and vehicles in a coarse-to-fine manner."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper has a clear motivation: it aims to address the trade-off between perception accuracy and communication bandwidth in heterogeneous multi-agent cooperative perception systems.\n\nThe idea of combining multiple codebooks with pruning is well-motivated and effectively improves communication efficiency while maintaining strong performance.\n\nExperiments are relatively comprehensive and demonstrate that the proposed framework achieves a good accuracy–bandwidth trade-off in heterogeneous V2I settings."}, "weaknesses": {"value": "The novelty is somewhat limited. The overall structure appears to combine elements of BEVFusion and CodeFilling, where BEV features are quantized via VQ-VAE-like multi-codebook representations.\n\nThe proposed codebook pruning strategy is relatively simple, and the multiscale fusion design resembles commonly used coarse-to-fine fusion approaches in prior work.\n\nThe experimental setup is simplified — each agent uses only one sensor modality, and it remains unclear how the proposed framework adapts to truly heterogeneous modalities or scales to multi-agent scenarios beyond the V2I case."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yjZLz8A7a4", "forum": "ab14H37SNc", "replyto": "ab14H37SNc", "signatures": ["ICLR.cc/2026/Conference/Submission20935/Reviewer_9G1Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20935/Reviewer_9G1Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907764706, "cdate": 1761907764706, "tmdate": 1762939033774, "mdate": 1762939033774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}