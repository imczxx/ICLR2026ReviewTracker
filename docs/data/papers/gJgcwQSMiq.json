{"id": "gJgcwQSMiq", "number": 5895, "cdate": 1757944057907, "mdate": 1762950399634, "content": {"title": "Jointly Optimized Backdoor Attack against Retrieval-Augmented Diffusion Models", "abstract": "Retrieval-augmented diffusion models (RAG-DMs) have gained widespread adoption across various applications, mitigating the data and compute demands of conventional diffusion models. Despite the success, their trustworthiness remains largely unexplored. Prior backdoor attacks have focused either on manipulating the image generation phase or on compromising the retrieval phase under the white-box setting, and they often suffer from knowledge conflicts between retrieved content and user prompts. To investigate the trustworthiness of black-box RAG-DMs, we propose the first jointly optimized backdoor (JOB) attack tailored to RAG-DMs under the black-box setting, which can jointly manipulate the generation and retrieval phases. Specifically, JOB injects a few target-class poisoned images into the knowledge base and learns simply a trigger through multi-objective optimization, guiding retrieval toward poisoned images and aligning the generated image with the target class while preserving benign performance. Experiments show that our method can effectively attack the black-box RAG-DMs with a high success rate compared to state-of-the-art methods.", "tldr": "", "keywords": ["backdoor attacks", "RAG mechanims", "diffusion models", "text-to-image models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0710812f7f963a0cc01021fe70660027904f4cba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Jointly Optimized Backdoor (JOB), a novel backdoor attack specifically designed for retrieval-augmented diffusion models (RAG-DMs) under the black-box setting. JOB introduces a multi-objective reinforcement learning framework that jointly optimizes word-level triggers across three objectives: retrieval success, generation alignment, and linguistic fluency. By poisoning only a small portion of the knowledge base, the method effectively manipulates both retrieval and generation phases to produce attacker-specified outputs while preserving benign behavior. Extensive experiments on multiple RAG-DM architectures and real-world online services demonstrate that JOB achieves high attack success rates and maintains image quality comparable to clean models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Novel and timely contribution**: This work is among the first to explore backdoor attacks on black-box RAG-DMs, a critical yet underexplored threat model for retrieval-augmented generative systems.\n- **Technical originality**: The reinforcement learning–based trigger optimization is an elegant approach that effectively bypasses the gradient inaccessibility in black-box environments.\n- **Strong empirical performance**: JOB achieves significant improvements in attack success rate while maintaining benign accuracy and image quality, demonstrating its practical effectiveness and stealthiness."}, "weaknesses": {"value": "- **Limited scope of discussion.** The paper focuses exclusively on class-conditional generation, without discussing open-ended text-to-image synthesis. It is unclear whether the proposed policy network can generalize to unseen prompts or out-of-distribution domains (e.g., novel text queries). Evaluating this would clarify the general applicability of JOB beyond predefined categories.\n- **Lack of analysis on query efficiency.** The RL strategy relies on rewards obtained from interactions with a black-box RAG-DM. If the method requires a large number of queries, especially when interacting with commercial APIs, the attack cost could be prohibitive. An ablation study analyzing the relationship between query count, attack success rate, and computational cost would make the approach’s practicality clearer.\n- **Insufficient cross-architecture transfer evaluation.** Although the study tests several RAG-DM variants, it does not examine whether triggers optimized for one model can transfer to others with similar architectures or retrievers. Conducting transferability experiments would reveal whether JOB learns model-specific triggers or more generalizable patterns.\n- **Potential unfairness in baseline comparison.** The main experiment adopts a trigger length of six tokens, while baselines such as BadRDM use much shorter triggers (e.g., “ab.”), potentially leading to unfair performance comparisons. Moreover, according to Eq. (6), JOB requires inserting the target class $y$ into the final query, whereas baseline methods do not follow this constraint. Aligning trigger lengths and query formulations would ensure a fairer evaluation.\n- **Limited defense exploration.** The paper only considers two simple defenses, i.e., retrieval filtering and query rephrasing. To improve comprehensiveness, the authors should also evaluate additional defense strategies such as embedding compression or quantization, back-translation, and LLM-based paraphrase generation. These would help assess the robustness of JOB under more realistic defensive settings."}, "questions": {"value": "- The proposed method appears conceptually similar to targeted adversarial attacks, where an adversarial perturbation (in this case, the trigger string) is optimized to induce a specific output. Could the authors clarify the fundamental distinction between JOB and conventional adversarial attacks? In particular, what characteristics make JOB qualify as a backdoor attack rather than an adversarial one, given that the optimization is performed at inference time without model retraining or parameter injection? A clearer conceptual boundary between these two threat models would help position the contribution more precisely.\n- How would JOB perform if the retriever or generator were periodically updated (e.g., fine-tuned with new data)? Does the optimized trigger retain its effectiveness across model updates, or would it require re-optimization?\n- The ASR drops sharply when the trigger length exceeds six. Could the authors provide an explanation for this phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mW80SPFlOB", "forum": "gJgcwQSMiq", "replyto": "gJgcwQSMiq", "signatures": ["ICLR.cc/2026/Conference/Submission5895/Reviewer_xtDm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5895/Reviewer_xtDm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761130197631, "cdate": 1761130197631, "tmdate": 1762918333580, "mdate": 1762918333580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "DxABBlDGAn", "forum": "gJgcwQSMiq", "replyto": "gJgcwQSMiq", "signatures": ["ICLR.cc/2026/Conference/Submission5895/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5895/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762950397363, "cdate": 1762950397363, "tmdate": 1762950397363, "mdate": 1762950397363, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel backdoor approach targeting RDMs, named JOB, to investigate the backdoor vulnerability under black-box settings. To perform the attack, the authors design a multi-objective reward function to optimize the trigger for successfully retrieving the target images while keeping the text fluency for stealthiness.  The authors provide extensive experiments to demonstrate the effectiveness of JOB, which achieves a satisfactory ASR and outperforms existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear and straightforward.\n2. The writing is good, and the figures are fascinating. \n3. The algorithm design is reasonable and appropriate, where each loss function serves as an important component for the attack effectiveness or text fluency.\n4. The authors present sufficient experiments across various models and scenarios, which validate the effectiveness of their methods.\n5. I appreciate the significance of this work as it considers a more practical scenario where the adversary cannot access the retriever."}, "weaknesses": {"value": "While I appreciate that the authors explore the black-box settings of backdoor RDMs and present an effective attack method, I feel uneasy about the trigger design of the proposed method. \n\nSpecifically, the authors incorporate the label of the target class (e.g., banana) into the triggered text. This is somewhat strange and unacceptable to me, as there is generally no information about the attack target within the triggered input in backdoor attacks. This can expose the attacker's goal and incur issues of unfair comparison, since the incorporation of the target label can facilitate the retrieval of target images.\n\nThe inclusion of a target label in the input prompt raises another question: if the prompt contains the word “banana” and the generated image depicts only a banana, should this be considered a successful attack, or merely a normal, unsatisfactory generation that neglects other intended objects or details?\n\nMoreover, the trigger is also kind of lengthy, which introduces an additional advantage over existing methods, such as BadRDM.\n\nConsidering the above aspects, I'm inclined to reject this paper. However, I'm open to discussing with the authors if they can provide convincing clarifications or additional evidence to change my view."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KiOcPzKpUZ", "forum": "gJgcwQSMiq", "replyto": "gJgcwQSMiq", "signatures": ["ICLR.cc/2026/Conference/Submission5895/Reviewer_yqfU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5895/Reviewer_yqfU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381227268, "cdate": 1761381227268, "tmdate": 1762918333157, "mdate": 1762918333157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies backdoor attacks on retrieval-augmented diffusion models (RAG-DMs) under a 'black-box' setting. The proposed JOB method jointly targets both retrieval and generation by (1) injecting a small set of poisoned images into the external knowledge base, and (2) optimizing a textual trigger via a reinforcement-learning-based multi-objective loss that promotes poisoned-image retrieval, target-aligned generation, and linguistic fluency. Experiments on RDM (PLMS/ DDIM) and commercial text-to-image services show good attack success rates than prior methods, while preserving benign utility and image quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n\n2. The proposed RL-based optimization is a natural and reasonable solution."}, "weaknesses": {"value": "1. **Problematic 'Blackbox' Claiming**: The paper incorrectly characterizes its setting as black-box. In fact, all experiments are conducted on the same RAG pipeline, and the only “black-box” component is the text-to-image model. This is essentially the same setting as BadRDM. The only distinction is that the authors avoid using gradient information from CLIP, but not using gradients does not make the system black-box. The attacker still knows the entire RAG mechanism, including the architecture, tokenizers, and word-embedding dictionary of the CLIP-based retriever. Therefore, the setting should be considered white-box rather than black-box. To make the claim legitimate, I suggest the authors evaluate under a true black-box scenario, in which both the T2I model and the RAG system are unknown to the attacker.\n\n2. **Questionable Attack Pipeline**: The backdoor requires the user to provide the complex, JOB-optimized trigger—something an ordinary user is extremely unlikely to input. In effect, only the attacker can reliably activate the backdoor, which makes the threat model self-targeting and the attack practically meaningless. To address this, I suggest that you present some very specific cases to show the damage that JOB may cause in the introduction or background."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pPhWxX4Evv", "forum": "gJgcwQSMiq", "replyto": "gJgcwQSMiq", "signatures": ["ICLR.cc/2026/Conference/Submission5895/Reviewer_Ra2d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5895/Reviewer_Ra2d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761646604184, "cdate": 1761646604184, "tmdate": 1762918332863, "mdate": 1762918332863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a joint optimization backdoor attack method targeting black-box RAG-DMs (Retrieval-Augmented Diffusion Models). The attacker does not modify model parameters but instead optimizes textual triggers and poisoned images within the RAG system to activate the backdoor, causing the model to generate attacker-specified images."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper focuses on a new threat scenario, backdoor attacks specifically designed for RAG-DMs."}, "weaknesses": {"value": "1. **The threat model is unclear.** \n\nIt is difficult to understand what the attack is trying to achieve. From my perspective (correct me if I am wrong), the attacker first injects some tampered images into the RAG document controlled by the deployer. Then, the attacker input a carefully \"crafted prompt\" into the deployer's service to retrieve the tampered image. However, in this setting, normal users or the deployer are unlikely to use such \"crafted prompts\", and the attacker already has the tampered image. Therefore, it is unclear what the real-world impact of this attack is and who the actual victim is.\n\n2. **The method seems easy to defend against.** \n\nThe adversarial trigger actually contains a textual description of the \"desired target class image\" (see Fig. 3). This raises two concerns: (1) does this still qualify as a stealthy backdoor attack? and (2) could the attack be easily filtered due to obvious textual patterns or unnatural sentence structures?\n\n3. The backdoor effect appears limited. \n\nSince the method jointly optimizes both the image and its corresponding textual trigger, there may be cases where the image has an unclear caption or a particular visual content that **cannot be reliably triggered**."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fFesYZqueg", "forum": "gJgcwQSMiq", "replyto": "gJgcwQSMiq", "signatures": ["ICLR.cc/2026/Conference/Submission5895/Reviewer_hzBz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5895/Reviewer_hzBz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998142988, "cdate": 1761998142988, "tmdate": 1762918332585, "mdate": 1762918332585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}