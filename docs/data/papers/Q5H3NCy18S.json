{"id": "Q5H3NCy18S", "number": 7037, "cdate": 1758005702781, "mdate": 1759897876416, "content": {"title": "SPACeR: Self-Play Anchoring with Centralized Reference Models", "abstract": "Developing autonomous vehicles (AVs) requires not only safety and efficiency, but also realistic, human-like behaviors that are socially aware and predictable. Achieving this requires sim agent policies that are human-like, fast, and scalable in multi-agent settings. Recent progress in imitation learning with large diffusion-based or tokenized models has shown that behaviors can be captured directly from human driving data, producing realistic policies. However, these models are computationally expensive, slow during inference, and struggle to adapt in reactive, closed-loop scenarios. In contrast, self-play reinforcement learning (RL) scales efficiently and naturally captures multi-agent interactions, but it often relies on heuristics and reward shaping, and the resulting policies can diverge from human norms. We propose human-like self-play, a framework that leverages a pretrained tokenized autoregressive motion model as a centralized reference policy to guide decentralized self-play. The reference model provides likelihood rewards and KL divergence, anchoring policies to the human driving distribution while preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our method achieves competitive performance with imitation-learned policies while being up to 10× faster at inference and 50× smaller in parameter size than large generative models. In addition, we demonstrate in closed-loop ego planning evaluation tasks that our sim agents can effectively measure planner quality with fast and scalable traffic simulation, establishing a new paradigm for testing autonomous driving policies.", "tldr": "", "keywords": ["Multi-agent reinforcement learning", "traffic simulation", "autonomous vehicles", "planning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1f0c6884064cc616623b98736d2216b13458e3b7.pdf", "supplementary_material": "/attachment/0d57dcc5d11d37799f6e45c41c09f2832cf0a368.zip"}, "replies": [{"content": {"summary": {"value": "* This paper introduces SPACER, a novel framework for training realistic and human-like sim agents that combines RL and IL.\n* The key idea is to have a lightweight, decentralized student RL policy that is anchored to a large pre-trained centralized teacher policy (reference model).\n* The reference model is pre-trained to capture human driving data and provides a realism signal as a reward as well as a KL divergence term to the student during self-play.\n* The KL divergence can be computed efficiently in closed-loop due to RL agent and reference model action space alignment.\nSPACER achieves competitive realism scores with 50x fewer parameters and 10x faster inference as well as lower collision rates and superior reactivity in closed-loop planner evaluation"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper tackles an important practical problem: Creating sim agents that are realistic and human-like, yet, fast and cheap to run.\n* The SPACER results are compelling: The small model performs well on WOSAC on the composite metric and clearly outperforms the baselines in collisions and offroad events.\n* The ablation results clearly show the importance of the KL divergence term.\n* The authors provide good examples of WOSAC weaknesses."}, "weaknesses": {"value": "* VRUs are not controlled and follow logs, which is a significant shortcoming.\n* Unclear benefits of the realism reward signal in the ablation. Also see my question below.\n* While the RL agent inference is fast, training is expensive (first need to train a reference policy, then run inference on it during RL).\n* The HR-PPO baseline is decentralized, which makes it weaker since SPACER can access a centralized reference model during training. A fairer comparison could be a centralized HR-PPO policy distilled into a decentralized one."}, "questions": {"value": "* The ablation shows that the realism reward signal provides hardly any benefit and only the KL term is critical. This result is surprising to me. Do you have an explanation for it? Did you study this more?\n* How sensitive is your method to the size of the discrete action space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T4PGCowPJI", "forum": "Q5H3NCy18S", "replyto": "Q5H3NCy18S", "signatures": ["ICLR.cc/2026/Conference/Submission7037/Reviewer_Vct6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7037/Reviewer_Vct6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525649145, "cdate": 1761525649145, "tmdate": 1762919236798, "mdate": 1762919236798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SPACER (Self-Play Anchoring with Centralized Reference Models), a framework for training large-scale, human-like driving agents in simulation. The key idea is to anchor reinforcement learning (RL) agents trained via self-play to a centralized, pretrained imitation-learning (IL) model. The centralized model serves as a human-likeness reference, providing both a log-likelihood reward and a KL-divergence regularization term that guide the self-play policy toward realistic behaviors while retaining scalability and reactivity. The approach achieves significantly improved realism on the Waymo Sim Agents Challenge compared to baseline self-play RL and imitation-only models, with 10× faster inference and 50× smaller model size."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clearly identifies and addresses the gap between imitation learning (realistic but non-reactive) and self-play RL (reactive but unrealistic).\n\n- Introduces a principled anchoring mechanism through likelihood and KL-based regularization.\n\n- Demonstrates clear ablation and comparative analysis against strong baselines."}, "weaknesses": {"value": "- The paper could elaborate on how anchoring parameters affect the trade-off between realism and exploration.\n\n- The reference model’s dependence on large imitation datasets may limit generalization to low-data domains.\n\n- The related-work section would benefit from citing related studies:\n\nA. Kuefler et al., “Imitating Driver Behavior with Generative Adversarial Networks,” IEEE IV 2017.\n\nR. P. Bhattacharyya et al., “Modeling Human Driving Behavior through Generative Adversarial Imitation Learning,” CoRR 2020.\n\nH. Chen, T. Ji, S. Liu, and K. Driggs-Campbell, “Combining Model-Based Controllers and Generative Adversarial Imitation Learning for Traffic Simulation,” IEEE ITSC 2022.\n\nK. Brown, K. Driggs-Campbell, and M. Kochenderfer, “Modeling and Prediction of Human Driver Behavior: A Survey,” arXiv:2006.08832, 2020."}, "questions": {"value": "How sensitive is the performance to the choice or size of the reference imitation model?\n\nCould the anchoring approach be generalized to other domains (e.g., pedestrian or cyclist simulation)?\n\nDoes the KL regularization ever constrain the policy too strongly, reducing behavioral diversity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aYh1RfWxQU", "forum": "Q5H3NCy18S", "replyto": "Q5H3NCy18S", "signatures": ["ICLR.cc/2026/Conference/Submission7037/Reviewer_nA91"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7037/Reviewer_nA91"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011132131, "cdate": 1762011132131, "tmdate": 1762919236487, "mdate": 1762919236487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPACeR (Self-Play Anchoring with Centralized Reference), a novel framework that integrates the scalability of self-play reinforcement learning with the realism of imitation learning. The key idea is to anchor decentralized self-play policies to a pretrained, centralized tokenized reference model via a KL-divergence alignment and a log-likelihood reward, enabling human-like behavior without relying on logged trajectories. This design allows SPACeR to achieve realistic, reactive, and efficient driving agents that are up to 10× faster and 50× smaller than large generative imitation models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. SPACeR introduces an elegant integration of self-play reinforcement learning with a pretrained tokenized reference model, using KL-divergence alignment and likelihood-based rewards to anchor decentralized policies toward human-like behaviors. The approach is conceptually clean, easy to implement, and avoids heavy reliance on heuristic reward shaping or large generative models.\n\nS2. The proposed lightweight decentralized policy (≈65k parameters) achieves over 10× faster inference and 50× smaller model size than state-of-the-art imitation-learning methods (e.g., SMART, CAT-K), while maintaining comparable realism. This demonstrates clear practical potential for large-scale, real-time autonomous driving simulation and planner evaluation."}, "weaknesses": {"value": "W1. The effectiveness of SPACeR heavily relies on the pretrained tokenized reference model. If the reference distribution is biased or limited in coverage, the learned self-play policies may inherit those biases and fail to generalize to unseen or long-tail behaviors. The paper would benefit from a sensitivity or ablation analysis on different reference model qualities.\n\nW2. The current experiments are restricted to vehicle agents, without incorporating pedestrians, cyclists, or mixed-traffic interactions. Since realistic urban driving often involves diverse and heterogeneous agents, demonstrating SPACeR’s adaptability to such settings would significantly strengthen its generality and practical relevance."}, "questions": {"value": "Q1. How sensitive is SPACeR to the quality and coverage of the reference model? For instance, would using a smaller or domain-shifted tokenized model significantly affect policy realism or stability?\n\nQ2. Could the reference signal (KL and likelihood reward) be updated dynamically or distilled into a lightweight surrogate during training to reduce reliance on a fixed pretrained model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uSodXqD6iN", "forum": "Q5H3NCy18S", "replyto": "Q5H3NCy18S", "signatures": ["ICLR.cc/2026/Conference/Submission7037/Reviewer_tztw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7037/Reviewer_tztw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762090857728, "cdate": 1762090857728, "tmdate": 1762919236154, "mdate": 1762919236154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}