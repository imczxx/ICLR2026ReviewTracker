{"id": "a9bOgeqbdB", "number": 2600, "cdate": 1757161568415, "mdate": 1763461717849, "content": {"title": "RAP: 3D Rasterization Augmented End-to-End Planning", "abstract": "Imitation learning for end-to-end driving trains policies only on expert demonstrations. Once deployed in a closed loop, such policies lack recovery data: small mistakes cannot be corrected and quickly compound into failures. A promising direction is to generate alternative viewpoints and trajectories beyond the logged path. Prior work explores photorealistic digital twins via neural rendering or game engines, but these methods are prohibitively slow and costly, and thus mainly used for evaluation. In this work, we argue that photorealism is unnecessary for training end-to-end planners. What matters is semantic fidelity and scalability: driving depends on geometry and dynamics, not textures or lighting. Motivated by this, we propose 3D Rasterization, which replaces costly rendering with lightweight rasterization of annotated primitives, enabling augmentations such as counterfactual recovery maneuvers and cross-agent view synthesis. To transfer these synthetic views effectively to real-world deployment, we introduce a Raster-to-Real (R2R) feature-space alignment that bridges the sim-to-real gap at the representation level. Together, these components form the Rasterization Augmented Planning (RAP) pipeline, a scalable data augmentation framework for planning. RAP achieves state-of-the-art closed-loop robustness and long-tail generalization, ranking 1st on four major benchmarks: NAVSIM v1/v2, Waymo Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results demonstrate that lightweight rasterization with feature alignment suffices to scale end-to-end training, offering a practical alternative to photorealistic rendering. Code will be released.", "tldr": "Photorealism is unnecessary for robust end-to-end driving; our RAP framework leverages lightweight rasterization and feature alignment to scale training with large-scale synthetic samples, achieving state-of-the-art performance across benchmarks.", "keywords": ["Autonomous Driving", "Planning", "Sim-to-Real"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec74fc0241c89fa1f19513b966f0cf2726b8e118.pdf", "supplementary_material": "/attachment/0d0255c8f5c4d4d2a99b0bfcb63be86d6db1dcb7.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical problem of brittleness in end-to-end autonomous driving models trained via imitation learning. The authors argue that this brittleness stems from the lack of \"recovery\" data, as models are only exposed to expert demonstrations. To overcome this, they propose Rasterization Augmented Planning (RAP), a scalable data augmentation framework.\n\nThe core contribution is a lightweight 3D rasterization pipeline that generates synthetic camera views from annotated geometric primitives (e.g., agent cuboids, lane polylines). This approach deliberately forgoes photorealism, arguing that semantic and geometric fidelity are sufficient for training robust driving planners, and that this method is far more scalable than computationally expensive alternatives like neural rendering or game engines.\n\nThis rasterization pipeline enables two key data augmentations: (1) **Recovery-oriented perturbations**, which simulate off-expert-path maneuvers to teach the policy how to recover from mistakes, and (2) **Cross-agent view synthesis**, which re-renders scenes from the perspective of other agents to dramatically increase the volume and diversity of training data.\n\nTo bridge the domain gap between the abstract rasterized views and real-world images, the paper introduces a Raster-to-Real (R2R) alignment module. R2R operates in the feature space, using a combination of spatial-level feature distillation (aligning real features to \"cleaner\" raster features) and global-level adversarial adaptation to enforce domain invariance.\n\nThe authors demonstrate the effectiveness of RAP through extensive experiments, achieving state-of-the-art results on four major benchmarks: NAVSIM v1/v2, the Waymo Open Dataset (WOD) Vision-based E2E Challenge, and Bench2Drive. The results show consistent improvements in closed-loop robustness and long-tail generalization, not only for their proposed model but also when RAP is applied to existing state-of-the-art planners."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Pragmatic and Well-Motivated Core Idea:** The central thesis—that photorealism is not necessary for training robust E2E planners and that semantic/geometric fidelity is sufficient—is compelling and challenges a prevailing trend in the community. This provides a practical, scalable, and computationally efficient alternative to the prohibitively expensive photorealistic simulation pipelines (NeRFs, 3DGS, etc.). This insight is a significant conceptual contribution.\n2. **Exceptional Experimental Validation:** The empirical evidence supporting the paper's claims is extensive and highly convincing.\n    - **Breadth:** Achieving #1 ranking on four diverse and challenging benchmarks (NAVSIM, WOD, Bench2Drive) is a remarkable feat and strongly validates the method's effectiveness and generalizability.\n    - **Rigor:** The evaluation covers both open-loop metrics (WOD) and, more importantly, pseudo-closed-loop (NAVSIM v2) and full closed-loop (Bench2Drive) simulations, directly addressing the core problem of closed-loop brittleness.\n    - **Model-Agnosticism:** The authors demonstrate that RAP is not just a component of their specific model but a general framework that provides significant gains when applied to other SOTA models (e.g., `RAP-iPad`, `RAP-DiffusionDrive`). This greatly strengthens the paper's impact.\n3. **Thorough and Insightful Ablation Studies:** The paper includes a comprehensive set of ablations that methodically validate the key design choices.\n    - The study on rasterization design (Table 5) clarifies why specific choices like solid faces and depth decay are important.\n    - The ablation on recovery perturbations (Table 6) directly links this augmentation to improved performance on the closed-loop-oriented NAVSIM v2 benchmark.\n    - The analysis of the R2R alignment module (Fig. 5) clearly shows the benefit of both spatial and global alignment.\n    - The scaling curve for cross-agent synthesis (Fig. 6) is particularly strong, demonstrating a clear log-scaling law that mirrors findings on real data scaling. This provides powerful evidence for the value of the generated synthetic data.\n4. **High-Quality Presentation:** The paper is well-written, clearly structured, and easy to follow. The figures, especially the comparative illustration in Figure 1 and the system overview in Figure 2, are excellent and effectively communicate the core concepts."}, "weaknesses": {"value": "1. **Oversimplification of the Static World:** The rasterization pipeline represents the world as a set of key primitives against a simple (often black) background. While the ablations show this is effective, it discards a vast amount of visual context from the static environment (buildings, foliage, non-annotated road signs, etc.). This simplification might be a vulnerability in complex urban scenes where such context is semantically important for driving decisions (e.g., navigating a complex, non-standard construction zone).\n2. **Information Loss.** A primary weakness of the RAP framework is the inherent information loss resulting from its reliance on abstract 3D rasterization. By training the real-world encoder to align its features with a \"clean\" representation derived solely from annotated primitives (e.g., agent cuboids and lane lines), the model is implicitly taught to ignore any visual information not present in the labels. This poses a significant safety risk, as the system may fail to perceive critical, unannotated hazards such as road debris, potholes, temporary construction signs, or police officers directing traffic. Furthermore, this abstraction discards subtle but predictive visual cues, like wet road surfaces or holes in the road, which are crucial for nuanced, proactive driving. While this approach demonstrably improves robustness against common visual variations like weather and lighting, it does so at the cost of creating a potential \"blindness\" to novel or out-of-annotation long-tail events, a critical limitation for real-world deployment.\n3. **RL Training.** A weakness is the paper's narrow focus on imitation learning, which fails to exploit the full potential of the proposed RAP framework. The authors have effectively built a fast and scalable driving simulator, an ideal environment for reinforcement learning (RL). By restricting their method to data augmentation for IL, they miss the opportunity to use RL to train an agent that could learn through active exploration, optimize for more complex rewards, and potentially discover policies superior to the original expert. This confinement to IL undersells the power of their core contribution, leaving its most transformative application as an RL training engine unexplored."}, "questions": {"value": "1. Regarding the R2R alignment module: The spatial alignment loss (Eq. 2) updates the real features `F^r` to match the frozen raster features `F^s`, treating the latter as a \"clean\" supervision signal. Could you elaborate on the intuition behind this directional alignment? Did you experiment with a symmetric loss or alternative objectives where both feature distributions are jointly optimized?\n2. Regarding the scaling laws (Fig. 6): The log-scaling behavior with cross-agent data is very promising. What do you theorize is the limiting factor for this scaling? Does the performance plateau come from the finite variety of agent behaviors in the log, or from the inherent domain gap that cannot be fully closed by R2R alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WBcqgVpb7G", "forum": "a9bOgeqbdB", "replyto": "a9bOgeqbdB", "signatures": ["ICLR.cc/2026/Conference/Submission2600/Reviewer_y7Na"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2600/Reviewer_y7Na"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2600/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760585191000, "cdate": 1760585191000, "tmdate": 1762916298735, "mdate": 1762916298735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose that 3D rasterization of annotated primitives are semantically similar to images, so it can be used as input for end-to-end driving. By doing so, multiple data augmentation tricks can be applied to end-to-end driving, without costly image rendering. A raster-to-real alignment is further proposed to align the two representations in feature spaces."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* For the input of end-to-end driving models, a 3D rasterization representation is proposed to be the substitute of raw images , which is interestingly to already have similar semantics in DINOv3 feature space.\n* With this representation, two types of augmentation are applied in the training of end-to-end driving model, with the need for costly image-space rendering in previous methods.\n* A raster-to-real alignment module is proposed to enforce feature consistency between rasterized and real inputs at both spatial and global levels."}, "weaknesses": {"value": "* On NAVSIM and WOD benchmarks, the method achieves SOTA performance. However, on the real close-loop benchmark Bench2Drive, the performance gain is incremental compared with baseline, which is contrary to Recovery-oriented perturbations."}, "questions": {"value": "* Any rules considered in Recovery-oriented perturbations, for example, not collide with nearby vehicles?\n* Why use minADE as metric in ablation study?\n* In Fig.5, why the performance of 100% real data is not the best?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tIxp5DXwAp", "forum": "a9bOgeqbdB", "replyto": "a9bOgeqbdB", "signatures": ["ICLR.cc/2026/Conference/Submission2600/Reviewer_HRhi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2600/Reviewer_HRhi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2600/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905298082, "cdate": 1761905298082, "tmdate": 1762916297256, "mdate": 1762916297256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an end-to-end driving framework that uses 3D rasterization instead of costly photorealistic rendering. By focusing on semantic and geometric fidelity and aligning synthetic with real data in feature space, RAP enables more flexible data augmentation and achieves robustness and generalization across multiple driving benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- replaces expensive photorealistic rendering, offering a controllable way to generate diverse driving scenarios.\n- provides experiments and ablations across multiple major benchmarks (NAVSIM, Waymo, Bench2Drive), consistently achieving strong results\n- bridges synthetic and real data efficiently, reducing the sim-to-real gap without costly photorealistic rendering"}, "weaknesses": {"value": "- RAP remains within the imitation learning framework, so it still can display issues like causal confusion and lack of active policy improvement.\n- the simplified rasterized scenes may miss fine-grained visual cues or rare real-world conditions that could matter for extreme edge-cases"}, "questions": {"value": "- do the authors plan to evaluate RAP outside of the IL setting? rl evaluations would demonstrate if the model can generalize under agent exploration\n- can the authors provide additional examples showing reconstruction results on very OOD scenes? e.g. scenes with objects that have very little representation in the training dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qPHVBNGTiL", "forum": "a9bOgeqbdB", "replyto": "a9bOgeqbdB", "signatures": ["ICLR.cc/2026/Conference/Submission2600/Reviewer_eDtm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2600/Reviewer_eDtm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2600/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762816708134, "cdate": 1762816708134, "tmdate": 1762916296117, "mdate": 1762916296117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}