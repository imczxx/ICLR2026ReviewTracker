{"id": "T5wkZJqzkz", "number": 13070, "cdate": 1758213262056, "mdate": 1759897467434, "content": {"title": "How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining", "abstract": "Curriculum learning is a powerful paradigm, yet its application to large language model (LLM) pretraining remains underexplored, especially in scenarios where high-quality data is limited yet crucial. Previous works have primarily focused on applying curriculum learning to LLM pretraining by searching for better data quality metrics. However, these approaches have yielded only marginal gains, and curriculum-based training is still not a standard practice. In this work, we explore the problem from the opposite perspective: if a good quality metric is available, can current curriculum learning strategies produce better results? We diagnose a key, yet overlooked, factor responsible for this deficiency: the interplay between the data order and the learning rate (LR) schedule. We find that while curriculum learning can greatly outperform pretraining with a uniform data distribution under a constant LR schedule, this advantage diminishes as the learning rate decays. Building on this observation, we propose replacing LR decay with model averaging, which involves computing a weighted average of last several model checkpoints. We find this strategy achieves better results than standard LR decay schedules, especially in a mid-training regime where only a portion of high-quality data is available. Furthermore, this approach reveals that model averaging is greatly strengthened with the occurrence of curriculum learning. Finally, we propose a co-designed strategy for curriculum-based LLM pretraining: combining a moderate LR decay with model averaging. This approach allows the model to strike a balance between learning effectively from high-quality data, reducing knowledge forgetting, and mitigating gradient noise. We find that this combination highlights a previously overlooked opportunity to improve pretraining by co-designing the data curriculum, LR schedule, and model averaging.", "tldr": "Use model weight average to enhance curriculum learning in LLM pretraining.", "keywords": ["LLM pretraining", "Curriculum Learning", "Model Weight Average"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91c92acb0fac50150ef15e55ff274ee5d99a5739.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper argues that **curriculum-based pretraining** (train low→high quality) quietly clashes with the usual **decaying learning-rate (LR)** schedules: when the best data finally arrives (late in training), the LR is tiny, so the model barely learns from it. Experiments with a **1.5B** model on **30B** tokens show that curricula look strong under a **constant LR**, but their gains **shrink under cosine/WSD decay** (Figure 1–2). The authors propose **Curriculum Model Averaging (CMA)**: keep a high/constant LR during training, then do **model averaging** over the last checkpoints (SMA/EMA/WMA) to stabilize. They further recommend **co-designing** a *moderate* LR decay with model averaging (**CDMA**), which outperforms standard decay with uniform order and also beats “decay + curriculum” alone, with especially clear gains in **mid-training** when only some high-quality data is available (Table 1–2; Figure 5). A simple theory sketch supports why curriculum + averaging can keep strong updates from high-quality data while still reducing noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper **clearly diagnoses** an intuitive but under-discussed coupling: high-quality data arrives when LR is tiny, dulling its impact. The **constant-LR** comparisons make this visible. \n- **CMA** is **simple and actionable**: keep LR high, then **EMA/SMA** the last checkpoints; implementation details are explicit.  \n- **CDMA** (moderate decay + averaging) finds an **under-explored optimum** that beats standard decay+uniform and decay+curriculum, especially in **mid-training**."}, "weaknesses": {"value": "- The study focuses on **one model size (1.5B) and a 30B-token corpus**; it’s unclear whether the same sweet spots hold for much larger models or different corpora/metrics. \n- **Baselines are limited** for some comparisons (e.g., stronger **learned curricula** or **adaptive/variance-aware** data-selection methods aren’t included), so it’s hard to judge competitiveness against the latest dynamic strategies. \n- The **mid-training** setting is promising but still tied to the specific phasing and quality signals here; more domains or public corpora would help establish external validity."}, "questions": {"value": "Do the CMA/CDMA gains persist for **larger models** (e.g., 7B/13B) and for other **quality metrics** (beyond DCLM/PreSelect)? Please include at least one bigger-model run."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xMcRvquPpJ", "forum": "T5wkZJqzkz", "replyto": "T5wkZJqzkz", "signatures": ["ICLR.cc/2026/Conference/Submission13070/Reviewer_QrZN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13070/Reviewer_QrZN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760860389451, "cdate": 1760860389451, "tmdate": 1762923796464, "mdate": 1762923796464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a critical and often-overlooked issue in curriculum-based LLM pretraining: the detrimental coupling between the data curriculum and the learning rate (LR) schedule. The authors diagnose that standard LR decay schedules, which are designed to reduce noise and stabilize training, conflict with the goal of curriculum learning (CL), which places the most valuable, high-quality data at the end of training. They frame this conflict through the lens of the LR's \"dual role,\" acting simultaneously as an update step size and an implicit importance weight for data. By decaying the LR, standard methods effectively \"waste\" the best data by learning from it with minimal update steps.\n\nTo resolve this, the paper proposes to decouple these two roles. Their primary solution, Curriculum Model Averaging (CMA), replaces aggressive LR decay with a constant high LR during training, relying on model averaging (e.g., EMA or SMA) over the final checkpoints to ensure stability. They further propose a co-designed strategy, CDMA, which combines a moderate LR decay with model averaging. Through extensive experiments on a 1.5B parameter LLM, the authors demonstrate that their proposed methods outperform standard pretraining baselines, especially when compared to the widely-used cosine schedule. The work identifies an \"optimal area\" of moderate decay where co-designing the curriculum, LR schedule, and model averaging yields the best results, highlighting a previously underexplored optimization regime."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  The paper's primary strength lies in its clear and intuitive diagnosis of a fundamental conflict in modern pretraining. \n\n2.  The authors provide strong empirical validation for their central thesis. \n\n3.  The proposed solutions, CMA and CDMA, are not ad-hoc but are direct, logical consequences of the initial diagnosis."}, "weaknesses": {"value": "1.  While the paper's framing is compelling, the core ideas—the conflict between CL and LR decay, and the use of model averaging for stabilization—are not entirely new. The provided literature survey highlights several precedents. For instance, Weinshall & Amir (2020) theoretically showed that optimal CL requires non-decaying LRs, and Jiang et al. (2021) empirically demonstrated that LR decay undervalues late-stage data. Similarly, model averaging techniques like SWA (Izmailov et al., 2018) have long been proposed as alternatives to LR decay. The paper could strengthen its contribution by more explicitly positioning its work against these specific precedents.\n\n2.  The experiments are missing a key and highly relevant baseline: training exclusively on a high-quality data subset for the same computational budget (e.g., by filtering out the bottom 80% of data and repeating the top 20%). Without this comparison, it is difficult to disentangle the benefits of the curriculum's data ordering from the benefits of simply focusing compute on high-quality data. If this simple filtering baseline performs comparably to CMA/CDMA, it would challenge the necessity of the curriculum itself.\n\n3.  The paper observes that a descending (high-to-low quality) curriculum performs poorly but misses an opportunity for deeper analysis. This ordering, when paired with a standard LR decay, seems intuitively synergistic (high LR on high-quality data, low LR on low-quality data).\n\n---\n\n[1] Weinshall, D., & Amir, G. (2020). On the Theory of Curriculum Learning. *Advances in Neural Information Processing Systems*.\n\n[2] Jiang, L., et al. (2021). Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt. *International Conference on Machine Learning*.\n\n[3] Izmailov, P., et al. (2018). Averaging Weights Leads to Wider Optima and Better Generalization. *Uncertainty in Artificial Intelligence*."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dChOlIR6UE", "forum": "T5wkZJqzkz", "replyto": "T5wkZJqzkz", "signatures": ["ICLR.cc/2026/Conference/Submission13070/Reviewer_qZNE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13070/Reviewer_qZNE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760944468233, "cdate": 1760944468233, "tmdate": 1762923796218, "mdate": 1762923796218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the overlooked interaction between data curriculum strategies and learning rate decay in LLM pretraining. The authors argue that when high-quality data are placed at the end of a curriculum schedule, the model fails to fully learn from these data since the LR has already decayed. To address this issue, the paper proposes Curriculum Model Averaging (CMA), which combines a quality-based curriculum with model averaging, replacing LR decay with CMA to focus the final model on high-quality signals. The authors also proposed CDMA, which combines CMA with LR decay. Experiments on a 1.5B-parameter model trained on 30B tokens show that CMA/CDMA consistently improve both validation loss and downstream task performance, especially on Core benchmarks (max.+2.50%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper identifies a realistic but previously underexplored issue -- the negative coupling between LR decay and quality-based curricula. The authors proposed a simple yet effective method, using checkpoint averaging to counteract LR over-decay, without additional retraining or architecture changes. The approach can be easily integrated into standard pipelines, and is computationally inexpensive. Theoretical modeling and gradient trajectory visualization illustrate how the proposed method alleviates the loss of learning signal caused by LR decay in curriculum learning."}, "weaknesses": {"value": "Results are mostly at 1.5B model parameters and 30B tokens. It remains uncertain whether the same trends hold for much larger models (e.g., 7B–70B) or longer training runs. The paper also lacks exploration of hyperparameters for checkpoint averaging (decay factor, checkpointing interval, number of checkpoints) or justification for the selected hyperparameter values."}, "questions": {"value": "1. How does CMA/CDMA perform on larger models (e.g., >=7B) or longer training (e.g. 100B+ tokens)? It would be better if you provide at least one larger-scale validation experiment to improve the generality of your claims.\n2. How did you tune the hyperparameters or select their specific values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GcdfCWqYkg", "forum": "T5wkZJqzkz", "replyto": "T5wkZJqzkz", "signatures": ["ICLR.cc/2026/Conference/Submission13070/Reviewer_vPHj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13070/Reviewer_vPHj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790621656, "cdate": 1761790621656, "tmdate": 1762923795958, "mdate": 1762923795958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}