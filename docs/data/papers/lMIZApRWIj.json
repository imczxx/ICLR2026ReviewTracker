{"id": "lMIZApRWIj", "number": 7825, "cdate": 1758037784113, "mdate": 1759897829829, "content": {"title": "Meta Pruning via Graph Metanetworks : A Universal Meta Learning Framework for Network Pruning", "abstract": "We propose an entirely new meta-learning framework for network pruning. It is a general framework that can be theoretically applied to almost all types of networks with all kinds of pruning and has great generality and transferability. Experiments have shown that it can achieve outstanding results on many popular and representative pruning tasks (including both CNNs and Transformers). Unlike all prior works that either rely on fixed, hand-crafted criteria to prune in a coarse manner, or employ learning to prune ways that require special training during each pruning and lack generality. Our framework can learn complex pruning rules automatically via a neural network (metanetwork) and has great generality that can prune without any special training. More specifically, we introduce the newly developed idea of metanetwork from meta-learning into pruning. A metanetwork is a network that takes another network as input and produces a modified network as output. In this paper, we first establish a bijective mapping between neural networks and graphs, and then employ a graph neural network as our metanetwork. We train a metanetwork that learns the pruning strategy automatically and can transform a network that is hard to prune into another network that is much easier to prune. Once the metanetwork is trained, our pruning needs nothing more than a feedforward through the metanetwork and some standard finetuning to prune at state-of-the-art. Our code is available at https://anonymous.4open.science/r/MetaPruning", "tldr": "We propose an entirely new universal meta-learning framework for network pruning. It has great generality and transferability and gets outstanding results.", "keywords": ["Model Pruning", "Model Compression", "Meta Learning", "Metanetwork", "Networks For Networks"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa3854620ee1edcde720ac85212c9999697065ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a universal meta-learning framework for network pruning built around a graph metanetwork: instead of crafting new importance scores, a learned metanetwork takes a model that is hard to prune (under any chosen criterion like ℓ₁/ℓ₂) and transforms it into an easy-to-prune model; practically, the network is converted to a graph and passed through a GNN (PNA backbone) that outputs a modified network, after which standard pruning and finetuning suffice, and the trained metanetwork can transfer across datasets and architectures without per-task retraining—thus contributing (i) the introduction of metanetworks to pruning with a concrete graph-based implementation, (ii) a universal, criterion-agnostic pipeline requiring only feed-forward + finetuning, and (iii) empirical evidence of strong performance on ResNet56/CIFAR-10, VGG19/CIFAR-100, and ResNet50/ImageNet, alongside the finding that finetuning after the metanetwork is necessary to recover accuracy and improve robustness."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper reframes pruning via a metanetwork that takes a neural network as input and outputs a modified, easier-to-prune network—implemented as a graph metanetwork with a PNA-based GNN backbone—rather than inventing a new saliency score. It further argues broad, criterion-agnostic applicability and highlights transferability once a metanetwork is trained.\n\n2. Strong empirical evidence across three classical CNN pruning tasks (ResNet56/CIFAR-10, VGG19/CIFAR-100, ResNet50/ImageNet) with headline improvements over most prior work, plus informative ablation via Acc-vs-Speed-Up curves showing slower accuracy decay after the metanetwork and finetuning. The paper also provides a concrete compute/memory cost comparison and amortization argument.\n\n3. The pruning pipeline is explicitly specified and the paper is well-structured (clear sections for methods, ablations, transfer), includes a reproducibility statement, and discloses LLM usage limited to grammar polishing—improving transparency.\n\n4. By offering a universal, criterion-agnostic route to make models easier to prune and demonstrating that a single trained metanetwork can be reused to prune many different networks with just feed-forward plus standard finetuning, the work has practical impact for reducing per-task engineering and scaling pruning to broader settings."}, "weaknesses": {"value": "1. The metanetwork is implemented as a PNA-based message-passing GNN, i.e., an MPNN classically upper-bounded in expressivity by the 1-dimensional Weisfeiler–Leman (1-WL) test; thus non-isomorphic but 1-WL-indistinguishable architecture-graphs (e.g., $2C_{3}$ vs $C_{6}$) may collapse to the same representation, so the “optimal” meta-pruning policy could lie outside the realized function class.\n\n2. The notion is presented qualitatively and illustrated ex post with accuracy–speedup plots, but it is not instantiated as a single, explicit training target. As a result, the training signals that steer the metanetwork may not be tightly aligned with the downstream pruning objective across architectures and sparsity regimes.\n\n3. The method requires finetuning after the metanetwork and after pruning (thousands of epochs are listed in configs), and meta-training demands high VRAM. Actionable: provide wall-clock/energy accounting and amortization curves (meta-training once vs. number of downstream prunes), plus sensitivity analyses for meta-epochs and both finetuning stages.\n\n4.  “Easy vs. hard to prune” remains descriptive rather than operational. The notion is presented qualitatively and illustrated ex post with accuracy–speedup plots, but it is not instantiated as a single, explicit training target. As a result, the training signals that steer the metanetwork may not be tightly aligned with the downstream pruning objective across architectures and sparsity regimes."}, "questions": {"value": "Regarding the weaknesses of the paper, I raise the following questions：\n\n1. Can authors demonstrate—on canonical 1-WL counterexamples constructed under your layer-DAG encoding(e.g., $2C_{3}$ vs $C_{6}$)—that the metanetwork yields distinct embeddings and different pruning decisions, or else provide a formal argument that such pairs cannot arise in your setting or would not alter the induced meta-pruning policy?\n\n2. What single scalar objective is optimized during meta-training that aligns with the accuracy–speedup outcome (e.g., AUC over a fixed speed range or max speedup at a fixed accuracy floor), and what is its precise mathematical definition and cross-backbone correlation with the reported curves?\n\n3. Can you provide a complete cost and amortization accounting—wall-clock, GPU-hours, kWh, and peak VRAM—for (a) meta-training, (b) post-metanetwork finetuning, and (c) post-pruning finetuning on a fixed hardware setup, together with the break-even reuse count versus strong baselines under matched accuracy/latency?\n\n4. In the absence of an explicit scalar target, can you show that your current loss is a consistent surrogate for the area/shape of the accuracy–speedup curve across pruning budgets and architectures (e.g., via monotonicity, calibration, or rank-correlation plots)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No Ethics Concerns"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IARrnoK3RM", "forum": "lMIZApRWIj", "replyto": "lMIZApRWIj", "signatures": ["ICLR.cc/2026/Conference/Submission7825/Reviewer_WztN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7825/Reviewer_WztN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761406085915, "cdate": 1761406085915, "tmdate": 1762919871615, "mdate": 1762919871615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to train a meta-network to take as input a trained model and outputs a modified model that is easier to prune under a chosen criterion. Neural network pruning then proceeds with standard scoring and fine-tuning. The authors argue that existing pruning methods depend on fixed, hand-crafted criteria or require per-model learning-to-prune procedures with limited transferability. The proposed method aims to learn general rules that transform hard-to-prune networks into easy-to-prune ones. The idea is to represent the input network as a graph, run a GNN meta-network to predict small residuals on node and edge parameters, and convert the graph back to a network. The training uses task loss to retain accuracy and a sparsity-regularization loss which aligns with a structural-pruning criterion which is a group-norm score with corresponding sparsity loss. Experiments show that a single trained meta-network improves accuracy and speed-up curves on common computer vision tasks, and achieves SOTA trade-offs without model specific training. Additional experiments show flexibility to other pruning criteria and transferability across datasets and architectures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of using a meta-network to transform a neural network to a pruning friendly one is interesting and new to me. The details are properly executed to design the proposed method. Experimental results on three classical CNN pruning tasks and a ViT show good accuracy and speed-up curves. The proposed method is shown to be robust to pruning criterion and has good transferability across similar datasets and model architectures without re-training the meta-network."}, "weaknesses": {"value": "The main experimental result table shows good numbers, but more baselines of recent structured, unstructured and hypernetwork pruning approaches would be desirable. The paper shows improved trade-off curves due to meta-network, but there’s limited analysis of which parameter and feature statistics, like channel saliency and inter-layer correlation, change. How much each model feature, like BN statistics, kernel encoding and residual edges, contributes to the performance. In general, I am not totally convinced that the proposed idea is always effective. It is also not clear if the conversion, padding and projection of edge features and the GNN’s memory and time footprint can scale to large backbones. How does meta-network depth and hidden layer size affect runtime and complexity. As the author claim that the framework works for any pruning criterion, experimental results on unstructured pruning and N:M sparsity are desirable. It is also not clear how to pick optimal meta-network checkpoint, as the meta-network training procedure does affect the accuracy and speed-up curve."}, "questions": {"value": "1. After the metanetwork transformation, which statistics of the target model change in a way that explains the improved prune-ability? It is distribution of group-norm scores, inter-channel correlations, or sensitivity at each layer? Why does the proposed meta-learning method works generally?\n2. How does the cost of network and graph conversion, and meta-network training and fine-tuning scale to larger pruning models and datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ACmNVVPbWD", "forum": "lMIZApRWIj", "replyto": "lMIZApRWIj", "signatures": ["ICLR.cc/2026/Conference/Submission7825/Reviewer_2FBa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7825/Reviewer_2FBa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761612499658, "cdate": 1761612499658, "tmdate": 1762919869660, "mdate": 1762919869660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors propose an network pruning method based on the meta-learning framework. The approach begins by representing the network to be pruned as a graph structure. Then, this method generates a new graph through meta network training (combining accuracy loss and sparsity loss) and returns it to the network."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The core advantage lies in its elimination of the need for specialized training for pruning each time. Training a meta network can prune any architecture network. Moreover, it also demonstrates strong generalization capabilities when transferring across datasets/architectures. Experimental details are fully described."}, "weaknesses": {"value": "The main text often uses the expressions \"no prior work has done something like this before\" and \"universally applicable\". However, meta networks to predict network transformations for pruning is highly similar with the previous paradigm of meta learning pruning. In Numerical Experiments, the performance advantage is not significant enough. On ResNet56/CIFAR-10, there is no substantial difference in accuracy compared to DepGraph and ATO, and there is a certain improvement in “Speed Up”; On VGG19/CIFAR-100, the accuracy of proposed method (69.75%) is not as good as DepGraph (70.39%). The authors claim that it outperforms almost all work on three tasks, but overall it seems to be competitive rather than clearly leading."}, "questions": {"value": "1.To more accurately reflect the contributions, the authors should clarify the difference between existing meta learning pruning and related work, based on Appendix A, and consider whether the expression is appropriate.\n\n2.The paper conducted major experiments on ResNet56 on CIFAR10, VGG19 on CIFAR100, and ResNet50 on ImageNet. The experimental scope appears somewhat limited. It would be beneficial to evaluate the proposed method across a wider range of classic models on datasets such as CIFAR10, CIFAR100, ImageNet.\n\n3.There are currently limited examples of \"cross architecture/cross dataset\". It would be meaningful if the author could provide more examples."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZyFaXgtU0G", "forum": "lMIZApRWIj", "replyto": "lMIZApRWIj", "signatures": ["ICLR.cc/2026/Conference/Submission7825/Reviewer_sMiZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7825/Reviewer_sMiZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721709281, "cdate": 1761721709281, "tmdate": 1762919868407, "mdate": 1762919868407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a meta-learning framework for network pruning, which can be applied to almost all types of networks.\nThe key idea is to use a metanetwork to transform a hard-to-prune network into an easy-to-prune one.\nThe framework consists of two phases: a meta-training phase, where a graph neural network learns transformation rules, and an application phase, where the trained metanetwork transforms a target network, followed by standard pruning and fine-tuning.\nThe authors claim that experiments on image classification tasks demonstrate that the proposed pruning framework achieves state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Introducing a metanetwork yields a reasonable accuracy improvement compared to the no-metanetwork baseline.\n- Multiple ablation studies support the effectiveness of the proposed approach.\n- Paper is well written and easy to follow"}, "weaknesses": {"value": "- Proposed metanetwork exhibits limited transferability across diverse model architectures and datasets.\nAlthough the paper claims transferability as a key advantage (Section 4.4), the transfer experiments are restricted to highly similar settings: architecturally similar networks (ResNet56 vs. ResNet110) and datasets of similar scale and domain (CIFAR10/100/SVHN). To more convincingly demonstrate the claimed transferability, it could be better to include: (1) transfer across substantially different architecture families (e.g., metanetwork trained on ResNet applied to VGG or ViT without retraining), and (2) transfer across different dataset scales or vision tasks (e.g., a metanetwork trained on CIFAR applied to ImageNet-scale networks, or to other vision tasks such as detection).\n- Potential requirement of many metanetworks to support various architectures and tasks.\n- Limited performance improvement over existing methods on large-scale datasets (e.g., ImageNet)."}, "questions": {"value": "- How does the metanetwork perform on larger-scale datasets such as ImageNet or on architectures with substantially different scale and structure such as VGGs?\n- Is it possible to build larger metanetwork that can support wide-range of tasks and datasets.\n- Does Meta-Pruning also reduce the cost of fine-tuning after pruning? If the cost of fine-tuning remains comparable to dense fine-tuning, methods such as Soft-Threshold Weight Reparameterization [1], which jointly optimize weights and connectivity during fine-tuning, could potentially discover better weight–pruning pairs. Could the authors clarify whether the proposed fine-tuning process provides any concrete benefit in terms of efficiency or performance compared with such joint optimization approaches?\n\n[1] Kusupati, A., et al. (2020). Soft threshold weight reparameterization for learnable sparsity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wETlgU5F0T", "forum": "lMIZApRWIj", "replyto": "lMIZApRWIj", "signatures": ["ICLR.cc/2026/Conference/Submission7825/Reviewer_sKJD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7825/Reviewer_sKJD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730566287, "cdate": 1761730566287, "tmdate": 1762919868034, "mdate": 1762919868034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}