{"id": "myosXZvY84", "number": 6007, "cdate": 1757950469720, "mdate": 1763676999862, "content": {"title": "Grounded Human-Attributed Description and Activity Recognition in Videos", "abstract": "Understanding characteristics and activities of individuals in complex multi-person environments is crucial for real-world applications. However, existing datasets often simplify this problem to grounded group activity recognition, single-person activity recognition, or close-set activity recognition, thus limiting the generalizability of models trained on these datasets. In this work, we introduce the task of Grounded Human-Attributed Description and Activity Recognition (GHADAR), which involves describing the characteristics and activity descriptions of \\textbf{every} person in a video, provided the location of the person in the video, which is more practical for real-world applications. To facilitate this, we introduce a new dataset derived from AVA-Actions by generating open-set captions for the person description and activity. In addition, we propose a novel method to effectively utilize the information contained in grounding during training by constraining the cross-attention masks during training in VLMs to improve performance for this task. Our experiments show that our method outperforms SOTA VLMs on this task. Finally, we demonstrate the limitations of existing evaluation metrics, which are overly reliant on human-annotations and exact text-text matching. As an added video-based evaluation, we propose a holistic VLM-based evaluation schema that compares concepts {\\em directly} between the video and the generated predictions. Thus, in this work, we develop a complete framework for GHADAR, including a dataset, a novel method and an evaluation schema, thereby establishing a strong foundation for future research in this domain.", "tldr": "A novel task, dataset and method for describing person and activity description in videos, given the spatial groundiga", "keywords": ["Activity Recognition", "Video Understanding", "Multimodal LLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a500370a44deabf7d76a5a9c1e70075bc70141fb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces the task of Grounded Human-Attributed Description and Activity Recognition (GHADAR) in videos, which aims to generate captions containing human appearance and activity given their locations. To facilitate this task, the authors construct the AVA-Captions dataset based on the AVA-Actions using a VLM and a person re-identification mechanism. The paper further proposes Constrained Attention Masking-based Pretraining (CAMP), an approach to improve VLMs for GHADAR, and provides an evaluation schema for GHADAR. Experiments on AVA-Captions and HC-STVG demonstrate that CAMP outperforms selected baselines in this task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The completeness and workload of this work are commendable. The authors present a full pipeline covering problem definition, dataset annotation, model adaptation, and complement evaluation.\n\n2. The proposed two-phase CAMP framework is well motivated and methodologically uncomplicated, yet demonstrates clear effectiveness as supported by the corresponding ablation studies."}, "weaknesses": {"value": "1. The paper does not summarize the role of the LLMs for this work in a separate section, which contradicts the ICLR 2026 submission policy.\n\n2. The AVA-Captions dataset is entirely generated by a VLM. However, an existing public dataset, RefAVA [1], already provides subject descriptions based on the AVA dataset similar to the AVA-Captions. RefAVA is fully human-annotated and cross-checked, excluding the VLM hallucination risks. Moreover, generating activity descriptions from the original AVA action labels appears somewhat redundant and contributes limited novelty.\n\n3. The evaluation focuses solely on the VQA setup. It remains unclear how the results of the proposed method would compare against combining state-of-the-art atomic video human action recognition models with an additional LLM-based description generation step from predicted activity labels. Furthermore, the evaluation for subject description lacks comparison with lightweight image captioning baselines.\n\n4. There is no ablation study on different backbones. The CAMP framework is only demonstrated on the InternVL3, and the paper does not examine its performance when the proposed masking strategy is extended or adapted to other architectures. This omission limits the claimed generality of CAMP on the VLM.\n\n5. The presentation quality requires improvement. In Fig. 2, the 'green action labels' is confusing; there are typos such as “respentively” (lines 464 & 465); the UNQ in Tab. 6 should be CMP, etc. While the equations in Section 4 are central to the method, several notational choices are ambiguous or unprofessional (e.g., the use of 'dont care', and overlapping conditions like $i \\leq j$ and $i \\leq T_V + T_Q$).\n\n6. The discussion of failure cases in the VLM-based pipeline is insufficient. For instance, the paper does not analyze errors in the VLM-generated activity descriptions during dataset preparation or inconsistencies in evaluation leveraging different VLMs. As an example, Nova Pro v1 reports higher subject description accuracy for the HC-STVG dataset than for the AVA-Captions dataset on the LLAVA-OV, whereas Qwen2.5-VL produces the opposite outcome under the same setting.\n\n[1] *Referring Atomic Video Action Recognition*, in ECCV 2024."}, "questions": {"value": "1. How was the threshold of 0.75 selected for the ReID similarity score used in person re-identification?\n\n2. What protocol or agreement was used during the manual validation to ensure the quality and correctness of the VLM-generated AVA-Captions?\n\n3. Can the authors also perform the ablation of the two CAMP phases on the AVA-Captions dataset, and provide additional metrics beyond SBert similarity for the bounding-box relaxation ablation to ensure a more unbiased assessment?\n\n4. Should the correct causal condition be $j \\leq i$ rather than $i \\leq j$, given that $Att$ is denoted as the the matrix for attention mask?\n\nPlease respond to both the weaknesses and the questions in the rebuttal phase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2LESUFChop", "forum": "myosXZvY84", "replyto": "myosXZvY84", "signatures": ["ICLR.cc/2026/Conference/Submission6007/Reviewer_HH6R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6007/Reviewer_HH6R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761587497397, "cdate": 1761587497397, "tmdate": 1762918412876, "mdate": 1762918412876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the task of Grounded Human-Attributed Description and Activity Recognition and presents a new dataset, AVA-Caption, derived from AVA-Actions. The proposed CAMP method constrains attention to enhance person-centered grounding for better description and activity recognition. A VLM-based evaluation metric is also introduced to assess captions directly against video content. Despite relying on the VLM’s comprehension ability, the work provides a promising direction for human-attributed video understanding."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper defines a new task, proposes a new dataset, and designs corresponding methods and evaluation protocols, forming a complete and coherent framework.\n2. The problem identification is accurate. for example, recognizing that existing datasets lack detailed human attribute descriptions and that current evaluation metrics fail to measure prediction quality precisely."}, "weaknesses": {"value": "1. The motivation is somewhat broad. The paper’s main contribution lies in proposing AVA-Caption, a dataset built upon AVA-Action that includes rich human attributes and action descriptions. However, the paper lacks experimental analysis showing the advantages, value, or necessity of this dataset compared to the original one.\n\n2. The writing is not entirely fluent. For instance, “We observed that when we trained a VLM on our dataset, it was often confusing actions done by one person with the other.” is awkward. Some tables also have formatting issues (e.g., Table 1 and Table 7).\n\n3. In terms of methodology, how are the proportions between Phase 1 and Phase 2 training determined? Overemphasis on Phase 1 may make the model focus too much on local details and ignore global context, while overemphasis on Phase 2 could cause PID confusion, potentially affecting overall performance. Moreover, the designed evaluation metric is not convincing: using a large model such as Qwen2.5-VL 32B for absolute scoring introduces biases, hallucinations, and lacks interpretability. Relative or local ranking might be more reliable. Also, the rationale behind evaluating along CRT, CMT, and UNQ dimensions needs clearer justification.\n\n4. In the experiments, Table 2 and Table 3 compare only two baselines — LLAVA-OV 0.5B and InternVL3-1B — which is insufficient for a convincing comparison. As I understand, CAMP refers to InternVL3-1B trained with both Phase 1 and Phase 2, so does the compared InternVL3-1B correspond to a version trained only on Phase 2?"}, "questions": {"value": "See the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4bHjF2JTLH", "forum": "myosXZvY84", "replyto": "myosXZvY84", "signatures": ["ICLR.cc/2026/Conference/Submission6007/Reviewer_bEqi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6007/Reviewer_bEqi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921546062, "cdate": 1761921546062, "tmdate": 1762918412591, "mdate": 1762918412591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new task, grounded human-attributed description and activity recognition, accompanied by a dataset (re-annotated AVA), a novel attention mask constraint (CAMP) to better align where a Transformer looks at (local vs global), and a VLM-based evaluation metric."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "In general, the paper is easy to follow. I appreciate that the authors identified problems with existing evaluation methods without blindly trusting existing metrics."}, "weaknesses": {"value": "Why did the authors not share video examples of their new dataset? This would have been very helpful in better understanding the quality. I have major concerns with regard to the text fidelity: the authors utilize an off-the-shelf method to describe the videos, but this runs the risk of the model incorporating text hallucinations (this is a bit of chicken-egg problem, where, in order to train a model, we have to utilize another model). For example, in Figure 2, on the left side, there is a visible cut in the sequence: how is this handled for the video? This is unclear from the description. I find Figure 4 even more concerning as the GT annotation texts seem to barely match the video frame description. Could the authors comment on how they ensured the quality of the data? From the samples they provide the quality seems to be a major bottleneck.\n\nI wonder if this is also one of the reasons why the text-based evaluation metrics are show poor performance: if the gt text is inaccurate, the model predictions might be ok but they will be compared to “bad” gt. In general, I disagree with the authors claim that the eval issues stem from “lexically different but semantically correct” captions - from the shown samples it seems more like that some of the gt captions are semantically incorrect.\n\nIn general,  I feel that the claim that existing methods have flaws is valid, but the authors should show this in a meaningful and reproducible way - showing just two samples (with poor gt) is not sufficient. \n\nI suggest that the authors utilize “Equation” to showcase their full-line equations to make discussing them easier. \n\nCould the authors describe what the “dont care” means in their Attention matrices? This notation is a bit unusual."}, "questions": {"value": "L209 “None” - should this be cursive or is this a typo?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "The proposed dataset heavily builds on an existing dataset (AVA) - I would like the authors to clarify if they will require the citation of the original dataset when citing their dataset."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UTsDEi84ZF", "forum": "myosXZvY84", "replyto": "myosXZvY84", "signatures": ["ICLR.cc/2026/Conference/Submission6007/Reviewer_C74a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6007/Reviewer_C74a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937377427, "cdate": 1761937377427, "tmdate": 1762918411994, "mdate": 1762918411994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}