{"id": "XDIqAGxSrE", "number": 24808, "cdate": 1758360552653, "mdate": 1759896747398, "content": {"title": "Influence Guided Sampling for Domain Adaptation of Text Retrievers", "abstract": "General-purpose open-domain dense retrieval systems must usually be trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches are to sample them uniformly, or proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning–driven sampling framework that adaptively reweighs training datasets guided by influence‑based reward signals and is much more lightweight w.r.t. to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing sampling from datasets that maximize the model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being *1.5×–4×* cheaper than them in terms of GPU compute needed. Our sampling strategy achieves a **5.03** absolute *NDCG@10* improvement while training a multilingual *bge-m3-dense* model and an absolute *NDCG@10* improvement of **0.94** while training *sentence-transformers/all-MiniLM-L6-v2*, even when starting from an expert assigned weights on a large pool of training datasets.", "tldr": "We address domain adaptation in text retrievers via data reweighting and propose an efficient influence-based sampling strategy.", "keywords": ["retrieval", "domain adaptation", "dataset sampling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3d186529006da180340fecb9f8de555b35c6c03d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Inf-DDS, a novel method that uses influence, the metric gain from a small number of updates on the development set, as a reward signal for training a dynamic data sampling strategy. This method is designed to optimize the training data distribution for downstream tasks, particularly in dense retrieval settings. Empirical results on multiple benchmarks, including MLDR and BEIR, demonstrate significant improvements over existing baselines. The paper also introduces engineering solutions like weighted Reptile to reduce computational and memory overhead, making the method more efficient. However, the paper does not sufficiently address the risks of overfitting and data leakage."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear and Direct Reward Design**: The use of influence as a reward based on actual metric improvements on the dev set makes the method straightforward and directly aligned with the downstream task's goals. \n2. **Effective Engineering Design**: The use of weighted Reptile for gradient sharing and memory efficiency shows thoughtful consideration for practical implementation, making the approach scalable to large datasets and models.\n3. **Stability and Robustness**: The method demonstrates stable learning trajectories, with less sensitivity to noisy gradients compared to alternative methods like DoGE or MultiDDS. The authors provide a useful analysis of sampling stability."}, "weaknesses": {"value": "1. **Potential Overfitting and Data Leakage**: The method uses a portion of the downstream test data as the development set to guide data sampling for the training set. This introduces the risk of **data leakage** and overfitting, as the model may learn to optimize for the specific dev set rather than generalizing to unseen data. This is a **key concern**, as shifting the dev set may lead to performance degradation when applied to a new, unseen dev split. The paper does not sufficiently investigate or mitigate this issue, nor does it perform experiments to confirm the stability of the approach across multiple dev splits.\n2. **Insufficient Theoretical Analysis**: While the paper presents empirical results, it lacks a detailed theoretical analysis of the potential biases and variance in the influence estimates, especially when the number of inner steps is small. The relationship between influence and long-term generalization remains unexplored. A more rigorous theoretical grounding would enhance the robustness of the method’s claims.\n3. **Unclear Generalization Across Domains**: The authors do not sufficiently explain or diagnose certain anomalies in their experiments, such as the strong upsampling of Swahili in the MLDR experiments. The paper should offer an analysis of why such samples provide improvements in performance and whether this is due to the model overfitting to particular domains or justifiable improvements in generalization.\n4. **Lack of Comparison with Related Works**: The paper does not cite or compare with proxy-model based methods like DoReMi algorithms, especially methods do not involve downstream task data. In the realm of dense embedding data sampling optimization, there are also previous works are not cited or compared in the paper. For example, tDRO (Task-level Distributionally Robust Optimization for Dense Retrieval) addresses similar issues of dataset-level weighting for improving domain robustness in the realm of dense embedding fine-tuning. \n5. **Compute and Efficiency Trade-offs**: The computational cost of the proposed method is not adequately quantified, particularly when scaling to large datasets. The paper lacks a detailed comparison of the compute cost, GPU memory usage, and sample efficiency between Inf-DDS and other methods, making it difficult to assess the scalability of the method in practical scenarios."}, "questions": {"value": "1. **Dev Split Sensitivity**: The method relies on a portion of the test data for the dev set. Could you provide experiments showing the sensitivity of the method to different dev splits and report the variance in performance across multiple splits?\n2. **Overfitting Risk**: Have you considered any measures to regularize the learning of the sampler to prevent overfitting to noisy or biased dev sets? How do you plan to mitigate the risk of data leakage or the model becoming overly sensitive to the dev set?\n3. **Computational Efficiency**: Can you provide a more detailed analysis of the computational trade-offs, such as GPU hour saving and sample efficiency, when compared to other methods?\n4. **Comparison with proxy-model based methods**: Can you compare your approach with the recent proxy-model based methods in dense embedding training, e.g. tDRO, to highlight the differences and potential advantages of Inf-DDS, particularly in terms of generalization across domains and robustness to dev-split variations?\n5. **Swahili Data Anomaly**: In your MLDR experiment, why does the Swahili language show a strong performance improvement when upsampled? Can you provide a more detailed analysis or an ablation study to explain this result?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6rF38RN4c9", "forum": "XDIqAGxSrE", "replyto": "XDIqAGxSrE", "signatures": ["ICLR.cc/2026/Conference/Submission24808/Reviewer_66zW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24808/Reviewer_66zW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924205884, "cdate": 1761924205884, "tmdate": 1762943202864, "mdate": 1762943202864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Inf-DDS, a reinforcement learning–based sampling framework designed to optimize training data selection for general-purpose open-domain dense retrieval systems. Unlike conventional sampling methods that rely on uniform distribution, proportional instance counts, or expert supervision, Inf-DDS adaptively reweights datasets using influence-based reward signals to enhance model performance on a target development set. The approach requires less GPU usage than gradient-based alternatives. Empirical evaluations across various text retrieval tasks show that Inf-DDS improves retrieval performance, including gains in NDCG@10 scores for both multilingual and sentence-transformer models.\n\nThis is a well written paper making a meaningful contribution to training dense retrievers using a more principled way to sample datasets."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work addresses the important topic of sampling datasets and tasks for training general-purpose dense retrievers in a more principled manner.\n- The proposed sampling method was shown to improve retrieval performance while requiring considerably less compute over the baselines across multiple datasets. \n- The presentation is clear overall, though Fig 2 (step 3) is not consistent with the pseudocode."}, "weaknesses": {"value": "- The choice for the initial dataset sampling probability distribution is not justified. \n- The performance across the dev sets is considered to be equally important, while it may not be the case."}, "questions": {"value": "- The paper stresses that larger dataset size doesn’t necessarily translate to more effective training, yet the dataset sampling probability distribution is initialized proportional to the dataset size. Why?\n- The results shown in Fig 4b is unintuitive. For test domains for which training data is available (FEVER and HotpotQA), why aren’t the matching training domains given the highest weight? Is it just Inf-DDS being imperfect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bHOtHmBnTw", "forum": "XDIqAGxSrE", "replyto": "XDIqAGxSrE", "signatures": ["ICLR.cc/2026/Conference/Submission24808/Reviewer_knDf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24808/Reviewer_knDf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975439939, "cdate": 1761975439939, "tmdate": 1762943202670, "mdate": 1762943202670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Inf-DDS, a dynamic data sampling method for domain-adaptive text retriever training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem of selecting informative samples during training is important, especially for reducing computation or improving convergence.\nThe core idea of prioritizing domains by observed dev improvement (rather than gradient alignment) is intuitive and close to the end goal."}, "weaknesses": {"value": "1. The paper optimizes proxy losses (e.g., InfoNCE / KD loss deltas) but does not demonstrate that these correlate with ranking metrics such as NDCG@10.\n2. Influence estimation requires extra forward/backward or Hessian-vector steps. The paper calls the method efficient but does not report GPU-hours / wall-clock / memory, so it is unclear whether gains outweigh the additional compute.\n3. The paper alternates between linear-normalized influence weights and softmax Reptile updates, but does not clearly state which variant is used where, nor how τ affects training, hindering reproducibility.\n4. No significance testing, so stability and generalizability remain unclear."}, "questions": {"value": "1. For BEIR / Sent-Trans / MLDR, could the authors clearly specify:\n(a) the training loss used for the retriever,\n(b) the proxy loss used to estimate influence,\n(c) the metric M whose change is used to define influence?\n\n2. The paper alternates between describing:\nlinear-normalized influence weighting, and softmax weighted Reptile updates. So which update rule is actually used in the main reported results？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KC9xGHxQu6", "forum": "XDIqAGxSrE", "replyto": "XDIqAGxSrE", "signatures": ["ICLR.cc/2026/Conference/Submission24808/Reviewer_9xTa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24808/Reviewer_9xTa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979426980, "cdate": 1761979426980, "tmdate": 1762943202429, "mdate": 1762943202429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Inf-DDS, a reinforcement-learning–based framework for domain adaptation in text retrievers. Instead of relying on static or gradient-based sampling, Inf-DDS uses influence scores—measuring how each training dataset affects performance on development sets—to update a sampling policy iteratively. It employs online proxy models and Reptile-style meta-updates to efficiently reuse gradients, thereby reducing GPU overhead. \n\nExperiments on BEIR, MLDR, and Sentence-Transformers training corpora show that Inf-DDS consistently improves NDCG@10 over MultiDDS, DoReMi, DoGE, and static sampling, with gains of up to +5.03 on MLDR and +0.94 over expert-curated weights on MiniLM. The method produces more stable sampling trajectories, generalizes across heterogeneous domains, and reduces compute by 1.5×–4× compared to gradient-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Novel influence-based reward mechanism offering more stable, interpretable sampling than gradient-based baselines.\n* Computational efficiency via gradient reuse and partial subsampling.\n* Clear motivation bridging influence functions with adaptive sampling."}, "weaknesses": {"value": "* Reward estimation cost: computing per-domain influence still scales poorly for very large dataset pools; proxy reliance may not generalize.\n* Influence score stability: although more stable than gradient-based methods, influence estimation still depends on the correctness of the proxy update steps.\n* Initialization sensitivity: Inf-DDS performance depends heavily on the initial sampling distribution.\n* Overfitting to dev sets: using dev-based rewards risks domain leakage.\n* Influence effect on unexpected domains (e.g., Swahili in MLDR) remains unexplained and raises questions."}, "questions": {"value": "Influence computation robustness: How does the method behave when proxy model updates and target metric diverge? Can influence computation amplify noise under distribution shifts?\n\nGranularity question: Could instance-level or cluster-level influence scoring outperform dataset-level scoring? What prevents InfDDS from integrating finer-grained sampling within each dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S7qMXtClkL", "forum": "XDIqAGxSrE", "replyto": "XDIqAGxSrE", "signatures": ["ICLR.cc/2026/Conference/Submission24808/Reviewer_j7wf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24808/Reviewer_j7wf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762831046887, "cdate": 1762831046887, "tmdate": 1762943202196, "mdate": 1762943202196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}