{"id": "MWu9EU6nkM", "number": 2134, "cdate": 1756995191410, "mdate": 1759898167501, "content": {"title": "SAGE: A FRAMEWORK FOR SEMANTIC-ALIGNMENT- GUIDED ENGINEERING OF PROMPTS AND FINE- TUNING IN INDUSTRIAL CONTROL TASKS", "abstract": "Large language models show great potential for code generation tasks, but automatic code generation for industrial control systems still faces challenges such as inaccurate semantic understanding, a lack of alignment evaluation, and a shortage of domain-specific fine-tuning models. Given the stringent requirements for real-time performance, security, logical rigor, and correct execution of industrial control code, existing general-purpose methods struggle to meet these demands. Therefore, this paper proposes a semantic alignment-guided prompt engineering approach for industrial control tasks. The approach consists of three core components: first, a dataset of function prompt formats covering five structured prompt patterns and a selection of 1,500 prompt examples for industrial control tasks is constructed; second, a semantic alignment analysis metric is designed to evaluate the semantic correctness and task consistency of code generated by different models; and third, an alignment-guided fine-tuning strategy is proposed, leveraging prompt-output-intent triples to enhance the model’s generation capabilities for industrial control tasks. Experiments are conducted on five mainstream 7B models: DeepSeek-7B, Qwen2.5-7B, InternLM2-7B, Mistral-7B, and Gemma-7B. Results show that after fine-tuning, the executable performance of Mistral-7B and DeepSeek-7B increased from 0.719 to 0.886 and from 0.676 to 0.837, respectively, and the BLEU scores increased from 3.79 to 7.45 and from 3.45 to 6.62, respectively. All models maintained intent consistency (Intent = 1.000). Gemma-7B and Qwen2.5-7B showed decreases in executable performance, success rate and BLEU, suggesting possible overfitting or distribution mismatch issues. The method proposed in this paper significantly improves the code executable performance and semantic alignment of some models in industrial control scenarios. It also reveals the sensitivity of model architecture to fine-tuning strategies, providing an important reference for subsequent architecture aware alignment optimization.", "tldr": "", "keywords": ["Fine-Tuning Strategy", "Industrial Control Code Generation", "Large Language Model", "Semantic Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7c6176751721003cb81bc3ed96e4884a47b105b.pdf", "supplementary_material": "/attachment/0662b80b222c864910a76b40d1e6aaf0f6b8e4e2.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce an automatic code generation framework for industrial control systems. The Functional Framework\nPrompt (FFP) dataset is introduced, the Semantic Alignment Score (SAS) to evaluate the code generation's quality, and the Alignment-Guided Fine-Tuning (AGFT)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The authors touch on an important topic which is the industrial control code generation. It is important in terms of human safety and avoiding asset damages. \n- They introduce Semantic Alignment Score (SAS) to quantify how well the model performs on this task.\n- They do alignment on various model and evaluate."}, "weaknesses": {"value": "- The presentation can be improved. From the abstract I had a difficulty to understand what this paper is about and had to read a few times to understand better. It should be easy to follow for people without understanding on industrial control codes. Some things you can include in the abstract: a brief explanation of the industrial control codes with a short example. Also, the mention of intent consistency terminology without any elaboration makes it confusing.\n- This approach has mixed results where some models show performance degradation where the authors attribute this to the architecture without any evidence.\n- I think you don’t need to list all the prompt examples in the main paper (section 3.1). One example for the reader to understand the logic is enough and then rest can be in appendix. \n- In SAS you identify the control codes and find the percent overlap with the intended. However, as you mentioned before with your ESD example, the order of the control codes is very important for the task. I don’t see this somewhere being evaluated, like a ranking metric. As a result, your alignment guided fine-tuning will also not take into consideration the order.\n- The methodology you propose is not that novel. The construction of the 5 prompt variations (prompt engineering) and the alignment guided finetuning (SFT?) are commonly used techniques nowadays.\n- You provide some seed prompt words and expand using LLMs to create more synthetic prompt words. This may introduce irrelevant or hallucinated outputs. (section 4.2)"}, "questions": {"value": "- Somewhere you mention Function Prompt Format. Is it FFP or FPF?\n- What does intent alignment 1.0 mean? If I give a new prompt would I get exactly the control codes I am expecting?   \n- Is alignment guided fine-tuning (AGFT) done with SFT or some Preference Optimization method?\n- More details on the dataset construction would be important. You provide \"prompt words\" to the prompt methods to construct using LLMs a synthetic dataset? Do you perform a train/validation/test split? If yes how? Is the intent alignment on the same set? \n-  It would be very helpful to provide an input prompt and expected output along with different LLM completions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "exbInlMUF2", "forum": "MWu9EU6nkM", "replyto": "MWu9EU6nkM", "signatures": ["ICLR.cc/2026/Conference/Submission2134/Reviewer_CJzP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2134/Reviewer_CJzP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761073805195, "cdate": 1761073805195, "tmdate": 1762916044320, "mdate": 1762916044320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the SAGE framework, which significantly improves the performance and semantic alignment of code generation in industrial control tasks through functional framework prompt, semantic alignment analysis and alignment-guided fine-tuning strategies. It solves key problems in industrial control code generation and provides an important reference for deploying domain-adaptive large language models in safety-critical environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper designed a dataset (FFP) containing five structured prompt formats and 1,500 prompt examples for industrial control tasks, enhancing the controllability and domain specificity of prompts.\n2. This paper quantitatively evaluated the alignment between the prompt structure and the semantics of the generated code based on intent label matching, providing a unified evaluation framework (SAS).\n3. This paper leveraged SAS to guide data selection and optimization, improving the adaptability of LLMs through lightweight techniques such as Low Rank Adaptation (LoRA), while maintaining generation safety and domain compliance."}, "weaknesses": {"value": "1. The observation that all models maintained perfect intent consistency (Intent = 1.000) even before fine-tuning raises a potential concern regarding the dataset's complexity or the discriminative power of the intent consistency metric as defined. This could indicate that the prompts in the dataset might be relatively straightforward in conveying intent, or that the current method for measuring intent consistency lacks the granularity to detect subtle misalignments. Future work should involve designing more challenging prompts where intent fulfillment is non-trivial and refining the intent consistency metric to capture partial or nuanced adherence to the intended functionality.\n\n2. While the paper introduces five structured prompt formats (B-A-B, C-A-R-E, R-I-S-E, R-T-F, T-A-G) within the FFP dataset, the rationale and criteria for specifically selecting these five formats are not sufficiently elaborated. A more detailed justification is needed, explaining whether these formats were derived from an analysis of common industrial control task types, inspired by successful prompting strategies in general code generation, empirically selected through pilot studies, or based on specific cognitive or logical reasoning principles they are intended to elicit. Clarifying the selection basis would strengthen the methodological foundation of the FFP dataset."}, "questions": {"value": "1. The claim that \"T-A-G consistently achieved the highest SAS on the SEN label across all models\" appears to be inaccurate based on the provided data. While this holds for DeepSeek-7B (Table 1), an examination of Table 1 (GPT) and Table 1 (Grok) shows that other prompt structures (e.g., R-I-S-E or C-A-R-E) achieve comparable or higher SAS scores on the SEN label. The results should be stated precisely to reflect the model-specific performance variations revealed by the data, as the current generalization is misleading.\n\n2. The fact that all models exhibit perfect intent consistency (Intent = 1.000) before any fine-tuning is a notable observation. This raises a critical question about the dataset's difficulty level and the sensitivity of the intent consistency metric. Could this indicate that the prompts in the FFP dataset are not sufficiently challenging to cause intent misinterpretation by the base models? Please discuss whether this perfect baseline score suggests a ceiling effect for this metric on your dataset and clarify its ongoing utility for evaluating model improvements post-fine-tuning.\n\n3. The paper introduces five prompt formats (B-A-B, C-A-R-E, R-I-S-E, R-T-F, T-A-G) but provides insufficient justification for their selection. The description that each \"emphasizes different reasoning aspects\" is vague. Please elaborate on the specific reasoning principles."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hl67UoI2CB", "forum": "MWu9EU6nkM", "replyto": "MWu9EU6nkM", "signatures": ["ICLR.cc/2026/Conference/Submission2134/Reviewer_H34E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2134/Reviewer_H34E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981676945, "cdate": 1761981676945, "tmdate": 1762916043882, "mdate": 1762916043882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents LLM-based industrial control code generation. This paper proposes structured prompt templates, a semantic alignment scoring scheme for control intent tags, and a LoRA fine-tuning strategy guided by these scores. Experiments on several 7B models show modest improvements in executability and BLEU on a custom prompt dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- A good problem has been studied as LLM reliability in safety-critical automation is important.\n- Authors made good effort in building a domain-specific prompt suite and evaluation tags.\n- Clear writing and motivation is well laid out.\n- Some empirical gains on small LLMs is shown."}, "weaknesses": {"value": "- The prompt formats and alignment scoring are more like prior prompt-pattern and tag-matching approaches, just re-labeled for industrial control.\n- The evaluation is very shallow. The dataset is small, no real industrial benchmarks, and BLEU is a weak metric for correctness in safety-critical code.\n- The claims of safety and correctness are not well demonstrated. No formal verification, runtime validation, or failure-mode analysis.\n- The model regressions are not substantial to convince. Some models degrade after tuning, but the paper gives only speculative explanations.\n- The paper reads more like a domain application report than a research contribution at the level expected for ICLR. There is no fundamental contribution. \n- The reproducibility is low for this paper. The authors do not mention what GPT they use and what Grok models they use. They do provide supplementary material but it is not clear in main paper.\n- Only 7B models (like Deepseek-7B) is used among open-source models. No usage of bigger models."}, "questions": {"value": "- How do you justify BLEU and “executability” as meaningful for safety and correctness in industrial systems?\n- Can you provide real industrial PLC programs, hardware-in-the-loop testing, or verification results instead of synthetic prompt sets?\n- Why do some models degrade after fine-tuning, and what analysis supports your explanation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZkA90J8t9W", "forum": "MWu9EU6nkM", "replyto": "MWu9EU6nkM", "signatures": ["ICLR.cc/2026/Conference/Submission2134/Reviewer_QBXc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2134/Reviewer_QBXc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023653979, "cdate": 1762023653979, "tmdate": 1762916043704, "mdate": 1762916043704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a solution to bridge the gap between current code generation performance and the requirements of industrial control systems, characterized by requirements such as real-time performance, security, and logical rigor. To overcome this issue, this work introduces SAGE, a semantic alignment-guided prompt engineering framework composed of:\n- A dataset of function prompt formats covering 5 structured prompt patterns (FFP dataset)\n- A semantic alignment analysis metric (SAS) to evaluate semantic correctness and task consistency of the generated code\n- An alignment-guided fine-tuning strategy, based on LoRA, where the prompt-output pairs are ordered using the semantic alignment score\n\nThe experiments are carried out using five models (Mistral-7B, DeepSeek-7B, Gemma-7B, Qwen2.5-7B, InternLM2-7B) with mixed results. Mistral-7B and DeepSeek-7B are showing improvements for all metrics used in this work: execution success rate, BLEU score and functional intent preservation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- This paper highlights a relevant problem: LLMs are now very good at generating code, but they struggle to consistently understand all the details of the prompt. In industrial control settings, this deficiency could pose relevant risks.\n- This paper introduces a new dataset made of prompts that are relevant for the industrial control domain, allowing comparison of code generation models. The dataset is designed around five prompt formats and seems easy to extend to more.\n- Given the particularity of the industrial control domain, this work proposes a novel method called Semantic Alignment Analysis (SAS) to verify the alignment of code generation. By leveraging a taxonomy made of key control intents (e.g., TIC (temperature control), FIC (flow control), INTLK (interlock logic)), it is possible to measure consistency of model output with the intended control intent (i.e., functional correctness). The taxonomy supports different programming languages."}, "weaknesses": {"value": "1. The paper is lacking clarity in some sections and presentation can be improved:\n    - The paper needs qualitative examples (I suggest a short one in the main paper, and more in the appendix): show representative prompts from the FFP, the model-generated code for those prompts, and the expected ground-truth code. Without examples it is hard to judge the difficulty or clinical relevance of the tasks. \n2. The two main important contributions (FFP dataset, SAS) are under-described. \n    - FFP dataset: the paper only reveals on page 8 that many samples were generated via LLMs; it lacks an analysis of correctness or human validation. Since the dataset is a primary contribution, stronger evidence of diversity and quality is required (e.g., there are 1500 prompts starting from 5 formats: no quality assessment of diversity).\n    - SAS: such metric appears to measure whether expected tags occur in the generated code, not whether the implementation of those tags is semantically correct. Moreover, SAS is used to filter training data but is not reported as a final evaluation metric in Table 2; if SAS’s only role is data filtering, its impact as a contribution is diminished. The correlation between SAS and final task metrics (Exec, BLEU, Success) is not shown.\n3. Clarity of experimental section can be improved, especially regarding Table 2 (more precise feedback in section \"Questions\"):\n    - Define better the metrics used.\n    - Improvements after AGFT are inconsistent across models. For the models that improve most (DeepSeek-7B and Mistral-7B), it is unclear whether the gain would be enough for the industrial domain.\n    - Analyze relationship between SAS and final metrics."}, "questions": {"value": "- The alignment-guided fine-tuning pipeline primarily applies LoRA on a curated dataset; the fine-tuning technique itself is standard, but is presented as a contribution. The main novelty here is that the dataset is curated using SAS to filter samples: I would rephrase the third contribution as a way to show the importance of SAS, rather than focusing on the training method. \n- How were the 1,500 samples produced? Which were authored by humans and which were LLM-generated? What human validation or correction steps were applied?\n- Question related to table 2 and weakness #3: \n    - How exactly is intent measured? Also given the fact it is always perfect, does it mean the prompts are too easy? \n    - What constitutes the ground-truth for BLEU? \n    - What exact versions of \"GPT\", \"Grok\", and \"DeepSeek\" were used?\n\nI am open to increasing my score if the authors clarify the points above, particularly regarding dataset construction, the role of SAS, and experimental details.\n\nMinor Comments:\n- The paper mentions LoRa (for instance at line 114), while the correct version is LoRA.\n- Missing space at line 235 after the period."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rZpIICNR4Y", "forum": "MWu9EU6nkM", "replyto": "MWu9EU6nkM", "signatures": ["ICLR.cc/2026/Conference/Submission2134/Reviewer_eAZ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2134/Reviewer_eAZ5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762081768258, "cdate": 1762081768258, "tmdate": 1762916043532, "mdate": 1762916043532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}