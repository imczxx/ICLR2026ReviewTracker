{"id": "uikLSN1yot", "number": 20689, "cdate": 1758309048093, "mdate": 1763432455239, "content": {"title": "The SMeL Test: A simple benchmark for media literacy in language models", "abstract": "The internet is rife with unattributed, deliberately misleading, or otherwise untrustworthy content. Though large language models (LLMs) are often tasked with autonomous web browsing, the extent to which they have learned the simple heuristics human researchers use to navigate this noisy environment is not currently known. In this paper, we introduce the Synthetic Media Literacy Test (SMeL Test), a minimal benchmark that tests the ability of language models to actively filter out untrustworthy and fictional information in context. We benchmark a variety of commonly used instruction-tuned LLMs, including \"reasoning\" models, and find that no model consistently succeeds; while reasoning in particular is associated with higher scores, even the best API model we test hallucinates up to 70% of the time. Remarkably, larger and more capable models do not necessarily outperform their smaller counterparts. We hope our work sheds more light on this important form of hallucination and guides the development of new methods to combat it.", "tldr": "Current language models are incapable of filtering out untrustworthy information in context.", "keywords": ["media literacy", "benchmark", "LLM"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39ecc5ba3f49382cdcd095c8b91ac49604c4c7da.pdf", "supplementary_material": "/attachment/a0d63abf93bb26ef5fcc22cf2856d8346a487b52.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces SMeL Test benchmark (Synthetic Media Literacy Test) to evaluate the ability of LLMs to identify and filter out untrustworthy information in context. The benchmark consists of three tasks that require awareness of source quality, i.e. 1). question answering given an untrustworthy source 2). question answering given a pair of reliable source and unreliable source with contradictory information 3). generate a summary that filters out untrustworthy information. To obtain input documents with different levels of trustworthiness, they use GPT model to generate synthetic documents that mimic the styles of different websites. For each task, they mimic real-world RAG setting by providing a mixture of these synthetic documents and some randomly sampled irrelevant documents as input context. They evaluate both open LLMs and API-based models on SMeL benchmark and measure the model performance by hallucination rate (i.e. how often the untrustworthy information is used by LLM). Their experiments reveal that even though LLMs can explicitly recognize untrustworthy sources, they still use untrustworthy information to perform the tasks. Increasing model sizes does not bring significant improvement in their benchmark. Reasoning models generally outperform non-reasoning ones."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper aims to address a critical challenge in RAG system by assessing the ability of LLMs to identify and filter out untrustworthy information when the input context contains a mixture of information sources\n- They propose a new benchmark called SMeL Test based on synthetic data to measure how often the LLMs are affected by untrustworthy information in question answering and summarisation\n- They conduct experiments with diverse state-of-the-art API models on SMeL Test benchmark and perform qualitative analysis"}, "weaknesses": {"value": "- All the documents in SMeL Test are generated by GPT-4o and contain fictional information. The trustworthiness of information is not clearly defined since there is no ground truth.\n- Model performance can be highly affected by the prompt instructions and positional bias, which may compromise the validity of the conclusions.\n- The writing style is not sufficiently academic. Some word choices and sentences are somewhat informal, e.g. ”deep research products consistently err”, “we say a hallucination occurs when…”, \"reasoning models do better\", “(Mostly) Real data”"}, "questions": {"value": "- Table 1 could be improved by using clearer and more descriptive row names, and the caption or paragraph should explain more details about how these numbers are calculated\n- In Figure 4, the evaluation metric of Gemini 2.5 Pro is computed on a different number of examples. The comparison among different models is not fair enough."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S4dgyfHMZn", "forum": "uikLSN1yot", "replyto": "uikLSN1yot", "signatures": ["ICLR.cc/2026/Conference/Submission20689/Reviewer_De3v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20689/Reviewer_De3v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761532082060, "cdate": 1761532082060, "tmdate": 1762934070112, "mdate": 1762934070112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new benchmark called the SMeL test, exploring whether models can recognize content from untrustworthy sources when synthesizing information. They do this by creating a series of RAG-like evals where the models receive documents and need to answer a question based on them. The results show that models do poorly at recognizing untrustworthy sources and abstaining to incorporate it into their answers. They also show some interesting case studies with reasoning vs non-reasoning models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed topic is important and timely\n- The paper is well-written and the benchmark seems relatively well-designed\n- A plethora of models are evaluated and there are numerous ablations"}, "weaknesses": {"value": "I think the paper is strong but these are the minor weakness I see. \n\n1: Some of the tasks are relatively arbitrary (e.g. Ignoring dubious sources requires the model to ignore the sources without really asking the model to do so). In many cases, I expect the model will need to be clearly told to ignore those sources. However, other sections of the eval does have this and I think this is more compelling. \n\n2. I think as a general problem for this area of research is that whether some sources are dubious is very contentious these days and I think it is hard for model providers to be able to design for this. However, the authors choose good domains to test this (no one should argue with fan fiction being excluded)."}, "questions": {"value": "o3 is one of the strongest models for searching and finding information - I wonder if these rates are relatively low because the model has been trained to be stronger at RAG. Do you think these are conflicting or could be resolved with better training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BHNaPvaYpA", "forum": "uikLSN1yot", "replyto": "uikLSN1yot", "signatures": ["ICLR.cc/2026/Conference/Submission20689/Reviewer_aokR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20689/Reviewer_aokR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942042953, "cdate": 1761942042953, "tmdate": 1762934069638, "mdate": 1762934069638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Synthetic Media Literacy Test (SMeL Test), a benchmark to test how well LLMs can differentiate (and filter) between trustworthy and untrustworthy/fictional data sources when providing answers or summaries. The paper runs this benchmark on contemporary open-weight and API LLMs, and uses a case-study using real news articles to validate its efficacy.\n\n**Benchmark data**: The benchmark consists of synthetic documents that mimic various sources with different levels of trustworthiness (Encyclopedia Britannica, NYT, Wikipedia, Reddit, 4chan, fanfiction.net, and unattributed untrustworthy documents). For a set of fictional controversial topics, the benchmark generates synthetic documents in the style of the various sources. It also includes \"false-positive\" documents (from the internet) that do not contain information about any of the fictional topics.\n\n**Benchmark tasks**: The benchmark has three types of tasks:\n1. Ignoring dubious sources: Given a factual question and a single document from an untrustworthy source (and unrelated false-positive documents), an LLM should abstain from providing an answer.\n2. Resolving contradictions: Given a factual question and two sources with contradicting information, an LLM should rely on the more trustworthy one when answering.\n3. Active filtering: An LLM is instructed to write a summary. Given two documents, one from a trustworthy and one from an untrustworthy source, the LLM should only rely on information from the trustworthy source.\nIn all tasks, models are explicitly instructed to ignore untrustworthy sources (and to abstain from answering for the first task), and sources are provided with their URL in-context.\n\n**Real-world data comparison**: The paper does a case-study on real news articles to compare benchmark on synthetic and real data. The authors use the ISOT Fake News Dataset to obtain pairs of real news articles about the same topic, one from a verified trustworthy source, and one from a verified partially trustworthy source. They then create a synthetic statement for every pair of articles, and insert it with slight variations into the individual articles.\n\n**Findings**: The paper contains many findings related to the question of how LLMs consider the trustworthiness of sources, for example\n1. Larger models in the same family (e.g., Gemma 3 4B vs. 27B) do not necessarily perform better.\n2. Reasoning models perform better than non-reasoning ones.\n3. All models (even from different families) have a similar internal ranking of how trustworthy sources are.\n4. Most models fail to abstain from answering a question if there is no reliable source.\n5. Models sometimes can express which sources are more trustworthy than others, but the same models do not act accordingly.\n6. For the \"resolving contradictions\" task, models perform worse on pairs of real articles with injected information than on pairs of synthetic documents."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Significant topic**: The overall problem this paper studies is relevant and contemporary. Frontier models often rely on search, and the internet is becoming increasingly cluttered with untrustworthy data. Hence, understanding how well LLMs handle sources of different trustworthiness helps users understand risks and to potentially fix issues.\n\n**Useful and rigorous datasets**: The dataset generation procedures (both the synthetic benchmark and the real-world news articles) seems to be done rigorously, and I cannot see any spurious biases. Hence, the benchmark seems sound. I also like the comparison to real news articles; the results hint that using synthetic data does not hurt the benchmark's efficacy a lot. And it seems straightforward to update/expand the synthetic dataset in the future due to its design.\n\n**Clear goals and limitations**: The authors motivate the significance of their benchmark well, and the goals are clear. They are also transparent about the limitations of using synthetic data. The three high-level benchmark tasks (at least as described in Section 2; ignoring subsequent issues) align well with the paper's goals."}, "weaknesses": {"value": "**Active filtering task is too limited**: The Section 2 description of the \"active filtering\" task mentions filtering between many sources. This is (in my opinion) the most interesting task, because it corresponds to \"deep research\", which is most affected by untrustworthy sources. However, later (Section 3), it becomes clear that this task only uses two documents. I think only using pairs of documents is highly restrictive and does not serve as a proxy for real-world performance of \"deep research\" systems; thus, currently only the first two tasks are truly insightful, and the third task is closer to the second one than to the real world.\n\n**LLMs being benchmarked are inconsistent**: While the paper considers a broad set of LLMs, their usage throughout the paper is inconsistent; it feels almost like two separate benchmarks, with only results from one or the other shown. This makes it very hard to judge the insights from the benchmark and often conflicts with the writing. Hence, I cannot really judge whether the conclusions are valid. I believe the paper would heavily benefit from a thorough cleanup in this regard, making sure that the writing, figures, and results are consistent. Some examples:\n- The experiment setup (L181-183) mentions Gemma 3, Llama 3, GPT-5, o4-mini, o3 Gemini 2.5 Pro, Claude 3.7\n- The immediately referenced appendix does not mention GPT-5, so it's not clear which checkpoint was used.\n- Figures 2,3,5 do not show the aforementioned Gemma 3 or Claude 3.7 models, but instead show GPT-4o.\n- Table 1 mentions o3*-mini*, yet another model. This model neither shows up in Table 1 nor its \"overflow\" Table 10.\n- The models in Figure 3 and 4 (\"resolving contradictions\" for synthetic and real data, respectively) are different, making a comparison impossible. Hence, I cannot easily verify the claim that \"the relative performance trends among models remain consistent\" on L253. However, this is the main justification for using a synthetic benchmark.\n- L234-235 mention \"Models in the Gemma and Llama families do not appear to improve with added size\". There are two different sizes from the Gemma 3 family, but for Llama, there is only Llama 3.1 8B and Llama 3.3 70B, which are not directly comparable.\n- The paper uses the newest versions of frontier models (GPT-5, Gemini 2.5 Pro), but only Claude 3.7 (not any of the 4.x models).\n- GPT-4o uses the `chatgpt-4o-latest` checkpoint, but this checkpoint is generally not recommended for use by OpenAI. Instead, a regular 4o checkpoint should be used.\n\n**Minor points**:\n1. The figures (and placement) in the main matter could be improved. Currently, Figures 2-4 are all combined on a single page, far from where they are referenced in writing. Listing the figures closer to where they are referenced would make the flow easier. For example, Figures 3 and 4 could be combined into two subfigures. Those two figures would also benefit from being homogenized; currently they use different models and have different x axes, which makes the comparison between synthetic and real data hard to assess.\n2. There is a non-anonymous URL on L1078-1079."}, "questions": {"value": "1. What is the reliable source in Figure 3? Is it averaged over EB+NYT+Wiki, or only one of them?\n2. Why does the real-world dataset not use untrustworthy sources (only partially trustworthy ones)? Using fully untrustworthy sources might reduce the gap in hallucination rate between synthetic and real data.\n3. The paper explicitly introduces three categories of sources (\"trustworthy\", \"potentially trustworthy\", \"objectively untrustworthy\"). How do the 7 sources in Section 2.1 map into those categories? I could not find anything relating to that in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MYpTphA7JG", "forum": "uikLSN1yot", "replyto": "uikLSN1yot", "signatures": ["ICLR.cc/2026/Conference/Submission20689/Reviewer_BRwb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20689/Reviewer_BRwb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950897610, "cdate": 1761950897610, "tmdate": 1762934069254, "mdate": 1762934069254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work explores the timely subject of detecting the trustworthy sources, especially targeting for the media information. Used three categories of tasks to evaluate the model performances systematically. Results show how the model size, reasoning availablility (i.e., reasoning models or not) impact the model performances and whether the models share similar judgements of the source qualities, and eventually SMeL scores."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The topic is timely\n\nElegantly proposed three categories of the evaluation tasks - ignoring dubious sources, resolving contradictions, and active filtering are all meaningful approaches to deal with such misinformation detection tasks. \n\nData used for the work are rich - from encyclopedia britannica (academic) to Reddit (a casual internet community forum) to the least trustworthy source (i.e., \"unknown\").\n\nWell-presented/ summarized results - the performances depending on the model scales, reasoning availability, or source quality assessment to show the overall results. Great visualization for the result analysis."}, "weaknesses": {"value": "The work is too heuristic and missing technical concepts to evaluate the validity of the work.\n- the work really depends on the data contents and the currently used data do not seem to have standardized methods to evaluate the validity to replicate the work.\n- Not sure about the technical depth of the work. It sounds more like a blog post or report of the model result analysis."}, "questions": {"value": "Would you specify more about the experimental setups?\nhow did you make three tasks?\n- ignoring dubious sources - how did you make the unreliable/irrelevant context, any formats that you use for the datasets to modify the original contents?\n- resolving contradictions - how did you make the reliable and perturbed versions of the factual questions? what are the standard filler? What prompts did you use for models to ignore the documents that are not trustworthy?\n- active filtering - again it would be nice to share the explicti prompts and why you designed such fixed promtps. etc. \n\nHow did you make the SMeL scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HXHpX9ByYu", "forum": "uikLSN1yot", "replyto": "uikLSN1yot", "signatures": ["ICLR.cc/2026/Conference/Submission20689/Reviewer_aMix"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20689/Reviewer_aMix"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762168981118, "cdate": 1762168981118, "tmdate": 1762934068854, "mdate": 1762934068854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}