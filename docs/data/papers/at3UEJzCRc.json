{"id": "at3UEJzCRc", "number": 4743, "cdate": 1757756454542, "mdate": 1759898016641, "content": {"title": "Progressive Semantic Fusion Transformer for Zero-Shot Temporal Action Localization", "abstract": "Zero-Shot Temporal Action Localization (ZSTAL) aims to classify and localize action instances from unseen categories in videos. Existing ZSTAL approaches predominantly rely either on visual modality alone or on stage-limited fusion of visual and textual modalities to generate proposals. Such approaches hinder text embeddings from providing semantic guidance throughout the pipeline, limiting the model’s ability to capture discriminative visual features of unseen actions. \nTo mitigate this limitation, we propose $\\textbf{PSFTR}$ ($\\textit{\\textbf{P}rogressive \\textbf{S}emantic \\textbf{F}usion \\textbf{TR}ansformer}$), a novel transformer-based method that progressively integrates textual semantics across stages of the pipeline. Specifically, PSFTR injects textual embeddings into both the $\\textbf{encoder}$ and $\\textbf{decoder}$ stages via a cross-attention mechanism, enabling the model to focus on text-relevant visual features and generate semantically guided learnable queries. Furthermore, during the $\\textbf{classification}$ stage, we design a query enhancement mechanism driven by textual semantic prototypes to refine the representations of action moments within the learnable queries. Extensive experiments on THUMOS14 and ActivityNet1.3 demonstrate that PSFTR achieves 28.99% mAP (+1.08%) and 29.91% mAP (+1.81%), respectively, validating the effectiveness of progressive semantic fusion for ZSTAL.", "tldr": "", "keywords": ["Temporal action localization", "Zero-shot", "Vision-Language models", "Multi-modal"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dbfa2b5fe4067689e1e9d2cfc4f59595dd630bf3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces the Progressive Semantic Fusion TRansformer (PSFTR) for Zero-Shot Temporal Action Localization (ZSTAL). \nIt appears that the proposed method extends DETR-like architectures by injecting text semantics into three parts of the model: \n(1) a Semantic Self-Attention Encoder (SSAE) \n(2) a Semantic-Guided Query Generator (SGQG) \n(3) a Semantic Prototype-Driven Enhancer (SPDE) \nResults on THUMOS14 and ActivityNet-1.3 show modest gains (~1–2% mAP) over prior ZSTAL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Semantic features are consistently integrated across encoder, decoder, and classifier, forming a coherent pipeline.\n2. The paper explores different prompt templates and SPDE fusion strategies, showing measurable effects.\n3. Clear details on training, hyperparameters, and architecture allow reproduction."}, "weaknesses": {"value": "I have three major concerns:\n1.  The evaluation framework is rather weak -  A major evaluation gap lies due to non-inclusion of the Charades-STA and HACS benchmarks.\n2. The proposed method mainly extends existing DETR-based ZSTAL frameworks by inserting CLIP text tokens; it introduces no new learning objective.\n3. Since CLIP text embedding has been used in the proposed method  I wonder how this is affecting the proposed method since CLIP poorly models verbs; action semantics that rely on motion (e.g., “opening vs closing”) are semantically ambiguous."}, "questions": {"value": "Is there any possibility of using a text encoder with better understanding of the \"verb\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xnBjZaT9W9", "forum": "at3UEJzCRc", "replyto": "at3UEJzCRc", "signatures": ["ICLR.cc/2026/Conference/Submission4743/Reviewer_dih9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4743/Reviewer_dih9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927394337, "cdate": 1761927394337, "tmdate": 1762917551241, "mdate": 1762917551241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the task of zero-shot temporal action localization, aiming to classify and localize action instances from unseen categories in videos. One key observation is that existing work produces action proposals without introduce text for interaction. Progressive Semantic Fusion TRansformer is proposed to mitigate the limitation. Experiments are carried out on THUMOS14 and ActivityNet1.3."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[+] The manuscript is well written, with clear logics.\n\n[+] The symbol definitions are clear, and the image visualization is complete.\n\n[+] Many ablation experiments are conducted to analyze the effectiveness of each component."}, "weaknesses": {"value": "[-] Introducing text for interaction during the production of action proposals can slow down the training/inference speed. The reviewer suggests comprehensive comparisons between this paper and existing methods from the perspective of efficiency (such as throughput, RT, GFLOPs)\n\n[-] THUMOS14 and ActivityNet1.3 are both small-scale datasets, which means that there have no reference value in real-world scenarios. For zero-shot settings, one simple approach to proving generalization is to evaluate across datasets, such as training on ActivityNet1.3 and testing on THUMOS14. The reviewer suggests providing this cross-dataset performance evaluation to better understand the significance of this work to the community.\n\n[-] In recent years, it has become a trend to unify temporal action localization and video temporal grounding in the form of multimodal large models. Some studies have demonstrated strong performance and generalization across closed-set, zero-shot, few-shot, and open-set scenarios. Please make comprehensive comparisons in terms of performance, generalization, efficiency, and practicality.\n\nUniversal Video Temporal Grounding with Generative Multi-modal Large Language Models. NeurIPS 2025"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pa9dHZcNzt", "forum": "at3UEJzCRc", "replyto": "at3UEJzCRc", "signatures": ["ICLR.cc/2026/Conference/Submission4743/Reviewer_c1Zh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4743/Reviewer_c1Zh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962742799, "cdate": 1761962742799, "tmdate": 1762917550382, "mdate": 1762917550382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PSFTR, a progressive semantic fusion transformer for zero-shot temporal action localization (ZSTAL). The method injects textual embeddings at three stages—encoder, decoder, and classifier—with the claim that “semantic guidance throughout the pipeline” improves cross-modal alignment and generalization to unseen classes. Experiments are conducted on THUMOS14 and ActivityNet-1.3."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clearly stated and the framework is modular and easy to follow.\n- The idea of incorporating text beyond a final similarity stage is reasonable to ZSTAL."}, "weaknesses": {"value": "- Limited novelty: the method is largely a staged combination of existing components. The three modules reuse standard mechanisms (cross-attention, deformable attention, prototype fusion) and primarily rearrange them in sequence. There is no fundamentally new learning principle, objective, or semantic reasoning mechanism. The level of contribution appears incremental.\n- The core claim (“semantic guidance throughout the pipeline”) is not actually demonstrated. Text embeddings are static CLIP vectors, not dynamically updated. The model uses repeated conditioning rather than true progressive semantic reasoning. No diagnostic analysis (feature visualization, probing, attention shift, semantic alignment metrics, etc.) is provided, so it remains unproven that text semantics meaningfully reshape visual representations.\n- Experimental evidence is not convincing: the method fails on the more complex dataset. THUMOS14 is widely considered the more challenging benchmark (denser action instances, higher temporal ambiguity), yet the proposed method underperforms recent baselines on this dataset. If semantic fusion truly improved fine-grained localization and unseen-class reasoning, we would expect the opposite trend. This discrepancy weakens the central claim.\n- Performance deteriorates under stricter IoU evaluation, contradicting the claimed benefit. Table 1 shows that the proposed method is competitive at loose IoU thresholds (0.30–0.40) but loses more clearly at stricter thresholds (0.60–0.70), and the margin vs. the strongest baseline widens as IoU increases. This indicates that the method improves coarse recall rather than precise boundary localization, directly conflicting with the claim of “enhanced fine-grained semantic guidance.”\n- No computational or efficiency analysis is provided. The architecture adds three cross-modal fusion stages, which likely increase FLOPs, parameters, and memory, but no complexity comparison is reported. It is unclear whether the modest gains justify the extra cost."}, "questions": {"value": "- Can you provide FLOPs, inference time, and parameter counts vs. baselines?\n- Can you show evidence that semantic fusion actually changes visual representations across stages?\n- Is a single fusion stage nearly as good as three? (Ablation necessary to justify “progressive.”)\n- Why does the method lag behind baselines specifically on THUMOS14 and high-IoU evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2rzrTxuGC3", "forum": "at3UEJzCRc", "replyto": "at3UEJzCRc", "signatures": ["ICLR.cc/2026/Conference/Submission4743/Reviewer_6YMm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4743/Reviewer_6YMm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762077728214, "cdate": 1762077728214, "tmdate": 1762917549770, "mdate": 1762917549770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the task of Zero-Shot Temporal Action Localization (ZSTAL), which involves identifying and temporally localizing actions in untrimmed videos, including categories that the model has never seen during training. The authors observe that existing ZSTAL methods suffer from three main issues: weak cross-modal alignment between text and vision, visually driven proposal generation that ignores semantics, and loss of discriminative detail during feature aggregation.\n\nTo address these challenges, the paper proposes the Progressive Semantic Fusion Transformer (PSFTR). The key idea is to progressively inject textual semantics derived from a pretrained CLIP model into multiple stages of the transformer instead of fusing them only at the end. This design allows the model to maintain semantic alignment and text-awareness throughout its processing pipeline. The three main modules work together to refine visual and textual representations, improving generalization to unseen action categories.\n\nThe proposed method is evaluated on THUMOS14 and ActivityNet1.3, where it consistently outperforms previous zero-shot baselines. Ablation studies confirm that each module contributes to performance gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper’s main strength is in the progressive fusion of textual semantics and visual features across multiple stages of the transformer architecture. This design goes beyond previous methods that only perform fusion at the final stage, making the model more semantically aware throughout its processing pipeline.\n- The work demonstrates originality through the idea of using semantics-guided queries and semantic prototype enhancement, which together help the model generalize to unseen actions. The experimental results are strong, with consistent improvements over state-of-the-art methods on standard benchmarks. The paper also includes comprehensive ablation studies that clearly show the contribution of each module.\n- In terms of clarity, the paper is generally well organized, with intuitive figures and a logical flow that makes the complex architecture understandable."}, "weaknesses": {"value": "- Evaluation is confined to standard zero-shot splits on THUMOS14 and ActivityNet1.3. The work does not explore cross-dataset or out-of-distribution data, which would strengthen the claim of broad zero-shot capability.\n- Only mean Average Precision (mAP) is reported. Important aspects such as temporal boundary accuracy, localization precision/recall trade-offs, and calibration or ranking consistency are not analyzed. This weakens the claim of improved temporal localization quality.\n- The ablations show each module’s contribution, but the paper could include more diagnostic insight into failure cases (e.g., unseen classes with motion ambiguity or long-duration segments).\n- Prompt design is briefly explored, but a more systematic study of linguistic variation (e.g., paraphrasing, template diversity) would strengthen claims about semantic fusion robustness.\n- The paper does not analyze how performance varies with semantic distance between seen and unseen classes; such analysis would clarify the robustness of semantic transfer.\n- Finally, the presentation of the fusion process could be made clearer for readers less familiar with attention-based architectures."}, "questions": {"value": "- Have you evaluated or considered evaluating PSFTR in cross-dataset or out of distribution zero-shot settings (e.g., training on ActivityNet, testing on THUMOS)? Such experiments would clarify whether the fusion generalizes beyond seen-domain distributions.\n- Since mAP aggregates both classification and boundary effects, could you provide complementary metrics such as temporal IoU completeness, boundary deviation, or segment precision/recall to support the claim of improved localization quality?\n- What types of unseen actions remain most challenging for PSFTR (e.g., long-duration, motion-ambiguous, or semantically distant classes)? Any insights from qualitative inspection or error clustering would be valuable.\n- Have you analyzed how PSFTR performs as the semantic distance between seen and unseen classes increases? For instance, do semantically closer unseen categories benefit more from the progressive fusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z9S1AEeJYx", "forum": "at3UEJzCRc", "replyto": "at3UEJzCRc", "signatures": ["ICLR.cc/2026/Conference/Submission4743/Reviewer_MJwk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4743/Reviewer_MJwk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148303104, "cdate": 1762148303104, "tmdate": 1762917549316, "mdate": 1762917549316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}