{"id": "zZ2RhS3nRI", "number": 7936, "cdate": 1758044619872, "mdate": 1759897821073, "content": {"title": "MIAU: Membership Inference Attack Unlearning Score for Quantifying the Forgetting Quality of Unlearning Methods", "abstract": "Machine unlearning aims to adapt the model’s internal representations as if the forget set was never part of training set. In this context, a central challenge lies in accurately evaluating whether forgetting has actually occurred. Membership Inference Attacks (MIAs) are commonly used for this purpose; however, existing approaches are limited, often relying on single comparison and lacking reference points such as baseline and retrained model performance. We propose the Membership Inference Attack Unlearning Score (MIAU), a systematic metric that quantifies how closely an unlearning method mirrors the behavior of a fully retrained model. MIAU evaluates the unlearned model by comparing how easily it can separate three different pairs of data: forgotten samples versus test samples, forgotten samples versus retained samples, and retained samples versus test samples. These comparisons are then normalized between the performance of the original model and fully retrained model, providing an interpretable and balanced score of unlearning quality. The MIAU is intended to be used as an offline auditing benchmark to select the most suitable unlearning method for a given model setup and application setting, so that once this choice is made, the method can be applied in practice without performing any additional retraining. Extensive experiments demonstrate that MIAU consistently distinguishes effective unlearning methods across various image classification benchmarks and model architectures. Further statistical tests and empirical evaluations on retrained models—trained on 25%, 50%, and 75% of the forget set—highlight inherent limitations of MIAs in capturing gradual forgetting, presenting need for complementary evaluation methods in unlearning assessment.", "tldr": "We propose the Membership Inference Attack Unlearning Score (MIAU), a principled metric that quantifies how closely an unlearning method approximates the behavior of a fully retrained model.", "keywords": ["Machine Unlearning", "Privacy Evaluation", "Forgetting Quality", "Membership Inference Attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a193edb86b1d064e557ddea96b1890b6c5ccd89a.pdf", "supplementary_material": "/attachment/dfeebf212bdb0660f74dfdc776d93cb1742fd8c5.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes MIAU (Membership Inference Attack Unlearning Score), a composite metric for evaluating machine unlearning. MIAU compares three MIA setups—Forget vs Test, Forget vs Retain, and Retain vs Test—then normalizes each attack’s accuracy between a baseline model trained on all data and a retrained model trained without the forget set. A logistic mapping turns a “gap-closure fraction” into a bounded 0–100 score, and the final MIAU is a weighted average (default equal weights). Experiments on MNIST, CIFAR-10/20, and MUCAC across ResNet-18, All-CNN, and ViT, plus several unlearning methods (Fine-tune, SSD, Amnesiac, Teacher), indicate MIAU can separate methods and is often monotone under “partial retraining” baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper introduces an interpretable metric that aggregates complementary MIA views and normalizes them between baseline and retrain references, yielding a single bounded score that’s easier to compare across methods and datasets.\n- This paper proposes a practical audit$\\rightarrow$deploy workflow: use a one-time retrained reference to select an unlearning method offline, then apply the chosen method in production—keeping evaluation principled while limiting operational overhead."}, "weaknesses": {"value": "- The contribution feels largely constructive/standardizing rather than conceptually new: it consolidates existing MIA signals with anchoring and a bounded mapping to improve interpretability, which is valuable but incremental in novelty.\n- The proposed audit$\\rightarrow$deploy workflow reads as a sensible formalization of common practice in unlearning evaluation, offering operational clarity but not introducing a fundamentally new deployment paradigm.\n- There is growing evidence that MIAs lose discriminative power as model capacity increases. The paper’s own ResNet vs. ViT results seem broadly consistent with this trend. It would strengthen the contribution to discuss the implications for MIAU at larger scales and, where feasible, include evaluations on substantially larger models or complementary privacy signals for regimes where MIA signal weakens.\n- The placement of Figure 2 may be a bit early: it introduces notation before the symbols are formally defined (later in the text), which can make a first read harder to follow. Consider moving the figure to the methods section where definitions appear, or adding a brief notational reference in the caption so the figure is self-contained.\n- For $\\beta,\\gamma,\\delta$, the paper fixes equal weights but offers no sensitivity study or guidance to select weights for different privacy/utility trade-offs. This limits practical tunability.\n\n[1] Duan, M., Suri, A., Mireshghallah, N., Min, S., Shi, W., Zettlemoyer, L., ... & Hajishirzi, H. (2024). Do membership inference attacks work on large language models?. arXiv preprint arXiv:2402.07841."}, "questions": {"value": "See Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NO or VERY MINOR ethics concerns only"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9ArOyn0uE6", "forum": "zZ2RhS3nRI", "replyto": "zZ2RhS3nRI", "signatures": ["ICLR.cc/2026/Conference/Submission7936/Reviewer_YQ9P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7936/Reviewer_YQ9P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550936726, "cdate": 1761550936726, "tmdate": 1762919958373, "mdate": 1762919958373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Membership Inference Attack Unlearning Score (MIAU), a composite metric to assess machine unlearning quality. MIAU aggregates three pairwise MIA tasks: Forget vs. Test, Retain vs. Forget, and Retain vs. Test, and normalizes performance between a original model and a retrained model. \nA logistic transformation maps gap closure into a 0–100 score. The authors propose MIAU as an offline auditing tool to select the best unlearning method for a model-dataset pair, avoiding repeated retraining in deployment. Experiments span image classification benchmarks (MNIST, CIFAR-10/20, MUCAC), model architectures (ResNet-18, All-CNN, ViT), and unlearning methods (Fine-tune, SSD, Amnesiac, Teacher). The paper claims MIAU captures gradual forgetting and overcomes limitations of single MIA evaluations."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly articulates failure modes of individual MIA tasks (Section 1.1) and motivates the need for a unified metric.\n- The experimental scope is reasonable, covering multiple datasets, models, unlearning methods, and robustness checks with multiple iterations.\n- Combining three complementary MIA tasks is a logical step beyond single-pair evaluations in prior work."}, "weaknesses": {"value": "- The core contribution appears to be a calibration and aggregation of three existing MIA comparisons (Forget vs. Test, Retain vs. Forget, Retain vs. Test), which are already well-established in the literature, which cited in L58-L59. The normalization via gap closure and logistic transformation feels like a minor post-processing step rather than a substantive innovation.\n\n- Although the paper asserts that MIAU detects imperfect unlearning where individual MIAs fail (e.g., retained representations despite low forget accuracy), but provides no targeted experiments to demonstrate this. For example, no adversarial unlearning setups, synthetic failure cases, or ablations are included. Without such evidence, the claimed improvement over raw MIAs remains unsubstantiated.\n\n- In L325-L327, the paper expects MIAU to increase strictly with partial forgetting (MIAU_{25%} < MIAU_{50%} < MIAU_{75%}), but offers no theoretical justification. MIA accuracy reflects binary classification on differing logit distributions; there is no inherent reason for monotonic improvement. Diverse training dynamics in partial retraining may alter separability unpredictably. This assumption is critical to the “gradual forgetting” claim and requires formal derivation or counterexamples.\n\n- Standard deviations frequently exceed 20 points on a 0–100 scale. This implies that the same unlearning method may receive scores differing by over 40 points across random seeds. Such instability prevents reliable method ranking and defeats the stated goal of efficient offline auditing."}, "questions": {"value": "- Can you provide experiments where MIAU detects imperfect unlearning (e.g., retained internal representations with low forget accuracy) while individual MIAs fail?\n- Why should MIAU increase monotonically with partial forgetting levels? Please include a theoretical derivation or counterexamples where this property fails.\n- How can the high standard deviations in Table 1 be reduced for practical use?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1DR4DQ9enu", "forum": "zZ2RhS3nRI", "replyto": "zZ2RhS3nRI", "signatures": ["ICLR.cc/2026/Conference/Submission7936/Reviewer_rN33"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7936/Reviewer_rN33"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739133981, "cdate": 1761739133981, "tmdate": 1762919957972, "mdate": 1762919957972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MIAU, an unlearning audit score that aggregates several MIAs and normalizes them between a baseline model and a fully retrained model, aiming to provide a quick offline way to compare unlearning methods without retraining."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The “offline audit --> choose a method --> deploy” workflow is well presented and could be convenient in practice.\n\n2. The evaluation is broad, spanning multiple datasets and architectures, with partial-retrain references that acknowledge MIA limits and attempt graded validation."}, "weaknesses": {"value": "1. The paper states that prior works “often rely on a single comparison and lack reference points (baseline/retrain).” However, this statement is inaccurate as many unlearning papers do compare to retrain/baseline, including (and not limited to):\n\n[1] Fan, Chongyu, et al. \"Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation.\" ICLR 2024.\n\n[2] Zhao, Kairan, et al. \"What makes unlearning hard and what to do about it.\" NeurIPS 2024.\n\n2. The argument that Retain–Test is essential as a generalization sanity check is not so convincing when standard retain/test accuracies already capture this. Moreover, for the forget-retain setup, the paper suggested that effective unlearning should increase separability between forget and retain sets, but this also risks enabling attackers to identify the forget set, which undermines privacy goals. As a result, I don't think the justification for these two additional steups is strong enough.\n\n3. There are some other questionable statements in the paper. E.g.\"after unlearning, accuracy on the forget set should drop slightly, ideally approaching test-level performance\" This sounds vague and can be misleading, as matching forget-set performance to test-set performance is not generally a sound or universal unlearning target."}, "questions": {"value": "1. You use a logit-based binary classifier as the MIA, but have you tried other (more advanced) MIAs? And how would the choice of MIA affect the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P4bz01xdST", "forum": "zZ2RhS3nRI", "replyto": "zZ2RhS3nRI", "signatures": ["ICLR.cc/2026/Conference/Submission7936/Reviewer_XsVe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7936/Reviewer_XsVe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934958710, "cdate": 1761934958710, "tmdate": 1762919957523, "mdate": 1762919957523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MIAU (Membership Inference Attack Unlearning Score), a new metric designed to evaluate the effectiveness of machine unlearning methods. Unlike prior approaches that rely on a single MIA setting or raw accuracy differences, MIAU integrates three complementary MIA comparisons, Forget vs Test, Forget vs Retain, and Retain vs Test, to capture residual memorization, removal specificity, and generalization stability. The key idea is to position an unlearned model between two meaningful reference points: the baseline model trained on all data and the fully retrained model without the forget set. Using a “gap‐closure fraction” and a calibrated logistic transformation, MIAU provides a normalized, interpretable score (0–100) indicating how closely an unlearning method approximates the privacy behavior of full retraining. Extensive experiments across datasets, architectures, and unlearning methods demonstrate that MIAU can differentiate strong and weak unlearning approaches and generally increases under partial retraining, reflecting progressive forgetting."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1)One of the most valuable aspects of the proposed MIAU score is the normalization between the fully trained model and the fully retrained model. This creates a principled evaluation interval and enables intuitive interpretation of “how much forgetting has been achieved.”\n\n(2)The authors evaluate MIAU on multiple benchmarks, including MNIST, CIFAR-10/20, and MUCAC, and across diverse model families such as ResNet, AllCNN, and ViT. This broad coverage enhances the external validity of the findings."}, "weaknesses": {"value": "(1)The proposed MIAU score entirely depends on membership inference attacks, inheriting several well-known limitations of MIAs themselves, such as instability across random seeds, sensitivity to data complexity, calibration issues, and weak discriminative power on well-generalized models (e.g., MNIST). As a result, the reliability of MIAU is fundamentally constrained by the weaknesses of its underlying signal.\n\n(2)The method implicitly assumes that a retrained model fully represents “ideal forgetting,” but this assumption does not necessarily hold. Retraining may still capture distribution-level information about the forgotten data or produce nontrivial MIA signals. Thus, anchoring the metric strictly between baseline and retrain introduces a conceptual bias and may misrepresent unlearning effectiveness in certain settings.\n\n(3)Different applications impose different priorities: privacy-focused scenarios emphasize Forget vs Test, while utility-centric ones emphasize Retain vs Test. Using fixed uniform weights lacks methodological grounding and may mask important trade-offs."}, "questions": {"value": "The experimental setup largely relies on weak or limited MIAs, uses small and simple datasets, includes only a narrow set of unlearning baselines, and lacks evaluation on more recent or complex models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xxy9Z3jVKc", "forum": "zZ2RhS3nRI", "replyto": "zZ2RhS3nRI", "signatures": ["ICLR.cc/2026/Conference/Submission7936/Reviewer_RBMY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7936/Reviewer_RBMY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953712031, "cdate": 1761953712031, "tmdate": 1762919956844, "mdate": 1762919956844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}