{"id": "rDWvX5dxGC", "number": 7522, "cdate": 1758025744153, "mdate": 1759897848156, "content": {"title": "Talk2Me: High-Fidelity and Controllable Audio-Driven Avatars with Gaussian Splatting", "abstract": "Audio-driven avatars are increasingly employed in online meetings, virtual humans, gaming, and film production. However, existing approaches suffer from technical limitations, including low visual fidelity (e.g., facial collapse, detail loss) and limited controllability in expression and motion, such as inaccurate lip synchronization and unnatural head motion. Besides, most existing methods lack explicit modeling of the correlation between facial expressions and head pose dynamics, which compromises realism.\nTo address these challenges, we propose Talk2Me, a high-fidelity, expressive, and controllable audio-driven framework comprising three core modules. Firstly, we enhance 3D Gaussian Splatting (3DGS) with Learnable Positional Encoding (LPE) and a modified Region-Weighted Mechanism to mitigate facial collapse and preserve fine details. Secondly, an Expression Generator (EG) with an Audio-Expression Temporal Fusion (AETF) module models the temporal relationship between audio and expression features, enabling accurate lip-sync and natural expression transitions. Thirdly, a Retrieval-Based Pose Generator (RBPG) explicitly captures the coupling between expressions and pose dynamics, with a Pose Refiner (PR) enhancing the naturalness and continuity of head motion.\nWe further construct a Mandarin monocular video dataset featuring diverse identities to evaluate cross-lingual generalization. Experiments demonstrate that Talk2Me outperforms state-of-the-art methods in visual quality, synchronization accuracy, and motion naturalness.", "tldr": "", "keywords": ["Talking head synthesis", "3D Gaussian Splatting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/efddf1b9e52fbbaa91257086d005d48b49384503.pdf", "supplementary_material": "/attachment/a0fd83448a5886dd8e7fe5fcfbdd242f8661c393.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Talk2Me, a framework combining 3D Gaussian Splatting, Learnable Positional Encoding, and a retrieval-based pose generator to create high-fidelity, controllable audio-driven avatars with improved facial animation, head movement, and lip sync, outperforming existing methods in quality and naturalness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors present a comprehensive set of experiments evaluating both reconstruction quality and audio-driven synchronization, with comparison to multiple baselines (GAN, NeRF, 3DGS). The results indicate clear advantages over existing methods.\n- The user study demonstrates a significant improvement in perceptual quality, especially in lip-sync accuracy and expression synchronization.\n- The paper makes a notable contribution by including a Mandarin dataset, which addresses cross-lingual generalization and enhances the robustness of the model."}, "weaknesses": {"value": "- While the framework performs well in synchronizing speech and motion, it ranks slightly lower in terms of raw image quality compared to certain other methods (e.g., PSNR and MS-SSIM). The paper should better justify the trade-off between fidelity and expressiveness in facial motion synthesis.\n- While the framework is promising in terms of fidelity and controllability, no details are provided on the computational efficiency of the system. Specifically, how feasible is the framework for real-time applications or large-scale deployment? A more in-depth discussion on the computational cost and real-time performance would be beneficial.\n- The paper suggests that the 3DGS method helps improve the consistency of facial geometry, but some visual artifacts (e.g., noticeable blinking issues, light preservation in the eye region) still remain, especially without LPE. Further improvement or clarification in these areas would improve the robustness of the method.\n- Some figures, such as Figure 3, are too blurry to understand."}, "questions": {"value": "- Could you provide more detailed comparisons with state-of-the-art real-time audio-driven avatar generation systems regarding computational efficiency and latency? How scalable is Talk2Me for large-scale, real-world applications or live environments?\n- The paper relies heavily on LPE to enhance geometric accuracy and texture fidelity. How does this approach compare to traditional methods for improving high-frequency texture details, such as NeRF or GAN-based models, especially in highly dynamic settings?\n- While Talk2Me demonstrates promising results with Mandarin speech, can it handle significant variations in accent, emotion, or speech clarity? How well does it generalize to other languages or noisy real-world audio data?\n- Could you elaborate on the temporal synchronization process within the AETF module? How does it manage edge cases where the audio is unclear or there are sudden pauses in speech?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wtEH7P3LTI", "forum": "rDWvX5dxGC", "replyto": "rDWvX5dxGC", "signatures": ["ICLR.cc/2026/Conference/Submission7522/Reviewer_FGpT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7522/Reviewer_FGpT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761278684514, "cdate": 1761278684514, "tmdate": 1762919626666, "mdate": 1762919626666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Talk2Me is a 3DGS-based, audio-driven 3D talking-head generation framework.  It addresses two major challenges:  \n1. Low fidelity, including facial collapse and loss of fine-grained details;  \n2. Controllable facial and head dynamics, and the correlation modeling between the two.  \n\nTo this end, the paper proposes several key components:  \n- A Learnable Positional Encoding (LPE) and a modified Region-Weighted Mechanism to achieve high-fidelity rendering;  \n- An Expression Generator (EG) equipped with an Audio-Expression Temporal Fusion (AETF) module for controllable and expressive facial motion;  \n- A Retrieval-Based Pose Generator (RBPG) combined with a Pose Refiner (PR) to achieve controllable and realistic head pose generation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The authors incorporate a wide range of sophisticated modules to enhance the realism and controllability of 3D talking-head generation.  \n- The Retrieval-Based Pose Generator (RBPG) is an interesting contribution, leveraging an audio–pose database to retrieve and model semantically related head movements corresponding to audio cues."}, "weaknesses": {"value": "1. (Major) The writing quality is poor. The paper reads like a technical report that lists many techniques and heuristics without clear logical flow. Existing methods and custom modifications are mixed together, leading to poor readability.  \n\n2. (Major) Although the method aims to improve fidelity and controllability, the demo videos still exhibit severe jitter, artifacts, and unsynchronized lip movements, suggesting that the proposed modules may not effectively address these issues.  \n\n3. (Minor) Many descriptions are unclear or missing crucial details (see Questions below). Likewise, Fig. 1 (overview) is very confusing and fail to illustrate how the components interact or operate."}, "questions": {"value": "- In Equation (2), what do $d_{\\text{far}}$ and $d_{\\text{near}}$ represent?  \n\n- In the original 3DGS formulation, there is no explicit variable $z$. What does $z$ denote in this paper? In Equation (7), does $z$ correspond to some attribute of Gaussian?  \n\n- In Section 4.3.1, what is the architecture of the Expression Encoder? What is its role in the overall pipeline? Additionally, in line 257, should the audio encoder be Wav2Vec instead of Wav2Lip (possible typo)?  \n\n- In Section 4.3.1, what exactly is the global decoder in the Region-Aware Attention Mechanism? What are its inputs and outputs, and how are global and local features fused? Could the authors clarify this with a more detailed figure or explanation?  \n\n- What does the dual-pathway specifically refer to? Does it indicate the two decoders for global and local feature processing?  \n\n- Could the authors provide an ablation study for the Pose Refiner (PR), comparing the retrieved poses and the refined poses to demonstrate its effectiveness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fv3AJx9wUX", "forum": "rDWvX5dxGC", "replyto": "rDWvX5dxGC", "signatures": ["ICLR.cc/2026/Conference/Submission7522/Reviewer_dpjk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7522/Reviewer_dpjk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536911653, "cdate": 1761536911653, "tmdate": 1762919626217, "mdate": 1762919626217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to address the challenges of low fidelity and limited controllability in audio-driven avatars by identifying key limitations in existing methods, such as reliance on traditional sinusoidal positional encoding leading to facial collapse and loss of fine details, as well as temporal misalignment and independent treatment of expression and head pose causing lip-sync errors and rigid motion. To overcome these issues, the authors present a 3DGS-based framework aimed at enhancing visual fidelity and enabling controllable facial and head dynamics. The proposed method incorporates Learnable Positional Encoding and a modified Region-Weighted Mechanism to address facial collapse and improve detail preservation. It also integrates the Eye Aspect Ratio feature for fine-grained blinking modulation and ensures identity consistency through 3DGS's inherent modeling capability. For expression controllability, an Expression Generator with an Audio-Expression Temporal Fusion module is proposed, enabling accurate lip synchronization and smooth expression transitions. For pose controllability, a Retrieval-Based Pose Generator and a Pose Refiner are introduced to generate natural and expressive head movements. The authors curate a Mandarin video dataset to assess cross-lingual generalization. Extensive evaluations on both English and Mandarin datasets demonstrate that Talk2Me achieves superior generation quality, synchronization accuracy, motion coherence, and expression controllability compared to existing methods. The main contributions include the presentation of Talk2Me, enhancements to 3D Gaussian Splatting, and the proposal of EG and RBPG for improved facial expression and head motion control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method is presented in a relatively clear manner, and the proposed approach is technically sound. The experimental validation is comprehensive, encompassing qualitative and quantitative comparative experiments, user studies, and ablation studies. Based on the presented results, the proposed method brings about certain performance improvements."}, "weaknesses": {"value": "The novelty of the work is somewhat limited. Although the authors introduce multiple modules, they all lack innovation. Specifically, the region-weighted mechanism and learnable positional encoding are rather trivial methods, and the retrieval-based pose generator is more of an engineering-oriented post-processing step. I do not perceive much algorithmic innovation in these components.\nIn terms of experiments, the authors' comparative experiments are all conducted on a white background. Moreover, when comparing with other algorithms, it seems that the white backgrounds of other algorithms have been removed, which leads to a jagged edge issue around the human figures. In fact, many previous methods generate the human figure along with the background. The authors' approach of removing the background for comparison might create an unfavorable condition for other methods. I hope the authors can explain the motivation behind removing the background.\nFor person-specific 2D talking faces, there is actually limited room for improvement. From both quantitative and qualitative comparisons, the enhancements brought by the proposed method are quite marginal."}, "questions": {"value": "I am particularly concerned about the issue of background removal mentioned in the weaknesses section, as I believe it may lead to unfair comparisons. Additionally, I would like to know whether the proposed method struggles to handle complex backgrounds, and I hope the authors can reply to this issue."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "rDZRnUWH0N", "forum": "rDWvX5dxGC", "replyto": "rDWvX5dxGC", "signatures": ["ICLR.cc/2026/Conference/Submission7522/Reviewer_CYo4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7522/Reviewer_CYo4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844642071, "cdate": 1761844642071, "tmdate": 1762919625458, "mdate": 1762919625458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Talk2Me to generate controllable audio-driven 3D avatars built on 3D Gaussian Splatting. The authors propose to address limitations in previous talking-head methods including the lack of fine-grained facial details and limited controllability of facial expression and head pose. Specifically, they enhance 3DGS with a Learnable Positional Encoding (LPE) and a modified region-weighted deformation mechanism for accurate deformation and fine facial structures. Moreover, they introduce an Expression Generator with an Audio-Expression Temporal Fusion module (AETF) to model temporal correlations between speech and expression and a Retrieval-Based Pose Generator (RBPG) with a pose refinement strategy to improve pose controllability. The method is trained on HDTF dataset and further evaluated on a Mandarin video dataset to prove the cross-lingual generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper aims to address challenges in existing audio-driven 3D facial animation methods, including facial-torso artifacts, unnatural motion, and audio–expression inconsistency. It combines learnable spatial encoding, region-aware deformation, and audio-conditioned expression and pose modeling into a unified 3DGS framework and shows comparisons with prior GAN-based, Nerf-based, and 3DGS-based baselines. The evaluations together with perceptual user study show certain improvement in the animation quality and lip-synchronization. In terms of clarity, the paper is overall easy to follow, with detailed architectural descriptions and ablation study that validates the design decisions. The implementation details and code also enhances reproducibility."}, "weaknesses": {"value": "1. The improvements shown in the paper are incremental, as comparisons are demonstrated under reconstruction setting, which means they all have same poses, eyeblinks etc. And the expression variation is small, making it hard to evaluate the claimed gains in motion controllability and naturalness.  \n2. Some details are also missing: the paper introduces a Mandarin monocular dataset, but provides only brief information and it would also be helpful to compare performance differences between the HDTF test set and the Mandarin set to support the cross-lingual generalization claim. Moreover, the training data processing including expression and pose coefficients definitions, and extraction are missing.\n3. Lack of results: there is no video ablation visualization and comparisons of head-pose generation against SOTA methods. The current experiments focus on reconstruction quality rather than demonstrating more motion-control advantages."}, "questions": {"value": "1. Is the model person-specific or generalizable to unseen identities? If it is person-specific, how much data per subject is required, and is the performance sensitive to data length and quality? \n2. The video results show relatively small head movements and limited expression variation (except lip motion). Is this due to limited motion in the training data or does the model implicitly bias toward average motion patterns? It would be helpful to show more examples with expressive motion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4qPKY9AbLy", "forum": "rDWvX5dxGC", "replyto": "rDWvX5dxGC", "signatures": ["ICLR.cc/2026/Conference/Submission7522/Reviewer_nyaT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7522/Reviewer_nyaT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884059579, "cdate": 1761884059579, "tmdate": 1762919625088, "mdate": 1762919625088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}