{"id": "lOQHpdi2o9", "number": 7370, "cdate": 1758018120257, "mdate": 1759897857073, "content": {"title": "When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs", "abstract": "Multimodal large language models (MLLMs) must resolve conflicts when different modalities provide contradictory information, a process we term modality following. Prior work measured this behavior only with coarse dataset-level statistics, overlooking the influence of models’ confidence in unimodal reasoning. In this paper, we introduce a new framework that decomposes modality following into two fundamental factors: relative reasoning uncertainty ( the case-specific confidence gap between unimodal predictions) and inherent modality preference( a model’s stable bias when uncertainties are balanced).\nTo validate this framework, we construct a controllable dataset that systematically varies the reasoning difficulty of visual and textual inputs. Using entropy as a fine-grained uncertainty metric, we uncover a universal law: the probability of following a modality decreases monotonically as its relative uncertainty increases. At the relative difficulty level where the model tends to follow both modalities with comparable probability what we call the balance point, a practical indicator of the model’s inherent preference. Unlike traditional macro-level ratios, this measure offers a more principled and less confounded way to characterize modality bias, disentangling it from unimodal capabilities and dataset artifacts.\nFurther, by probing layer-wise predictions, we reveal the internal mechanism of oscillation: in ambiguous regions near the balance point, models vacillate between modalities across layers, explaining externally observed indecision. Together, these findings establish relative uncertainty and inherent preference as the two governing principles of modality following, offering both a quantitative framework and mechanistic insight into how MLLMs resolve conflicting information.", "tldr": "", "keywords": ["Modality Conflict", "Multimodal LLMs", "Interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cff9772996479784a9541278c2444fce879c9fb2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper measures whether vision and language models (VLMs) rather follow the image information or the text information when visual and text inputs contradict each other. The authors propose a framework that decomposes modality following into two factors: relative reasoning uncertainty (the confidence gap between unimodal predictions) and inherent modality preference (a model’s bias when uncertainties are equal). The paper proposes a toy (controllable) dataset and entropy-based uncertainty analysis, they uncover a universal law that a model’s likelihood to follow a modality decreases monotonically with its relative uncertainty, and they link this behavior to internal “oscillations” observed across model layers near ambiguity points."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Clear and original framing**: The paper presents a compelling idea, namely measuring modality bias by observing which modality an MLLM follows when presented with conflicting information in the two modalities.\n* **Strong model coverage**: The authors evaluate a comprehensive set of models spanning diverse LLM backbones (LLaVA and Qwen-based variants), lending credibility and generality to the observed patterns.\n* **Mechanistic interpretability insight**: The analysis of internal oscillation patterns under uncertainty is very interesting and valuable mechanistic perspective, loved this.\n* **Well-written and accessible**: Despite a few phrasing hiccups in the abstract, the paper is overall clearly structured.\n** High relevance and timeliness:** Understanding how MLLMs resolve conflicting multimodal information is highly pertinent to current challenges in multimodal reasoning, perception, and interactive AI systems."}, "weaknesses": {"value": "* **Toy-level dataset and limited task diversity**: The evaluation relies entirely on a synthetic color-recognition setup with simple geometric shapes and textual descriptions. While this design enables tight control over difficulty tiers, it offers little ecological validity. Modality-following behavior may depend on data type, domain, or task semantics, so it remains unclear whether the proposed framework and its findings generalize to real-world multimodal reasoning tasks such as visual QA, caption grounding, or instruction following. Modality following might be data-type and domain-dependent and this work does not show how far this method and its findings generalise.\n* **Missing comparison to existing interpretability frameworks for multimodal imbalance:** Although the paper cites prior work such as Parcalabescu & Frank (2022; 2024), it does not meaningfully connect its findings to earlier analyses of modality contribution and cross-modal influence. Important related frameworks such as the Perceptual Score (Gat et al., 2021) and Frank et al. (2021), “Vision-and-language or vision-for-language?” -- all of which reached contradicting conclusions -- are not discussed and not compared to, leaving the paper insufficiently situated within the broader interpretability and multimodal reasoning literature.\n* **Entropy-based uncertainty analysis may not be robust**: The framework hinges on token-level output entropy as a proxy for reasoning uncertainty. However, entropy primarily captures surface-level distributional sharpness rather than true epistemic or aleatoric uncertainty, and it can be influenced by decoding strategies or output vocabulary size or task ambiguity. Without calibration checks or comparison to alternative uncertainty estimates, the reliability of the conclusions drawn from entropy remains uncertain."}, "questions": {"value": "The sentence starting in L015 is confusingly framed. It reads: \"Prior work measured this behavior only with coarse dataset-level statistics, overlooking the influence of models’ confidence in unimodal reasoning.\" It is unclear what \"dataset-level\" relates to overlooking a completely new object of measurement, namely confidence in unimodal reasoning.\n\nL018-L019 parentheses are broken because there is a whitespace after them → remove the whitespace\n\nL021: \"using entropy...\" → entropy of what?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lUaJLpyHlh", "forum": "lOQHpdi2o9", "replyto": "lOQHpdi2o9", "signatures": ["ICLR.cc/2026/Conference/Submission7370/Reviewer_VBB8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7370/Reviewer_VBB8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761486998178, "cdate": 1761486998178, "tmdate": 1762952140983, "mdate": 1762952140983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates \"modality following\" in MLLMs, a phenomenon describing how models resolve conflicts between contradictory visual and textual information. The authors argue that prior work, which relies on coarse, macro-level statistics, overlooks the critical role of a model's case-specific uncertainty in its unimodal reasoning. To address this, the paper introduces an innovative analytical framework that decomposes modality following into two core factors: (1) relative reasoning uncertainty, which is the case-specific confidence gap between predictions based on vision versus text alone, and (2) inherent modality preference, a model's stable, intrinsic bias toward one modality when uncertainties are balanced. To validate this framework, the authors construct a controllable dataset that systematically varies the reasoning difficulty of both visual and textual inputs. Using output entropy as a quantitative measure of uncertainty, the paper discovers a \"unified monotonic law\": the probability of a model following a modality decreases monotonically as its relative uncertainty increases. This law allows the authors to define the \"balance point\"—the relative uncertainty value at which a model is equally likely to follow either modality—as a principled, quantitative metric for its inherent preference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a novel and principled analytical framework that deconstructs the complex phenomenon of modality following into two more fundamental components: relative reasoning uncertainty and inherent modality preference. This approach moves beyond the prior work to offer a more powerful explanatory model, representing a conceptual contribution to understanding multimodal conflict resolution.\n2. The work is supported by a relatively rigorous experimental design, centered on a custom-built dataset where visual and textual reasoning difficulty can be independently and systematically controlled. This allows for the precise isolation and study of the core variables (uncertainties) and their effect on model behavior, setting a standard for mechanistic and interpretability research in large models.\n3. The empirical discovery of a \"unified monotonic law\" is a simple yet useful finding that organizes seemingly chaotic model choices into a single, predictable pattern. The \"balance point\" concept that emerges from this law provides a principled method for quantifying and disentangling a model's intrinsic biases from confounding factors like its unimodal capabilities or dataset-specific artifacts."}, "weaknesses": {"value": "1. The core findings are predominantly derived from a synthetic \"toy dataset\" focused on color and attribute recognition of simple geometric shapes. While this controlled setting is ideal for validating fundamental principles, it raises questions about the generalizability of the conclusions to more complex, subtle, and semantic conflicts found in real-world scenarios. It is not yet clear if the framework, especially the stability of a model's \"balance point,\" holds across a wider variety of realistic tasks.\n2. The entire analysis relies heavily on using output token entropy as the sole proxy for model uncertainty. Although entropy is shown to correlate with designed difficulty, the paper does not sufficiently justify this choice over other uncertainty quantification methods or discuss its potential limitations. The robustness of the findings is therefore contingent on the validity of this single metric.\n3. The experiments are conducted on a limited set of six MLLMs from the two families. While these are relevant models, they represent a narrow slice of the architectural landscape. The claim of discovering a \"universal law\" would be significantly strengthened by validating the framework on a more diverse set of models with different architectures and training paradigms.\n4. The paper provides a strong diagnostic framework but offers limited discussion on its practical implications for improving models. For example, it is unclear how this understanding could be used to steer a model's modality preference during alignment or to develop methods that reduce internal oscillations and make models more decisive."}, "questions": {"value": "1. How do you envision your framework applying to more open-ended, generative tasks where a model must produce long-form text? In such scenarios, the entropy of a single token may be insufficient to capture the model's overall uncertainty. How would the concept of \"relative uncertainty\" need to be adapted or extended for these more complex cases?\n2. Regarding the \"internal oscillations,\" have you analyzed their distribution across the model's layers? For instance, do oscillations occur more frequently in early, middle, or late layers of the network? Does the magnitude of the oscillations correlate with the overall model uncertainty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t0vwc3AR9s", "forum": "lOQHpdi2o9", "replyto": "lOQHpdi2o9", "signatures": ["ICLR.cc/2026/Conference/Submission7370/Reviewer_yL48"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7370/Reviewer_yL48"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632287058, "cdate": 1761632287058, "tmdate": 1762919499847, "mdate": 1762919499847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how multimodal models decide whether to follow textual or visual information when the two modalities provide conflicting cues. The authors propose a principled framework that decomposes this “modality-following” behavior into two underlying components: relative unimodal reasoning uncertainty and the model’s inherent modality preference.\n\nTo test this framework, they design a controllable toy dataset that allows systematic manipulation of reasoning difficulty in each modality, thereby inducing varying uncertainty levels. By quantifying uncertainty via the output entropy of answer tokens, the study establishes a clear empirical relationship: the probability that a model follows a modality monotonically decreases as that modality’s uncertainty increases. Moreover, each model exhibits a subjective balance point, a stable threshold reflecting its internal preference toward vision or text.\n\nAt the mechanistic level, the authors uncover layer-wise oscillations in ambiguous conditions, where the model alternates between textual and visual answers before converging. This oscillation provides a plausible explanation for externally observed indecision"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The finding that the probability of following a modality decreases monotonically as its relative uncertainty increases is interesting!\n- The paper provides a clear and interpretable decomposition of multimodal decision-making, distinguishing between case-specific uncertainty and model-level bias.\n- The paper’s analysis is systematic, the hypotheses are well-motivated, and the results are supported by clear empirical trends and visualizations"}, "weaknesses": {"value": "- Some prior works on semantic bias, language bias might worth discussing. For instance, [1] argue that linguistic priors learned during pre-training can “hack” or dominate visual inference, which appears conceptually related to the present work’s notion of modality preference.\n- Model selection for Figure 4(a) and 4(b).\nDifferent model sets are used in these two subfigures, but it is unclear whether the remaining models exhibit similar behavioral patterns as the three presented ones. \n- Model identification in Figure 6(a).\nThe paper does not specify which model generated the visualization in Figure 6(a). It remains uncertain whether the observed layer-wise oscillation behavior is consistent across different model architectures or specific to one instance.\n\n[1] Han, Junlin, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, and Filippos Kokkinos. \"Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training.\" arXiv preprint arXiv:2509.26625 (2025)."}, "questions": {"value": "- How the average concept oscillations in figure 5 is computed ? For example, we have 500 samples, and for each sample i, model has x_i layer-wise oscillations, is it computed as \\sum_{I}{x_i}/500 or \\sum_{I}{x_i}/layer number/500 or other ways?\n\n\n- Expand Literature Discussion on Semantic and Language Biases.\n- Provide details on all models analyzed in Figure 4 and indicate whether unreported models display comparable patterns.\n- If some models diverge from the main trend, discussing these exceptions would offer valuable insights into when and why the proposed law holds.\n- Identify the specific model used to produce the layer-wise oscillation visualization in Figure 6(a).\n- Include (or mention in the appendix) parallel analyses for other models to demonstrate the robustness and generality of this internal oscillation phenomenon."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iIVxZn3HSH", "forum": "lOQHpdi2o9", "replyto": "lOQHpdi2o9", "signatures": ["ICLR.cc/2026/Conference/Submission7370/Reviewer_x1XN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7370/Reviewer_x1XN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811587175, "cdate": 1761811587175, "tmdate": 1762919499481, "mdate": 1762919499481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how MLLMs handle conflicting information between text and images. It shows that modality-following behavior arises from two key factors: relative reasoning uncertainty (confidence gap between unimodal predictions) and inherent modality preference (a model’s stable bias). Using a controllable dataset and entropy-based uncertainty measures, the authors find a monotonic law—the likelihood of following a modality decreases as its relative uncertainty grows—and identify layer-wise oscillations near the balance point, explaining indecision. The work provides a unified and interpretable framework for understanding modality preference in MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel conceptual framework – Clearly decomposes modality-following behavior into relative uncertainty and inherent preference, offering a principled explanation beyond prior dataset-level analyses.\n2. Mechanistic insight – The layer-wise oscillation analysis provides an interpretable link between internal model dynamics and external behavioral indecision.\n3. Practical interpretability – The notion of a “balance point” gives a simple quantitative metric for comparing inherent modality preferences across models."}, "weaknesses": {"value": "1. Limited scenario diversity – Both experimental settings focus on geometric shape perception tasks with relatively simple and synthetic visual scenes. This narrow scope limits the generalizability of the findings to more complex, real-world multimodal reasoning scenarios.\n2. Lack of downstream validation —— The paper does not demonstrate whether understanding or controlling modality preference improves practical tasks (e.g., VQA or captioning).\n3. Interpretability gap —— While oscillation is shown as an internal correlate of indecision, causal evidence connecting it to model training or architecture is still limited.\n4. Figure formatting issue – In Figure 4, the font sizes and label alignments are inconsistent, which slightly affects readability and visual coherence."}, "questions": {"value": "1. Your analysis elegantly decomposes modality-following behavior into relative uncertainty and inherent preference. However, how can this framework inform practical improvements in multimodal model design or training?\n2. The experiments mainly validate the correlation between H^{(t)} (text entropy) and accuracy under unimodal (T, Q) inputs. However, this only demonstrates that entropy reflects uncertainty in isolated textual reasoning. How does this directly justify its use as the governing variable for modality-following behavior in the conflict setting, where cross-modal interactions and contextual dependencies might differ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NaKW1F34sK", "forum": "lOQHpdi2o9", "replyto": "lOQHpdi2o9", "signatures": ["ICLR.cc/2026/Conference/Submission7370/Reviewer_VpJP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7370/Reviewer_VpJP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762580101166, "cdate": 1762580101166, "tmdate": 1762919499138, "mdate": 1762919499138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}