{"id": "eecCPK8upD", "number": 12882, "cdate": 1758211171482, "mdate": 1759897479478, "content": {"title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training", "abstract": "Recent work in continual learning has identified the *stability gap* -- a temporary decline in performance on previously learned tasks when new tasks are introduced. This gap reflects an imbalance between rapid adaptation and robust retention at task boundaries, underscoring the need for mechanisms that reconcile plasticity and stability. In biological systems, this balance is achieved through multi-timescale dynamics regulated by neuromodulatory processes. By contrast, artificial networks rarely incorporate comparable mechanisms. Although optimizers such as momentum-SGD and Adam introduce implicit forms of regularization, they too exhibit stability gaps. Notably, such gaps arise even under ideal joint training, making it crucial to study them in this setting to disentangle their causes from other sources of forgetting. Drawing inspiration from noradrenergic bursts, which enhance neuronal gain under uncertainty, we introduce *uncertainty-modulated gain dynamics* as a two-timescale approach that balances adaptation and retention by adjusting learning rates and flattening the energy landscape. Evaluating on domain- and class-incremental MNIST, CIFAR, and mini-ImageNet benchmarks under joint training, we demonstrate that neuromodulated gain dynamics effectively attenuate the stability gap. Furthermore, our analysis highlights parallels with noradrenergic function in cortical circuits, providing mechanistic insights into how noradrenaline supports perceptual integration with minimal interference.", "tldr": "We mitigate the stability gap in continual learning with a noradrenaline-inspired gain mechanism that adapts learning rates and flattens the loss landscape, reducing interference across tasks.", "keywords": ["stability gap", "continual learning", "neuromodulation", "stability-plasticity dilemma"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba0e1f2b7d798a579c57a80c006daf163e42a8d1.pdf", "supplementary_material": "/attachment/74467f3bb500af9826ae268db3592690609753ab.zip"}, "replies": [{"content": {"summary": {"value": "The stability gap in Continual Learning is a phenomenon in which performance temporarily declines when a new task is introduced. Previous studies have shown that this occurs mainly because the strong signal from the loss leads to rapid and strong weight modifications. The authors of this study examined this phenomenon in deep learning models from the perspective of biological models. Biological models can learn new tasks through a multi-timescale dynamics that gradually allows new information to be incorporated into neurons. Inspired by this, the authors present an optimiser that accounts for two-timescale dynamics (fast and slow), enabling the balance between adaptation and stability in sequentially learned models. To study the problem in detail, the experiments focus on an environment with sequential tasks but assuming that all previous tasks are accessible (joint learning). Experiments are conducted on different variants of MNIST, CIFAR, and Mini ImageNet."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Works inspired by biological models help us better understand how we can expand the capabilities of current models. The authors do a good job of linking a specific limitation (the stability gap) of current models to mechanisms that can inspire future improvements.\n- The idea of adapting the optimiser with a two-timescale approach is interesting. As the authors mention, it follows the line of work that seeks fast adaptations (fast weights) and longer-term adaptations."}, "weaknesses": {"value": "- The main contribution of the work is an optimiser that approximates the two-timescale gradient descent strategy to mitigate the stability gap. However, no significant benefits are seen in the results. Table 1 shows a slight improvement over SGD but introduces a level of complexity that has not been studied. Figure 2 shows that NGM-SGD helps control the loss in Task 1, but there is no significant difference compared to SGD.\n- At the beginning of Section 4, it is stated that the aim is to study ‘How to optimise and not what to optimise’, but I believe the two components are closely related and difficult to study independently.\n    - Could you explain in more detail what you mean?\n- The paper raises two very interesting questions in the introduction (lines 62-64). However, I am not sure that they are answered in the paper. In Section 3, the method is presented, but no direct connection is made to the biological brain."}, "questions": {"value": "- Can we view \"g\" as a value similar to the momentum of other optimisers, but which changes as described in equation 2?\n- Did you conduct experiments in which you reset the optimisers for each task? As mentioned in the paper, the momentum of Adam or M-SGD can negatively affect the stability gap, which can be tackled by resetting the momentum values.\n- Did you conduct experiments with different values for the number of iterations? Previous work has shown that models that are trained for more iterations achieve greater stability. It would be interesting to study this statement under this scenario.\n- The motivation for proposing this new optimiser is to reduce the strong weight modifications by using the g values (which should capture most of the task change). Did you study how the changes in the weights \"w\" compare with those of other optimisers? Could the reduction in plasticity have a long-term effect on the model's performance?\n    - Conclusions may not be difficult to obtain for simple tasks or those with few iterations. More complex tasks or those with greater changes in task distribution may have different behaviours.\n- Line 303 mentions that they restrict modulation to only the last layer of the ResNet. Did you experiment with the entire model? Did you conduct experiments with pre-trained models (ResNet pre-trained with ImageNet and on datasets outside the distribution, such as CUB)?\n    - The latter may be interesting, as it would give an idea of how the optimiser behaves when big variations in the weights are needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xwif76FqgL", "forum": "eecCPK8upD", "replyto": "eecCPK8upD", "signatures": ["ICLR.cc/2026/Conference/Submission12882/Reviewer_KNC8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12882/Reviewer_KNC8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550956545, "cdate": 1761550956545, "tmdate": 1762923669363, "mdate": 1762923669363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies and formalizes the “stability gap” in continual learning—transient performance drops at task boundaries that persist even under ideal joint training—and attributes it to optimization dynamics rather than objective choice. It proposes a biologically inspired optimizer, Noradrenergic Gain-Modulated SGD (NGM-SGD), which introduces a fast timescale, uncertainty-driven neuronal gain (triggered by output entropy) atop standard weight updates, effectively scaling the loss and flattening local curvature to curb overshoot while accelerating adaptation. The authors provide an intuitive two-timescale interpretation (fast gain × slow weights) and show that modest, head-only gain modulation suffices for deep architectures. Across class- and domain-incremental benchmarks, NGM-SGD consistently narrows the stability gap (higher min-ACC, lower avg-SG) without sacrificing final accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work draws an interesting inspiration from noradrenaline-driven gain modulation, offering a principled lens on when and how a learner should adapt under distribution shifts in continual learning."}, "weaknesses": {"value": "1. The paper is difficult to follow. It leans heavily on terminology from biological neural circuits without sufficient plain-language grounding or progressive intuition.\n2. The method is primarily targeted at narrowing the stability gap during task transitions, but it does not directly address the central challenge of continual learning—catastrophic forgetting over long horizons. The proposed gain-modulated SGD feels like a modest variant of existing two-timescale or adaptive-step-size ideas rather than a fundamentally new optimization principle.\n3. The evaluation is limited to small or mid-scale datasets. Without results on larger, more realistic settings, it is hard to assess robustness, training dynamics under real-world complexity, and computational overhead.\n4. The comparisons focus on relatively dated optimizer baselines, and the reported improvements are modest. The method is not evaluated within mainstream continual learning frameworks. This raises uncertainty about extensibility and practical utility."}, "questions": {"value": "see Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "beaPhlQCk7", "forum": "eecCPK8upD", "replyto": "eecCPK8upD", "signatures": ["ICLR.cc/2026/Conference/Submission12882/Reviewer_gDN2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12882/Reviewer_gDN2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930812493, "cdate": 1761930812493, "tmdate": 1762923668841, "mdate": 1762923668841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the stability gap problem in continual learning. The authors propose a biologically-inspired approach based on noradrenergic gain modulation, where neuronal gains are dynamically adjusted based on prediction uncertainty. The proposed NGM-SGD implements uncertainty-driven gain boosts that create fast-slow weight dynamics and flatten the loss landscape. The approach is evaluated on domain-incremental and class-incremental benchmarks under ideal joint training conditions, demonstrating reduced stability gaps compared to standard optimizers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "**Solid theoretical foundation**\nThe mathematical framework connecting gain modulation to fast-slow weight decomposition is clear and intuitive. The analysis showing how gain boosts flatten the loss landscape provides mechanistic insight.\n\n**Clear, implementable algorithm.**\nThe application of NGM-SGD seems simple, with standard SGD weight update plus a gain update driven by prediction entropy each iteration. The lack of architectural changes, replay buffers, or extra losses makes it practical, and the idea that such a minimal mechanism can shrink the stability gap in continual learning is compelling."}, "weaknesses": {"value": "**Novelty with respect to biological grounding.**\nThe authors claim that no prior work has adopted a bio-inspired approach to mitigate the stability gap or connected it back to adaptive biological learning. However, the complementary learning systems (CLS) literature has long modeled fast/slow learning via mimicking the hippocampus–neocortex interactions of the brain, and many continual learning methods explicitly borrow this paradigm through dual-memory architectures, replay-based consolidation, etc. [1, 2] The proposed gain-modulated fast/slow decomposition closely echoes this CLS framing. The manuscript should acknowledge these approaches, explain the similarities or distinctions, and include comparisons or at least a reasoned discussion against strong CLS-style baselines. Without this positioning, the biological novelty claim feels somewhat overstated.\n\n**Limited empirical scope.**\nWhile the paper notes the simplicity of its benchmarks as a limitation, this is not a minor caveat. It is a necessary extension to substantiate the paper. Without evaluations on more challenging settings (e.g., longer task streams, larger-scale datasets, online/streaming protocols without task IDs, and realistic memory/compute constraints) it is difficult to conclude that the method genuinely mitigates stability gaps rather than benefiting from the specifics of the setup. Also, comparisons should be made with recent continual learning methods specifically designed to address stability gaps. Expanding the study to stronger baselines and modern architectures would further strengthen the claim.\n\n**Hyperparameter sensitivity**\nThe method introduces additional hyperparameters, requiring task-specific tuning. The values vary significantly across datasets, suggesting the method may not generalize well. A guidance on how to set these parameters for new tasks would be helpful.\n\nReferences\n\n[1] Arani, Elahe, Fahad Sarfraz, and Bahram Zonooz. \"Learning fast, learning slow: A general continual learning method based on complementary learning system.\" arXiv preprint arXiv:2201.12604 (2022).\n\n[2] Pham, Quang, Chenghao Liu, and Steven Hoi. \"Dualnet: Continual learning, fast and slow.\" Advances in Neural Information Processing Systems 34 (2021): 16131-16144."}, "questions": {"value": "1. The performance improvements seems modest and inconsistent. Why does vanilla SGD or other baselines sometimes outperform multi-timescale optimizers? This seems counterintuitive given your multi-timescale argument.\n\n2. As mentioned, in CNN experiments, gain modulation is applied only to the output layer. What happens if you apply gain modulation to all layers in CNNs instead of just the output layer? Is there an adaptive way to slightly modify the methodology so that gain modulation can be applied to all layers of CNN?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gsqvI9Rmyf", "forum": "eecCPK8upD", "replyto": "eecCPK8upD", "signatures": ["ICLR.cc/2026/Conference/Submission12882/Reviewer_4gom"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12882/Reviewer_4gom"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934123619, "cdate": 1761934123619, "tmdate": 1762923668407, "mdate": 1762923668407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Comment"}, "comment": {"value": "We thank all reviewers for their detailed feedback and apologise if parts of the manuscript did not clearly convey the main aims of our work. Several concerns appear to stem from misunderstandings about the scope and focus of the paper. We have distilled the paper to the following main points:\n\n- **Problem statement.** Recent continual learning (CL) studies reveal a stability gap -- a temporary drop in performance on past tasks when new ones are introduced -- highlighting an imbalance between rapid adaptation and long-term retention. This work introduces an adaptive gain mechanism that acts as a multi-timescale, task-agnostic optimizer by splitting weight updates into fast and slow components. This mechanism also induces a weight reparametrization that alters the effective curvature of the loss landscape during the forward pass, leading to smoother and more stable gradient steps during learning and effectively reducing the stability gap in online continual learning settings. We specifically test this under full-replay continual learning benchmarks to isolate the stability gap from other sources of forgetting, and observe that our method consistently attenuates the stability gap across benchmarks.\n\n- **Scope of benchmarks.** We use joint training and small task streams intentionally to isolate the stability gap from other sources of forgetting and study its origin in a clean setting. Our goal is to analyse how the optimiser contributes to transient instabilities, not to propose a full CL system. We will make this distinction explicit.\n\n- **Choice of baselines.** Our method operates at the optimiser and weight-update level, so standard optimisers (SGD, MSGD, Adam) are the most appropriate baselines. Recent stability-gap methods rely on architectural changes, pretrained models, or task-dependent components that are not directly comparable to our online task-agnostic setting.\n\nWe appreciate the reviewers' insights and will revise the manuscript to clarify the aims of the study, improve the ML-oriented framing, and better articulate how NGM-SGD fits within the broader continual-learning landscape. We believe these revisions will significantly improve the clarity and impact of the work.\n\nAdditionally, we would like to highlight updated performance gains in Table 1. We acknowledge that the original computation of avg-min-ACC and WC-ACC mistakenly included Task 1, which does not reflect plasticity-stability trade-offs. Recomputing these metrics for tasks $k=2, ..., T$, shows better performance of NGM-SGD against other optimizers, consistent with the reported figures. avg-ACC and avg-SG remain the same as initially presented. The corrected Table 1 will be included in the revised manuscript.\n\n| Benchmark            | Method   | avg-min-ACC           | WC-ACC               |\n|----------------------|----------|------------------------|-----------------------|\n| Split MNIST          | NGM-SGD (ours)  | **97.570 ± 0.465**     | **97.696 ± 0.373**    |\n|                      | MSGD     | 72.518 ± 0.403         | 77.628 ± 0.329        |\n|                      | Adam     | 71.047 ± 3.753         | 76.537 ± 3.003        |\n|                      | SGD      | 95.887 ± 2.887     | 96.293 ± 2.311    |\n| Split CIFAR-10       | NGM-SGD (ours)  | **79.485 ± 3.875**     | **80.880 ± 3.349**    |\n|                      | MSGD     | 52.828 ± 3.898         | 58.348 ± 3.521        |\n|                      | Adam     | 63.450 ± 7.839         | 67.692 ± 6.335        |\n|                      | SGD      | 68.712 ± 4.958         | 71.868 ± 4.007        |\n| Split mini-ImageNet  | NGM-SGD (ours)  | **33.165 ± 2.435**     | **36.040 ± 2.058**    |\n|                      | MSGD     | 26.775 ± 2.417         | 30.416 ± 2.029        |\n|                      | Adam     | 28.300 ± 3.371     | 32.052 ± 2.768    |\n|                      | SGD      | 28.345 ± 3.671     | 30.288 ± 3.309        |\n| Rotated MNIST        | NGM-SGD (ours)  | **90.015 ± 1.451**     | **91.542 ± 0.973**    |\n|                      | MSGD     | 84.329 ± 1.359         | 87.823 ± 0.914        |\n|                      | Adam     | 87.177 ± 1.060         | 89.910 ± 0.721    |\n|                      | SGD      | 89.107 ± 1.935     | 90.787 ± 1.300    |\n| Domain CIFAR-100     | NGM-SGD (ours)  | **53.420 ± 2.804**     | **58.290 ± 1.916**    |\n|                      | MSGD     | 51.370 ± 2.234     | 56.867 ± 1.556    |\n|                      | Adam     | 51.615 ± 2.653     | 57.323 ± 1.840    |\n|                      | SGD      | 49.050 ± 3.177     | 54.950 ± 2.229    |"}}, "id": "2KuEoeQZZc", "forum": "eecCPK8upD", "replyto": "eecCPK8upD", "signatures": ["ICLR.cc/2026/Conference/Submission12882/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12882/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12882/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763419164861, "cdate": 1763419164861, "tmdate": 1763419164861, "mdate": 1763419164861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the stability gap in continual learning by introducing uncertainty-modulated gain dynamics as a two-timescale gradient descent adjustment  that balances adaptation and retention by adjusting learning rates and flattening the energy landscape. The authors demonstrate analytically that this neuron modulation gain induces fast-slow weight scales and flattens the local loss surface near the minima. The authors evaluate their method on MNIST, CIFAR, and mini ImageNet continual learning benchmarks against baseline optimizers: momentum-SGD, Adam, and SGD."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Empirical evidences shows that NGM-SGD reduces test loss at task boundaries.\n- Empirical evidence shows that NGM-SGD reduces the stability gap."}, "weaknesses": {"value": "- See the first bullet point in the questions, why is it necessary to compare NGM-SGD only to other optimizers: SGD, Adam, MSGD? Could there not exist some continual learning method that outperforms MSGD in the metrics illustrated in Table 1? Given this lack of a comparison to existing continual learning methods, why do the results support the efficacy of NGM-SGD?\n- Overall, the empirical results are mixed, see Table 1. For instance, the baseline optimizers attain comparable if not often better performance on many of the reported metrics, than NGM-SGD. While some clear benefits of NGM-SGD are observed, the overall mixed results and limited scale and scope of the experiments puts into question the efficacy of the method.\n- While the biological motivation well motivates the proposed method, for many readers parsing this information can be difficult. It would be useful for many readers if the algorithm and contributions were distilled algorithmically, rather than solely being motivated from a neurological phenomenon, earlier in the paper."}, "questions": {"value": "- Why are only SGD, Adam, and MSGD evaluated against NGM-SGD? Could there be existing CL methods that outperform NGM-SGD and would be worthwhile comparing? How should we think of NGM-SGD interfacing with other continual learning interventions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vV8MQhwVTJ", "forum": "eecCPK8upD", "replyto": "eecCPK8upD", "signatures": ["ICLR.cc/2026/Conference/Submission12882/Reviewer_CCxn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12882/Reviewer_CCxn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047357712, "cdate": 1762047357712, "tmdate": 1762923668073, "mdate": 1762923668073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces a method that effectively decomposes weights into two components, a slow component and a fast component. This is implemented as a neuronal gain that is multiplied to the weights of the neural network. At each step, this gain is decayed to some base value and increased based on the neural network’s uncertainty, which is biologically inspired by noradrenaline. The paper shows that their approach mitigates the “stability gap” that occurs in continual learning when switching tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of using gain modulation as a flexible way to handle distribution shifts, and showing it can help with the stability gap is good, and it is empirically validated in the supervised continual learning experiments that are presented.\n- Neuronal gain as a proxy for task complexity is interesting. The results make sense as the neuronal gain is essentially moving average of the entropy of the outputs."}, "weaknesses": {"value": "- Overall, while the method does result in an optimizer that mitigates the stability gap, it does seem to do that at the expense of overall performance. \n- The proposed method has significantly more hyperparameter configurations evaluated compared to the baselines (15x more). This could very easily be the reason for any performance gains of NGM-SGD.\n- I am not sure leaning so heavily into the biological framing is useful/correct. One of the contributions is listed as “We link our algorithmic gain bursts to noradrenergic neuromodulation” but there is not really much in the paper linking what happens in the biology to what’s happening in the artificial networks, other than a vague notion of uncertainty. Saying it’s biologically inspired is fine, but framing one of the contributions of your paper as “noradrenergic neuromodulation” seems like overclaiming. It’s also unclear if what is used for the uncertainty proxy in the paper (the entropy of the softmax) is valid given the networks are uncalibrated.\n- I think a bit more work should be done on the dynamics of how the gain evolves with how the output evolves. Specifically, if the gain goes up, the norm of all weights increases, how does that affect the effective learning rate?"}, "questions": {"value": "- The flattening effect that you mention, isn’t it mitigated by the network taking effectively smaller steps? Could you give more detail into how the trajectory followed by the optimizer would look like the trajectory in a flattened minima?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nehyDHGxpy", "forum": "eecCPK8upD", "replyto": "eecCPK8upD", "signatures": ["ICLR.cc/2026/Conference/Submission12882/Reviewer_NrGM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12882/Reviewer_NrGM"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762387719932, "cdate": 1762387719932, "tmdate": 1762923667811, "mdate": 1762923667811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}