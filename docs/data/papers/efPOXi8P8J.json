{"id": "efPOXi8P8J", "number": 8538, "cdate": 1758090279761, "mdate": 1759897777885, "content": {"title": "LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization", "abstract": "The advent of autonomous agents is transforming interactions with Graphical User Interfaces (GUIs) by employing natural language as a powerful intermediary. Despite the predominance of supervised fine-tuning (SFT) methods in current GUI agents for achieving spatial localization, these methods face substantial challenges due to their limited capacity to accurately perceive positional data. Existing strategies, such as reinforcement learning, often fail to assess positional accuracy effectively, thereby restricting their utility. In response, we introduce Location Preference Optimization (LPO), a novel approach that leverages locational data to optimize interaction preferences. LPO uses information entropy to predict interaction positions by focusing on zones rich in information. Besides, we further introduce a dynamic location reward function based on physical distance, reflecting the varying importance of interaction positions. Supported by Group Relative Preference Optimization (GRPO), LPO facilitates an extensive exploration of GUI environments and significantly enhances interaction precision. Comprehensive experiments demonstrate LPO's superior performance, achieving SOTA results across both offline benchmarks and real-world online evaluations. Our code will be made publicly available soon.", "tldr": "We introduce Location Preference Optimization (LPO), a novel method that enhances GUI interactions by utilizing locational data and information entropy to improve spatial accuracy.", "keywords": ["GUI Agent Interaction", "Location Preference Optimization"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56310043b304059edd7fca9d062561a5294a4957.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Location Preference Optimization (LPO), a reinforcement learning framework for improving GUI agent interactions by optimizing spatial accuracy. The method defines two additional reward functions — (1) a window-based information density reward based on image entropy, and (2) a dynamic location reward based on the Euclidean distance between predicted and ground-truth coordinates. These rewards are integrated into a GRPO (Group Relative Preference Optimization) structure to fine-tune multimodal LLM-based GUI agents. The authors claim that LPO achieves state-of-the-art results on several GUI interaction and grounding benchmarks such as Mind2Web, VisualWebBench, ScreenSpot V2, and WebVoyager."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and clearly organized. Each component of LPO is systematically introduced with visual illustrations\n\n2. The experimental section is extensive, including multiple offline and online benchmarks as well as ablations.\n\n3. Reported results show consistent gains across several metrics and datasets, indicating that the method is at least empirically effective."}, "weaknesses": {"value": "1. The proposed LPO framework builds upon GRPO and mainly introduces two additional handcrafted reward functions. While these rewards are intuitively reasonable and practically useful, they appear to be straightforward extensions of existing concepts in spatial reasoning and reinforcement learning. The paper would benefit from clarifying the conceptual novelty or theoretical insight beyond this combination.\n\n2. The claim that SFT or prior RL methods \"cannot assess positional accuracy effectively\" is not strongly substantiated. The paper fails to provide an analysis or diagnostic experiment showing why existing reward formulations fail."}, "questions": {"value": "no question"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HGV1OBlkMr", "forum": "efPOXi8P8J", "replyto": "efPOXi8P8J", "signatures": ["ICLR.cc/2026/Conference/Submission8538/Reviewer_NSyy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8538/Reviewer_NSyy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761223773260, "cdate": 1761223773260, "tmdate": 1762920397420, "mdate": 1762920397420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LPO, a new approach for improving how autonomous agents interact with GUIs using natural language. Existing methods based on SFT and RL struggle with precise positional understanding, which affects interaction accuracy. LPO addresses these limitations by utilizing information entropy to focus on informative regions of the interface and employing a dynamic reward based on physical distance to better evaluate interaction positions. Supported by GRPO, LPO enables more comprehensive exploration and optimization within GUI environments. Experimental results show that LPO achieves superior performance on offline benchmarks and online evaluations, surpassing previous approaches in GUI interaction and grounding tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes an comprehensive and innovative approach to improving spatial accuracy for GUI agents. By integrating window-based information entropy and a dynamic location-based reward, the methodology provides a precise and context-sensitive scheme for guiding agent interactions. This dual-reward design tackles key shortcomings in existing spatial localization strategies, which often relied on coarser IoU thresholds or rigid boundary rules. Although the idea is simple itself, it works for GUI agents's spatial accuracy.\n- The use of GRPO as an optimization backbone facilitates extensive exploration and fine-grained preference tuning, enhancing both the breadth and depth of agent learning. Another major strength is the rigorous evaluation across both offline and online benchmarks, where the proposed framework consistently achieves strong, state-of-the-art performance.\n- The paper’s mathematical clarity and well-structured methodology make its contributions easier to understand. The practical alignment with natural language-driven interfaces emphasizes its relevance for real-world multimodal agents."}, "weaknesses": {"value": "- I strongly suggest that the authors address the citation problems present in the original paper. Proper and consistent citation is essential for both the credibility of the work and for guiding readers to relevant prior research. When the author's name is part of your sentence, include only the year in parentheses after the name. When the author and year are both in parentheses, put both together in \"()\".\n-  Some design decisions, such as the choice of grid resolution for window partitioning and the parameters used for entropy computation, are set empirically and may require further adaptation for different GUI environments or tasks. This reliance on heuristics can limit the method’s out-of-the-box generalizability. How does the choice of grid resolution (M, N) for window partitioning affect the performance of the information entropy-based reward?\n- The dependence on exact action-type matching in the reward calculation may also magnify penalties for misclassification, which could undermine improvements in spatial precision if the agent struggles to identify the correct action type."}, "questions": {"value": "- What strategies are used to compute the normalized histogram for entropy calculation, and how sensitive is the entropy measure to the number of bins (B)?\n- When aggregating per-point rewards, how are multiple targets with overlapping spatial coordinates handled to avoid over-penalization?\n- Does the methodology support non-rectangular or dynamically resizable GUI window partitions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6bINNAdJLe", "forum": "efPOXi8P8J", "replyto": "efPOXi8P8J", "signatures": ["ICLR.cc/2026/Conference/Submission8538/Reviewer_2MDH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8538/Reviewer_2MDH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636074987, "cdate": 1761636074987, "tmdate": 1762920395739, "mdate": 1762920395739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles precise spatial localization for GUI agents by introducing LPO. LPO uses a novel reward function combining information entropy to find important zones and a dynamic location reward based on physical distance for precision. This reward, optimized via GRPO, achieves SOTA results on multiple GUI benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Intuitive design. Using physical distance as a continuous precision signal is a clear improvement over prior coarse, binary rewards.\n- Strong empirical performance, achieving SOTA results across a comprehensive suite of offline and online benchmarks .\n- Clear writing."}, "weaknesses": {"value": "- My main concern is the core assumption for the $r_w$ reward: that high information entropy correlates with interaction likelihood 1. This heuristic seems brittle and non-generalizable. In many realistic scenarios, the opposite can be true;  For example, in a minimalist UI, a critical \"Submit\" button might be a simple, large, solid-color block (very low entropy). Conversely, a non-interactive, complex advertisement banner on the same page would have very high entropy. In this common case, $r_w$ would actively penalize the correct action and reward focusing on the distractor. This suggests the $r_w$ component may be learning a spurious correlation specific to the benchmark data.\n\n- The paper mentions \"Significant Computational Overhead\". If LPO is an order of magnitude more expensive than baselines for a modest accuracy gain, I believe the overall contribution is limited. Specifically, how much more computationally expensive is it (for training and inference, respectively)? And for each, are there potential ways to reduce this cost?"}, "questions": {"value": "- (from weakness 1) Have the authors tested robustness in scenarios where entropy negatively correlates with the target (e.g., minimalist UIs vs. complex, non-interactive backgrounds)?\n\n- Why was a pixel-level entropy metric chosen over a more semantic, learned approach? For example, the vision model's own semantic features could be used to predict a general \"interactability map\" (e.g., a map predicting the likelihood of a region being a button, link, or text field). While this would require training on an annotated dataset (potentially necessitating new labeling effort if existing datasets are insufficient), this learned $r_w$ would seem to solve the heuristic's core weakness and be far more generalizable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5gRpdTdJYy", "forum": "efPOXi8P8J", "replyto": "efPOXi8P8J", "signatures": ["ICLR.cc/2026/Conference/Submission8538/Reviewer_J5vV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8538/Reviewer_J5vV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814036912, "cdate": 1761814036912, "tmdate": 1762920395322, "mdate": 1762920395322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The current paper proposes a method for fine-tuning GUI agents, focusing on interaction positions, using a novel reward function in a GRPO-inspired loss.\n\nPrecisely, the reward is the product of an inverse distance to target bonus and a reward derived from the information entropy of the window (patch) that contains that location.\n\nThe intuition is that the agent should be rewarded for focusing on areas rich in information."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- clear presentation\n- strong empirical results"}, "weaknesses": {"value": "- given that the novelty comes mainly from the information-"}, "questions": {"value": "1. Isn't this method actually using standard GRPO, and in fact just proposing a novel reward function?\n2. Is the histogram of colours really a measure of how rich in interactive elements the area is? For example, an image would be a big distractor for this metric. Or 10 small buttons or a large one could lead to the same pixel value histogram."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tvySCcrrXI", "forum": "efPOXi8P8J", "replyto": "efPOXi8P8J", "signatures": ["ICLR.cc/2026/Conference/Submission8538/Reviewer_fcd7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8538/Reviewer_fcd7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956184134, "cdate": 1761956184134, "tmdate": 1762920394842, "mdate": 1762920394842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce Location Preference Optimization (LPO) , an approach to leverages locational data to optimize interaction preferences. LPO employs Group Relative Preference Optimization (GRPO) for preference optimization. \n\nReward design is designed in two parts, 1) information entropy, to predict interaction positions by focusing on zones rich in information; 2) a dynamic location reward function based on physical distance, reflecting the varying importance of interaction positions. Comprehensive experiments demonstrate LPO’s superior performance, achieving SOTA results across both offline benchmarks and real-world online evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is simple but effective with strong generalization across different tasks.\n\n2. The reward design raises the information entropy metric, which is effective according to author’s experiments, but ignored by previous methods."}, "weaknesses": {"value": "Key details were somewhat lacking in methodology and experiment parts - please see questions section below."}, "questions": {"value": "1. In section 4.1, the screenshot is divided into M times N non-overlapping rectangles, but the settings of hyper-parameters M, N should be discussed. \n\n2. In fig. 2, why the bottom-right has a 1.0 entropy score. What is “Key interactive areas, such as login, search, and editing zones, align with user interaction tendencies” means and how to achieve that. \n\n3. In section 4.2, Euclidean distance refers to the same reward along with the circle, while most of the icon is designed as rectangle. The contradiction should be discussed.\n\n4. In section 4.2, why there is an averaging operation across K coordinates, since each step only has single ground-truth. \n\n5. In section 5.1, most training details should be discussed, like the type of training data in SFT phase, the data composition and dynamics during RL training. \n\n6. All the experiments are built upon the fine-tuned Ovis-2-8B, is there any special design in SFT phrase before LPO, could other models in different architecture benefit from LPO, like Qwen or Intern family?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1vjJSyHDbt", "forum": "efPOXi8P8J", "replyto": "efPOXi8P8J", "signatures": ["ICLR.cc/2026/Conference/Submission8538/Reviewer_JXud"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8538/Reviewer_JXud"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152467417, "cdate": 1762152467417, "tmdate": 1762920394446, "mdate": 1762920394446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}