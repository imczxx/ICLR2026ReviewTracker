{"id": "XPIEkFdEDi", "number": 12936, "cdate": 1758211839209, "mdate": 1763658897326, "content": {"title": "AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs", "abstract": "The deployment of large language models (LLMs) is increasingly constrained by memory and latency bottlenecks, motivating the need for quantization techniques that flexibly balance accuracy and efficiency. \nRecent work has introduced multi-precision models, which enable inference at multiple precisions within a single model depending on runtime constraints.\nTo support such flexibility, quantized weights are often stored as bit-planes, where hardware efficiency improves when the compute operates directly at the bit-plane level and activates only the precision required by each request.\nIn this work, we present AnyBCQ, a hardware-friendly multi-precision extension of Binary-Coded Quantization (BCQ) that supports direct bit-plane operations.\nBy representing weights as binary bit-planes with corresponding scale factors, AnyBCQ enables bit-plane–level computation and maps naturally to accelerator-friendly, bit-parallel arithmetic.\nOur progressive precision expansion mechanism incrementally refines scaling factors while reusing previously assigned binary codes, yielding monotonic improvements in accuracy as additional bits are enabled. \nWe further co-design a specialized kernel that exploits the BCQ structure to support dynamic per-request precision selection with negligible overhead. \nExperiments on recent LLMs demonstrate that AnyBCQ significantly narrows the accuracy drop in the low-bit regime (e.g. 2-bit), remains competitive at higher precision, and achieves throughput gains of up to $3.0\\times$ over half precision and $1.2\\times$ over state-of-the-art multi-precision methods. \nBy aligning algorithmic flexibility with hardware efficiency, AnyBCQ provides a practical foundation for multi-precision LLM deployment across diverse service-level objectives.", "tldr": "", "keywords": ["LLM", "Quantization", "Anyprecision"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ede8a435ee109ca53e05eab0fd2da4be2bb47419.pdf", "supplementary_material": "/attachment/adf0c08d52cedcd0876a1a350cbec863a20bcf10.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents AnyBCQ, a quantization framework that encodes each weight in a large language model as binary bit-planes plus scaling factors, enabling direct bit-plane–level operations. This representation allows dynamic per-request precision control (i.e. using fewer bits when possible) with negligible runtime overhead, and the authors design a specialized CUDA kernel to exploit this structure efficiently. In experiments, AnyBCQ significantly reduces accuracy loss in extremely low-bit regimes (e.g. 2-bit), remains competitive at higher precisions, and achieves throughput speedups up to ~3.0× over half precision and ~1.2× over existing multi-precision methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper proposes an algorithm-system codesign to support multi-precision LLM with BCQ. The method is novel and the problem (multi-precision LLM) is meaningful.\n\n2. The methodology is written with clarity and the diagrams are easy to follow.\n\n3. I agree that BCQ is more hardware-friendly as compared to k-means methods like AnyPrecision LLM. \n\n4. Both acc and latency are improved with AnyBCQ."}, "weaknesses": {"value": "1. Could you explain why BCQ is better than non-uniform quantization like SqueezeLLM in terms of accuracy? I am not very familiar with quantization work using BCQ. Is it comparable with the SOTA?\n\n2. The evaluation datasets are mainly on classification. How about the acc/latency performance on generative dataset like HumanEval?\n\n3. The speedup over AnyPrecision LLM is not that significant and for one point, it is even slower. Could you explain why there is that outlier?"}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UECaJN5Efy", "forum": "XPIEkFdEDi", "replyto": "XPIEkFdEDi", "signatures": ["ICLR.cc/2026/Conference/Submission12936/Reviewer_7UfK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12936/Reviewer_7UfK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760482308389, "cdate": 1760482308389, "tmdate": 1762923698271, "mdate": 1762923698271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank the reviewers for their valuable feedback. In the revised manuscript, we have adopted the following color scheme to make it easier to identify changes made in response to each reviewer: \n\nReviewer d4yW – Orange, \n\nReviewer fpbz – Blue, \n\nReviewer iC2H – Red, \n\nReviewer 7UfK – Violet."}}, "id": "Ak4z0pprCT", "forum": "XPIEkFdEDi", "replyto": "XPIEkFdEDi", "signatures": ["ICLR.cc/2026/Conference/Submission12936/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12936/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12936/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763659000985, "cdate": 1763659000985, "tmdate": 1763659000985, "mdate": 1763659000985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AnyBCQ proposes a hardware-efficient multi-precision quantization framework for LLMs, built upon Binary-Coded Quantization (BCQ). The method introduces a progressive precision expansion mechanism that allows a single model to run inference at multiple bit-widths (e.g., 2, 3, or 4-bit) using shared binary bit-planes and per-precision scaling factors.  AnyBCQ operates directly on binary bit-planes, which aligns naturally with accelerator-friendly arithmetic. They also designed a CUDA kernel that supports dynamic per-request precision selection with negligible overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- AnyBCQ tightly integrates algorithmic quantization with practical kernel-level optimization.\n- The proposed mechanism allows incrementally adding new bit-planes derived from residuals while freezing previously learned binary codes.\n- Authors compare their method with state-of-the-art methods and provide similar or better accuracy-throughput trade-offs. \n- Authors provide a well structured algorithm with reproducibility plans including CUDA kernel code release."}, "weaknesses": {"value": "- All experiments are conducted on NVIDIA A100 GPUs. It would strengthen the paper to validate results on other GPU generations ( Hopper, Ada etc) or alternative hardware such as TPUs or custom inference accelerators.\nGiven that AnyBCQ supports dynamic bit-width selection, it would be particularly interesting to test it on novel architectures with native mixed-precision or variable-bit support.\n\n- The paper lacks energy or memory bandwidth measurements, which are crucial to substantiate the claimed hardware efficiency.\n- The paper lacks theoretical depth including any formal conditions."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qKTjPR2Z16", "forum": "XPIEkFdEDi", "replyto": "XPIEkFdEDi", "signatures": ["ICLR.cc/2026/Conference/Submission12936/Reviewer_iC2H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12936/Reviewer_iC2H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463022726, "cdate": 1761463022726, "tmdate": 1762923698001, "mdate": 1762923698001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present the AnyBCQ framework for multi-precision LLMs based on binary-coded quantization (BCQ). The authors solve system-level challenges for executing multi-precision models and validate their design with CUDA kernels. The innovations avoid bit-transposition and LUT lookups by executing directly on the bit-planes, which they compare to Any-Precision LLM as a baseline and show consistent benefits."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to follow\n- The problem is well-described and sufficiently motivated, although LUT lookups can be hardware accelerated.\n- The benefits are consistent and clear, and bit-transposition is consistently a challenging overhead to work around in existing literature."}, "weaknesses": {"value": "- While the work effectively addresses system-level challenges, its main usage of quantization and error recovery largely build on established methods. The contributions may be better aligned with venues primarily focused on infrastructure and software systems. That said, the results are valuable for advancing practical LLM deployment.\n- The authors attribute performance gains to both bit-transposition and LUT lookups, but the kernel evaluation is only presented at a high level. While the bit-transposition challenge is well-recognized, it remains unclear how much the LUT lookup contributes to overall latency. A breakdown of the latency components would strengthen the analysis, particularly since LUT operations could be hardware-accelerated."}, "questions": {"value": "- What is the latency breakdown of AnyPrecisionLLM vs. AnyBCQ? How much overhead is bit-transposition vs. LUT lookup?\n- Have the authors tested larger models, like Qwen3-32B or Llama3-70B? It is often observed that some optimizations may perform differently in these larger models.\n- The authors claim that the performance gap increases as matrix sizes grow, but the throughput gains seem fairly constant on end-to-end evaluations (+/- 20 tokens per second). How is this rationalized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EksKrs9zxO", "forum": "XPIEkFdEDi", "replyto": "XPIEkFdEDi", "signatures": ["ICLR.cc/2026/Conference/Submission12936/Reviewer_fpbz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12936/Reviewer_fpbz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778586715, "cdate": 1761778586715, "tmdate": 1762923697713, "mdate": 1762923697713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AnyBCQ, a multi-precision quantization framework for LLMs based on Binary-Coded Quantization (BCQ). The core idea is to share binary bit-planes across precisions while storing precision-specific scale factors, enabling hardware-efficient, dynamic-precision inference."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The central idea of extending BCQ to a multi-precision setting via shared binary codes and specialized, per-precision scales is novel and interesting.\n2. Figures (e.g., Figure 1, Figure 3) are effective in visualizing the method's mechanics and differentiating it from prior non-uniform quantization work.\n3. The co-design of the quantization algorithm with a hardware-efficient CUDA kernel, which avoids the typical overheads of non-uniform methods, is a strength."}, "weaknesses": {"value": "1. The paper's core contribution, sharing binary codes while maintaining separate scales, is not highlighted sufficiently early. The introduction should be more compact and should proactively introduce the fundamentals of BCQ. Many readers may be more familiar with index-based quantization (e.g., AWQ/GPTQ), and this method is different. Visualizing the storage savings (from Table 1) earlier in the paper could help ground this contribution.\n\n2. In Table 2, the \"Proposed (fixed-precision)\" baseline outperforms ShiftAddLLM, especially at 2/3 bits. The paper needs to clarify if \"fixed-precision\" is a plain BCQ implementation or if it includes additional optimizations not present in ShiftAddLLM.\n\n3. ShiftAddLLM is absent from the end-to-end performance evaluations in Table 4 and Figure 4. Please provide.\n\n4. The \"fixed-precision\" model, lacking the shared-binary constraint, should theoretically always outperform the \"multi-precision\" model. However, in Table 2, the multi-precision model occasionally scores higher. Please explain. Providing perplexity scores (C4/Wikitext) for both fixed- and multi-precision models would also be beneficial.\n\n5. The motivation for multi-precision models is dynamic, runtime adaptation (e.g., token-level precision switching). However, all evaluations are static (i.e., the entire inference is run at a single, fixed bit-width). The paper would be much stronger if it included a case study demonstrating a practical scenario that achieves a better accuracy/latency trade-off than any single static precision.\n\n6. The paper's most significant gains are in the 2-bit regime. My major concern is about practical utility. As shown in Table 4, the 2-bit perplexity is extremely high (e.g., 19.01 for Llama-8B), rendering the model practically unusable for coherent generation. Given that 4-bit quantization is acknowledged as nearly lossless with tolerable latency, the paper should provide a compelling use case or scenario where such a high-perplexity 2-bit model would actually be deployed. Or, illustrate scenarios where parts of the response could be generated at 2-bit precision without significantly harming overall quality."}, "questions": {"value": "Please see the weaknesses section and clarify these points in your rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FbWl7lPH0G", "forum": "XPIEkFdEDi", "replyto": "XPIEkFdEDi", "signatures": ["ICLR.cc/2026/Conference/Submission12936/Reviewer_d4yW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12936/Reviewer_d4yW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892832497, "cdate": 1761892832497, "tmdate": 1762923697443, "mdate": 1762923697443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}