{"id": "a4z7OlgSxC", "number": 13609, "cdate": 1758219778296, "mdate": 1763408037784, "content": {"title": "Q-learning with Posterior Sampling", "abstract": "Bayesian posterior sampling techniques have demonstrated superior empirical performance in many exploration-exploitation settings. However, their theoretical analysis remains a challenge, especially in complex settings like reinforcement learning.\nIn this paper, we introduce Q-Learning with Posterior Sampling (PSQL), a simple Q-learning-based algorithm that uses Gaussian posteriors on Q-values for exploration, akin to the popular Thompson Sampling algorithm in the multi-armed bandit setting. We show that in the tabular episodic MDP setting, PSQL achieves a regret bound of $\\tilde O(H^2\\sqrt{SAT})$, closely matching the known lower bound of $\\Omega(H\\sqrt{SAT})$. Here, S, A denote the number of states and actions in the underlying Markov Decision Process (MDP), and $T=KH$ with $K$ being the number of episodes and $H$ being the planning horizon. Our work provides several new technical insights into the core challenges in combining posterior sampling with dynamic programming and TD-learning-based RL algorithms, along with novel ideas for resolving those difficulties. We hope this will form a starting point for analyzing this efficient and important algorithmic technique in even more complex RL settings.", "tldr": "Near optimal regret bounds for Q-learning with posterior sampling algorithm in a tabular episodic RL setting", "keywords": ["Reinforcement Learning Theory", "Regret Analysis", "Posterior Sampling", "Q-learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5fd6e609e802bc3e4c3dc87543d0e96d0735cb05.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Posterior Sampling Q-Learning (PSQL), a tabular method that maintains Gaussian posteriors over Q-values and uses posterior samples for exploration; to obtain a $\\tilde{O}(H^{2}SAT^{1/2})$ regret bound, the analyzed variant makes the bootstrapped target optimistic by taking the maximum over multiple posterior draws for the next-state value. While the vanilla single-sample version exhibits strong empirical performance, the optimistic variant required for the theory is not competitive, lagging behind RLSVI and offering no gains over Staged-RandQL on the authors’ own benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Provides a near-optimal $\\tilde{O}(H^{2}SAT^{1/2})$ regret analysis for a posterior-sampling variant of tabular Q-learning.\n- The vanilla practical instantiation consistently outperforms classical baselines such as UCB-QL and Staged-RandQL on the reported tasks, showing promise for posterior-sampling driven exploration in tabular settings."}, "weaknesses": {"value": "- The empirically evaluated “vanilla” algorithm differs from the theoretically analyzed optimistic variant; the latter is not competitive with RLSVI or Staged-RandQL on the reported benchmarks, leaving the practical relevance of the theory unclear. This mismatch makes it difficult to understand the paper’s positioning: the theoretically grounded algorithm underperforms in practice and does not improve existing regret bounds, yet the practical algorithm—which shows promise—is not benchmarked against other practical exploration baselines on more demanding tasks.\n- Experimental coverage is very limited (two toy tabular environments) and omits stronger practical baselines (e.g., recent posterior-sampling or optimistic methods); if the practical implementation departs from the analyzed algorithm, broader comparisons to other exploration strategies become essential, but they are absent here.\n- Presentation can be clarified: the introduction currently reads as an unstructured list of related work rather than positioning the contribution, and some statements about model-based methods could be refined.\n- The “first Bayesian posterior” claim is confusing: several prior works already use posterior-sampling within Q-learning, so the novelty of this phrasing is unclear and should be clarified."}, "questions": {"value": "- Given that the analyzed optimistic variant performs worse than RLSVI and Staged-RandQL, how do the authors reconcile the theoretical guarantees with practical usefulness? Can they provide intuition or evidence that the analyzed algorithm offers benefits beyond these baselines?\n- If the practical PSQL* deviates from the analyzed variant, do the authors plan to benchmark it against more practical exploration algorithms such as [1] on more challenging domains (e.g., Atari, Maze2D) to establish empirical competitiveness?\n- In lines 118–119 the paper says model-based methods directly model the rewards and transitions “instead of” the implied value function or policies; could the authors clarify this phrasing? In practice, model-based RL often uses learned models to improve the value function and/or policy.\n- What exactly are the authors claiming as “first” in the first contribution point, “provided by the Bayesian posterior”? Could they clarify how their notion of “Bayesian posterior” differs from earlier posterior-based Q-learning efforts (e.g., [1]) and what concrete novelty they intend to highlight?\n\n[1] Ishfaq et al., Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo, ICLR 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IFIDcjbn2M", "forum": "a4z7OlgSxC", "replyto": "a4z7OlgSxC", "signatures": ["ICLR.cc/2026/Conference/Submission13609/Reviewer_7R7w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13609/Reviewer_7R7w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761261718726, "cdate": 1761261718726, "tmdate": 1762924192400, "mdate": 1762924192400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Missing related work and comparison against existing posterior-sampling based Q-learning algorithms"}, "comment": {"value": "Dear Authors,\n\nWe would like to point out some highly related previous works that the authors failed to mention and discuss which can be seen as efforts to design posterior-sampling based/ Bayesian Q-learning algorithms.\n\nIn Line 224, the authors claim that \"We first present a Bayesian posterior-based derivation of the Q-learning update rule\". However, [1], [2], [3] -- all of them can be seen as posterior based Q-learning algorithm. We thank the `Reviewer 7R7w` for pointing out [2] as an earlier effort in this direction. We would appreciate if the authors could acknowledge and compare against these earlier efforts in their submission.\n\nIn Line 201-203, the authors say that:\n\n> There have been relatively limited studies on model-free, sample-efficient and computationally efficient Bayesian algorithms. Dann et al. (2021) proposed one such framework but is computationally intractable. Our work aims to fill this gap.\n\nHowever, in [3], we provided a tractable and provably efficient version of Feel-Good Thompson Sampling algorithm along with thorough experiments in Atari and `N-Chain` environments.\n\nIn Line 373, the authors discuss mutliple sampling techniques. Multiple sampling for gaining optimism is also used in LSVI-PHE in [1]. Could you please discuss how the approach presented here differs from that of LSVI-PHE?\n\n\n[1] Ishfaq et al., Randomized Exploration for Reinforcement Learning with General Value Function Approximation, ICML 2021\n\n[2] Ishfaq et al., Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo, ICLR 2024\n\n[3] Ishfaq et al., More Efficient Randomized Exploration for Reinforcement Learning via Approximate Sampling, RLC 2024"}}, "id": "FeKVr4p5Wq", "forum": "a4z7OlgSxC", "replyto": "a4z7OlgSxC", "signatures": ["~Haque_Ishfaq1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Haque_Ishfaq1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13609/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763263715753, "cdate": 1763263715753, "tmdate": 1763263715753, "mdate": 1763263715753, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed a model-free exploration method based on introducing the idea of Randomized Least-Squares Value Iteration (RLSVI) into the Q-learning framework. This algorithm, called Posterior Sampling Q-learning (PSQL), uses a Gaussian posterior estimate of Q-values and achieves a regret guarantee of $\\tilde{O}(H^2 \\sqrt{SAT})$, which is the same as Q-learning with UCB bonuses or Staged Randomized Q-learning. Additionally, the algorithm shows strong empirical performance on standard low-dimensional benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting alternative explanation of the UCB-Q-learning learning rate, that appears from the additional entropy regularization in the variational approximation, with a clear intuition of \"collapse avoidance\" with entropy due to bias in the estimate;\n- Strong empirical performance as well as theoretical regret guarantees;"}, "weaknesses": {"value": "- Lack of empirical comparison with a usual RandQL. Although this method does not offer the same rigorous guarantees as its staged version, it would be interesting to compare PSQL* and a usual RandQL without stages.\n- The regret bound does not match the regret bound of a variance-reduced version of Q-learning (Li et al. 2021);"}, "questions": {"value": "- It would be beneficial to discuss an RLSVI-style model-based algorithm that achieves the minimax optimal regret guarantee (Xiong et al. 2022).\n- What prevents you from using a standard RSLVI analysis with a single sample there?\n- What prevents you from extending the sketch of the proof in Appendix F to a complete proof for a variance-shaped noise? What is a main challenge there?\n- Is it possible to provide a deep-learning version of your method?\n\n### References\n\nXiong, Z., Shen, R., Cui, Q., Fazel, M., & Du, S. S. (2022). Near-optimal randomized exploration for tabular Markov decision processes. Advances in neural information processing systems, 35, 6358-6371."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NWMFR43bFf", "forum": "a4z7OlgSxC", "replyto": "a4z7OlgSxC", "signatures": ["ICLR.cc/2026/Conference/Submission13609/Reviewer_wHwf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13609/Reviewer_wHwf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400592360, "cdate": 1761400592360, "tmdate": 1762924192021, "mdate": 1762924192021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Q-Learning with Posterior Sampling (PSQL), a model-free reinforcement learning algorithm that employs Gaussian posteriors on Q-values for exploration. By interpreting Q-learning as a Bayesian inference problem with a regularized ELBO objective, the authors design a conceptually simple algorithm that combines single-sample posterior sampling for action selection with multiple-sample optimism for target computation. This design allows them to prove a near-optimal regret bound of O(H^2 \\sqrt(SAT)), while preliminary experiments show competitive or superior empirical performance compared to UCBQL, RLSVI, and Staged-RandQL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The work is theoretically grounded, algorithmically simple, and provides new insights into the Bayesian interpretation of Q-learning. The regret guarantee is strong, and the analysis tackles key challenges in combining posterior sampling with TD learning."}, "weaknesses": {"value": "Using Gaussian posteriors on Q-values may destroy important structural properties of Q-functions (e.g., boundedness or Bellman consistency), since Gaussian distributions are unbounded. The choice of posterior variance is subtle and strongly affects performance, requiring careful tuning. Moreover, the use of multiple posterior samples for target computation increases the algorithm’s computational complexity, and the theoretically unanalyzed single-sample variant (PSQL*) outperforms the analyzed one in practice, indicating a gap between theory and implementation."}, "questions": {"value": "1. Can alternative posterior distributions preserve Q-value structure more faithfully than Gaussians?\n\n2. Is there a principled or adaptive way to select the posterior variance to avoid heuristic tuning?\n\n3. Can the multiple-sampling step for optimism be replaced with a cheaper or more elegant alternative?\n\n4. How does the approach scale to function approximation or deep RL settings?\n\n5. How does posterior optimism interact with TD bootstrapping bias in long horizons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KPMOrA1kGg", "forum": "a4z7OlgSxC", "replyto": "a4z7OlgSxC", "signatures": ["ICLR.cc/2026/Conference/Submission13609/Reviewer_UBp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13609/Reviewer_UBp9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553974980, "cdate": 1761553974980, "tmdate": 1762924191723, "mdate": 1762924191723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce PSQL, a novel model-free method that utilizes Gaussian Bayesian inference on Q-values. This approach incorporates a different target value based on the optimism principle, though this optimistic target value does not influence the actual decision-making process. The paper provides regret bounds that are nearly optimal and comparable to those achieved by other posterior sampling methods. Additionally, the authors establish a modified version, PSQL, which uses the target value chosen as in standard Q-learning. The results demonstrate that PSQL outperforms several baseline methods in tabular environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Authors discuss the limitations of the analysis of the vanilla PSQL algorithm"}, "weaknesses": {"value": "- In my opinion, the empirical results are not sufficiently extensive. It would be interesting, for example, to consider a comparison with the PSRL algorithm, which was shown to outperform Staged-RandQL in a recent study (Tiapkin et al., 2023). Furthermore, the paper lacks a comparison in more complex environments, specifically those with a continuous state space;\n- Another interesting direction would be to extend this algorithm to more practical scenarios with a general state space. If this is possible, what quantity should be chosen for the variance in that setting?\n- What regret bound can be achieved in the \"vanilla version\" of the PSQL algorithm? Could a non-trivial polynomial bound be established\n- It appears there is a potential issue with the inequality  $\\mathbb{E}[\\tilde{X}^{\\text{alt}}| \\bar{\\mathcal{O}}^{\\text{alt}}] \\leq \\underline{X}$ in Lemma E.1. This is because the statement assumes $\\mathbb{E}[\\tilde{X}]\\geq \\underline{X}$, but the inequality seems to require the opposite for the proof to hold."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7ox6h6q2Gp", "forum": "a4z7OlgSxC", "replyto": "a4z7OlgSxC", "signatures": ["ICLR.cc/2026/Conference/Submission13609/Reviewer_ZF74"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13609/Reviewer_ZF74"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904864145, "cdate": 1761904864145, "tmdate": 1762924191395, "mdate": 1762924191395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}