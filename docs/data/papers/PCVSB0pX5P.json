{"id": "PCVSB0pX5P", "number": 7262, "cdate": 1758013532107, "mdate": 1759897863090, "content": {"title": "Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance", "abstract": "The proliferation of Large Language Models (LLMs) has demonstrated remarkable capabilities, elevating the critical importance of LLM safety.  However, existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic protection, failing to ensure safety for the nuanced and complex behaviors of modern LLM systems. To address this problem, we solve LLM safety from legal compliance perspectives, named **safety compliance**. In this work, we posit relevant established legal frameworks as safety standards for defining and measuring safety compliance, including the EU AI Act and GDPR, which serve as core legal frameworks for AI safety and data security in Europe. To bridge the gap between LLM safety and legal compliance, we first develop a new benchmark for safety compliance by generating realistic LLM safety scenarios seeded with legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization (GRPO) to construct a safety reasoner, **Compliance Reasoner**, which effectively aligns LLMs with legal standards to mitigate safety risks. Our comprehensive experiments demonstrate that the Compliance Reasoner achieves superior performance on the new benchmark, with average improvements of +10.45% for the EU AI Act and +11.85% for GDPR.", "tldr": "We provide a novel perspective for LLM safety research, leveraging established legal frameworks as gold standards for ensuring LLM safety.", "keywords": ["Safety", "Legal NLP"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dee2b135e8c7cd6a7250eb8c3993101e68c33099.pdf", "supplementary_material": "/attachment/7ff767db7cc906d9bbaa14b5f686ae09bff20492.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a framework to incorporate legal compliance (EU AI Act and GDPR standards) in LLM safety verification process. First, the authors provide a benchmark based on legal statues to synthesize LLM safety scenarios. Then, a compliance reasoner is finetuned and aligned with GRPO based on a reward that combines safety compliance and response format."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work focuses on a very critical problem as incorporating legal compliance data into LLM safety frameworks is crucial for real-life application.\n- Experiments are extensive and results are promising.\n- The frameworks as a whole is engineered in a sophisticated way with using legal statues as seeds in a tree structure in benchmark creation and a two-stage reasoner optimization.\n- Paper is clear, well-written and well-organized.\n- Source code is provided."}, "weaknesses": {"value": "- The benchmark is entirely synthetic and relatively small. The data was generated by a single model following a specific template. It is unclear whether a model trained on this data would generalize to real-world user queries or legal cases very well.\n- In GRPO, reasoning chain is ignored in the reward calculation.\n- Formatting reward is binary and I am not sure if it correctly reflects whether the response fits the format or not.\n- Overall, this paper feels like a nice engineering work rather than introducing a novel algorithm or framework.\n- Human evaluations are limited (50 samples only)."}, "questions": {"value": "- Do you have a qualitative analysis of error cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fyTsNfRZ8R", "forum": "PCVSB0pX5P", "replyto": "PCVSB0pX5P", "signatures": ["ICLR.cc/2026/Conference/Submission7262/Reviewer_qYCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7262/Reviewer_qYCn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761515862842, "cdate": 1761515862842, "tmdate": 1762919394256, "mdate": 1762919394256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new approach to LLM safety centered on legal compliance, particularly with frameworks like the EU AI Act and GDPR. By creating a benchmark dataset through synthesized safety data, the author develops a model called Compliance Reasoning, trained using Group Relative Policy Optimization (GRPO), to improve LLM safety performance. Using this Compliance Reasoning approach, the paper demonstrates how existing safety data can be extrapolated to ensure alignment with legal compliance standards."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The author presents a new perspective on LLM safety focused on legal compliance, emphasizing its importance for real-world deployment.  \n2. The proposed Compliance Reasoner, fine-tuned with GRPO on the newly introduced benchmark dataset, achieves substantially better legal compliance performance compared to existing LLM safety guard models.  \n3. The author demonstrates that the Compliance Reasoner generalizes effectively to existing safety datasets.  \n4. The paper is clearly written, making it easy to understand and follow."}, "weaknesses": {"value": "1. **Need for Qualitative Analysis**: Qualitative results are essential to better understand how the model ensures legal compliance in safety contexts. For instance, the Compliance Reasoner effectively identifies compliance with regulations such as the EU AI Act and GDPR, demonstrating strong qualitative examples, whereas traditional LLM safety guard models fail to do so and produce unsatisfactory qualitative responses. Additionally, it would be valuable to show how existing LLM safety datasets change or improve after being processed by the Compliance Reasoner.  \n\n2. **Limited Range of Baseline Models**: The evaluation appears to focus only on a narrow range of model sizes (around 8B parameters). Including comparisons with larger models—such as 14B, 32B, or 70B—could provide deeper insight into the scalability and robustness of the proposed approach. Moreover, for closed-source models, it would strengthen the study to include evaluations on a broader range of models (e.g., GPT-5, Claude, and Gemini 2.5 Pro) rather than limiting the comparison to GPT-4o-mini.\n\n3. **Experimental Design and Missing Results**: Similar to the previous point, the experimental setup could be more rigorous. In Section 6.2, while the author compares performance with baseline models, the results for the Compliance Reasoner itself are missing, which weakens the comparison. Additionally, before applying the Compliance Reasoner to existing LLM safety datasets, a human evaluation of those datasets should be conducted. Comparing human-assessed performance before and after applying the Compliance Reasoner would provide stronger evidence of its effectiveness. Finally, running experiments with closed-source models on newly generated safety data would help ensure consistency with the results presented in Tables 3 and 4."}, "questions": {"value": "Q1. Why do the results in Table 1 (EU AI Act) show similar performance across different models? For instance, in Chapter 10, the performance of all general-purpose models appears nearly identical. This raises concerns about the quality or potential noise within the evaluation dataset.\n\nQ2. How do state-of-the-art closed-source models—such as GPT-5, Gemini Pro 2.5, and Claude—perform on the newly generated safety dataset?\n\nThe paper presents a novel perspective on LLM safety; however, the experiments appear insufficient. I would be inclined to raise my evaluation score if the authors clearly address the weaknesses and questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ow0Oa6Mmzu", "forum": "PCVSB0pX5P", "replyto": "PCVSB0pX5P", "signatures": ["ICLR.cc/2026/Conference/Submission7262/Reviewer_5t9Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7262/Reviewer_5t9Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761592001083, "cdate": 1761592001083, "tmdate": 1762919393871, "mdate": 1762919393871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the development of Safety Compliance, based on EU AI Act and GDPR. The main idea is two-fold: (i) to design a benchmark data synthesis using DeepSeek-AI (2025) to generate AI Safety scenarios, such as 1,648 safety compliance case samples from EU AI Act and 1,012 for GDPR; and (ii) to design a RL-based Compliance Reasoner using an existing Group Relative Policy Optimization (GRPO) from Deepseekmath [Shao et.al 2024a]."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of utilizing legal compliance data to design guardrails for LLM Safety Alignment is interesting. Leveraging safety compliance documentations for safety alignment provides additional and value-added perspectives. \n\nThe use of EU AI act and GDPR to showcase the idea and the development of safety compliance reasoner is illustrative and of practical relevance. Also the benchmark benchmark data synthesis using DeepSeek-AI (2025), which generate AI Safety scenarios, such as 1,648 safety compliance case samples from EU AI Act and 1,012 for GDPR, could be of practical value for the safety alignment research community if the authors would make it publicly available. \n\nThe experiments were performed over the 13 chapters of EU AI act and 11 chapters of EU GDPR, and show the comparison of the proposed safety compliance reasoner-GRPO with five LLM families of baseline models and four existing LLM Safety Guardrails chosen by the authors."}, "weaknesses": {"value": "The paper could benefit from clear elaborations from a number of aspects:\n\n(1) Both EU AI act and GDPR are privacy compliance. Hence the safety compliance reasoner is mainly geared on the privacy compliance covered in EU AI act and GDPR. \n\n(1.1) Given the safety alignment covers safety issues also in harmful content, dishonest/non-factual content, misinformation and biases in addition to privacy, it is unclear whether the proposed safety compliance reasoner will be effective in a broader safety context. \n\n(1.2) Given the EU AI act and GDPR are EU privacy compliance regulations, the paper did not articulate whether the proposed safety compliance reasoner will work or applicable when the privacy compliance regulation is from other regions of the world, like HIPPA or the California Privacy Rights Act (CPRA).\n\n(2) The paper only compared SFT reasoner and RL reasoner using an existing Group Relative Policy Optimization (GRPO) from Deepseekmath [Shao et.al 2024a]. It is unclear the motivations of choosing GRPO instead of other RL methods. The reward model and the policy (reasoning trace) model are not clearly described in terms of learning and data flows. \n\n(3) The experiments section needs more elaboration on the measurement results reported. For example, Table 3 shows two cases (chapters) that Compliance-reasoner-SFT performs better than Compliance-reasoner-GRPO, and three cases the proposed compliance-reasoner performs worse compared to either single LLM or some existing LLM safety guardrails. Detailed analysis should be provided. Similar issues with other measurement results reported such as Table 4 and Table 5."}, "questions": {"value": "See weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "k08CeLKdaT", "forum": "PCVSB0pX5P", "replyto": "PCVSB0pX5P", "signatures": ["ICLR.cc/2026/Conference/Submission7262/Reviewer_C1jV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7262/Reviewer_C1jV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918871531, "cdate": 1761918871531, "tmdate": 1762919393504, "mdate": 1762919393504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}