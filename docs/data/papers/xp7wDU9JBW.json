{"id": "xp7wDU9JBW", "number": 10079, "cdate": 1758160043749, "mdate": 1759897675560, "content": {"title": "CoMem: Compositional Concept-Graph Memory for Continual Vision–Language Learning", "abstract": "Continual vision–language learning is crucial for multimodal tasks such as image–text retrieval, visual question answering, and grounded reasoning in dynamic environments, yet deployed systems must learn from non-stationary streams under strict privacy and memory budgets, where naïve finetuning forgets and harms transfer. We aim to sustain stable yet plastic capability in this setting without storing raw data, enabling reuse and recombination across domains and tasks. We present CoMem, a framework that treats compositional structure as the unit of memory and rehearsal: it incrementally organizes knowledge into a compact graph of concepts and relations and rehearses directly in feature space by conditioning practice signals on sampled subgraphs. A lightweight compositional consistency objective keeps part–whole predictions coherent, while teacher-informed, uncertainty-aware filtering limits off-manifold drift. Across cross-domain retrieval, structured concept learning, and continual multimodal VQA, CoMem achieves state-of-the-art retention and transfer alongside consistent gains on SVLC and VQACL/CLOVE under matched memory and parameter budgets. By casting structure as memory and rehearsing where learning happens (feature space), CoMem provides a privacy-friendly and testable paradigm for reliable continual adaptation without raw exemplars.", "tldr": "", "keywords": ["VLM", "Vision Language Learning", "Continual Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88cb2802f02d9bfc2d2b3b5ce5410dbaa858b244.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CoMem, a continual vision–language learning framework designed for dynamic, privacy-constrained environments where models must learn from non-stationary multimodal data streams without storing raw samples. Unlike traditional fine-tuning methods that cause catastrophic forgetting, CoMem conceptualizes compositional structure as the core unit of memory and rehearsal. The model incrementally builds a compact concept–relation graph, using feature-space rehearsal conditioned on sampled subgraphs to sustain knowledge retention. A compositional consistency objective ensures coherence between parts and wholes, while teacher-informed and uncertainty-aware filtering helps balance plasticity and stability. Experiments across multiple multimodal tasks, including cross-domain retrieval, structured concept learning, and continual VQA, demonstrate that CoMem achieves state-of-the-art retention and transfer performance under matched memory and parameter budgets."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Methodology: The idea of treating compositional structure as memory and conducting rehearsal directly in feature space is innovative and well-motivated for privacy-limited continual learning.\n\n2. Comprehensive Analysis: The paper clearly articulates how the proposed consistency and filtering mechanisms work together to preserve stability and plasticity, offering solid theoretical and empirical insight.\n\n3. Strong Experimental Validation: Results across multiple multimodal tasks demonstrate consistent improvements in both retention and transfer, with fair ablation comparisons."}, "weaknesses": {"value": "1. The introduction could be better structured and written. It does not clearly establish the relevant background or smoothly motivate the authors’ claims. As a result, the logical flow is somewhat fragmented, making it difficult for readers to follow the argument and understand the setting.\n\n2. The CoMem framework involves multiple components in its training objective. It is unclear how sensitive the overall performance is to the balance among these losses. This complexity might hinder the practical application of CoMem in new scenarios. The authors are encouraged to provide a more analysis or discussion on the stability of these hyperparameters and their effect on robustness.\n\n3. In Table 4, increasing the number of trainable parameters does not appear to yield clear performance improvements. The authors could strengthen their claims by including experiments with smaller parameter budgets to further analyze how performance gains scale with parameter count across different methods."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EgcpN2Seh3", "forum": "xp7wDU9JBW", "replyto": "xp7wDU9JBW", "signatures": ["ICLR.cc/2026/Conference/Submission10079/Reviewer_YWUY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10079/Reviewer_YWUY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761232764450, "cdate": 1761232764450, "tmdate": 1762921468758, "mdate": 1762921468758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel model framework, CoMem, designed to address the privacy and catastrophic forgetting challenges inherent in real-world Continual Vision-Language Learning (CVLL) deployment. CoMem achieves its goals through a structured, three-stage process. First, during Concept Induction, the model extracts attributes, entities, and relations from new data to update the Concept-Graph Memory. This memory is key, as it stores abstract prototypes and anchors instead of sensitive raw data, making the framework privacy-friendly while mitigating forgetting. Second, in Graph-Conditioned Replay, a small subgraph is selected from memory to generate synthetic features. The model then uses these features to \"rehearse\" past knowledge, effectively preventing catastrophic forgetting of previously learned data. Finally, during Joint Optimization, CoMem trains on a mixed batch of real data from the new task and the synthetic features from memory. A comprehensive total loss function, formulated as a weighted sum of distinct loss components, is implemented to balance the acquisition of new knowledge with the retention of previously learned information. CoMem was evaluated against state-of-the-art methods across three continual learning benchmarks: cross-domain retrieval, structured concept learning (SVLC), and continual Visual Question Answering (VQA). Across all evaluations, CoMem demonstrated superior results, achieving both the highest average performance and the lowest average forgetting rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. State of the Art performance: The proposed model achieves state-of-the-art results, demonstrating the best retention and lowest average forgetting (AF) across multiple benchmarks.\n\n2. Novel Approach: The proposed model achieves its results with a novel “structure-as-memory” approach to effectively solve both privacy and memory issues."}, "weaknesses": {"value": "1. Reliance on Upfront Parsing: The current method relies on a \"lightweight text parsing\" step to extract concept and relation candidates from the text.\n\n2. Fixed Relation Schema: The framework assumes a \"fixed relation schema\". This weakness, as the authors note it \"may constrain coverage in open-world settings\" where new, unseen types of relations might emerge.\n\n3. How do you ensure the generated features are semantically rich and diverse enough to capture the complex, nuanced interactions within a subgraph, and not just an average or blurry representation?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ttjHrZPwg6", "forum": "xp7wDU9JBW", "replyto": "xp7wDU9JBW", "signatures": ["ICLR.cc/2026/Conference/Submission10079/Reviewer_hXmH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10079/Reviewer_hXmH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710255705, "cdate": 1761710255705, "tmdate": 1762921468410, "mdate": 1762921468410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "COMEM proposes an innovative framework for continual vision-language learning, whose core idea is to treat ​​compositional structure​​ as the fundamental unit of ​​memory​​ and ​​rehearsal​​, rather than storing raw data. The core methodology involves organizing knowledge by structuring the data stream into a compact graph of concepts and relations, and conducting rehearsal by directly generating replay samples in the feature space based on sampled subgraphs. This approach integrates a lightweight compositional consistency constraint and a teacher model filtering mechanism, effectively balancing stability and plasticity. Under strict privacy and memory constraints, the method achieves superior retention and transfer performance across multiple challenging tasks, including cross-domain retrieval, structured concept learning, and continual visual question answering, significantly mitigating the problem of catastrophic forgetting in continual vl learning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "-1. Innovative memory mechanism design and privacy friendly learning paradigm: COMEM's biggest innovation lies in using composite structures as memory units, which not only saves storage space but also captures the intrinsic connections between concepts. Due to not storing any raw image or text data, COMEM naturally meets strict privacy protection requirements. All replay operations are performed in the latent feature space, avoiding the risk of sensitive data leakage and making it particularly suitable for deployment in privacy sensitive scenarios such as healthcare and finance.\n\n-2. COMEM's component design has great flexibility: orthogonal to parameter- efficient methods, it can be used in conjunction with adapters such as LoRA. Supporting different subgraph sampling strategies, combining compositional consistency constraints,  teacher- and uncertainty-informed filtering mechanisms to improve its anti forgetting ability.\n\n-3. Resource friendly: Only a total anchor budget of 64K is needed to achieve good experimental results.\n\n-4. Excellent experimental performance: Significant improvement in cross domain retrieval tasks, structured concept learning, and continuous VQA tasks, robust to different hyperparameters and ViT scales, and stable performance curves in the long-horizon learning process of 18 tasks."}, "weaknesses": {"value": "-1. The core of COMEM relies on a Fixed Relation Schema, which means that the identification and organization of its concepts and relationships are carried out within a predefined framework. This method is highly effective in handling known and well structured data streams, but may limit its adaptability in fully open environments.\n\n-2. Bias of Teacher Model: Although the Teacher Informed Filtering mechanism can train stably, it may also transfer the cognitive biases or knowledge blind spots of the teacher model itself to the student models.\n\n-3. COMEM is a multi-component complex system, and its training involves multiple stages such as concept induction, subgraph sampling, feature generation, and multi-objective optimization. This complexity brings a high threshold for engineering implementation\n\n-4. The appendix explores the impact of Task Orders, where COMEM's forgetting degree (AF) increases significantly when tasks appear in an adversarial order. This indicates that the stability of the model depends to some extent on the \"friendliness\" of the data flow. This affects its generalization in real-world scenarios."}, "questions": {"value": "refer to Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eGd4TWoNaT", "forum": "xp7wDU9JBW", "replyto": "xp7wDU9JBW", "signatures": ["ICLR.cc/2026/Conference/Submission10079/Reviewer_cTbP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10079/Reviewer_cTbP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894442200, "cdate": 1761894442200, "tmdate": 1762921468153, "mdate": 1762921468153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a continual learning framework that builds a concept–relation graph from image–text pairs by extracting (attribute, entity, relation) triplets. It then performs subgraph-conditioned replay in the representation space, combined with teacher-guided filtering and compositional consistency losses, to mitigate forgetting. Experiments on cross-domain retrieval, structured concept matching, and continual VQA report improvements over several baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "Clear motivation for feature-level replay with a graph memory under data-retention constraints.\n\nUse of teacher confidence/entropy gating and compositional consistency is conceptually reasonable.\n\nBroad empirical coverage with multiple tasks and ablations indicates non-trivial engineering effort."}, "weaknesses": {"value": "Method design is somehow too complex but the presentation is poor.\nFigure 1 (method overview) is hard to read.\nThe diagram is overcrowded: font is too small, visual hierarchy is unclear, and symbols in the figure do not align cleanly with those in the text. It is difficult to grasp the training and replay flow from the overview alone.\n\nHeavy notation but unclear definition.\nThe paper uses many symbols (for concepts/relations/subgraphs/generator variables, temperatures, loss weights, etc.) without clear definition (e.g., s_align in Eq.(2), three α in Eq. (3) are undefined). Some symbols appear to change meaning across sections.\n\nA large number of hyperparameters with no systematic tuning protocol.\nThe method includes multiple thresholds (confidence/entropy), loss weights (distillation/consistency/contrastive), structural choices (subgraph size, anchor budget, low-rank dimension), and kernel/sampling parameters. The paper should clarify how hyperparameters are chosen, validation splits, and search budgets.\n\nNot all captions can be parsed into complete (a, e, r) triplets (or maybe in some cases, more than one entity); handling of such cases is unspecified."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7u7gqQuAf1", "forum": "xp7wDU9JBW", "replyto": "xp7wDU9JBW", "signatures": ["ICLR.cc/2026/Conference/Submission10079/Reviewer_mzsw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10079/Reviewer_mzsw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992338008, "cdate": 1761992338008, "tmdate": 1762921467911, "mdate": 1762921467911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}