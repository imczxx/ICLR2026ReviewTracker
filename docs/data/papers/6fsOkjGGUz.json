{"id": "6fsOkjGGUz", "number": 606, "cdate": 1756753406398, "mdate": 1759898250831, "content": {"title": "HART: Human Aligned Reconstruction Transformer", "abstract": "We introduce HART, a unified framework for sparse-view human reconstruction. Given a small set of uncalibrated RGB images of a person as input, it outputs a watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat representation for photorealistic novel-view rendering. Prior methods for clothed human reconstruction either optimize parametric templates, which overlook loose garments and human-object interactions, or train implicit functions under simplified camera assumptions, limiting applicability in real scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body correspondences, and employs an occlusion-aware Poisson reconstruction to recover complete geometry, even in self-occluded regions. These predictions also align with a parametric SMPL-X body model, ensuring that reconstructed geometry remains consistent with human structure while capturing loose clothing and interactions. These human-aligned meshes initialize Gaussian splats to further enable sparse-view rendering. While trained on only 2.3K synthetic scans, HART achieves state-of-the-art results: Chamfer Distance improves by 18–23% for clothed-mesh reconstruction, PA-V2V drops by 6–27% for SMPL-X estimation, LPIPS decreases by 15–27% for novel-view synthesis on a wide range of datasets. These results suggest that feed-forward transformers can serve as a scalable model for robust human reconstruction in real-world settings. Code and models will be released.", "tldr": "", "keywords": ["human reconstruction", "computer vision", "computer graphics"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f7825b6bd4d169d2ee9fb871f54ed85ed1917585.pdf", "supplementary_material": "/attachment/b043adff8e9a0b467054e945ba309174b7b6cc07.zip"}, "replies": [{"content": {"summary": {"value": "HART represents a strong empirical contribution through a well-engineered combination of recent achievements on geometric and human reconstruction. It’s elegant and practical to integrate point-map prediction, occlusion-aware geometry completion, and SMPL-X alignment under a transformer framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of VGGT-based multi-view fusion with per-pixel SMPL-X attribute prediction (tightness vectors and body-part labels) is an interesting adaptation of general 3D reconstruction transformers to the human domain.\n2. The paper is clearly structured, with explicit loss formulations, architecture diagrams, and references to code/model release. These increase its reproducibility and transparency."}, "weaknesses": {"value": "1. Although cross-domain generalization is demonstrated, the model is trained entirely on synthetic THuman 2.1 data. How does the model perform when trained or fine-tuned on real-world multi-view captures? Are there domain adaptation or photometric normalization strategies considered?\n2. The method heavily depends on VGGT and DPSR. Beyond combining existing modules, what new representational capability does HART itself introduce? Could the same gains be achieved by tuning VGGT + DPSR with human-specific priors?\n3. The paper does not provide a detailed sensitivity study on the loss weights or robustness to view number reduction (e.g., from 4 to 2 views).\n4. How sensitive is HART to small pose misalignments or temporal inconsistencies (e.g., slight motion between views)? Could it handle unsynchronized or monocular sequences?\n5. Despite being a feed-forward system, the Gaussian splatting stage still involves iterative optimization. Could a fully feed-forward rendering head be learned to replace this step for true real-time NVS?"}, "questions": {"value": "1. How does the model perform when trained or fine-tuned on real-world multi-view captures? Are there domain adaptation or photometric normalization strategies considered?\n\n2. Besides the existing modules, what new representational capability does HART itself introduce? Could the same gains be achieved by tuning VGGT + DPSR with human-specific priors?\n\n3. Could a fully feed-forward rendering head be learned to replace this step for true real-time NVS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "99apqnDanw", "forum": "6fsOkjGGUz", "replyto": "6fsOkjGGUz", "signatures": ["ICLR.cc/2026/Conference/Submission606/Reviewer_iAZE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission606/Reviewer_iAZE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725923102, "cdate": 1761725923102, "tmdate": 1762915563803, "mdate": 1762915563803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HART, a transformer-based framework for sparse-view human reconstruction (clothed mesh, SMPL-X estimation, novel-view synthesis) with occlusion-aware DPSR and residual normal learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. One strength of HART is its unified transformer-based framework, which jointly outputs watertight clothed meshes, aligned SMPL-X body meshes, and Gaussian-splat representations for novel-view rendering from sparse uncalibrated RGB images—avoiding fragmented workflows of prior methods that handle these tasks separately. It achieves SOTA results (e.g., 18–23% CD improvement in clothed mesh reconstruction) across datasets like THuman 2.1 and DNA-Rendering, even with training on only 2.3K synthetic scans .\n\n2. Another strength lies in its practical optimizations for human-specific challenges: it uses occlusion-aware DPSR with a 3D U-Net to recover complete geometry in self-occluded regions (a limitation of general 3D reconstruction backbones like VGGT) and leverages residual normal learning (with Sapiens priors) to enhance surface detail. These tweaks, though built on existing modules, better adapt feed-forward transformers to sparse-view human reconstruction scenarios ."}, "weaknesses": {"value": "1. HART’s novelty is not in new conceptual ideas, but in systematically integrating and adapting existing modules for sparse-view human reconstruction: it combines transformer architecture, occlusion-aware DPSR, and residual normal learning (no new network/reconstruction paradigm); applies Gaussian splatting (mainstream rendering) with human mesh constraints (no new rendering method); tailors pre-trained VGGT/DINOv2 (for general tasks) to human-specific tasks (no new extractors/encoders), focusing on cohesive pipeline optimization.\n\n2. The method heavily relies on external components: removing the Sapiens model (for base normal prediction) results in blurrier surfaces, and disabling the indicator grid refinement leads to incomplete geometry in self-occluded regions, showing its weak independence from these specific auxiliary modules\n\n3. HART is trained solely on 2,345 synthetic scans from the THuman 2.1 dataset, which lacks the diversity of real-world human variations (e.g., plus-size bodies, sheer fabrics, heavy coats, or diverse ethnic features). This limits the validity of claims about \"robust generalization to real-world settings,\" as the training data does not fully represent real-world complexity.\n\n4. While HART uses RANSAC and PnP to estimate camera parameters (instead of assuming centered principal points), the paper does not quantify the accuracy of these estimated parameters (e.g., comparison with ground-truth camera poses on test datasets). This omission hides potential biases—if camera estimation is inaccurate under certain conditions (e.g., extreme poses), it could confound the evaluation of reconstruction/rendering performance."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GxWwE9sSBb", "forum": "6fsOkjGGUz", "replyto": "6fsOkjGGUz", "signatures": ["ICLR.cc/2026/Conference/Submission606/Reviewer_veT3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission606/Reviewer_veT3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904285213, "cdate": 1761904285213, "tmdate": 1762915563636, "mdate": 1762915563636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a feed-forward transformer that takes a set of >=3 uncalibrated RGB images as input, and produces a watertight clothed mesh, aligned SMPL body mesh, and 3DGS representation for photorealistic novel-view rendering. The transformer design is initialized from VGGT, with DPT heads for residual normals (on top of Sapiens), pointmaps (which are combined with normal predictions for differentiable Poisson surface reconstruction and clothed mesh reconstruction), and SMPL tightness vectors (as in ETCH) as well as body part label maps (which are together decoded into SMPL-X meshes). The method is trained on 2.3k human meshes from THuman2.1 dataset. 2D Gaussian surfels can be optionally outputted for novel-view synthesis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Developing feed-forward VGGT-style architectures for clothed human reconstruction from multiple views is an important and interesting problem.\n- The method is straightforward and well-motivated. The key introduced components (normal residual prediction and indicator grid refinement) help improve the quality of the geometry according to an ablation study\n- The method produces better results than baselines such as VGGT, MAtCha, PuzzleAvatar, LaRa, etc."}, "weaknesses": {"value": "- The reconstructed meshes lack very fine-grained details, such as detailed clothing textures, fingers, and hair. In the conclusion, the authors claim this is due to limited indicator grid resolution. Fundamentally, despite predicting residuals on top of a Sapiens normal prior, it seems the geometry quality is slightly better than VGGT but not substantially better.\n- The impact of predicting residual normals on top of Sapiens and indicator grid refinement seems qualitatively very small, at least according to Figure 9 and Table 8. It is not clear to me whether the impact is significant.\n- The method requires >=3 uncalibrated human images captured in the same body pose, which may be difficult to obtain in the wild outside of synchronized, sparse-view camera capture setups. Extending the method to dynamic monocular videos may be an interesting avenue for future research"}, "questions": {"value": "- Which dataset and sequences are the ablation studies in A.7 performed on?\n- Regarding the statistical significance of the ablation studies in A.7, is it possible to show per-sequence ablation results or error bars with multiple randomly-selected view sets per scene, to confirm whether the proposed design choices yield consistent improvements on every sequence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gphy9ElieT", "forum": "6fsOkjGGUz", "replyto": "6fsOkjGGUz", "signatures": ["ICLR.cc/2026/Conference/Submission606/Reviewer_PdSb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission606/Reviewer_PdSb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942100340, "cdate": 1761942100340, "tmdate": 1762915563501, "mdate": 1762915563501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HART, a unified feed-forward transformer framework for sparse-view human reconstruction, which  simultaneously predicts clothed human geometry, body pose, and GS. The work builds upon  VGGT and extends it to human-centric reconstruction by introducing multiple prediction heads and an occlusion-aware differentiable Poisson reconstruction (DPSR) module. The experiment shown improvements against baselines in geometric, rendering and pose estimation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The transformer based feed-forward pipeline should be much higher efficient compared to optimization- or diffusion-based methods；\n- It is reasonable to achieve various human-centric tasks, like pose, pointmap or geometry and potential tracking, in a unified model."}, "weaknesses": {"value": "1. I think the claim of  “scalable model” in abstract is not proper, because hart is trained with 3d scans dataset.\n2. The performance is not impressive for me, especially the geometry and smplx results. The human mesh in fIg.3 and video demos, are over-smoothed and on the same par with Vggt predictions. The challenges of  pose estimation tasks mainly lies in hand region and in-the-wild challenging cases. However, this work dose not preform very well. \n3. I have great concerns about the normal predictions of DPT heads, even if it is designed for dense prediction tasks. It will be better to include some normal outputs   before/after DPT refinement.  \n4. Another concern is the accuracy of the PnP-predicted camera poses. It is critical, as the estimated poses are used in both DPSR  and GS fitting, where accumulated errors could degrade overall performance. \n5. The comparisons are not sufficient. although this work focus on mv setting, the comparisons should include single view based methods, like SiTH and PSHuman. With more input views, this work should perform better than them."}, "questions": {"value": "as listed in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vy61PkoQ7J", "forum": "6fsOkjGGUz", "replyto": "6fsOkjGGUz", "signatures": ["ICLR.cc/2026/Conference/Submission606/Reviewer_D7TP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission606/Reviewer_D7TP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762127374889, "cdate": 1762127374889, "tmdate": 1762915563348, "mdate": 1762915563348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}