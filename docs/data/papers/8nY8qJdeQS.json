{"id": "8nY8qJdeQS", "number": 19656, "cdate": 1758298031234, "mdate": 1759897027707, "content": {"title": "Federated Learning With $L_{0}$ Constraint Via Probabilistic Gates For Sparsity", "abstract": "Federated Learning (FL) is a distributed machine learning setting that requires multiple clients to collaborate on training a model while maintaining data privacy. The unaddressed inherent sparsity in data often results in overly dense models and poor generalizability under data and client participation heterogeneity. We propose FL with an $L_0$ constraint on the density of non-zero parameters, achieved through a reparameterization using probabilistic gates and their continuous relaxation: originally proposed for sparsity in centralized machine learning. We show that the objective for $L_0$ constrained stochastic minimization naturally arises from an entropy maximization problem of stochastic gates and propose an algorithm based on federated stochastic gradient descent for distributed learning. We demonstrate that the target density ($\\rho$) of parameters can be achieved in FL, under data and client participation heterogeneity, with minimal loss in statistical performance for linear models: $\\emph{(i)}$ Linear regression (LR). $\\emph{(ii)}$ Logistic regression (LG). $\\emph{(iii)}$ Softmax multi-class classification (MC). $\\emph{(iv)}$ Multi-label classification with logistic units (MLC), and compare the results with a magnitude pruning-based algorithm for sparsity in FL. Experiments on synthetic data with target density down to $\\rho = 0.05$ and publicly available datasets, including e2006-tfidf, RCV1, and MNIST, with target density down to $\\rho = 0.005$, demonstrate that our approach consistently works well in both sparsity recovery and statistical performance.", "tldr": "L_{0} constrained minimization objective can be derived from entropy maximization of stochastic gates, and a fedSGD algorithm is effective in sparsity recovery and statistical performance for linear models, compared to magnitude pruning.", "keywords": ["Federated learning", "linear models", "sparsity", "$L_{0}$ constraint", "probabilistic gates", "reparameterization", "data heterogeneity", "client participation heterogneity", "maximum entropy principle", "free energy", "negative ELBO"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b276a648ddc2032e927af850ae70cbafecae8f77.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to enhance Federated Learning (FL) by introducing an L0 constraint on the density of non-zero parameters to enforce model sparsity. This mechanism, implemented using a reparameterization technique with Probabilistic Gates, aims to counteract the issues of overly dense models and poor generalizability arising from unaddressed data sparsity and various forms of heterogeneity common in FL settings. By leveraging techniques like the Binary Concrete Distribution and the Gumbel max trick, the approach allows for flexible model training from either dense or sparse initialization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a novel application of the L0 constraint via Probabilistic Gates into the Federated Learning (FL) framework, specifically targeting model density. This represents a creative combination of established sparsity techniques (like the Binary Concrete Distribution and Gumbel max trick) to address a challenging problem in the unique, distributed FL setting."}, "weaknesses": {"value": "1.While heterogeneity is simulated, the paper needs a more direct and comprehensive comparison against other state-of-the-art FL sparsity or pruning techniques (e.g., FL-adapted L1/L2 regularization, magnitude pruning, or methods based on the Lottery Ticket Hypothesis). This would clearly delineate the unique advantages of the L0 constraint approach.\n2.The paper mentions that the parameter p controls the expected number of non-zero parameters. However, a detailed ablation study on the sensitivity of the final performance (accuracy vs. sparsity) to the choice of the sparsity hyperparameter is essential for practical use and is currently missing."}, "questions": {"value": "1.How are the Probabilistic Gate variables specifically treated during the Federated Aggregation step? How does this choice ensure that the sparsity constraint remains satisfied across global model rounds?\n2.In highly heterogeneous settings, how are the sparse structures determined by the Probabilistic Gates aggregated across different clients?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dh6WrLbxWi", "forum": "8nY8qJdeQS", "replyto": "8nY8qJdeQS", "signatures": ["ICLR.cc/2026/Conference/Submission19656/Reviewer_x11n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19656/Reviewer_x11n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888048338, "cdate": 1761888048338, "tmdate": 1762931505942, "mdate": 1762931505942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an FL framework, that enforces $L_0$ constraint on the model parameters to induce sparsity by utilizing probabilistic gates to make the problem tractable. This is motivated by the poor generalization of the dense models in heterogeneous FL settings. The paper further demonstrates that how the $L_0$ constraint is connected to entropy maximization of stochastic gates. They then utilize this insight to derive the $L_0$ constraint for the FL setting. The resulting algorithm called FLoPS allows simultaneous updates of model parameters, gate parameters and a Lagrange multiplier that controls the level of sparsity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The key insight in the paper connecting the $L_0$ constraint to entropy maximization of stochastic gates is novel.\n2. The utilization of the above insight for deriving the $L_0$ constraint for the FL settings enables a new learning setup."}, "weaknesses": {"value": "1. All experiments in the paper are conducted on linear models, therefore the scalability and soundness of the method on more commonly used non-linear models remain untested.\n2. The convergence guarantee or stability conditions of the joint optimization is under-discussed."}, "questions": {"value": "1. Can you provide intuition on the proposed connection between entropy maximization and $L_0$ constraints? \n2. Do all clients contribute parameters in every iteration? If not, how are the missing gates handled?\n3. How sensitive is the algorithm to the decay and pruning schedule?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ls3vHpV0hC", "forum": "8nY8qJdeQS", "replyto": "8nY8qJdeQS", "signatures": ["ICLR.cc/2026/Conference/Submission19656/Reviewer_biEp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19656/Reviewer_biEp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930389814, "cdate": 1761930389814, "tmdate": 1762931505527, "mdate": 1762931505527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FLoPS, a federated learning algorithm for learning sparse models with controlled parameter density using probabilistic gates and Hard Concrete relaxation. The authors adapt the L0-constrained optimization framework from Gallego-Posada et al. (2022) to the FL setting, derive the objective from entropy maximization principles, and propose a distributed algorithm based on FedSGD. Experiments on synthetic and real datasets demonstrate superior sparsity recovery and statistical performance compared to magnitude pruning baseline (FedIter-HT) under data and client participation heterogeneity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. New prespective on gate-based L0 sparsity to FL: Addresses a gap in the literature where probabilistic gates have not been applied to federated sparse learning.\n\nS1. Extensive experiments across multiple datasets, sparsity levels (0.5%-95%), and heterogeneity conditions (data, client participation).\nConsistent improvements over baseline: FLoPS demonstrates better True Discovery Rate and statistical performance than FedIter-HT across all settings.\n\nS3. Achieves target density through constrained optimization rather than tuning regularization coefficients.\n\nS4. Entropy maximization derivation (Section 2.2) provides alternative theoretical perspective on L0 regularization."}, "weaknesses": {"value": "W1. Section 2.2 (entropy derivation) is isolated from the FL application. It derives the centralized formulation but provides no insights for distributed optimization, convergence, or aggregation strategy.\n\nW2.  No theoretical analysis of whether FLoPS converges in heterogeneous FL settings. Does the algorithm converge under non-IID data? How does heterogeneity affect convergence rate? What is the relationship between three learning rates?\n\n\nW3. The paper applies standard weighted averaging to (\\hat{\\theta}, \\phi) without justification. Why is this optimal? Why not aggregate \\theta directly? \n\nW4. How does a consistent global sparsity pattern emerge from heterogeneous local updates when different clients prefer different features? The constraint is only enforced server-side - why is this sufficient?\n\nW5. The paper provide limitted analysis, it requires to have  (i) communication cost comparison, (ii) computational overhead, (iii) ablations on design choices.\n\nW6. Limited baseline comparison: Only compares with FedIter-HT. Missing comparisons with: (i) Lasso-based FL methods (Frandi et al. 2016, Sehic et al. 2022), (ii) Standard FedAvg with post-hoc pruning, (iii) Centralized training (upper bound)"}, "questions": {"value": "Q1. Can you provide convergence guarantees for FLoPS under data heterogeneity? What is the convergence rate and how does it depend on heterogeneity level?\n\nQ2. How do you ensure global sparsity pattern emerges from conflicting local preferences?\n\nQ3. What is the total communication cost (rounds × message size)? Does FLoPS converge faster enough to offset the 2× parameter overhead per round?\n\nQ4. refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OKpvWmvyvu", "forum": "8nY8qJdeQS", "replyto": "8nY8qJdeQS", "signatures": ["ICLR.cc/2026/Conference/Submission19656/Reviewer_tYdA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19656/Reviewer_tYdA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933259646, "cdate": 1761933259646, "tmdate": 1762931505064, "mdate": 1762931505064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}