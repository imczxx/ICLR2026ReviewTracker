{"id": "rhPnkTKfMy", "number": 8843, "cdate": 1758099772853, "mdate": 1763586945902, "content": {"title": "Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset", "abstract": "Pretraining large language models (LLMs) on high-quality, structured data such as mathematics and code substantially enhances reasoning capabilities. However, existing math-focused datasets built from Common Crawl suffer from degraded quality due to brittle extraction heuristics, lossy HTML-to-text conversion, and the failure to reliably preserve mathematical structure. In this work, we intro-\nduce Nemotron-CC-Math, a large-scale, high-quality mathematical corpus constructed from Common Crawl using a novel, domain-agnostic pipeline specifically designed for robust scientific text extraction. Unlike previous efforts, our pipeline recovers math across various formats (e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx and a targeted LLM-based cleaning stage. This approach preserves the structural integrity of equations and code blocks while removing boilerplate, standardizing\nnotation into L A T EX representation, and correcting inconsistencies. We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+(133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably, Nemotron-CC-Math-4+ not only surpasses all prior open math datasets-including Mega-Math, FineMath, and OpenWebMath-but also contains 5.5× more tokens than FineMath-4+, which was previously the highest-quality math pretraining dataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to +12.6. \ngains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines, while\nalso improving general-domain performance on MMLU and MMLU-Stem.\nWe present the first pipeline to reliably extract scientific content—including\nmath—from noisy web-scale data, yielding measurable gains in math, code, and\ngeneral reasoning, and setting a new state of the art among open math pretraining\ncorpora. To support open-source efforts, we release our code1 and datasets 2\n.", "tldr": "We build Nemotron-CC-Math, a high-quality math corpus from Common Crawl that outperforms prior math corpora, boosting LLM performance on math, code, and reasoning tasks.", "keywords": ["Mathematical Reasoning", "Web-Scale Data Curation", "LLM-Based Cleaning", "Pretraining datasets", "Deduplication"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/063d0179ee4e206ac81e43f863a0dda7cd598606.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper redesigns the text extraction pipeline for web data curation, firstly employing Lynx, a text-based browser, to retain texts, and employing Phi-4 to rewrite text for quality, resulting in Nemotron-CC-Math. It delivers better data quality than existing the-state-of-the-art open-source corpora, such as MegaMath, FineMath."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-written and structured paper, solid experiments;\n2. The lynx’s introduction, which reliably captures equations and maintains code indentation, avoids the heuristics DOM tree operations, such as MegaMath.\n3. The ablation on different refinement models is solid."}, "weaknesses": {"value": "I believe that the effectiveness of Lynx should be evaluated through an apples-to-apples comparison. For example, the quality of Lynx versus DOM tree optimization (as introduced in MegaMath) on the same mathematical web pages could be compared under a controlled setting."}, "questions": {"value": "In addition, the potential negative impact introduced during the document refinement process should be clearly discussed. I believe this could be an important point that warrants further analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5GRHQ7gAuC", "forum": "rhPnkTKfMy", "replyto": "rhPnkTKfMy", "signatures": ["ICLR.cc/2026/Conference/Submission8843/Reviewer_AQgW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8843/Reviewer_AQgW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923513053, "cdate": 1761923513053, "tmdate": 1762920609247, "mdate": 1762920609247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response to Reviewers and Updated Manuscript: Addition Quality Assessment (Section 3.3)"}, "comment": {"value": "We thank all the reviewers for their time and valuable feedback. \n\nWe are delighted that reviewers found **the contribution to open science and open source of our work commendable** (reviewer upxT), and recognized our proposed pipeline as **robust and proper** (reviewer upxT),  describing it as **very sound** (reviewer \"yQeP\"), that **redesigns the text extraction pipeline for web data curation** (reviewer AQgW), effectively **avoiding information loss from naive HTML-to-text extraction** (reviewer yQeP).\n\nWe are pleased that reviewers considered our **domain-agnostic extraction pipeline** (reviewer yQeP) an **effective solution** (reviewer upxT), that **addresses the failure mode of previous web math extractors** (reviewer upxT). They also emphasized that it **reliably captures equations and maintaining code structure** (reviewers yQeP, AQgW), while **unifying MathJax/KaTeX/MathML into LaTeX** - a contribution that they found **very important but often under-estimated in previous works** (reviewer yQeP).\n\nWe are glad that reviewers found our delivered dataset to be **larger and higher quality than existing open-source alternatives** (reviewer upxT), **delivering better data quality than SOTA corpora such as MegaMath and FineMath** (reviewer AQgW), and that our **experiments clearly demonstrate the dataset’s quality** (reviewer yQeP).\n\nReviewers also described our **experimental results as strong** (reviewer upxT), **solid** (reviewer AQgW), and **showing very promising results** (reviewer yQeP). They highlighted our ablation on different refinement models being solid (reviewer AQgW) and noted that **our qualitative examples clearly demonstrate the superiority of our pipeline in preserving structure** (reviewer upxT). The **scale of our experiments was considered sufficient**, **showing good results across a range of benchmarks** (reviewer upxT).\n\nWe are further encouraged that reviewers described our paper as **well-written and structured** (reviewer AQgW).\n\n***New Analysis Added:*** \n\nWe have added a new section (Section 3.3) dedicated to a direct, quantitative assessment to rigorously compare the extraction fidelity of different datasets. We randomly sampled 100 documents from the shared subset of OWM, FineMath, MegaMath, and our corpus, evaluating four key aspects of content preservation:\n\n1)  Math Preservation (0–3 or N/A): Correctness and completeness of mathematical expressions unified to LATEX.\n2) Code Preservation (0–3 or N/A): Structural and semantic integrity, syntax, and functional behavior of preserved code blocks.\n3) Faithfulness (0–3): Preservation of core scientific content integrity without omission or meaning alteration.\n4) Readability (0–3): Overall clarity, organization, coherence, and textual flow of the output document.\n\nWe employed OpenAI gpt-5.1 as an automated judge to assess the conversion quality. The judge was provided with the original raw text (extracted via Lynx) and the converted document from each dataset, guided by a detailed scoring rubric and evaluation instructions (see Appendix A.6). We report the mean score for each dataset across samples and four dimensions. \n\nThe results show that our method preserves math and code substantially better than all baselines, including MegaMath (which also uses an LLM), due to our use of Lynx for stable rendering. While Faithfulness is slightly lower than FineMath and OWM—an expected and acceptable trade-off due to our LLM-based rewrite step that may compress or rewrite contextual details—it remains much higher than MegaMath. Crucially, our readability score is the highest among all datasets, decisively demonstrating that our approach successfully produces a corpus with superior textual coherence and clarity while retaining superior mathematical and code structure. We updated the paper and see Section 3.3.\n\nFinally, we reaffirm our **commitment to open science** and will **release our full pipeline and dataset to the research community**."}}, "id": "YUvl98RRCZ", "forum": "rhPnkTKfMy", "replyto": "rhPnkTKfMy", "signatures": ["ICLR.cc/2026/Conference/Submission8843/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8843/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8843/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763583306712, "cdate": 1763583306712, "tmdate": 1763593759485, "mdate": 1763593759485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Nemotron-CC-Math, a large, high-quality math corpus built from Common Crawl. It uses a domain-agnostic extraction pipeline: layout-aware `lynx` rendering plus structure-preserving LLM cleaning to unify MathJax/KaTeX/MathML into LaTeX, keep equation and code structure, and remove boilerplate.\n\nIt releases two datasets: Nemotron-CC-Math-3+ (133B tokens) and Nemotron-CC-Math-4+ (52B tokens), claimed to surpass MegaMath, FineMath, and OpenWebMath; 4+ has about 5.5× the tokens of FineMath-4+. As for experiments, the authors show that pretraining an 8B model on this data yields 4.8-12.6 on MATH and 4.6-14.3 on MBPP+, with additional gains on benchmarks like MMLU."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. I think overall the data pipelines are very sound. the authors combines layout-aware lynx rendering with structure-preserving LLM cleaning, avoiding information loss from naïve HTML-to-text extraction.\n\n2. The authors also unify MathJax/KaTeX/MathML into LaTeX, preserving equation and code structure while removing boilerplate, which is very important but often under-estimated in previous works.\n\n2. The experiments show very promising results, further demonstrating the quality of the datasets."}, "weaknesses": {"value": "I don't see any obvious weaknesses."}, "questions": {"value": "I do have some questions for the authors:\n\n1. why pre-train on math also boosts code performance? if so, have you compared this with other math&code-related datasets? such as stack-edu, megamath-code?\n\n2. How do you detect and constrain LLM over-editing or hallucinations (e.g., symbol renaming, citation mismatches, skipped derivations)? Did you conduct manual sampling review and an inter-annotator agreement (IAA) evaluation?\n\n3. Could you report quantitative results for LaTeX/code parseability, structural consistency (e.g., AST-based edit distance), and rendering consistency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TaYZZfThmr", "forum": "rhPnkTKfMy", "replyto": "rhPnkTKfMy", "signatures": ["ICLR.cc/2026/Conference/Submission8843/Reviewer_yQeP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8843/Reviewer_yQeP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983978559, "cdate": 1761983978559, "tmdate": 1762920608822, "mdate": 1762920608822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces nemotron-cc-math, a math dataset created by utilizing the Lynx text-based browser. By using the browser, the HTML can be rendered as structured format with equation and code layout as human read them. The content is then fed through an LLM (Phi-4 14B) to clean up the boilerplates. The final dataset retains data passing Fineweb classifier (3+), which contains 133B tokens, which is one of the largest set at this quality."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Robust and Proper Pipeline: The lynx + LLM-cleaner pipeline is an effective solution. It addresses the failure mode of previous web math extractors that corrupt math and code. The qualitative examples provided clearly demonstrate its superiority in preserving structure. While this method makes sense, both the lynx rendering and an 14B LLM cleaner are expensive for many practitioners. So the open sharing of this resource will help the community significantly.\n- The paper delivers a dataset that is both larger and higher quality than existing open-source alternatives. The 133B high quality portion is larger than FineMath, though smaller than the 300B MegaMath.\n- Strong experiment results: the methods are tested on an 8B mid-training checkpoint, with 100B and 300B experiments. The scale of the experiment should be sufficient and it shows good results on a range of benchmarks.\n- Once again, the contribution to open science and open source of this work is commendable."}, "weaknesses": {"value": "This paper can benefit from additional experimental settings. The main experiments are conducted at the mid-train setting. Will there be some confounding factor from the base model itself? Further, would larger amount of unique tokens be helpful and how much repetition can this dataset be used?\n\nThe readers would also benefit from learning about the filtered out portion, i.e., Nemotron-cc-math-1-3. The token size, quality and corresponding model performance may provide valuable information."}, "questions": {"value": "How much total tokens are their without the quality filter? Is there an estimate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ze4sCUtdKM", "forum": "rhPnkTKfMy", "replyto": "rhPnkTKfMy", "signatures": ["ICLR.cc/2026/Conference/Submission8843/Reviewer_upxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8843/Reviewer_upxT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136277751, "cdate": 1762136277751, "tmdate": 1762920608398, "mdate": 1762920608398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}