{"id": "rhPnkTKfMy", "number": 8843, "cdate": 1758099772853, "mdate": 1759897760170, "content": {"title": "Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset", "abstract": "Pretraining large language models (LLMs) on high-quality, structured data such as mathematics and code substantially enhances reasoning capabilities. However, existing math-focused datasets built from Common Crawl suffer from degraded quality due to brittle extraction heuristics, lossy HTML-to-text conversion, and the failure to reliably preserve mathematical structure. In this work, we intro-\nduce Nemotron-CC-Math, a large-scale, high-quality mathematical corpus constructed from Common Crawl using a novel, domain-agnostic pipeline specifically designed for robust scientific text extraction. Unlike previous efforts, our pipeline recovers math across various formats (e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx and a targeted LLM-based cleaning stage. This approach preserves the structural integrity of equations and code blocks while removing boilerplate, standardizing\nnotation into L A T EX representation, and correcting inconsistencies. We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+(133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably, Nemotron-CC-Math-4+ not only surpasses all prior open math datasets-including Mega-Math, FineMath, and OpenWebMath-but also contains 5.5× more tokens than FineMath-4+, which was previously the highest-quality math pretraining dataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to +12.6. \ngains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines, while\nalso improving general-domain performance on MMLU and MMLU-Stem.\nWe present the first pipeline to reliably extract scientific content—including\nmath—from noisy web-scale data, yielding measurable gains in math, code, and\ngeneral reasoning, and setting a new state of the art among open math pretraining\ncorpora. To support open-source efforts, we release our code1 and datasets 2\n.", "tldr": "We build Nemotron-CC-Math, a high-quality math corpus from Common Crawl that outperforms prior math corpora, boosting LLM performance on math, code, and reasoning tasks.", "keywords": ["Mathematical Reasoning", "Web-Scale Data Curation", "LLM-Based Cleaning", "Pretraining datasets", "Deduplication"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/29dfb346f733757159fee485774735333c45aa41.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper redesigns the text extraction pipeline for web data curation, firstly employing Lynx, a text-based browser, to retain texts, and employing Phi-4 to rewrite text for quality, resulting in Nemotron-CC-Math. It delivers better data quality than existing the-state-of-the-art open-source corpora, such as MegaMath, FineMath."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-written and structured paper, solid experiments;\n2. The lynx’s introduction, which reliably captures equations and maintains code indentation, avoids the heuristics DOM tree operations, such as MegaMath.\n3. The ablation on different refinement models is solid."}, "weaknesses": {"value": "I believe that the effectiveness of Lynx should be evaluated through an apples-to-apples comparison. For example, the quality of Lynx versus DOM tree optimization (as introduced in MegaMath) on the same mathematical web pages could be compared under a controlled setting."}, "questions": {"value": "In addition, the potential negative impact introduced during the document refinement process should be clearly discussed. I believe this could be an important point that warrants further analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5GRHQ7gAuC", "forum": "rhPnkTKfMy", "replyto": "rhPnkTKfMy", "signatures": ["ICLR.cc/2026/Conference/Submission8843/Reviewer_AQgW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8843/Reviewer_AQgW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923513053, "cdate": 1761923513053, "tmdate": 1762920609247, "mdate": 1762920609247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Nemotron-CC-Math, a large, high-quality math corpus built from Common Crawl. It uses a domain-agnostic extraction pipeline: layout-aware `lynx` rendering plus structure-preserving LLM cleaning to unify MathJax/KaTeX/MathML into LaTeX, keep equation and code structure, and remove boilerplate.\n\nIt releases two datasets: Nemotron-CC-Math-3+ (133B tokens) and Nemotron-CC-Math-4+ (52B tokens), claimed to surpass MegaMath, FineMath, and OpenWebMath; 4+ has about 5.5× the tokens of FineMath-4+. As for experiments, the authors show that pretraining an 8B model on this data yields 4.8-12.6 on MATH and 4.6-14.3 on MBPP+, with additional gains on benchmarks like MMLU."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. I think overall the data pipelines are very sound. the authors combines layout-aware lynx rendering with structure-preserving LLM cleaning, avoiding information loss from naïve HTML-to-text extraction.\n\n2. The authors also unify MathJax/KaTeX/MathML into LaTeX, preserving equation and code structure while removing boilerplate, which is very important but often under-estimated in previous works.\n\n2. The experiments show very promising results, further demonstrating the quality of the datasets."}, "weaknesses": {"value": "I don't see any obvious weaknesses."}, "questions": {"value": "I do have some questions for the authors:\n\n1. why pre-train on math also boosts code performance? if so, have you compared this with other math&code-related datasets? such as stack-edu, megamath-code?\n\n2. How do you detect and constrain LLM over-editing or hallucinations (e.g., symbol renaming, citation mismatches, skipped derivations)? Did you conduct manual sampling review and an inter-annotator agreement (IAA) evaluation?\n\n3. Could you report quantitative results for LaTeX/code parseability, structural consistency (e.g., AST-based edit distance), and rendering consistency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TaYZZfThmr", "forum": "rhPnkTKfMy", "replyto": "rhPnkTKfMy", "signatures": ["ICLR.cc/2026/Conference/Submission8843/Reviewer_yQeP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8843/Reviewer_yQeP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983978559, "cdate": 1761983978559, "tmdate": 1762920608822, "mdate": 1762920608822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces nemotron-cc-math, a math dataset created by utilizing the Lynx text-based browser. By using the browser, the HTML can be rendered as structured format with equation and code layout as human read them. The content is then fed through an LLM (Phi-4 14B) to clean up the boilerplates. The final dataset retains data passing Fineweb classifier (3+), which contains 133B tokens, which is one of the largest set at this quality."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Robust and Proper Pipeline: The lynx + LLM-cleaner pipeline is an effective solution. It addresses the failure mode of previous web math extractors that corrupt math and code. The qualitative examples provided clearly demonstrate its superiority in preserving structure. While this method makes sense, both the lynx rendering and an 14B LLM cleaner are expensive for many practitioners. So the open sharing of this resource will help the community significantly.\n- The paper delivers a dataset that is both larger and higher quality than existing open-source alternatives. The 133B high quality portion is larger than FineMath, though smaller than the 300B MegaMath.\n- Strong experiment results: the methods are tested on an 8B mid-training checkpoint, with 100B and 300B experiments. The scale of the experiment should be sufficient and it shows good results on a range of benchmarks.\n- Once again, the contribution to open science and open source of this work is commendable."}, "weaknesses": {"value": "This paper can benefit from additional experimental settings. The main experiments are conducted at the mid-train setting. Will there be some confounding factor from the base model itself? Further, would larger amount of unique tokens be helpful and how much repetition can this dataset be used?\n\nThe readers would also benefit from learning about the filtered out portion, i.e., Nemotron-cc-math-1-3. The token size, quality and corresponding model performance may provide valuable information."}, "questions": {"value": "How much total tokens are their without the quality filter? Is there an estimate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ze4sCUtdKM", "forum": "rhPnkTKfMy", "replyto": "rhPnkTKfMy", "signatures": ["ICLR.cc/2026/Conference/Submission8843/Reviewer_upxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8843/Reviewer_upxT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136277751, "cdate": 1762136277751, "tmdate": 1762920608398, "mdate": 1762920608398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}