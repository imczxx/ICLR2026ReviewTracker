{"id": "F25Mg4gNfw", "number": 9067, "cdate": 1758109301440, "mdate": 1759897745482, "content": {"title": "$\\mu$-Parameterization for Mixture of Experts", "abstract": "Recent years have seen a growing interest and adoption of LLMs, with Mixture-of-Experts (MoE) emerging as a leading architecture in extremely large models. Currently, the largest open-source models reach over $1$T parameters. At such scales, hyperparameter tuning becomes prohibitively expensive. Precisely for this reason, the $\\mu$Transfer is becoming a key technique. It allows for seamless transfer of optimal hyperparameters across model scales, resulting in a huge reduction in tuning costs. However, existing work has primarily focused on dense LLMs, leaving MoE architectures unexplored. In this work, we derive a $\\mu$-Parameterization for MoE, providing theoretical guarantees for feature learning across model widths. Our experiments demonstrate that the optimal learning rate reliably transfers across model sizes, establishing a foundation for efficient hyperparameter tuning in large-scale MoE models.", "tldr": "In this work, we derive a $\\mu$-Parameterization for MoE - a parameterization with theoretical guarantees for feature learning across MoE model widths.", "keywords": ["Mixture of Experts", "LLM", "$\\mu$-Transfer", "$\\mu$-Parameterization", "learning rate"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03ed6e81877b9c193d31c6883d98bddce4f4d160.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose an extension of $\\mu$-parameterization for MoE architectures, enabling transfer of optimal learning rates across increasing MoE model width. The authors provide a theoretically grounded derivation of their parameterization, along with limits for which the theoretical guarantees cease to hold, such as for varying MoE granularity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper discusses an important practical and empirical topic -- hyperparameter transfer across model scale -- and uses principled foundations to derive theoretical results with accompanying guarantees and limitations. The paper is also clear and the logic is easy to follow."}, "weaknesses": {"value": "The main weakness of the paper is the extremely limited experimental validation. The authors only present experiments for one single hyperparameter, the learning rate, in one single MoE model, Switch transformer, for one dataset, C4. The theoretical work is interesting but with such limited experimental results it becomes infeasible to really assess the impact and empirical consequences of the work. On a related note, the paper is short at only 6 pages, so I'm curious as to why the authors didn't consider using some of the ample additional page limit to more extensively verify their work. \n\nThe technical novelty is also limited, it being an extension of $\\mu$-parameterization [Yang et al, NeurIPS 2021] to MoE architectures. In my view, extending theoretical works or methods centered on dense models to MoE is still worthwhile and interesting, but it does then require a thorough analysis of how MoE presents new challenges and opportunities. In this work, however, the analysis appears substantially more limited than in Yang."}, "questions": {"value": "Is there a reason for the paper being so short, and the experimental validation being so limited? There are numerous datasets, modalities, and models you could have considered, for which experimental results would help support your work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9XCJpP5iKC", "forum": "F25Mg4gNfw", "replyto": "F25Mg4gNfw", "signatures": ["ICLR.cc/2026/Conference/Submission9067/Reviewer_Qdp6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9067/Reviewer_Qdp6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760953231196, "cdate": 1760953231196, "tmdate": 1762920777374, "mdate": 1762920777374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of hyperparameter tuning in extremely large Language Models (LLMs), particularly those utilizing the Mixture-of-Experts (MoE) architecture. MoE has emerged as a leading architecture for scaling LLMs, but existing $\\mu$-Parameterization ($\\mu$P) techniques, which enable transfer of optimal hyperparameters across model scales ($\\mu$Transfer), have previously focused only on dense architectures.\nThe authors derive a theoretically grounded $\\mu$-Parameterization for MoE, building upon the Tensor Programs 5 (TP5) framework. The core theoretical finding classifies the expert weights ($E_1, E_2$) as hidden weights and the router weight ($R_0$) as an output weight within the $\\mu$P framework. The paper empirically verifies that this $\\mu$P scheme successfully transfers the optimal learning rate across varying model widths (up to 2048). Furthermore, the authors introduce a simplified parameterization (simpleP) that also achieves transferability, and they investigate other scaling axes, finding that while increasing the number of experts preserves transfer, changing MoE granularity breaks it."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**: The primary strength is the novel theoretical derivation and empirical validation of $\\mu$P for Mixture-of-Experts. This is a crucial extension of existing Tensor Program theory that had previously overlooked sparse architectures. The paper also introduces and evaluates simpleP, a heuristic parameterization, providing a useful comparison point.\n\n**Quality**: The theoretical analysis is high quality, providing a principled method for scaling MoE components by classifying expert weights as hidden and the router as output, thereby ensuring stable gradient and activation scales across widths. The theoretical argument is supported by technical proofs regarding covariance (Appendix B). The empirical results, showing successful learning rate transfer for both $\\mu$P and simpleP across a wide range of widths, are compelling.\n\n**Clarity**: The paper is well-written, making complex theoretical concepts accessible. The tables summarizing the parameterization rules are particularly helpful.\n\n**Significance**: This work directly addresses the compute bottleneck inherent in training LLMs over 1T parameters by enabling cost-efficient hyperparameter tuning. The ability to transfer optimal learning rates is a foundational step toward efficient large-scale MoE training."}, "weaknesses": {"value": "**Missing critical information for experiments**: It’s unclear from the text what is the range of model sizes considered in the experiments. Can the authors add a table on the sizes of models considered in terms of overall number of parameters and number of active parameters?"}, "questions": {"value": "**Utility of $\\mu$P over simpleP**: Since simpleP (heuristic, experts only reparameterized) empirically shows strong learning rate transferability similar to the full $\\mu$P (router and experts reparameterized), what practical benefits does the theoretically grounded $\\mu$P provide over simpleP? Do the theoretical guarantees translate into demonstrably better final model performance, faster convergence, or improved stability during training runs, especially at the largest tested widths?\n\n**Clarification on Figures**: The authors in lines 110-112 claim “We scale the model width, with the number of experts and top-k kept fixed. The hidden dimension of each expert grows proportionally with the model width, so that the ratio between them remains constant.” Does this mean that for results in Figure 1 and Figure 2a, both the model dimension and number of experts are being scaled simultaneously? Or in Figure 1, only model dimension is increased with fixed number of experts and in Figure 2, only number of experts is increased with model dimension fixed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8HaGG7kqe7", "forum": "F25Mg4gNfw", "replyto": "F25Mg4gNfw", "signatures": ["ICLR.cc/2026/Conference/Submission9067/Reviewer_FdRu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9067/Reviewer_FdRu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761466723047, "cdate": 1761466723047, "tmdate": 1762920776416, "mdate": 1762920776416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This addresses the substantial cost associated with hyperparameter tuning in extremely large-scale Language Models (LLMs), particularly those utilizing the Mixture-of-Experts (MoE) architecture."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper solves a major engineering bottleneck for large-scale AI research. By enabling the transfer of optimal learning rates across MoE model scales."}, "weaknesses": {"value": "1. The paper is not well-organized and presented. It would be better to write the texts and give more beautiful pictures for this venue. \n2. The simpleP parameterization is shown to be highly effective at learning rate transfer, performing similarly to $\\mu$P. A more in-depth discussion is needed to theoretically justify why simpleP, which is less complex than the full $\\mu$P derivation, works so well, and what specific scenarios would necessitate the complexity of the full $\\mu$P."}, "questions": {"value": "1. Could the authors provide the results for the expert-count and granularity ablations (currently run under simpleP) using the proposed $\\mu$-Parameterization ($\\mu$P)?\n2. The simpleP parameterization seems to achieve learning rate transfer very successfully16. Could the authors provide a brief theoretical analysis or justification for why simpleP works effectively in this context, and explain the key non-trivial theoretical differences that make $\\mu$P the preferred choice over simpleP for massive-scale MoE training?\n3. While the paper focuses on the learning rate, $\\mu$-Transfer is typically used to transfer other critical hyperparameters (e.g., initialization scale, weight decay). Do the theoretical guarantees of the derived $\\mu$P extend to these other hyperparameters in MoE architectures? If so, could the authors provide preliminary empirical evidence demonstrating the successful transfer of one additional hyperparameter (e.g., weight decay) across model widths using $\\mu$P?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0V3KmVb0PP", "forum": "F25Mg4gNfw", "replyto": "F25Mg4gNfw", "signatures": ["ICLR.cc/2026/Conference/Submission9067/Reviewer_HLwj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9067/Reviewer_HLwj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834937906, "cdate": 1761834937906, "tmdate": 1762920775517, "mdate": 1762920775517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}