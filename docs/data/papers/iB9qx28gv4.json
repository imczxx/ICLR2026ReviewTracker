{"id": "iB9qx28gv4", "number": 4553, "cdate": 1757706035250, "mdate": 1759898026873, "content": {"title": "4D Latent World Model for Robot Planning", "abstract": "Learned world models are emerging as a powerful paradigm in robotics, offering a promising path toward task generalization, long-horizon planning, and flexible decision-making. However, prevailing approaches often operate on 2D video sequences, inherently lacking the 3D geometric understanding necessary for precise spatial reasoning and physical consistency. Recent work has begun to inject 3D signals into video world models (e.g., depth and normals), improving spatial understanding but still operating on surface-level projections that can struggle under occlusion and viewpoint changes. To overcome this limitation, we introduce the *4D Latent World Model*, which learns to predict the evolution of a scene's 3D structure within a compact latent space, conditioned on observations and textual instructions. The latent space encodes the scene holistically and can be decoded into diverse 3D formats (e.g., 3D Gaussian Splatting), enabling a more complete and physically consistent scene understanding. This 4D latent world model serves as a planner, generating future scenes that are translated into executable actions by a goal-conditioned inverse dynamics model. Experiments demonstrate that our model generates futures with superior visual quality, physical consistency, and multi-view coherence compared to state-of-the-art video-based planners. Consequently, our full planning pipeline achieves superior performance on complex manipulation tasks, exhibits robust generalization to novel visual conditions, and proves effective on real-world robotic platforms.", "tldr": "We propose a 4D latent world model which generates dynamic 3D structures and benefits robot planning.", "keywords": ["world model", "robot planning", "4D generation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6006283ebc60cdb5adba8a2d57ddbc1644e5c5aa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a 4D latent world model to predict the evolution of 3D scenes in a latent representation. The 4D latent world model integrates a Single Dynamics Model (SD) and a Latent Generator Model (LG), designed to forecast changes in 3D voxels and their corresponding features. Leveraging this 4D latent world model, the authors develop a robot planning framework that utilizes predicted point clouds for future forecasting. Qualitative and quantitative results demonstrate the effectiveness of the proposed method on ManiSkill3 and LIBERO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The concept of a 4D world model is promising and has garnered significant interest within the research community.\n- The authors propose a technically sound framework by decomposing the challenges of the 4D world model into coarse geometry prediction and feature prediction.\n- The authors develop a robot planning framework and demonstrate that the proposed 4D world model enhances the performance of robot planning.\n-The visual results validate the effectiveness of the proposed method in simulated environments."}, "weaknesses": {"value": "Major weakness:\n- Although the authors acknowledge that the current method is limited to surface-level projections (L5, L82), the proposed 4D world model still relies on surface points (L192), which are back-projected into a 3D representation.\n- The concept of the 4D latent world model appears unclear. The fundamental representation of the world model is a voxel grid, with each voxel corresponding to a latent feature. Consequently, the proposed 4D latent world model is not entirely \"latent.\"\n- While the authors state that \"3D reasoning becomes apparent in high-precision manipulation\" (L52), the Single Dynamics Model (SDM) only supports coarse predictions (as shown in Figure 5). It is unclear how this supports high-precision manipulation.\n\nMinor weakness:\n- Although the Latent Generator Model (LG) predicts latent features, the robot planning framework solely utilizes point clouds from the SDM. This diminishes the significance of predicting future latent features."}, "questions": {"value": "- The rendering results appear impressive in synthetic environments. How do the real-world rendering results perform, and does the method suffer from a significant sim-to-real gap?\n- Is it possible to incorporate future latent predictions into the robot planning framework, instead of relying solely on point clouds? \n- How are the feature embeddings from different views aggregated (L192)?\n- The training details for the sparse encoder (L192) and sparse decoder (L194) are not provided. If my understanding is correct, this procedure aims to map DINO features to 3D Gaussians. Why not directly map RGB to 3D Gaussians?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8CnlmIZ7fN", "forum": "iB9qx28gv4", "replyto": "iB9qx28gv4", "signatures": ["ICLR.cc/2026/Conference/Submission4553/Reviewer_XFnZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4553/Reviewer_XFnZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761543767642, "cdate": 1761543767642, "tmdate": 1762917437142, "mdate": 1762917437142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces 4D Latent World Model, a framework that learns to model and predict 4D trajectories—that is, the evolution of 3D scene representations over time—in a latent space. The model comprises two components: a Single Dynamics (SD) module that captures coarse voxel-level occupancy dynamics, and a Latent Generator (LG) module that focuses on finer-grained visual appearance and local geometry. The outputs from SD and LG are fused and converted into point-cloud representations, which are then processed by a diffusion-based inverse-dynamics head to generate low-level action sequences from the high-level latent plan.\n\nFor evaluation, the authors compare their work to Wan-2.1 and Tesseract on generative quality, and to both of them and Diffusion Policy (DP) and DP3 on a single robotic manipulation task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. With a 4D world-model operating as planning backbone, they show improvements in robustness against visual and viewpoint changes.\n2. The paper is well written and easy to follow."}, "weaknesses": {"value": "While the paper makes an interesting conceptual argument that injecting 3D semantics into video world models can improve visuomotor planning and robotic control, the experimental evidence does not sufficiently support this claim. In its current form, the evaluation appears limited in choice of baselines and tasks.\n\n1. The paper primarily compares to UniPi and TesserAct, which are relatively weak references for assessing world-model quality. To substantiate the argument that 3D semantics improve video world models, comparisons to stronger baselines such as Video Latent Diffusion Model (Video LDM), OpenSora, or DreamerV3 would be more appropriate.\n\n2. The proposed planning approach closely resembles Decision Diffuser, but the experiments are restricted to only three ManiSkill3 tasks, with near-zero performance from competing baselines. This makes it unclear whether the reported results reflect genuine improvements or task-specific tuning. A fairer and more comprehensive evaluation would include additional manipulation benchmarks where the baselines have reported their results, like RLBench (UniPi and TesserAct), Kuka stacking (Decision Diffuser), Meta-World.\n\n3. It would be informative to include an ablation comparing the flow-matching objective with the standard diffusion objective to clarify how the chosen training formulation affects model performance.\n\n4. The real-world experiments are presented only qualitatively. Reporting success rates or other quantitative results would provide stronger evidence that the learned 4D world model improves real-robot control.\n\n5. It would be helpful to include an ablation demonstrating whether utilizing the full latent representation z contributes more to the final performance compared to using a simplified representation p.\n\nOverall, the experimental design does not yet convincingly demonstrate that injecting 3D semantics into video world models leads to measurable performance gains. With broader and more rigorous comparisons, this could become a stronger contribution.\nI am open to adjusting my score if the authors can address my concerns during the rebuttal."}, "questions": {"value": "Please see my questions in the above section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3oaCq8FFMe", "forum": "iB9qx28gv4", "replyto": "iB9qx28gv4", "signatures": ["ICLR.cc/2026/Conference/Submission4553/Reviewer_fvTr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4553/Reviewer_fvTr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761692669218, "cdate": 1761692669218, "tmdate": 1762917436896, "mdate": 1762917436896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes learning a world model over 3D representations to enable robot planning with generalization capability. Instead of relying on explicit 3D representations, the authors use a latent space that can be rendered into point clouds or 3D Gaussian splatting. In the experiments, the proposed approach is compared with several baselines and shows superior performance in multiple simulated tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses an important research question and provides comparisons against several baselines. The learned dynamics model is also evaluated through real-world rollouts."}, "weaknesses": {"value": "1: Learning the dynamics for robot planning over 3D representations has been extensively studied. The authors primarily discuss world models based on video prediction but should also address prior work that learn dynamics over 3D representations (e.g., [1-7]).  Please clarify the main conceptual and methodological differences between this paper and those works. \n\n2: What’s the definition of robot action in this paper? The paper uses multiple terms such as “instruction”, “task plan”, and “joint states”. Please clarify what’s the actual interface to the robot? \n\n3: Related to my last point, there are missing details about the inverse dynamics module. Why is the full latent representation not required? One motivation for learning latent representations is to encode diverse features efficiently. How is the inverse dynamics model trained-what data, supervision, and objective functions are used? \n\n4: In section 4.2, the paper mentions “closed-loop planning”, what’s the control frequency, and under what conditions is replanning triggered? \n\n5: In tables 1 and 2, the baselines sometimes outperform the proposed approach. Please discuss the insights behind these results.  \n\n6: Could the inverse dynamics model be used to enable closed-loop control in real-world experiments? This ties back to the definition of robot actions. Additionally, what frequency can the system achieve for closed-loop planning in the real world? \n\n7: What are the failures modes of the proposed approach? How diverse is the training dataset, and to what extent can the model generalize to out-of-distribution scenarios. \n\n8: For the planning module, it would be valuable to include an ablation study on other planning approaches (e.g., sampling-based or gradient-based methods) to better understand its role and why learning an inverse dynamics model is necessary. \n\nReferences: \n\n[1]: H. Chen, Y. Niu, K. Hong, S. Liu, Y. Wang, Y. Li, and K. R. Driggs-Campbell, “Predicting object interactions with behavior primitives: An application in stowing tasks,” in 7th Annual Conference on Robot Learning, 2023. \n\n[2]: Y. Huang, C. Agia, J. Wu, T. Hermans, and J. Bohg, “Points2plans: From point clouds to long-horizon plans with composable relational dynamics,” in 2025 IEEE International Conference on Robotics and Automation (ICRA), 2025.\n\n[3]: H. Jiang, H.-Y. Hsu, K. Zhang, H.-N. Yu, S. Wang, and Y. Li. Phystwin: Physicsinformed reconstruction and simulation of deformable objects from videos. arXiv preprint arXiv:2503.17973, 2025.\n\n[4]: H. Shi, H. Xu, S. Clarke, Y. Li, and J. Wu, “Robocook: Longhorizon elasto-plastic object manipulation with diverse tools,” in 7th Annual Conference on Robot Learning, 2023.\n\n[5]: H. Shi, H. Xu, Z. Huang, Y. Li, and J. Wu, “Robocraft: Learning to see, simulate, and shape elasto-plastic objects in 3d with graph networks,” The International Journal of Robotics Research. \n\n[6]: Y. Huang, N. C. Taylor, A. Conkey, W. Liu, and T. Hermans, “Latent Space Planning for Multi-Object Manipulation with EnvironmentAware Relational Classifiers,” IEEE Transactions on Robotics (T-RO), 2024.\n\n[7]: D. Driess, Z. Huang, Y. Li, R. Tedrake, and M. Toussaint, “Learning multi-object dynamics with compositional neural radiance fields,” in Conference on robot learning."}, "questions": {"value": "See the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IooZQwBBHU", "forum": "iB9qx28gv4", "replyto": "iB9qx28gv4", "signatures": ["ICLR.cc/2026/Conference/Submission4553/Reviewer_2THB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4553/Reviewer_2THB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969315660, "cdate": 1761969315660, "tmdate": 1762917435283, "mdate": 1762917435283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a 4D latent world model for robot planning that predicts how a 3D scene evolves over time from multi‑view images and a text instruction. The core idea is to encode a scene into a sparse 3D voxel latent and roll it forward with two modules: a Single Dynamics (SD) model for coarse geometry and a Latent Generator (LG) for appearance/features. \n\nOn simulated manipulation tasks in ManiSkill3 and LIBERO, the method reports strong multi‑view 3D consistency and competitive task success relative to video‑based planners and imitation baselines. Qualitative real‑world rollouts from 4 RGB‑D cameras are presented."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall it's well motived. It shows multi‑view consistency improvements. Also large gains on 3D metrics supporting the main claim that modeling directly in a 3D latent enforces consistency across views. \n\nPlanning performance and robustness. In simulation we see it outperforms the two video‑based world model baselines and is comparable to strong imitation policies; robustness under lighting/background/camera shifts is also solid."}, "weaknesses": {"value": "Might need some analysis that connects world modeling quality to policy performance. Likely the accuracy of small region is more important than general image distance metrics. \n\nMissing qualitative real world experiments."}, "questions": {"value": "Is there particular motivation of using the sparse latent representation inspired by SLAT? \nThe contraction needs 40 camera views which seems unrealistic in real. Any ablation on the number of camera views?\nWhat if you do inverse dynamics on the latents (before using LG to fill in detailed feature representations). This ablation seems important to understand the value of the proposed design"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bGFjicWDF2", "forum": "iB9qx28gv4", "replyto": "iB9qx28gv4", "signatures": ["ICLR.cc/2026/Conference/Submission4553/Reviewer_7oiN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4553/Reviewer_7oiN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056977202, "cdate": 1762056977202, "tmdate": 1762917434802, "mdate": 1762917434802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}