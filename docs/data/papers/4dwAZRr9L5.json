{"id": "4dwAZRr9L5", "number": 19238, "cdate": 1758294711313, "mdate": 1759897050677, "content": {"title": "Mixed-Timestep Spiking Neural Networks with Temporal Alignment for Ultra-Low Latency Conversion", "abstract": "Spiking Neural Networks (SNNs) are intrinsically energy-efficient. However, most existing models enforce a uniform time-step across all layers, which limits flexibility and degrades performance under low-latency inference. \nTo address this limitation, we propose Mixed-Timestep Spiking Neural Networks (MT-SNNs), a paradigm in which each layer operates with an optimally selected time-step, thereby enabling the joint optimization of accuracy and latency.\nWithin MT-SNNs, we develop a quantization-aware conversion framework that maps a pre-trained ANN to a mixed-timestep SNN. \nSpecifically, we first establish an equivalence principle between activation bit-width and time-steps: the SNN time-step $T$ can be theoretically approximated by the activation quantization level $2^n$ in the source ANN. Based on this theory, different activation bit-widths of ANNs can be layer-wisely mapped to the corresponding time-steps of SNNs. \nThen, we jointly optimize  the quantization levels and firing thresholds to obtain an optimal parameter combination, where the overall $T$ is minimized. As a result, a optimized accuracy-latency trade-off is achieved.\nFinally, we identify a temporal dimension mismatch issue in MT-SNNs and propose a temporal alignment scheme to address this issue, ensuring proper propagation of activations across layers. \nExtensive experiments on CIFAR-10, ImageNet-1k, CIFAR10-DVS and DVS-Gesture demonstrate the effectiveness of our approach. On ImageNet-1K, our MT-SNNs achieve 73.63\\% top-1 accuracy with only 4.88 time-steps, advancing the state of the art.", "tldr": "This paper proposes Mixed-Timestep Spiking Neural Networks (MT-SNNs) , develops a quantization conversion framework, optimizes parameters, solves temporal mismatch, and validates via experiments (73.63% ImageNet-1K accuracy with 4.88 steps).", "keywords": ["Spiking neural networks; Mixed-Timestep Spiking Neural Networks; ANN-SNN Conversion; Temporal Alignment"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6efa64383ed5818d4a225cec2c1c54dbc9bf2b7a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors proposed a Mixed-Timestep Spiking Neural Network to allow each layer to operate in different timestep settings, thereby saving latency."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The presentation is clear, but I'm afraid that the motivation of this paper is problematic from the outset. The proposed method cannot reduce the latency of converted SNNs."}, "weaknesses": {"value": "When allowing mixed timesteps across layers, the proposed method enforces sequential dependencies in converted SNNs, where the (i+1) layer must wait for the (i) layer to complete computations for all its timesteps before processing can begin. While current SNNs that can be deployed on neuromorphic hardware actually follow an asynchronous, step-by-step pipeline, this approach changes this paradigm so that the model can only run layer by layer.\n\nTherefore, under such a paradigm shift, the overall inference latency of the converted SNN should correspond to the **SUM** of the timesteps across all layers, rather than their **AVERAGE**. However, as described in Section 5.1, the authors instead report an “average inference time-step\", i.e., the averaged layer-wise timesteps as the latency indicator. \n\nConsequently, the latency results reported in this paper are incorrect. The overall delay is not reduced, as the authors claim, but is in fact **increased** due to the sequential execution pipeline that it adopts. \n\nIt’s also worth noting that ANN-to-SNN methods published after 2024 will generally include the results on the Transformer architecture, as ResNet/VGG is somewhat outdated. However, this paper does not demonstrate the effectiveness of its approach on the Transformer.\n\nTherefore, I am afraid that the motivation of this paper is problematic from the outset, and I am unable to give a positive rating to the paper."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "h7VIRHkBnH", "forum": "4dwAZRr9L5", "replyto": "4dwAZRr9L5", "signatures": ["ICLR.cc/2026/Conference/Submission19238/Reviewer_4CD7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19238/Reviewer_4CD7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761481855959, "cdate": 1761481855959, "tmdate": 1762931217228, "mdate": 1762931217228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a mixed-time-step methodology grounded in a \"conversion-and-training\" paradigm. The method establishes the equivalence relationship between mixed-quantized Artificial Neural Networks (ANNs) and the mixed time-steps of Spiking Neural Networks (SNNs), and further develops a corresponding conversion methodology. Subsequently, the optimal parameter combination is attained through the joint optimization of quantization precision and neurons' firing thresholds. Moreover, the paper rectifies inconsistencies among different time-steps by means of an average firing rate method. Experiments conducted on diverse datasets validate that the proposed methodology delivers performance advantages."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The discussion on loss surfaces across four parameter combinations is insightful. As the loss surface is dominated by the loss function and sample data, a more in-depth theoretical analysis of the optimal parameter combination for optimization is recommended.\n- This paper is logically structured and highly accessible."}, "weaknesses": {"value": "- The equivalence between Spiking Neural Networks (SNNs) and Artificial Neural Networks (ANNs) has been discussed, and low-latency conversion methods have been proposed[1].\n- Moreover, mixed-timestep SNN conversion approaches have also been put forward[2].\n- The average temporal alignment strategy may introduce additional overhead. It is acknowledged that the effort to show the advantages of the proposed average temporal alignment strategy is valuable. However, simulations conducted on GPUs lack persuasiveness. It is recommended that the authors explore the application of neuromorphic hardware.\n- A performance comparison with state-of-the-art methods across tasks(such as CIFAR10 with [2]) is encouraged, though this is not deemed critical.\n\n[1]Bu T, Fang W, Ding J, et al. Optimal ANN-SNN conversion for high-accuracy and ultra-low-latency spiking neural networks. ICLR, 2023.\n[2]Du K, Wu Y, Deng S, et al. Temporal Flexibility in Spiking Neural Networks: Towards Generalization Across Time Steps and Deployment Friendliness. ICLR, 2025.\n\nMinor:\n- line 040~041 '...enabeling enabeling...' duplication\n- line 160~161 '...to the resolve inherent temporal dimention mismatch issue...'\n- As some papers have been published (such as [1], [2]), please update the references."}, "questions": {"value": "- What's the main difference between the proposed work and [1][2]?\n- Could you provide the experimental comparison against [1][2]?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MrSqDvBmz7", "forum": "4dwAZRr9L5", "replyto": "4dwAZRr9L5", "signatures": ["ICLR.cc/2026/Conference/Submission19238/Reviewer_wxFv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19238/Reviewer_wxFv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988125225, "cdate": 1761988125225, "tmdate": 1762931216831, "mdate": 1762931216831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Mixed Timestep SNNs, a novel framework that allows different layers to operate with distinct inference timesteps. Within an ANN-to-SNN conversion paradigm, the authors model per-layer timesteps as learnable parameters and jointly optimize them with activation thresholds via end-to-end training. To address inter-layer misalignment caused by heterogeneous timesteps, they introduce an \"average temporal alignment\" mechanism: the temporal average of upstream spiking outputs is repeated as a static input to downstream layers, effectively decoupling temporal dependencies across layers. Experiments demonstrate that MT-SNNs achieve a state-of-the-art performance for low-latency SNN conversion"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.This paper innovatively treats timesteps as differentiable, learnable variables within the ANN-to-SNN conversion framework, moving beyond conventional fixed or heuristic timestep assignments.\n\n2.The method is thoroughly evaluated across multiple architectures and datasets, with comparisons to recent SNN approaches such as QCFS and SRP, yielding convincing accuracy and efficiency results."}, "weaknesses": {"value": "1. Experiments are limited to moderate-scale CNNs. It remains unclear whether the approach generalizes to more complex architectures like Vision Transformers or large-scale models, where heterogeneous timesteps could introduce training instability or convergence issues.\n\n2. The method enforces strictly sequential inter-layer execution: layer l must complete all $T_l$ timesteps before layer l+1 can begin. In contrast, many existing ANN-to-SNN conversion techniques support pipelined or truly asynchronous inference, where layers operate concurrently and total latency scales with the slowest layer rather than the cumulative sum. This fundamental difference in execution models calls into question the fairness of latency–accuracy comparisons reported in the paper."}, "questions": {"value": "1. If layers must be executed sequentially, should the compare the total inference time-step be defined as $\\sum_l T_l$ with other works rather than the reported average time-step? Please clarify the time-step used throughout the evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eVbZJHsdlV", "forum": "4dwAZRr9L5", "replyto": "4dwAZRr9L5", "signatures": ["ICLR.cc/2026/Conference/Submission19238/Reviewer_eJsg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19238/Reviewer_eJsg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762595627652, "cdate": 1762595627652, "tmdate": 1762931216573, "mdate": 1762931216573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}