{"id": "Q8qIaICkvZ", "number": 8527, "cdate": 1758089609915, "mdate": 1759897778530, "content": {"title": "First return, entropy-eliciting explore", "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning abilities of Large Language Models (LLMs) but it struggles with unstable exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a structured exploration framework that identifies high-uncertainty decision points in reasoning trajectories and performs targeted rollouts to construct semantically grounded intermediate feedback. Our method provides targeted guidance without relying on dense supervision. Empirical results on mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable training, produces longer and more coherent responses, and increases the proportion of fully correct trajectories. These results highlight the framework's effectiveness in improving LLM reasoning through more robust and structured exploration.", "tldr": "FR3E stabilizes RL for LLM reasoning by using entropy to identify uncertain steps and launching targeted explorations from those points, creating efficient feedback without dense supervision.", "keywords": ["Reinforcement Learning; LLM Reasoning; Entropy"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10027f873c1af9aceea8e934d63799e30360021a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes FR3E, a novel RL fine-tuning algorithm that identifies highly uncertain tokens and performs rollouts from these tokens to estimate values for intermediate tokens. These values are then utilized to adjust advantages for stable learning. FR3E demonstrates superior performance to GRPO++ on various math reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of structured rollout based on token entropy, is both novel and timely.\n- The proposed method consistently improves performance against GRPO across various benchmarks and models."}, "weaknesses": {"value": "- Section 4.3 lacks a clear motivation. Specifically, it does not adequately explain how advantage modulation promotes exploration.\n- The paper does not provide values for key hyperparameters, such as the number of forking tokens and the number of responses generated for each forking token.\n- The paper compares FR3E against GRPO++, but the latter is mentioned without any introduction.\n- It is questionable whether the paper did a fair comparison with GRPO++."}, "questions": {"value": "- For a given prompt, do you first generate one base response, then identify K forking tokens, and from each of those tokens, generate M new responses? And to confirm, is the total number of generated responses per prompt, K * M, equal to 16?\n- Following up on my previous question, why was the number of responses per prompt for GRPO++ set to 4? This value seems a bit low to me. Wouldn't it be more effective to use 16, consistent with FR3E? For a fairer comparison, FR3E should be evaluated against GRPO++ with a group size of 16. In FR3E, if the forking tokens are concentrated at the beginning of the sequence, the computational cost becomes nearly identical to that of parallel sampling. In fact, your method is more expensive, as it also incurs the additional cost of sampling the base response.\n\n- Could you elaborate on the advantage computation process? Specifically, for the M responses generated from the forking token, are advantages calculated consistently with GRPO, including standard normalization? Also, what is the procedure for calculating the advantage for the base response?\n- What is the role of advantage modulation? Given a forking token, the sum of advantages for the generated M responses should be zero, so what does multiplying them by a coefficient actually change?\n- Why was the PPO clip ratio set to [0.22, 0.28], a choice that differs from the conventional settings in GRPO ([0.2, 0.2]) and DAPO ([0.2, 0.28])?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J60LUKPELz", "forum": "Q8qIaICkvZ", "replyto": "Q8qIaICkvZ", "signatures": ["ICLR.cc/2026/Conference/Submission8527/Reviewer_xwRF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8527/Reviewer_xwRF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760696875454, "cdate": 1760696875454, "tmdate": 1762920388494, "mdate": 1762920388494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes FR3W, which is a structured RL exploration framework that (i) discovers top high-entropy tokens in the reasoning trajectories, (ii) conducts structured rollouts at different high-entropy states, and (iii) learns via an adaptive advantage modulation factor. Extensive evaluations based on Qwen series show that the proposed method outperforms GRPO++ on math tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1)\tThe proposed idea is simple and sound (targeted exploration at high-entropy states), achieving good overall performance on math tasks.\n2)\tThe authors have conducted further model analyses on the training dynamics, sources of gains for a better understanding.\n3)\tThe writing is clear and the method is easy to follow."}, "weaknesses": {"value": "1)\tThis work mainly concentrates on the math tasks. Is this work still effective in other tasks, such as more challenging agent-related scenarios with sparser reward signals?\n2)\tThere have been quite a few entropy-aware RL methods recently, which can be mentioned in related works (the differences should be discussed to highlight the contribution proposed by this work).\n3)\tThe base reasoning trajectory is essential in FR3E. It is strongly suggested that the authors could give an in-depth discussion on how to select satisfactory base reasoning trajectories that potentially lead to success.\n4)\tIn Page 5, the figure could be replaced by high-entropy samples in this work.\n5)\tThe effectiveness of the advantage modulation factor \\alpha should be evaluated. For example, FR3E w/o $\\alpha$, and FR3E w/o $\\alpha$ when $\\alpha<1$, are two promising ablation versions that should be compared (indicating whether downscaling the positive signal is beneficial).\n6)\tFR3W should be evaluated on other base LLM series besides Qwen2.5.\n7)\tWhy did the authors select GRPO++ as the only baseline? There are some methods that adopt similar ideas (e.g., DAPO [1], which also adopts clip-higher), which should be compared as baselines.\n8)\tThe detailed training costs (e.g., the overall costs of rollout) should be given (the explanation in Appendix D.4 is obscure). Does the model improvement mainly come from more rollouts?\n\n[1] Yu Q, Zhang Z, Zhu R, et al. Dapo: An open-source llm reinforcement learning system at scale[J]. arXiv preprint arXiv:2503.14476, 2025."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z2kOxKlzN8", "forum": "Q8qIaICkvZ", "replyto": "Q8qIaICkvZ", "signatures": ["ICLR.cc/2026/Conference/Submission8527/Reviewer_Yyi2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8527/Reviewer_Yyi2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761040511637, "cdate": 1761040511637, "tmdate": 1762920387841, "mdate": 1762920387841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposes a novel framework FR3E (First Return, Entropy-Eliciting Explore) to make exploration for Reinforcement Learning with Verifiable Rewards (RLVR) more structured. The framework consists of two phases, which are the namesakes for the framework: First Return and Entropy-Eliciting Explore. In the first phase reasoning steps are identified by using token-level entropy to identify tokens with a high uncertainty. From these tokens the top-K tokens, with K being a hyperparameter, are selected, which are then used to divide the trajectory into reasoning steps. For each of these intermediate reasoning steps, generation is restarted and based on the partial trajectory (till this reasoning step) rollouts are generated to provide reward signal for the reasoning step. Additionally the advantages of the reasoning steps are adapted based on the rewards of the reasoning step as well as its predecessor to encourage exploration and to stabilize training.\n\nDisclosure: I accidently learned the author names by checking the references of another manuscript, that I reviewed for ICLR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* topic is relevant and timely\n* clearly written\n* reasonable evaluation:\n  * models of three different sizes are tested\n  * quite a few benchmarks tested"}, "weaknesses": {"value": "* limited novelty: Besta et al. (Reasoning Language Models: A Blueprint, arXiv:2501.11223, Jan. 2025) already propose the use of entropy as a metric to identify decisions point as well as outcome-driven process based rewards, albeit to be fair they only present their ideas without actually implementing and evaluating them.\n* some aspects of evaluation I expected are missing:\n  * GRPO++ details are missing, inclusive a discussion why GRPO++ is a competitive baseline\n  * 5.1: hyperparameter such as K are missing\n  * cost analysis, for example:\n    * additional compute cost compared to GRPO++? (D.4 is not that convincing)\n* reproducibility statement: number of GPUs are not discussed in 5.1\n\nminor issues:\n* abstract: \"benchmarks(AIME24)\" - missing whitespace\n* related work: missing brackets around the references\n* preliminary: already discuss phases before they are introduced in 3.4\n* Figure 1, caption: Start sentences with a capital letter.\n* Fig. 3 d) to f) - missing y axis label\n* 4.3: \"in the appendix C\" -> \"in Appendix C\"\n* 5.1: no references for DeepScaler/SimpleRL/VeRL and for the benchmarks\n* Figure 3: consider removing the titles within the subfigures for better readability (also Figures 4 and 5)\n* 5.2: no reference for GRPO++\n* 5.3.2, Higher Entropy Enables Healthier Exploration: \"a similar pattern appears at a different scale (Figure 3b) and on Qwen2.5-32B (Figure 3c)\" - Shouldn't this be switched, i.e. \"different scale (Figure 3c) and on Qwen2.5-Math-7B (Figure 3b)\"? Or should the second part completely be omitted, since the Qwen2.5-Math-7B results are discussed in the subsequent sentence?\n* 5.3.2, line 431: \"achieve s\" - typo: unnecessary whitespace\n* Appendix B: no references for DAPO dataset\n* Appendix C, equation 16: $O_j$ might not have been defined\n* references:\n  * consider the proper capitalization of the titles, at least for proper names and abbreviations to improve readability\n  * place of publication of arXiv references can only surmised from the URL\n  * [Brown et al. 2020], [Wei et al. 2022] - properly capitalize the journal/booktitle: \"Advances in neural information processing systems\" to be consistent with [Ouyang et al. 2022]\n  * [Cui et al. 2025b], [Ecoffet et al. 2019], [Forootani 2025], [Guo et al. 2025], [Pignatelli et al. 2023], [Wu et al. 2024], [Zhou et al. 2022]  - cited differently than the other arXiv references\n  * [Lightman et al. 2023] - was published at ICLR '24\n  * [Ouyang et al. 2022] - missing volume number and pages numbers to be consistent with [Brown et al. 2020]\n  * [Ranzato et al. 2016] - doesn't look like a proper bibtex entry\n  * [Zhou et al. 2022] - was published at ICLR '23"}, "questions": {"value": "* introduction: CoT [Wei et al. 2022] is a prompting scheme, so why it is cited in regards to RL?\n* Did you conduct an analysis into how many tokens with high uncertainty are found, i.e. how reliable the top-K mechanism works?\n* Maybe I did not read the manuscript properly, but what value of K did you use in your evaluation study. It seems not to be mention in 5.1.\n* How to ensure that words are not split at the token level?\n* Did you test other model families?\n* How can Qwen2.5-Math-7B can reach the maximum sequence length if it plateaus in Figure 4 at around 2.5k tokens, when the maximum token length is 16k?\n* FR3E produces longer chains: How did you verify that they are consistent and do not result in over-thinking, which is one of the aspect that you want to address with the framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rpcgmNyUuB", "forum": "Q8qIaICkvZ", "replyto": "Q8qIaICkvZ", "signatures": ["ICLR.cc/2026/Conference/Submission8527/Reviewer_RSPV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8527/Reviewer_RSPV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912011833, "cdate": 1761912011833, "tmdate": 1762920387436, "mdate": 1762920387436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FR3E (First Return, Entropy-Eliciting Explore), a structured exploration framework that identifies high-entropy points in\nreasoning trajectories and performs targeted rollouts to construct semantically grounded intermediate feedback. This method provides targeted guidance without relying on dense supervision, solving granular credit assignment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Performance gains: FR3E demonstrates either superior or at least competitive performance compared to GRPO++, notably for general-purpose LLMs (Qwen2.5-7B, Qwen2.5-32B), with more modest gains for domain-specific models (Qwen2.5-Math-7B).\n\n2. Improved training dynamics: FR3E shows notably higher and more stable entropy throughout training, visible in Figure 3, suggesting healthier exploration and avoidance of entropy collapse.\n\n3. Fine-grained credit assignment: The adaptive advantage modulation component keeps advantage estimates well centered and tightly distributed around zero, which theoretically reduces gradient estimator bias and allows for more stable optimization."}, "weaknesses": {"value": "### Method\n1. The process of advantage calculation is insufficiently descriptive in the main text. I have seen Appendix C, but still a little confused. For trajectories that share the same prefix (*e.g.*, $P_{j, m], P_{j,0}$ ) but different rewards, do they have different advantages on the shared tokens? For one trajectory, are the advantages over all tokens in FR3E the same, or do they differ depending on the divided state?\n\n### Experiments\n2. Missing relevant hyperparameters: In Section 4, the authors divide the original trajectory into $K$ state blocks and generate $M$ targeted rollouts for each state. What's $K$ and $M$?\n\n3. Unclear computational costs: As the authors present in Appendix D.1, GRPO++ employs default rollout numbers of 4 per prompt. In FR3E, it seems to require $K \\\\cdot M$ rollouts per prompt, which is much more than GRPO++baseline. It raises concerns on whether the performance gain stems from a larger number of rollouts. I suggest providing detailed training time, token usage, and inference costs comparison with baselines.\n\n4. The model is limited to the Qwen2.5 series. While the Qwen2.5 series is well-pretrained to provide a solid foundation in post-training, it also raises concerns on data contamination in widely used benchmarks [1]. Consequently, breakthroughs are predominantly observed for the mathematically strong Qwen2.5 series on benchmarks such as MATH-500, AMC, and AIME, and seldom transfer to models like Llama. I believe a more in-depth investigation on other model families (*e.g.*, Llama) is needed to validate the effectiveness of FR3E.\n\n### Missing References\n5. The dataset and benchmarks used in this paper are not cited (Section 5.1), *i.e.*, GSM8K, Math500, Minerva Math, Gaokao2023en, OlympiadBench, which is inappropriate.\n\n6. There are several works that enhance the exploration capability in RL training [2][3][4]. I suggest discussing them in the related work.\n\n---\n\n[1]  Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination. arXiv preprint arXiv:2507.10532\n\n[2] Reasoning with Exploration: An Entropy Perspective on Reinforcement Learning for LLMs. arXiv preprint:2506.14758\n\n[3] TreeRL: LLM Reinforcement Learning with On-Policy Tree Search. ACL 2025\n\n[4] Reasoning with Reinforced Functional Token Tuning. arXiv preprint:2502.13389"}, "questions": {"value": "1. What is GRPO++? What's the difference with vanilla GRPO? I don't see any description or reference.\n\n2. Why not use DAPO as a baseline? Is DAPO better than so called GRPO++?\n\n3. I notice that FR3E achieves a longer response length than GRPO++ in Figure 4. Does this mean FR3E encourages overthinking? The authors claim \"FR3E enables longer and more consistent reasoning chains compared to GRPO++\", but I don't see any supported evidence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7jjPs8p5gu", "forum": "Q8qIaICkvZ", "replyto": "Q8qIaICkvZ", "signatures": ["ICLR.cc/2026/Conference/Submission8527/Reviewer_uSFe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8527/Reviewer_uSFe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998036006, "cdate": 1761998036006, "tmdate": 1762920387033, "mdate": 1762920387033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}