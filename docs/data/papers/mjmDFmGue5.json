{"id": "mjmDFmGue5", "number": 6046, "cdate": 1757951517830, "mdate": 1763733298908, "content": {"title": "SURGE: Surrogate Gradient Adaptation in Binary Neural Networks", "abstract": "The training of Binary Neural Networks (BNNs) is fundamentally based on gradient approximation for non-differentiable binarization operations (e.g., sign function). However, prevailing methods including the Straight-Through Estimator (STE) and its improved variants, rely on hand-crafted designs that suffer from gradient mismatch problem and information loss induced by fixed-range gradient clipping. To address this, we propose SURrogate GradiEnt Adaptation (SURGE), a novel learnable gradient compensation framework with theoretical grounding. SURGE mitigates gradient mismatch through auxiliary backpropagation. Specifically, we design a Dual-Path Gradient Compensator (DPGC) that constructs a parallel full-precision auxiliary branch for each binarized layer, decoupling gradient flow via output decomposition during backpropagation. DPGC enables bias-reduced gradient estimation by leveraging the full-precision branch to estimate components beyond STE's first-order approximation. To further enhance training stability, we introduce an Adaptive Gradient Scaler (AGS) based on an optimal scale factor to dynamically balance inter-branch gradient contributions via norm-based scaling. Experiments on image classification, object detection, and language understanding tasks demonstrate that SURGE performs best over state-of-the-art methods.", "tldr": "", "keywords": ["Binary Neural Network", "Straight-Through Estimator", "Gradient Compensation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f0de1b02c83c7a9defeecca98a0c7f8c1429a17d.pdf", "supplementary_material": "/attachment/8f69355da56122cf57e676643bfde8751c07714d.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a training-time scheme for 1-bit / low-precision networks that adds a parallel full-precision auxiliary branch alongside each binarized layer. The forward output remains identical to the binarized model, while the auxiliary path is used only to inject a surrogate/compensated gradient in backprop. An adaptive gradient scaling factor is introduced to balance gradients from the binary and auxiliary branches. The auxiliary branch is discarded at inference, so deployment cost is unchanged. Experiments report modest accuracy gains on vision (e.g., ImageNet) and NLP (e.g., GLUE)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Simple to implement: a same-shape auxiliary operator plus a scalar to fuse gradients; easy to integrate into standard training pipelines.\nDeployment cost unchanged: the auxiliary path is removed at test time, keeping inference identical to the binarized model.\nBroad applicability (claimed): results across image classification/detection and BERT-style finetuning show consistent, if small, gains."}, "weaknesses": {"value": "Lack of novelty (primary concern).\nThe core idea—keeping a full-precision “shadow”/auxiliary module during training to provide a better surrogate gradient—is conceptually close to established binarization/QAT practices that maintain latent full-precision parameters and/or design proxy/straight-through gradients. The proposed adaptive scaler is essentially a heuristic based on gradient-norm ratios; the theoretical support relies on strong assumptions (e.g., near-aligned directions), and the derivation largely reduces to tuning an empirical constant. Overall, the paper reads as an engineering variation on surrogate-gradient training rather than a fundamentally new principle.\n\nUndercuts the purpose of quantized training.\nThe method introduces an unquantized, isomorphic module during training for every binarized layer. This adds nontrivial compute and memory, negating the expected efficiency benefits of low-precision training. Even if inference is unaffected, the paper does not convincingly argue that its training-time cost/throughput is competitive with (a) a strong binarized baseline without the auxiliary path, and (b) a standard full-precision training run of the same model. Without a rigorous cost breakdown (FLOPs/BOPs, wall-time, throughput, peak memory per layer) the claimed advantages for “quantized training” are not substantiated.\n\nMissing baselines and fairness.\nThe comparisons omit several strong and recent 1-bit and low-bit training baselines. In particular, there is no head-to-head against influential BNN methods such as IR-Net, ReActNet, RBNN, ReCU, FDA-BNN, or RBONN; on the NLP side, BiT is missing or not reproduced under its best practices. Beyond pure 1-bit, there is also no comparison to widely-used low-bit QAT methods (e.g., LSQ at 2–4 bits) that often match or exceed 1-bit accuracy with far less training instability. The absence of these baselines makes it hard to judge the true merit of the method.\n\nLiteratures should be added:\nBitNet: 1-bit Pre-training for Large Language Models, JMLR 2025\nLatent Weight Quantization for Integerized Training of Deep Neural Networks, IEEE TPAMI 2025\nJetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization, ICML 2024\n“Recurrent Bilinear Optimization for Binary Neural Networks (RBONN),” ECCV 2022 (oral)\n“BiT: Robustly Binarized Multi-distilled Transformer,” NeurIPS 2022\n\n\nThe adaptive scaling derivation leans on restrictive assumptions (e.g., small angular deviation, isotropic noise). The paper does not empirically validate these assumptions (e.g., measuring cosine similarity of gradient components, variance across layers/steps) nor analyze stability (distribution of the scaling factor over training, sensitivity to the hyper-constant). This weakens the theoretical claims.\n\nClaims of architecture/task generality are not matched by breadth of experiments. Modern and larger backbones (e.g., ConvNeXt/ViT), stronger detection/segmentation stacks, or larger-scale NLP (beyond GLUE finetuning) are absent. The reported gains are modest on the tested settings and could plausibly be absorbed by better tuning or distillation.\n\nImportant training-efficiency metrics (FLOPs/BOPs, wall-clock per epoch, throughput, peak activation memory) and per-layer overhead breakdowns are missing. Baseline reimplementations and ablation choices (e.g., with/without distillation on NLP) are not fully documented, raising fairness concerns."}, "questions": {"value": "In what precise sense is your auxiliary full-precision branch different from keeping latent FP weights and using tailored STE/backward hooks? Can you reproduce most of the gains with a cheaper backward-only proxy (no full FP module), and show why that is insufficient?\n\nProvide a layer-wise compute/memory breakdown and end-to-end throughput/wall-time vs (a) the same binarized model without the auxiliary branch, and (b) a full-precision baseline. Include BOPs/FLOPs, activation checkpointing, and peak memory statistics.\n\nTest on modern/deeper backbones (ConvNeXt/ViT), larger detection/segmentation suites, and larger-scale NLP/pretraining tasks to substantiate architecture/task-agnostic claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ppv3nNGSnz", "forum": "mjmDFmGue5", "replyto": "mjmDFmGue5", "signatures": ["ICLR.cc/2026/Conference/Submission6046/Reviewer_o5rk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6046/Reviewer_o5rk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703393637, "cdate": 1761703393637, "tmdate": 1762918428382, "mdate": 1762918428382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training framework for binary neural networks (BNNs) that aims to reduce the bias introduced by the Straight-Through Estimator (STE). The core idea is to construct a Dual-Path Gradient Compensator, where each binary layer is accompanied by an auxiliary full-precision branch used only in the backward pass. During training, the binary branch provides standard STE gradients, while the auxiliary branch contributes an additional gradient term scaled by a dynamically computed coefficient through the Adaptive Gradient Scaler module. The authors claim that this combination yields “less biased” gradient estimates and improves convergence stability without modifying the forward computation. Empirical results are reported on vision and language benchmarks, where SURGE slightly outperforms several prior BNN training methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on the gradient mismatch problem in BNNs, which remains an important and unsolved issue in this area. The proposed idea is straightforward and easy to implement, requiring only an auxiliary operator of the same shape and a single scalar to combine gradients. The experiments cover both vision and language benchmarks, providing a reasonably broad evaluation of the approach."}, "weaknesses": {"value": "(1) The motivation for directly adding the auxiliary branch gradient to the main branch is insufficient. The paper does not show any correlation between the two in direction or information, nor does it analyze whether this operation truly reduces gradient bias or merely introduces additional noise and instability.\n\n(2) The technical contribution is limited. Adding an auxiliary gradient branch lacks solid theoretical justification. The reviewer has comprehensive concerns about the theoretical work presented in this paper. The authors rely on many assumptions without validation, and the derivations proceed largely through coarse approximations. More seriously, as in the appendix, the authors might confuse some random variables with their expectations.\n\n(3) The magnitude of empirical improvement is small. Compared with recent BNN training methods, the performance gains are limited on most benchmark tasks.\n\n(4) Minor: there are several typos in the paper, e.g., a symbol is missing in Equation (7)."}, "questions": {"value": "(1) The authors should clarify the overall mathematical form of the optimization problem, including the model being optimized, the optimization objective, and the corresponding “gradient” information. In the training of BNNs, the ultimate goal should be to update the BNN parameters according to the network output. On this basis, the authors need to carefully explain how the “gradient” is approximated or replaced, and whether it remains consistent with the intended optimization objective. The notation system and the problem formulation throughout the paper should also be kept consistent and coherent.\n\n(2) The reviewer suggests that the authors visualize their results on very small models (for example, in two dimensions) to verify the correctness of the “gradient.” Such visualization would help confirm whether the proposed surrogate gradient behaves as expected and is consistent with the theoretical claims.\n\n(3) Since the proposed method relies on precise full-precision computation, it may contradict the original motivation of quantization. The authors are encouraged to clarify whether the introduced auxiliary branch preserves or compromises the computational efficiency that BNNs are expected to provide."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0S7rH2Eunm", "forum": "mjmDFmGue5", "replyto": "mjmDFmGue5", "signatures": ["ICLR.cc/2026/Conference/Submission6046/Reviewer_6WTK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6046/Reviewer_6WTK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726296187, "cdate": 1761726296187, "tmdate": 1762918428052, "mdate": 1762918428052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces “SURGE” (SURrogate GradiEnt Adaptation), a new learnable gradient compensation framework for improving the training of Binary Neural Networks (BNNs). The approach is grounded in a dual-path architecture: a Dual-Path Gradient Compensator (DPGC) that injects a parallel full-precision auxiliary branch to enhance gradient signals during training, and an Adaptive Gradient Scaler (AGS) that dynamically scales auxiliary gradients. The authors provide theoretical justification for this adaptive scaling and back the method with experiments on image classification, object detection, and NLP tasks, reporting improvements over state-of-the-art BNN training methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The use of a dual-path auxiliary branch (DPGC) carefully decouples the forward inference path from the backward gradient path, maintaining inference efficiency while strengthening training gradients. This directly addresses well-known sources of optimization instability in BNNs.\nThe adaptive scaling (AGS) is not hand-tuned, but instead derived as an approximately optimal scaling rule (Theorem 1, Section 5, and Appendix B). The mathematical exposition thoroughly details and justifies the gradient composition and scaling.\nThe method is validated across standard benchmarks in vision (CIFAR-10, ImageNet-1K), detection (PASCAL VOC), and language (GLUE), showing consistent and in many cases substantial improvements over strong BNN baselines. Table 1 (Page 7) and Table 2 (Page 8) each clearly show that SURGE outperforms existing methods such as ReCU, IR-Net, and RBONN in Top-1 and Top-5 accuracy.\nAn ablation study (Table 6, Page 9) teases apart the impact of DPGC and AGS, and explores parameterizations and compensation scope. Figure 3 (Page 9) effectively demonstrates how adaptive scaling—rooted in theory—improves upon fixed scaling factors, reinforcing both the soundness and the practicality of the design."}, "weaknesses": {"value": "1.The experimental section, while broad, suffers from several omissions. On ImageNet for example, only ResNet-18 is used. There is no indication that the results generalize to significantly deeper or more modern architectures (e.g., ResNet-50/101, EfficientNet, ViT). Lacking these, it is unclear whether the claimed improvements are specific to compact/legacy models or more widely applicable.\n2.Object detection results (Table 4) are presented for a single backbone, without evidence for transferability to more challenging detection benchmarks (e.g., COCO) or modern architectures.\n3.GLUE experiments in Table 5 do not include metrics for individual subtasks beyond those provided for other baselines, creating ambiguity about where gains are realized and how stable the approach is across tasks.\n4.The dual-path architecture, while discarded during inference, necessarily doubles (or nearly doubles) memory and computation during training, as acknowledged in Section F (Page 18). Appendix E provides numbers, but no mitigation strategies for large-scale, distributed, or memory-constrained training scenarios, where these costs may render the approach impractical, especially for larger models. While Table A quantifies some training overhead for ImageNet, it does not empirically cover scaling to larger modern models or distributed settings.\n5.While the adaptive scaling rule in AGS is theoretically derived (Theorem 1, Page 6 and Appendix B), the argument relies heavily on simplified assumptions (e.g., isotropic noise, zero-mean error vectors). The practical efficacy is demonstrated mainly at a single value of $\\eta$, but a more nuanced analysis of AGS’s stability and sensitivity—especially in pathological or highly non-convex regimes—is lacking. Figure 3(b) (Page 9) does show optimality for one $\\eta$, but the analysis is shallow.\n5.There is insufficient discussion of potential instability or rare failure cases, e.g., what happens if the full-precision path’s gradients become poorly conditioned or if AGS fails to adapt due to scale disparity.\n6.The paper claims more stable and less biased gradient estimates, yet provides no direct evidence via convergence diagnostics (e.g., loss curves, gradient norms over epochs, or variance measures). A more detailed investigation (beyond small ablation tables) into how training (not just end results) progresses under compensation would be valuable."}, "questions": {"value": "1.Scalability: How does SURGE behave for substantially larger or more modern architectures? Does the training overhead become prohibitive for ResNet-50/101 or ViT models? Do you envision variants that scale the auxiliary branch more efficiently (e.g., class-conditional paths, group convolutions)?\n2.Robustness and Convergence: Have you monitored optimization stability and convergence in pathological or ill-conditioned settings? Are there conditions under which AGS’s adaptivity fails or leads to gradient explosion/vanishing?\n3.Ablation on Overhead vs. Accuracy Trade-off: Could you provide more empirical data or analysis linking the training time/memory overhead to accuracy improvements, especially for larger batch sizes, datasets, or distributed hardware?\n4.Disambiguation from Related Dual-Path/Bi-Real Methods: Beyond architectural drawings, can you clarify concretely how your dual-path compensator differs in mechanism and impact from Bi-Real Net’s skip connections or BinaryDuo’s coupling strategy? What pitfalls/tradeoffs did you observe during comparative trials?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jjSFbYEsMK", "forum": "mjmDFmGue5", "replyto": "mjmDFmGue5", "signatures": ["ICLR.cc/2026/Conference/Submission6046/Reviewer_ihX2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6046/Reviewer_ihX2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888813127, "cdate": 1761888813127, "tmdate": 1762918427676, "mdate": 1762918427676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the training of binary neural networks. Standard pseudo-gradient methods for updating the parameters such as STE are prone to errors. The paper proposes to use an alternate path in the backward pass for correcting this pseudo gradient."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The notion that that the standard STE gradient estimator's error can be corrected using alternate paths is a valuable research direction."}, "weaknesses": {"value": "The two main issues with the paper are lack of clarity and mathematical rigour. \n\n1. Section 4, which describes the proposed method is extremely hard to understand. I tried going over the pytorch code for CIFAR resnet and the \"binarized_modules.py\" file in the supplementary. I still could not fully follow the details. The Algorithm A in the appendix also does not make things clear. I recommend the following. Take a simple neural net architecture (say a single hidden layer fuly connected ReLU network on a simple 2d dataset) and illustrate how full precision parameters, STE binary parameters, and SURGE binary parameters very during training and at the end of training. Even if this example does not illustrate the advantages of SURGE over STE, it would help the readers to grasp the details.\n\n2. The theoretical analysis section is very hand-wavy and non-rigorous. Partial derivatives have fixed meanings and we are not free to redefine them. For example Equation 7 is clearly wrong, as for all binary (or discrete) forward function the backward operation is either underfined or 0. The intention is clearly an approximation, but that cuases confusion, as I was trying to see if an equality between LHS and RHS is implied anywhere for quite some time.\n\n3. Continuing on the hand-wavy nature of Section 5. What is being assumed exactly in defniition 1? Definitions are separate from assumptions. Better to state assumptions separately. What is a \"better gradient\"? \n\n4. Section 4.2 is also hand-wavy. Why is $\\lambda*$ defined the way is it? In what sense is it \"optimal\"? What is a squiggly equal to doing inside Theorem 1? Theorem 1 and its \"proof\" has too many squiggles to be a valid theorem. \n\n5. A reasonable approach to a theorem would make more assumptions on setting and data (like in comment 1 above) and argue that the proposed update rule would lead to a better binary neural network. \n\n6. A last minor issue would be the very small empirical outperformance of SURGE over other BNNs (like RECU). While there is intrinsically nothing bad about a small improvement like this, the readers would like to know are there any simple situations (synthetic settings are fine too) where the proposed method outperforms STE and RECU by a large margin and the reason for the outperformance is clearly illustrated."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CGwDYPPiCb", "forum": "mjmDFmGue5", "replyto": "mjmDFmGue5", "signatures": ["ICLR.cc/2026/Conference/Submission6046/Reviewer_72X1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6046/Reviewer_72X1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762345345177, "cdate": 1762345345177, "tmdate": 1762918427281, "mdate": 1762918427281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}