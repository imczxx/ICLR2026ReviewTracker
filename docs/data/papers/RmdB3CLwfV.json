{"id": "RmdB3CLwfV", "number": 6492, "cdate": 1757986923596, "mdate": 1759897911512, "content": {"title": "Beyond Heuristics: Globally Optimal Configuration of Implicit Neural Representations", "abstract": "Implicit Neural Representations (INRs) have emerged as a transformative paradigm in signal processing and computer vision, excelling in tasks from image reconstruction to 3D shape modeling. Yet their effectiveness is fundamentally limited by the absence of principled strategies for optimal configuration—spanning activation selection, initialization scales, layer-wise adaptation, and their intricate interdependencies. These choices dictate performance, stability, and generalization, but current practice relies on ad-hoc heuristics, brute-force grid searches, or task-specific tuning, often leading to inconsistent results across modalities. We introduce OptiINR, the first unified framework that formulates INR configuration as a rigorous optimization problem. Leveraging Bayesian optimization, OptiINR efficiently explores the joint space of discrete activation families—sinusoidal (SIREN), wavelet-based (WIRE), variable-periodic (FINER)—and continuous initialization parameters. This systematic approach replaces fragmented manual tuning with a coherent, data-driven optimization process. By delivering globally optimal configurations, OptiINR establishes a principled foundation for INR design, consistently maximizing performance across diverse signal processing applications.", "tldr": "OptiINR frames INR design as optimization via Bayesian optimization, finding layerwise activations and inits. It beats manual baselines on image, audio, and 3D; spectra show reconstructions near ground truth, replacing ad-hoc tuning.", "keywords": ["Implicit Neural Representations", "Bayesian Optimization", "Spectral Bias"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c6d9ec15bff988bc9a204bd8f5d34b998ae51ae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces OptiINR, a unified framework that replaces manual heuristics with principled global optimization for configuring Implicit Neural Representations (INRs). It uses Bayesian Optimization (BO) to jointly select discrete activation families and continuous initialization parameters across network layers. This approach overcomes the fundamental \"capacity-convergence gap\" inherent in conventional INR design. Experiments demonstrate OptiINR consistently discovers superior, globally optimal configurations, significantly outperforming hand-tuned baselines across 1D audio, 2D image, and 3D shape reconstruction tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. OptiINR is the first unified framework to formulate INR configuration as a mathematically rigorous global optimization problem solved efficiently by Bayesian Optimization.\n2. The method delivers globally optimal configurations that consistently achieve state-of-the-art results"}, "weaknesses": {"value": "1. Lack of comparison with:\n- SPDER: Semiperiodic Damping-Enabled Object Representation\n- FreSh: Frequency Shifting for Accelerated Neural Representation Learning\n\n2. In related works, authors should mention that 2D images can also be reconstructed by Gaussian Components\n- GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting\n- MiraGe: Editable 2D Images using Gaussian Splatting\n\nwhich gives higher scores.\n\n3. While Bayesian Optimization (BO) is sample-efficient relative to brute-force search. The paper should strengthen its efficiency claim by comparing OptiINR against recent, low-overhead hyperparameter tuning methods (e.g., FreSh) to justify its high training expense.\n\n4. The paper should contain some visualization of the model.\n\n5. The paper suffers from a lack of focus. The authors devote excessive space to general descriptions in the Background and Method sections. \n\n6. The paper's clarity, impact, and self-contained nature are hindered by placing essential material in the Appendix. Key experimental results and crucial theoretical underpinnings, including the Error Fields (Appendix B), the Spectral Analysis of Audio (Appendix C), should be moved to the main body of the manuscript.\n\n7. The paper's organization would be improved by using paragraphs instead of numerous fragmented subsections.\n\n8. The authors are encouraged to provide analysis or experiments demonstrating the framework's scalability and efficiency when applied to significantly larger and deeper INR architectures (such as those used in high-fidelity NeRF applications)."}, "questions": {"value": "1. Why were direct performance comparisons excluded against highly relevant, modern methods like SPDER and the efficient initializer FreSh?\n\n2. How does the method scale computationally when applied to significantly larger and deeper INR architectures common in high-fidelity tasks like NeRF?\n\n3. Why were crucial theoretical proofs and detailed experimental figures, essential for validation, relegated to the Appendix instead of the main body?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iNkkGUdHZQ", "forum": "RmdB3CLwfV", "replyto": "RmdB3CLwfV", "signatures": ["ICLR.cc/2026/Conference/Submission6492/Reviewer_Vq9w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6492/Reviewer_Vq9w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761166213068, "cdate": 1761166213068, "tmdate": 1762918865703, "mdate": 1762918865703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the improving the performance of existing implicit neural representations (INRs) by globally optimizing the configurations (e.g., activation functions, initialization strategies, layer-wise adaptation and so on). To achieve this, a Bayesian optimization framework is proposed, replacing previous manual heuristic-driven tuning with global configuration search. Experiments on 3 representation tasks verify the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed OptiINR is interesting and higher performancer is achieved compared with previous SOTAs."}, "weaknesses": {"value": "1. The experimental design in this paper has limitations that undermine the reliability of the results. Although the authors conducted three tasks, all of them are fundamental fitting/representation tasks (the authors conflate the concepts of \"representation\" and \"reconstruction\" throughout the paper, misclassifying some representation tasks as reconstruction). The authors failed to validate their approach through inverse problem solving, which is the core scenario where the value of INR truly shines. Specific inverse problems could include classical cases like NeRF or other challenging tasks (as discussed in recent INR surveys)."}, "questions": {"value": "1. Authors claim that \"OptiINR ... advancing the fundamental understanding of what constitutes an optimal implicit neural representation\"(line 298). However, I do not see any discussions on this issue.\n2. The results presented in the experiments are produced by applying the OptiINR to an INR library with different architectures and parameters. Could authors post the configurations produced by the OptiINR for different tasks?\n\nSome typos,\n1. Line 44, the FINER and FINER++ should be cited after the \"capacity-convergence gap\", since this gap is firstly observed and summarized by FINER and FINER++.\n2. line 125, the command for citing SIREN paper should be \"\\citep{}\" instead of \"\\cite{}\" used in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "396hmYo2fN", "forum": "RmdB3CLwfV", "replyto": "RmdB3CLwfV", "signatures": ["ICLR.cc/2026/Conference/Submission6492/Reviewer_kaVS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6492/Reviewer_kaVS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535688427, "cdate": 1761535688427, "tmdate": 1762918864946, "mdate": 1762918864946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes OptiINR, the first method for globally optimizing the configuration of Implicit Neural Representations (INRs). Instead of relying on heuristic or manual tuning for activations, initialization scales, and hyperparameters, OptiINR formulates the configuration process as a Bayesian optimization problem. The framework searches a mixed-variable space that includes categorical variables (activation family: SIREN, WIRE, FINER, Gauss, etc.) and continuous variables (frequency, scale, initialization parameters). By leveraging Bayesian optimization, OptiINR efficiently explores the joint space of discrete activation families—such as sinusoidal (SIREN), wavelet-based (WIRE), and variable-periodic (FINER)—and their associated continuous initialization parameters. The experiment results were performed in 1D, 2D and 3D datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea is quite novel. It transformed a problem into an optimization problem that avoids manual fine-tuning. The sampling method was efficient as it reduced the complexity from O(Sn^3) to O(n^3 + Sn^2)."}, "weaknesses": {"value": "[1] More computational resources analysis should be reported. For example, the training time will significantly increase if it is formulated as an optimization problem. In addition, the number of parameters and Gflop should be reported.\n\n[2] How does this method perform in PDE task?\n\n[3]An ablation study about some important parameters should be provided.\n\n[4]More explanation about why Theorem (Matheron’s Rule) is important and how it related to the framework.\n\n[5] More recent baselines should also be added. For example, LosslessINR [r1]. IGA [r2]\n\n[r1]Han, Woo Kyoung, et al. \"Towards Lossless Implicit Neural Representation via Bit Plane Decomposition.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n[r2] Shi, Kexuan, et al. \"Inductive Gradient Adjustment For Spectral Bias In Implicit Neural Representations.\" arXiv preprint arXiv:2410.13271 (2024)."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vsFzQyXVPe", "forum": "RmdB3CLwfV", "replyto": "RmdB3CLwfV", "signatures": ["ICLR.cc/2026/Conference/Submission6492/Reviewer_Tceo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6492/Reviewer_Tceo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955008856, "cdate": 1761955008856, "tmdate": 1762918864538, "mdate": 1762918864538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a Bayesian-based approach to choose the hyperparameters and configurations for Implicit Neural Representations. The proposed method jointly optimizes the network configurations and weights according to a given task. The presented results depict state-of-the-art results over common activation functions in INRs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Choosing correct hyperparameters for INRs is a cumbersome process which involves expensive grid searches. Formulating this step within a rigorous Bayesian framework is useful to the community. I also like the presentation of the paper which involves self contained concepts required to understand the method."}, "weaknesses": {"value": "As far as I am aware, the presented method just reuses well studied Bayesian-based hyperparameter sweep methods in the context of INRs. This limits the novelty of the paper. However, I am leaning towards acceptance since this reformulation is useful to the community in my opinion. \n\nThe proposed method (despite being efficient compared to a grid search) is expensive. Therefore it might not be useful in time consuming tasks such as NeRFs. The authors have also not included any NeRF experiments, which makes its applicability in one of the most popular tasks of INRs doubtful."}, "questions": {"value": "Can you provide any metrics to showcase how efficient is the proposed method against a grid search?\n\nCan you provide time durations taken for each task in finding the optimal configurations?\n\nCan you provide any NeRF experiments with the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5UKCm3F9Pb", "forum": "RmdB3CLwfV", "replyto": "RmdB3CLwfV", "signatures": ["ICLR.cc/2026/Conference/Submission6492/Reviewer_51xP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6492/Reviewer_51xP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762078564034, "cdate": 1762078564034, "tmdate": 1762918864281, "mdate": 1762918864281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}