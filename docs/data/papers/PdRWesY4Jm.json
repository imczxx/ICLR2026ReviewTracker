{"id": "PdRWesY4Jm", "number": 1188, "cdate": 1756862029635, "mdate": 1759898222744, "content": {"title": "Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding", "abstract": "Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability.\nWe introduce **Lemon**, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. \nUnlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling.\nTo handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning.\n**Lemon** establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.", "tldr": "A unified multimodal model for universal 3D spatial reasoning and understanding", "keywords": ["3D Uderstanding", "Multimodal Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1b919d3f566c9adb880c3bba5ec3688aefb23051.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LEMON, a 3D large language model  that unifies various 3D understanding tasks within a transformer-based architecture. The authors demonstrate that a dedicated 3D encoder is unnecessary for point cloud representation and instead propose a simple yet effective tokenization method. Built upon this design, LEMON, trained through a curated three-stage curriculum, achieves state-of-the-art performance on multiple 3D understanding and captioning benchmarks while exhibiting promising scalability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a strong 3D-LLM with a simple but effective point cloud tokenization strategy.\n\n2. It provides evidence that conventional 3D encoders may limit general 3D understanding performance.\n\n3. The proposed LEMON model achieves state-of-the-art results across diverse 3D understanding benchmarks under the presented training pipeline."}, "weaknesses": {"value": "1. **Potential unfair comparison with baselines:** Most baseline models are trained on smaller or different datasets. A controlled fine-tuning experiment (e.g., on ScanQA and SQA in Stage-3) is recommended to isolate the effect of training data size.\n\n2. **Limited evaluations:** The evaluation on 3D-GRAND is primarily used to assess hallucination in 3D-LLMs rather than to rigorously examine their 3D spatial understanding ability. The details of the *100 challenging 3D spatial QA* set (Line 264) are not provided. How is this test set constructed, and are the results stable across runs? Moreover, [MSQA - NeurIPS 2024](https://arxiv.org/abs/2409.02389) and [Beacon3D - CVPR 2025](https://arxiv.org/abs/2503.22420) are widely used 3D understanding benchmarks, yet the paper does not include their results. Are zero-shot results on these benchmarks available?\n\n3. **Limited ablation of 3D encoders:** While the paper argues that pretrained 3D encoders are unnecessary, it only ablates PointNet++. More recent encoders, such as ReCon++ (in ShapeLLM) and PointBERT (in PointLLM), should also be examined.\n\n4. **Evaluation bias:** Many metrics rely on GPT-based judgments, which may introduce bias. Additional objective language metrics, such as CIDEr, ROGUE-L, BLEU-1-4, EM, and Refined-EM, are recommended to strengthen evaluation reliability."}, "questions": {"value": "1. How does the model preserve object-level captioning ability after Stage-3 fine-tuning on scene-level datasets? Has the effect of Stage-3 fine-tuning been measured on the benchmarks reported in Table 2?\n\n2. Since 3D-GRAND provides grounded 3D annotations, were these annotations removed during fine-tuning, given that LEMON does not include a grounding mechanism?\n\n3. The paper presents case studies on simulated point clouds. Are additional results available for real-world scanned datasets such as ScanNet?\n\n4. Do the special tokens contribute significantly to 3D positional encoding? Are there corresponding ablation studies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4SCkqyHzVX", "forum": "PdRWesY4Jm", "replyto": "PdRWesY4Jm", "signatures": ["ICLR.cc/2026/Conference/Submission1188/Reviewer_ixqM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1188/Reviewer_ixqM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761216291201, "cdate": 1761216291201, "tmdate": 1762915701174, "mdate": 1762915701174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a scalable 3D Large Multimodal Model (Lemon) for universal spatial scene understanding, which jointly processes 3D point cloud patches and language tokens within a unified sequence. The authors evaluate the model on object recognition, captioning, and question answering tasks to demonstrate its performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Lemon does not rely on a pre-trained 3D encoder but instead develops a structured patchification and tokenization scheme, which is an interesting design choice. However, the authors do not provide an ablation study to justify the effectiveness of this component.\n2. The proposed three-stage training curriculum progressively builds the model’s capability from object-level to scene-level understanding."}, "weaknesses": {"value": "1. The paper’s main claimed contribution is the unified architecture for 3D point clouds and language. However, in my view, prior works such as **PointLLM** have already adopted a unified token-based framework for fine-tuning LLMs using both 3D point tokens and text tokens. Lemon mainly replaces the 3D encoder with FPS sampling and a linear projector, without introducing substantial architectural novelty. Therefore, I remain skeptical about the level of innovation.\n2. The discussion of related work is insufficient. Several recent and more advanced 3D multimodal LLMs—such as **Inst3D-LMM (CVPR 2025)** and **Video-3D LLM (CVPR 2025)**—are not discussed or compared. A more comprehensive review and comparison would strengthen the paper.\n3. Additional experiments on more benchmarks (e.g., **ScanRefer** and **Multi3DRefer** for 3D visual grounding, as well as 3D detection tasks) are needed to further validate Lemon’s performance.\n4. In **Figure 2**, it is unclear why both *“point patch tokens”* and *“3D patch embeddings”* are presented simultaneously—aren’t they essentially the same representation?"}, "questions": {"value": "Lemon’s main advantage should be computational efficiency compared to other 3D LMMs. However, as shown in **Table 7**, the inference time difference appears marginal, and other 3D LMMs also achieve very short runtimes. This raises doubts about whether removing the 3D encoder is truly necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1lUtaBt8S7", "forum": "PdRWesY4Jm", "replyto": "PdRWesY4Jm", "signatures": ["ICLR.cc/2026/Conference/Submission1188/Reviewer_6FmE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1188/Reviewer_6FmE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761480735310, "cdate": 1761480735310, "tmdate": 1762915701051, "mdate": 1762915701051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes LEMON, a decoder-only 3D large multimodal model (3D-LMM) that performs 3D point-cloud tokenization via patching and adopts Qwen2.5-7B for a three-stage training pipeline. The model is evaluated primarily on Spatial Scene QA, Embodied QA, and 3D Captioning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unlike encoder-based methods, this paper explores an **encoder-free paradigm**, representing a novel design direction.\n2. The **3D patch tokenization** strategy is compelling and may flexibly support both scene-level and object-level 3D representation."}, "weaknesses": {"value": "1. It remains unclear how performance would change if the tokenization method were not patch-based, but instead adopted a **hierarchical downsampling scheme** similar to PointBERT.\n2. The comparison setup may not be fully fair: although re-finetuning balances the training distribution, the **training data size and LLM base models differ** across methods. It would be more convincing to evaluate with **Vicuna-1.1 and PointLLM data/training stages** to demonstrate encoder-free potential.\n3. The baseline comparison lacks **recent SOTA models**, such as **MiniGPT-3D**.\n4. **Scene-level validation** needs strengthening: if 3D patchification is designed for scene tasks, experiments on **ScanQA, Scan2Cap**, or similar datasets are recommended.\n5. For **object-level generalization**, results on datasets **beyond Objaverse** would help verify robustness and transferability.\n6. Please report efficiency metrics as well."}, "questions": {"value": "1. **Alternative Tokenization Strategy Evaluation:**\n   The current model design relies exclusively on patch-based tokenization for 3D inputs. However, it remains unclear how the model would perform under alternative tokenization strategies, particularly those based on **hierarchical downsampling**, as adopted in PointBERT and related approaches. A controlled comparison between patchification and hierarchical sampling would help clarify whether the proposed tokenization mechanism is optimal for capturing multi-scale geometric information and whether the choice of tokenization architecture materially affects downstream performance.\n\n2. **Fairness of Comparative Evaluation:**\n   Although the paper reports re-finetuning of baseline models to mitigate training distribution inconsistencies, the comparative evaluation may still lack fairness due to differences in **training dataset size, composition, and LLM base architectures** across methods. To more rigorously validate the encoder-free paradigm, it would be beneficial to include experiments where LEMON is trained under identical conditions to representative baselines (e.g., **Vicuna-1.1 and PointLLM training setups**), including both data sources and training curricula. Such controlled comparisons would strengthen claims regarding architectural superiority rather than advantages stemming from data or base model scale.\n\n3. **Missing Recent SOTA Baselines:**\n   The benchmark suite does not currently include several **recent state-of-the-art 3D LMMs**, notably **MiniGPT-3D** and other contemporary models in the same category. Incorporating these models into the comparative analysis—particularly on shared evaluation tasks—would provide a clearer picture of the proposed method’s competitiveness relative to the latest advances in the field.\n\n4. **Insufficient Scene-Level Evaluation:**\n   Given that the proposed 3D patchification mechanism is intended to support both object-level and scene-level reasoning, the evaluation on complex 3D scene understanding remains relatively limited. To substantiate claims regarding scene-level capability, it would be valuable to include experiments on established scene-centric benchmarks such as **ScanQA, Scan2Cap**, or comparable datasets. These results would help verify whether the unified tokenization and decoder-only architecture scale effectively to real-world scene understanding scenarios.\n\n5. **Generalization Beyond Objaverse at Object Level:**\n   The majority of object-level evaluations rely on Objaverse, which may introduce dataset-specific bias. To assess generalization capability more rigorously, it is recommended to evaluate the model on **additional object-level datasets** beyond Objaverse. Examples include ShapeNet-based benchmarks, Objaverse-XL subsets, or other publicly available 3D asset collections. Demonstrating consistent performance across multiple datasets would strengthen the claim that the proposed model generalizes well to diverse object domains.\n\n6. **Efficiency and Resource Metrics Reporting:**\n   In addition to accuracy-based evaluations, the paper would benefit from reporting **efficiency metrics**, such as computational cost, inference speed, memory footprint, and throughput. Given the unified transformer design and removal of modality-specific encoders, such metrics are essential to illustrate the proposed method’s practical efficiency advantages (or trade-offs) relative to hybrid encoder architectures. Highlighting these aspects would provide a more comprehensive understanding of the system’s real-world applicability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YjmQbbw7vf", "forum": "PdRWesY4Jm", "replyto": "PdRWesY4Jm", "signatures": ["ICLR.cc/2026/Conference/Submission1188/Reviewer_3DHG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1188/Reviewer_3DHG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792009020, "cdate": 1761792009020, "tmdate": 1762915700910, "mdate": 1762915700910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Lemon, a single transformer that ingests point-cloud “patch tokens” and text tokens in one sequence (via a linear projector + special separator tokens), claiming to remove modality-specific encoders and achieve strong results on 3D captioning, recognition, and scene-level spatial QA, with a three-stage curriculum (obj-recog → object captioning/grounding → scene QA). Ablations include scaling curves and a PointNet++ insertion that reportedly hurts performance"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "esults across object/scene tasks and qualitative comparisons vs 2D VLMs (e.g., GPT-4V) are extensive; Lemon-7B appears competitive on several benchmarks."}, "weaknesses": {"value": "The paper contrasts Lemon with “non-transformer” encoders, but PointLLM and ShapeLLM are also transformer-based. The real distinction is removing the separate 3D encoder block in favor of early fusion inside one transformer—this needs to be framed clearly and compared fairly.\n\nLemon still performs hierarchical geometric patchification (splits/FPS/standardization) plus a projector—effectively a tokenizer with hierarchy. Without compute/param attribution, the claim of eliminating redundant encoders reads like moving capacity rather than removing it.\n\nNo per-module FLOPs/params/latency (patchification, projector, transformer, token counts). No scaling curves (tokens ↔ accuracy/latency). Deployability and efficiency claims remain unsubstantiated.\n\nBaselines differ in LLM family/size, data volume, and token budgets. We need same-LLM, same data, same token budget, compute-matched comparisons to isolate architectural benefits."}, "questions": {"value": "To justify the \"universal\" claim, the model must be evaluated on tasks that require non-textual spatial output. Add experiments on a standard 3D visual grounding or 3D object detection benchmark (eg, ScanRefer or ScanNet detection) to demonstrate the model can precisely localize its understanding.\n\nCompute & latency table (per module): patchification (incl. FPS), projector, transformer forward, total tokens, memory footprint—report FLOPs, params, p50/p90 latency on 8×H100 and single-GPU.\n\n\nPatchification alternatives: swap Z→Y→X with SFC (Hilbert/Z-order), and test learned partitioners; show effect on spatial QA and captioning\n\nPointNet++ ablation extension: frozen vs fine-tuned; xyz vs xyz+rgb; transformer-encoder baseline; match parameter/FLOP budgets to Lemon’s front end"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "55QOqKMtyC", "forum": "PdRWesY4Jm", "replyto": "PdRWesY4Jm", "signatures": ["ICLR.cc/2026/Conference/Submission1188/Reviewer_RZ2B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1188/Reviewer_RZ2B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930063697, "cdate": 1761930063697, "tmdate": 1762915700548, "mdate": 1762915700548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}