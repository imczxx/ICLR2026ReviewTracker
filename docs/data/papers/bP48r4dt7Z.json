{"id": "bP48r4dt7Z", "number": 23503, "cdate": 1758344671327, "mdate": 1759896811590, "content": {"title": "DevOps-Gym: Benchmarking AI Agents in Software DevOps Cycle", "abstract": "Even though demonstrating extraordinary capabilities in code generation and software issue resolving, AI agents' capabilities in the full software DevOps cycle are still unknown.\nDifferent from pure code generation, handling the DevOps cycle in real-world software, including developing, deploying, and managing, requires analyzing large-scale projects, understanding dynamic program behaviors, leveraging domain-specific tools, and making sequential decisions. \nHowever, existing benchmarks focus on isolated problems and lack environments and tool interfaces for DevOps.\nWe introduce DevOps-Gym, the first end-to-end benchmark for evaluating AI agents across core DevOps workflows: build and configuration, monitoring, issue resolving, and test generation. \nDevOps-Gym includes 698 real-world tasks collected from 30+ projects in Java and Go.\nWe develop a semi-automated data collection mechanism with rigorous and non-trivial expert efforts in ensuring the task coverage and quality.\nOur evaluation of state-of-the-art models and agents reveals fundamental limitations: they struggle with issue resolving and test generation in Java and Go, and remain unable to handle new tasks such as monitoring and build and configuration.\nThese results highlight the need for essential research in automating the full DevOps cycle with AI agents.", "tldr": "", "keywords": ["Benchmark", "Agent", "Tool Call"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da4663f34dbe738907b15481e8d9786ff7393c8d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a benchmark for evaluating SWE Agents on four different activities frequently encountered in the software development cycle: build/configuration, monitoring, bug fixing and test generation. They source and vet tasks for Java and Go and evaluate across popular coding agents and models. Their analysis shows that current systems fall short on configuration and monitoring tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a novel perspective to evaluating coding agents. The tasks presented in the paper go beyond popular bug-fixing setting and are sourced from Java and Go projects as opposed to the popular python setting. Especially the monitoring tasks which requires the system to interact with the environment over time. The difference in performance between agents also indicates the tasks to be a good discriminator. Evaluation with this benchmark maybe useful to practitioners working on industrial codebases that require maintenance beyond fixing bugs."}, "weaknesses": {"value": "1. Two of the four tasks are not novel: bug-fixing and test generation are well explored in existing literature.\n\n2. While the paper presents useful contributions to practitioners who may want to evaluate SWE-agents for DevOps use cases, the paper offers limited insight into the behaviour of current agent frameworks or models, limiting usefulness to agent and model developers and hence relevance to this venue."}, "questions": {"value": "1. What are guards against agent finding trivial ways to resolve tasks. For example in the build/configuration task, what if the agent modifies the build scripts to pass trivially? \n\n2. How is evaluation performed for the monitoring task. The paper mentions that the LLM has to identify the type of anomaly? How is the response from the LLM evaluated to determine correctness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bq7r1dMyxO", "forum": "bP48r4dt7Z", "replyto": "bP48r4dt7Z", "signatures": ["ICLR.cc/2026/Conference/Submission23503/Reviewer_jCaQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23503/Reviewer_jCaQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780454848, "cdate": 1761780454848, "tmdate": 1762942687676, "mdate": 1762942687676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a benchmark, DevOps-gym, for evaluating agents on end-to-end DevOps workflows. The authors curate a benchmark of nearly 700 tasks from 30+ repositories in Java and Go languages. The main contribution of the paper is in covering the entire spectrum of DevOps operations - build, monitor, fix and test. The benchmark comes with pre-set runtime environments and tools. The experimental results on a few agents and multiple LLMs show that they succeed to varying degrees across the task categories, and generally well-below the success rate on popular benchmarks like swe-bench."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper makes a comprehensive contribution, covering all phases of DevOps operations. The benchmark is based on real-world repositories and targets languages common in industry like Java and Go (compared to benchmarks that target Python). The experimental setup is well-designed.\n- The paper is written very nicely. The key aspects of design of the benchmark are explained properly, showing both the diversity of sub-tasks within each task categories, and clearly specifying input, output and evaluation metrics. The analysis of results is also insightful.\n- The experimental evaluation is supportive of the value of this benchmark, by demonstrating that the current agents and LLMs are not very successful on this benchmark and their performance varies across task categories."}, "weaknesses": {"value": "- The benchmark curation involved synthetic data generation where faults are injected by experts. The paper claims that they are inspired by real-world scenarios, but the adequate details are not provided.\n- The paper uses OpenHands and mini-SWE-Agent as harnesses. Though they may allow access to the terminal, their tools are primarily designed for assisting agents in GitHub issue resolution. No effort is made to design an agentic harness that specifically aids the agent in DevOps tasks.\n- It is well-known that test-time scaling can improve the performance of agents substantially. The paper does not conduct scaling experiments.\n- The benchmark does not cover the CI/CD aspects of DevOps tasks and does not throw light on how these agents could operate in CI/CD pipelines with platform-specific tools, e.g., GitHub actions.\n- Typo: interpreting their returns"}, "questions": {"value": "- How many tasks out of 698 are real and how many are synthetic? What is the distribution by problem types (e.g., memory leaks, build failure, etc.) between real vs synthetic tasks? What is the reason those cannot be found naturally in the wild if your task distribution is to capture the real-world distribution?\n- How would the performance of the LLMs change if DevOps-specific agent harness is prepared and made available to them?\n- How does the performance of the agents change with test-time scaling, e.g., sampling multiple trajectories?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xGsIOm55mM", "forum": "bP48r4dt7Z", "replyto": "bP48r4dt7Z", "signatures": ["ICLR.cc/2026/Conference/Submission23503/Reviewer_NdRZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23503/Reviewer_NdRZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879368172, "cdate": 1761879368172, "tmdate": 1762942687149, "mdate": 1762942687149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DevOps-Gym, a benchmark for evaluating AI agents across the full DevOps cycle, covering build & configuration, monitoring, issue resolving, and test generation. The benchmark includes 698 tasks from 30+ real-world Java and Go repositories. Evaluation of state-of-the-art agents reveals poor performance across all stages, with particularly severe failures on monitoring (0-23.33%) and build tasks (12.5-58.33%). While this addresses a genuine gap in existing benchmarks, the execution has significant methodological weaknesses that limit the contribution's impact. There are some concerns outlined below."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Addresses Real Gap:** First benchmark attempting end-to-end DevOps evaluation\n2. **Realistic Task Design:** Docker environments with real tools better simulate practice than synthetic environments, and covering diverse DevOps stages (though incomplete)\n3. **Extensive Manual Effort:** Authors invested significant time in task construction and validation\n4. **Negative Results Are Valuable:** Showing current agents fail badly on DevOps tasks is an important finding\n5. **Practical Relevance:** High potential impact for industrial applications"}, "weaknesses": {"value": "1. **No statistical rigor**: Missing confidence intervals, significance tests, multiple runs; single-run results unreliable given tiny task counts.\n2. **Small dataset**: Only 30 monitoring and 48 build tasks, which is far smaller than benchmarks like SWE-bench, so this limits generalizability.\n3. **Limited analysis**: The paper reports binary accuracy for monitoring, but there is no partial credit or false-positive analysis provided.\n4. **Limited baselines**: Excludes major frameworks (Devin, Aider, etc.), so it's unclear if the results are significantly better than SOTA."}, "questions": {"value": "1. **Statistical Rigor:** Why no confidence intervals, multiple runs, or significance tests? This is standard practice for benchmark papers. With small sample sizes (30 monitoring, 48 build tasks), single-run results are highly unreliable and prevent drawing meaningful conclusions about agent capabilities.\n\n2. **Synthetic Task Validation:** Do you have some evidence that expert-injected bugs match real-world distributions? The work relies heavily on synthetic tasks for monitoring and build stages but doesn't provide validation that these reflect actual production issues. Without this validation, it seems incorrect to claim the benchmark represents \"real-world\" DevOps challenges?\n\n3. **Ground Truth Agreement:** It might be good to include inter-annotator agreement scores. The paper mentions \"three senior DevOps engineers independently validate\" monitoring tasks and use multiple experts for other stages, but report on agreement metrics (Cohen's kappa, etc.). If experts disagree, then the validity of the canonical answer is questionable, which limits the benchmark credibility.\n\n4. **Contamination Threshold:** Why allow up to 20% contamination instead of requiring something much smaller? The paper talks about decontamination procedures but then also includes repositories with substantial contamination. The standpoint is a bit confusing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SuVvo52HRG", "forum": "bP48r4dt7Z", "replyto": "bP48r4dt7Z", "signatures": ["ICLR.cc/2026/Conference/Submission23503/Reviewer_EL6m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23503/Reviewer_EL6m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886966432, "cdate": 1761886966432, "tmdate": 1762942686821, "mdate": 1762942686821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DEVOPS-GYM, a benchmark created to test how well AI agents can handle the full software DevOps cycle, including building and configuration, monitoring, bug fixing, and test generation. It goes beyond previous benchmarks that focus only on coding or bug fixing by including real DevOps tasks that require tool use, environment setup, and step-by-step decision making. The dataset contains 698 tasks collected from over 30 open-source projects in Java and Go, combining real GitHub issues with tasks designed by experts to cover common DevOps problems.\n\nThe benchmark was built with attention to realism and fairness. The authors manually recreated build environments, monitored runtime behavior, and avoided data contamination from existing model training data. Each task type evaluates a different part of the DevOps process: building tasks test if the agent can fix dependency or build errors, monitoring tasks test detection of issues like memory leaks or CPU spikes, issue-resolving tasks test whether agents can produce correct patches, and test-generation tasks check if agents can create regression tests that fail on buggy code and pass after the fix.\n\nResults show that even strong AI agents and large models still perform poorly on DevOps tasks. The best agent achieved around 58% accuracy in build and configuration, but less than 25% in monitoring, issue fixing, and test generation. The study finds that current systems struggle to plan multiple steps, use DevOps tools properly, and reason about dynamic system states. It concludes that existing models are far from being able to automate the entire DevOps cycle."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s main strength lies in its clear motivation and solid contribution to an underexplored area. It identifies that while many existing benchmarks test AI on programming or debugging tasks, none assess performance across the complete DevOps pipeline, which involves building, monitoring, fixing, and testing software systems. By defining this broader scope, the authors push evaluation toward real-world DevOps scenarios that require understanding of environments, tool interaction, and sequential task execution. This is a meaningful extension beyond static code generation benchmarks and provides a realistic testbed for future research.\n\nAnother strong point is the benchmark’s detailed and careful construction. It covers nearly seven hundred real-world tasks from more than thirty Java and Go projects, including both real GitHub issues and synthetic cases designed by experts to represent frequent DevOps problems. Each task is clearly defined with inputs, expected outputs, and evaluation criteria. The inclusion of four DevOps stages—build/configuration, monitoring, issue resolving, and test generation—ensures comprehensive coverage. The benchmark also integrates command-line tools commonly used in DevOps such as Maven, Gradle, JUnit, and iostat, allowing agents to operate in realistic conditions instead of simulated text environments.\n\nThe authors use contamination checks to prevent overlap with training data, manually reconstruct real-world project environments, and validate that each task is reproducible. They also test multiple well-known agent frameworks and models, providing a broad and transparent performance comparison. Their analysis goes beyond simple accuracy reporting; they identify specific failure patterns such as agents’ difficulty with tool usage, temporal reasoning, and multi-step planning. This combination of realism, technical rigor, and analytical depth makes the benchmark a valuable and trustworthy foundation for studying AI in software DevOps automation."}, "weaknesses": {"value": "The paper, while ambitious and valuable, has several weaknesses that limit its overall contribution.\n\nFirst, the scope is too broad relative to the experimental depth. Although it claims to evaluate the entire DevOps cycle, the number of tasks for some categories—such as monitoring or build/configuration—is relatively small compared to the hundreds of issue-resolving and test-generation cases. This uneven distribution makes the evaluation appear unbalanced, and it is unclear whether the benchmark fully captures the complexity of real-world DevOps workflows. Some parts, like deployment or CI/CD automation, are also missing, which weakens the claim of “end-to-end” coverage.\n\nSecond, the dataset construction process is heavily manual and lacks scalability. The paper mentions that each task can take over ten hours of expert work to reproduce and verify, suggesting that expanding the dataset or maintaining it over time will be difficult. The strong reliance on expert labor reduces the reproducibility and accessibility of the benchmark. In addition, while the authors describe contamination checks, they provide limited quantitative details or validation of how effective these checks are. Without transparency on contamination metrics or repository overlap, it’s hard to judge the fairness of the evaluation.\n\nThird, the evaluation analysis is somewhat surface-level. While the results show that agents perform poorly, the discussion of why these failures occur remains descriptive rather than diagnostic. For example, the paper reports that agents fail in tool usage or temporal reasoning but does not analyze whether this is due to prompt design, insufficient model context, or poor environment instrumentation. There is also limited analysis of error types or task difficulty variations. The work lacks ablation or controlled experiments that could explain which aspects of the benchmark are truly challenging."}, "questions": {"value": "1) The benchmark includes 698 tasks from 30+ Java and Go projects, but DevOps typically spans far more diverse environments and languages. How confident are the authors that these two languages and this limited set of projects represent the broader DevOps space, including cloud-native and containerized systems?\n2) The paper claims to prevent contamination using prefix-completion tests, but no quantitative contamination rates are reported. Could the authors provide clearer evidence or benchmarks for how effective this process was across repositories?\n3) The benchmark covers four stages but omits key aspects of DevOps such as deployment, CI/CD automation, rollback, and infrastructure management. Do the authors plan to extend to these in future work, or do they believe these tasks require fundamentally different agent capabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8tysegM8Ff", "forum": "bP48r4dt7Z", "replyto": "bP48r4dt7Z", "signatures": ["ICLR.cc/2026/Conference/Submission23503/Reviewer_uo9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23503/Reviewer_uo9F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140661098, "cdate": 1762140661098, "tmdate": 1762942686545, "mdate": 1762942686545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}