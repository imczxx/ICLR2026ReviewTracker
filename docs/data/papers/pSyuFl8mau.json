{"id": "pSyuFl8mau", "number": 7598, "cdate": 1758028849183, "mdate": 1762941258512, "content": {"title": "MultiModalPFN: Extending Prior-Data Fitted Networks for Multimodal Tabular Learning", "abstract": "Recently, TabPFN has gained attention as a foundation model for tabular data. However, it struggles to integrate heterogeneous modalities such as images and text, which are common in domains like healthcare and marketing, thereby limiting its applicability. To address this, we present the Multi-Modal Prior-data Fitted Network (MMPFN), which extends TabPFN to handle tabular and non-tabular modalities in a unified manner. MMPFN comprises per-modality encoders, modality projectors, and pre-trained foundation models. The modality projectors serve as the critical bridge, transforming non-tabular embeddings into tabular-compatible tokens for unified processing. To this end, we introduce a multi-head gated MLP and a cross-attention sampler that extract richer context from non-tabular inputs while mitigates attention imbalance issue in multimodal learning. Extensive experiments on medical and general-purpose multimodal datasets demonstrate that MMPFN consistently outperforms competitive state-of-the-art methods and effectively exploits non-tabular modalities alongside tabular features. These results highlight the promise of extending prior-data fitted networks to the multimodal setting, offering a scalable and effective framework for heterogeneous data learning.", "tldr": "multi-modal tabular deep learning with pretrained foundation models", "keywords": ["multimodal", "tabular", "deep", "learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9b8c6efa3ee7c22acc543f987c3acb2c4623cf2a.pdf", "supplementary_material": "/attachment/0e7d666153cd65022c373f182cd91ef1250297d3.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes MMPFN, a multimodal prior-data fitted network that handles tabular and non-tabular modalities in a unified manners to address modality heterogeneity. This model includes a multi-head gated MLP (MGM) that aims to address overcompressed non-tabular embeddings and a cross-attention sampler (CAS) that aims to mitigate the attention imbalance issue. \n\nThe proposed  model is evaluated on image-tabular and text-tabular datasets and compared against prior multimodal approaches."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* S1: This paper explores image-tabular representation learning, which is of interest to the community. \n\n* S2: The insight of attention imbalance and the experiment of using different number of image features is interesting."}, "weaknesses": {"value": "**Major Weakness**\n\n* W1: Methodology details for MGM and CAS are not efficient. Providing some semantic illustrations or necessary equations could help readers to understand.\n\n* W2: The novelty of this paper appear to be limited. The proposed method seems to be a straightforward adaptation of PFN to the multimodal domain. Gated MLP and cross-attention sampler have been studied in prior works [R1]. \n\n* W3:  The attention imbalance analysis is superficial. Assuming similar expected attention weights across modalities is overly simplistic. actually they can very different (Figure 3 shown in the paper). Furthermore, Figure 3 suggests notable differences. Furthermore, Equation 3 implies balancing token numbers could mitigate imbalance, yet Figure 2 shows the best results with 8 image features, this inconsistency is not yet discussed.\n\n* W4: To address token over-compression, it is unclear why the authors use gated MLPs to expand the [CLS] token instead of leveraging non-[CLS] tokens in the embedding sequence. \n\n* W5: The experimental settings of comparing methods are unclear. TIP and MMCL use ResNet-50 as the image encoder but MMPFN uses ViT-B. In addition, TIP and MMCL are SSL pre-training strategies. Do the authors conduct pre-training from TIP and MMCL before comparing with MMPFN? It is doubtable whether the performance gain comes from a unfair comparison. \n\n* W6: The paper’s presentation needs refinement. Structural organization, clarity, and proofreading (e.g., “while mitigating” in line 21) all require attention.\n\n[R1] Li, Junnan, et al. \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\" ICML 2023."}, "questions": {"value": "**Primary  Questions/Suggestions**\n\n* QS1: What distinguishes MMPFN from prior multimodal PFN adaptations? Please clarify the methodological novelty.\n\n* QS2: Could the authors provide more detailed descriptions (and possibly equations) for MGM and CAS?\n\n* QS3: How do you ensure fairness in comparisons?\n\n* QS4: Improving the presentation of the paper could make it better"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rFCBrarSTP", "forum": "pSyuFl8mau", "replyto": "pSyuFl8mau", "signatures": ["ICLR.cc/2026/Conference/Submission7598/Reviewer_JCqE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7598/Reviewer_JCqE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760979462872, "cdate": 1760979462872, "tmdate": 1762919679173, "mdate": 1762919679173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We respectfully request to withdraw our submission from consideration. Upon further evaluation, we identified aspects of the work that require revision to ensure the quality of our result. We appreciate your understanding and thank the reviewers and area chairs for their time and effort."}}, "id": "SpSXMtnAnm", "forum": "pSyuFl8mau", "replyto": "pSyuFl8mau", "signatures": ["ICLR.cc/2026/Conference/Submission7598/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7598/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762938336729, "cdate": 1762938336729, "tmdate": 1762938336729, "mdate": 1762938336729, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MMPFN (multimoda prior data fitted network) that extends TabPFN to handle multimodal inputs (e.g., tabular + image). The authors basically use modality-specific encoders and project non-tabular embeddings to tabular embedding space and feed all the featuers to TabPFN backbone for inference. The projector relies on multi-head gated MLP (expands cls token into multiple tokens) and cross-attention sampler to avoid token-count induced attention imbalance. The authors showed on a few medical and general multimodal benchmarks that their proposed architecure outperforms baselines while requiring a lot less finetuning compute."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- the motivation for the proposed designs seems to be clear \n- the evaluation is pretty broad, e.g., covering medical and general datasets and the performance gain seems to be consistently better.\n- the paper is relatively simple to read/parse!"}, "weaknesses": {"value": "- the main issue with the paper is that the proposed architecture changes are pretty ad-hoc. given the specificity of the task (e.g., image/text + tabular dataset), and the added complexity of the proposed architecture, it's arguable whether this contribution has lots of impact or meaningfully progress even this specific direction of improved multi-modal tabular model.\n- the main result of performance comparison to baselines in Table 1 and 2 can be strengthened if the authors can be more explicit about the encoders / components used in these baselines and/or dataset used for pretraining/finetuning. Because without apple-to-apple comparison, we are not sure if the gain is from the dataset, the modeling, or the scale of the model, etc. \n- Comparing TabPFN vs. MMPFN, it seems that the performance gain from (1) architecture improvement (2) relying on image information as well gives pretty modest gains. It would be nice to know relative proportion of contribution to performance gain from architecture change vs. adding image modality.\n- can author explain the reasoning that finetuning is done with batch size of 1 with max training step of 100? does it mean the mdoel is finetuned on 100 examples only. It would be useful/informative if the authors could provide some reasoning on this choice and some ablations on varying batch size and fintuning step's affect on performance so that we have a better idea the sensitivity of the proposed approach to these very important hyperparameters."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JAxXc2ni32", "forum": "pSyuFl8mau", "replyto": "pSyuFl8mau", "signatures": ["ICLR.cc/2026/Conference/Submission7598/Reviewer_YaBg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7598/Reviewer_YaBg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886790409, "cdate": 1761886790409, "tmdate": 1762919678744, "mdate": 1762919678744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Multi-Modal Prior-data Fitted Network (MMPFN), a framework that extends the TabPFN model to handle datasets containing a mix of tabular, image, and text data.\nThe architecture uses separate pre-trained encoders for each modality and uses a trainable modality projector to transform the text and image features. In this way, the newly added image and text can be integrated into the whole model for downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The motivation of this paper is clear: adding new modalities to tabular networks. \n\nThis paper is well-written and easy to follow."}, "weaknesses": {"value": "This paper chooses a Q-former (Li et al. 2023) projector to convert the image and test features. Why choose the Q-former to transform vision features with Llava (Liu et al. 2023) have demonstrated that MLP has better convergence and data efficiency than Q-former.  I think the justification for this design is not sufficient. \n\nThe performance of the proposed method is not sufficient; it would be better to show other advantages of this method against the baselines.  On some sub-tasks, the old baselines showcase an obvious performance advantage, and the proposed method is a data-friendly framework, so why does the performance lag behind the baselines?\n\nThe technical contribution of this paper is not significant the major module shares a similar design with Q-former."}, "questions": {"value": "Will finetuning TabPFN cause the performance drop on single modality tasks? Can lora obtain better performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XBf6weJo7Y", "forum": "pSyuFl8mau", "replyto": "pSyuFl8mau", "signatures": ["ICLR.cc/2026/Conference/Submission7598/Reviewer_vrdT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7598/Reviewer_vrdT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015246585, "cdate": 1762015246585, "tmdate": 1762919678386, "mdate": 1762919678386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}