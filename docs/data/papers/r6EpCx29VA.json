{"id": "r6EpCx29VA", "number": 12487, "cdate": 1758208142369, "mdate": 1763720497298, "content": {"title": "LLEOT: A Privacy-Enhancing Offsite Tuning Framework via Loss Landscape Elevation", "abstract": "Adapting large language models (LLMs) to domain-specific tasks via fine-tuning is often infeasible: model parameters are protected by intellectual property, while sensitive data cannot be shared due to privacy regulations. Offsite Tuning addresses this by training adapters on emulators of the original model, but current emulators retain substantial inference ability, exposing model capability privacy and risking misuse. We propose Loss Landscape Elevation Offsite Tuning (LLEOT), a framework that secures both data and model capability privacy. Its core component, Loss Landscape Elevation (LLE), enforces a fixed loss margin between emulator and model, which we theoretically show (Theorem 1) simultaneously (i) degrades emulator inference through perplexity amplification and (ii) preserves gradient alignment, ensuring consistent convergence of prompt optimization. Combined with Collaborative Prompt Knowledge Distillation (CPKD), our method enables adapters trained on emulators to transfer effectively to the original model. Extensive experiments on the OpenBookQA, SocialIQA, ARC-Challenge, and WebQuestions datasets confirm LLEOT achieves strong adaptation while mitigating emulator misuse.", "tldr": "", "keywords": ["Language models", "Model privacy", "privacy-preserving transfer learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f2f213c89c8cbe1d52df6f88c8fa2ba6068a3c19.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes LLEOT a framework that secures both data and model capability privacy.  Its core component, Loss Landscape Elevation (LLE), enforces a fixed loss margin between an emulator of the system and the original model.  The authors theoretically show that LLE (i) degrades emulator inference through perplexity amplification and (ii) preserves gradient alignment, ensuring consistent convergence of prompt optimization.  The authors conduct extensive experiments on various question-and-answering datasets to confirm LLEOT achieves strong adaptation while mitigating emulator noise."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an interesting and practical challenge of fine-tuning pre-trained LLMs in a setting where, both, the model and data are private and cannot be disclosed.  The proposed solution allows the model maintainer to train a surrogate model, that preserves model capability privacy, and to distribute this surrogate to the data maintainer for refinement.  The paper is largely well-written and the methodology appears novel."}, "weaknesses": {"value": "Critically, this paper does not discuss how the parameter of the emulator are adapted back into the original model.  The paper states, \"We require that transferring the trained weights $\\Delta^*$ back to the original model...should yield performance comparable to that achieved by directly optimizing $\\mathcal{M}$ on the dataset...without requiring access to $\\mathcal{M}$ itself.\"  This seems like a strong constraint that is nontrivial to enforce and it is unclear how the authors ensure this parameter transfer remains valid.  This is particularly concerning given that the emulator is intentionally degraded so as to not leak model capability.  \n\nAnother critical issue is that it is somewhat mysterious how the authors enforce the LLE constraint in Eq. (6)--note that $\\mathcal{L}_{\\mathcal{M}}$ is never defined.  Ensuring that the loss landscape between the emulator and model are equivalent, up to an additive constant, seems like a nontrivial constraint to enforce, yet the authors provide no discussion of how this is done in practice.  Indeed, Algorithm 1 (Line 12) states \"Optimize $\\mathcal{E}^*$ with respect to Equation (10)\" which I suspect is a typo as Eq. (10) is not related to the LLE optimization at all.  Looking over the proof of Theorem (1) it is also not obvious how the authors go from Eq. (12) to Eq. (14) as this is not a simple exponentiation of Eq. (12) (the constant factor is incorrect).\n\nA slightly lower priority issue is that the evaluation methodology needs some clarification.  In particular it is not clear whether the accuracy numbers reported are for the emulator (after fine tuning) or if they are accuracy on the original model $\\mathcal{M}$ after adaptation of the new parameters (i.e. $\\mathcal{M}_{\\Theta + \\Delta^*}$).  \n\nSome minor issues:\n* Algorithm 1 (Line 19) : I think the gradient update is incorrect here as $P^*$ is never initialized to anything\n* L291-292 : The statement is incorrect, I think you need to swap the references to the model and the emulator\n* L374 : Change \"upper bound\" to \"lower bound\""}, "questions": {"value": "Theorem 1 states that the perplexity of the emulator scales exponentially with perplexity of the original model.  One would expect that, as H increases, emulator performance would degrade exponentially.  Yet Fig. 4 suggests this is not the case, and in fact there is relatively little relationship between H and the model inference capability.  Please clarify this apparent contradiction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "91HQa5jpYQ", "forum": "r6EpCx29VA", "replyto": "r6EpCx29VA", "signatures": ["ICLR.cc/2026/Conference/Submission12487/Reviewer_SphT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12487/Reviewer_SphT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761068846365, "cdate": 1761068846365, "tmdate": 1762923362360, "mdate": 1762923362360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an intermediate model to isolate user data from direct access to the closed-source model. By combining knowledge distillation and local fine-tuning strategies, a new model is ultimately trained to adapt to the user’s data. The paper also provides theoretical analyses to support the feasibility of convergence and conducts experimental validation on several standard datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper focuses on privacy scenarios involving data and closed-source models, demonstrating a certain degree of practical foresight and forward-looking applicability.\n\n2. The paper presents a complete and coherent narrative, clearly describing the entire training process. The language is well-organized and easy to understand."}, "weaknesses": {"value": "1. The algorithmic design in the paper does not strictly adhere to its initial idea; in fact, the subsequent experimental section exhibits significant shortcomings. The original design intention (line 144), the architecture and parameters of this emulator should differ from those of the original model, and its inference capability should approximate that of a randomly initialized model. However, in the later algorithmic implementation, the authors construct the emulator by applying layer pruning to the original model, effectively assembling a subnetwork that retains portions of the original model connected through skip connections. This approach contradicts the stated objective, as the resulting emulator is not an independent model but rather a structurally reduced version of the original one.\n\n2. Such simplification is unacceptable, as it would inevitably lead to the leakage of the original parameters of the closed-source model. In the algorithm section, the authors do not mention whether noise injection or other privacy-preserving mechanisms are applied to the pruned parameters. As a result, the current version of the work presents serious issues in terms of experimental validity and privacy protection.\n\n3. The formulation in Equation (6) does not appear to generalize to cases where the emulator is initialized as a random model. In essence, the effectiveness of LLE relies on the assumption that the loss landscapes of the model $M$ and $\\theta$ are sufficiently similar. This requirement implicitly necessitates a degree of parameter correlation or alignment between the two models—an assumption that reintroduces the parameter exposure problem. Consequently, the claimed privacy preservation objective is in conflict with the underlying design of LLE.\n\n4. There is a lack of genuine privacy analysis in the paper. The authors do not provide any effective quantitative evaluation of how much improvement is achieved in terms of model parameter privacy and data privacy, respectively. Without explicit metrics or empirical validation, the claimed privacy enhancement remains qualitative and unsubstantiated.\n\n5. There is no effective convergence analysis provided in the paper. Although the LLE formulation ensures that the emulator’s gradient maintains the same direction as the original gradient, it remains unclear whether incorporating the soft prompt feature alignment loss introduces gradient distortion during the optimization of the original text-based training process. In other words, the paper does not analyze whether the combined objectives preserve the global convergence properties or potentially alter the optimization dynamics of the original model."}, "questions": {"value": "1. Line 144 mentions that the proxy model should exhibit performance comparable to that of the original model under random parameter initialization. Could the authors clarify the Weak 1 and 2? Specifically, what protective measures are implemented to safeguard the original parameters when layer pruning is used in the experiments? Moreover, is it possible that, through multiple query accesses, one could reconstruct or steal the full set of model parameters from the pruned model?\n\n2. What is the theoretical basis for the term $H$ in Equation (6), and why can it be considered equal?\n\n3. Why does CPKD show completely irregular performance across different pruning ratios in Table 2? When the pruning ratio is 0.2, CPKD has almost no effect, but at 0.5, it has a significant impact. What causes this phenomenon?\n\n4. Why does WebQA perform so poorly under the third loss combination in Table 3 (much lower than the other three datasets)?\n\n5. How were the CPL results in the paper evaluated? I couldn’t find a detailed explanation of the testing procedure like which dataset were the CPL measurements conducted, and what specific tests were performed?\n\n6. Why do some of the training results for the pruned model in Table 1 exceed those of the full-parameter fine-tuning model? This seems unreasonable. Does it imply that the results of the full-parameter fine-tuning are not actually optimal?\n\n7. How should the length of the soft prompt be selected during the training phase in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xUhYM5ZaiC", "forum": "r6EpCx29VA", "replyto": "r6EpCx29VA", "signatures": ["ICLR.cc/2026/Conference/Submission12487/Reviewer_rGBy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12487/Reviewer_rGBy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656208445, "cdate": 1761656208445, "tmdate": 1762923361920, "mdate": 1762923361920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study is the first to reveal that existing Optimal Transport (OT) methods may inadvertently expose a model’s capability, posing privacy risks. To mitigate this, the authors introduce LLEOT, a novel OT framework centered around the Latent Leakage Elimination (LLE) technique. The proposed method effectively disrupts emulator inference to prevent privacy leakage while preserving gradient consistency between the emulator and the original model. This preservation ensures that adapters trained on the emulator remain valid when applied to the original model. Extensive experiments demonstrate that LLEOT achieves state-of-the-art performance in safeguarding model privacy without compromising model utility."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The study uncovers a previously neglected risk of model capability privacy leakage in Offsite Tuning, revealing that existing emulators can retain significant inference capability, potentially allowing malicious data owners to extract or exploit proprietary model knowledge.\n\n2. The proposed Loss Landscape Elevation Offsite Tuning (LLEOT) framework introduces the Loss Landscape Elevation (LLE) technique, which effectively disables emulator inference while maintaining gradient alignment with the original model. Theoretical analysis (Theorem 1) guarantees that LLE increases emulator perplexity and preserves convergence toward the same optimal prompt.\n\n3. The integration of LLE with Collaborative Prompt Knowledge Distillation (CPKD) enables efficient distillation for soft prompts, ensuring that adapters trained on the emulator transfer seamlessly to the original model.\n\n4. Extensive experiments confirm that LLEOT delivers state-of-the-art results, achieving stronger privacy protection without sacrificing model performance compared to existing approaches."}, "weaknesses": {"value": "1. Limited Baseline Comparison\nThe experimental evaluation includes only two baselines — Offsite Tuning (OT) and CRaSh (2023) — which are insufficient to represent the current state of the field. Considering the rapid progress in model adaptation and privacy-preserving fine-tuning methods in 2024–2025, incorporating more recent baselines is necessary for a fair and comprehensive comparison. Additionally, comparison with other privacy-oriented LLM training frameworks would further demonstrate the effectiveness and generality of the proposed approach.\n\n2. Complex Training Procedure and Reproducibility Concerns\nWhile the proposed method is promising, the training pipeline appears overly complex, involving multiple loss functions and hyperparameters (e.g., ω₁, ω₂, and ω₃ in Equation 5). The paper does not analyze the training stability or sensitivity to these parameters, which raises concerns about robustness. Without such analysis, the reproducibility and practicality of the method in real-world or large-scale scenarios remain uncertain."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ocBHfEko6M", "forum": "r6EpCx29VA", "replyto": "r6EpCx29VA", "signatures": ["ICLR.cc/2026/Conference/Submission12487/Reviewer_mgAz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12487/Reviewer_mgAz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762115484177, "cdate": 1762115484177, "tmdate": 1762923361608, "mdate": 1762923361608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}