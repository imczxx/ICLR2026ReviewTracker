{"id": "0cBlhTTHfx", "number": 23607, "cdate": 1758346265734, "mdate": 1763072534569, "content": {"title": "DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis", "abstract": "Existing text-to-video (T2V) evaluation benchmarks, such as VBench and EvalCrafter, suffer from two main limitations. (i) While the emphasis is on subject-centric prompts or static camera scenes, camera motion which is essential for producing cinematic shots and the behavior of existing metrics under dynamic motion are largely unexplored. (ii) These benchmarks typically aggregate video-level scores into a single model-level score for ranking generative models. Such aggregation, however, overlook video-level evaluation, which is vital to selecting the better video among the candidate videos generated for a given prompt.\nTo address these gaps, we introduce DynamicEval, a benchmark consisting of systematically curated prompts emphasizing dynamic camera motion, paired with 45k human annotations on video pairs from 3k videos generated by ten T2V models. DynamicEval evaluates two key dimensions of video quality: background scene consistency and foreground object consistency.\nFor background scene consistency, we obtain the interpretable error maps based on the Vbench motion smoothness metric. Our key observation based on the error maps is that while the Vbench motion smoothness metric shows promising alignment with human judgments, it fails in two cases, namely,   occlusions/disocclusions arising from camera and foreground object movements. Building on this, we propose a new background consistency metric that leverages object error maps to correct two major failure cases in a principled manner. Our second innovation is the introduction of a foreground consistency metric that tracks points and their neighbors within each object instance to better assess object fidelity.\nExtensive experiments demonstrate that our proposed metrics achieve stronger correlations with human preferences at both the video level and the model level (an improvement of more than $2$\\% points), establishing DynamicEval as a more comprehensive benchmark for evaluating T2V models under dynamic camera motion.", "tldr": "DynamicEval is a benchmark for camera-motion video generation, introducing interpretable background and foreground consistency metrics that align better with human preferences than existing deep-feature methods.", "keywords": ["Text-to-video generation", "generative models", "evaluation metrics", "quality assessment", "human annotations", "video benchmark", "video dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7135ad907ccee7b57055cff96b83fa8eb1b08f44.pdf", "supplementary_material": "/attachment/ab114df4df86021a6354d2cfea54d4e6a8efadd8.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes DynamicEval, a benchmark consisting of systematically curated prompts emphasizing dynamic camera motion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper considers evaluating video generation from a different aspects, i.e., dynamic.\n\n2. The paper is easy to follow.\n\n3. The paper fulfills 9 pages."}, "weaknesses": {"value": "1. There is already some works focusing on dynamic aspects: Evaluation of Text-to-Video Generation Models: A Dynamics Perspective (NeurIPS 2024). The paper either cites the paper or discuss the differences with that. It is unacceptable becausue they are too similar.\n\n2. The writing is poor. As shown in Fig. 1 and 2, the fonts in figures are too small to be checked.\n\n3. The evaluation is not comprehensive. Compared to Vbench, which evaluates video generation from many aspects. The paper only considers the dynamic aspect.\n\n4. The prompts in the dataset are not from the real world datasets, such as VidProM. It is sythesized which limits the practical use of this paper.\n\n5. There is no ethnic approvement for using human sources to evaluate. Does they work fairly? Is there any forced labor happens?"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "B5YRpEFwb6", "forum": "0cBlhTTHfx", "replyto": "0cBlhTTHfx", "signatures": ["ICLR.cc/2026/Conference/Submission23607/Reviewer_oNBz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23607/Reviewer_oNBz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722654525, "cdate": 1761722654525, "tmdate": 1762942732956, "mdate": 1762942732956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "xF9fmD6vXP", "forum": "0cBlhTTHfx", "replyto": "0cBlhTTHfx", "signatures": ["ICLR.cc/2026/Conference/Submission23607/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23607/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763072533243, "cdate": 1763072533243, "tmdate": 1763072533243, "mdate": 1763072533243, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ​**DynamicEval**​, a new benchmark and metric suite for evaluating *text-to-video (T2V)* models under ​**dynamic camera motion**​, a setting largely ignored by existing benchmarks such as VBench and EvalCrafter.\n\nDynamicEval contributes:\n\n1. **A dataset** of 3 000 videos from 10 T2V models, with 45 000 pairwise human annotations on two key dimensions — **background (BG) consistency** and **foreground (FG) object consistency** — focusing on scenes with explicit camera motion.\n2. ​**Two fine-grained automatic metrics**​:\n   * ​**MS-Debias**​, a debiased optical-flow–based background consistency score that masks foregrounds and occlusion areas in VBench’s Motion Smoothness metric.\n   * ​**Tracker-FG**​, a CoTracker-based object-consistency score measuring local deformation smoothness among point tracks within each object.\n3. **Empirical results** showing higher human correlation (≈ +2 pp video-level accuracy, +0.2 PLCC) than prior metrics, and a curated prompt suite emphasizing diverse camera motions.\n\nThe work aims to improve *video-level* evaluation and offers a resource for developing future T2V metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "#### 1. Originality\n\n* Tackles an underexplored aspect — **dynamic camera motion evaluation** — that is critical for cinematic T2V quality.\n* Introduces a new **procedural prompt-generation pipeline** and a **large-scale human study** focused on motion-rich prompts.\n* The **debiasing of optical-flow error maps** and the **track-based FG metric** are creative, interpretable extensions of existing metrics.\n\n#### 2. Clarity\n\n* Writing is well structured and readable; figures (e.g., error-map visualizations) illustrate ideas effectively.\n* Equations and algorithmic steps for both metrics are explicit and reproducible.\n\n#### 3. Significance\n\n* Provides a **missing benchmark component** for evaluating dynamic video generation—a growing focus in commercial and open research T2V models.\n* The metrics are **plug-and-play** and interpretable, likely to become standard diagnostic tools.\n* Potentially useful for *reinforcement-learning-from-video-feedback* and model-selection pipelines.\n\n#### 4. Quality\n\n* Methodology is rigorous: ablation across dynamic/static subsets, full-agreement subsets, and multiple baselines.\n* Clear quantitative improvements support claims.\n* Dataset construction and reliability checks follow perceptual-quality-assessment best practices."}, "weaknesses": {"value": "* **Limited Scope of Evaluation Dimensions**\n  Only background and foreground consistency are addressed. Other key axes (temporal coherence of lighting, motion realism, text-prompt faithfulness) are untouched; the benchmark might thus be incomplete as a holistic evaluation suite.\n* **Reliance on Pre-trained Vision Modules**\n  Although acknowledged, heavy dependence on RAFT and CoTracker trained on real videos may bias results when synthetic artifacts violate their priors. Experiments quantifying metric stability under flow/track noise would strengthen robustness claims.\n* **Dataset Bias and Size**\n  \n  * 100 prompts × 10 models × 3 videos yields 3 000 clips ≈ short scenes — perhaps insufficient diversity for generalization.\n  * Prompts are GPT-generated; linguistic diversity and style variance may still be limited.\n* **Statistical Analysis**\n  Improvements of 2 pp are meaningful but modest; confidence intervals or significance testing (e.g., bootstrap CI of human-metric correlation) are missing.\n* **Accessibility / Reproducibility Timing**\n  The dataset and code are “to be released,” which weakens verifiability at review time."}, "questions": {"value": "**Broader Impact** – Could DynamicEval incentivize models to over-smooth motion to maximize scores? Any mitigation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3t6mkuccxr", "forum": "0cBlhTTHfx", "replyto": "0cBlhTTHfx", "signatures": ["ICLR.cc/2026/Conference/Submission23607/Reviewer_V5mv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23607/Reviewer_V5mv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816622237, "cdate": 1761816622237, "tmdate": 1762942732662, "mdate": 1762942732662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose DynamicEval, a new benchmark comprising 100 dynamic-focused prompts, 3,000 generated videos from 10 models, and 45,000 human pair-wise preference annotations. Concurrently, it proposes two new metrics: MS-Debias for background (BG) consistency, which \"debiases\" VBench's motion smoothness by masking out foreground objects and occlusion artifacts , and Track-FG for foreground (FG) consistency, which uses a point tracker (CoTracker) to analyze the smoothness of relative distances between internal object points. Experiments show these new metrics achieve over 2% points higher correlation with human preferences than baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The primary strength of this work lies in its improvement of the metrics for background consistency and foreground (subject) consistency."}, "weaknesses": {"value": "1. Limited Evaluation Scope: The paper reduces the complex, multi-dimensional problem of \"dynamic video quality\" to only two dimensions: FG and BG consistency. This is an overly narrow view that ignores other critical aspects of video generation, such as: Text-Video Alignment, Motion Plausibility, Aesthetic Quality, etc.\n2. A Flawed \"Rigid-Body\" Assumption: The core design of the Track-FG metric is fundamentally flawed. By assessing consistency based on the \"smoothness of distances\" between tracked points, the metric implicitly assumes that foreground objects are rigid or near-rigid. This is factually incorrect for most real-world scenarios. Non-rigid objects like a person talking (facial deformation), a flag waving, water rippling, or cloth moving in the wind are all \"consistent\" and high-quality, yet the distances between points on their surfaces should change dramatically. The Track-FG metric would incorrectly penalize these correct, high-quality generations as \"inconsistent\" or \"distorted.\n3. Methodological Loophole: \"Cheating\" via Static Videos: The paper's goal is to evaluate \"dynamic camera motion\". However, the authors admit in their own analysis (Fig. 2b) that many models—especially open-source and older ones—failed to follow the dynamic prompts and instead generated static videos. The authors' decision to \"retain static videos in the evaluation\"  creates a critical methodological flaw. A static video will naturally achieve a near-perfect score on any \"consistency\" metric. This means models are effectively rewarded with higher consistency scores for failing to adhere to the benchmark's primary goal (dynamic motion). This \"static video loophole\" invalidates the benchmark's claim of evaluating dynamic scenes.\n4. Incremental Novelty: The paper's contributions are largely incremental. The work consists of \"debiasing\" an existing metric (VB-MS) and replacing another (VB-SC) with a different technique (tracking) to measure the same dimension of consistency. The paper does not, as its title \"Rethinking Evaluation\" suggests, propose any new, fundamentally different evaluation paradigms or dimensions.\n5. Error Propagation Risk: Both metrics rely on a complex and fragile \"toolchain\" of off-the-shelf models (RAFT , CoTracker , LLMs, GroundingDINO , and SAM-2 ). As the authors partially acknowledge, these tools were trained on real videos. Their reliability on generated videos, which may contain unique artifacts, is questionable. This creates an ambiguity: a low score could mean the video is poor, or it could mean the evaluation tool (e.g., CoTracker) failed to track points on a distorted video."}, "questions": {"value": "see weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3FCkdOdtO9", "forum": "0cBlhTTHfx", "replyto": "0cBlhTTHfx", "signatures": ["ICLR.cc/2026/Conference/Submission23607/Reviewer_TTWy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23607/Reviewer_TTWy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982679289, "cdate": 1761982679289, "tmdate": 1762942732297, "mdate": 1762942732297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DynamicEval, a benchmark focused on evaluating dynamic motion, which includes 100 prompts with camera motion descriptions. To assess motion quality, two new metrics are introduced: Background Consistency and Subject Consistency. Specifically, pixel-level tracking is used to capture more fine-grained spatial awareness and long-term temporal consistency. Additionally, the paper determines whether a video is static or dynamic based on pixel-level point movement. The proposed metrics were tested for alignment with human annotations, and experiments show that they achieve a 2% improvement in agreement with human judgments compared to baseline metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a benchmark for evaluating video motion quality, which includes 100 prompts describing camera motion.\n\n2. The paper uses point tracking to capture finer spatial perception and long-term temporal consistency at the pixel level.\n\n3. The paper is clearly and logically written, outlining the research gap and explicitly defining the evaluation metrics."}, "weaknesses": {"value": "1.Compared to VBench, the measurement of foreground and background consistency in this paper replaces RAFT with CoTracker and adds masking. The construction of the metrics in this paper is incremental relative to VBench.\n\n2.The evaluation of camera motion in this paper is simply divided into static and dynamic. In video generation, camera motion involves complex 3D spatial relationships, so this simple classification makes the evaluation metrics rather limited.\n\n3.Limited evaluation dimensions: As a benchmark paper, the evaluation primarily covers only three aspects—foreground consistency, background consistency, and whether the scene is dynamic.\n\n4.The evaluation method relies on multiple external models, such as detection, segmentation, and tracking, making it susceptible to the limitations of these models. If one or more of these external models fail, the proposed metrics can be easily affected."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GYYd1aHuMb", "forum": "0cBlhTTHfx", "replyto": "0cBlhTTHfx", "signatures": ["ICLR.cc/2026/Conference/Submission23607/Reviewer_ezqH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23607/Reviewer_ezqH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989182593, "cdate": 1761989182593, "tmdate": 1762942732009, "mdate": 1762942732009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}