{"id": "V3SwjdEGAK", "number": 4365, "cdate": 1757666689220, "mdate": 1759898037045, "content": {"title": "Attention Needs to Focus: A Unified Perspective on Attention Allocation", "abstract": "The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues---representational collapse and attention sink. While existing work has proposed solutions for these problems, they are typically addressed in isolation, lacking a unified analysis of the root cause or a comprehensive solution for both problems.\nIn this paper, we present a unified perspective, arguing that these seemingly disparate issues stem from a single underlying phenomenon: improper attention distribution. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments demonstrate that Lazy Attention resolves attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58\\% attention sparsity.", "tldr": "We introduce Lazy Attention, which combines head/dimension-wise positional discrimination with Elastic-Softmax to reduce attention overload and underload, producing more focused attention.", "keywords": ["large language models", "self-attention", "attention sink", "representational collapse", "positional discrimination", "elastic-softmax"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c45acfd2623b940c8264df878b8929d72545e3fd.pdf", "supplementary_material": "/attachment/cfd45a0eb04e5fde4f378e3c2215a38eed2d9f1a.zip"}, "replies": [{"content": {"summary": {"value": "This paper identifies two fundamental failure modes in the standard Transformer self-attention mechanism: Attention Overload (attention is spread too broadly, blurring semantic features and causing representational collapse) and Attention Underload (attention is forced onto irrelevant tokens like \"attention sinks\" due to softmax's normalization constraint).\n\nTo address both issues within a unified framework, the authors propose Lazy Attention, which consists of two key components:\n\n**Positional Discrimination**: A hybrid positional encoding combining RoPE with learnable, head-specific attention biases to sharpen token distinctions and alleviate overload.\n\n**Elastic-Softmax**: A modified normalization that subtracts a learnable, head-specific offset before applying a ReLU, allowing the model to assign zero attention to irrelevant tokens and mitigate underload."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Unified Perspective**: The paper proposes a unified theoretical framework that connects two seemingly disparate problems (representational collapse and attention sink) to a single root cause: improper attention allocation. \n\n**Comprehensive Evaluation**: The method is rigorously tested against a wide array of strong baselines, including Transformer variants, recurrent architectures (Mamba2, RetNet), and streaming inference methods, across multiple model scales and nine diverse benchmarks.\n\n**Compatibility**: The authors explicitly address and ensure the method's compatibility with optimized kernels like FlashAttention, a crucial point for real-world adoption."}, "weaknesses": {"value": "**Limited Scale of Models**: The largest model scale tested is 760M parameters. While this is standard for architectural research, the effectiveness of Lazy Attention in today's state-of-the-art models (e.g., those with 10B+ parameters) remains an open question. The behavior of attention mechanisms can change significantly with scale.\n\n**Computational Overhead of Elastic-Softmax**: The paper notes that Elastic-Softmax requires two passes to be compatible with FlashAttention. While the complexity remains O(nÂ²), the actual wall-clock time and memory overhead compared to a single-pass softmax are not quantified. This is an important practical consideration.\n\n**Ablation on Sparsity's Utility**: While the high sparsity is celebrated, the paper does not directly demonstrate the resulting computational speedups or memory savings during inference. Showing end-to-end latency improvements would strengthen the claim of efficiency."}, "questions": {"value": "1. How do the authors anticipate the performance and the learned behaviors (e.g., bias patterns, offset values) of Lazy Attention would change when scaled to models with billions of parameters? Do you expect the \"overload\" and \"underload\" phenomena to persist or change in nature at that scale?\n\n2. The authors mention Elastic-Softmax is compatible with FlashAttention but requires two passes. Could you quantify the resulting training and inference latency overhead compared to standard softmax attention? Does the achieved sparsity translate into a net reduction in inference time?\n\n3. The initialization of the Elastic-Softmax offset Ï„=1.0 is crucial. Was this value found through extensive ablation? What is the sensitivity of the model's final performance to this initial value?\n\n4. How does Lazy Attention, which learns sparsity dynamically, compare conceptually and empirically to pre-defined sparse attention patterns (e.g., sliding window, BigBird)? Does it learn similar patterns, or does it discover more optimal, task-specific sparsity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y29Vix7S4t", "forum": "V3SwjdEGAK", "replyto": "V3SwjdEGAK", "signatures": ["ICLR.cc/2026/Conference/Submission4365/Reviewer_Ayxf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4365/Reviewer_Ayxf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724958101, "cdate": 1761724958101, "tmdate": 1762917317867, "mdate": 1762917317867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that two pervasive Transformer pathologiesâ€”representational collapse and attention sinkâ€”stem from the same root cause: improper attention allocation. It diagnoses two extremes: Attention Overload (too many tokens get comparably high weights) and Attention Underload (no token is relevant, but softmax must distribute probability mass somewhere, often to early â€œsinkâ€ tokens). To address both, the authors propose Lazy Attention, which combines: 1) Positional Discrimination: standard RoPE plus learnable distanceâ€‘dependent, headâ€‘wise biases added to the score to sharpen distinctions across heads and dimensions. Heatmaps show varied learned decay patterns across layers/heads. 2) Elasticâ€‘Softmax: a postâ€‘softmax, perâ€‘head offset with ReLU, to zeroâ€‘out weights of irrelevant keys and neutralize sink behavior. The authors visualize learned offsets by layer  and provide alternatives/ablations.\n\nOn FineWebâ€‘Edu pretraining (10B/100B tokens) with 340M and 760M models, Lazy Attention achieves competitive or better accuracy on 8 reasoning benchmarks while reducing sink mass and increasing sparsity. The method also shows improved length extrapolation from a 512â€‘token training context to 1024/2048 on WikiText/LAMBADA. For implementation, Elasticâ€‘Softmax is described as FlashAttentionâ€‘compatible via two passes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "++ The paper connects collapse and sink via â€œallocation extremes,â€ then uses interventions to localize and characterize sink behavior and its dependence on positional encoding. This yields two crisp takeaways about variance footprints and the role of RPEs in shaping weights rather than embeddings.\n\n++ The learnable headâ€‘wise distance biases plus RoPE sharpen focus; the Elasticâ€‘Softmax filter removes lowâ€‘relevance mass and alleviates sink. The layerâ€‘wise offset patterns and bias curves are informative and align with the overload/underload story.\n\n++ At 340M/760M, accuracy is on par or better than strong baselines across diverse tasks while showing 59.58% sparsity and reduced sink ratio. Perplexity degrades less with increasing length from 512â†’2048 versus a standard Transformer."}, "weaknesses": {"value": "-- The paper defines ð›¼=ReLU(softmaxâˆ’ðœ_ð‘–) and does not reâ€‘normalize before applying ð‘‰, so the sum of weights is no longer 1, making the output scale dataâ€‘/offsetâ€‘dependent. The metric density/sink is clear for softmax (sum=1) but less interpretable when mass is removed. In addition, there are sign/initialization inconsistencies: Â§4.2 sets ðœ0^(â„Ž)=1 and â€œdivided evenly across the ð‘– attended tokens,â€ whereas Tableâ€¯2 reports the best variant using ðœ_â„Ž/seq_len; Fig.â€¯5 shows negative offsets in early layers yet claims â€œaggressive filtering,â€ which contradicts the subtractâ€‘thenâ€‘ReLU formula. These details impact stability and should be clarified with formulas and pseudocode.\n\n-- Elasticâ€‘Softmax requires two passes (App.â€¯F), and headâ€‘wise distance biases â€‹introduce many parameters (Fig.â€¯8/9). The paper lacks wallâ€‘clock training/inference time, throughput/latency, and KVâ€‘cache or peak memory comparisons versus standard attention and other sparse/streaming variants. Without these, the claimed practical benefits (e.g., sparsity) are hard to evaluate.\n\n-- The method â€œadopts a larger RoPE base ðµâ€ but does not specify the value/rationale nor ablate it. Likewise, the distanceâ€‘bias parameterization (range, bucketization vs. perâ€‘distance tables, regularization) is not detailed; Fig.â€¯8 caption mentions a max range 1024, conflicting with Â§5.1â€™s 4096 context length. Clear specs and ablations on ðµ, bias ranges, and learned patterns would bolster the claims."}, "questions": {"value": "1. Exact Elasticâ€‘Softmax algorithm: Do you reâ€‘normalize after thresholding? If not, how do you prevent scale drift across layers/heads?\n\n2. Compute/memory impact: What are the measured FWD/BWD times and max memory for Lazy Attention vs. softmax attention on the same hardware and sequence lengths, including the twoâ€‘pass overhead? Please include KVâ€‘cache and activation footprints.\n\n3. RoPE base ðµ & bias parameterization: What ðµ did you use, and how sensitive are results to ðµ? Are the distance biases bucketed (like T5/ALiBi) or per distance up to a cutoff (Fig.â€¯8/9)? How big is the parameter table per head/layer, and how does it scale to 32k+ tokens?\n\n4. Normalizationâ€‘aware metrics: Since Elasticâ€‘Softmax can reduce total mass, how are density and sink computed? Could you report (i) total surviving mass, (ii) fraction assigned to sink, and (iii) fraction to nonâ€‘sink, so the numbers remain interpretable across methods?\n\n5. Veryâ€‘long and streaming settings: How does Lazy Attention perform on >32k context and standard longâ€‘context QA (e.g., LongBench variants)? Any interaction with streaming methods (e.g., StreamingLLM) once sink is suppressed?\n\n6. Gradients through ReLU thresholding: Early in training, many weights may be zeroed; did you observe optimization instabilities or head collapse? Any tricks (warmups/entropy terms) to keep gradients flowing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iNryFsxd0s", "forum": "V3SwjdEGAK", "replyto": "V3SwjdEGAK", "signatures": ["ICLR.cc/2026/Conference/Submission4365/Reviewer_q3b5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4365/Reviewer_q3b5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134375275, "cdate": 1762134375275, "tmdate": 1762917317622, "mdate": 1762917317622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies that the representational collapse and attention sink in LLMs stem from improper attention allocation, which manifests as two failure modes: Attention Overload and Attention Underload. To address these issues, it proposes Lazy Attention, integrating Positional Discrimination (combining RoPE with learnable attention biases) and Elastic-Softmax (a modified normalization function)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a unified perspective to explain two long-standing problems (representational collapse and attention sink) in Transformer architectures,.\n2. Extensive experiments on diverse benchmarks and different model scales demonstrate that Lazy Attention not only mitigates attention sink but also achieves competitive performance compared to state-of-the-art baselines in some cases."}, "weaknesses": {"value": "1.The paper lacks clear definitions and explanations for key elements in Figure 2. For Figure 2a, the notation \"Mask@2\" is introduced when inserting a fixed [Mask] token during pre-training, but it fails to specify what \"2\" refers to. \n\n2. In Line 254, the notations \"d\" and \"D\" are not explicitly defined. \n\n3. Formula 4 introduces a learnable attention bias term b^(h)_{|i-j|}, which is a head-specific bias dependent on the relative distance |i-j| between tokens. The paper does not discuss the potential memory overhead of this bias: for each attention head, a bias value needs to be stored for every possible relative distance (up to the maximum sequence length, e.g., 4096 in experiments). As the sequence length increases, the number of bias parameters grows linearly with the sequence length, which may significantly increase memory consumption, especially for long-context models. Additionally, the paper does not evaluate whether this distance-dependent learnable bias affects the modelâ€™s extrapolation abilityâ€”whether the bias learned for short sequences can generalize to longer sequences remains unaddressed.\n\n4. The results in Table 1 show an inconsistent pattern between WikiText perplexity (ppl) and LAMBADA (LMB) ppl for the proposed Lazy Attention, which raises doubts about its effectiveness. For the 340M parameter model, Lazy Attention achieves a WikiText ppl of 25.32, but its LMB ppl is 31.84, which is worse than baselines. The paper does not explain this inconsistency.\n\n5. There are inconsistencies in the notation of the Elastic-Softmax offset. In Formula 4 (Section 4.1), the paper uses the notation $\\tau_0^h$ for the offset, while in Table 2, the notation is written as $\\tau_h^0$. It is not clarified whether these are two distinct notations for the same parameter. Furthermore, Formula 4 implies that the default initial value of $\\tau_0^h$ is 0, but Table 2 shows that the variant with $\\tau_0^h$ = 1 achieves better performance.\n\n6. Figure 6 compares the perplexity of different models across varying sequence lengthsfor length extrapolation, but the superiority/inferiority of Lazy Attention relative to baselines is inconsistent between subfigure (a) and subfigure (b). \n\n7. In, Formula 5, Elastic-Softmax relaxes the softmax constraint that attention weights sum to 1. The paper does not address the potential training instability caused by non-normalized attention weights: if the sum of attention weights is too large, it may amplify the impact of noisy tokens; if the sum is too small (e.g., most weights are suppressed to 0, leading to a sum close to 0), the model may fail to capture sufficient contextual information. The paper does not report whether such sum deviations occur during training.\n\n8. The paper uses two different notations for learnable biasesâ€”\"m\" in Line 258 and \"b\" in Line 264â€”but does not clarify their differences."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u4akXA4e2I", "forum": "V3SwjdEGAK", "replyto": "V3SwjdEGAK", "signatures": ["ICLR.cc/2026/Conference/Submission4365/Reviewer_Wg8F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4365/Reviewer_Wg8F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136902063, "cdate": 1762136902063, "tmdate": 1762917317272, "mdate": 1762917317272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Lazy Attention, which aims to unify two well-known attention pathologies (i.e., representational collapse and attention sink) under a single explanatory framework. The method combines Positional Discrimination (adding learnable head- and distance-specific biases on top of RoPE) and Elastic-Softmax (a ReLU-threshold applied after softmax) to promote sparse, focused attention. Experiments on 340M- and 760M-parameter models report moderate accuracy improvements, reduced sink metrics, and claimed attention sparsity of up to 59.6%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear conceptual framing**. The paper presents a coherent narrative linking two common attention failure modes as manifestations of improper allocation, supported by intuitive visual analyses.\n\n- **Simplicity and modularity**. Both proposed components, including score-level positional bias and post-softmax filtering, are easy to implement within standard transformer codebases.\n\n- **Comprehensive qualitative evidence**. Attention heatmaps, offset distributions, and learned bias profiles provide interpretability and consistent visual trends."}, "weaknesses": {"value": "- **Ambiguous core mechanism.** The paper defines Elastic-Softmax as applying a token-wise ReLU to `(softmax â€“ Ï„_i^h)`, yet later sections describe thresholding as a fixed `Ï„_h / seq_len`. These two formulations are not equivalent and lead to different scaling behaviors. Moreover, in the reported ablations, the learned Ï„ values often become negative, effectively *adding* probability mass instead of filtering low-attention entries, which is the opposite of the stated mechanism. \n\n- **Unnormalized probability mass and scaling drift.** Applying a post-softmax ReLU breaks probability conservation, yet no renormalization is performed. As a result, attention magnitude may vary with sequence length or sparsity, confounding comparisons and potentially destabilizing training objectives.\n\n- **Incomplete baseline coverage**. Strong probability-sparsifying or normalization-free alternatives (e.g., sparsemax, entmax) are omitted from large-scale comparisons, leaving the empirical novelty and significance of the proposed method under-substantiated.\n\n- **Unmeasured real-device efficiency**. Despite claiming FlashAttention compatibility, the two-pass design lacks any measured latency/throughput/memory data or speedups tied to induced sparsity."}, "questions": {"value": "What is the exact, implemented Elastic-Softmax formula (including threshold normalization and sign), and how does it reconcile with the reported offset statistics?\n\nDo you renormalize the post-ReLU weights, and if so, how? If not, how do you control for scale drift across layers and sequence lengths?\n\nUnder which tasks/datasets does Elastic-Softmax (not merely the positional bias) yield statistically significant gains over its ablation across multiple seeds?\n\nCan you add same-recipe comparisons against sigmoid attention and sparsemax/entmax at the larger scale and report variance, tuning budgets, and training stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FBwq11ItXb", "forum": "V3SwjdEGAK", "replyto": "V3SwjdEGAK", "signatures": ["ICLR.cc/2026/Conference/Submission4365/Reviewer_PQ4b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4365/Reviewer_PQ4b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762479587551, "cdate": 1762479587551, "tmdate": 1762917316977, "mdate": 1762917316977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}