{"id": "IaeZcYpRxD", "number": 5159, "cdate": 1757857835565, "mdate": 1759897991262, "content": {"title": "Diffusion Models are Kelly Gamblers", "abstract": "We draw a connection between diffusion models and the Kelly criterion for maximizing returns in betting games. We find that conditional diffusion models store additional information to bind the signal $X$ with the conditioning information $Y$, equal to the mutual information between them. Classifier-free guidance effectively boosts the mutual information between $X$ and $Y$ at sampling time. This is especially helpful in image models, since the mutual information between images and their labels is low, a fact which is intimately connected to the manifold hypothesis. Finally, we point out some nuances in the popular perspective that diffusion models are infinitely deep autoencoders. In doing so, we relate the denoising loss to the Fermi Golden Rule from quantum mechanics.", "tldr": "Many aspects of diffusion models can be understood through the lens of mutual information.", "keywords": ["Diffusion models", "information theory", "representations", "guidance"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c21601f1b1a629e821a4983878e27831610362f0.pdf", "supplementary_material": "/attachment/b557c2d08ba339541e71339c21c8dc38d25d67f7.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a number of conceptual connections between diffusion models, physics, and information theory. The main contribution is an analysis of how information about the label, Y, is dynamically injected into X during the diffusion dynamics. The machinery to calculate this comes from non-equilibrium thermodynamics, and results in an intuitive expression in terms of mutual information (which can be related to gambling via the Kelly criterion, motivating the title). The results are used to motivate discussion of qualitative properties of diffusion models like how CFG scale affects generation, the behavior of inter-pixel correlation during generation, and a variational interpretation of diffusion."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The analysis of diffusion in terms of dynamical entropy production from Seifert and others was interesting. The notation and intuitions used in the molecular dynamics literature are quite different, so it was nice to see it adapted to this problem. I was curious how the choice of reverse protocol to define total entropy production is made, as Seifert gives several options. Perhaps this detail can be found in the reference provided. \n\nThe TC(X) part was new to me, I have not seen a diffusion paper write or study this expression."}, "weaknesses": {"value": "The structure of the paper is unusual for ICLR. There are no empirical results presented in the main paper, these are all in the supplementary material. From this point of view, I'd consider this a \"theory only\" contribution. But as a theory paper it was unclear to me what the key result is that the paper was trying to build toward. \n\n# Key result / Contributions? \nI found the connections interesting, but it was hard me to pinpoint the specific contributions and whether each connection is superficial or not. \n \n-  Kelly criterion. An entire section (and the title) highlights the Kelly criterion, but this connection seemed incidental to me. I didn't see how it was used either in derivations or empirical results, besides adding an operational interpretation to mutual information. \n\n- Mutual information. \nOne of the main contributions seems to be to give an alternate derivation of the mutual information expression from (Franzese, 2024) using the the stochastic thermodynamics ideas from (Seifert 2012) and (Premkumar 2025). \nBut here again, the high level treatment leaves me wondering if anything significant is being stated. What is the difference between S (entropy using physics notation) and S_tot? Well, in Seifert S_tot is meant to include \"total entropy production\", which includes entropy that is added to the medium due to dissipation. Some particular assumptions are required to have a meaningful derivation for entropy added to the medium as we haven't explicitly modeled the medium in this context, and for these I had to go back to (Premkumar 2025) Appendix A. Regardless, we model total entropy production as entropy of the system plus the medium and we get results like Eq. 7 and 9 which are *regular entropy* plus some term, presumably related to dissipation in the medium. Then when we \"derive\" mutual information, the dissipation terms drop out and we get the standard expression S_X - S_X|Y. Did the detour to complexify the math by adding dissipation contribute in any way? I'm only a little familiar with Seifert 2012, but more than most people in this field. The explanation in the paper does not convey what is being done, nor justify why this machinery is necessary. I don't see how vague statements like the \"network stores S_tot worth of information..\" can be connected to the mathematical meaning of total entropy production. \nIn the end, the final derivation of mutual information in C.1 doesn't use any ideas from Seifert anyway, that I can see. \n\n\n\n- Infinite tower. \nThe reinterpretation in Eq. 24 doesn't really use or require quantum mechanics in any way.  The idea that each term in the sum/integral can be viewed as a separate \"shallow\" autoencoder is, I think, widely appreciated as no one trains a diffusion model by running the infinitely deep encoder, but rather by sampling an encoder that represents a term in the integral (Song & Ermon explicitly talk about this kind of shallow ensemble of autoencoders in their work). \n\n\n- Expressions like those in Eq. 22-23 are common in refs 1-3, and [5, Eq. 16]. The connection between MI and guidance is also explored in [1] below. \n\n# Related work \nThe paper covers a lot of ideas relating diffusion to information theory, compression and variational methods, but the related work does not reflect existing work that has also explored these connections. \n\n- Information theory\n\n[1] Information Theoretic Text-to-Image Alignment (ICLR 2025). This work analyzes the mutual information between text and images. They show that CFG scale increases mutual information, but they also include an analysis that shows that it decreases the overall visual quality, and propose an optimal trade-off. \n\n[2] Diffusion PID: Interpreting Diffusion via Partial Information Decomposition (NeurIPS 2024) \n\n[3] Information-Theoretic Proofs for Diffusion Sampling arXiv:2502.02305\n\n[4] https://par.nsf.gov/servlets/purl/10084196. I saw and included this other one by Reeves, because it analyzes the mutual information in the Gaussian case specifically, which is what a lot of your empirical results in the appendix were based on. \n\n- Variational / autoencoder connections\n\n[5] Variational Diffusion Models (NeurIPS 2021). I feel this is a kind of foundational paper for this connection.\n\n\n- Compression \n\n[6] Progressive Compression with Universally Quantized Diffusion Models (ICLR 2025). The compression angle is discussed, there is lots of great work including this example."}, "questions": {"value": "My main question is for clarification of the intended contributions and which part of your arguments are novel compared to existing works.\n\nGenerally I'm very interested and excited about the physics connections presented, and hoping I can get clarification on the key results / contributions in the rebuttal stage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8KJHudF8IP", "forum": "IaeZcYpRxD", "replyto": "IaeZcYpRxD", "signatures": ["ICLR.cc/2026/Conference/Submission5159/Reviewer_VRvv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5159/Reviewer_VRvv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761269780409, "cdate": 1761269780409, "tmdate": 1762917915601, "mdate": 1762917915601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Conditional diffusion models need to generate high-quality images consistent with a given prompt or label. This is not necessarily computationally easy, since the word \"dog\" is associated with an extremely large number of complex images, and is hence not that informative about individual pixel colors and intensities. The authors claim that diffusion models learn a 'rational' strategy which is analogous to satisfying the Kelly criterion in betting games."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The writing is grammatical and stylish, and there are few figures (one in the main text, more in SI) but they are nice-looking. The paper appears to be quite original and the authors demonstrate knowledge of a wide range of fields. \n\nThe insight that an information-theoretic perspective can help us understand why diffusion models work well, and in particular why classifier-free guidance works well, is interesting."}, "weaknesses": {"value": "This paper is really frustrating. While the language is nice and the ideas sound original, the more I go over its various points, the more I find them unclear and lacking in support. Overall, I am left with an impression that this paper is more poetry than substance. \n\n**What precisely is being claimed?** The first issue is that it is not exactly clear in many cases what the claims of the paper are. Almost all of the math is known, and there are almost no experiments (in the main text, at least; I did not read the SI in detail). As far as I can tell, the claims are mostly of an 'interpretive' nature: this is potentially an interesting way to *think* about diffusion models. While new ways of thinking can be intrinsically interesting, it isn't clear to me what the consequences or predictions of this are.\n\nI'm still confused about the Kelly gambler analogy, and am more confused the more I think about it. For example, what does it mean to \"bet\" or \"invest\" here? It seems like the meat of the analogy is that both betting and sample generation involve mutual information (MI) and conditional MI. But I'm not sure what to make of it beyond that. If the authors claim that diffusion models are implicitly solving an optimal control problem (say, a problem where one has to choose how to invest bits of information at different times during sample generation, or something like that), they should spell out that problem and show how diffusion models solve it optimally compared to other possible strategies. \n\nAnother subtle issue with the titular claim of the paper is that a lot of the math actually studies \"entropy-matching models\" introduced by a paper from earlier this year (see line 181); these are different from what the authors call \"score-matching models\", which are what people normally think of as \"diffusion models\". **This is a really big deal!** It feels like a sleight of hand; it feels like the authors liked the Kelly gambler analogy, and modified diffusion models until it became true-ish.\n\n**Support for claims.** As I said above, I can't find new math results or nontrivial experiments. Also, a lot of the math applies to \"entropy-matching models\" rather than diffusion models proper. The paper makes a lot of claims about how diffusion models behave, and why they work, and so on. Frankly, the number of such claims per page is dizzying. \n\nIt's possible that they are all interesting and true. I am definitely open to this possibility. But the authors should really focus on a few of them (especially the titular claim) and show that they really are true. Moreover, the truth of these claims ideally has some interesting consequence or yields interesting predictions.\n\nA given claim should have some combination of math, experiments, and citations to support it. Interesting claims which are 'intuitively' or probably true should be removed and supported in future work.\n\n**Gibberish.** Some of the claims made are slightly unhinged. Consider especially Section 6, the last section: in it, the authors claim that (i) \"a diffusion model is an infinite tower of variational autoencoders\" and that (ii) a decomposition of the log-likelihood relevant to diffusion models resembles Fermi's golden rule from quantum mechanics. \n\nI'm not sure what to make of these claims. I am not excited to say this; I've used the Sakurai quantum mechanics textbook the authors cite. What is the substance? What is being said about how diffusion models work? What are the new predictions/consequences? Like the Kelly gambler analogy, it seems like the authors are squinting and seeing a vague relationship, but it is not clear if the relationship is useful or interesting. Moreover, the claims of the last section appear to not relate to the Kelly gambler claims much. My impression is that the authors have packed a lot of 'this might be kind of cool' material into a loosely organized package.\n\n**Overall assessment.** I value the authors' attempt to leverage work from information theory and physics to better understand diffusion models. I really do value theoretical insight of this kind. But I think significant restructuring, new evidence, and tamping down of claims would be necessary to make this paper coherent."}, "questions": {"value": "My most important questions are these:\n\n1. What new consequences/predictions does the Kelly gambler view yield? Also, can you explain it better? What does it mean for a diffusion model to \"bet\" or \"invest\"? Is there some sort of way to formalize the betting problem and show trained diffusion models solve it well?\n\n2. Can the authors clarify which of their claims (e.g., the Kelly gambler one) apply just to entropy-matching models, versus both entropy-matching models and more conventional diffusion models? This wasn't clear to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VnPtwWeEtD", "forum": "IaeZcYpRxD", "replyto": "IaeZcYpRxD", "signatures": ["ICLR.cc/2026/Conference/Submission5159/Reviewer_9PhK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5159/Reviewer_9PhK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947513402, "cdate": 1761947513402, "tmdate": 1762917914796, "mdate": 1762917914796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors analyze diffusion models through the lens of entropy and mutual information, leading to analysis of the effect of conditioning on labels via guidance or latent variables in diffusion autoencoders.   The entropy-matching perspective draws connections with stochastic thermodynamics and lends itself readily to analysis of information-theoretic quantities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I am very much in favor of information theoretic perspectives on generative models such as VAEs or diffusion models.   The entropy matching perspective in the paper lends itself nicely to reasoning about information theoretic quantities.   \n\nI like the empirical investigation of the mutual information of CFG-guided models with $w > 1$ in App. Fig 4.   Motivated by lines 263-272, this involves fitting a further conditional diffusion model to samples generated with CFG and estimating MI using Eq 13-14.  \n\nMy opinion of the paper improved dramatically upon reading through the Appendix, where intuitively reasonable qualitative statements in the paper were supported by evidence.   As discussed in Weaknesses below, I think this analysis should be more tightly integrated into the main text to further strengthen the paper.\n\n\nThe paper covers a wide range of topics related to information theoretic quantities in diffusion models, although take-home messages should be more tightly packaged (see below).   I imagine the number of topics could be reduced to provide a more focused and streamlined paper."}, "weaknesses": {"value": "I am left struggling to isolate the main contribution of the paper.    \n- Based on my reading of the paper and comments below, I believe the authors have material that *might* be used to argue that\n    - (i) analysis of the properties of mutual information in diffusion models with label vs. latent variable conditioning \n    - leads to (ii) recommendations on how to provide better conditioning mechanism for diffusion models.   \n\nHowever, the current submission seems to lack the structured arguments to lead to any such recommendations.\n\n*Grounding Conclusions in Empirical Evidence*\n\nThe paper is strongest when providing empirical evidence to link statements about MI and statements about diffusion models. Having established these links, ideally we would obtain insight into how to best condition our diffusion models in scenarios of interest.\n*Unfortunately, most empirical evidence for main text statements is relegated to the Appendix.*\n\nFor example:\n- In Lines 274-296, the point that \"perceptual details contribute the more entropy than labels\" is succinctly supported by App Fig 8 (the unconditional model might also be interesting to show alongside to strengthen the claim).  This should be in the main text.\n    - L 285-286 \"This is why diffusion models often stray from the conditioning signal during generation:  the mutual information between images and labels is intrinsically low\" could be supported by calculating MI for a standard conditional vs. unconditional model (alongside the below).\n- The saturation of CFG in App Fig 4 seems interesting to analyze and discuss in detail within this context (on images?).  For example, this could replace and/or strengthen Line 353-354 \"However, CFG does have a fundamental limitation:  if the underlying dependence between X and Y is weak, amplification of the signal can only go so far.\"\n- This would seem to provide principled motivation for an argument in favor of conditioning with learned autoencoder representations $Z$.    \n\nIt might also be interesting to consider how this story holds or changes for the common setting of conditioning with text prompts.  Dimension-wise MI has been used for analysis in [3,4]\n\n*DAE*\n\nIn Sec 5, I would appreciate more explicit tracking of the parameter $\\gamma$ and the KL divergence terms during the discussion.   Eq 20 pertains the diffusion decoder, while the discussion in lines 338-348 is not grounded in the parameter $\\gamma$.\n- L355-358 is not sufficiently clear, particularly L 355: \"a second diffusion model is trained to generate from Y the latent Z first\".  Why is this?  How does Y figure into the discussion?  Is this based on idealized behavior of the encoder $Z$ capturing 'semantic label information'? \n    - the IB objective in Eq 32 does not include both reconstruction and classification losses.    We could imagine using $z$ to both condition the diffusion reconstruction loss for $p_{\\text{diffusion}}(x|z)$ **and** an auxiliary classification loss $p(y|z)$, which would encourage compression of the label information.   This appears in many existing VAE works (among others: [1], [2] )\nAgain, more time spent making the IB discussion precise would strengthen the paper.\n\n\n\n[1] Alemi & Fisher 2018 \"TherML\", \n[2] Gao & Chaudhari 2021 \"A Free-Energy Principle for Representation Learning\"\n[3] Kong et. al ICLR 2024 \"Interpretable Diffusion via Information Decomposition\"\n[4] Dewan et. al Neurips 2024, \"Diffusion PID: Interpreting Diffusion via Partial Information Decomposition\n \n\n*Additional questions / comments:*\n- L074-105:  The link between MI and Kelly Betting seems orthogonal to the links between MI and diffusion models studied in the paper and could seemingly be removed.  That said, I was intrigued by the title!  \n    - The exposition could be more clear to state that we are betting based on knowledge of $H(X) \\neq \\log_2 6$.   I thought Lines 90-94 could be understood directly in terms of `we bet based on $H(X|Y)$, e.g. perfect knowledge'.\n\n- L289-292 re: continuous MI seems out of place, since we have been discussing MI for discrete labels Y.  I am not sure I've understood its relevance to the current claims, but it seems better placed when discussing DAEs in Sec 5 for continuous latents, if at all  \n\n- $\\mathcal{L}_{DEM}$ appears for seemingly the first time in Eq 18, perhaps this should also be used in Eq. 6.\n\n- Typos with $Z_\\text{sem}$ vs. $Z_{\\text{sem}}$ (s.b. $Z_{\\text{per}}$) in App E2.   Would appreciate further detail on how these are representations are obtained / trained."}, "questions": {"value": "I am unsure of the contribution of Sec 6 (although I did not admittedly did not understand the quantum connection) and its connection with the rest of the paper.\n- The autoencoding interpretation in Eq. 21-23 appears nearly identical to VDM [5] or the Information-Theoretic Diffusion results of [6].  The latter derives the same pointwise likelihood expression by adapting existing results relating MMSE and the derivative of MI with respect to SNR of a Gaussian Noise channel [7].    It is unclear to me what insight the exposition in L378-403 adds to this picture, \n- The use of $p_t^{(\\text{eq})}$ appears to play a similar role as the reference Gaussian data in Kong et. al 2023, but could this connection be made explicit?   This would allow moving the Infinite Tower discussion in Sec 6 up to Sec. 3 and any subsequent findings to be more tightly integrated with the autoencoder analysis in Sec 5.   \nThe latter is more speculative, but [5,6] should be cited and I am interested to better understand the insight of Sec 6 beyond the well-known denoising-mean prediction intuition in L395-401.\n\n[5] Kingma et. al, \"Variational Diffusion Models\", Neurips 2021.\n[6] Kong et. al, \"Information-Theoretic Diffusion\", ICLR 2023.\n[7] Guo et. al 2005 \"Mutual Information and Minimum Mean-Square Error in Gaussian Channels\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pm3hXvIyMN", "forum": "IaeZcYpRxD", "replyto": "IaeZcYpRxD", "signatures": ["ICLR.cc/2026/Conference/Submission5159/Reviewer_mLfw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5159/Reviewer_mLfw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137717086, "cdate": 1762137717086, "tmdate": 1762917914528, "mdate": 1762917914528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel theoretical perspective linking diffusion models to the Kelly criterion from information theory. The core idea is that when a diffusion model generates a sample $X$ conditioned on some variable $Y$, it behaves like a Kelly gambler who bets optimally using side information to maximize long-term growth, in this case, maximizing mutual information. Overall, the paper’s framing is creative but lacks sufficient novelty, theoretical rigor, and empirical evidence to substantiate its claims. Strengthening literature grounding, adding original experiments, and clarifying contributions would greatly improve it."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of this paper lies in its unifying interdisciplinary theoretical insight: it elegantly bridges generative diffusion modeling, information theory, and optimal decision-making by showing that conditional diffusion models act like Kelly gamblers. This framing clarifies how diffusion models implicitly manage information between data and conditions, recasting classifier-free guidance as an information amplification mechanism that increases mutual information between the generated sample and its condition."}, "weaknesses": {"value": "1. While the paper introduces an appealing new connecting between diffusion models and the Kelly criterion, the underlying idea of linking information theory and generative modeling is not new. Prior works such as Kong et al. [1,2] on Information-Theoretic Diffusion, Franzese et al., [3] and Yu et al. [4] have already analyzed diffusion models through a mutual information lens, addressing information transport, entropy flow, and guidance mechanisms. Moreover, several papers have used diffusion models explicitly for mutual information estimation. The current work neither cites nor situates itself clearly within this body of literature.\n\n2. Many concepts (e.g., Entropy Matching [5], DAE [6], Neural Entropy [5]) are reformulations of existing ideas. The new terms like Kelly Criterion and Infinite Tower are briefly introduced but not developed into rigorous theory or practical insight.\n\n3. Most experiments in the appendix appear reused from Neural Entropy [5] with little new validation of the proposed Kelly-gambler framework. \n\n4. The paper’s interdisciplinary framing mixes information theory, generative modeling, and physics in a dense, sometimes confusing way. Simplifying terminology and focusing on the central argument would improve readability. \n\n\n[1] Kong, Xianghao, Rob Brekelmans, and Greg Ver Steeg. \"Information-theoretic diffusion.\" arXiv preprint arXiv:2302.03792 (2023).\n\n[2] Kong, Xianghao, et al. \"Interpretable diffusion via information decomposition.\" arXiv preprint arXiv:2310.07972 (2023).\n\n[3] Franzese, Giulio, Mustapha Bounoua, and Pietro Michiardi. \"MINDE: Mutual information neural diffusion estimation.\" arXiv preprint arXiv:2310.09031 (2023).\n\n[4] Yu, Longxuan, et al. \"MMG: Mutual Information Estimation via the MMSE Gap in Diffusion.\" arXiv preprint arXiv:2509.20609 (2025).\n\n[5] Premkumar, Akhil. \"Neural entropy.\" arXiv preprint arXiv:2409.03817 (2024).\n\n[6] Preechakul, Konpat, et al. \"Diffusion autoencoders: Toward a meaningful and decodable representation.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OkY97BhkEm", "forum": "IaeZcYpRxD", "replyto": "IaeZcYpRxD", "signatures": ["ICLR.cc/2026/Conference/Submission5159/Reviewer_fA2j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5159/Reviewer_fA2j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155121377, "cdate": 1762155121377, "tmdate": 1762917914225, "mdate": 1762917914225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}