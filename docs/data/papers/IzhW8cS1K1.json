{"id": "IzhW8cS1K1", "number": 21273, "cdate": 1758315688633, "mdate": 1759896931372, "content": {"title": "Contrastive Inverse Reinforcement Learning for Highway Driving Behavior Optimization", "abstract": "Autonomous driving systems are expected to not only replicate proper human driving behavior, but also adapt to dynamic driving scenarios. Imitation learning (IL) and inverse reinforcement learning (IRL) methods are potential tools to reproduce human behaviors. \nTraditional IRL methods are not highly sample-efficient and sometimes generalize poorly, especially in autonomous driving with limited vehicle demonstrations and driving behavior distribution shifts. In this paper, we propose a Contrastive Inverse Reinforcement Learning (CIRL) framework that enhances reward learning via self-supervised contrastive representations. The proposed CIRL method improves efficiency and robustness by 1) integrating reward regularization into the contrastive loss and 2) employing momentum encoders to stabilize contrastive feature learning under driving-specific perturbations.\nFurthermore, our approach supports personalized driving policies by modeling individual driving styles using a small number of vehicle demonstration data. Extensive experiments on the NGSIM US-101 and I-80 highway dataset demonstrate that the proposed CIRL framework consistently outperforms state-of-the-art IRL methods, achieving improvements of 12.5\\% in human-likeness, 86.2\\% in safety, and 17.8\\% in generalization to new environments. In addition, the ablation study of key designs further validates the necessity of each key component, confirming that momentum encoding, reward regularization, and learnable similarity functions collectively contribute to CIRL’s robust and generalizable performance in real-world driving scenarios.", "tldr": "", "keywords": ["Inverse reinforcement learning", "contrastive learning", "highway drving behavior", "driving optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f0e078845d61ca46ed56fcebd7027c29b5f9f89e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a new method for autonomous driving based on IRL and contrastive learning. To improve the robustness of IRL, the paper integrates reward regularization into the contrastive loss and use momentum encoders. Experiments on the NGSIM US-101 and I-80 highway dataset shows the paper achives better performannce than previous IRL methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea to combine constrative learning and IRL is novel.\n2. The method achives better performance on the the NGSIM US-101 and I-80 highway dataset."}, "weaknesses": {"value": "1. The motivation to use the momentum encoder and how to obtain the augmented states is unclear.\n2. The  NGSIM US-101 and I-80 highway dataset is too simple. Consider evaluating on the more challenging nuplan benchmark.\n3. No driving videos are provided for better understanding."}, "questions": {"value": "1. In figure 3, the yellow cluster corresponds to 980 expert scenes, and purple points represent 6000 random scenes. Why not compare the policy generated scenes, which is more related to driving performance?\n2. Does other method uses the constrative features leading to better robustness like GAIL and AIRL?\n3. Address the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fJzeeJuXa4", "forum": "IzhW8cS1K1", "replyto": "IzhW8cS1K1", "signatures": ["ICLR.cc/2026/Conference/Submission21273/Reviewer_JAAN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21273/Reviewer_JAAN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21273/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760517747103, "cdate": 1760517747103, "tmdate": 1762941664669, "mdate": 1762941664669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the CIRL (Contrastive Inverse Reinforcement Learning) framework for highway driving behavior optimization, aiming to improve the efficiency, robustness, and generalization of IRL methods. Traditional IRL approaches in autonomous driving often suffer from low sample efficiency and poor generalization. CIRL enhances reward learning through the following key mechanisms: (1) integrating an L2 reward regularization term into the contrastive loss to ensure that features are both discriminative and reward-consistent; (2) employing a momentum encoder to stabilize contrastive feature learning, thereby improving the model’s robustness to perturbations in the driving environment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) CIRL successfully incorporates contrastive learning into the Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL) framework, significantly enhancing the learned reward function’s robustness and generalization to noise, distribution shifts, and sparse data.\n(2) In experiments on the US-101, CIRL achieves extremely low crash rates (CR) and termination rates (TR) under both general and personalized settings, surpassing all baselines in safety metrics. Moreover, the method maintains high performance even when trained with only a few demonstrations (e.g., 15 trajectories) or a small number of vehicles (e.g., 5 cars), demonstrating excellent sample efficiency."}, "weaknesses": {"value": "(1) Introducing an L2 reward regularization term in contrastive learning enforces that the reward values between augmented states are exactly equal. However, in MaxEnt-IRL, only the relative values of the reward function (equivalence class) are meaningful. This constraint on absolute value consistency is theoretically unnecessary and may interfere with the IRL optimization process.\n(2) CIRL defines the similarity function using a learnable weight matrix W. Although ablation studies confirm its superior performance, the paper lacks an in-depth analysis of how W captures a generalized reward structure within the driving state space."}, "questions": {"value": "CIRL significantly outperforms human driving demonstrations in safety metrics such as collision rate (CR). Please discuss whether this improvement in safety performance (i.e., the policy becoming safer) comes at the cost of reduced human-likeness (HL) in driving behavior."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9BA5smbdcP", "forum": "IzhW8cS1K1", "replyto": "IzhW8cS1K1", "signatures": ["ICLR.cc/2026/Conference/Submission21273/Reviewer_e9Qs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21273/Reviewer_e9Qs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21273/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874177104, "cdate": 1761874177104, "tmdate": 1762941664277, "mdate": 1762941664277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper contributes a contrastive IRL algorithm for highway driving behavior optimization. It uses a contrastive loss with reward regularization for representation learning, and a momentum encoder for stabilizing contrastive features. Experiments show that the proposed algorithm achieves strong performance, and ablation studies show that the each key algorithmic component is helpful."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Autonomous driving is an important application.\n* The proposed method is simple but achieves strong empirical performance.\n* The writing is generally good, with a well-written abstract, but there are various issues for the latter parts, as discussed below."}, "weaknesses": {"value": "The paper starts with a well-written abstract and introduction, but the writing then becomes sloppy at quite a few places, as illustrated below.\n\nIn the definition of MDP, \"transition dynamics\" is not defined, and the sentence for the reward function is awkward and jumps directly to a linear reward function without any explanation, while Figure 1 suggests that a reward network is used so the reward is not linear.\n\nLines 109-111 mentions that MaxEnt IRL but it is not sufficiently explained and also seems to be unnecessary at this point. MaxEnt IRL is mentioned again in Eq. (5), but the associated distribution is undefined.\n\nFigure 1: \"Prior knowledge\" appear in the figure, but this doesn't seem to appear in the text.\n\nThere are various issues regarding the \"Convergence Proof of Eq. equation 4. See Appendix A.2.\" at line 199. This seems to come from nowhere. Looking at A.2, the presentation needs improvement too. There should be at least a statement on what \"convergence\" is being proved at the very beginning. The assumption on non-negativitiy of the loss is unnecessary, and the validity of the assumption on the L-smoothness is unclear. In addition, the proof seems to be for gradient descent rather than stochastic gradient descent. Overall, the analysis seems to be done not for the sake of necessity but for the sake of appearance of sophistication.\n\nAlgorithm 1 is somewhat cryptic. For example, line 1 is just \" Feature Extraction: zq , zk+\". The use of a reward network  \n\nMinor comments\n* \"reference Huang et al. (2023; 2021)\" and similar: \"reference\" is not needed.\n* Hybrid IRLRen et al. (2024) and similar: reference should be inside brackets.\n* \"An expert trajectory is defined as $\\zeta$\": clarify what exactly is in $\\zeta$."}, "questions": {"value": "Please refer to weaknesses and clarify if my understanding is incorrect."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5Za1j6V85T", "forum": "IzhW8cS1K1", "replyto": "IzhW8cS1K1", "signatures": ["ICLR.cc/2026/Conference/Submission21273/Reviewer_qTpt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21273/Reviewer_qTpt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21273/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998309114, "cdate": 1761998309114, "tmdate": 1762941663980, "mdate": 1762941663980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Contrastive Inverse Reinforcement Learning (CIRL) framework for autonomous highway driving behavior optimization. The method combines self-supervised contrastive feature learning with maximum entropy inverse reinforcement learning (MaxEnt IRL). It introduces (1) a reward-regularized contrastive objective to align learned representations with human behavioral rewards, and (2) momentum encoders to stabilize training and mitigate distributional shifts. Experiments on the NGSIM US-101 and I-80 datasets show that CIRL outperforms state-of-the-art IRL baselines (GAIL, AIRL, MEIRL, Hybrid IRL, EscIRL) in terms of human-likeness, safety, and cross-environment generalization. The ablation studies further confirm the contribution of each module (momentum encoder, reward regularization, and learnable similarity). The paper also demonstrates personalized driving style adaptation with limited demonstrations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Innovative integration of contrastive learning and IRL with a focus on real-world robustness and sample efficiency.\n2. The reward regularization term aligns latent representations with behavioral semantics, leading to improved interpretability.\n3. The momentum encoder mechanism stabilizes learning and mitigates overfitting under distribution shifts.\n4. Extensive experiments across two realistic driving datasets (US-101 and I-80) demonstrate consistent improvements in human-likeness (+12.5%), safety (+86.2%), and generalization (+17.8%).\n5. Comprehensive ablation studies show the necessity of each design component.\n6. The framework supports personalized driving policy learning from small demonstration sets, a valuable real-world feature.\n7. The paper is empirically strong and demonstrates practical feasibility for autonomous systems.\n8. Code and experimental design are reproducible based on the provided details (assuming release)."}, "weaknesses": {"value": "1. Theoretical justification is shallow. The connection between contrastive representations and reward function learning is described empirically rather than analytically.\n2. Reward regularization assumption (ℓ² alignment between augmented states) may be unrealistic in dynamic traffic, where slight augmentations can alter intent or safety conditions.\n3. Hyperparameter sensitivity (momentum factor m, β, λ) and robustness analysis are missing.\n4. Statistical significance and variance of results (e.g., over multiple random seeds) are not reported.\n5. Negative sample selection in contrastive training is underspecified, potentially impacting reproducibility.\n6. Reward network architecture and training stability metrics are not provided.\n7. The distinction from EscIRL (2025) and other recent contrastive IRL methods is somewhat incremental.\n8. The dataset scale (20 vehicles training, 20 testing) may be insufficient to generalize to large-scale real traffic patterns.\n9. The convergence proof (Appendix A.2) is standard and does not address the joint optimization of encoders and reward network.\n10. Failure cases or limitations (e.g., over-cautious driving, failure in edge cases) are not discussed."}, "questions": {"value": "1. How are negative pairs selected in the contrastive loss? Are they from the same vehicle at different times or across different vehicles?\n2. How sensitive is the performance to the momentum coefficient m and the regularization weight β?\n3. Could the authors provide quantitative comparisons of convergence stability (e.g., variance of training loss across runs)?\n4. Does the reward regularization risk suppressing small but meaningful behavioral differences between similar states?\n5. How does CIRL perform in non-highway or multi-agent dense traffic scenarios (e.g., urban intersections)?\n6. Is there any theoretical link between the learned contrastive embedding and the maximum entropy reward structure?\n7. Would combining CIRL with visual or LiDAR-based state representations (beyond NGSIM trajectories) improve robustness?\n8. How does the personalized driving mode adapt to conflicting driver preferences or inconsistent demonstrations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UDitaQ5APU", "forum": "IzhW8cS1K1", "replyto": "IzhW8cS1K1", "signatures": ["ICLR.cc/2026/Conference/Submission21273/Reviewer_N9Ao"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21273/Reviewer_N9Ao"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21273/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999842695, "cdate": 1761999842695, "tmdate": 1762941663745, "mdate": 1762941663745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}