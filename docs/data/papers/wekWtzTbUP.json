{"id": "wekWtzTbUP", "number": 3833, "cdate": 1757544426453, "mdate": 1759898067612, "content": {"title": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion Models", "abstract": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing Large Language Models (LLMs), where both pre-training and RL post-training stages are grounded in the same log-likelihood formulation. In contrast, recent RL approaches for diffusion models, most notably Denoising Diffusion Policy Optimization (DDPO), optimize an objective different from the pretraining objectives--score/flow matching loss.\nIn this work, we establish a novel theoretical analysis: DDPO is an implicit form of score/flow matching with noisy targets, which increases variance and slows convergence. Building on this analysis, we introduce Advantage Weighted Matching (AWM), a policy-gradient method for diffusion. It uses the same score/flow-matching loss as pretraining to obtain a lower-variance objective and reweights each sample by its advantage. In effect, AWM raises the influence of high-reward samples and suppresses low-reward ones while keeping the modeling objective identical to pretraining. This unifies pretraining and RL conceptually and practically, is consistent with policy-gradient theory, and reduces variance, yielding faster convergence.\nThis simple yet effective design yields substantial benefits: on the GenEval, OCR, and PickScore benchmarks, AWM delivers up to a $24\\times$ speedup over Flow-GRPO (which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX, without compromising generation quality.", "tldr": "", "keywords": ["Diffusion Model; Reinforcement Learning; Flow Matching"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/94c645d021fb0258ae1876308017660e53cd14e2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Advantage Weighted Matching (AWM), a method for applying reinforcement learning to diffusion models by directly optimizing the pre-training score/flow matching objective. The central claim is that prior methods like Denoising Diffusion Policy Optimization (DDPO) are inefficient because they implicitly optimize a score-matching objective with noisy targets, which leads to high-variance gradients. The authors' proposed solution, AWM, avoids this issue by re-weighting the standard \"clean-data\" score matching loss with an advantage term derived from a policy-gradient algorithm. The paper demonstrates that this approach leads to significant wall-clock speedups (up to 24x) over a Flow-GRPO baseline on benchmarks for compositionality, text rendering, and human preference, while achieving similar final performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, and the proposed method is empirically effective. The primary strengths are:\n\nStrong Empirical Results:  the paper demonstrates significant training speed-ups. An 8-24x reduction in GPU hours is a substantial engineering improvement that makes RL fine-tuning for diffusion models more practical and accessible. The experiments are conducted on relevant, modern architectures (SD3.5M, FLUX) and standard benchmarks.\n\nThe paper offers a plausible explanation for the inefficiency of DDPO by linking it to a high-variance, noisy-target score-matching objective."}, "weaknesses": {"value": "Despite the impressive speed-up numbers, there are significant concerns regarding the paper's overall contribution and the soundness of its core claims:\n\nLimited Novelty of the Core Idea: The central mechanism of AWM—multiplying a regression loss by a reward-based weight—is a direct application of the well-known Reward-Weighted Regression (RWR) principle. The paper acknowledges prior and concurrent work (e.g., Lee et al. 2023, McAllister et al. 2025) that proposes nearly identical mechanisms. The contribution of this paper appears to be an application of this known technique to an online policy-gradient framework, which feels more incremental than foundational. The novelty seems to lie more in the theoretical \"framing\" of the problem rather than in the method itself.\n\nSignificant Discrepancies Between Theory and Practice: The theoretical argument, while elegant, shows some critical disconnects from the practical implementation, undermining its explanatory power:\nThe authors state that AWM \"unifies pretraining and RL,\" yet they find that the theoretically-grounded ELBO weighting from pre-training must be discarded in favor of a simple uniform weighting (w(t)=1) to achieve good results. This suggests the theoretical alignment is weaker than claimed and that the method's success relies on an empirical heuristic rather than the core theory.\nThe main theoretical result (Theorem 1) relies on \"omitting the discretization error.\" This is a strong assumption that sidesteps the complexities of the discrete-time sampling process that all practical models use. This simplification weakens the claim that the theory accurately describes the behavior of real-world implementations.\n\nInsufficient Experimental Depth: The experiments focus heavily on training speed but lack depth in other critical areas. There is no qualitative analysis of failure modes, which is essential for understanding the trade-offs of the new method. For example, does the more direct optimization of AWM lead to a reduction in output diversity or make it more susceptible to reward hacking? Without this analysis, it is difficult to assess whether the speed-up comes at a hidden cost. The comparison to Flow-GRPO could also be strengthened by providing more details on the hyperparameter tuning for the baseline."}, "questions": {"value": "Given the conceptual similarity to Reward-Weighted Regression, could the authors please clarify what they see as the primary novel contribution of AWM beyond applying this principle to an online RL setup for diffusion models? Why should this be considered more than an effective application of a known technique?\n\nThe disconnect between the ELBO theory and the practical success of uniform time-weighting is a major concern. Does this not undermine the central claim that AWM successfully \"realigns\" RL with the pre-training objective? Could the success of AWM be less about aligning with the pre-training likelihood objective and more about the general stability of directly optimizing a single-step objective compared to DDPO's multi-step approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0ZExWbDY2T", "forum": "wekWtzTbUP", "replyto": "wekWtzTbUP", "signatures": ["ICLR.cc/2026/Conference/Submission3833/Reviewer_FSZB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3833/Reviewer_FSZB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761596146618, "cdate": 1761596146618, "tmdate": 1762917055322, "mdate": 1762917055322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Advantage Weighted Matching (AWM), a training algorithm for the reinforcement learning (RL) fine-tuning of diffusion models.  The authors note that Denoising Diffusion Policy Optimization (DDPO) implicitly performs score matching with noisy targets, which increases variance and slows down convergence. AWM is then proposed to address this issue by applying advantage-weighted score/flow matching. Experimental results show that AWM achieves significant speedups over Flow-GRPO."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper discusses the relationship and connection between DDPO  and DSM (Denoising Score Matching). \n\n2. Experimental experiments show that the proposed AWM method achieves acceleration across multiple models, such as SD3.5M and FLUX. \n\n3. It is acknowledged in Appendix C.1.3 that AWM uses the same ELBO policy proxy  as  the work FPO (FMPG), referred to as concurrent work, but differs in its theoretical motivation and experiments."}, "weaknesses": {"value": "1.  The paper repeatedly claims that AWM  *decouples training from sampling* and *supports any sampler and noise level* as a core advantage over DDPO. However,  no evidence is provided to substantiate this claim, as all experiments use only Euler-Maruyama sampling.   \n \n\n2. The paper's claim of consistency with *policy gradient theory* is questionable. The authors use a uniform weight \n$w(t)=1$  instead of the correct ELBO weight, introducing bias into the importance-weighted ratio. This results in a biased estimator of the true likelihood ratio, violating the foundation of policy gradient methods. While the paper acknowledges this discrepancy, but not theoretically addressed. \n \n\n3. The paper seems to claim that AWM is a lower-variance version of DDPO by representing formulation, but they optimize different objectives: DDPO uses a per-step policy with temporal credit, as the authors convey, while AWM uses a sequence policy without temporal credit. This isn't variance reduction but rather a fundamentally different approach. The theoretical contribution remains unclear, as the reported 24× speedup lacks a breakdown into: (1) computational cost (1 loss vs. T likelihoods), (2) variance reduction, and (3) implementation differences (e.g., LLaDA 1.5 tricks mentioned on line 310 but not ablated).\n\n\n4.  The paper's presentation significantly hinders a clear understanding of the methodology. It not only fails to provide a clear comparison of the loss function for the Flow-GRPO baseline, but it also fails to clearly present the loss objective for AWM.\n\n5.  The paper's notation and logic are flawed and difficult to follow. For example, although the paper claims to select a flow-matching noise schedule in Sec. 2.1, the manuscript repeatedly switches between $v_\\theta$, $s_\\theta$, and $D_\\theta$.  Some notations appear rigidly without any explanation, such as  $\\pi_{\\theta_{\\text{old}}}$ and $v_{\\theta_{\\text{ref}}}$.  Additionally, some formulas are unclear, such as the awkward appearance of Eq. (8)."}, "questions": {"value": "1.  Why does the paper lack comparisons to any preference alignment baselines, aside from Flow-GRPO?\n\n2.   Does the proposed method tend to sacrifice generation diversity for high rewards more than Flow-GRPO's per-step strategy?  \n\n3.  Some claims are vague and misleading. For example, the paper states, \"Euler–Maruyama can increase estimator variance, and using an exponential integrator yields the *analytical variance*.  Is this conclusion valid under the same conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8BkSS4TC8L", "forum": "wekWtzTbUP", "replyto": "wekWtzTbUP", "signatures": ["ICLR.cc/2026/Conference/Submission3833/Reviewer_ZC3a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3833/Reviewer_ZC3a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735823887, "cdate": 1761735823887, "tmdate": 1762917054899, "mdate": 1762917054899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a key misalignment between RL and pretraining in diffusion models, unlike in the field of LLMs where both stages share a consistent log-likelihood objective. This mismatch leads to high variance and slow convergence. The authors theoretically prove that DDPO is equivalent to performing DSM with noisy data and that this noisy conditioning inflates variance. To resolve this, they propose Advantage-Weighted Matching (AWM), which applies advantage weighted score or flow matching to preserve the pretraining objective while amplifying high-reward samples."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe proof that DDPO is equivalent to noisy DSM spells out why existing RL methods for diffusion models suffer from high variance and slow convergence. This analysis is important and fills a gap in understanding diffusion RL.\n2.\tAWM reuses the same DSM/FM objective as diffusion pretraining and adds advantage based weights. This symmetry makes RL for diffusion conceptually similar to RL for LLMs and allows training and sampling to be decoupled, enabling flexible samplers.\n3.\tAWM reduces the variance of policy gradients and achieves significant speed ups in experiments without sacrificing quality. The speed up is particularly important for computationally intensive diffusion models."}, "weaknesses": {"value": "1.\tThe DDPO to DSM equivalence theorem relies on the time‑reversal property of diffusion processes and explicitly ignores the Euler–Maruyama discretization error. This assumption simplifies the proof but is not generally valid during practical training, where finite step sizes and non‑Gaussian noise schedules can significantly affect the reverse‑process likelihood. So, this equivalence may break down in real implementations.\n2.\tThere is a potential for weight collapse. Especially when rewards have high variance, advantage weighting might disproportionately emphasize a small subset of samples, potentially harming diversity. The paper does not discuss strategies to prevent such weight collapse or to normalize advantages.\n3.\tAWM is positioned as sampler-agnostic and timestep-decoupled, but the experiments predominantly use a single sampler. Validating with diverse ODE or SDE samplers and few-step distilled samplers would substantiate the claimed flexibility.\n4.\tThe paper mentions ReFL, DRaFT and FMPG but does not provide direct empirical or analytical comparisons with them. Since these methods represent major alternative directions for diffusion RL, i.e., reward backpropagation and ELBO-based policy gradients, a deeper comparison would stand out AWM’s complementary strengths, such as variance reduction and sampler-agnostic training. Only comparing against Flow-GRPO is limited."}, "questions": {"value": "1. How does AWM’s performance depend on the noise and bias in the reward function? For example, would inaccurate or sparse rewards lead to unstable advantage weights?\n\n2. The paper evaluates SD3.5M and Flux. How does AWM scale to larger models (e.g., SD‑XL) or to diffusion models for video, audio, or multimodal tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8wSebw9UoS", "forum": "wekWtzTbUP", "replyto": "wekWtzTbUP", "signatures": ["ICLR.cc/2026/Conference/Submission3833/Reviewer_abUX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3833/Reviewer_abUX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977407619, "cdate": 1761977407619, "tmdate": 1762917054707, "mdate": 1762917054707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and studies an diffusion RL post-training approach -- Advantage Weighted Matching (AWM). By reweighting each sample by its advantage, this approaches achieves low variance and faster convergence. Experiments have been conducted on Stable diffusion 3.5 and Flux."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written, and I mostly enjoyed reading it. The idea is clear: reweighting the sample by it advantage relying on the GRPO pipeline. The numerical experiments also support the proposed approach."}, "weaknesses": {"value": "There are several weaknesses that I hope the authors can address and explain.\n\n(1) It seems to me that incorporating the advantage in policy gradient is natural (line 316), and this idea has been explored elsewhere. The authors may explain what is the main novelty here.\n\n(2) Lemma 1: this result is well known (see e.g., Score-based diffusion models via stochastic differential equations, Statist. Surv., Tang and Zhao, Thm 4.3), and in the original paper of Vincent. The authors may simply refer to there.\n\n(3) Thm 1: the viewpoint that DDPO is doing denoising score matching does not seem to be a \"secret\". See e.g., \nReward-Directed Score-Based Diffusion Models via q-Learning, arXiv:2409.04832, Gao, Zha and Zhou;\nScore as Action: Fine-Tuning Diffusion Generative Models by Continuous-time Reinforcement Learning, ICML25, Zhao et al.\n\n(4) Experiments: the authors did experiments using the PickScore. I wonder if the results are similar in other metrics, e.g., HPSv2, Vendi score and ImageReward? \n\n(5) Figure 7(b): it is interesting that for beta = 0.2, the training curve goes done rapidly; while for beta = 0.4 it works well. I wonder if there is a \"threshold\" (between 0.2 and 0.4) below which the curve fails to improve."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ulVH2MFb45", "forum": "wekWtzTbUP", "replyto": "wekWtzTbUP", "signatures": ["ICLR.cc/2026/Conference/Submission3833/Reviewer_pVy6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3833/Reviewer_pVy6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027409657, "cdate": 1762027409657, "tmdate": 1762917054543, "mdate": 1762917054543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}