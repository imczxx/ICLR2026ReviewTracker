{"id": "7n5eoDZdZ5", "number": 10945, "cdate": 1758185225491, "mdate": 1759897619605, "content": {"title": "FLoRA-NA: Nearly Accurate Aggregation for Federated Low-Rank Adaptation", "abstract": "With the rapid emergence of foundation models and the increasing need for fine-tuning across distributed environments, Federated Low-Rank Adaptation (FedLoRA) has recently gained significant attention. Despite its potential, current FedLoRA methods face notable challenges due to inexact updates. Existing approaches have attempted to mitigate this issue, but they often introduce a \\emph{local-global generalization gap} and incur \\emph{substantial communication overhead}, limiting their scalability and effectiveness.  \nTo address these limitations, we propose \\textbf{F}ederated \\textbf{Lo}w-\\textbf{R}ank \\textbf{A}ggregation with \\textbf{N}early \\textbf{A}ccurate Estimation (FLoRA-NA). FLoRA-NA leverages the local LoRA matrices on the server to estimate the aggregated matrices $\\hat{A}$ and $\\hat{B}$, which are then distributed to clients for local updates. This surrogated aggregated matrices minimizes the divergence between ideal $\\bar{W} = \\sum^{U}_{u=1}B_u A_u$ and practical updates $\\hat{W} = \\hat{B}\\hat{A}$ without adding communication cost beyond vanilla FedLoRA. By doing so, FLoRA-NA achieves communication efficiency and bridges the gap between local personalization and global generalization, addressing a key limitation of prior personalized FedLoRA approaches.\nWe conduct extensive evaluations across diverse tasks, including natural language understanding, mathematical reasoning, and code-solving ability using various foundation models. Experimental results consistently demonstrate that FLoRA-NA achieves state-of-the-art performance while maintaining low communication overhead.", "tldr": "This paper introduces a nearly accurate, communication efficient Federated LoRA aggregation that bridges the gap between local and global generalization.", "keywords": ["Federated Learning", "Low-rank Adaptation", "Data heterogeneity", "language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/193efdefd529c1aba6d678d3c49deade2a813ef5.pdf", "supplementary_material": "/attachment/55cbdd53ca24611da9f40d9a2024da2d6d3a0a7c.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes Federated Low-Rank Aggregation with Nearly Accurate Estimation (FLoRA-NA). FLoRA-NA leverages the local LoRA matrices on the server to estimate the aggregated matrices \\\\(\\hat{A}\\\\) and \\\\(\\hat{B}\\\\), which are then distributed to clients for local updates, to address the aggregation errors in FL with LoRA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed FLoRA-NA effectively addresses the aggregation errors of FL with LoRA by approximating the \"ideal\" aggregation.\n- The paper is well-written and easy to understand.\n- The extensive experiments validate the effectiveness of the proposed method."}, "weaknesses": {"value": "- How to solve the optimization problem in Equation (6) is not clearly explained.\n- Lines 337-340, \"SVD-based methods may introduce numerical inaccuracies due to floating-point precision errors, potentially degrading the overall performance of FedLoRA.\" It's better to add some performance comparisons in Table 1."}, "questions": {"value": "- Lines 196-195, \"These vectors can be interpreted as transformation vectors that determine how each client‚Äôs local LoRA gradients are linearly combined during aggregation.\" Why can it be considered as a linear combination?\n- Why is there such a large discrepancy between the local and global performance of FFA-LoRA? Aren't the local and global models the same? Is it due to the differences in the test data for local and global models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V408qsTdeM", "forum": "7n5eoDZdZ5", "replyto": "7n5eoDZdZ5", "signatures": ["ICLR.cc/2026/Conference/Submission10945/Reviewer_K9VB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10945/Reviewer_K9VB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761293378359, "cdate": 1761293378359, "tmdate": 1762922139755, "mdate": 1762922139755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FLoRA-NA, a ‚Äúnearly accurate‚Äù aggregation method for federated LoRA fine-tuning. The authors argue that existing FedLoRA methods suffer from aggregation errors when averaging low-rank matrices across clients. To mitigate this, FLoRA-NA introduces auxiliary weighting vectors ( P, Q ) to linearly combine local LoRA matrices, claiming to approximate the true aggregated matrix product without additional communication cost. The method is backed by convergence analyses and evaluated on standard NLP, reasoning, and code datasets."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Empirical thoroughness: The experiments are broad and cover many baselines, including federated LoRA, DoRA, and HiRA variants, on both language and reasoning tasks.\n\n\n- Nice writing and clear presentation: The narrative is coherent and polished, and the paper is written well."}, "weaknesses": {"value": "`W1: The core technical idea is not convincing`\n\nThe claimed ‚Äúnearly accurate aggregation‚Äù is essentially a weighted averaging of LoRA matrices with learned coefficients ( P, Q ). This approach is neither theoretically optimal nor computationally cheaper compared to well-established matrix approximation methods.\n \nIf the goal is to minimize $| \\bar{B}\\bar{A} - \\tfrac{1}{U} \\sum_u B_u A_u |,$, then taking truncated SVD or other low-rank approximation methods provably yield the best low-rank reconstruction in Frobenius norm. The proposed optimization over two scalar vectors $ P, Q \\in \\mathbb{R}^U $ is an ad-hoc and suboptimal heuristic, without any proof that it approximates the optimal low-rank structure. \n\n\nThe paper never explains why this weighting approach should outperform SVD-based aggregation, nor provides any theoretical justification beyond hand-waving about computational efficiency. The argument that ‚ÄúSVD introduces numerical inaccuracies‚Äù is very weak, in practice, taking truncated SVD is stable and extremely fast.\n\n`W2: Execution time comparison is misleading`\n\nIn Table 1, SVD and Gram-Schmidt are reported as being 10‚Äì15 times slower than FLoRA-NA. However, the comparison is unfair: taking truncated SVD matrices would in fact run much faster than full SVD (10s of times faster or so), and this is confirmed in other works like LoRA-GA (https://arxiv.org/abs/2407.05000), without any loss in performance.\n\nThus, the claim of computational efficiency is unsubstantiated and incorrect, in my opinion.\n\n`W3: Overclaiming novelty`\n\nMost of the claimed advantages (no extra communication, better generalization) stem simply from tuning a linear combination of existing LoRA matrices. This is closer to a reweighted averaging trick than a novel algorithmic contribution. The ‚Äúnearly accurate‚Äù branding oversells what is, in essence, a parameter-weighted variant of FedAvg.\n\nThe idea of using truncated SVD as the best inexact approximation has already been established in FedEx-LoRA, which makes the proposed approach in this paper appear redundant.\n\n`W4: Conceptual inconsistency in results`\n\nThe paper claims FLoRA-NA achieves ‚Äúnearly exact‚Äù aggregation, yet FedEx-LoRA, which performs exact aggregation, yields lower accuracy (Table 2). This is not explained logically, the authors state it is because ‚Äú the residual error is added to the frozen pretrained model, which ensures the accuracy of forward update, but cannot fully participate in gradient updates of A and B matrices.,‚Äù but this line is vague and unconvincing.\n\nIf the proposed method is truly more ‚Äúaccurate,‚Äù it should not outperform an exact method unless the claim itself is overstated.\nCould you please clarify this?\n\n`W5: Missing related work`\n\nThe paper fails to discuss recent federated low-rank methods such as Ravan (https://arxiv.org/abs/2506.05568) and Fed-SB (https://arxiv.org/abs/2502.15436), which provide a clear communication‚Äìaccuracy Pareto frontier compared to this and other works - at a quick glance. This omission weakens the positioning of FLoRA-NA as a ‚Äústate-of-the-art‚Äù solution. The authors should include comparisons with these works, or at the very least, discuss them.\n\n---\n\nWhile the empirical coverage in the work is impressive, the core contribution is mathematically weak, and the claimed superiority over SVD-based methods is not supported by any evidence (theoretical or empirical). The paper‚Äôs theoretical and experimental arguments do not convincingly establish that FLoRA-NA offers any principled or practical advantage."}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8McdcSupVR", "forum": "7n5eoDZdZ5", "replyto": "7n5eoDZdZ5", "signatures": ["ICLR.cc/2026/Conference/Submission10945/Reviewer_6558"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10945/Reviewer_6558"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761309798429, "cdate": 1761309798429, "tmdate": 1762922139250, "mdate": 1762922139250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits FedLoRA and argues that separately averaging the low-rank matrices ùê¥ and ùêµ on the server introduces an aggregation bias. The authors propose FLoRA-NA, which learns aggregation weights to approximate the ideal bilinear average and derive a convergence bound including this bias term. Experiments on NLP and reasoning benchmarks show moderate improvements over standard FedLoRA. The idea is intuitive and empirically effective, but conceptually incremental and lacks strong causal or theoretical justification."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly identifies a practical limitation of FedLoRA aggregation and formalizes it as an explicit bias term. The proposed solution is simple, communication-efficient, and easy to implement.\nExperiments are well-organized and consistently show moderate but stable performance gains across multiple benchmarks."}, "weaknesses": {"value": "The paper‚Äôs main motivation that the standard FedLoRA aggregation rule BA=1/U‚àëBuAu introduces a harmful aggregation bias is intuitively appealing, but the presented evidence and methodology raise several concerns:\n\n1. Lack of causal proof that aggregation bias harms performance.\nThe paper defines an error term ùúå = ‚à•ùêµÀâùê¥Àâ‚àí1/ùëà‚àëùêµùë¢ùê¥ùë¢‚à• and incorporates it into a convergence upper bound. However, this analysis merely indicates that model convergence may depend on œÅ; it does not prove that a non-zero œÅ necessarily degrades performance. No lower-bound analysis, sensitivity test, or monotonicity relation is provided, making the claim that ‚Äúthe bias is inherently harmful‚Äù insufficiently supported.\n\n2. Possible confounding with non-IID data heterogeneity.\nThe reported improvements could stem from alleviating data heterogeneity rather than from the aggregation rule itself. Since non-IID strength is not controlled and there is no oracle baseline (directly computing 1/ùëà‚àëùêµùë¢ùê¥ùë¢), the experiments cannot disentangle aggregation effects from data heterogeneity. To establish a causal link, the authors should fix the data partition and vary only the aggregation operator while measuring the correlation between œÅ and performance.\n\n3. Overstated claim that the standard FedLoRA rule is ‚Äúerroneous.‚Äù\nThe paper repeatedly describes the classical FedLoRA averaging as ‚Äúerroneous‚Äù or ‚Äúfundamentally flawed.‚Äù Yet the analysis only shows algebraic non-equivalence, not that this difference is detrimental under general conditions. Without stronger theoretical or experimental isolation, such a statement appears exaggerated.\n\n4. Limited methodological novelty.\nThe idea of learning aggregation weights on the server side is far from new. Similar learnable aggregation or re-weighted averaging strategies have been extensively explored in earlier federated optimization, distillation, and low-rank model aggregation works (2018‚Äì2021). The proposed approach learning separate coefficients \nùëÉ,ùëÑ for the A and B matrices is essentially a linear re-weighted averaging scheme adapted to the LoRA setting. Hence, the contribution is largely engineering-oriented reuse of an existing idea rather than a conceptual innovation.\n\n5. Suggested clarifications.\nExplicitly quantify how œÅ affects optimization error or final convergence (e.g., linear vs. sublinear dependence).\nAdd an oracle baseline that uses the true average 1/ùëà‚àëùêµùë¢ùê¥ùë¢ to decouple non-IID effects. Discuss how this approach fundamentally differs from prior learnable aggregation methods rather than merely showing empirical improvements.\n\n6. Alternative perspective: directly addressing non-IID heterogeneity may be more fundamental.\nThe paper attributes performance degradation to inaccurate matrix aggregation, but an equally plausible explanation is that client-specific differences in ùê¥ùë¢,ùêµùë¢ caused by non-IID data are the real source of the problem. Methods such as FRLoRA (ICLR 2025), which reduce client drift through residual accumulation and subspace alignment, inherently make client updates more consistent thereby indirectly shrinking œÅ without modifying the aggregation rule itself. In this sense, tackling heterogeneity at its root might offer a more principled and effective solution than merely adjusting the aggregation formula."}, "questions": {"value": "The motivation and theoretical justification remain unconvincing. The claim that the standard FedLoRA aggregation rule is ‚Äúfundamentally wrong‚Äù is overstated and lacks causal evidence. The proposed learnable-weight scheme is not methodologically novel and revisits well-studied ideas under a new name. Moreover, non-IID heterogeneity is not controlled and could fully explain the observed performance gap. In contrast, approaches like FRLoRA that directly mitigate client drift may address the root cause more effectively. Overall, the work appears to be an engineering refinement rather than a conceptual breakthrough."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YHh6SzNPcZ", "forum": "7n5eoDZdZ5", "replyto": "7n5eoDZdZ5", "signatures": ["ICLR.cc/2026/Conference/Submission10945/Reviewer_SjRG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10945/Reviewer_SjRG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640829731, "cdate": 1761640829731, "tmdate": 1762922138733, "mdate": 1762922138733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FLoRA-NA, a federated low-rank adaptation method designed to improve update accuracy without increasing communication cost. It estimates aggregated LoRA matrices on the server to reduce the divergence between ideal and practical updates, thereby narrowing the local‚Äìglobal generalization gap. Experiments across language understanding, reasoning, and code tasks show that FLoRA-NA achieves state-of-the-art global performance with strong communication efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a well-defined problem in FedLoRA, i.e., the inexact update issue and the resulting local‚Äìglobal generalization gap.\n2. Extensive experiments across diverse tasks such as language understanding, reasoning, and code generation demonstrate consistent improvements over strong baselines.\n3. The paper provides a clear theoretical analysis."}, "weaknesses": {"value": "1. The introduced matrices P and Q may increase communication costs, but a detailed analysis is missing.\n2. The experiments focus mainly on language-related tasks; evaluations on other modalities, e.g., vision-language, would better demonstrate the method‚Äôs generality.\n3. The paper does not discuss the limitations of the proposed method, which would be valuable for understanding its potential weaknesses and applicability boundaries."}, "questions": {"value": "1. The manuscript ignores important implementation details concerning the optimization of the introduced matrices P and Q. In particular, Algorithm 1 does not show how P and Q are updated (e.g., update rules, gradients, local vs. server updates, or whether they are trained jointly with other parameters). Please clarify these points and update Algorithm 1 (or provide a supplement) to include explicit optimization steps.\n\n2. Are the matrices P and Q layer-wise, or are they shared across multiple layers? Please clarify their scope and how they are applied in the model.\n\n3. If P and Q are layer-wise, the additional communication cost introduced by these matrices should not be ignored. Please discuss how this overhead affects overall efficiency and whether it is accounted for in the experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IBsTpAIwUs", "forum": "7n5eoDZdZ5", "replyto": "7n5eoDZdZ5", "signatures": ["ICLR.cc/2026/Conference/Submission10945/Reviewer_FWSC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10945/Reviewer_FWSC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881556528, "cdate": 1761881556528, "tmdate": 1762922138365, "mdate": 1762922138365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}