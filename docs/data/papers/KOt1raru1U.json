{"id": "KOt1raru1U", "number": 8667, "cdate": 1758094164370, "mdate": 1759897770685, "content": {"title": "Repeated Integer Linear Programming for Bit Selection in neural network quantization", "abstract": "Network quantization methods, which have been widely studied to reduce model size and computational cost, are now becoming well established as practical solutions. \nMixed-precision quantization, which assigns optimal bit widths to layers, blocks, or other substructures, offers a promising approach to balance model performance and efficiency. However, determining the optimal bit configuration is a challenging combinatorial optimization problem, as it requires selecting discrete bit widths for multiple substructures across the network. In this paper, we propose an efficient algorithm that approximates the problem as an integer linear program and iteratively explores the bit-configuration space. Our method utilizes a small set of unlabeled samples with a low computational overhead, making it compatible with both widely adopted quantization methods: post-training quantization and quantization-aware training. We demonstrate the effectiveness of our approach in both settings, consistently achieving superior performance compared to single-precision baselines and existing bit-selection methods.  The code will be released upon acceptance.", "tldr": "We propose an efficient algorithm for bit selection using repeated integer linear programming.", "keywords": ["neural network quantization", "mixed precision", "bit selection", "model compression"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7c824cb0a63a7389814eae0a9bb26e184585bf8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes RIBS (Repeated Integer Linear Programming for Bit Selection), a novel framework for determining optimal bit-width configurations in mixed-precision quantization of neural networks. The key insight is to cast the bit selection process as a series of integer linear programming (ILP) subproblems, which are iteratively solved to efficiently navigate the combinatorial configuration space. The approach is flexible and can be seamlessly integrated with both post-training quantization (PTQ) and quantization-aware training (QAT) pipelines, requiring only a small amount of unlabeled calibration data. Extensive experiments on standard benchmarks—including ResNet-18, MobileNetV2, and DeiT-T models evaluated on ImageNet—demonstrate that RIBS consistently achieves superior accuracy–efficiency trade-offs compared to existing uniform-precision and mixed-precision baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The introduction of a repeated ILP formulation for bit selection represents a novel and insightful perspective on the combinatorial optimization challenge inherent in mixed-precision quantization. The overall workflow of RIBS is clearly articulated and conceptually straightforward, while maintaining a relatively low computational overhead. These characteristics make the approach easy to reproduce and appealing for both research exploration and practical deployment.\n- The RIBS framework exhibits strong flexibility and broad applicability. It is largely quantization-method agnostic, allowing seamless integration with a variety of both post-training quantization (PTQ) and quantization-aware training (QAT) pipelines. This generality enhances its potential impact, as the method can serve as a modular optimization component across diverse quantization strategies and architectures."}, "weaknesses": {"value": "- The paper validates RIBS only on the ImageNet classification benchmark using three moderate-sized models. Its scalability to larger architectures (e.g., vision or language foundation models) and generalizability to other tasks (such as object detection, segmentation, or NLP) remain unexplored. This omission limits the scope and external validity of the claims.\n- In the PTQ evaluation, the full-precision baselines vary across compared methods, which may significantly influence the post-quantization accuracy and confound comparisons. Similarly, in the QAT setting, training configurations and costs do not appear to be consistently controlled. For instance, the authors employ a specialized distillation strategy not adopted by previous baselines, and fail to report key hyperparameters (e.g., number of epochs, learning rate schedules), which are crucial for assessing fairness and training efficiency.\n- Although the overall framework is conceptually simple, its practical performance depends strongly on several hyperparameters—such as the update size (M), number of iterations (N₍iter₎), and the definition of \"substructures\". The paper would benefit from a more systematic discussion or analysis on how these hyperparameters should be tuned for different architectures and tasks, and how sensitive the final quantization accuracy is to these design choices. Moreover, because substructures are randomly sampled in each iteration, the final accuracy is expected to exhibit variance across runs. Yet, only a single result is reported. It remains unclear whether this value corresponds to the best run, the average, or a typical outcome across multiple trials.\n- Since the central objective of RIBS is to achieve an optimal balance between computational efficiency and model accuracy, the paper should include a more thorough analysis of time and resource overhead. In particular, comparisons against both metric-based and optimization-based mixed-precision methods in terms of computational cost, convergence time, and accuracy would help clarify RIBS’s true efficiency. Given that the method requires tuning several model-specific hyperparameters, a transparent discussion of its end-to-end computational footprint is essential for assessing practical usability."}, "questions": {"value": "- Could the authors provide additional evidence or analysis to substantiate that the proposed method indeed yields a better—or provably optimal—solution under the conditions stated in Theorem 1? A more rigorous theoretical or empirical justification would strengthen the claimed optimality of the approach.\n- In the quantization-aware training (QAT) experiments, was the proposed method used solely to determine the optimal bit-width configuration, or were the quantized weights obtained from RIBS also employed as initialization for subsequent QAT fine-tuning? \n- It would be valuable to visualize or analyze how the bit-width assignments differ across layers when using the proposed method versus a baseline approach. Such comparisons could offer deeper insight into what structural patterns RIBS captures and how they contribute to performance gains.\n- As noted in the weaknesses, the proposed algorithm appears largely independent of the specific loss function adopted in Algorithm 1. However, the paper lacks experiments demonstrating robustness with respect to different loss formulations. Including such results would reinforce the generality and stability of the method.\n\nOverall, my primary concern lies in the experimental section, which appears too limited to substantiate the central claim that the proposed method effectively bridges the gap between the ILP-derived solution and the true minimizer. The current empirical evidence does not convincingly demonstrate that the approach consistently yields near-optimal or superior results in practice. Additional controlled experiments or analyses—such as comparisons with ground-truth optimal solutions on smaller test cases or convergence studies—would be necessary to validate this theoretical claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ak3pIZ8IBB", "forum": "KOt1raru1U", "replyto": "KOt1raru1U", "signatures": ["ICLR.cc/2026/Conference/Submission8667/Reviewer_2uoc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8667/Reviewer_2uoc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761486246728, "cdate": 1761486246728, "tmdate": 1762920484168, "mdate": 1762920484168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RIBS which approximates mixed-precision quantization by iteratively exploring the bit-configuration space through solving an integer linear programs. It requires a small set of calibration data and can be applicable to both PTQ and QAT."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Practical problem to solve: Sensitivity difference to quantization bit-width leads to per-layer different bit-width quantization aiming to extreme optimization. However, decision of proper per-layer bit-width in quantization is an important and complex problem with balancing tradeoff between efficiency and accuracy. While the model can have n^l complexity (n for the number of bit-width candidate and l for the number of layers), for example, 3^53 =1.938×10^25 with [2,4,8]-bits in MobileNet-v2, finding the best case in vast combination of bit-widths efficiently is in high demand but difficult to solve.\n2. Strong results across all budget (BOPs and model size) on both PTQ and QAT. Considering this paper is proposing only bit-width selection method and the saturated accuracy improvement of recent quantization works, improvements shown in the paper is valid."}, "weaknesses": {"value": "I integrated the weaknesses points into Section:Qeustions."}, "questions": {"value": "1. In Corollary 2 and 3, is assumption of independent loss that divides and adds the contributions of each layer separately (= the bit-width of each substructure affects only its own loss contribution) with the hamming distance of one a valid method to calculate correct L(b)? While the ILP can be constructed with this assumption, in the realistic case, application of quantization on earlier layers always affects to later layers due to numerical difference of output activations from unquantized earlier layers. For example, as stated in the paper about strong correlations, when we quantize layer j to extremely lower bit, it will also affect the loss contribution of later layers (j+1, j+2, …). This would be also same on block-wise update strategy.\n2. In Figure 5, seems the number of computation (BOPs) of RIBS exactly matches to the BOPs of MetaMix. What is the selected bit-width in each cases? Doesn’t it match to the bit-width of MetaMix? If it matches, how RIBS can give better result than MetaMix?\n3. While the sensitivity of quantization on each layer is displayed, the bit-width selection result cannot be shown on the paper. What is the result of the bit-width selection of RIBS on both MobileNet-v2 and ResNet-18? Does it match to the per-layer sensitivity the authors proposed?\n4. Thorough explanation of the notations are required. What does ‘K’, ‘Cmax’, and superscript of ‘b’ stand for in Equation 2, 3, and 4?\n5. When we lower the target bit-width below 4-bit, accuracy degradation seems large compared to the unquantized network which can lead to the problem of severe damage in 2-bit and 3-bit. How’s the appearance of 2, 3-bit on each models? In what layers, 2-bit and 3-bit appears?\n6. Is there any reason you didn’t include 1-bit in the bit candidate set?\n7. Is there any reason you didn’t compare MobileNet-v3 rather than MobileNet-v2?\n8. How much is the searching cost (time) of RIBS in both PTQ and QAT? In QAT, do we run RIBS bit-width searching every iteration or just only once?\n\nMinor:\n1. Dagger sign in Figure 5 doesn’t appear on the figure.\n2. The quality of some figures (low resolution and incomplete appearance of some characters) is not good."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VdNfdGbZLT", "forum": "KOt1raru1U", "replyto": "KOt1raru1U", "signatures": ["ICLR.cc/2026/Conference/Submission8667/Reviewer_zKij"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8667/Reviewer_zKij"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761514467181, "cdate": 1761514467181, "tmdate": 1762920483713, "mdate": 1762920483713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores bit allocation across model layers for mixed-precision quantization. Moving from existing ILP-based bit allocation approach, the paper proposes a repeated ILP scheme, where the model is first quantized to a higher precision, then redo ILP with the quantized model to a lower precision. Theoreitical justifications are provided for the proposed method. Additional techniques like random block update are utilized to further enhance the performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty-wise, this paper identifies the previously overlooked issue of inaccurate proxy of single-round ILP method for bit allocation. Theoreitical justifications are provided to show the condition for ILP to work.\n2. The presentation of the paper is clear and easy to follow\n3. Experiments across different models and quantization methods show improved performance with the bit allocation achieved by the proposed method."}, "weaknesses": {"value": "1. The cost of the proposed method is not discussed. As shown in the algorithm, the proposed method requires multiple rounds of bit allocation and weight reconstruction before achieving the target precision. In the case where the target precision is low or the model is deep requiring more times of sampling, the cost may quickly scale up and even go beyond the QAT cost. Given the larger cost, the method may need to be compared with training-based bit allocation methods like [1,2,3]\n2. There are multiple things in the proposed method that may contribute to the improved performance. It is unsure if the improved performance is contributed by better quantization bit allocation, better initialization point for PTQ/QAT from the iterative search process, or both. Ablation study on training a model with the final bit allocation from scratch or perform PTQ to the bit allocation directly on pretrained FP model is needed to better understand the process.\n3. Experiments are limited to small-scale models, making it unsure if the method can scale up well. Since the method depends on sampling a small portion of the model for each step of update, having a deeper model may leads to exponential growth of sampling needed to cover the full model.\n\n[1] BSQ, ICLR 2021, https://arxiv.org/abs/2102.10462\n\n[2] CSQ, DAC 2023, https://arxiv.org/abs/2212.02770\n\n[3] MSQ, ICCV 2025, https://arxiv.org/abs/2507.22349v1"}, "questions": {"value": "Please provide more discussion regarding the weakness 1 and 2 listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iXuKDimyAE", "forum": "KOt1raru1U", "replyto": "KOt1raru1U", "signatures": ["ICLR.cc/2026/Conference/Submission8667/Reviewer_sMKD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8667/Reviewer_sMKD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761902844, "cdate": 1761761902844, "tmdate": 1762920483295, "mdate": 1762920483295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Repeated Integer Linear Programming for Bit Selection (RIBS), an efficient, iterative method to solve the mixed-precision quantization problem. The method approximates the complex combinatorial optimization problem of mixed-precision quantization as a sequence of solvable ILP problems. RIBS is computationally light, uses a small calibration dataset , and is compatible with both PTQ and QAT methods. It achieves SOTA accuracy in both PTQ (as recRIBS) and QAT settings across various CNNs and ViTs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method leverages a small set of unlabeled calibration samples, making it efficient to find the solutions.\n\nThe method is for both QAT and PTQ which is method-unaware. And The experimental results are highly significant. It achieves SOTA accuracy in both QAT and PTQ settings."}, "weaknesses": {"value": "The mathematical analysis: The analysis is driven by the observation that layer sensitivity to bit-width is non-uniform. Achieving the best bit-width combination hinges on accurately modeling the loss increase, $\\Delta\\mathcal{L}(b_i)$, for each layer $i$ when its precision is changed5. Yet, in the resulting ILP formulation, this essential per-layer loss term is used as a fixed, constant coefficient in the objective function, which potentially limits its ability to capture true loss dynamics across configurations.\n\nUnquantified Computational Overhead in recRIBS: The paper claims low computational overhead. However, the core technical innovation in recRIBS—sequentially optimizing the reconstruction error for all blocks following the modified block when calculating $\\Delta \\mathcal{L}$ (Algorithm 1, line 8) —is a potentially time-consuming operation. \n\nMotivation：The motivation for mixed-precision quantization is the observed variation in accuracy degradation across layers. If this degradation isn't solely based on a layer's position (layer number), why isn't it correlated with more objective metrics like the number of parameters or computational size (MACs) of that specific layer?"}, "questions": {"value": "Please look at the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H0RLtCpeEB", "forum": "KOt1raru1U", "replyto": "KOt1raru1U", "signatures": ["ICLR.cc/2026/Conference/Submission8667/Reviewer_nhL4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8667/Reviewer_nhL4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828937732, "cdate": 1761828937732, "tmdate": 1762920482682, "mdate": 1762920482682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}