{"id": "2c9TjRbAib", "number": 271, "cdate": 1756732992491, "mdate": 1759898269719, "content": {"title": "Enhancing Agentic Search via Data Synthesis on Hierarchical Constraint Satisfaction", "abstract": "Deep research becomes increasingly important as people seek to solve complex problems that require gathering and synthesizing information from diverse sources. A key capability in this process is agentic search, where an LLM-agent iteratively retrieves relevant information across multiple sources while performing multi-step reasoning. However, developing effective agentic search systems is challenging due to the lack of high-quality training data that reflects the complexity of real-world research tasks. \nTo address this gap, we introduce InfoSeek, a novel data synthesis framework that conceptualizes agentic search as a Hierarchical Constraint Satisfaction Problem (HCSP), where solving a task requires satisfying layered constraints across multiple levels of sub-problems.\nInfoSeek employs a Diffusion–Retrospection process: in the diffusion phase, the framework expands outward from a seed webpage, generating constraints that connect to neighboring pages and forming an exploration tree; in the retrospection phase, a subtree is sampled and backtracking constraints are introduced, which are then blurred and integrated into an HCSP instance.\nAs a generic framework, InfoSeek can be easily extended to other domains beyond web, facilitating ad-hoc optimization of deep research. To our knowledge, InfoSeek is the first publicly released framework in this area, complete with open-source code and well-curated datasets. Extensive experiments on diverse information-seeking benchmarks show that training on InfoSeek-generated data substantially improves agentic search performance, delivering significantly larger gains than traditional datasets across diverse model backends and training strategies, thereby validating the effectiveness of our approach.", "tldr": "InfoSeek is the first open-source framework with a large-scale dataset that generates complex, research-like tasks to train LLMs for stronger agentic search and deep reasoning.", "keywords": ["data synthesis", "agentic search", "large language models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/90ea553642c74d07b808b44410b75e8934386b3f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces **InfoSeek**, a novel data synthesis framework that formulates agentic search as a **Hierarchical Constraint Satisfaction Problem (HCSP)**. Through a **Diffusion–Retrospection** process, InfoSeek constructs over **50K QA pairs** and **16.5K reasoning trajectories**, achieving substantial improvements across multiple benchmarks, including **BrowseComp-Plus**, thereby validating the approach’s effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* Clear and principled theoretical formulation via HCSP (Eqs. (1)–(3)).\n* Well-designed synthesis algorithm producing layered, verifiable data (Algorithm 1).\n* Large, high-quality dataset (Table 2) with controlled complexity.\n* Strong experimental results: InfoSeeker-3B achieves **15.3%** on BrowseComp-Plus, outperforming GPT-4.1 and Search-R1-32B.\n* Open-sourced code and data ensure transparency and reproducibility."}, "weaknesses": {"value": "* **Limited baseline details:** RL-based baselines (e.g., Search-R1, InForage) lack hyperparameter disclosure.\n  → *Recommendation:* Provide full training details for reproducibility.\n* **Insufficient failure analysis:** While accuracy improvements are notable, deeper case-level error analysis is needed.\n  → *Recommendation:* Include failure taxonomy or qualitative examples.\n* **Heuristic ambiguity:** The implementation of “blur parent” and “depth expansion” lacks concrete examples.\n  → *Recommendation:* Add illustrative instances in the appendix."}, "questions": {"value": "* Can the HCSP formulation be generalized to multimodal or retrieval-augmented settings?\n* Did the authors explore continuous reward functions (e.g., confidence-weighted accuracy) during RL training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1K1Pb1EyNh", "forum": "2c9TjRbAib", "replyto": "2c9TjRbAib", "signatures": ["ICLR.cc/2026/Conference/Submission271/Reviewer_NXXo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission271/Reviewer_NXXo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776807873, "cdate": 1761776807873, "tmdate": 1762915482886, "mdate": 1762915482886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces InfoSeek, a framework for synthesizing high-quality, complex training data to enhance agentic search systems driven by LLMs. The core innovation is the formalization of deep research tasks as a Hierarchical Constraint Satisfaction Problem (HCSP). The HCSP structure guides the data synthesis process by systematically decomposing a complex query into a tree of sub-goals (the hierarchy) and enforcing logical rules and dependencies between intermediate facts (the constraints). This process ensures that the generated search trajectories and final answers are logically consistent, fully verifiable, and structurally complex, effectively addressing the critical lack of high-fidelity training data for multi-step reasoning agents. The resulting dataset is shown to significantly improve agent performance on complex search benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The hierarchical component of HCSP is perfectly suited to model the multi-step nature of agentic search. It provides a formal mechanism for structurally decomposing a high-level goal into mandatory, verifiable sub-goals, directly addressing the challenge of long-horizon planning and complex reasoning. By explicitly defining the variables, domains, and constraints within the HCSP, the authors gain granular control over the difficulty and scope of the synthetic tasks. This allows for the systematic generation of highly diverse training examples (e.g., tasks requiring 3, 5, or 7 dependencies), which is crucial for robust agent fine-tuning.\n\n2. Models trained on InfoSeek data, even with simple training protocols (supervised fine-tuning and basic reinforcement learning), consistently surpass strong baselines, including many more carefully engineered agentic baselines with sophisticated optimization. Ablation studies confirm that optimizing models with InfoSeek data yields clear and significant gains. Furthermore, analyses on dataset complexity and scale validate that performance improves as more complex (higher \"vertex count\") and larger subsets of the InfoSeek data are used, which supports the effectiveness of the HCSP formulation."}, "weaknesses": {"value": "1. HCSP excels at tasks with hard constraints (e.g., \"Find all companies with Revenue > $1B AND P/E < 15\"). However, it exhibits poor solution coverage for high-value, real-world business problems that rely on subjective judgment, synthesis of abstract concepts, or probabilistic forecasting. For instance, an agent tasked with formulating a strategic recommendation such as \"Which market segment should a B2B SaaS company enter next?\" requires weighing conflicting qualitative data (e.g., \"market sentiment\", \"brand alignment\", \"regulatory risk outlook\") where the \"constraints\" are fuzzy, constantly changing, and lack a binary, verifiable truth state. The HCSP framework, being optimized for explicit logic, struggles to formally represent and generate high-quality training data for solutions that are fundamentally open-ended and context-dependent.\n\n2. The HCSP primarily models the structure of a correct solution. It provides limited intrinsic coverage of effective search heuristics, resource management, information filtering, or error recovery processes that define a high-quality agentic trajectory. The agent is trained on what a valid answer is, not necessarily how to robustly find it in a noisy, real-world environment."}, "questions": {"value": "How does the framework ensure that the LLM, when generating the search trajectory (intermediate queries and reasoning steps), truly learns the HCSP's structural constraints and dependencies rather than just learning to parrot a factually-correct final answer? Is there a metric or auxiliary loss (beyond final accuracy) that specifically penalizes the agent for generating logically inconsistent or redundant intermediate queries, thereby transferring the HCSP's formal rigor into the agent's operational logic?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JfpOEUz0NE", "forum": "2c9TjRbAib", "replyto": "2c9TjRbAib", "signatures": ["ICLR.cc/2026/Conference/Submission271/Reviewer_28z3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission271/Reviewer_28z3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971359161, "cdate": 1761971359161, "tmdate": 1762915482330, "mdate": 1762915482330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes InfoSeek, a data-synthesis framework that casts agentic search as a Hierarchical Constraint Satisfaction Problem (HCSP) and generates training data via a Diffusion–Retrospection process. It releases ~50k QA pairs and 16.5k trajectories, adds quality controls for difficulty/verifiability, and shows consistent gains on multiple QA and deep-research benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The Diffusion–Retrospection design yields controllable structural complexity and unique, verifiable answers.\n* Releases >50k QA pairs and 16.5k trajectories, with code, prompts, and datasets, enhancing reproducibility.\n* Analyses show benefits from structural complexity and dataset size, and failure-rate increases with more vertices, supporting the HCSP design."}, "weaknesses": {"value": "### Major Weaknesses\n\n1. **Staleness/mischaracterization of related work availability.** In Related Work, the paper claims that “more advanced pipelines such as WebSailor and WebShaper … are not publicly available.” However,  by July 2025 both WebSailor and WebShaper have been open-sourced. ([WebSailor](https://huggingface.co/Alibaba-NLP/WebSailor-3B/tree/main) [WebShaper](https://huggingface.co/datasets/Alibaba-NLP/WebShaper)) In both functionality (agentic web research with structured planning/verification) and scale (released models/data/resources), WebSailor-3B is highly comparable to InfoSeeker-3B. Therefore, this statement is outdated, and the author should include WebSailor in the comparison. Such an oversight should not appear in a submission dated September 24, 2025.\n2. **Limited evaluation diagnostics.** While headline scores improve, the paper lacks detailed error analysis, significance testing, and human evaluation of reasoning traces/interpretability, making it hard to judge where improvements come from and whether behaviors are stable.\n3. **Novelty vs. prior formalisms.** HCSP is compelling, but the paper could better differentiate it theoretically from existing hierarchical/multi-step reasoning formalisms beyond illustrative examples; stronger connections (or guarantees) would clarify the leap over multi-hop task formalizations.\n\n### Minor Weaknesses\n\n1. **Limited model scope.** Experiments focus almost exclusively on a **3B** model variant, leaving unclear whether the gains hold at larger or smaller scales and how results trade off with compute/latency. Evaluating additional sizes (e.g., 7–8B/32B) or providing scaling curves would strengthen generality claims.\n2. **Ablation granularity.** The paper does not isolate the impact of blurring vs. depth expansion operations separately; a targeted ablation could validate each component’s necessity.\n3. **Reward design sparsity.** RL uses a binary reward (format + answer). More informative rewards (e.g., step-level correctness, retrieval quality) might further stabilize learning; an ablation would help."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YzvWB7mNrF", "forum": "2c9TjRbAib", "replyto": "2c9TjRbAib", "signatures": ["ICLR.cc/2026/Conference/Submission271/Reviewer_vAbD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission271/Reviewer_vAbD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993029435, "cdate": 1761993029435, "tmdate": 1762915482127, "mdate": 1762915482127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Developing effective agentic search systems is difficult due to a lack of complex, realistic training data. InfoSeek formalizes complex search tasks as Hierarchical Constraint Satisfaction Problems (HCSP), which require satisfying layered constraints across multiple sub-problems. It uses a Diffusion-Retrospection process. First, it \"diffuses\" outward from a seed webpage to build an exploration tree of entities and constraints. Second, it \"retrospects\" by traversing the tree in reverse to synthesize a complex question that requires hierarchical reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This framework effectively captures the layered dependencies (both parallel and sequential) of real-world research, moving beyond simpler multi-hop or flat constraint problems.\n2. This is the first publicly released framework in this area, complete with open-source code and a large-scale dataset (50k+ QA pairs). This provides a valuable, reproducible, and extensible resource for the research community.\n3. Training on InfoSeek-generated data substantially improves agentic search performance. Notably, a 3B parameter model (InfoSeeker-3B) trained on this data achieves impressive results, surpassing several strong closed-source systems (like GPT-4.1 and Sonnet 4) on the complex BrowseComp-Plus benchmark. This highlights the framework's efficiency in distilling deep research capabilities into smaller models."}, "weaknesses": {"value": "1. The restrictiveness of synthetic data models is a potential drawback.\n2. The authors indirectly prove the training's effectiveness via result improvement, but a quantitative analysis of the InfoSeeker agent's runtime error modes is missing.\n3. I feel that the InfoSeek dataset is still not hard enough."}, "questions": {"value": "1. Could we discuss the main areas where the gap between BM25 and GPT-5 lies?\n2. How robust is the HCSP data in handling irrelevant information or unexpected search paths? Why is InfoSeek better at generalizing to more complex Agentic Search benchmarks compared to traditional Multi-hop QA (Question Answering) datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Krk7JhPchj", "forum": "2c9TjRbAib", "replyto": "2c9TjRbAib", "signatures": ["ICLR.cc/2026/Conference/Submission271/Reviewer_8YAE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission271/Reviewer_8YAE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762174297860, "cdate": 1762174297860, "tmdate": 1762915481892, "mdate": 1762915481892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}