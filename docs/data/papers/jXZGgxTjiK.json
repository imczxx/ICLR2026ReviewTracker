{"id": "jXZGgxTjiK", "number": 7088, "cdate": 1758007251641, "mdate": 1759897873340, "content": {"title": "ALMC: Adaptive LLM-based Multi-Agent Collaboration Across Diverse Task Domains", "abstract": "Large language model-based multi-agent systems (LLM-MAS) are effective at solving complex tasks by coordinating specialized agents. However, existing frameworks rely on a small set of predefined scenarios with static role configurations and rigid collaboration structures, limiting their adaptability across diverse task domains. We propose the Adaptive LLM-MAS Collaboration (ALMC) framework, which dynamically recruits agents and configures collaboration patterns according to task demands through three collaborative components: a Manager Agent that synthesizes task-specific role compositions and an executable workflow, a Judge Agent that evaluates execution quality, and a Solution Optimizer Agent that persists and reuses high-quality configurations via retrieval-augmented generation. The framework supports human-in-the-loop review and creates a learning loop where previous superior configurations improve future executions on similar tasks. By using ALMC, collaborations become adaptive, auditable, and reusable across domains. Code is available at: https://anonymous.4open.science/r/ALMC-2E0F.", "tldr": "", "keywords": ["LLM Agent", "Dynamic LLM Multi-Agent Collaboration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4d2a7694d8542ce96da71fbe295b33bcd76cbae3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new framework called ALMC (Adaptive LLM-based Multi-agent Collaboration), which is an adaptive multi-intelligent body collaboration framework based on the Large Language Model (LLM). The framework aims to address two major challenges that exist in current multi-intelligent body systems: (1) The contradiction between generality and specialization: existing systems are either too general, leading to poor performance on complex tasks, or too specialized, making it difficult to adapt to new task domains and (2) lack of accumulated experience: most systems are unable to learn and reuse solutions from past successes. The experiments were conducted using different base bigram models (e.g., GPT-3.5, GPT-4o-mini, Llama-3.1-8B, etc.). The results show that ALMC maintains strong performance on different base models, proving the robustness of its framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The ALMC framework demonstrates robust adaptability by dynamically generating task-specific agent compositions and workflows. This is strongly supported by its superior performance not only across four distinct domains but also in the out-of-domain stress test (chemistry), where it surpassed both general-purpose and specialized baseline methods.\n- The integration of the Judge Agent and Solution Optimizer Agent creates a novel learning loop. This allows the system to systematically assess, store, and reuse successful collaboration patterns, addressing a common limitation in multi-agent systems and contributing to more stable and improved performance over time.\n- The paper provides comprehensive empirical evidence showing that ALMC achieves state-of-the-art or highly competitive performance against strong baselines. The results are consistent across multiple base LLMs (e.g., GPT-3.5-turbo, GPT-4o-mini), highlighting the robustness and effectiveness of the proposed framework itself."}, "weaknesses": {"value": "- There has been a significant amount of existing works like AgentVerse [1], CaptainAgent [2], GPT-Swarm [3], etc, about dynamically building an agent team for task solving, and I cannot identify the difference between this work and those previous works (and also the author didn't describe the difference between these works and the proposed work).\n- The core claim of the Solution Optimizer and Judge Agent is that the system learns from past successes. However, the experiments do not provide direct evidence of this learning process. A crucial missing experiment would be a longitudinal study: by processing a dataset in sequential chunks (as described in the `Section 4.3.2`), the authors should demonstrate a clear and consistent performance improvement from the first chunk to the last. Without this, the benefit of experience accumulation remains more of a theoretical assertion than an empirically proven advantage.\n- The experiments are conducted on well-defined, single-turn tasks (like Q&A or generating a single function). It is unclear how ALMC would scale to more complex, multi-step, or long-horizon problems (e.g., developing a complete software module from scratch, requiring iterative refinement and dependency management). An experiment on a more complex benchmark would be necessary to test the limits of the Manager Agent's planning capabilities and to assess whether the dynamically generated workflows remain coherent and efficient as task complexity increases.\n\nRefs:\n\n[1] Chen, Weize, et al. \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors.\" The Twelfth International Conference on Learning Representations. 2023.\n\n[2] Song, Linxin, et al. \"Adaptive in-conversation team building for language model agents.\" arXiv preprint arXiv:2405.19425 (2024).\n\n[3] Zhuge, Mingchen, et al. \"Language agents as optimizable graphs.\" arXiv preprint arXiv:2402.16823 (2024)."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ygsYZVr5zY", "forum": "jXZGgxTjiK", "replyto": "jXZGgxTjiK", "signatures": ["ICLR.cc/2026/Conference/Submission7088/Reviewer_K7Lw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7088/Reviewer_K7Lw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760479834180, "cdate": 1760479834180, "tmdate": 1762919268145, "mdate": 1762919268145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ALMC proposes a task-adaptive multi-agent framework where a Manager retrieves prior high-quality workflows and synthesizes a task-specific configuration of roles, phased steps, and collaboration rules, execution proceeds via pairwise dialogues that freeze intermediate artifacts for the next phase, and a Judge plus Solution Optimizer evaluate results and persist successful configurations into a RAG memory for reuse across similar tasks; experiments across coding, medical QA, quantitative reasoning, and finance suggest higher accuracy than common debate or voting baselines, with additional transfer to chemistry-style tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Proposes a task-adaptive orchestration that learns and reuses workflows via a Manager–Judge–Solution Optimizer loop with RAG-backed memory, and emphasizes pairwise, phase-scoped dialogues with frozen intermediates.\n- Presents a clear end-to-end algorithm specifying configuration synthesis (R,P,G), execution, evaluation, and persistence, with ablations on agent count and Judge/Optimizer contributions, plus cost/latency reporting alongside accuracy across multiple domains.\n- The paper delineates roles, phases, collaboration rules, and intermediate artifacts in a structured manner; figures and pseudocode make the dataflow and decision points easy to follow.\n- Demonstrates consistent gains over common multi-agent baselines across coding, medical QA, quantitative reasoning, and finance, and showcases transfer via configuration reuse, timely given ongoing evidence that stronger orchestration can outperform naive debate or simple voting."}, "weaknesses": {"value": "- Report results under the same backbone, temperature, stopping rules, tool permissions, and token budgets, and include variance over multiple random seeds with significance tests. This avoids hidden advantages from longer debate chains or richer tool access.\n- The claimed determinism of pairwise collaboration should be tested against Multi-Agent Debate, Tree-of-Thoughts, and Graph-of-Thoughts under matched budgets, with accuracy-vs-tokens frontiers.\n- Persisting and retrieving configurations without clear indexing fields, similarity thresholds, and domain isolation risks cold-start inefficiency, unstable cross-domain transfer, and evaluation contamination (including leakage and privacy exposure). Add ablations for “no-memory / in-domain / cross-domain” and disclose retrieval specifics and leakage mitigations.\n- The paper lacks discussion and head-to-head comparison with closely related lines\n\n[1] Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration\n\n[2] Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks\n\n[3] AutoAgents: A Framework for Automatic Agent Generation\n\n[4] Voyager: An Open-Ended Embodied Agent with Large Language Models"}, "questions": {"value": "- Provide controlled ablations that swap ALMC’s intra‑phase pairwise controller with Multi‑Agent Debate (MAD) and search‑style controllers such as Tree‑of‑Thoughts or Graph‑of‑Thoughts under matched budgets. Plot accuracy‑vs‑tokens frontiers and comment on stability.\n- When the memory is empty or sparse, what is the fallback (e.g., “no‑memory ALMC”) and performance delta?\n- What index fields and similarity thresholds gate reuse across tasks/domains? Provide ablations for no‑memory / in‑domain / cross‑domain reuse.\n- State exact decoding settings (temperature, top‑p, max turns), stop rules, tool permissions, and token budgets shared by all baselines; report mean±std over multiple seeds and significance tests. This helps avoid hidden advantages from longer debate chains or richer tool use."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8qIed2AIbT", "forum": "jXZGgxTjiK", "replyto": "jXZGgxTjiK", "signatures": ["ICLR.cc/2026/Conference/Submission7088/Reviewer_o48m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7088/Reviewer_o48m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934017052, "cdate": 1761934017052, "tmdate": 1762919267746, "mdate": 1762919267746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an adaptive framework based on dynamic role synthesis, reusable experiential memory, and human-AI collaborative review. The system autonomously generates task-specific role combinations and execution workflows according to task requirements, while continuously optimizing performance through experience reuse. It successfully achieves both generality without requiring pre-defined domain libraries and specialization through customized task configurations, thereby enabling efficient and stable cross-domain task execution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper conducts comprehensive experiments across five challenging domains (coding, medicine, mathematics, finance, and chemistry), using both in-domain and out-of-domain datasets to evaluate ALMC. Results show consistent improvements over both general-purpose and domain-specific baselines.\n2. The proposed “dynamic configuration of roles and collaboration modes + RAG-based experience reuse” allows the system to automatically generate role configurations, phase divisions, and workflows tailored to each task. This effectively overcomes the rigidity of predefined roles and static workflows, enhancing adaptability and knowledge transfer efficiency in multi-agent systems."}, "weaknesses": {"value": "1. The proposed method relies heavily on architectural intuition and empirical evidence rather than formal theoretical analysis. It lacks discussion of convergence guarantees, expressivity trade-offs, or provable limits of adaptive composition mechanisms.\n2. Although the paper claims to reduce human engineering effort, each task still requires a Human-in-the-Loop Gate (HITL-Gate) review before execution. The actual utility and cost of this human involvement are not quantitatively analyzed.\n3. Each task phase adopts a pairwise dialogue structure (two agents interacting over multiple turns), which is claimed to prevent deadlocks. However, for large-scale or parallel tasks requiring multiple agents, such a fixed structure could become a bottleneck. The paper does not discuss these limitations or demonstrate performance in more complex or asynchronous settings."}, "questions": {"value": "1. Can the authors quantify the actual intervention rate or the overhead introduced by human-in-the-loop review? Is this process scalable for real-world deployment?\n2. Is there any theoretical or empirical analysis of when and why the pairwise agent mechanism converges to high-quality solutions, and under what conditions suboptimal negotiation or agent conflicts may arise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9N4W4FfJLH", "forum": "jXZGgxTjiK", "replyto": "jXZGgxTjiK", "signatures": ["ICLR.cc/2026/Conference/Submission7088/Reviewer_siHQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7088/Reviewer_siHQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989250610, "cdate": 1761989250610, "tmdate": 1762919267269, "mdate": 1762919267269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes *ALMC*, a multi-agent framework with three roles (Manager, Optimizer, Judge) aiming to achieve adaptive collaboration across different task domains (code, medical, math, finance). The authors claim that ALMC dynamically designs agent roles, generates stage-wise workflows, and reuses past experiences through RAG-based retrieval."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper resembles a synthesis of prior multi-agent ideas, implemented via prompt engineering. The baselines are outdated, the “adaptivity” is ill-defined, and the claims overreach the evidence."}, "weaknesses": {"value": "“Manager–Judge–Optimizer” is widely used multi-agent patterns already exist. The method is not a novel solution.\n\nAdditionally, “adaptive” aspect is not learned or optimized, but *prompt engineering. \n\nThe claim of “continuous self-improvement” is unsupported.\nThe “RAG memory” is simply text retrieval without validation or analysis of retrieval quality.\n\nFor experiments: \nThe paper repeatedly asserts domain generalization, but the experimental setup only involves standard benchmarks (HumanEval, MedQA, MMLU subsets).\nThere is **no transfer or few-shot evaluation** proving actual *adaptation*.\nThe authors compare only against very early/old frameworks."}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nqi5S93TXQ", "forum": "jXZGgxTjiK", "replyto": "jXZGgxTjiK", "signatures": ["ICLR.cc/2026/Conference/Submission7088/Reviewer_DuXb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7088/Reviewer_DuXb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158612419, "cdate": 1762158612419, "tmdate": 1762919266875, "mdate": 1762919266875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}