{"id": "KL1LNgg73s", "number": 11900, "cdate": 1758204534730, "mdate": 1759897547808, "content": {"title": "FedGMR: Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity", "abstract": "Federated learning (FL) holds strong potential for distributed machine learning. Yet in heterogeneous environments, Bandwidth-Constrained Clients (BCCs) often fail to participate effectively due to limited communication capacity, leading to slow convergence and degraded generalization.\nTo tackle this issue, we propose FedGMR—Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity. FedGMR progressively increases each client's model capacity during training, enabling BCCs to contribute constantly throughout the training. In addition, a tailored transmission and aggregation mechanism is designed to better accommodate system-level heterogeneity. We establish convergence guarantees under mask-aware aggregation, showing that time-averaged, coverage-weighted densities govern enlarged errors, and that GMR provably tightens the gap to full-model FL. Extensive experiments on FEMNIST, CIFAR-10, and ImageNet-100 show that FedGMR achieves faster convergence and higher accuracy, especially under high heterogeneity and non-IID settings.", "tldr": "", "keywords": ["Federated Learning", "Heterogeneity", "Model Pruning", "Model-heterogeneous"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b189db313a813def416b8c43154039e5384bc4f8.pdf", "supplementary_material": "/attachment/391173c5c6ba459054c0f04bbb4ffa3fd5937947.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces FedGMR to address the limitations of Bandwidth-Constrained Clients (BCCs) and model heterogeneity in federated learning. In many real-world FL systems, clients have varying communication capacities and cannot always train or transmit full models, leading to slow convergence and poor generalization. To mitigate this, FedGMR progressively restores model capacity on each client over time, allowing BCCs to contribute continuously. The method also introduces a transmission and aggregation mechanism that aligns updates from heterogeneous clients and ensures consistent model integration across asynchronous updates. The authors provide theoretical convergence guarantees, showing that the average sub-model density influences the error bound and that the proposed gradual restoration narrows the gap to full-model federated learning. Experimental results on FEMNIST, CIFAR-10, and ImageNet-100 demonstrate that FedGMR delivers faster convergence and higher accuracy than existing FL baselines, particularly in settings with severe non-IID data and system heterogeneity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Introduces a new concept of Gradual Model Restoration (GMR), which progressively increases each client’s model capacity.\n+ Designs a mask-aware transmission and aggregation mechanism that aligns updates from heterogeneous clients, ensuring stable training despite differing model structures.\n+ Provides formal convergence guarantees under the proposed mechanism.\n+ Offers a mathematically grounded explanation of how gradual restoration narrows the performance gap to full-model FL.\n+ Conducts extensive experiments on diverse and well-recognized benchmarks: FEMNIST, CIFAR-10, and ImageNet-100."}, "weaknesses": {"value": "- As BCCs gradually restore larger sub-models, their computation and communication loads increase, making them more prone to straggling (delayed updates). This straggler effect can partially offset the benefits of gradual restoration, especially in asynchronous environments where delayed clients slow global progress or introduce stale gradients. The paper does not analyze or mitigate this trade-off, nor quantify how much density increase is sustainable before latency outweighs the learning gain. This limitation suggests that while FedGMR improves participation for BCCs initially, it may reintroduce asynchrony inefficiencies in later training stages as model sizes grow.\n- FedGMR enables BCCs to train sub-models to mitigate the straggler effect. Recent works such as [R1] and [R2] leverage tiering approaches to mitigate the straggler effect, which should be discussed and compared in the paper.\n\n[R1] Chai, Zheng, Yujing Chen, Ali Anwar, Liang Zhao, Yue Cheng, and Huzefa Rangwala. \"FedAT: A high-performance and communication-efficient federated learning system with asynchronous tiers.\" In Proceedings of the international conference for high performance computing, networking, storage and analysis, pp. 1-16. 2021.\n\n[R2] Mohammadabadi, Seyed Mahmoud Sajjadi, Syed Zawad, Feng Yan, and Lei Yang. \"Speed up federated learning in heterogeneous environments: a dynamic tiering approach.\" IEEE Internet of Things Journal (2024).\n\n- Experiments are conducted mainly on image classification datasets (FEMNIST, CIFAR-10, ImageNet-100); no evaluation on other domains such as NLP, speech, or sensor data, which would strengthen the generality of the approach. \n- Results focus heavily on accuracy and convergence, with no measurement of communication overhead, latency. Also, the results are given under a fixed wall-clock budget. It is suggested to also provide results under a fixed target accuracy and compare the convergence time.\n- The performance of FedGMR on transformer models is not evaluated.\n- The methodology section is dense, and key components like mask-aware aggregation are only briefly explained."}, "questions": {"value": "1. How does FedGMR handle the growing computational and communication burden as sub-models expand for BCCs? Please analyze or visualize the relationship between model density and update delay. An ablation showing when the latency begins to outweigh the accuracy gain would help quantify this trade-off.\n2. How does FedGMR compare conceptually and empirically with tier-based asynchronous frameworks like FedAT ([R1]) and Dynamic Tiering ([R2])? Please include a discussion or experimental comparison with these approaches, since both aim to mitigate straggler effects. Highlight how FedGMR’s gradual model restoration differs from or complements tiering-based synchronization strategies in terms of communication cost and convergence speed.\n3. Can FedGMR generalize beyond image classification tasks? Include or discuss experiments in non-vision domains such as NLP or sensor data to show generality, since many FL applications (e.g., federated BERT fine-tuning) involve non-visual modalities.\n4. Provide additional results under a fixed target accuracy and report the time to convergence for each baseline. This would allow a fairer assessment of FedGMR’s training efficiency. Include measurements of communication overhead and latency, since these are central to evaluating the benefits of gradual model restoration.\n5. How does FedGMR perform with Transformer or attention-based architectures, which are now common in FL applications? Add an experiment or at least a discussion on how model restoration and mask-aware aggregation behave when applied to Transformers. The impact on gradient synchronization and sub-model alignment would be insightful.\n6. How scalable is FedGMR in large federations (e.g., thousands of clients) where communication delays and resource diversity are more extreme? Discuss the scalability of  FedGMR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7KLBlxfLQb", "forum": "KL1LNgg73s", "replyto": "KL1LNgg73s", "signatures": ["ICLR.cc/2026/Conference/Submission11900/Reviewer_ooYy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11900/Reviewer_ooYy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761503550654, "cdate": 1761503550654, "tmdate": 1762922912374, "mdate": 1762922912374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a new heterogeneous FL frameowork, FedGMR. This framework consists of mainly three components, gradual density adjustment, buffered mask-aware aggregation, and incremental model splitting. In addition to those main components, the authors employ semi-asynchronous update aggregation scheme to alleviate the straggler effect (synchronization cost) in FL. The authors provides theoretical analysis of convergence behaviors as well as empirical results of accuracy comparisons across SOTA heterogeneous FL methods. Overall, the paper shows promising FL performance, however, I see several strong weaknesses that should be addressed."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors consider important and realistic issue in FL, the heterogeneous system environments together with data heterogeneity.\n2. The theoretical analysis well incorporate the proposed method into the traditional analysis framework.\n3. Asynchronous scheme looks promising. I especially appreciate this approach since most of recent FL studies just simply consider synchronous model aggregations that are not realistic."}, "weaknesses": {"value": "**Comments on Main Idea**\n\n1. The authors assume that weak clients become straggler mainly due to the limited network bandwidth. However, such weak clients tend to have limited system resources such as slow compute power and a limited memory space. Thus, gradually increasing density may make some weak clients with limited resources impossible to join the training any more. Therefore, I do not think the problem definition is convincing.\n\n2. In this work, 'model-heterogeneous' term may mislead readers. In general, 'heterogeneous' indicates independently designed entities. E.g., heterogeneous data distributions do not have any dependencies across local datasets. However, this work assumes that weak clients have limited system resources and are assigned with models differently prunned. Basically, however, they share the same model architecture. Thus, this study does not cover model-heterogeneious FL. It would rather be a prunning-based sub-model distribution method.\n\n3. How is GMR equation (4) built? What is the meaning of multiplying learning rate $\\lambda$ to the ratio of time difference to the BCC's time? The main equation is not well substantiated due to the limiated explanation. The authors should focus more on **why** (4) is the best choice rather than **how** only.\n\n4. In theoretical analysis, the assumptions are too strong. Especially, the bounded noise (assumption 3) and bias (assumption 4) are unrealistic. In addition, most of recent studies do not rely on the assumption of bounded gradient magnitude (assumption 2). Instead, the bounded gradient variance assumption is used popularly. Given these strong assumptions, the results are not that convincing.\n\n5. Also, there are $f_1$, $f_2$, and $f_3$ in Theorem 2, but they have never been defined before. What does it mean by the subscripts?\n\n6. Finally, I recommend analyzing the complexity of the derived convergence rate so that the performance can be easily compared with other methods. Since there are many indirect notations such as $\\mathcal{A}$ and $\\mathcal{B}$, it is not easy to compare the performance.\n\n7. FedGMR obviously uses much more server-side system resources such as memory space for local models. However, other methods do not require such strong resources at the server-side. E.g., HeteroFL just align the submodels and directly average the parameters. Fjord also just randomly prunned local models are aggregated at the server-side. Thus, even though FedGMR achieves higher accuracy than other methods, the performance gain probably comes from using more resources. Table 1 does not consider such differences and just directly compare the achieved accuracy within the same amount of time. I do not think it is a fair comparison.\n\n8. As compared to the best-performing SOTA method, the performance gain of FedGMR is not significant. E.g., FEMNIST non-IID performance gap between FedGMR and FedAsync is just 0.9% when non-IIDness is low. When non-IIDness is high, Fjord becomes the best. For CIFAR-10, the difference becomes 1.1~1.2%. Considering that FedGMR uses more server-side resources, I think this performance gain is not that impressive.\n  \n9. Some recently published and strongly related heterogeneous FL methods could be directly compared in Table 1 or at least discussed in Section 2. Some examples are as follows.\n\n[1] Liu et al., Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout, TKDD, 2025.\n\n[2] Lee et al., Embracing Federated Learning: Enabling Weak Client Participation via Partial Model Training, IEEE Trans. on Mobile Computing, 2024.\n\n[3] Liu et al., No One Left Behind: Inclusive Federated Learning over Heterogeneous Devices, KDD, 2022. \n\n**Comments on Presentation Quality**\n\n1. The contribution summary at the end of Introduction section does not deliver meaningful information while taking up the space of several lines. I see many recent literature has this summary at the same position, but what is the point of having them? The summary includes nothing new, the proposed idea, the existance of convergence analysis, and experimental settings. I strongly recommend removing them and use the space for other more critical things, e.g., more experimental results or detailed discussion of main ideas.\n\n2. Figure 1 does not clearly explain the workflow. First, what is the meaning of numbers? The caption explains the steps in the order of 3, 4, 1, and 2. It seriously confuses readers. Second, after the step 2, there are two horizontal flows, 3 and 4. The figure neither explains what they are nor why they appear in the figure. Overall, the figure should be improved much to deliver key ideas.\n\n3. Algorithm 1 is not self-contained. What is IMSc? What is GMR? They should be at least connected to any equations or subsections where the method is explained. \n\n4. There are too many acronyms which seriously hurts readability. BAC, BCC, GMR, IMS, MHFL, MA, GA, MRI, etc... I suggest using their full names unless there are some special reasons. Just few are okay, but there are too many now."}, "questions": {"value": "Some questions are included in the above weakness section. Please carefully address them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "CQlxnychbd", "forum": "KL1LNgg73s", "replyto": "KL1LNgg73s", "signatures": ["ICLR.cc/2026/Conference/Submission11900/Reviewer_7UBb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11900/Reviewer_7UBb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761509344026, "cdate": 1761509344026, "tmdate": 1762922911935, "mdate": 1762922911935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes FedGMR-Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity. It progressively increases each client’s model capacity during training, and a tailored transmission and aggregation mechanism is designed to better accommodate system-level heterogeneity. Theory and experiments on FEMNIST, CIFAR-10, and ImageNet-100 confirm the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work proposes FedGMR, the first FL framework that dynamically restores sub-model capacity via GMR. Two auxiliary mechanisms, IMS for efficient transmission and BufferMaskFedAvg for robust aggregation under structural inconsistency, are designed to enhance FedGMR.\n2. A convergence guarantee is established to show that the average sub-model density across clients and time governs error accumulation.\n3. Experiments on FEMNIST, CIFAR-10, and ImageNet-100 demonstrate faster convergence and robustness of proposed method."}, "weaknesses": {"value": "1. What’s the motivation of GMR? Although authors claim that GMR can dynamically restore sub-model capacity, the communication capacity is limited for BCCs. Is it essential for them to train the full model？ Furthermore, the current design of GMR fails to clearly demonstrate how the principles articulated by the authors in Section 3.1 are implemented.\n2. The paper's logic seems confused. If the core contribution is GMR, what problems do the other two auxiliary components solve, and are they considered core contributions of the paper as well?\n3. Although authors provide the convergence guarantee, the analysis simplifies the core asynchronous process, lacking sufficient theoretical rigor. Given that many recent works [1-3] on asynchronous federated learning have already provided comprehensive convergence analyses for their respective frameworks, this section needs to be strengthened.\n4. The experiments only compare against the synchronous submodel training method. The authors should demonstrate the performance of the proposed method in an asynchronous scenario; it is suggested that combined asynchronous baselines be added for comparison to fully prove the method's effectiveness.\n5. The paper needs to explicitly indicate which empirical results (figures or tables) demonstrate that the proposed method achieves faster convergence speed than other baselines.\n6. The paper's readability needs improvement. A lot of abbreviations without a clear meaning are not feasible for readers to understand the work. Figure 1 fails to clearly show the complete process of the proposed method, and its caption is confusing and inconsistent with the legend.\n\n---\n[1] Sharper convergence guarantees for asynchronous SGD for distributed and federated learning.\n\n[2] Asynchronous Federated Optimization\n\n[3] FADAS: Towards Federated Adaptive Asynchronous Optimization"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "El4MPeQMYH", "forum": "KL1LNgg73s", "replyto": "KL1LNgg73s", "signatures": ["ICLR.cc/2026/Conference/Submission11900/Reviewer_q72V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11900/Reviewer_q72V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700575817, "cdate": 1761700575817, "tmdate": 1762922911435, "mdate": 1762922911435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}