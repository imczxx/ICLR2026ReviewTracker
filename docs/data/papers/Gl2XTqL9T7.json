{"id": "Gl2XTqL9T7", "number": 19080, "cdate": 1758293395491, "mdate": 1759897061998, "content": {"title": "$\\texttt{LIME}$: Making LLM Data More Efficient with Linguistic Metadata Embeddings", "abstract": "Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose $\\texttt{LIME}$ ($\\textbf{Li}$nguistic $\\textbf{M}$etadata $\\textbf{E}$mbeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. $\\texttt{LIME}$ substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, $\\texttt{LIME}$ improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, $\\texttt{LIME$^{+1}$}$, that can guide token generation. \nGiven prior metadata for the next token, $\\texttt{LIME$^{+1}$}$ improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.", "tldr": "", "keywords": ["Language Models", "LLM", "Metadata Embeddings", "Pre-Training", "Tokenization", "Data Efficiency", "Linguistic"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf6a3ee0d0841c371aa1ba537ce313215fd89e99.pdf", "supplementary_material": "/attachment/f64737064b9b3296a5307e7c77febec8e504df93.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes LIME and LIME+1, methods for incorporating token-level linguistic metadata (POS/NER tags) into the embedding layer of large language models. While the idea is simple and yields modest improvements in data efficiency, the LIME+1 variant raises significant conceptual and practical concerns."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposal is minimal architectural modification (just adding metadata embeddings). Implementation is straightforward, and the overhead in parameters/compute is negligible.\n\nThe data-efficiency gains reported are interesting (e.g., same loss with ~56% less data at 500M). \n\nAnalysis of subword cohesion and token coupling offers some interpretability. In fact, the approach in LIME+1 is indeed interesting and promising since it also allows faster adaptation during pretraining stage."}, "weaknesses": {"value": "1) (Minor Weakness) The core idea of injecting linguistic or syntactic tags into embeddings has been explored in earlier works. For example, see [1,2].\n\n2) Experiments are limited to English web data. However, metadata extraction quality degrades on noisy or multilingual corpora. It’s unknown whether LIME works with other data, languages.\n\n3) While the direction is interesting, the LIME+1 results rely on handcrafted oracle metadata and thus overstate the method’s practical impact. The causal inconsistency substantially weakens the contribution.\n\nFor now I recommend rejection, but I'm open to increase the score if the authors can provide a learnable or inference-consistent variant of LIME+1.\n\n[1] Armengol-Estape, Jordi, Marta R. Costa-jussà, and Carlos Escolano. \"Enriching the Transformer with Linguistic Factors for Low-Resource Machine Translation.\" Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021). 2021.\n\n[2] Sennrich, Rico, and Barry Haddow. \"Linguistic Input Features Improve Neural Machine Translation.\" Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers. 2016."}, "questions": {"value": "1) Did you try LIME on data with spelling error? \n\n2) How should one apply the LIME on other languages?\n\n3) I do not clearly understand how POS/NER tags are generated during inference in LIME+, can you further explain it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4444rkGd3h", "forum": "Gl2XTqL9T7", "replyto": "Gl2XTqL9T7", "signatures": ["ICLR.cc/2026/Conference/Submission19080/Reviewer_rBYA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19080/Reviewer_rBYA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761187677155, "cdate": 1761187677155, "tmdate": 1762931103971, "mdate": 1762931103971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata embeddings, such as POS or NER tags. The authors claim that LIME substantially improves pre-training efficiency, which adapts to the training data distribution up to 56% faster."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "None"}, "weaknesses": {"value": "1. **Information leakage from non-causal metadata makes the entire evaluation invalid.**  The main issue is the fundamental mismatch between the non-causal metadata signal and the strictly causal autoregressive language modeling task.\n   * High-quality linguistic annotators (like spaCy) are inherently non-causal. They require bi-directional context to resolve ambiguity. For instance, in the sentence \"The old man the boat,\" the POS tag for the second \"man\" can only be correctly identified as a verb (VB) by observing the future token \"boat.\"\n   * In the LIME training paradigm, the model at step `t` receives `meta_t`, a tag that was computed using information from the entire sentence, including future tokens `t+1, ..., N`. This constitutes a direct leak of future information into the model's input. The training task is therefore no longer a pure prediction of the future from the past; it is a simplified task where the model is given clues derived from the answer. The reported 56% efficiency gain is thus highly suspect. It cannot be reliably attributed to improved learning from linguistic knowledge, as it is confounded by the fact that the model is being trained on an artificially easier, causally compromised objective. The comparison to a baseline model is meaningless.\n   * The LIME+¹ variant is an extension of this flaw. By directly providing the metadata of the next token (`meta_{t+1}`) to predict `token_{t+1}`, the evaluation setup is completely invalidated. Claiming massive improvements on reasoning and arithmetic is deeply misleading, as the model is not demonstrating superior intelligence but rather the ability to exploit a direct hint about the solution. The evaluation framework is unsuitable for making any claims about generative or reasoning capabilities.\n\n2. **Flawed Model Architecture for Practical Generation.** The paper states that \"all transformer components beyond the embedding layer, including the loss function, remain fully agnostic to the metadata.\" This creates an architectural mismatch. The model is trained to expect metadata as input, yet its training objective (cross-entropy loss on the next token) provides no mechanism for it to learn to predict this metadata. The model is therefore incapable of generating the very input it relies on. While the authors briefly mention adding a metadata prediction head in the discussion, this is only presented as future work."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YV2myRRENU", "forum": "Gl2XTqL9T7", "replyto": "Gl2XTqL9T7", "signatures": ["ICLR.cc/2026/Conference/Submission19080/Reviewer_CCaQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19080/Reviewer_CCaQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761224358119, "cdate": 1761224358119, "tmdate": 1762931103606, "mdate": 1762931103606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LIME, a method for decoder-only LLM pre-training that augments linguistic tokens with linguistic metadata coming from a rule-based or spaCy-style linguistic pass, followed by alignment to the model’s original tokenizer. The metadata is embedded and added to the normal token embedding, adding only very small size of parameters and “negligible” compute. Its variant, LIME+1, shifts the metadata by one position and assumes that the metadata of the next token is available at inference time. On 500M–2B Gemma-style models trained on 302B DCLM-Baseline tokens, the authors report improved data efficiency in pre-training and downstreaming task performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a very clear and straightforward mechanism: augmenting token embeddings with one or two small embedding tables corresponding to linguistic metadata (e.g., POS or NER tags). This is conceptually simple, easy to implement, and requires minimal architectural changes. The authors also provide a concrete alignment procedure to handle cases where linguistic tokens and subword tokens do not align one-to-one, which makes the approach practically applicable.\n2. The emprical results show consistent improvement for the next-token prediction task. \n3. The paper provides analysis on token-level effects. In particular, the authors examine accuracy shifts across tokens within words of different lengths, providing plausible evidence that LIME helps models better couple subword tokens belonging to the same linguistic unit. This analysis is a nice touch that helps explain why the proposed method improves performance, rather than merely reporting higher scores."}, "weaknesses": {"value": "1. My main concern is that the BASELINE model used in the experiment might not be enough to illustrate the efficacy of the proposed meMy main concern is with the definition of the Baseline model used in the experiments. The paper states that “models trained with LIME Tokenization but no additional metadata embedding layers are referred to as Baseline.” However, this setup already benefits from the modified tokenization procedure, which may itself contribute to the observed gains. I think a more convincing baseline would be a model trained directly with the standard tokenizer, without applying LIME Tokenization at all. This would better isolate the effect of the proposed metadata embeddings from that of the preprocessing changes.\n2. The LIME+1 variant appears to function more as an oracle setting than a realistic deployment method. The paper highlights two special cases: reasoning, where the expected syntactic class may be known, and arithmetic, where numerical metadata is available. But these are exceptions, which might need careful curation. In most language modeling or generative tasks, the metadata of the next token is not known in advance, making the comparison to LIME+1 somewhat unfair and limiting its practical relevance.\n3. Downstream improvements are modest for plain LIME; the very large improvements are mostly for LIME+1.\n4. Another concern is about the access of the linguistic metadata. The proposed method depends on an external linguistic annotator (e.g., spaCy) to provide POS and NER tags. These annotations are not error-free, and the cost of running such models at scale could become non-trivial for very large pre-training pipelines. Furthermore, as language models grow in size, they may already infer much of this linguistic information internally. It would be useful to discuss whether the benefits of explicit metadata injection persist for stronger baselines, or whether they primarily help smaller or weaker models."}, "questions": {"value": "1. Could you quantify the computational and annotation cost of generating linguistic metadata for the entire 302B-token corpus? For instance, how much additional preprocessing time or compute does this require relative to standard pre-training?\n2. How robust is LIME to noise in the linguistic annotations? You mention that the annotator achieves about 97% POS accuracy and 86% NER F-score. What happens if the annotation quality is deliberately reduced—for example, by introducing random label noise or using a weaker tagger? Does model performance degrade smoothly or abruptly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3BPePyXn3K", "forum": "Gl2XTqL9T7", "replyto": "Gl2XTqL9T7", "signatures": ["ICLR.cc/2026/Conference/Submission19080/Reviewer_Cjmm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19080/Reviewer_Cjmm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925563206, "cdate": 1761925563206, "tmdate": 1762931103134, "mdate": 1762931103134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LIME, a method for decoder-only LLM pre-training that augments linguistic tokens with linguistic metadata coming from a rule-based or spaCy-style linguistic pass, followed by alignment to the model’s original tokenizer. The metadata is embedded and added to the normal token embedding, adding only very small size of parameters and “negligible” compute. Its variant, LIME+1, shifts the metadata by one position and assumes that the metadata of the next token is available at inference time. On 500M–2B Gemma-style models trained on 302B DCLM-Baseline tokens, the authors report improved data efficiency in pre-training and downstreaming task performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a clear and straightforward mechanism: augmenting token embeddings with one or two small embedding tables corresponding to linguistic metadata (e.g., POS or NER tags). This is conceptually simple, easy to implement, and requires minimal architectural changes. The authors also provide a concrete alignment procedure to handle cases where linguistic tokens and subword tokens do not align one-to-one, which makes the approach practically applicable.\n2. The emprical results show consistent improvement for the next-token prediction task. \n3. The paper provides analysis on token-level effects. In particular, the authors examine accuracy shifts across tokens within words of different lengths, providing plausible evidence that LIME helps models better couple subword tokens belonging to the same linguistic unit."}, "weaknesses": {"value": "1. My main concern is that the BASELINE model used in the experiment might not be enough to illustrate the efficacy of the proposed method. The paper states that “models trained with LIME Tokenization but no additional metadata embedding layers are referred to as Baseline.” However, this setup already benefits from the modified tokenization procedure, which may itself contribute to the observed gains. I think a more convincing baseline would be a model trained directly with the standard tokenizer, without applying LIME Tokenization at all. This may isolate the effect of the metadata embeddings.\n\n2. The LIME+1 variant is more like an oracle setting than a realistic method. The paper presents two special cases: reasoning and arithmetic. In most language modeling or generative tasks, the metadata of the next token is not known in advance, making the comparison to LIME+1 somewhat unfair and limiting its practical relevance.\n\n3. Downstream improvements are modest for LIME; the very large improvements are mostly for LIME+1.\n\n4. Another concern is about the access of the linguistic metadata. The proposed method depends on an external metadata (e.g., spaCy) to provide POS and NER tags. These annotations are not error-free, and the cost of running such models at scale could be huge. Furthermore, as language models grow in size, they may already infer this metadata information internally. It would be useful to discuss whether the benefits of explicit metadata injection still exists for stronger baselines, or whether they are helpful just for smaller or weaker models."}, "questions": {"value": "1. Could you quantify the computational and annotation cost of generating metadata for the entire 302B-token corpus? For instance, how much additional preprocessing time or compute does this require compared to standard pre-training?\n2. How robust is LIME to noise in the metadata annotations? You mention that the annotator achieves about 97% POS accuracy and 86% NER F-score. What happens if the annotation accuracy is deliberately reduced? For example, by introducing random label noise or using a weaker annotation method? Does model performance degrade smoothly or abruptly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3BPePyXn3K", "forum": "Gl2XTqL9T7", "replyto": "Gl2XTqL9T7", "signatures": ["ICLR.cc/2026/Conference/Submission19080/Reviewer_Cjmm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19080/Reviewer_Cjmm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925563206, "cdate": 1761925563206, "tmdate": 1763370840740, "mdate": 1763370840740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LIME that incorporates linguistic representation in token embeddings in language modeling. It is a simple method that simply adds additional embedding information based on the part of speech as well as the named entity of the tokens. \nThe authors demonstrated various benefits of doing so at 500M, 1B, and 2B model scales—it can improve training efficiency by achieving similar loss using a half of the tokens, and can improve the model predictions on a wide range of tasks during inference time. \nIn addition, they also design a mechanism that can leverage the next token’s linguistic embedding to steer the generation, and it demonstrates further improvements in specific downstream tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I think this paper has several strengths\n1. I like this idea: It is simple and flexible – it doesn’t require sophisticated architectural updates of the language models and can be applied in many different cases (even for recent new tokenizer-free LMs). Also the LIME+ steering is a really good addition to this approach and seems to work well. \n2. The experimental design is solid and the results seem to show that the method can be beneficial for relatively small scale language models. (I think at a larger scale, the language model may be able to learn such information relatively easily and the improvements may be less significant, but I think the improvement should be solid at a small scale.)  \n3. The method can solidly improve the generation quality – I think it’s reasonable in that by incorporating the linguistic properties and better tokenization, it makes it easier for the models to make next token predictions. Effectively, the linguistic representation can guide the model to search inside a smaller latent space, and thus the models can allocate more predictive power for the problem itself rather than the prediction tasks."}, "weaknesses": {"value": "I don’t see a clear weakness of this paper. I think the method, experiment, and presentation is solid. One critique might be that the conclusions might be different at a larger model scale, yet I don’t think that’s feasible to do in an academic environment."}, "questions": {"value": "1. For generation tasks, when only partial words (i.e., only having one or few tokens in a word) are generated, how does the method deduce the linguistic labels for them? \n2. I am in particular interested in some corner cases – how would the model react when spacy makes an incorrect prediction for a token? Also can the used  handle multi-lingual inputs – it seems only English Spacy is used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lBLVcgvB40", "forum": "Gl2XTqL9T7", "replyto": "Gl2XTqL9T7", "signatures": ["ICLR.cc/2026/Conference/Submission19080/Reviewer_LK2J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19080/Reviewer_LK2J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125861862, "cdate": 1762125861862, "tmdate": 1762931102621, "mdate": 1762931102621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}