{"id": "9YYxUKe7Ru", "number": 6751, "cdate": 1757994545930, "mdate": 1759897896499, "content": {"title": "LoopTool: Closing the Data–Training Loop for Robust LLM Tool Calls", "abstract": "Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency.\nWe introduce \\textbf{LoopTool}, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) \\textit{Greedy Capability Probing (GCP)} diagnoses the model's mastered and failed capabilities; (2) \\textit{Judgement-Guided Label Verification (JGLV)} uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) \\textit{Error-Driven Data Expansion (EDDE)} generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs.\nExperiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.", "tldr": "", "keywords": ["Tool-Call Agent", "Iterative Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9edab6996aa762b17701f54a830aa51e6af37435.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LoopTool, a closed-loop, model-aware data evolution framework for improving large language models’ (LLMs) tool-use capabilities. The framework iteratively refines both data and model parameters using GRPO reinforcement learning, achieving significant improvements. The resulting 8B model surpasses its 32B generator and attains state-of-the-art results on BFCL-v3 and ACEBench benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive framework design. The integration of GCP, JGLV, and EDDE forms a conceptually complete pipeline that addresses diagnosis, label refinement, and data augmentation in a unified framework.\n2. Empirical completeness. The authors present quantitative results on multiple public benchmarks (BFCL-v3 and ACEBench) and include ablations for each component (Table 4), demonstrating systematic experimentation."}, "weaknesses": {"value": "1. Poor presentation quality.\n   - The contribution list reads more like an implementation description, especially for contribution 2 and 3.\n   - Figures and tables are poorly formatted—many are embedded directly within paragraphs with insufficient spacing, which severely disrupts readability.\n   - Figure 1 is visually unpolished, and the label “Greedy Capacity Probing” appears to be a typographical error, as “Capability” is used elsewhere.\n\n2. lack of novalty. The paper claims that existing approaches treat data generation and model training as two non-interactive processes. However, similar ideas have been explored in prior self-adaptation [1] and self-challenging [2] paradigms, where models iteratively generate or select new data based on their own behavior to improve subsequent training. As such, the conceptual contribution of LoopTool appears incremental rather than fundamentally novel.\n\n3. Marginal performance improvements. Despite the complexity of the proposed iterative framework, the reported gains over training solely on the initial seed dataset are relatively modest (see Figure 2). Considering the substantial additional overhead—such as repeated data synthesis and the reliance on a larger model for data generation—the cost–benefit ratio appears unfavorable.\n\n4. Limited experimental scope on model backbone.\nThe experiments are conducted solely on the Qwen3 series, without evaluation on models of different architectures. As a result, the generality of the proposed approach remains unverified.\n\n[1] A. Zweiger et al. Self-Adapting Language Models\n\n[2] Y. Zhou et al., Self-Challenging Language Model Agents"}, "questions": {"value": "1. Given the limited improvement, what is the practical efficiency gain (if any) when accounting for computation and time cost?\n2. How does LoopTool compare against simpler iterative fine-tuning baselines that periodically regenerate part of data without the additional JGLV/EDDE modules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MljQU6bDaI", "forum": "9YYxUKe7Ru", "replyto": "9YYxUKe7Ru", "signatures": ["ICLR.cc/2026/Conference/Submission6751/Reviewer_F8Dd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6751/Reviewer_F8Dd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761066902274, "cdate": 1761066902274, "tmdate": 1762919034245, "mdate": 1762919034245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LoopTool is a closed-loop, model-aware data–training system for tool-augmented LLMs. It repeatedly diagnoses capability gaps, verifies/corrects labels, and expands hard examples, then retrains with GRPO—so the data distribution adapts to the model’s evolving needs. On BFCL-v3 and ACEBench, the 8B student surpasses peer open-source models of similar size and, on several dimensions, even outperforms its 32B generator/judge. Ablations and iteration curves show that high-PPL sampling, JGLV, and EDDE each contribute substantially to the gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A fully automatic, model-aware iterative pipeline that tightly couples data generation with training for tool use; continual diagnosis and error-targeted synthesis keep supervision aligned with the model’s evolving capabilities.\n\n2. JGLV and EDDE are well-motivated and practically effective.\n\n3. Solid coverage of executable benchmarks (BFCL-v3, ACEBench) plus thorough ablations and iteration analyses."}, "weaknesses": {"value": "1. Training and closed-loop verification are largely confined to the Qwen family (student, generator, and judge), raising concerns about same-source bias and cross-backbone generalization.\n\n2. The GRPO + binary reward setup lacks convergence and stability analysis."}, "questions": {"value": "1. Does LoopTool generalize beyond Qwen backbones under matched budgets and training steps?\n\n2. Can the RL component provide convergence/stability guarantees or empirical bounds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zug2QrEdCC", "forum": "9YYxUKe7Ru", "replyto": "9YYxUKe7Ru", "signatures": ["ICLR.cc/2026/Conference/Submission6751/Reviewer_N4cR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6751/Reviewer_N4cR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761207743748, "cdate": 1761207743748, "tmdate": 1762919033782, "mdate": 1762919033782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LoopTool, a framework designed to improve the tool-calling capabilities of LLMs by creating a dynamic, \"closed-loop\" process between model training and data synthesis. The authors identify the limitations of static training datasets, which fail to adapt to a model's evolving state or correct for persistent label noise. LoopTool addresses this by iteratively: (1) diagnosing model weaknesses using Greedy Capability Probing (GCP); (2) using a judge model to verify and correct errors in both model predictions and the original dataset labels (Judgement-Guided Label Verification, JGLV); and (3) generating new, challenging data specifically targeting these identified failures (Error-Driven Data Expansion, EDDE). The authors demonstrate that an 8B model trained with this framework achieves state-of-the-art results for its scale on the BFCL-v3 and ACEBench benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental evaluation is a clear strength. The authors provide a validation on two relevant benchmarks (BFCL-v3 and ACEBench). The ablation studies are comprehensive and effectively isolate the contributions of each component.\n2. The paper is well-written and clearly structured. The proposed LoopTool framework and its three constituent modules are explained in a logical and easy-to-follow manner."}, "weaknesses": {"value": "1. This paper has a limited contribution. The core idea of identifying model failures, synthesizing targeted \"hard\" data based on those failures, correcting errors, and retraining is a very direct and intuitive workflow. This process mirrors the standard approach that many practitioners would intuitively apply when attempting to improve a model's performance on a specific, well-defined task.\n2. While the authors have effectively engineered and automated this process into a \"framework,\" the conceptual contribution feels incremental. The individual components are not novel in themselves. Error Diagnosis (GCP): Analyzing model failures is a standard part of any development cycle. Targeted Synthesis (EDDE): Using identified errors to generate new, hard samples is a well-known concept in data augmentation and curriculum learning."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MduE6EX4Y6", "forum": "9YYxUKe7Ru", "replyto": "9YYxUKe7Ru", "signatures": ["ICLR.cc/2026/Conference/Submission6751/Reviewer_MRJu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6751/Reviewer_MRJu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761380458977, "cdate": 1761380458977, "tmdate": 1762919033210, "mdate": 1762919033210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LoopTool proposes a closed-loop, model-aware framework for improving tool-use capabilities of large language models (LLMs) by tightly integrating data synthesis and model training. The pipeline iteratively diagnoses model weaknesses via Greedy Capability Probing, refines noisy labels using Judgement-Guided Label Verification with an open-source judge model (Qwen3-32B), and expands hard examples through Error-Driven Data Expansion. Experiments show that an 8B model trained with LoopTool outperforms its 32B data generator and achieves state-of-the-art results on BFCL-v3 and ACEBench among models of similar scale—all without relying on closed-source APIs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.LoopTool introduces a fully automated, self-contained iterative pipeline that dynamically adapts training data to the model’s evolving capabilities, significantly improving tool-calling performance while avoiding costly closed-source models.   \n2.The framework uniquely combines label verification and error-driven data expansion in a synergistic loop, enabling both purification of noisy synthetic data and targeted generation of challenging samples, which leads to measurable gains over strong baselines."}, "weaknesses": {"value": "1.Using Qwen3-32B as the evaluator may introduce errors due to its limited capability, potentially causing error accumulation across iterations; the paper does not adequately address how such annotation errors are mitigated over successive loops.  \n2.The core idea of updating training data based on model performance during iterative training has been explored in prior work such as REVERSEGEN[1]; the paper lacks a clear comparison highlighting its conceptual or technical distinctions.  \n3.The experiments are limited to only four iterations, with diminishing returns observed; the paper does not investigate whether a performance saturation point exists or whether further iterations could harm generalization by overfitting to hard examples.  \n4.The effectiveness of data generated and verified by Qwen3-32B is only evaluated on Qwen-based models; it remains unclear whether this data benefits other model families such as Llama.\n\n[1]Forewarned is Forearmed: Leveraging LLMs for Data Synthesis through Failure-Inducing Exploration, ICLR 2025"}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DnM0rZas70", "forum": "9YYxUKe7Ru", "replyto": "9YYxUKe7Ru", "signatures": ["ICLR.cc/2026/Conference/Submission6751/Reviewer_Lr4c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6751/Reviewer_Lr4c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990438392, "cdate": 1761990438392, "tmdate": 1762919032800, "mdate": 1762919032800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}