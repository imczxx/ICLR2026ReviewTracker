{"id": "5F93RfQ12T", "number": 9573, "cdate": 1758128242363, "mdate": 1763646178968, "content": {"title": "Efficient Zero-shot Inpainting with Decoupled Diffusion Guidance", "abstract": "Diffusion models have emerged as powerful priors for image editing tasks such as inpainting and local modification, where the objective is to generate realistic content that remains consistent with observed regions. In particular, zero-shot approaches that leverage a pretrained diffusion model, without any retraining, have been shown to achieve highly effective reconstructions. However, state-of-the-art zero-shot methods typically rely on a sequence of surrogate likelihood functions, whose scores are used as proxies for the ideal score. This procedure however requires vector-Jacobian products through the denoiser at every reverse step, introducing significant memory and runtime overhead. To address this issue, we propose a new likelihood surrogate that yields simple and efficient to sample Gaussian posterior transitions, sidestepping the  backpropagation through the denoiser network. Our extensive experiments show that our method achieves strong observation consistency compared with fine-tuned baselines and produces coherent, high-quality reconstructions, all while significantly reducing inference cost.", "tldr": "", "keywords": ["Diffusion models", "zero-shot", "guidance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30c10b4a11318c2a1e18c8cea9caa4db2377391b.pdf", "supplementary_material": "/attachment/68e762847db37bf8ba901bf5d4119ade7ede8a8e.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes DING, a zero-shot diffusion-based inpainting framework that eliminates vector-Jacobian product (VJP) computations by decoupling the likelihood evaluation from the denoiser input. The method claims efficiency gains (reduced memory/runtime) while maintaining or surpassing the quality of prior zero-shot and fine-tuned baselines on FFHQ, DIV2K, and PIE-Bench datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, and the mathematical exposition is clear.\n2. The authors provide extensive experimental results across multiple datasets and metrics, consistently showing DING’s advantages. They also make a commendable effort to ensure fair comparisons, clearly documenting baseline hyperparameters.\n3. The inclusion of runtime and memory analyses further strengthens the empirical evaluation."}, "weaknesses": {"value": "1. The mathematical derivation in Section 3.2 relies on several unverified approximations (see questions below). These steps seem heuristic and lack theoretical justification. Moreover, the closed-form update in Eq. (3.4) depends critically on Gaussian-Gaussian conjugacy, which holds only for linear masking operators. As a result, the proposed method is mathematically fragile and inherently task-specific. Given this restriction, there is little reason to prefer a task-agnostic zero-shot formulation over simply training a dedicated inpainting model, since DING offers no clear theoretical or practical advantage beyond the inpainting setting.\n2. The paper evaluates against general posterior-sampling methods but omits comparison to established fast zero-shot inverse-problem solvers such as DDNM and DiffPIR, which also target inpainting and avoid VJP overhead. DDNM, in particular, is well known for producing high-quality inpainting results when $σ_y$=0.\n3. Although the derivation includes a measurement-noise parameter ($σ_y$) in the likelihood and posterior update (Eq. 3.4), all experiments appear to assume a noise-free setting ($σ_y$=0.0). Consequently, the practical behavior of DING under noisy observations is unknown. Since $σ_y$ directly controls the trade-off between data fidelity and prior regularization, omitting this analysis limits the method’s generality and makes it unclear whether the surrogate posterior remains effective when measurements are noisy.\n4. DING is reported to outperform an SD3.5-Inpainting model trained for 12 M iterations with ControlNet conditioning. This is counterintuitive to me, as I’d expect a model of that scale, trained so extensively for a dedicated inpainting task, to possess a stronger conditional prior in principle. Although the authors match wall-clock runtime between DING and the fine-tuned SD3.5-Inpainting model, the comparison remains asymmetric. DING performs 56 denoiser evaluations (iterative posterior refinement), whereas the fine-tuned model runs only 28 steps of one-shot generation.\n\n**Minor Comments:**\n- Lines 397-402, the authors repeat themselves across two consecutive sentences.\n- Including results on random inpainting masks would also strengthen the evaluation and demonstrate robustness across mask types."}, "questions": {"value": "1. *Regarding weakness 1:* In equation (3.4), authors define the mixture of Gaussians (over $\\mathbf{z}_s$) that DING samples from. But the true posterior transition π($\\mathbf{x}_s​$∣$\\mathbf{x}_t$​, $\\mathbf{y}$) is not Gaussian as it depends nonlinearly on the denoiser. So under what conditions does the Gaussian mixture in Eq. (3.4) approximate the true posterior transition? Is there a theoretical approximation guarantee?\n2. *Regarding weakness 1:* By decoupling the denoiser input from the latent, does DING still produce unbiased posterior means and variances?\n3. *Regarding weakness 3:* Can authors comment on DING’s performance when $σ_y$ > 0? Does the method remain stable and accurate when the measurement noise increases?\n4. *Regarding weakness 4:* Would DING still outperform the fine-tuned inpainting model if both used the same number of diffusion steps, and the fine-tuned model employed its default CFG weighting of 7.0 instead of the reduced value of 4.0?\n5. *Regarding weakness 4:* In Figure 4 and Table 4, the fine-tuned inpainting model appears to modify observed pixels, resulting in lower cPSNR. This behavior is unexpected. Can the authors explain the cause? Is it due to suboptimal hyperparameters, the 512×512 test resolution differing from the model’s 1024×1024 training resolution, or an architectural limitation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cMlWArUj3B", "forum": "5F93RfQ12T", "replyto": "5F93RfQ12T", "signatures": ["ICLR.cc/2026/Conference/Submission9573/Reviewer_S6KU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9573/Reviewer_S6KU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936385048, "cdate": 1761936385048, "tmdate": 1762921126588, "mdate": 1762921126588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DING (Decoupled INpainting Guidance), a zero-shot diffusion-based inpainting method that eliminates the need for backpropagation through the denoiser during inference. Existing zero-shot inpainting methods require computing vector–Jacobian products (VJPs) through the diffusion denoiser network at every reverse step, which leads to high computational and memory costs.\nDING addresses this by proposing a decoupled likelihood approximation, where the denoiser is evaluated at an independent proxy variable rather than the current latent state. This removes the dependency between the denoiser and the transition density, enabling closed-form Gaussian posterior sampling without backpropagation. The authors evaluate DING on multiple benchmarks using Stable Diffusion 3.5 as the pretrained prior. DING demonstrates superior trade-offs between reconstruction fidelity, perceptual realism, and computational efficiency compared to recent zero-shot inpainting and posterior-sampling methods such as FLOWCHEF, DAPS, and PSLD. The proposed method even outperforms a fine-tuned SD3 inpainting model despite requiring no additional training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The computational inefficiency of current zero-shot diffusion-based inpainting methods is a relevant and well-identified issue.\n+ The “decoupling” of the denoiser input and the posterior likelihood evaluation is a simple but effective modification that removes the need for costly gradient computations.\n+ Experimental results show clear improvements in runtime and memory consumption.\n+ DING achieves consistently better or comparable scores in FID, pFID, LPIPS, and cPSNR across datasets and even outperforms fine-tuned baselines under a fair computational budget."}, "weaknesses": {"value": "- While the decoupling trick is brilliant, the method builds upon a well-studied line of “posterior sampling with diffusion priors”, such as DPS, FLOWDPS, DAPS, PnP-FLOW, and mainly contributes an engineering-level efficiency improvement rather than a new theoretical insight.\n- The derivation of the closed-form Gaussian posterior is mathematically sound, but the paper lacks deeper analysis or guarantees about the approximation error introduced by the decoupling. The Bayesian justification is somewhat heuristic.\n- DING is designed only for inpainting, as the Gaussian observation model is directly tied to pixel-wise missing-region reconstruction. The paper’s claim that the approach can generalize to “other inverse problems” remains speculative.\n- The paper is dense with mathematical notation and references, for example, multiple layers of priors, posterior transitions, likelihood surrogates, which makes it unnecessarily hard to follow, especially in Sections 2–3.\n- All experiments use Stable Diffusion 3.5 with standard image domains. It would strengthen the work to include more challenging or domain-specific cases (e.g., medical or scientific imaging) where zero-shot efficiency matters most."}, "questions": {"value": "1. Can the authors clarify how runtime and memory are measured? Specifically, does the reported 2.9s runtime include both proxy sampling and denoiser evaluations (2× per step)?\n2. Since the derivation assumes a Gaussian likelihood over masked pixels, how easily can this framework extend to deblurring, super-resolution, or compressive sensing tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tkFz2n2hpq", "forum": "5F93RfQ12T", "replyto": "5F93RfQ12T", "signatures": ["ICLR.cc/2026/Conference/Submission9573/Reviewer_PHGS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9573/Reviewer_PHGS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973944209, "cdate": 1761973944209, "tmdate": 1762921126278, "mdate": 1762921126278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new idea for diffusion-based inpainting based on classifier guidance. Classifier guidance is a well-established technique for conditioning, but it has some shortcomings (the $t$-step conditioning distributions have to be all known). The paper expands on some well-known approximation ideas to devise a new form of transition, which does not require the additional backpropagation step for the classifier guidance. Based on the approximated conditioned transition formulation, the paper introduces an auxiliary variable to make the transition probability a mixture distribution. Now, the transition can be performed exactly in a Monte Carlo manner. Experiments demonstrate that the proposed method achieves superb performance with large gaps."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Clever idea. I like the reformulation technique. Incorporating the Dirac approximation idea into the conditional transition formulation is one thing, but the auxiliary variable idea is also a nice touch.\n\n- State-of-the-art performance with large gaps."}, "weaknesses": {"value": "- I'm generally convinced by the formulation and the results, but one question remains: Seeing \"Related methods\" section and Appendix A.2, I cannot help but think whether the proposed method is just providing an alternative story to already existing methods. Appendix A.2 highlights some technical differences, but it is difficult to grasp the distinct value of the proposed method. What is the practical/core advantage of the proposed method in terms of theory/effectiveness compared to the methods mentioned in those sections? On the other hand, PnPFlow shows much worse performance in the experiments. Why is this?\n\n- My review score is currently 6, but it is solely due to the above concern. I'll decide my final score after seeing the rebuttal."}, "questions": {"value": "Please see the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7NrJ1kHEzo", "forum": "5F93RfQ12T", "replyto": "5F93RfQ12T", "signatures": ["ICLR.cc/2026/Conference/Submission9573/Reviewer_MmE3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9573/Reviewer_MmE3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020708496, "cdate": 1762020708496, "tmdate": 1762921125718, "mdate": 1762921125718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DING (Decoupled INpainting GuidANCE), a zero-shot inpainting method that leverages a novel likelihood surrogate to enable efficient sampling with pre-trained diffusion models. By decoupling the denoiser evaluation from the likelihood surrogate, DING eliminates the need for costly vector-Jacobian products (VJPs) or backpropagation through the denoiser at each sampling step, leading to significant reductions in runtime and memory usage. Extensive experiments on benchmarks including FFHQ, DIV2K, and PIE-Bench demonstrate that DING achieves a strong trade-off between fidelity and realism, outperforming both state-of-the-art zero-shot methods and a fine-tuned Stable Diffusion 3 inpainting model, particularly under low NFE (number of function evaluations) budgets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed decoupled likelihood surrogate enables VJP-free sampling, directly addressing a major bottleneck in prior zero-shot diffusion guidance methods. This results in significant reductions in both runtime and memory usage, as empirically validated in Table 1.\n2.  Across three challenging inpainting benchmarks (FFHQ, DIV2K, PIE-Bench), DING consistently outperforms existing zero-shot baselines and even surpasses a specialized fine-tuned SD3 model. The results are supported by both quantitative metrics (Tables 2–7) and compelling qualitative examples (Figures 4–8).\n3.   The method operates efficiently in latent space and requires only forward passes through the denoiser, making it highly suitable for real-world applications with constrained computational resources.\n4. The paper includes thorough ablation studies (e.g., on the necessity of doubled NFEs per step in Table 6, and DDIM noise schedules in Table 7), and baselines are carefully tuned to ensure fair comparisons."}, "weaknesses": {"value": "1. While the decoupled likelihood surrogate is a clear improvement in efficiency and simplicity, it can be viewed as an incremental extension of existing VJP-free or mixture-based guidance methods (e.g., Wu et al., 2023; Janati et al., 2024/2025a). The paper could more explicitly articulate its conceptual departure from these prior works to better highlight its foundational contribution.\n 2. The manuscript lacks a systematic analysis of failure cases or challenging scenarios, such as highly irregular masks, extreme semantic gaps, or out-of-distribution content. A more detailed discussion of limitations would strengthen the paper’s practical relevance.\n3. Although the paper compares against several strong baselines, it omits recent zero-shot inpainting or image-editing methods such as “Inpaint Anything,” “DiffEdit,” or “pix2pix-zero.” Including these would provide a more comprehensive evaluation of DING’s relative performance and efficiency."}, "questions": {"value": "1. Could the authors provide a more systematic analysis of where DING tends to fail? For instance, how does it perform on very large or very small masked regions, or in cases with significant semantic discontinuities? Does it ever overfit to the observed pixels at the expense of realism or coherence?\n2. Are the authors able to include comparisons with recent methods such as “Inpaint Anything,” “DiffEdit,” or other zero-shot image-editing approaches? If not, could they at least provide a qualitative or quantitative discussion of how DING might compare in terms of performance and efficiency?\n3. The method is currently tailored for inpainting. Could the authors comment on the feasibility of extending the decoupled guidance framework to other inverse problems (e.g., super-resolution, deblurring) while maintaining similar efficiency gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zuLisqGi13", "forum": "5F93RfQ12T", "replyto": "5F93RfQ12T", "signatures": ["ICLR.cc/2026/Conference/Submission9573/Reviewer_SWvq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9573/Reviewer_SWvq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073227592, "cdate": 1762073227592, "tmdate": 1762921125388, "mdate": 1762921125388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}