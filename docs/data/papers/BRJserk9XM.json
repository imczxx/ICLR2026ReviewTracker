{"id": "BRJserk9XM", "number": 5936, "cdate": 1757947677280, "mdate": 1763673043277, "content": {"title": "Repair Aware Forgetting: An Iterative Approach to Unlearning in T2I Diffusion Models", "abstract": "Text-to-image diffusion models trained on web-scale data can reproduce unsafe, private, or copyrighted content. We seek an unlearning procedure that removes such content while explicitly preserving benign performance. We formulate unlearning as repair-aware constrained optimization and introduce PURE (Preference-based UnleaRning in tExt-to-image diffusion). PURE operationalizes this with three ideas: (i) a distributional trust region around a strong reference model via a KL penalty so forgetting cannot drift on retain prompts; (ii) a diffusion-tailored, negative-only preference objective that downweights unsafe generations without paired safe examples; and (iii) an alternating schedule of short forgetting steps and lightweight repair steps on retain data that yields self-stabilizing updates and keeps image quality high. On Imagenette, PURE achieves almost perfect unlearning in 100 steps with near-perfect retain accuracy and the best FID among baselines. On I2P, it reduces NSFW generations by over 50% relative to prior state of the art, using only 50 forget samples on a single GPU. PURE is simple to implement, and both sample and compute efficient. Overall, PURE consistently outperforms ESD, FMN, and SalUn on both unlearning efficacy and fidelity, demonstrating a practical path to safe T2I diffusion without retraining or paired supervision.", "tldr": "PURE is a repair-aware unlearning approach for T2I diffusion that uses a negative-only preference based forget loss, with an alternating forget-then-repair schedule, to erase targeted content while preserving retain accuracy and low FID.", "keywords": ["unlearning", "image generation", "text-to-image", "diffusion models", "preference alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/730c0ad037d112f0a0376eb75a09716e4d780738.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce PURE (Preference-based UnleaRning in tExt-to-image diffusion), a concept unlearning method for diffusion models. The key advantages of the method are that it operates on un-paired negative samples, and keeps quality high on concepts unrelated to the one being forgotten."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Doesn't require safe pairs for negative samples defining the concept\n- Keeps quality high on concepts unrelated to the one being forgotten.\n- Computationally and sample efficient"}, "weaknesses": {"value": "- The work does not compare to recent work such as AdvUnlearn (Zhang 2024, https://arxiv.org/abs/2405.15234), which also use an explicit repair loss component and outperform SalUn on some benchmarks.\n- The work does not ablate the impact of the alternating objectives in PURE. How does K_FG and K_RT impact performance? What if they are optimized at the same time (such as with gradient accumulation)?"}, "questions": {"value": "- How does PURE compare to AdvUnlearn? (Zhang 2024, https://arxiv.org/abs/2405.15234)\n- What is the impact of the alternating objectives in PURE? In particular:\n    - How do K_FG and K_RT impact performance? \n    - What if they are optimized at the same time (such as with gradient accumulation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns. Dangers with NFSW and sensible material are professionally discussed and treated."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vMxYL8CZHu", "forum": "BRJserk9XM", "replyto": "BRJserk9XM", "signatures": ["ICLR.cc/2026/Conference/Submission5936/Reviewer_48ED"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5936/Reviewer_48ED"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761689946371, "cdate": 1761689946371, "tmdate": 1762918362085, "mdate": 1762918362085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for \"unlearning\" a conditional distribution from a text2image diffusion model. The approach uses the negated preference loss over the samples it shouldn't be able to score, and in between ensures that the model doesn't steer away from the original distribution using KL regularization within a neighborhood (the trust region). The performance of the model is evaluated on several metrics, and it performs overall well; most importantly, it seems not to mess with the distribution outside of the defined scope. The method is evaluated on Imagenette and prompts that promote nudity or similar images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The repair-aware unlearning is formulated well and derived as a KL-constrained objective, which is well documented and used in many settings. The approach is \"simple\" in the good sense, easy to understand, and the approach performs well."}, "weaknesses": {"value": "The main weakness of the paper is a lack of ablations. The paper’s claims rely on repair-awareness and the KL trust region, but key isolations are absent.\n\n1. For instance, is the alternation necessary? Why not mix them?\n2. What if the KL term is dropped?\n3. What if GA is combined with the KL? (i.e. not the dpo style and use retain data)\n\nThe lack of ablations is the main reason for the current score. I look forward to seeing whether you address this or convince me why it's not needed."}, "questions": {"value": "Did you consider using a clip model to check for the UA? Or some sort of OOD UA given overlap with the targeted images. E.g., if \"dog in snow\" is targeted, can it still generate \"dog in sand\" or does it only remove the compositional setting? The metrics used rely on downstream classifiers; it would be curious to see a zero-shot one used also.\n\nHow dows RA/FID/GenA vary with the $\\beta$ ? Only UA is shown. \n\nWhy did you not try to unlearn more than one concept?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c2X5Lryr6v", "forum": "BRJserk9XM", "replyto": "BRJserk9XM", "signatures": ["ICLR.cc/2026/Conference/Submission5936/Reviewer_YqSE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5936/Reviewer_YqSE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753947260, "cdate": 1761753947260, "tmdate": 1762918361670, "mdate": 1762918361670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PURE (Preference-based UnleaRning in tExt-to-image diffusion)—a repair‑aware unlearning method for T2I diffusion models. The key idea is to alternate short forgetting steps with lightweight repair steps while constraining each forgetting update to stay close to a strong reference model via a KL trust region. Methodologically, the authors (i) formulate unlearning as KL‑regularized constrained optimization, (ii) derive a negative‑only preference objective for diffusion by marginalizing the (unknown) preferred sample and applying Jensen’s inequality, yielding a path‑averaged logistic loss computable with standard diffusion estimators, and (iii) implement an alternating schedule that stabilizes training and preserves benign capabilities. On Imagenette class‑wise forgetting, PURE attains 100% Unlearning Accuracy (UA) with ~100 steps, ~99.6% Retain Accuracy (RA), and the best FID among training baselines; on I2P, PURE reduces NSFW generations by >50% using only 50 forget samples on a single A100. An ablation‑like comparison shows improved stability and sample/compute efficiency versus ESD, SalUn, and GA (+repair), with FMN strong for generalization but poor at actual unlearning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The KL trust‑region view ties forgetting to utility preservation and leads to a principled DPO‑style negative‑only loss for diffusion; the derivation is neat and implementation‑friendly.\n\n+ Table 2 (p.7) shows 100% UA, ~99.6% RA, FID 0.97, outperforming ESD/SalUn/GA(+repair) in both unlearning and fidelity; FMN fares well on generalization but substantially fails unlearning—a helpful diagnostic.\n\n+ Uses only 50 forget samples and one A100, highlighting good practicality."}, "weaknesses": {"value": "+ Safety is evaluated primarily on nudity (I2P) using a single detector (NudeNet) and template prompts. Generality to other unsafe concepts (violence, copyrighted logos, protected attributes), adversarial prompts, and jailbreaks is not assessed.\n\n+ UA/RA/Gen.A rely on classifier accuracy (Imagenette, ImageNet tail), which are proxies for generative behavior; failure or bias in the classifier can misstate UA/RA.\n\n+ Lack of ablations on KFG/KRT (phase lengths), and retain/forget set sizes.\n\n+ The KL constraint is enforced on retain prompts used in training. It is not obvious how well this regularizes behavior for unseen retain prompts, or under prompt composition (retain + near‑unsafe semantics). Empirical stress tests would help."}, "questions": {"value": "+ Could you provide sensitivity curves for KFG/KRT (forget vs. repair steps), and show the UA/RA/FID trade‑off under these variations?\n\n+ How does PURE perform on other unsafe targets (e.g., violence, copyrighted characters, hate symbols), and with adversarially composed prompts?\n\n+ How does performance scale with forget set size (from 10→100 samples) and with retain set size? Is there a minimal retain budget where repair remains effective?\n\n+ Please provide exact GPU‑hours, wall‑clock per step, and FLOPs for PURE and training baselines under the same forget‑set size to backup your efficiency claims"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m9XFBJtHfe", "forum": "BRJserk9XM", "replyto": "BRJserk9XM", "signatures": ["ICLR.cc/2026/Conference/Submission5936/Reviewer_joSz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5936/Reviewer_joSz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845702192, "cdate": 1761845702192, "tmdate": 1762918361220, "mdate": 1762918361220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}