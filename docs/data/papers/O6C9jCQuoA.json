{"id": "O6C9jCQuoA", "number": 5114, "cdate": 1757850582464, "mdate": 1759897993863, "content": {"title": "Neural Darwinism: A Theoretical Framework for Representation Evolution in Convolutional Neural Networks", "abstract": "We propose a Darwinian framework that provides a mathematical foundation for neural representation dynamics, conceptualizing deep learning as an evolutionary process governed by selection and adaptation. In this framework, neurons are treated as competing entities whose survival is determined by a Darwinian Score, a derived fitness measure  reflecting the informational richness, functional relevance, and adaptive dynamics of neural activity over training. This score induces a constrained optimization problem that explicitly balances model compactness with predictive fidelity, from which we derive theoretical guarantees on approximation error. Building on this foundation, we introduce Neural Darwinian Culling (NDC), a practical algorithm that adaptively eliminates neurons of low evolutionary fitness during training while preserving high-value units. Experiments on standard vision benchmarks show that NDC achieves substantially higher sparsity with improved generalization compared to state-of-the-art pruning methods, and ablation studies confirm the effectiveness of the Darwinian Score as a unifying principle. Our work thus establishes both a general evolutionary framework of neural representation and its operational realization for efficient deep learning.", "tldr": "", "keywords": ["Neural Darwinism", "Darwinian Score", "Neural Darwinian Culling", "Convolutional Neural Networks"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9521f7f6b7e5feeb65d067bc0df9e36b9d8e9c62.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper propose to conduct neural network pruning by using the proposed the Darwinian score, which is designed to measure the information richness, functional relevance, and adaptive dynamics of each neuron."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "None."}, "weaknesses": {"value": "1. The format of citation is wrong, which is hard to read.\n\n2. There are some strange statements, such as ' Because information-theoretic quantities appearing below are sensitive to .....', which is hard to understand.\n\n3. For the evolution-based pruning method, there should be a discussion about 'Towards evolutionary compression. KDD 2018.', which also implements network pruning with evolutionary algorithm.\n\n4. The calculation of the histogram/probability is problematic, how to cover the output range of activations such as ReLU? Which is impossible.\n\n5. For the Darwinian Entropy, the diversity of the probability does not equals to the diversity of the information, especially when there is no alignment between the variables. \n\n6. The compared methods are old (works from 2020 and 2022), which is hard to verify the effectiveness of the proposed method. Moreover, the reference for Cropit should be included."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rvAfAxsk9m", "forum": "O6C9jCQuoA", "replyto": "O6C9jCQuoA", "signatures": ["ICLR.cc/2026/Conference/Submission5114/Reviewer_RN6F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5114/Reviewer_RN6F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761497785896, "cdate": 1761497785896, "tmdate": 1762917884118, "mdate": 1762917884118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Darwinian framework that provides a mathematical foundation for\nneural representation dynamics, conceptualizing deep learning as an evolutionary\nprocess governed by selection and adaptation. This notion is formalized by defining a Darwinian Score (DS) that quantifies a neuron’s evolutionary fitness through three measurable components:\n\nNeuron Darwinian Entropy (NDE): information diversity and non-redundancy of activations;\n\nActivation–Gradient Contribution (AGC): functional contribution to loss minimization;\n\nNeuron Adaptivity Score (NAS): the degree of adaptive evolution of neuron parameters over training.\n\nThese measures are combined multiplicatively to enforce a “survival-of-the-fittest” criterion within neural architectures. Based on this theory, the authors propose the Neural Darwinism Culling (NDC) algorithm, which prunes neurons dynamically during training according to their Darwinian Score. Theoretical results guarantee bounded approximation error after pruning, and experiments on CIFAR-10 and Tiny-ImageNet demonstrate that NDC achieves superior accuracy and sparsity trade-offs compared to state-of-the-art pruning methods (SNIP, SNAP, CroPit, DPF, etc.)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Concept and biological inspiration - The idea of using Darwinian evolutionary process in deep learning is interesting and novel.\nTheory - The algorithm is supported by theorems and proofs.\nUnified view of pruning and adaptation- By tying representational diversity, functional importance, and adaptability into a single fitness score, the authors unify multiple pruning philosophies (magnitude-, sensitivity-, and gradient-based) under one framework.\nExperimental results - many experiments were conducted to support the claim."}, "weaknesses": {"value": "Theory - The main theorem assumes a long list of many assumptions that are not so natural or trivial to me. \nProofs - The proofs are quite straightforward and follow from importance sampling, without too many new observations.\nExperiments - The experiments are on small data sets and CNN type of networks, but I could not see a more modern results on transformers and larger datasets. In general, it is not clear how the idea is scalable, especially when we need to maintain all these scores along the training phase. \nRelation to existing evolutionary optimization – The connection to neuroevolution and evolutionary strategies could be elaborated further, particularly distinguishing this work from gradient-free evolutionary algorithms. Also, evolutionary algorithms are not considered a successful or common tool, both in the industry or academy."}, "questions": {"value": "Please explain in more details the quantitative analysis of the relative impact of NDE, AGC, and NAS components."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wWDMaQT2Br", "forum": "O6C9jCQuoA", "replyto": "O6C9jCQuoA", "signatures": ["ICLR.cc/2026/Conference/Submission5114/Reviewer_pyu3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5114/Reviewer_pyu3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890911988, "cdate": 1761890911988, "tmdate": 1762917883761, "mdate": 1762917883761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, a novel pruning framework, called Neural Darwinism Culling (NDC), is proposed. The key idea is that neurons are assumed to behave like competing agents, with which the authors derive the 'Darwinian score', that combines (a) an entropy/divergence–based activation statistic (NDE), (b) an activation–gradient coupling (AGC), and (c) a weight-trajectory “adaptivity” score (NAS). Theoretical guarantees for the efficacy of the proposed method are also provided. Last, empirical studies are conducted on various standard image processing tasks on standard CNN architectures, where the NDC method is shown to match or outperform near baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The Darwinism score, combining change in weights, information diversity (NDE, Def 3.2), and functional utility (based on the gradient score, AGC, in Def. 3.3) is useful and captures significant information about relevant neurons for pruning/retaining.\n* NDC appears to outperform several near baselines, with models pruned with it retaining higher accuracy across all sparsity levels.\n* The mathematical derivations appear to be correct (with a few caveats regarding assumptions, see Questions), and adds guarantees that motivates hte use of this method. \n* The writing of the paper is generally quite clear and easy to follow, though there is an issue with the Definitions in section 3 (see 'Weaknesses')."}, "weaknesses": {"value": "* The definitions proposed in Section 3 are overly long. For instance, in Definition 3.1 (NDE), the equation defining the NDE score is stated 18 lines after the definition begins. The setup should be kept outside the definition, which in turn should be crisp and self-contained.\n* The experimental slate should contain ImageNet experiments, which by now are standard in the pruning literature, instead of just TinyImageNet.\n* Theorem 3.6 states a worst-case change in prediction (over samples), while an Expectation bound would be more useful."}, "questions": {"value": "* How does the proposed method perform when the number of epochs is increased/decreased?\n* Have the authors considered incorporating the NDC method while training a model from scratch? That is, given a few initial training epochs, NDC would then be applied during training, thereby obtaining a sparse model from scratch. Is there any reason to think this is not possible?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BOzj1BW3Hy", "forum": "O6C9jCQuoA", "replyto": "O6C9jCQuoA", "signatures": ["ICLR.cc/2026/Conference/Submission5114/Reviewer_AtZi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5114/Reviewer_AtZi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998076274, "cdate": 1761998076274, "tmdate": 1762917883559, "mdate": 1762917883559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}