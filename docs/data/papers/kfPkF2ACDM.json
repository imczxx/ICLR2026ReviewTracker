{"id": "kfPkF2ACDM", "number": 7025, "cdate": 1758005324394, "mdate": 1759897877562, "content": {"title": "When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling", "abstract": "Ensembling Large Language Models (LLMs) has gained attention as a promising approach to surpass the performance of individual models by leveraging their complementary strengths. In particular, aggregating models’ next-token probability distributions to select the next token has been shown to be effective in various tasks. However, while successful for short-form answers, its application to long-form generation remains underexplored. In this paper, we show that using existing ensemble methods in long-form generation requires a careful choice of ensembling positions, since the standard practice of ensembling at every token often degrades performance. We identify two key factors for determining these positions: tokenization mismatch across models and consensus in their next-token probability distributions. Based on this, we propose $\\textbf{SAFE}$, ($\\textbf{S}$table $\\textbf{A}$nd $\\textbf{F}$ast LLM $\\textbf{E}$nsembling), a framework that selectively ensembles by jointly considering these factors. To further improve stability, we introduce a probability sharpening strategy that consolidates probabilities spread across multiple sub-word tokens representing the same word into a single representative token. Our experiments on diverse benchmarks, including MATH500 and BBH, demonstrate that SAFE outperforms existing methods in both accuracy and efficiency, with gains achieved even when ensembling fewer than 1\\% of tokens.", "tldr": "We propose a method that identifies the optimal points for applying ensembling, thereby improving both accuracy and efficiency in LLM collaboration.", "keywords": ["LLM Ensemble", "probability-level ensemble", "speculative decoding"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a544e7b6006eb9519ed78e22f632d3c302282eaf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies ensemble methods in long-form generation tasks, demonstrating that existing ensemble methods fail to perform well in these tasks because of tokenization mismatch across models and divergence in their next-token probability distributions. To address this problem, they propose SAFE, a novel ensemble method. Experiments show that SAFE achieves better performance and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The focus on long generation/CoT tasks is a novel perspective for model ensemble research.\n2. The proposed approach leads to better performance across different benchmarks and ensemble strategies. It also address the huge inference overhead.\n3. The writing is clear and easy to follow."}, "weaknesses": {"value": "1. Some related works should be discussed and compared, which could impact the novelty and relative performance:\n    a) Speculative decoding is already utilized to accelerate model ensemble in CoS [1].\n\n    a) The problem of OOV can be addressed by ensembling at a larger granularity (e.g., word of span), as shown in SweetSpan [2]. I think the same idea is used in SAFE.\n\n    b) EVA [3] (which is the first token-level ensembling approach) shows that the divergence between models could be addressed by dynamically adjusting the weight of different models according to their confidence. I would suggest that the authors try these approaches.\n\n2. For the verification step, the proposed token is only accepted when it is the most possible one for all models. However, modern LLMs typically use random sampling with topk/topp renorm for diversity. This is crucial for generating long sequences without rotting and further test-time scaling. I think expanding the acceptance threshold would be a more reasonable approach.\n\n[1] Fu, Jiale, et al. \"Fast Large Language Model Collaborative Decoding via Speculation.\" \n\n[2] Xu, Yangyifan, et al. \"Hit the sweet spot! span-level ensemble for large language models.\"\n\n[3] Xu, Yangyifan, et al. \"Bridging the Gap between Different Vocabularies for LLM Ensemble.\""}, "questions": {"value": "1. What is the main contribution compared to existing approaches?\n2. What would be the result if greedy decoding is replaced with common sampling techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D312rE8Ybw", "forum": "kfPkF2ACDM", "replyto": "kfPkF2ACDM", "signatures": ["ICLR.cc/2026/Conference/Submission7025/Reviewer_vm3X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7025/Reviewer_vm3X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761229592244, "cdate": 1761229592244, "tmdate": 1762919228294, "mdate": 1762919228294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new framework, SAFE, to determine when to ensemble models' next-token generation probability. The framework allows ensemble happen only when (1) no tokenization mismatch occurs across models and (2) the models’ next-token distributions show low consensus. SAFE increase model inference efficiency and accuracy on long-form generation under ensemble."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Disclaimer: reviewer is not an expert on the topic.\n\n1. The paper is well-presented.\n2. I think the paper's idea of using token mismatching and token confidence to decide whether to ensemble is intuitive. SAFE can be applied to any existing next-token ensemble method.\n3. The paper experiments clearly show gains for efficiency and accuracy for using SAFE."}, "weaknesses": {"value": "Disclaimer: reviewer is not an expert on the topic.\n\nI'd like to see an ablation study for how each of the two criteria in SAFE impact ensemble method performance.\n\nI will check other reviewers' comment on the weakness part of this paper."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vVPw30OKGR", "forum": "kfPkF2ACDM", "replyto": "kfPkF2ACDM", "signatures": ["ICLR.cc/2026/Conference/Submission7025/Reviewer_ux3H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7025/Reviewer_ux3H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761497525701, "cdate": 1761497525701, "tmdate": 1762919227908, "mdate": 1762919227908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that LLM ensembling only needs to occur for a subset of tokens determined by: (1) the mismatch in tokenization across different models and (2) the level of consensus in their next-token probability distributions. The authors introduce SAFE (Stable And Fast LLM Ensembling), a framework designed to find the best tokens for ensembling during long-sequence generation. SAFE uses a speculative strategy where one \"drafter\" model generates a lookahead sequence of tokens, and the other \"verifier\" models identifies the specific token-level points within that sequence that require ensembling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-motivated and clearly written.\n\n* The experiments are fairly rigorous. The method achieves better quality and efficiency over baseline ensembling methods.\n\n* A notable systems contribution is the implementation of KV caching for ensembling, which the authors also apply to baselines.\n\n* It is an interesting finding that ensembling at a small fraction of tokens (less than 20%) can significantly improve the overall generation quality."}, "weaknesses": {"value": "* The paper introduces the term \"OOV-like,\" but this appears to be identical to the existing concept of \"non-canonical\" tokenization found in prior work [1, 2, 3, 4]. The paper fails to cite or discuss this relevant literature on tokenization.\n\n* The probability sharpening strategy seems arbitrary. The choice of the 0.1 threshold is not well-justified. There are other standard ways to sharpen distributions (e.g., using a geometric mean instead of an arithmetic mean) that are not explored or compared.\n\n* The experiments relating to efficiency could be more complete to show where exactly the speedups are coming from (see questions below).\n\n[1] Cao, Kris, and Laura Rimell. \"You should evaluate your language model on marginal likelihood over tokenisations.\" arXiv preprint arXiv:2109.02550 (2021).\n\n[2] Geh, Renato Lui, et al. \"Where is the signal in tokenization space?.\" arXiv preprint arXiv:2408.08541 (2024).\n\n[3] Vieira, Tim, et al. \"Language Models over Canonical Byte-Pair Encodings.\" arXiv preprint arXiv:2506.07956 (2025).\n\n[4] Chatzi, Ivi, et al. \"Canonical Autoregressive Generation.\" arXiv preprint arXiv:2506.06446 (2025)."}, "questions": {"value": "1. To confirm my understanding, is it true that if you apply SAFE ensembling between a model and itself, it will give the same result as just sampling directly from the model?  I want to confirm that ensembling does not induce temperature-scaling-like effects that boost performance.\n\n2. The authors repeatedly claim that vocabulary alignment is a primary expense, e.g., \"the number of costly ensemble operations grows with sequence length. This expense arises primarily from vocabulary alignment...\" and \"high computational cost of repeated vocabulary alignment makes such approaches inefficient.\" Can you elaborate on this? My understanding is that the primary bottleneck for ensembling is the cost of forward passes through multiple models, not vocabulary alignment. Why is alignment expensive in this setup?\n\n3. The latency is shown to be lower, but what is the maximum memory usage (peak memory) of SAFE compared to the baselines?\n\n4. What is the computational overhead of repeatedly calling the verifier models' tokenizers during the verify step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TFfvzdblyr", "forum": "kfPkF2ACDM", "replyto": "kfPkF2ACDM", "signatures": ["ICLR.cc/2026/Conference/Submission7025/Reviewer_gPZv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7025/Reviewer_gPZv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761613039667, "cdate": 1761613039667, "tmdate": 1762919227562, "mdate": 1762919227562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a LLM Ensemble method, specifically categorized as an ensemble-during-inference method.   \nThe proposed method uses a main model to generate fragment-level information, and then utilizes several other models as validators to check the \"potential error-prone areas\", and finally integrates the outputs of multiple models in these areas."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed generate-verify-ensemble method, in terms of its overall approach and framework design (i.e., from a high-level perspective), is reasonable and intuitive."}, "weaknesses": {"value": "**Weaknesses and Suggestions.**\n\n\n1) The presentation of the method and experiments is not clear enough. This version of the paper requires significant revisions to make the presentation clearer and more accessible for readers.  \nSpecifically: The presentation in the method section (Section 3) is not clear enough. Regarding the experiment section, for example: (i) In Section 4.2, when introducing the set of LLMs considered, mentioning the prior work proposed by Yao et al. is unnecessary and may cause confusion, as the LLM set in this paper differs from that in the prior work. (ii) The meanings of \"GaC + SAFTE\" and \"UniTE + SAFE\" are not clearly explained in the Section 4.2.\n\n2) For the related work section, it is recommended to include an introduction to some classic works on speculative decoding, as the research in this paper is closely related to speculative decoding.\n\n3) The main experimental results (Table 2) presented in this paper seem to show marginal performance improvement. It is recommended to include an overall average result in Table 2 to show the average performance improvement across all cases. Further experiment tuning and the design of new tricks could help improve the performance of the proposed method."}, "questions": {"value": "Please refer to the above \"Weaknesses and Suggestions\" and the following:  \n1) The 5.72% performance improvement mentioned in the introduction—where is it reflected in the tables? The explanation of this in the experiment section seems unclear.  \n2) What aspect or meaning does the term \"stable\" in the paper's title primarily refer to? It would be helpful to explain and address this question from both the methodological and experimental result perspectives. It is recommended that the authors consider this issue in the final version of the paper. \n3) If the authors intend to emphasize efficiency, it is recommended that they provide a comparison of the overall runtime of the proposed method and the baselines on these datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p4hHTZTwLQ", "forum": "kfPkF2ACDM", "replyto": "kfPkF2ACDM", "signatures": ["ICLR.cc/2026/Conference/Submission7025/Reviewer_4xbi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7025/Reviewer_4xbi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757174040, "cdate": 1761757174040, "tmdate": 1762919227201, "mdate": 1762919227201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}