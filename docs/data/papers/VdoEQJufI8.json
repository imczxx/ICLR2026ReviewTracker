{"id": "VdoEQJufI8", "number": 14480, "cdate": 1758236983212, "mdate": 1759897367699, "content": {"title": "SPARC-RTL: Stochastic Prompt-Assisted RTL Code Synthesis", "abstract": "Large language models (LLMs) show promise in code generation yet struggle with Hardware Description Languages (HDLs) such as Verilog, where models often get stuck in flawed reasoning paths that block accurate synthesis and bug fixing. We introduce Stochastic Prompt Assistance (SPA), a lightweight methodology that leverages LLM prompt sensitivity by injecting controlled, non-semantic perturbations into prompts. This approach encourages exploration of diverse reasoning paths without requiring additional feedback loops. Implemented in the \\texttt{SPARC-Debugger}, our automated framework pairs SPA with hierarchical error-pattern matching and achieves consistent gains on Verilog code-generation and debugging benchmarks, including critical “0-to-1” unlocks where baseline models fail entirely. SPA thus provides a complementary and orthogonal mechanism to existing decoding and refinement strategies, improving the reliability of LLMs in HDLs and offering a principled path for broader application in high-precision formal reasoning domains.", "tldr": "", "keywords": ["NLP: Prompt Engineering / Prompting", "NLP: (Large) Language Models", "NLP: Generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/074d389e7d7a09e620fc5b071453192e01402a96.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Stochastic Prompt Assistance (SPA), a lightweight methodology for improving large language model (LLM) performance in hardware description language (HDL) tasks, specifically Verilog code generation and debugging. SPA leverages LLM prompt sensitivity by injecting controlled, non-semantic perturbations (syntactic noise) into prompts, encouraging exploration of diverse reasoning paths without additional feedback loops. The authors implement SPA in the SPARC-Debugger framework, which combines SPA with hierarchical error-pattern matching for iterative code refinement. Empirical results on VerilogEval (code generation) and CirFix (debugging) benchmarks show consistent gains, including \"0-to-1\" unlocks where baseline models fail. SPA is shown to be complementary and orthogonal to existing decoding and refinement strategies, improving reliability in HDLs and potentially other formal reasoning domains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper introduces a simple yet novel idea—injecting stochastic noise into prompts—as a lightweight way to increase solution diversity for Verilog code generation and debugging, addressing a niche but practically important domain.\n\n* The proposed SPARC-Debugger framework is well-engineered and systematically integrates prompt perturbation with hierarchical error pattern matching, showing practical gains in empirical evaluations across several Verilog tasks and LLMs.\n\n* Although focused on Verilog, the idea of prompt-level diversity through structured noise has broader implications for other domains suffering from brittle prompting, and could be extended to tasks beyond HDL."}, "weaknesses": {"value": "* The paper frames its motivation around fixing localization and prompt sensitivity issues in Verilog debugging, but what it implements is a stochastic search strategy that increases the chance of a correct fix via diversity—without validating that localization or sensitivity were the true limiting factors in the first place.\n* The paper claims to address the “localization bottleneck” but never directly measures localization accuracy or provides evidence that SPA improves error pinpointing, undermining one of its central motivations.\n* The reported performance gains may stem from output diversity alone (i.e., trying multiple prompts), not necessarily from the specific noise-injection strategy. No comparisons are made against standard diversity methods (e.g., temperature sampling, prompt ensembling), leaving the benefit of SPA unclear.\n* The paper only compared itself with several self-designed noise-injection baselines. The comparison can be more generalized to include more inference-time prompting enhancement methods for Verilog code generation or finetuned specialised models.\n* Although the paper provides an attention-based hypothesis for how noise perturbs inference, these theoretical claims remain speculative without any attention maps, token attribution studies, or concrete analysis of how noise shifts model behavior."}, "questions": {"value": "* Can you provide direct evidence that SPA improves error localization (rather than just overall success rate)?\nHow does SPA compare to other diversity-enhancing baselines like prompt ensembling or temperature sampling?\n* Can you provide a deeper analysis of how the noise influences model behavior, especially with attention or activation statistics?\n* Are there plans to evaluate SPA on larger, industrial-scale RTL designs or other formal reasoning domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6B8wyWxaE0", "forum": "VdoEQJufI8", "replyto": "VdoEQJufI8", "signatures": ["ICLR.cc/2026/Conference/Submission14480/Reviewer_TnRX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14480/Reviewer_TnRX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604958212, "cdate": 1761604958212, "tmdate": 1762924881820, "mdate": 1762924881820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight method called \"Stochastic Prompt Assistance\" (SPA) to address the problem of LLMs easily getting \"stuck on incorrect inference paths\" when dealing with hardware description languages ​​(Verilog).\n\nThe core argument of this paper is that the sensitivity of LLMs to prompts should not be seen as a defect, but rather as a feature to be utilized. SPA forces the model to explore diverse inference paths by injecting controlled, non-semantic perturbations (i.e., \"noise\") into the input prompts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Adding noise to the input to improve performance on a high-precision task is a novel and inspiring concept. It cleverly transforms a common problem in LLMs (cue sensitivity) into a usable tool. The classification of PRN, SRN, and KIN is clear."}, "weaknesses": {"value": "1. SPA requires the generation of N cue variants. In the SPARC-Debugger framework, the functional verification phase requires even more than $N_a$ hypotheses, each requiring $N_s$ fixes, leading to an explosion of inference costs on the order of $N \\times (N_a \\times N_s)$. The paper claims that SPA is \"lightweight,\" but its practical application (especially within the framework) seems to have enormous computational overhead.\n2. The evaluation on CirFix is ​​limited to a subset of 7 modules, excluding larger IPs such as i2c and sha3. This makes it difficult for us to determine whether an SPA can be extended to more complex, cross-file errors."}, "questions": {"value": "1. The paper claims SPA is \"lightweight,\" but its application within the SPARC-Debugger framework appears costly. Compared to the baseline model, how much does fixing a bug using this framework (e.g., $N_a=5, N_s=5$) increase the overall computational overhead (e.g., total number of tokens or inference calls)?\n2. Regarding the \"0 to 1\" breakthrough in the SDRAM task: This is a key result. Could the authors provide some qualitative analysis? Specifically, why did the baseline model fail (get stuck on which erroneous inference path)? And how did the successful SPA perturbation (PRN+End) help the model circumvent this problem?\n3. How were the noise parameters chosen? (e.g., noise length L, insertion position Pos). For the main results, it seems that \"PRN+End\" was chosen a posteriori as the optimal strategy. However, in practical applications, how should users a priori choose the optimal noise type and parameter combination?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k3ozr3nLyJ", "forum": "VdoEQJufI8", "replyto": "VdoEQJufI8", "signatures": ["ICLR.cc/2026/Conference/Submission14480/Reviewer_NpCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14480/Reviewer_NpCn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969531911, "cdate": 1761969531911, "tmdate": 1762924881393, "mdate": 1762924881393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a simple framework called Stochastic Prompt Assistance, consisting of an input-level diversification technique that injects small, controlled perturbations into LLM prompts to help models “unstick” from brittle reasoning paths during Verilog/RTL code generation. The method is implemented within an automated SPARC-Debugger framework that combines hierarchical error-pattern matching with iterative prompt diversification and compilation/simulation checks.\n\nEmpirically, the authors show that SPA improves pass@1 rates and even achieves several “zero to one” unlocks, particularly with configurations like PRN+End, while remaining orthogonal to decoding-level randomness: temperature, top-p, etc. The approach is lightweight, reproducible, and complements existing HDL repair and generation frameworks.\n\nOverall, the paper is clearly motivated and easy to follow, with a well-defined problem and convincing empirical evidence. That said, there are still some major limitations: 1) the experiments are limited to two dated benchmarks (CirFix and VerilogEval), 2) the evaluation focuses narrowly on pass@1 under fixed settings, and 3) the paper lacks a strictly controlled, equal-budget comparison against strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Simple and creative approach with no architectural changes or retraining, making it easy to integrate into existing workflows and thus could be easily validated \n- It shows that SPA complements exploration provided by decoding randomness: extensive section discussing orthogonality to decoding stochasticity  \n- SPAR-Debugger is nice, provideing a closed-loop eval environment with error-pattern matching and simulation checks \n- Ablation studies are comprehensive, covering noise length, type, and insertion position"}, "weaknesses": {"value": "- Evaluations are restricted mainly to VerilogEval and a small subset of CirFix, which limits generalizability to the “real world”; might benefit from using RTLLLM and CVDP \n- The eval metrics are narrowly focused on pass@1 without reporting pass@k, iteration counts, and runtime/cost/budget tradeoffs \n- Comparisons with prior systems such as RTL-Repair are largely illustrative, without controlled for compute and iteration budgets \n- The paper is relatively light on theoretical rigor, providing little quantitative and mechanistic evidence/insights why perturbation work"}, "questions": {"value": "- Can you provide controlled-budget head-to-head comparison against CirFix, RTL-repair on settings such as tool-call counts, iteration, decoding settings? \n\n- Which bug classes receive the largest SPA gains? \n\n- What guardrails, if any, have you used to prevent the LLM from injecting comments etc that drift into irrelevant semantics and thereby alter the intention & meaning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YS3GZqKrkW", "forum": "VdoEQJufI8", "replyto": "VdoEQJufI8", "signatures": ["ICLR.cc/2026/Conference/Submission14480/Reviewer_Khyv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14480/Reviewer_Khyv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972188448, "cdate": 1761972188448, "tmdate": 1762924880455, "mdate": 1762924880455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SPARC-RTL, a method for improving LLM on VHDL tasks through Stochastic Prompt Assistance. SPA is a lightweight technique that injects controlled, non-semantic perturbations (noises) into prompts to help LLMs escape flawed reasoning paths. Three types of noise are explored: (1) Pure Random Noise: Random characters from ASCII set (2) Structured Random Noise: Syntactically valid but ignorable content (e.g., random comments). (3) Knowledge-Influenced Noise: Valid but task-irrelevant semantic content. The authors regard these perturbations alter the LLM's attention patterns, encouraging exploration of diverse reasoning trajectories without requiring additional feedback loops. The authors implement SPA in an automated framework that combines hierarchical error-pattern matching, iterative refinement loops for compilation and functional validation, and SPA-driven prompt diversification at each iteration. Evaluation on Verilog debugging (CirFix) and code generation (VerilogEval) shows consistent gains of 7-10% absolute improvement in pass@1 rates across models (Llama 3.3, GPT-4o, GPT-4o-mini). In some cases, there are \"0-to-1\" unlocks where baseline prompting completely failed (0% success) but SPA achieved solutions (e.g., 20-90% success on previously unsolvable tasks)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. I love this paper's idea of taking advantage of prompt sensitivity as an opportunity, instead of a bug. This reframing is conceptually fresh and actionable. And the motivation for this is also reasonable, which helps LLMs to escape from debugging traps and potentially being able to identify real fixes. The non-semantic changes guarantee that the prompt still makes sense.\n2. This paper presents a well-designed framework for debugging VHDL programs. The three-tier classification is well-structured and hierarchical. This progression from pure chaos → structured syntax → misleading semantics is coherent and provides a principled way to explore the perturbation space.\n3. The method proposed by this paper can actually also apply to other languages by proposing semantic-agnostic perturbations, which shows the potential of this technique."}, "weaknesses": {"value": "1. This paper does not study how its perturbation interacts with other kinds of perturbation like simply tuning the temperature in experiments. I would like to know whether this is truly a supplement to other decoding strategies, or there will be some overlap?\n2. While the prompt perturbation is the key technique of this paper, the framework contains a lot of designs on how to fix compilation errors   by matching existing error messages and using preset prompts. How these components help the framework identify new bugs is unclear in the experiments.  \n3. The detail implementation of choosing perturbation strategy and generate perturbation is also not fully illustrated."}, "questions": {"value": "1.  Can you provide a direct comparison with equal inference budget on how SPA coordinates with temperature tuning?\n2. Can you provide a detailed ablation on how each component in the framework contributes to the overall accuracy improvement?\n3. Can you elaborate on how we choose perturbation strategy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NfTunu4DmW", "forum": "VdoEQJufI8", "replyto": "VdoEQJufI8", "signatures": ["ICLR.cc/2026/Conference/Submission14480/Reviewer_iEpF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14480/Reviewer_iEpF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016058158, "cdate": 1762016058158, "tmdate": 1762924879102, "mdate": 1762924879102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}