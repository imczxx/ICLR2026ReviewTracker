{"id": "LvyDiPIBw4", "number": 12586, "cdate": 1758208793794, "mdate": 1763218430346, "content": {"title": "M4V: Multimodal Mamba for Efficient Text-to-Video Generation", "abstract": "Text-to-video generation has significantly enriched content creation and holds the potential to evolve into powerful world simulators. However, modeling the vast spatiotemporal space remains computationally demanding, particularly when employing Transformers, which incur quadratic complexity in sequence processing and thus limit practical applications. Recent advancements in linear-time sequence modeling, particularly the Mamba architecture, offer a more efficient alternative. Nevertheless, its plain design limits its direct applicability to multimodal and spatiotemporal video generation tasks. To address these challenges, we introduce M4V, a multimodal Mamba framework for efficient text-to-video generation. Specifically, a MultiModal diffusion Mamba (MM-DiM) block is designed within the framework to enable seamless integration of multimodal information and spatiotemporal modeling. In detail, we introduce a novel multimodal token re-composition design, which employs a bidirectional scheme for multimodal information integration through simple token arrangement, along with visual registers to enhance spatial–temporal consistency. As a result, the MM-DiM blocks in M4V reduce FLOPs by 45% compared with the attention-based alternative when generating videos at 768$\\times$1280 resolution. Additionally, several training strategies are explored in this work to provide a better understanding of training text-to-video models using only publicly available datasets. Extensive experiments on text-to-video benchmarks demonstrate M4V's ability to produce high-quality videos while significantly lowering computational costs. Code will be made publicly available.", "tldr": "", "keywords": ["Video generation", "Mamba"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/753b3f48b34bf8647af4dac03ecf0057d6c5a45b.pdf", "supplementary_material": "/attachment/aee78771f03009433ecc5aa97c5b9c43f3314212.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed a post-training architecutral modification, which leverages multimodal Mamba, to reduce the computational cost of T2V model while preserving high-quality generation capability.  Extensive experiments demonstrate the effectiveness of M4V model across multiple evaluation metrics."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The writting is satisfactory with clear motivation and readiblity, well-organized sections and insightful figures, which helps understanding the proposed method.\n\n2. The application of Mamba to reduce and computation cost is reasonable. The authors successfully address the challenges of extending Mamba to multimodal information processing, while achieving a good trade-off between cost and performance, as shown in Table 4.\n\n3. The experiments is comprehensive to demonstrate the overall performance and effect detailed designs."}, "weaknesses": {"value": "1. It is reasonable that applying Mamba would degrade the performance, as it is sub-optimal to the Transformer block. Though the M4V (Pyramidflow) behaves worse than original Pyramidflow, the M4V (Wan2.1) achieves significan improvement over orignal Wan2.1. Is this phenomenon coming from the initialization strategy? In addition, L288 says that M4V (Pyramidflow) is partly initialized from pretrained attention weights, while Table 1 does not show this detail.\n\n2. Further, in Table 1, the best Semantic Score is Wan2.1 rather than M4V. Please correct this.\n\n3. There lacks a specific model variant for user study. Is it built on Pyramidflow or Wan2.1 framework? From the user study in Figure 3, I notice that the M4V behaves worse than HunyuanVideo in Motion Smoothness and Semantic Coherence, while Table 1 demonstrates a contrast results. Could you provide a further explanation?\n\n4. Table 3 supports the effectiveness of text token re-composition in improving video-text consistentcy. It there any other method to help integrate multimodal information? For example, if we ignore the computation cost, how about interleaving the text tokens with video tokens? Such as \"[Text];[Video Scale 1];[Text];[Video Scale 2];[Text]\". Would this style of integration get better consistency?\n\n5. The authors list 8 types of Mamba scan paths in Section A.1. However, it is not clear how to arrange different types of scanning in different MM-DiM blocks. Does the order of different types significantly influence the model performance?\n\nMinor Questions (It would be better to explain with Experiment Results)\n\n6. How about training M4V from scratch rather than applying initialization?\n\n7. If we ignored the computational cost, what is the performance upper bound of leveraging pure Mamba for the T2V model?"}, "questions": {"value": "My questions are listed in the Weaknesses part. If the authors could address my concerns, I would further raise the final score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HZ6Lh4BEZN", "forum": "LvyDiPIBw4", "replyto": "LvyDiPIBw4", "signatures": ["ICLR.cc/2026/Conference/Submission12586/Reviewer_Qio2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12586/Reviewer_Qio2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761032741342, "cdate": 1761032741342, "tmdate": 1762923435519, "mdate": 1762923435519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "d4fYyrWdwa", "forum": "LvyDiPIBw4", "replyto": "LvyDiPIBw4", "signatures": ["ICLR.cc/2026/Conference/Submission12586/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12586/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763218429549, "cdate": 1763218429549, "tmdate": 1763218429549, "mdate": 1763218429549, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a multimodal momba framework for efficient text-to-video generation, in which a multimodal diffusion momba (MM-DiM) block has been designed to boost seamless integration of multimodal information and spatiotemporal modeling. From the presented results, the proposed method achieves better score while requiring lower computation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1、The motivation is clear, i.e., employ Mamba architecture as a more efficient alternative for T2V generation.\n\n2、The evaluation is comprehensive, including both objective indicators and user surveys, as well as time-based testing."}, "weaknesses": {"value": "1、The presented results are somewhat confusing. On one hand, the proposed method reduces computational cost by replacing some MMDiT blocks with the proposed MMDiM. On the other hand, it has only been trained on public datasets. Therefore, it is unclear why the proposed method surpasses MMDiT-based methods—trained on curated indoor datasets—in both visual quality and computational efficiency, especially considering that it is initialized from an image model.\n\n2、For efficient text-to-video (T2V) generation, various active explorations have been conducted, such as model quantization and step distillation, which have demonstrated significant effectiveness. There is insufficient evidence to justify replacing the well-established MMDiT architecture with MMDiM.\n\n3、Based on the presented results, the generated videos exhibit limited motion dynamics and low aesthetic quality.\n\n4、It is recommended to explore the compatibility of this method with model-agnostic acceleration techniques, such as TeaCache (Timestep Embedding Tells: It's Time to Cache for Video Diffusion Models)."}, "questions": {"value": "Please refer to the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rSrJxaj7y1", "forum": "LvyDiPIBw4", "replyto": "LvyDiPIBw4", "signatures": ["ICLR.cc/2026/Conference/Submission12586/Reviewer_W5kk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12586/Reviewer_W5kk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836011950, "cdate": 1761836011950, "tmdate": 1762923435104, "mdate": 1762923435104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes M4V, a text-to-video generation framework that replaces Transformer blocks with a novel MultiModal Diffusion Mamba (MM-DiM) block. The core claim is that this adaptation significantly reduces computational complexity while maintaining generation quality. The authors demonstrate results on two base architectures (PyramidFlow and Wan2.1) and employ additional training strategies like reward learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a timely and important problem: reducing the high computational cost of transformer-based video generation models.\n2. The proposed MM-DiM block presents a structured approach to adapting the Mamba architecture for multimodal, spatiotemporal data, featuring a detailed token re-composition strategy.\n3. The experimental section is extensive, including benchmarks on VBench, human evaluation, and ablation studies."}, "weaknesses": {"value": "1. Insufficient comparison with efficient alternatives: The paper fails to situate its work within the broader landscape of efficiency-focused methods. A critical omission is a comparison with other prevalent techniques for reducing the complexity of attention, such as sparse attention (e.g. SpargeAttention), or other efficient transformers. Without this, it is impossible to judge whether Mamba is the most effective path to efficiency or if similar gains could be achieved with more established, optimized methods.\n2. Unclear novelty and missing comparison with prior works: The core technical idea bears significant conceptual similarity to prior work like \"Scaling Diffusion Mamba with Bidirectional SSMs for Efficient Image and Video Generation\" (Arxiv:2405.15881), which also explores Mamba for diffusion-based generation (including video generation). Thus, the presented paper's novelty remains unclear.\n3. Disconnected contributions: The use of a separate reward tuning stage appears to be a method to boost final benchmark scores. However, this technique is orthogonal to the paper's main contribution (the MM-DiM architecture). Its prominent use raises a critical question: is the quality of the pure Mamba-based model insufficient, requiring an auxiliary, generic performance-boosting method to remain competitive? This undermines the claim that the proposed architecture itself maintains high quality, suggesting it may inherently lead to a quality drop that needs to be compensated for."}, "questions": {"value": "1. Given the existence of numerous other methods for achieving sub-quadratic complexity in transformers (e.g., sparse attention), why was the Mamba pathway chosen? Can you provide ablation studies or arguments demonstrating that your MM-DiM block provides a superior efficiency-quality trade-off compared to these established alternatives?\n2. The method in Arxiv:2405.15881 also uses Mamba for diffusion modeling. What are the specific, fundamental differences between your MM-DiM block and their approach, particularly regarding multimodal fusion? Clearly, the authors present multi-modal fusion method, which is novel, but the core architecture is still similar to the prior Mamba-based video diffusion model. Please provide a quantitative and qualitative comparison to clarify the novelty of your contribution.\n3. The reward learning stage is presented as an \"Additional Improvement.\" If this stage is removed, what is the performance gap between the M4V model and the baselines? Does the core MM-DiM architecture, without post-training reward tuning, cause a significant drop in quality metrics that this separate technique is required to close?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "meYDfiGKVu", "forum": "LvyDiPIBw4", "replyto": "LvyDiPIBw4", "signatures": ["ICLR.cc/2026/Conference/Submission12586/Reviewer_5Gre"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12586/Reviewer_5Gre"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926823526, "cdate": 1761926823526, "tmdate": 1762923434534, "mdate": 1762923434534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}