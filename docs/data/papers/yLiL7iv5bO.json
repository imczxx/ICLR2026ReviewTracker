{"id": "yLiL7iv5bO", "number": 22762, "cdate": 1758335183369, "mdate": 1763738508223, "content": {"title": "Flow-Matching based Refiner for Molecular Conformer Generation", "abstract": "Low-energy molecular conformers generation (MCG) is a foundational yet challenging problem in drug discovery. \nDenoising-based methods include diffusion and flow-matching methods that learn mappings from a simple base distribution to the molecular conformer distribution. However, these approaches often suffer from error accumulation during sampling, especially in the low SNR steps, which are hard to train. To address these challenges, we propose a flow-matching refiner for the MCG task. \nThe proposed method initializes sampling from mixed-quality outputs produced by upstream denoising models and reschedules the noise scale to bypass the low-SNR phase, thereby improving sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the generator–refiner pipeline improves quality with fewer total denoising steps while preserving diversity.", "tldr": "", "keywords": ["Molecular conformer generation; Flow matching; Refiner"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f86dcb81717a5fc04e2a6effe01f69eb129357f.pdf", "supplementary_material": "/attachment/2e8895f985a804bf75419e5e3b30d96eb7f6ac25.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a flow-matching-based refiner, coupled with existing denoising models, to improve molecular conformer generation.\n\nIn particular, the proposed refiner directly starts from conformers generated by diverse upstream denoising models,  thereby skipping the high-noise phase and improving final performance.\n\nExperiments demonstrate that the proposed pipeline (generator plus refiner) achieves higher quality with fewer total sampling steps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed refiner is plug-and-play, compatible with conformers generated by diverse upstream denoising models, and requires no model-specific tuning.\n\n2. Compared with baselines using a single denoising model, employing the proposed refiner as the post-processor improves performance while reducing sampling steps.\n\n3. The whole paper is excellently written."}, "weaknesses": {"value": "1. As shown in Algorithm 1, while the refiner is trained using the flow-matching paradigm, $x_0$ is directly derived from $x_1$. This establishes a deterministic mapping between them, rather than a genuine distributional transport. In this case, the refiner functions more as a conditional corrective regressor than as a genuine flow field that transports one distribution to another.\n\n2. Essentially, the proposed refiner represents a form of learned refinement, which makes it necessary to compare with other refinement methods, especially those typical optimizers like MMFF and UFF. The absence of such comparisons leaves unanswered questions regarding the refiner's effectiveness relative to established techniques.\n\n3. The paper lacks an officially recommended reproducibility statement, which is crucial for ensuring the credibility of this work. Moreover, the absence of code availability further hinders the ability to reproduce the presented results."}, "questions": {"value": "1. Could you clarify why Algorithm 1 does not align with Equation 6?\n\n2. Could you clarify the rationale behind directly implementing the proposed refiner by fine-tuning on ET-Flow? What would be the implications of training from scratch? Additionally, how might the results vary if different architectures were utilized?\n\n3. Could you describe the use of Large Language Models (LLMs) in your paper? According to the ICLR 2026 Author Guide, failure to disclose significant usage of Large Language Models (LLMs) may result in the desk rejection of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vPtr1dGXUb", "forum": "yLiL7iv5bO", "replyto": "yLiL7iv5bO", "signatures": ["ICLR.cc/2026/Conference/Submission22762/Reviewer_Xj5K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22762/Reviewer_Xj5K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22762/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761399247009, "cdate": 1761399247009, "tmdate": 1762942375556, "mdate": 1762942375556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the task of molecular conformer generation, where models often suffer from error accumulation in early high-noise stages. To mitigate this, the authors introduce a flow-matching–based refiner that starts from existing conformers instead of pure noise, effectively bypassing low-SNR phases. This approach enhances both conformer quality and sampling efficiency, producing more accurate and diverse molecular structures with fewer steps."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Introduces a flow-matching–based refiner that improves molecular conformer generation (MCG) by refining upstream outputs instead of starting from pure noise.\n- Bypasses the low signal-to-noise ratio (SNR) regime and reduces early-step error accumulation typical in denoising-based models.\n- Improves precision without collapsing conformer diversity.\n- Provides theoretical justification for noise scale rescheduling, robustness to time-step mismatch, and diversity preservation."}, "weaknesses": {"value": "- Code is not available.\n- Although the refiner shortens sampling time, the paper does not clarify its memory overhead or overall computational trade-offs relative to the achieved performance gains.\n- The refiner assumes that upstream conformers are reasonably accurate structures with small random deviations. However, its performance when the upstream samples are systematically biased or incorrect remains unclear.\n- The paper lacks qualitative analysis or visualization of instances where the refiner degrades conformer quality, which limits understanding of its robustness.\n- The paper does not include a clear schematic or main figure illustrating the overall model architecture, which makes it difficult for readers to understand how the refiner integrates with the upstream generator and the sampling process."}, "questions": {"value": "- How robust is the refiner when upstream conformers contain systematic structural biases or chemically incorrect geometries rather than small random perturbations?\n- In what specific cases does the refiner fail or degrade conformer quality, and what underlying factors contribute to these failure modes?\n- How does the proposed refiner balance reduced sampling time against potential increases in memory consumption or computational cost, and what are the quantitative trade-offs involved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CHKoYohPrR", "forum": "yLiL7iv5bO", "replyto": "yLiL7iv5bO", "signatures": ["ICLR.cc/2026/Conference/Submission22762/Reviewer_jEgW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22762/Reviewer_jEgW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22762/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822587138, "cdate": 1761822587138, "tmdate": 1762942375202, "mdate": 1762942375202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a \"flow-matching based refiner\" architecture for molecular conformer generation (MCG) tasks. The approach addresses a key limitation in denoising generative models—error accumulation from early, high-noise steps—by introducing a refiner model that operates on outputs from upstream generative methods, skipping the problematic low-SNR regime through noise rescheduling and leveraging flow-matching with a carefully chosen base distribution. The refiner is shown to be plug-and-play with different upstream methods and is validated on the GEOM-DRUGS and GEOM-QM9 benchmarks, exhibiting improvements in sample quality (as measured by AMR and coverage), sample diversity, and chemical property alignment, even with fewer sampling steps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method’s theoretical justification is detailed, with a concrete analysis of the implications of time-step mismatch and noise scale alignment. The derivation (see Equations in Section 4.1 and Algorithm 1) is mathematically sound and elucidates how the refiner provides a principled (rather than heuristic) way to bypass the ill-behaved regime in diffusion/flow models.\n2. Experimental results are compelling: Tables 1 and 2 show consistent improvement in AMR and coverage on both GEOM-DRUGS and GEOM-QM9, even when using fewer total sampling steps than the baseline models. Table 3 further demonstrates enhanced performance in chemical property prediction.\n3. The paper offers ablation studies and broad benchmarking, including effects of sampling steps (Table 5, Table 6) and per-conformer improvement/downgrade analysis (Table 4, Table 7), supporting the claim that the refiner meaningfully improves MCG robustness and output quality."}, "weaknesses": {"value": "1. The mathematical justification for the noise scale self-calibration is partially heuristic. While the paper introduces $t^* = 1 - \\sigma^*/\\sigma$ and suggests the schedule handles value-range coverage, this only provides an approximate match between upstream and refiner noise regimes. There is no guarantee that in complex distributions this calibration will avoid suboptimal updates, nor is the implications of possible misalignment deeply discussed; this is a potential source of instability or model non-robustness, especially in molecules with unusual geometry or heavy tails in error distributions.\n2. Scope of empirical studies is still centered on established (or highly similar) datasets (GEOM-DRUGS, GEOM-QM9). While the results are strong, the generalizability claim (\"plug-and-play\", \"needs no per-model tuning\") is not fully supported for more diverse, realistic or larger-scale datasets (e.g., proteins, or non-Geom drug-like molecules). There are no results or analyses regarding transferability to domains with different data distributions or conformational complexity.\n3. Absence of direct ablation between refiner, re-tuned upstream models, and truly hybrid or joint training strategies. In Table 1 and Figure 1, the comparison is between using more sampling steps in the generator alone versus adding a refiner with less total steps. While this demonstrates efficiency, it leaves open the question whether re-training the upstream generator to exploit more sampling steps (while increasing capacity or tuning the noise schedule) could close the gap, or if the refiner's effect would be additive if trained jointly. These are closely related to the claims of plug-and-play and step-efficiency."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K6L3bX9MgM", "forum": "yLiL7iv5bO", "replyto": "yLiL7iv5bO", "signatures": ["ICLR.cc/2026/Conference/Submission22762/Reviewer_CRoH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22762/Reviewer_CRoH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22762/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850953219, "cdate": 1761850953219, "tmdate": 1762942374882, "mdate": 1762942374882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method in molecular conformation generation (MCG) that cascades an upstream denoising generation model (diffusion or flow-matching) with a \"Refiner\". Instead of starting from pure noise, the Refiner uses the conformation of the upstream model as its initial state and bypasses the low signal-to-noise ratio (low-SNR) phase by resetting/aligning the noise scale, thereby reducing early error accumulation and aiming to achieve better geometric accuracy and chemical property consistency with fewer total steps. The paper compares the benefits of allocating budget to \"adding steps\" versus \"integrating the Refiner\" in GEOM-Drugs and GEOM-QM9, and reports improvements in AMR/COV and several property metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.To address the challenges of learning low SNR steps, error propagation, and amplification in the early stages of denoising, we propose an intuitive and engineering implementation method that \"skips the pure noise stage,\" aligning with recent error propagation analysis.\n\n2.Refiner can theoretically be used \"plug and play\" to perform post-processing on various upstream model outputs, making it highly valuable for practical applications.\n\n3.Comparing multiple strong baselines (MCF/ET-Flow/DMT) with two major benchmarks (QM9/Drugs), the report presents ensemble indices and chemical property results, and provides per-conformer improvement and degradation ratios.\n\n4.Comparing the benefits of \"increasing the number of upstream sampling steps\" with \"integrating Refiner\", the study emphasizes the scenario where a better median AMR and comparable COV are achieved with fewer total steps."}, "weaknesses": {"value": "1.The main text introduces an explicit t-dependency between the random interpolation term s(t)z and u_t = -sigma varepsilon + s'(t)z in the Refiner's interpolation and target velocity; however, the training objective of Algorithm 1 is written as u_t \\leftarrow x_1 - x_0, which is completely independent of t and s(t), and does not reflect the noise derivative term (it even does not conform to the notation of Eq.(7)/(8)). If Algorithm 1 is implemented, the model will regress a constant velocity, which does not conform to the Flow-Matching training paradigm of \"random interpolation-conditional velocity regression\", and cannot reflect the instantaneous velocity scheduling when \"bypassing low SNR\".\n\n2.In real systems, upstream geometric errors often exhibit conformation-dependent/structure-dependent anisotropic distributions (coupling and constraint of bond lengths/bond angles/dihedral angles, ring strain, steric barriers, etc.). Using an isotropic Gaussian approximation may underestimate the local errors and non-Gaussian tails in difficult cases. It is recommended to provide empirical tests (e.g., covariance spectra/thick tail tests of the atom-by-atom residuals of ET-Flow/DMT outputs and real conformations) to support whether this assumption is sufficient for \"t-realignment\".\n\n3.During training, a \"data core\" basis distribution is constructed using x_{0}=x_{1}+σε; during testing, the upstream-generated 'hat{x}' is used as x_{0}. The noise and bias distributions of these two distributions are not equivalent.\n\n4.When comparing \"50-step single model\" vs. \"20 + 20 (generation + refinement) = 40 steps\", it is necessary to clarify whether the cost per step is comparable (whether the two-stage model structure/forward FLOPs are equivalent, and whether both include PC correction steps). If Refiner's per-step cost is lighter or heavier, an equivalent cost comparison in milliseconds/molecule or FLOPs/molecule should be provided to avoid confusion between \"fewer steps\" and \"less computational power/latency\".\n\n5.Several increases (e.g., GEOM-Drugs COV mean +1.95%) were small, and variance/confidence intervals/multiple random seed replication experiments were missing. It is recommended to report the mean ± standard deviation or 95% CI for each curve/table, along with the number of repetitions and significance tests.\n\n6.Fig.1 suggests adding error bars, multiple random seeds and coordinate axis units (AMR units Å), and model parameter quantity/single-step cost annotations to ensure a fair comparison of \"adding steps vs. adding Refiner\"."}, "questions": {"value": "1.Inconsistency between Algorithm 1 and Eq. (6–9): Was z actually sampled during training and regression performed using u_{t}=−σε+s(t)z? Or was a simplified constant objective x_{1}−x_{0} used? Please provide the final executable formula and pseudocode used, and explain the performance difference between the two.\n\n2.In practice, is explicit estimation or search used (e.g., using early steps to locate based on velocity norm or neighborhood degree stability), or is a match simply \"bumped\" into a fixed schedule? If the latter, how significant is the impact of mismatch on performance, and are there any adaptive strategies? Can statistical evidence (such as per-atom residual ellipsoidal spectra, heavy-tailed tests, or molecular subsets of ring systems) be provided to support or define the applicability of this hypothesis?\n\n3.Besides AMR-Recall/COV, were distribution metrics for conformational family coverage (such as KL/FID for conformational energy histogram/dihedral distribution, and spread for Cartesian vs. interior coordinates) evaluated? Are there any negative examples of \"oversmoothing\" or \"mode collapse\" visualized?\n\n4.How does the total latency/total FLOPs of the two-stage system compare to that of the single-stage system under the same quality conditions? Are the number of PC corrections consistent?\n\n5.Refiner, derived from the ET-Flow architecture/weight fine-tuning, does its performance when transferred to MCF/DMT significantly depend on the upstream error morphology? Should we try a cross-robustness table that \"trains with A and refines with B/C\"?\n\n6.If we change x_{0} during training to “upstream samples + slight random perturbation”, can we further approximate the test distribution and bring robustness gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yo0dGWJf92", "forum": "yLiL7iv5bO", "replyto": "yLiL7iv5bO", "signatures": ["ICLR.cc/2026/Conference/Submission22762/Reviewer_WFCh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22762/Reviewer_WFCh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22762/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890906452, "cdate": 1761890906452, "tmdate": 1762942374459, "mdate": 1762942374459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}