{"id": "G7fHaxLTx5", "number": 14296, "cdate": 1758232259442, "mdate": 1759897378166, "content": {"title": "Prior-Informed Flow Matching for Graph Reconstruction", "abstract": "We introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a prior, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and state-of-the-art generative baselines in reconstruction accuracy.", "tldr": "We introduce Prior-Informed Flow Matching (PIFM), a conditional flow matching model for graph reconstruction that ehnaces classical embedding-based predictors by learning a global coupling.", "keywords": ["flow matching", "graph inference", "inverse problems"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c983d4e69a6b42c4c42e05e8a6489afc38b37927.pdf", "supplementary_material": "/attachment/3e3d9eee022d71823eb85c2d3d4bbcce100ba8ec.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. The authors reframe the graph reconstruction process as a two-stage method, grounded in distortion-perception theory. First, it uses a prior to form an informed initial estimate of the adjacency matrix, which serves as an approximation of the Minimum Mean Squared Error estimator to minimize distortion. Second, it applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs by learning a global structural coupling to enhance perceptual quality . Experiments show competitive results on three datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework of the paper is novel. Combining traditional graph embedding method with flow model is a good try, bridging the gap between local estimator and global estimator. \n\n2. The paper is clear and easy to follow."}, "weaknesses": {"value": "1. The experiments lack some strong baselines for link prediction. The authors mainly compare PIFM with its own prior, which is fine and, in my view, justifies the effectiveness of the flow matching technique. However, the absence of SOTA link prediction methods makes the experiments less persuasive.\n\n2. The scale of the datasets is too limited and not representative. As stated by the authors, the model inherits the scalability challenges of diffusion models, and the selected datasets are too small for the task. It would be better to expand the size of the datasets and include various types, such as molecule datasets, bioinformatics datasets, social network datasets, etc."}, "questions": {"value": "1. Utilizing prior knowledge for graph reconstruction is a good point, but prior knowledge is not always ready to use. I wonder when prior knowledge is required and can significantly improve the model?\n\n2. For the graph embedding method selection, what's the criterion? Can we incorporate other GNN methods into the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4jVyWamSHZ", "forum": "G7fHaxLTx5", "replyto": "G7fHaxLTx5", "signatures": ["ICLR.cc/2026/Conference/Submission14296/Reviewer_vUVC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14296/Reviewer_vUVC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760844928576, "cdate": 1760844928576, "tmdate": 1762924741486, "mdate": 1762924741486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed mainly a method for graph reconstructino with 2 steps:\n1. initial graph reconstruction\n2. refinement via optimal transport"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method makes sense to me in a high level way. It resembles a two-stage refinement process: the first stage performs a probability-based estimation, while the second stage learns an actual mapping from the imprecise reconstruction to the real graph."}, "weaknesses": {"value": "I am not very clear about the motivation about this task or the motivation of the proposed method. See questions."}, "questions": {"value": "There are several points that are confusing to me:\n\n1. I do not fully understand the difference between graph reconstruction and graph generation. In graph generation, there are already ways to make the generated graphs very similar to the training data by leveraging priors that encourage overfitting toward the training distribution. With my understanding, using such a model, one could directly achieve graph reconstruction through an inpainting-like process. Is that correct?\n\n2. For the first-stage graph probability estimation, I am confused why the method needs to rely on either graphon or GraphSAGE, for the following reasons:\n* Graphon assumes that graphs of different sizes follow the same underlying distribution, which is often not true in real-world datasets. For example, once molecular size increases, this assumption can break.\n* GraphSAGE predicts link existence using node embeddings, which is known to be less effective than graph transformers for such tasks. Instead, a graph generative model could also provide a probabilistic estimation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zlTsGn3dN8", "forum": "G7fHaxLTx5", "replyto": "G7fHaxLTx5", "signatures": ["ICLR.cc/2026/Conference/Submission14296/Reviewer_cqqu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14296/Reviewer_cqqu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817970524, "cdate": 1761817970524, "tmdate": 1762924741116, "mdate": 1762924741116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Prior-Informed Flow Matching (PIFM), a novel approach to graph reconstruction that combines edge predictors with global structural priors. By using flow-based generative models, PIFM refines initial adjacency matrix estimates. Experimental results on three main datasets show that PIFM outperforms existing methods in link prediction, edge recovery, and denoising tasks, highlighting its effectiveness in graph topology reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivations.\n\n2. Novel methodological contribution that integrates structural priors with flow-based modeling.\n\n3. Insightful presentation.\n\n4. Comprehensive experimental validation."}, "weaknesses": {"value": "1. On page 11, the reference appears to be missing the paper title: \"Santiago Segarra, Antonio G. Marques, Gonzalo Mateos, and Alejandro Ribeiro. Ieee trans. signal and info. process. over networks. IEEE Transactions on Signal and Information Processing over Networks, 3(3):467–483, 2017\". Does this correspond to Segarra, Santiago, et al. \"Network topology inference from spectral templates.\" IEEE Transactions on Signal and Information Processing over Networks 3.3 (2017): 467-483?\n\n2. In the link prediction experiments (5.2), the prior modules combined with PIFM (SIGL, node2vec, GraphSAGE) achieve strong performance in terms of AUC and AP; however, a relatively high false negative rate remains at fixed thresholds (e.g., 0.5). It is recommended that the authors consider improving the experiments to enhance discrimination at specific thresholds. Furthermore, a high false negative rate is also observed in the expansion task (Table 3), and it is suggested that the authors discuss this phenomenon in more detail.\n\n3. It is recommended that the authors provide an analysis of the training time of the model on graphs of different scales, and compare it with baseline methods, in order to better understand the practical performance of PIFM and its potential computational overhead.\n\n4. The experimental evaluation includes a few recent baselines from the past two years.\n\n5. In Assumption AS2, the authors assume that edges are conditionally independent given the latent structure. However, as noted in GraphRNN (You J, Ying R, Ren X, Hamilton W, Leskovec J. Graphrnn: Generating realistic graphs with deep auto-regressive models. InInternational conference on machine learning 2018 Jul 3 (pp. 5708-5717). PMLR.), graph structures exhibit complex and non-local dependencies among edges. This assumption may limit performance on datasets with intricate structural patterns, e.g., REDD-B. It would be helpful for the authors to clarify further."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bJthMubRN7", "forum": "G7fHaxLTx5", "replyto": "G7fHaxLTx5", "signatures": ["ICLR.cc/2026/Conference/Submission14296/Reviewer_V7Mm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14296/Reviewer_V7Mm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859667676, "cdate": 1761859667676, "tmdate": 1762924740699, "mdate": 1762924740699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Prior-Informed Flow Matching (PIFM), a method that improves the training of flow-based generative models for graphs by learning the prior distribution jointly with the flow dynamics. Standard flow matching (FM) assumes a fixed isotropic Gaussian prior, which can poorly capture structural dependencies in discrete or topological data such as graphs. PIFM introduces a learnable prior network $p_\\psi(x_0)$ integrated into the FM objective, allowing the model to adapt its latent initialization to the data manifold while maintaining compatibility with continuous normalizing flows and neural ODE solvers.\n\nThe authors derive the modified objective, show that it retains the tractable FM loss form, and provide an analysis suggesting that learning the prior improves alignment between forward and backward trajectories, thus enhancing likelihood estimation and sample quality.\nEmpirical results on three graph benchmarks, ie. IMDB-B, PROTEINS, and ENZYMES, demonstrate that PIFM  and demostrates an improvement on graph recosntruction. \n\nThis paper proposes Prior Informed Flow Matching (PIFM), a flow-based framework for graph reconstruction from partial observations.\nUnlike existing diffusion or flow models that assume isotropic priors, PIFM introduces structural priors derived from classical graph embedding methods; e.g. Node2Vec, GraphSAGE, or SIGL (graphons), to initialize a rectified flow model that learns a global transport map from these local predictions to the ground-truth adjacency distribution.\n\nFormally, PIFM reformulates graph topology inference through the distortion perception trade-off, aiming to minimize reconstruction error (distortion) while improving perceptual realism (measured via distributional alignment). The method ensures permutation equivariance at every stage and defines a two-step process: a) a local prior predicts edge probabilities (MMSE estimator); b) a flow-matching model refines these predictions via continuous interpolation, learning global dependencies between edges.\n\nEmpirical evaluations on three benchmark datasets, IMDB-B, PROTEINS, and ENZYMES, demonstrate that PIFM improves improves AUC and AP over both embedding-based priors and diffusion-based baselines. Achieves lower $\\text{MMD}^2$, indicating closer alignment between reconstructed and real graphs. Recovers key graph statistics (degree, triangles, clustering coefficient) more faithfully as reconstruction steps increase."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper identifies a real gap between local graph embedding methods (which miss global consistency) and generative flow models (which lack structural priors).\n- The reinterpretation of graph reconstruction under the perception–distortion theory is grounded.\n- The integration of priors into a rectified flow-matching process is natural, lightweight, and permutation-equivariant.\n- Results consistently show improvements over classical and generative baselines in both link prediction and blind reconstruction (expansion and denoising).\n- The paper includes multiple masking ratios (10%, 50%), different priors, and step counts (K = 1, 100), showing how reconstruction quality scales with iterative refinement.\n- Figures (especially Figs. 1–3) are well-designed and complement the text effectively. It was a fun read!"}, "weaknesses": {"value": "- The idea of \"learning flow trajectories from prior-informed initializations\" builds directly on existing rectified flow work (Liu et al., 2023; Albergo et al., 2023). The main novelty lies in applying it to graph reconstruction, not in the core algorithm.\n- Experiments are restricted to small graph benchmarks (IMDB-B, PROTEINS, ENZYMES). No evidence of scalability to large or molecular graphs (e.g., ZINC, ogbg-molhiv).\n- The use of MMD² and basic graph statistics is informative but lacks comparison to more advanced generative quality metrics.\n- The \"perception–distortion\" formulation and optimal transport connections are stated clearly but not deeply analyzed (no formal proofs).\n- While PIFM is conceptually efficient, the additional training cost of priors and flow is not quantified."}, "questions": {"value": "1. How sensitive is PIFM to the choice of prior (Node2Vec vs GraphSAGE vs SIGL)?\n2. What is the relative computational cost of PIFM compared to standard Flow Matching with Gaussian priors?\n3. Could the learned rectified flow generalize across graph families (e.g., trained on IMDB-B, tested on PROTEINS)?\n4. Have you tried visualizing intermediate adjacency reconstructions ($A_t$) to verify smooth transitions?\n5. How does the choice of K (number of reconstruction steps) affect training stability and memory usage?\n6. Would the approach extend naturally to heterogeneous or attributed graphs, as suggested in the conclusion?\n7. Minor addition in 153 refer to Appendix B.4 and C"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BOgO1ArYFH", "forum": "G7fHaxLTx5", "replyto": "G7fHaxLTx5", "signatures": ["ICLR.cc/2026/Conference/Submission14296/Reviewer_WKDw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14296/Reviewer_WKDw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929747540, "cdate": 1761929747540, "tmdate": 1762924740261, "mdate": 1762924740261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose an approach, PIFM, to construct a graph from partially absorbed graph, where classical embedding models often fails to keep global consistency and generating models struggle will incorporate structural prior .\n\nPIFM  aims to bridge the gap by integrating embedding-based priors to get initial estimate and refine with continuous-time flow matching. Here the prior are computed using the graph sage where each node gets an embedding, and is used to create n x n adjacency matrix as an initial estimate, which is further refined with flow-based algorithm."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem of completing the missing edges in the graph is important and has real world significance as not all information is explicitly available."}, "weaknesses": {"value": "1. The idea of identifying missing link based on existing link is not novel as several algorithm have been proposed like TransE, TransH and several GNN based algorithms. \n2. The approach has been tested on only three datasets however several datasets exists for link prediction. Given the high level of importance and high level of existing research it would be beneficial to add a few more datasets. Moreover the statistics of the datasets is not available in the paper making it hard to determine the behaviour of proposed model based on the size of the dataset and number of nodes.\n3. The results in table 1 are confusing. Good AUC but bad FNR indicate imbalance number of present and absent edges i.e the number of edges to be predicted are not much less compared to number of missing edges and hence the score on AUC and AP can be high and does no clearly show the advantage of the model in predicting present edges. It would be useful to add MMD score as two graph can be isomorphic."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9ZPKHPPK8M", "forum": "G7fHaxLTx5", "replyto": "G7fHaxLTx5", "signatures": ["ICLR.cc/2026/Conference/Submission14296/Reviewer_a7bB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14296/Reviewer_a7bB"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762319513975, "cdate": 1762319513975, "tmdate": 1762924739743, "mdate": 1762924739743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}