{"id": "18gC6pZVVc", "number": 6362, "cdate": 1757973603023, "mdate": 1763573855041, "content": {"title": "Geometry-aware 4D Video Generation for Robot Manipulation", "abstract": "Understanding and predicting dynamics of the physical world can enhance a robot's ability to plan and interact effectively in complex environments. While recent video generation models have shown strong potential in modeling dynamic scenes, generating videos that are both temporally coherent and geometrically consistent across camera views remains a significant challenge. To address this, we propose a 4D video generation model that enforces multi-view 3D consistency of generated videos by supervising the model with cross-view pointmap alignment during training. Through this geometric supervision, the model learns a shared 3D scene representation, enabling it to generate spatio-temporally aligned future video sequences from novel viewpoints given a single RGB-D image per view, and without relying on camera poses as input. Compared to existing baselines, our method produces more visually stable and spatially aligned predictions across multiple simulated and real-world robotic datasets. We further show that the predicted 4D videos can be used to recover robot end-effector trajectories using an off-the-shelf 6DoF pose tracker, yielding robot manipulation policies that generalize well to novel camera viewpoints.", "tldr": "We propose a 4D video generation model that enforces geometric consistency across views to generate spatio-temporally aligned RGB-D sequences, enabling downstream applications of robot manipulation tasks via pose tracking.", "keywords": ["Video Generation", "Robot Manipulation", "3D Perception"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a22da1a6b32cce1e0356b182ac99bced2e5b75c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a geometry-aware 4D video generation model tailored for robot manipulation, which enforces multi-view 3D consistency during training via cross-view pointmap alignment and leverages pretrained video diffusion models for temporal coherence, enabling the generation of spatio-temporally aligned RGB-D sequences from novel viewpoints without camera pose inputs. It evaluates the model on both simulated (e.g., StoreCerealBoxUnderShelf) and real-world (e.g., TwistCapOffBottle) robotic tasks. Additionally, the predicted 4D videos can be used with an off-the-shelf 6DoF pose tracker (e.g., FoundationPose) to extract robot end-effector trajectories."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Enables geometry-consistent 4D video generation for multi-object dynamic robot manipulation scenes (a gap in prior 3D-aware methods limited to single objects and static backgrounds) by enforcing cross-view pointmap alignment during training, ensuring spatio-temporal consistency across novel camera viewpoints without relying on camera poses at inference.\n2. Integrates pretrained video diffusion models’ temporal priors with a dedicated geometry-consistent loss for pointmaps, achieving joint optimization of RGB video quality, depth accuracy, and multi-view 3D consistency.\n3. Bridges 4D video generation to practical robot manipulation: predicted RGB-D sequences can extract robot end-effector trajectories via an off-the-shelf 6DoF pose tracker (e.g., FoundationPose) and infer gripper open/close states, leading to higher task success rates than visuomotor policy baselines."}, "weaknesses": {"value": "1. Relies on multi-view RGB-D datasets with varied camera viewpoints for training, which are challenging to collect in real-world settings due to hardware constraints and calibration requirements, and high-quality depth data acquisition in real scenarios remains difficult.\n2. The inference speed of the video generation model is relatively slow, making closed-loop planning for robot manipulation impractical compared to end-to-end behavior cloning policies.\n3. The baseline comparisons lack direct competition with state-of-the-art methods specifically designed for multi-object dynamic robot manipulation scenarios; selected baselines (e.g., 4D Gaussian, SVD) either have different task scopes or lack 3D consistency modeling, weakening the persuasiveness of performance superiority.\n4. Key implementation details for reproducibility are insufficiently disclosed, such as the specific camera sampling parameters (e.g., exact pitch/yaw ranges), training hyperparameter schedules (e.g., learning rate decay strategy), and threshold values for gripper state inference (e.g., distance threshold δ), which may hinder result validation.\n5. The multi-view cross-attention mechanism, a core component for 3D consistency, lacks unique design details; it is not clarified how it adapts to pointmap geometric features (e.g., whether attention weights correlate with 3D distances), making it indistinguishable from generic cross-attention modules."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lYVHdHz24a", "forum": "18gC6pZVVc", "replyto": "18gC6pZVVc", "signatures": ["ICLR.cc/2026/Conference/Submission6362/Reviewer_RXK1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6362/Reviewer_RXK1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761367382289, "cdate": 1761367382289, "tmdate": 1762918653582, "mdate": 1762918653582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a geometry-aware 4D video generation model for robot manipulation. The method introduces cross-view pointmap alignment as a geometric supervision during training. The geometric constraint allows the model to learn a shared 3D scene representation, enabling the joint optimization of spatial and temporal consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method achieves geometric consistency in the generated videos across multiple views through cross-view geometric supervision and cross-attention mechanism.\n2. The model shows a significant improvement in task success rate for manipulation tasks compared to baseline methods.\n3. The generated 4D videos can be directly combined with an off-the-shelf 6DoF pose tracker to extract robot end-effector trajectories."}, "weaknesses": {"value": "1. The authors need to fully explain the difference between their proposed method and the joint spatio-temporal consistency optimization methods mentioned in references [3, 4]. The authors should specifically show how the proposed approach is better suited for the multi-object, dynamic robot manipulation scenes.\n2. In Table 1, the $FVD-{n}$ scores for Task 2 and Task 4 are not significantly different from the SVD finetuning baseline. More results from additional tasks are needed to verify the advantage of the proposed method over SVD finetuning.\n3. The current experiments mainly focus on rigid object manipulation, lack experimental evaluation on non-rigid objects (e.g., cloth)."}, "questions": {"value": "1. The current method uses geometric supervision via feature passing and cross-attention from view $v_n$ to view $v_m$. Have the authors investigated the effect of $v_m \\to v_n$ or $v_n \\rightleftarrows v_m$ on geometric consistency and final generation quality? Additionally, a sensitivity analysis on $\\lambda$ should be included to determine if there is a trade-off between FVD and mIoU performance.\n2. Since the current evaluation is primarily based on two views, what is the performance of this framework when the number of input views is increased (e.g., three or four views)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OVrvBJHQbv", "forum": "18gC6pZVVc", "replyto": "18gC6pZVVc", "signatures": ["ICLR.cc/2026/Conference/Submission6362/Reviewer_b1Za"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6362/Reviewer_b1Za"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479095304, "cdate": 1761479095304, "tmdate": 1762918653278, "mdate": 1762918653278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel 4D video generation framework that jointly enforces temporal coherence and multi-view 3D geometric consistency for robotic manipulation tasks. The core idea is to supervise a diffusion-based video generation model using cross-view pointmap alignment, inspired by DUSt3R, to ensure that predicted RGB-D sequences from different camera views correspond to a shared 3D scene representation. The method does not require camera poses at inference time, yet it generalizes to novel viewpoints. The authors demonstrate that the generated 4D videos can be used with off-the-shelf 6DoF pose trackers (e.g., FoundationPose) to extract accurate robot end-effector trajectories, enabling successful execution of manipulation policies in both simulation and real-world settings. Experiments across three simulated and four real-world tasks show consistent improvements over strong baselines in video quality, depth accuracy, and cross-view consistency."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Originality: The integration of cross-view pointmap alignment into a video diffusion model for 4D generation is novel. Unlike prior 4D methods that assume known camera poses or operate on static scenes, this work handles dynamic, multi-object manipulation without pose inputs at test time.\n2. Quality: Experiments are thorough, covering both simulation (LBM) and real-world domains, with ablations, multiple baselines (SVD variants, 4D Gaussian), and downstream policy evaluation.\n3. Clarity: The problem formulation, method description, and results are presented with exceptional clarity. Figures and tables are informative and well-designed.\n4. Significance: Enables pose-free novel-view video generation for robotics—a practical advance for real-world deployment where extrinsic calibration is often unavailable or unreliable. The demonstrated success in policy extraction via off-the-shelf trackers lowers the barrier for applying generative models in robotics."}, "weaknesses": {"value": "1. Computational Cost: Inference takes ~30 seconds per 10-frame rollout (Table 3), which limits real-time or closed-loop use. While acknowledged in §5, more discussion on latency-accuracy trade-offs or potential optimizations (e.g., sparse prediction, distillation) would strengthen practical impact.\n\n2. Real-World Data Scale: The real-world dataset includes only 20 demos per task. While fine-tuning from simulation helps, it’s unclear how performance scales with more diverse real data or more complex scenes (e.g., clutter, deformables).\n\n3. Dependency on FoundationPose: The policy pipeline assumes access to a high-quality 6DoF tracker and a CAD model of the gripper. Generalization to arbitrary tools or unmodeled objects is not explored."}, "questions": {"value": "1. Camera Pose at Training: The method requires camera extrinsics during training to project pointmaps into a shared frame. How sensitive is performance to errors in these extrinsics? Could the system be made fully self-supervised (e.g., via structure-from-motion)?\n\n2. Generalization Beyond Grippers: The pose tracking focuses on the robot end-effector. Can the same 4D video be used to track object poses (e.g., cereal box, apple) for object-centric manipulation? This would broaden applicability.\n\n3. Failure Modes: In Table 2, Task 3 has the lowest success rate (~53%). What are the dominant failure modes? Are they due to video generation errors, pose tracking inaccuracies, or open-loop execution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Prh85nEsOu", "forum": "18gC6pZVVc", "replyto": "18gC6pZVVc", "signatures": ["ICLR.cc/2026/Conference/Submission6362/Reviewer_ctzc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6362/Reviewer_ctzc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994286496, "cdate": 1761994286496, "tmdate": 1762918652955, "mdate": 1762918652955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a robot control strategy based on image-conditioned 4D video generation.\n\nA 4D video diffusion model is conditioned on an RGB-D image pair; this generates a pair of pointmap videos (eg, that can be deprojected into a dynamic point cloud). The authors then propose to run FoundationPose-based pose tracking on these generated videos to extract gripper poses, which can then be executed on a robot.\n\nThis is similar to prior work in using RGB video for robot control (eg, Dreamitate), but includes extra geometry supervision for the video model.\n\nExperimental results show that this results in robot control policies that generalize better to unseen initial object poses and novel camera viewpoints than methods like Dreamitate and Diffusion Policy.\n\nOverall: the core idea of the paper seems simple but valuable. I'm suggesting a weak accept."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I found this to be a  well-written paper, with a clear formulation and convincing empirical results. There's reasonably thorough evaluation across stages: for both 4D generation and for learned robot policies.\n\nI think the robot learning community would benefit from these results; it's a nice existence proof for one way video/world models might be useful for robot learning."}, "weaknesses": {"value": "While the baselines are reasonable, they do feel slightly \"set up to fail\". For video generation the baselines are RGB only while the proposed method is trained on RGB-D; same for policy rollout. The core message of the paper, however, is in part that the geometry component is critical so perhaps this is fine.\n\nWhile the approach is elegant and the results are strong, the system depends heavily on multi-view RGB-D data and an external pose tracker (FoundationPose). This raises questions about scalability to real-world deployment, especially in less instrumented or monocular setups.\n\nThe reliance on diffusion-based generation also makes inference relatively slow, which (for now) limits applicability to closed-loop control or online replanning."}, "questions": {"value": "How robust is FoundationPose on generated 4D video? Are any of your task failures caused by perception failures, for example if you cannot successfully track the gripper pose or open/close state?\n\nThe paper currently only presents task success rates for unseen object poses/novel camera viewpoints. It makes sense to me that methods with less 3D inductive bias would fail here. Do you have success rates for *seen* object poses and/or *fixed* camera viewpoints? It would be helpful, for example, to know how well the 4D video-based approach does compared to RGB video or vanilla diffusion policy when there's less of a train/test gap."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LTrbF0Aq9x", "forum": "18gC6pZVVc", "replyto": "18gC6pZVVc", "signatures": ["ICLR.cc/2026/Conference/Submission6362/Reviewer_QvcZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6362/Reviewer_QvcZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002770741, "cdate": 1762002770741, "tmdate": 1762918652580, "mdate": 1762918652580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}