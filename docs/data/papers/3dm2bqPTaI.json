{"id": "3dm2bqPTaI", "number": 10680, "cdate": 1758179394142, "mdate": 1763513416275, "content": {"title": "Efficient Self-Guided Editing for Text-Driven Image-to-Image Translation", "abstract": "Diffusion-based generative models achieve impressive text-driven image synthesis, largely due to classifier-free guidance (CFG), which enhances semantic alignment through blending conditional and unconditional denoising predictions. However, in text-guided image editing, CFG frequently induces structural drift, with the unconditional branch generating spatial mismatches. Prior approaches mitigate this by adding a reconstruction branch to steer the unconditional predictions, yet this consumes substantial GPU memory and computational resources. Our empirical studies uncover the inherent trade-off between semantic accuracy and structural integrity, pinpointing the null-text branch as the key culprit. We introduce a Target-Guided Unconditional Branch that repurposes semantic cues from the target prompt and initial denoising inputs from the source image to ensure spatial consistency. This method delivers superior editing quality without extra computational burden, serving as an efficient substitute for traditional CFG-dependent editing methods. Our experiments on PIE-Bench demonstrate that our method outperforms state-of-the-art baselines in structure preservation and background retention while maintaining comparable semantic alignment, all with reduced inference time and GPU memory usage.", "tldr": "", "keywords": ["Diffusion Model", "Image-to-Image Translation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/301367eaf8b2eb4db961b8bac138bca800aeae4a.pdf", "supplementary_material": "/attachment/a1c055be9bf3faa42a43ceec61efa86a428fbbec.zip"}, "replies": [{"content": {"summary": {"value": "The submissions tackles the problem that in text-guided image editing with diffusion models, classifier-free guidance (CFG) improves semantic alignment but often induces structural drift. The authors identify the unconditional (null-text) branch in CFG as the main source of this structural drift. To mitigate the drift, the submission proposes to inject the mid-level features and self-attention from the target-prompt (conditional) branch to the unconditional branch."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main idea of using the artefacts from the conditional branch for the unconditional one looks novel for me, it does eliminate the need for an extra reconstruction branch (as in PnP/MasaCtrl)."}, "weaknesses": {"value": "(1) In terms of experimental evaluation, I did not find the user study comparison, all the quantitative results are reported in terms of automated performance metrics. In my opinion, human-based evaluation is necessary for such kind of papers.\n\n(2) The submission does not compare to the relevant baselines of fast inversion-based editing (LEDITS++: Limitless Image Editing using Text-to-Image Models. Brack et al, An Edit Friendly DDPM Noise Space: Inversion and Manipulations. Huberman-Spiegelglas et al) both in terms of quality and efficiency.\n\n(3) The submission provides quite limited justification for the main design choices (TG, SNS). In my opinion, the authors do not convincingly explain why the target-guidance and the source-noise sharing should jointly improve structure preservation while maintaining edit strength. I appreciate the sections 4.1–4.2 but they are not convincing enough."}, "questions": {"value": "Please, address my concerns in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VBV2RWaUiT", "forum": "3dm2bqPTaI", "replyto": "3dm2bqPTaI", "signatures": ["ICLR.cc/2026/Conference/Submission10680/Reviewer_LU8t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10680/Reviewer_LU8t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846536981, "cdate": 1761846536981, "tmdate": 1762921928845, "mdate": 1762921928845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "bLn5DwY6Cn", "forum": "3dm2bqPTaI", "replyto": "3dm2bqPTaI", "signatures": ["ICLR.cc/2026/Conference/Submission10680/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10680/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763513415361, "cdate": 1763513415361, "tmdate": 1763513415361, "mdate": 1763513415361, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Efficient Self-Guided Editing (ESG) which incorporates two techniques for inversion-based editing aimed at improving unconditional predictions within CFG for higher spatial consistency. Also, by eliminating the need in additional source preservation branch, ESG yields faster editing and lower memory usage. ESG is evaluated on PIE-bench and shows editing improvements over several inversion methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The method avoids using the external source-reconstruction branch and thus enables more efficient inversion-based editing."}, "weaknesses": {"value": "* Lack of proper justification for why TG and SNS are reasonable. It remains unclear to me why TG and SNS should improve source preservation and provide strong editing results. I can see some discussions in Secs. 4.1 and 4.2, but they did not help me much. I highly encourage including additional analysis, clarifications, and discussions.\n\n* A few simple baselines that may improve source preservation for inversion-based editing are missing. I believe both are valuable to illustrate the importance of TG and SNS:\n\n1) Interval guidance [1,2], where CFG is turned off for high-noise steps. It should significantly improve spatial consistency while preserving editing strength.\n\n2) Early-stopped DDIM inversion: stopping at intermediate timesteps, e.g., t = 700–800, which also improves source preservation.\n\n* The method is essentially a CFG modification that resembles some improved CFG methods [3,4], but is applied to inversion-based editing. These methods also modify the unconditional prediction, and thus comparisons with them are important. Moreover, other CFG modifications based on prediction projection could also be relevant baselines [5,6].\n\n* Missing baselines on efficient inversion-based image editing [7,8].\n\n* The method is designed for UNet-based diffusion models, while current s.o.t.a. models are based on transformers and flow matching.\nSince multiple inversion-based editing methods already exist for the models such as FLUX [9,10,11], I believe it is important to adapt the method to transformer-based models, evaluate it on SOTA models (e.g., FLUX, Cosmos-Predict2) and compare against the recent baselines.\n\nMoreover, among the UNet-based models, the experiments are only on SD2.1, which is now largely outdated. I would expect results on SDXL at least. The comparison with [9] would be valuable as well.\n\n**Minor**\n\n* The experiment in the Introduction is confusing, as it uses specific terminology before any background or literature review is provided. Even well-informed readers may be misled by different variants of “conditional” and “unconditional” branches without a precise description of how these options work.\n* Missing brackets for citations: \\cite -> \\citep\n\n---\n\n[1] Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models. NeurIPS2025\n\n[2] Invertible Consistency Distillation for Text-Guided Image Editing in Around 7 Steps. NeurIPS2025\n\n[3] CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models. ICLR2025\n\n[4] Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling. CVPR2025\n\n[5] Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models. ICLR2025\n\n[6] CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models. 2025\n\n[7] An Edit Friendly DDPM Noise Space: Inversion and Manipulations. CVPR2024\n\n[8] LEDITS++: Limitless Image Editing using Text-to-Image Models. CVPR2024\n\n[9] Tight Inversion: Image Conditioned Inversion for Real Image Editing. CVPR2025\n\n[10] Semantic Image Inversion and Editing using Stochastic Rectified Differential Equations. ICLR2025\n\n[11] Stable Flow: Vital Layers for Training-Free Image Editing. CVPR2025"}, "questions": {"value": "Please address the concerns in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kSLP80TR9P", "forum": "3dm2bqPTaI", "replyto": "3dm2bqPTaI", "signatures": ["ICLR.cc/2026/Conference/Submission10680/Reviewer_kVTE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10680/Reviewer_kVTE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850434950, "cdate": 1761850434950, "tmdate": 1762921928476, "mdate": 1762921928476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In text-guided image editing, CFG can enhance semantic fidelity but exhibit distortions in the original image’s geometric structure and spatial coherence. To explore the trade-off between structural fidelity and semantic alignment, the authors attempt to perform two systematic empirical studies:\n- Compare image editing with and without CFG.\n- Compare two distinct prompt-injection strategies, one is injecting a source’s branch into both condition and unconditional branches, and another is injecting only the conditional branch into the unconditional branch.\n\nThey further propose Efficient Self-Guided Editing (ESG), a framework designed to enhance structural preservation and semantic fidelity in DDIM-based text-driven image editing, without the need for an additional source reconstruction branch."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Extensive experiments are conducted\n- The writting is easy to follow"}, "weaknesses": {"value": "- The motivation is not very clear, as the inherent CFG mechanism already controls the trade-off between unconditional and conditional inputs.\n- Figure 1 lacks clarity and is not easy to interpret. The text annotations within the figure are difficult to read, which weakens the presentation of the paper’s motivation.\n- What does the source’s branch refer to in line 83?\n- Figure 2, which presents the main architecture of the method, is not intuitive and is difficult to understand. Both the figure and its caption lack clarity, making it hard for readers to grasp the overall workflow. \n- In Figure 3, the visual results of the proposed ESG method appear quite similar to those of PnP, showing only marginal improvements.\n- In Table 1, the proposed method performs worse than PnP in terms of CLIP similarity. The paper does not provide an explanation or analysis for this performance gap\n- In Table 2, the authors claim that editing with only the condition branch losses semantic alignment with the target prompt. These metrics cannot measure this property. Given visual comparison can help verify this anlaysis."}, "questions": {"value": "- What does the empty set in equation4-7 mean?\n- The CGF is used to blend the uncondition and uncontion branches, and it is an ajustable constant. In this paper, the authors do not specify the exact value of this constant."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper mainly lacks visual evidence to support the claims, as well as an analysis of the CFG value settings and their relationship to the proposed method."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8qTKwfENkc", "forum": "3dm2bqPTaI", "replyto": "3dm2bqPTaI", "signatures": ["ICLR.cc/2026/Conference/Submission10680/Reviewer_zhU6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10680/Reviewer_zhU6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894199369, "cdate": 1761894199369, "tmdate": 1762921928097, "mdate": 1762921928097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper “Efficient Self-Guided Editing for Text-Driven Image-to-Image Translation” introduces a training-free diffusion editing framework called ESG that aims to balance semantic alignment to the target prompt and preservation of the original image structure. The authors identify that classifier-free guidance often causes structural drift, mainly due to the unconditional (null-text) branch, which tends to diverge spatially during denoising. To fix this, the authors propose two key strategies: a) Target-Guided Unconditional Branch and b) Source Noise Sharing. This approach avoids the extra reconstruction branch needed in methods like MasaCtrl or PnP (prior works), reducing GPU memory and computational cost. Experiments on PIE-Bench show ESG better preserves structure and background while maintaining similar CLIP-based semantic consistency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper clearly recognizes the motivation for method design, and it is well-explained in the Introduction section (Sec. 1.)\n\nS2. The paper proposes two novel components for image-to-image translation, which derives critical performance improvements as explained in the Experiment Section.\n\nS3. The proposed method shows superior performance in a quantitative manner compared to baselines (SDEdit, MasaCtrl, and Plug-and-Play)."}, "weaknesses": {"value": "**Major Weakness**\n\nW1. This paper only compares with the three baselines; that is, SDEdit, MasaCtrl, and Plug-and-play. However, there are more recent state-of-the-art work for diffusion-based image-to-image translation, such as Diffusion Self-Guidance, LocInv, DDS, Motion Guidance, and etc. I think comparison with more recent works is also required.\n\nW2. The experiment section only focuses on the object change tasks, such as Rusty car → Shiny car and young woman → old woman. I was wondering if the method is applicable to more difficult tasks, such as 1) object addition (or object duplication), 2) object deletion, and 3) enlarging or shrinking the object size. Extensive experiments with additional tasks is required.\n\nW3. The authors use Stable diffusion v2.0 and v2.1 for experiments. However, recent works use the Transformer-based diffusion model, beyond U-Net based Stable Diffusion. Is it possible to apply the proposed method into the DiT-based model, such as Stable Diffusion v3?\n\n\n\n**Minor Weakness**\n\nW4. The overall writing can be improved. Especially citations styles (Author et al. (year)) can be changed to improve the readability of the main paper."}, "questions": {"value": "Please check the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gP7H5U9l1r", "forum": "3dm2bqPTaI", "replyto": "3dm2bqPTaI", "signatures": ["ICLR.cc/2026/Conference/Submission10680/Reviewer_vrJW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10680/Reviewer_vrJW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961618707, "cdate": 1761961618707, "tmdate": 1762921927642, "mdate": 1762921927642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}