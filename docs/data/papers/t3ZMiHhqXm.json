{"id": "t3ZMiHhqXm", "number": 2439, "cdate": 1757086712106, "mdate": 1759898147926, "content": {"title": "Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models", "abstract": "Vision-language models trained on large-scale multimodal datasets show strong demographic biases, but the role of training data in producing these biases remains unclear. A major barrier has been the lack of demographic annotations in web-scale datasets such as LAION-400M. We address this gap by creating person-centric annotations for the full dataset, including over 276 million bounding boxes, perceived gender and race/ethnicity labels, and automatically generated captions. These annotations are produced through validated automatic labeling pipelines combining object detection, multimodal captioning, and finetuned classifiers. Using them, we uncover demographic imbalances and harmful associations, such as the disproportionate linking of men and individuals perceived as Black or Middle Eastern with crime-related and negative content. We also show that 60-70\\% of gender bias in CLIP and Stable Diffusion can be linearly explained by direct co-occurrences in the data. Our resources establish the first large-scale empirical link between dataset composition and downstream model bias.", "tldr": "We release person-level demographic annotations for LAION-400M and show that imbalances in the data largely explain demographic biases in CLIP and Stable Diffusion.", "keywords": ["dataset bias", "model bias", "laion-400m"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c7b85b27d1e15b934b4764b1dad6e15eb2d468e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper annotates the images of LAION 400M by providing bounding boxes of person detections, inferred gender and race labels, as well as detailed captions for each detected person. The annotations reveal significant issues: males and people with Middle Eastern or Black appearances are more strongly correlated crime-related content or negative sentiment, a lot of which is shown to propagate into the models trained on the studied dataset. The authors also study the themes of captions concerning the racial and gender combinations, and uncover interesting patterns."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides gender and racial annotations for images in LAION-400M, which is a significant contribution given that LAION400M is web-scraped.\n2. Instead of using off-the-shelf gender and race classifiers, authors particularly finetune models (e.g., SIGLIP for gender) to make them more aware of the domain, and to handle female, male, mixed and unclear cases (and the equivalent for race).\n3. Analysis reveals more association of males and Middle-Eastern/Black races with crime and negative sentiments\n4. Authors also attempt to tie dataset bias with model bias (CLIP and Stable Diffusion), and find a significant overlap."}, "weaknesses": {"value": "1. The paper examines LAION400M, which was used to train earlier versions of OpenCLIP and Stable Diffusion, hardly used now. The biases are expected to grow with scale, as emphasized by previous papers [a], and a similar experiment on LAION 2B, DataComp, etc would have helped us analyse biases in more modern models. \n2. The identity-topic associations are dependent on the automatically generated captions from pretrained caption generators. Such models may carry their own biases, and it is hard to say if the generated captions are accurate. Similarly, topic analysis on the original captions may have revealed more patterns.\n3. The authors study biases in downstream models via social categories. Crime-related and Sentiment-based analysis would have been valuable too. It often does not guarantee that presence of social category c in captions would ensure that the persons present in the corresponding images actually belong to category c, due to misalignment issues [b].\n4. The authors mention that 60-70% of the biases in downstream models can be explained by those in the datasets. However, they do not discuss what leads to the rest of the biases, especially in Stable Diffusion.\n\n[a] Birhane et al., 'Into the LAION’s Den: Investigating Hate in Multimodal Datasets', NeurIPS D&B 2023\n\n[b] Udandarao et al., 'No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance', NeurIPS 2024"}, "questions": {"value": "1. What if the gender and race classifiers were not finetuned, and instead the ensemble VLMs were used to annotate the entire dataset? Do the authors avoid it for computational costs? What are their thoughts on analysis on some other dataset like CC-12M or DataComp - is a separate finetuning required for those cases too?\n2. How do the authors verify that YOLOv11-l does not have gender/racial biases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "054Bnamrrw", "forum": "t3ZMiHhqXm", "replyto": "t3ZMiHhqXm", "signatures": ["ICLR.cc/2026/Conference/Submission2439/Reviewer_DJbC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2439/Reviewer_DJbC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484726416, "cdate": 1761484726416, "tmdate": 1762916238674, "mdate": 1762916238674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a large-scale study that adds person-centric annotations to the LAION-400M dataset. It automatically labels each detected person with perceived gender and race or ethnicity using a combination of object detection models and MLLMs. The paper then analyzes demographic distributions, harmful associations such as links to crime or negative sentiment, and the relationship between dataset bias and model bias in CLIP and Stable Diffusion. The main finding is that about 60–70% of model bias can be explained by co-occurrence patterns in the data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important and timely topic.\n\n- While prior work has provided protected-attribute annotations for smaller datasets such as COCO or GCC, this paper scales the effort to LAION-400M, a much larger and more representative dataset, aligning with recent trends in large-scale data research.\n\n- The analysis is comprehensive and provides multiple insights. For example, the paper uncovers detailed demographic distributions, harmful associations, and correlations between data composition and model outputs.\n\n- The study linking dataset bias and model bias is particularly interesting. The finding that a significant portion of model bias can be explained by dataset co-occurrences highlights the need for the community to address dataset-level bias more seriously."}, "weaknesses": {"value": "- There is a potential risk that biases from the MLLMs used for demographic attribute annotation and caption generation propagate into the resulting annotations. For instance, if these models make more errors for certain genders or races, their biases may directly influence the final dataset. Although the paper validates agreement with human-labeled datasets, three concerns remain:\n\n1. The agreement is relatively low for race annotations, and it is questionable to dismiss this easily. If certain racial groups have higher error rates, the final demographic distribution could diverge significantly from reality.\n\n2. Relatedly, the paper only reports aggregate error rates but does not analyze error trends. It would be important to know whether errors are uniformly distributed or concentrated on specific groups, as this strongly affects the reliability of MLLM-based annotations.\n\n3. No human study was conducted to verify the quality of the obtained annotations. While comparison with datasets such as FACET provides a proxy for human validation, a small-scale human study (even around 1K samples) on LAION-derived annotations would greatly strengthen confidence in their accuracy.\n\n- In the dataset–model bias correlation experiment, the paper mentions that “the remaining 30–40% of bias stems from nonlinear or higher-order effects,” but does not provide any quantitative or qualitative analysis of these effects. Since this bias-transfer analysis is one of the central contributions, including at least some empirical investigation or hypothesis testing for these unexplained components would significantly reinforce the paper’s impact."}, "questions": {"value": "**Overall assessment and suggestions**\n\nThis paper presents an interesting and valuable attempt to provide protected-attribute annotations and large-scale demographic analysis for a dataset of the scale of LAION-400M. The topic is timely and important, and the effort to enable systematic auditing of web-scale data is commendable.\n\nHowever, I believe the paper does not sufficiently analyze (or mitigate) the potential biases introduced by the automatic annotation pipeline itself, especially those arising from the MLLMs used for labeling gender and race. Given that this work focuses on human-centric annotations, such limitations are critical and cannot be easily overlooked.\n\nIf the rebuttal provides a convincing analysis or additional validation addressing this issue, I would be happy to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U9YGULs1Nn", "forum": "t3ZMiHhqXm", "replyto": "t3ZMiHhqXm", "signatures": ["ICLR.cc/2026/Conference/Submission2439/Reviewer_gDXh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2439/Reviewer_gDXh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720912945, "cdate": 1761720912945, "tmdate": 1762916238382, "mdate": 1762916238382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to examine demographic patterns in a large web dataset and how they reflect in downstream generative systems. The paper augments LAION-400M image-caption dataset by including person-level annotations, encompassing bounding boxes around individuals, with automatically inferred gender and ethnicity labels, and detailed captions for each detected person.\n\nWith this annotated dataset, the paper examines how different gender and ethnic groups are represented and how these identities intersect with themes like crime, sentiment, and broader contextual associations. They also explore how gender-related biases in the dataset correlate to biases in two models trained on LAION-400M, CLIP and Stable Diffusion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The study proposes an interesting methodology to examine demographic biases in pretraining datasets and their effects on generative models.\n* The scope of this work is impressive, both in its scale and in its comprehensive examination of demographic patterns and biases within the dataset.\n* The paper presents important findings on how sentiments and topics are associated with gender and ethnic identities, and to what extent gender bias in model generations correlates with biases in the training dataset.\n* The additional annotations might facilitate future research, such as studying other forms of dataset-model interaction"}, "weaknesses": {"value": "* While the paper ambitiously combines large-scale demographic annotation with multiple layers of analysis, its broad scope makes the presentation overly dense. As a result, some key details and justifications are underdeveloped or omitted from the main paper, limiting clarity and depth in certain areas. \n* To generate gender and ethnicity labels, the paper fine-tunes a SigLIP classifier using a subset of the LAION-400M data labelled by three different MLLMs. The labelling process relies on consensus among these models, with training (and testing) data primarily drawn from images where all agreed. While this approach enhances label reliability, it may bias the classifier toward clear-cut examples and limit its robustness to the ambiguous or noisy cases.\n\nMinor typos:\nLine 52 mentions the word “intersectionalidentity”\nLine 291, the caption says “compound score (orange)”, color is not exactly orange"}, "questions": {"value": "Some suggestions: \n\n* The paper uses YOLOv11-l for bounding box generation, relying on evaluations from datasets such as FACET and PHASE. Since LAION is a much noisier web-based dataset with the possibility of multiple people per image, assessing the detector’s accuracy on a subset of LAION would strengthen the work and ensure reliability in this context.\n\n* The paper mentions a qualitative analysis of MLLMs over 4,939 bounding boxes to select one model for generating person-specific captions; however, the description of this process is somewhat vague. Providing more detail on this selection procedure and, if feasible, extending similar qualitative analysis to the gender and ethnicity detection pipelines would strengthen the reliability of the overall methodology.\n\nI am willing to reconsider my assessment based on authors' response to the above. \n\n* (Minor): it would be good to hear authors' take on where they think the annotations could be used in the future"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The paper includes example images of clearly identifiable people from the LAION-400M dataset. Since LAION contains web-scraped content without explicit consent, displaying such images may violate privacy of individuals."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ozVTAnSgz7", "forum": "t3ZMiHhqXm", "replyto": "t3ZMiHhqXm", "signatures": ["ICLR.cc/2026/Conference/Submission2439/Reviewer_uTpU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2439/Reviewer_uTpU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976967092, "cdate": 1761976967092, "tmdate": 1762916238019, "mdate": 1762916238019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper generated person-centric annotations over LAION-400M, resulting in 270M odd detected person bounding boxes, 200M perceived gender and race/ethnicity labels (after filtering), and person-centric captions generated by MLLMs. The labels are used to quantify the level of demographic imbalances, thematic associations (via sparse autoencoders), and quantify how much of observed gender bias in CLIP and Stable Diffusion can be linearly explained by dataset co-occurrences.\n\nThe paper quantifies the extent to which bias in downstream models can be attributed to biases in the dataset. Futhermore, the dataset can be useful for future studies to understand dataset-model interactions in propagating or amplifying biases in large-scale datasets. The reliability of the findings can be improved by auditing potential issues of bias in the proposed process (MLLM ensemble labeling -> classifier training -> dataset labeling -> bias estimation)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- While the prevalence of bias in large models, and its attribution to bias in training datasets is well known, there are no large-scale annotated datasets with demographic labels. So the proposed dataset could be a valuable resource for more fine-grained studies seeking to understand and mitigate bias in models trained on large models.\n\n- The workflow (YOLOv11 person detection -> MLLM ensemble labeling -> SigLIP finetuning -> full-dataset labeling -> analyses) for automated labeling is quite reasonable, well described and should be reproducible in principle.\n\n- The attempt to quantitatively relate dataset co-occurrence statistics to measured model biases (CLIP, Stable Diffusion) has been lacking and this paper fills the gap."}, "weaknesses": {"value": "There are several weaknesses in the proposed methodology, which reduces the reliability of the quantitative findings.\n\n- Labeling relies on an MLLM ensemble consensus. However, these MLLMs may have inherent biases that would now propagate through the rest of the method. There is no analysis on the errors and biases of the MLLM ensemble. Similarly, bias in the pre-trained and fine-tuned SigLIP has not been analyzed.\n\n- All the presented results in the paper are point estimates. How reliable are these estimates? Confidence intervals are missing. Similarly, how sensitive are the correlation estimates to hyperparameter choices in the full pipeline?\n\n- Statements like “60–70% of gender bias in CLIP and Stable Diffusion can be linearly explained by direct co-occurrences in the data” are stronger than warranted. The result shows a strong *correlation* but not necessarily *causation*. These claims need to either be substantiated or rephrased."}, "questions": {"value": "- In cases where the MLLM ensemble agree or disagreed, how do they relate to specific demographic groups?\n\n- Bias analysis of the gender and race classifiers.\n\n- To understand the reliability and robustness of the claims, confidence intervals and sensitivity to hyperparameters."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper has proactively addressed potential ethics concerns, especially with respect to automated labeling of perceived gender/race and how it might not reflect reality. I think the authors have given sufficient thought to these concerns and addressed them adequately."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "V5T28YFsaZ", "forum": "t3ZMiHhqXm", "replyto": "t3ZMiHhqXm", "signatures": ["ICLR.cc/2026/Conference/Submission2439/Reviewer_sfn7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2439/Reviewer_sfn7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128056603, "cdate": 1762128056603, "tmdate": 1762916237822, "mdate": 1762916237822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}