{"id": "NuAfgBTLA5", "number": 24711, "cdate": 1758359562191, "mdate": 1759896753315, "content": {"title": "DoReMi - Difficulty-Oriented Reasoning Effort Modeling of Science Problems for Language Models", "abstract": "We introduce DoReMi (Difficulty-Oriented Reasoning Effort Modeling), a structured framework leveraging an extended Bloom's taxonomy to comprehensively characterize intrinsic problem difficulty for large language models on scientific reasoning tasks. DoReMi systematically annotates problems along seven cognitive and methodological axes using judge LLMs distinct from those being evaluated, with human annotations confirming the validity of these assessments. We empirically quantify LLM reasoning effort through metrics including minimum reasoning tokens required for solution, expected trials to first success. Our validation demonstrates strong agreement across diverse judge LLMs spanning both open-source and proprietary LLMs. Evaluations on GPQA, ARC, and SuperGPQA reveal that our multidimensional difficulty fingerprints correlate strongly with and enable accurate predictive modeling of LLM reasoning effort. DoReMi enables principled difficulty-aware subset selection that substantially outperforms static-difficulty baselines while providing interpretable diagnostics that uncover emergent reasoning capabilities across successive model generations. This framework offers actionable insights for benchmark design and targeted post-training improvements toward higher-order reasoning skills.", "tldr": "Analysing how Bloom’s taxonomy-based difficulty metrics help measure the reasoning effort required by LLMs on science benchmarks, revealing key differences across model generations and offering guidelines for evaluation and improvement.", "keywords": ["Evaluation", "Difficulty analysis", "Reasoning models", "Reasoning effort", "Scientific benchmarks", "LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/261c5a1720ffff2e7bb34a663d5bbb584451642f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce DoReMi (Difficulty-Oriented Reasoning Effort Modeling), a structured framework that leverages an extended Bloom’s taxonomy to comprehensively characterize the intrinsic difficulty of scientific reasoning tasks for large language models. DoReMi systematically annotates problems along six cognitive and methodological axes using judge LLMs distinct from those being evaluated, with human annotations confirming the validity of these assessments. The authors empirically quantify LLM reasoning effort through metrics including the minimum reasoning tokens required for a solution and the expected number of attempted runs to the first correct answer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors present a clearly articulated framework for constructing evaluations, supported by analysis comparing the results with human annotations and enhanced with visualizations through accompanying figures."}, "weaknesses": {"value": "The evaluation construction method proposed by the authors is not particularly novel, but it lacks references to and comparisons with similar works, such as:  \n[1] WritingBench: A Comprehensive Benchmark for Generative Writing  \n[2] HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models  \n[3] DynamicBench: Evaluating Real-Time Report Generation in Large Language Models  \nas well as other relevant open-source efforts."}, "questions": {"value": "Same as above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cdCYIHRqEx", "forum": "NuAfgBTLA5", "replyto": "NuAfgBTLA5", "signatures": ["ICLR.cc/2026/Conference/Submission24711/Reviewer_5CxN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24711/Reviewer_5CxN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761473326923, "cdate": 1761473326923, "tmdate": 1762943170805, "mdate": 1762943170805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DoReMi, a framework that uses an extended Bloom's taxonomy to characterize the intrinsic difficulty of scientific reasoning problems for LLMs. The authors systematically annotate problems along six axes (Cognitive Level, Knowledge Dimension, Method Difficulty, Definition Completeness, Knowledge Breadth, Number of Reasoning Steps) and correlate these with empirical reasoning effort metrics (MRT, R2FCA). The framework enables difficulty-aware evaluation and provides interpretable diagnostics of LLM reasoning capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Well-Motivated Problem: The paper addresses a genuine need in LLM evaluation - moving beyond single-dimensional accuracy scores to understand why problems are difficult for reasoning models. \n- Theoretically Grounded Framework: Using Bloom's taxonomy provides a principled, interpretable foundation rather than ad-hoc difficulty metrics. The six-axis extension is thoughtfully designed."}, "weaknesses": {"value": "- Should demonstrate your metrics through RL training: The paper repeatedly claims DoReMi provides \"actionable insights for targeted post-training improvements”.  However, no experiments validate that training on DoReMi-selected samples actually improves model performance. Suggested experiments:\n    - Train smaller models using DoReMi-guided curriculum learning\n    - Compare sample efficiency against random or static difficulty baselines\n    - Perform targeted fine-tuning on identified weak Bloom axes\n    - Use DoReMi difficulty scores for reward shaping in RL\n\n- Judge Model Overlap Concerns: The paper uses reasoning LLMs as judges, but also evaluates reasoning LLMs. While they claim judges are \"distinct from those being evaluated,\" some overlap exists (e.g., o3-mini as judge, o3-mini-high as evaluation target). Potential for judges to be biased toward difficulty patterns they themselves exhibit\n\n- Generalization Concerns: Evaluated only on science/STEM problems - unclear if taxonomy applies to other reasoning domains (coding, math, logic puzzles). All benchmarks are multiple-choice or short-answer - what about open-ended reasoning?"}, "questions": {"value": "Please refer weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7DLy1M2AYf", "forum": "NuAfgBTLA5", "replyto": "NuAfgBTLA5", "signatures": ["ICLR.cc/2026/Conference/Submission24711/Reviewer_x121"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24711/Reviewer_x121"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881687422, "cdate": 1761881687422, "tmdate": 1762943170543, "mdate": 1762943170543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a learning-based framework to provide a comprehensive estimation of problem difficulty for LLMs. The framework considers six cognitive and methodological axes based on Bloom’s taxonomy and combines the performance of the target LLM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a structured and multidimensional evaluation framework to provide a more comprehensive and precise estimate of the question complexity\n2. The paper illustrates the applications of the proposed framework: filtering challenging questions and providing a systematic analysis of LLM reasoning capabilities."}, "weaknesses": {"value": "1. The pipeline involves substantial LLM usage: question labeling requires LLM inference, and reasoning effort calculation requires multiple samples from the target LLM. The whole pipeline seems to be computationally expensive\n2. The predictor training process requires collecting responses from target LLMs. It is unclear whether the learned neural network generalizes to other models. If not, Phases 2-4 would need to be repeated from scratch for each new target LLM. \n3. Mathematical problems like GSM8k are usually considered as a benchmark to evaluate LLM reasoning capabilities. Including performance on math problems would provide a more comprehensive evaluation of the current framework."}, "questions": {"value": "1. For the 6 dimensions of extended Bloom’s taxonomy, what is the reason to include “Definition Completeness”? This property seems more related to the solvability, not the difficulty. For “Number of Reasoning steps”, how to define the essential logical action? Is there any question with multiple correct solutions, and the number of reasoning steps is different?\n2. The MRT is selected as the primary feature to train the neural network, because it presents the highest correlation. Why does this metric correlate more strongly with the designed complexity/difficulty definition than other metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l9Oijm43nV", "forum": "NuAfgBTLA5", "replyto": "NuAfgBTLA5", "signatures": ["ICLR.cc/2026/Conference/Submission24711/Reviewer_Q9m8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24711/Reviewer_Q9m8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916592778, "cdate": 1761916592778, "tmdate": 1762943170270, "mdate": 1762943170270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}