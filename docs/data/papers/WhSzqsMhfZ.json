{"id": "WhSzqsMhfZ", "number": 14774, "cdate": 1758243414260, "mdate": 1759897350081, "content": {"title": "Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment", "abstract": "Reward models (RMs) are crucial for aligning large language models (LLMs) with diverse cultures. Consequently, evaluating their cultural awareness is essential for further advancing global alignment of LLMs. However, existing RM evaluations fall short in assessing cultural awareness due to the scarcity of culturally relevant evaluation datasets.\nTo fill this gap, we propose Cultural Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures across 4 cultural domains.\nOur extensive evaluation of state-of-the-art RMs reveals their deficiencies in modeling cultural awareness and demonstrates a positive correlation between performance on CARB and downstream multilingual cultural alignment tasks.\nFurther analysis identifies the spurious correlations within culture-aware reward modeling, wherein RM's scoring relies predominantly on surface-level features rather than authentic cultural nuance understanding.\nTo address these, we propose Think-as-Locals to elicit deeper culturally grounded reasoning from generative RMs via reinforcement learning from verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate preference judgments and high-quality structured evaluation criteria generation. \nExperimental results validate its efficacy in mitigating spurious features interference and advancing culture-aware reward modeling.", "tldr": "This work introduces a culture-aware reward modeling benchmark that reveals defects in existing reward models, and proposes an RLVR-based method to address these limitations.", "keywords": ["cultural awareness", "reward model", "LLM Alignment", "RLHF", "RL", "Dataset", "Benchmark", "Multilingual Evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf52a74b79ab7527ca61c5ab9b0c83bc591b1e22.pdf", "supplementary_material": "/attachment/aac022ba660510ceeeda33170a134559681f6b45.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors focused on the important problem of cultural awareness in reward models (RM). Their contributions include proposing a multilingual, culturally aware reward model (CARB), covering 10 typologically diverse cultures and 4 key cultural dimensions. By analyzing over 20 classifier-based and generative RMs, the authors demonstrate that generative RMs outperform classifier-based ones in capturing cultural nuance. Their results further highlight that value alignment as the most difficult domain. The paper also contributes to improving the RMs with Think-as-Locals, a dual reward function to improve the training of culturally aware RMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper focused on a timely and the important problem of cultural awareness in reward models. \n\nS2: The proposed CARB benchmark explicitly designed for cultural awareness in reward models, filling a genuine research gap, unlike prior multilingual benchmarks ignoring this aspect. CARB covers 10 typologically diverse cultures and 4 key cultural dimensions.\n\nS3: The paper provides comprehensive evaluations on both classifier-based and generative RMs (over 20 models), yielding valuable insights on comparative performance. Nice causal and spurious correlations analysis provide interesting insights of why current RM failed at cultural awareness. \n\nS4: The proposed dual reward function (correctness + criteria appropriateness) is justified and experimentally validated to show effectiveness."}, "weaknesses": {"value": "W1: Although some human validation exists (L173), the dataset’s independence from model-generated bias and with human cultural experts in judging CARB as well as RM judgments.\n\nW2: While carefully filtered and validated by some humans, the CARB benchmark heavily depends on LLM generated data. Including authentic human generated examples would strengthen the benchmark.\n\nW3: [Minor] Despite the coverage of 10 cultures, CARB favours high(er)-resource languages. Low-resource cultures (e.g., African or Indigenous) remain underrepresented, including some of the lower-resource cultures could further strengthen the work and show generalization."}, "questions": {"value": "Comment: Values in Fig. 3 are a bit difficult to see, consider using a darker colour for the grid."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bsBJl9Sbkl", "forum": "WhSzqsMhfZ", "replyto": "WhSzqsMhfZ", "signatures": ["ICLR.cc/2026/Conference/Submission14774/Reviewer_9wq4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14774/Reviewer_9wq4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533798877, "cdate": 1761533798877, "tmdate": 1762925128488, "mdate": 1762925128488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses an under explored yet crucial dimension of aligning large language models (LLMs): cultural awareness in reward modeling. The authors first diagnose the shortcomings of existing reward model (RM) evaluations, which mainly focus on general capabilities and lack culturally grounded benchmarks. To bridge this gap, they introduce CARB (Cultural Awareness Reward modeling Benchmark), covering 10 cultures across 4 cultural domains. The authors then show that CARB scores correlate strongly with downstream multilingual cultural alignment tasks, based on which the paper claims CARB is effective as an evaluation benchmark for RM. Subsequently, the authors show that current RMs struggle to model authentic cultural nuances, often relying on surface-level correlations rather than meaningful cultural reasoning.\nTo mitigate the identified issues, the authors finally propose Think-as-Locals, a reinforcement learning from verifiable rewards (RLVR) approach, designed to elicit deeper, culture-aware reasoning and reduce spurious feature reliance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and easy to follow, with a clear logical flow: each section identifies a limitation and proposes a corresponding solution. \n\n- The topic is timely and important, addressing cultural inclusivity/awareness, a crucial but underexplored aspect of global AI alignment.\n\n- The strategy \"Think-as-Locals\" seems to be an effective RLVR method to elicit deep cultural understanding in generative RMs.\n\n- The experiments are quite extensive with large-scale multilingual evaluation and ablation studies."}, "weaknesses": {"value": "**Major Weaknesses**\n\n(W1) The authors use GPT-4o to translate English prompts into other languages, and report that three human annotators manually refined them. However, according to Section B.1, these annotators are independent undergraduate and graduate students. However, the authors do not report if the human annotators are native speakers (or even fluent speakers) of the studied languages. This raises concerns about the quality and cultural authenticity of the prompts. \n\n(W2) The same concern applies to the human annotation agreement results. If the annotators are not from the respective cultural backgrounds, the evaluation may not fully reflect genuine cultural preferences. A more rigorous evaluation involving native speakers/members of the cultural regions would increase the reliability of CARB and better ensure that it captures authentic human judgments.\n\n(W3) In Section 5, GPT-4o is used to rate generated responses for cultural relevance, faithfulness, and helpfulness. These are complex subjective dimensions, and it is debatable whether GPT-4o can serve as a reliable judge for such evaluations. Even if using LLM-as-a-Judge might be necessary for an efficient evaluation, the authors should provide a human meta evaluation of the LLM-judge in the different cultural settings.\n\n(W3) While the Think-as-Locals method appears effective, it likely increases computational cost, as rewards depend on the full reasoning traces rather than final outputs.\n\n\n**Minor Weaknesses**\n\n- The selected set of cultures is essential to this work and its list should not be hidden in the Appendix (Table 4).\n\n- While CARB covers 10 cultures, its cultural coverage remains limited compared to the global landscape; especially with regard to lower resource languages. Extending the benchmark to more low-resource or underrepresented regions (e.g., from Africa) would enhance representativeness.\n\n- Figure 5 shows that the linear relationship is weak and statistically insignificant for M-RewardBench but strong for CARB. The paper should provide a clearer explanation or hypothesis for this difference.\n\n- Including an analysis of failure cases (e.g., where Think-as-Locals still misinterprets cultural cues) would enhance the understanding of this approach.\n\n- Cultural evaluation is a rapidly growing area and the authors list a lot of relevant works. Some additional, recent works could also be relevant to their section on related work: https://aclanthology.org/2024.acl-long.345/ https://aclanthology.org/2024.acl-long.862/ https://aclanthology.org/2025.naacl-long.402/ https://arxiv.org/abs/2505.21693\n\n- The proposed reward R_{appr} is claimed to be motivated by intrinsic probability, but no intuitive explanation is given. A brief intuition behind this design would improve understanding.\n\n- The authors mention \"CARE\" in line 275. I first thought this was a typo (of their \"CARB\"). But actually, it is a different dataset which is only properly introduced with reference in line 408.\n\n**Typos **\n\nLine 143: structured. -> structured, (comma instead of full stop)"}, "questions": {"value": "- How scalable is CARB in practice? Can it be easily extended to new cultural domains or low-resource languages with limited data?\n\n- How does CARB perform when applied to unseen cultures or languages in post-training or adaptation scenarios?\n\n- Did the human annotators have a suitable background to be able to judge and annotate the diverse cultural regions? (see W2)\n\n- Does the LLM-as-a-Judge align with human judgments? (see W3)\n\n- How does the training time of the more complex procss compares to using a standard reward computed on the final answer only? (see W4)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "psL9obT0t2", "forum": "WhSzqsMhfZ", "replyto": "WhSzqsMhfZ", "signatures": ["ICLR.cc/2026/Conference/Submission14774/Reviewer_SHu3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14774/Reviewer_SHu3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760764780, "cdate": 1761760764780, "tmdate": 1762925127893, "mdate": 1762925127893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CARB (Cultural Awareness Reward modeling Benchmark), a benchmark designed to evaluate and improve the cultural awareness of reward models (RMs) used in aligning large language models (LLMs). CARB includes 10 cultures and 4 domains covering cultural commonsense, values, safety, and linguistics, using a Best-of-N evaluation on human-curated prompts and LLM-generated responses. Results show that current RMs perform inconsistently across cultures, often relying on spurious correlations such as surface linguistic features rather than genuine cultural understanding, though CARB scores correlate positively with downstream multilingual cultural alignment performance. To address these limitations, the paper also introduces Think-as-Locals, an RLVR-based framework that encourages RMs to explicitly generate culturally grounded evaluation criteria before judgment. Experiments demonstrate that Think-as-Locals mitigates spurious biases and enhances the cultural sensitivity and reasoning depth of reward models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**S1.** CARB covers 10 cultures with 4 cultural domains, and the final dataset contains 8,576 prompts with 24 LLM-generated responses, which is very diverse.\n\n**S2.** The study involves large-scale and comprehensive evaluations of a lot of different reward models (such as classifier-based and generative reward models), cross-lingual comparisons, and correlation studies with downstream multilingual alignment tasks.\n\n**S3.** The paper introduces new framework called Think-as-Locals that is shown to encourage culturally grounded reasoning and effectively mitigates spurious correlations in reward modeling, as it outperform strong baselines across multilingual cultural benchmarks, even compared to other method with retraining (i.e. RM-R1)."}, "weaknesses": {"value": "**W1.** Here I group all weaknesses related to CARB as a dataset:\n- CARB relies heavily on GPT-4o for prompt generation and translation, which may introduce translationese or Western-centric phrasing [1]. While three human annotators refined the prompts, the dataset spans ten cultures, which makes me assume that only a subset of languages or cultures received native-level review.\n- The construction of “rejected” responses using embedding similarity and the validation of human annotations with GPT-4 risk circular reasoning, as embedding distance and model agreement do not necessarily capture true cultural contrast or correctness.\n- During generation of the dataset (Figure 14), it appears to assume a one-to-one correspondence between language and culture, which would overlook multilingual or multicultural contexts where a single language represents diverse cultural norms.\n- Some cultural values and commonsense knowledge evolve over time, which potentially misrepresenting shifting cultural norms.\n- Finally, the cultures being considered are limited to languages that are relative high/medium-resourced languages. However, I would say 10 cultures/languages are quite a lot already if the dataset is assessed by native people.\n\n**W2.** Here I group all weaknesses related to Multilingual Alignment Performance evaluation:\n- While I agree that m-reward-bench does not provide multicultural evaluation, I don't think correlation of performance between one benchmark and another is a good indicator of a well-correlated benchmark in terms of cultural knowledge, as there could be many confounding factors that represent such correlation. In addition, it only measures linear correlation, doesn't necessarily mean there is no correlation, furthermore in terms of causality that CARB represents multicultural benchmark. \n\nSuppose correlation were a good measure. Notice that include-base-44 itself consists of academic and professional exams, which also includes cultural-agnostic questions, AlpacaEval (which is the source of OMGEval) contain a lot of questions that are not culturally related (as shown in small number of localizations in the original paper), and BLEND's answers depend on the annotations being given (not representative of the culture of a country as a whole), so another reliance on GPT-4 as a judge wouldn't be great. If correlation were a good measure, this wouldn't remove the possibility that the performance could be correlated to the cultural-agnostic questions.\n\n- Section 5 claims strong “multilingual” correlation between CARB and downstream alignment, yet Section 6.2 explicitly shows that reward models exhibit cross-lingual inconsistency by assigning divergent scores to the same semantic content across languages. Wouldn't this show that CARB’s predictive validity about assessing non-shallow cultural features to be wrong?\n\n**W3.** While the result for the reward model framework looks promising on m-reward-bench and CARB, there is a lack of evidence that the reward model can be used to successfully post-train a policy model (RLHF or DPO). \n\n## References\n\n[1] Yan, J., Yan, P., Chen, Y., Li, J., Zhu, X., & Zhang, Y. (2024). Gpt-4 vs. human translators: A comprehensive evaluation of translation quality across languages, domains, and expertise levels. arXiv preprint arXiv:2407.03658."}, "questions": {"value": "Beyond my comments in weaknesses, here are my questions:\n\n**Q1.** Related to **W1**, what kind of metric did you use to measure the inter-annotator agreement between GPT-4o since it's in percentage?\n\n**Q2.** Most of the reward models used are LLM-as-a-judge(s) or reward model that is trained in English, how about specific multilingual models that (possibly) learn cultures during training (from recent related multilingual works), such as mR3 [1], Multilingual Nemotron [2], m-prometheus [3]?\n\n**Q3.** After RLVF, seems like there is much more improvement in CARB compared to m-reward-bench, and I wonder if it's due to in-domain evaluation as the curated training dataset also involves CARB?\n\n## References\n\n[1] Anugraha, D., Hung, S. Y., Tang, Z., Lee, A. E. S., Wijaya, D. T., & Winata, G. I. (2025). mR3: Multilingual Rubric-Agnostic Reward Reasoning Models. arXiv preprint arXiv:2510.01146.\n\n[2] Wang, Z., Zeng, J., Delalleau, O., Egert, D., Evans, E., Shin, H. C., ... & Kuchaiev, O. (2025). HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks. arXiv preprint arXiv:2503.04378.\n\n[3] Pombal, J., Yoon, D., Fernandes, P., Wu, I., Kim, S., Rei, R., ... & Martins, A. F. (2025). M-Prometheus: A Suite of Open Multilingual LLM Judges. arXiv preprint arXiv:2504.04953."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oR2eIUukpU", "forum": "WhSzqsMhfZ", "replyto": "WhSzqsMhfZ", "signatures": ["ICLR.cc/2026/Conference/Submission14774/Reviewer_ZBoR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14774/Reviewer_ZBoR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783080972, "cdate": 1761783080972, "tmdate": 1762925127439, "mdate": 1762925127439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a high-quality dataset called the Cultural Awareness Reward Modeling Benchmark (CARB). By benchmarking a series of reward models on CARB, the authors identify spurious correlations in culture-aware reward modeling, which shows that RM scores often depend on surface-level features rather than genuine cultural understanding. To address this problem, they propose Think-as-Locals, a method designed to elicit deeper, culturally grounded reasoning in generative reward models through reinforcement learning with verifiable rewards. This approach helps mitigate the influence of spurious features and advances culture-aware reward modeling."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper introduces a high-quality benchmark, Cultural Awareness Reward Modeling Benchmark (CARB), which covers 10 distinct cultures and typologically diverse languages across four domains. This benchmark is a valuable contribution for evaluating the cultural awareness of reward models.\n2. The authors conduct comprehensive experiments using a range of models and datasets, enabling a thorough comparison of different models’ capacities for cultural awareness.\n3. The paper proposes Think-as-Locals, a method that improves reward model performance by over 10% while reducing correlations with surface-level features."}, "weaknesses": {"value": "1. The paper attempts to cover too many aspects within a single work, without providing sufficient description or analysis for each section. For instance, Table 2 compares the performance of many models, but the accompanying explanation is brief and lacks analysis of why certain models outperform others.\n2. The analysis in Section 4 appears redundant and somewhat unrelated to the main narrative of the paper; it could be moved to the Appendix.\n3. Think-as-Locals is the core method proposed in this paper, yet the authors do not provide enough justification or explanation for its design choices."}, "questions": {"value": "1. Line 159 – What are the reference embeddings referring to?\n2. What is the definition of Classes 3, 4, and 5 in Figure 2?\n3. Table 3 is somewhat confusing. Why does it include comparisons with many models that were not trained using the proposed method, instead of focusing only on models trained by the authors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JjaTvLFy1P", "forum": "WhSzqsMhfZ", "replyto": "WhSzqsMhfZ", "signatures": ["ICLR.cc/2026/Conference/Submission14774/Reviewer_wu8s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14774/Reviewer_wu8s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970099299, "cdate": 1761970099299, "tmdate": 1762925127018, "mdate": 1762925127018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}