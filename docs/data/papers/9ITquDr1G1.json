{"id": "9ITquDr1G1", "number": 25606, "cdate": 1758369445645, "mdate": 1759896713690, "content": {"title": "Contrastive-Aligned Knowledge Distillation for Collaborative Code Completion via Multi-Agent Reinforcement Learning", "abstract": "We introduce a novel multi-agent reinforcement learning (MARL) framework for code completion in a collaborative manner, and address the important issue for successful collaboration in code completion: balancing semantic alignment and specialized expertise among the agents. The proposed method incorporates Contrastive Alignment Module (CAM) and Distilled Knowledge Transfer (DKT) mechanism, which allows agents to share coherent representations without losing domain-specific knowledge. CAM embeddings between agents might be aligned through a contrastive learning goal and would create a coordinate measurement of the space in which all embeddings agree (without homogenizing individual capabilities), but DKT would dynamically distil some knowledge from a high-performing teacher agent to others using a regularized KL-divergence goal.", "tldr": "", "keywords": ["Contrastive-Aligned Knowledge"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eeafd4319a9ce1ad4fb803ca5798c5d18b4eaab9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper claims to use Multia-agent Reinforcement Learning to improve code completion. The method combines Contrastive Alignment module and Distilled Knowledge Transfer so that embeddings between agents are aligned"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper is really bad, it's hard to find a strength."}, "weaknesses": {"value": "The paper is very badly written, from careless typos that show the paper was barely revised (e.g. \"spezialised knowlage\"; \"speciz Elvis\"), to a lack of cohesive structure between the different sections and terrible use of equations (virtually the same equation is rewritten in slightly different way in (2), (4), (5), (10), (14), (18)). When I finished reading the paper I sincerely couldn't have explained to someone else what is the main proposal of the paper and how it was implemented in the experimental evaluation.\n\nI am not even convinced the authors know what Multiagent Reinforcement Learning is (which is what is claimed to be the main contribution of the paper). The background on that simply says that in a MARL problem each agent has its own actions - with absolutely no description of the other elements of a Multiagent MDP. Most importantly, there isn't a single sentence in the whole paper dedicated to explaining what is the state-action-reward space for the code completion domain, which directly means that the reader can't have a minimal idea of how the baselines were implemented.\n\nThe paper has to be entirely rewritten - and I am not even convinced it makes any sense at all to use MARL in this domain. Whatever the authors did in their experiment, it is not MARL."}, "questions": {"value": "No Question, paper is a clear reject."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ii0YYZAgc3", "forum": "9ITquDr1G1", "replyto": "9ITquDr1G1", "signatures": ["ICLR.cc/2026/Conference/Submission25606/Reviewer_Vorc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25606/Reviewer_Vorc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760761320712, "cdate": 1760761320712, "tmdate": 1762943491946, "mdate": 1762943491946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a multi-agent reinforcement learning (MARL) framework for code completion that combines:\n\n\t1.\tContrastive Alignment Module (CAM) to align latent representations across agents\n\n\t2.\tDistilled Knowledge Transfer (DKT) for dynamic “teacher → student” imitation\n\n\t3.\tA routing scheduler using soft attention over agents\n\n\t4.\tMoCo queue + Sparse MoE experts inside agents for specialization\n\nClaims: improved completion metrics (EM/ES/SE) and stronger “specialization retention” compared to single-agent and naive multi-agent baselines (Tables 1–3)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Acknowledges the alignment vs. specialization dilemma in collaborative code modeling.\n\n2. The idea of using contrastive similarity to guide teacher selection is reasonable in theory (Eq.9).\n\n3. Ablation tables show some component contributions, albeit marginal.1."}, "weaknesses": {"value": "1. All components are known and simply combined: multi-agent code modeling (MAPoRL: Park et al., 2025; AgentCoder: Huang et al., 2023), contrastive alignment (MoCo-Code, SimCLR-Code), collaborative KD (CKD: Zhu et al., 2025; Yang et al., 2023), MoE for specialization (CodeT5+ family). The paper does not introduce a new algorithmic principle.\n\n2. Eq.(5) is SimCLR-style contrastive loss with changed notation. Eq.(8) is standard KD with L2 regularization. Eq.(15) claims gradient synergy without theoretical justification or analysis of conflicting gradients.\n\n3. CodeGPT and MARL-Joint are not competitive code completion baselines in 2025/2026. Strong baselines such as CodeT5+, StarCoder, StarCoder2, and DeepSeek-Coder are missing. Claims of state-of-the-art performance are unsubstantiated.\n\n4. The Specialization Retention metric (Eq.19) is ad-hoc and does not assess disentanglement, cross-agent interference, or negative transfer.\n\n5. The environment structure, reward horizon, and credit assignment in MARL are not clearly defined. The scheduler could likely be replaced by supervised routing with no RL.\n\n6. Code completion requires compilation, runtime correctness, and human evaluation. Token-level exact match is insufficient for semantic code quality.\n\n7. The authors acknowledge 23\\% slower inference and quadratic scaling in contrastive components. Without practical feasibility, the method is unlikely to see adoption."}, "questions": {"value": "1.\tHow is this a MARL setting if each agent predicts code independently and no joint action sequences are evaluated?\n\n\t2.\tWhy hide comparisons with CodeT5+/StarCoder that would outperform your 125M agents with zero collaboration?\n\n\t3.\tWhere are human evals, compile success, static analysis correctness? Token-level edit distance ≠ sanity.\n\n\t4.\tHow do you guarantee expertise preservation, rather than embedding homogenization?\n\n\t5.\tHow is teacher selection (Eq.9) not reinforcing a single dominant policy, leading to collapse?\n\n\t6.\tWhy is Eq.(15) even true? Show gradient alignment or stop making causal claims.\n\n\t7.\tWhere is the analysis of conflicts when CAM and DKT gradients oppose each other?\n\n\t8.\tHow does performance scale >4 agents? Complexity grows quadratically in contrastive pairs — is that workable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5apx6M4MbI", "forum": "9ITquDr1G1", "replyto": "9ITquDr1G1", "signatures": ["ICLR.cc/2026/Conference/Submission25606/Reviewer_hxQN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25606/Reviewer_hxQN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560123065, "cdate": 1761560123065, "tmdate": 1762943491721, "mdate": 1762943491721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a MARL framework for collaborative code completion, introducing two core components:\n1. A Contrastive Alignment Module (CAM) to align semantic embeddings across agents while maintaining their specialized knowledge.\n2. A Distilled Knowledge Transfer (DKT) mechanism for dynamic teacher-student knowledge sharing based on task performance and embedding similarity.\n\nThe model further integrates a Cross-Agent Scheduler, Momentum Contrast (MoCo), and a Sparse Mixture-of-Experts (MoE) architecture to enhance stability and specialization. Experiments on PY150, JavaCorpus, and MultiLangBench*datasets show significant improvements in code completion accuracy and specialization retention over baselines such as CodeGPT, MARL-Joint, and MoCo-Code."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The topic (collaborative code completion) is timely and relevant to MARL and software engineering.  \n- The authors conduct experiments across several datasets and include ablations."}, "weaknesses": {"value": "1. My main concern is that this paper lack of genuine novelty. The method combines existing paradigms (contrastive learning + KD + MARL) without substantive conceptual innovation for me.  \n2. No formal analysis supports why contrastive alignment and distillation jointly improve collaboration.  \n4. The writing is poor."}, "questions": {"value": "1. How exactly does the CAM–DKT interaction differ from prior contrastive distillation methods (e.g., CKD)?  \n2. Have the authors analyzed training stability and convergence beyond qualitative curves?  \n3. Can the system scale beyond four agents without exponential computational cost?  \n4. Why is there no comparison with recent large-scale collaborative LLM frameworks (e.g., MAPoRL 2025)?  \n5. What ablation evidence directly isolates the contribution of bidirectional CAM–DKT coupling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "geryzA7HPU", "forum": "9ITquDr1G1", "replyto": "9ITquDr1G1", "signatures": ["ICLR.cc/2026/Conference/Submission25606/Reviewer_3dJF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25606/Reviewer_3dJF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743710238, "cdate": 1761743710238, "tmdate": 1762943491480, "mdate": 1762943491480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-agent RL (MARL) algorithm for code completion. They develop a novel algorithm where there are 2 key novel components: a contrastive alignment module which is uses to align the internal representations of different agents and a distilled knowledge transfer module that helps the agents share knowledge, which I believe has the same motivation as papers like Distral [1], which aim to distill knowledge between different components of an algorithm. They show that this enables better code completion that baseline methods, ablate the various components, and show a figure that they argue shows that collaboration is improving.\n\n[1] Distral: Robust Multitask Reinforcement Learning"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The results are good and the idea is sensible. Distral was a very powerful Distillation and transfer learning algorithm and its not hard to imagine that ideas like this would improve MARL algorithms for code completion. \n\nIt's also good that you compare against many baselines."}, "weaknesses": {"value": "there are many types. e.g.\n- \"A dynamic knwoledge\"\n- \"significantly rodeomoter to both singleagent and naive multi-agent baseline\". \"rodeomoter\"?\n\nthe related work needs work for explaining how this contribution fits within previous context. They describe prior work by they don't explain *what* is different about their algorithm of method\n\nthey don't really motivate using a MARL algorithm instead of a single agent. They only say that this will enable agents with specialized knowledge but its really not obvious that this will be better. I think an *experiment* that shows this is critical for this argument.\n\nmany things were unclear:\n- what is \"semantically similar contexts across agent\"?\n- why does a residual adaptive $\\Delta_i$ preserve domain-specific features? why do you want this?\n- I don't have a sense of what the algorithm does *overall* and why. I don't quite understand why you're distilling knowledge transfer?\n- you introduce \"CROSS-AGENT SCHEDULER WITH DYNAMIC COLLABORATION\" in section 4.3 for the first time and I have no idea why you're introducing this and how its related to your algorithm. likewise for 4.4 or 4.5\n\nFor the ablations, and for all numbers, what are the errors across multiple seeds? The values are so close that it's hard to compare.\n\nI don't find Figure 2 compelling at all that this shows collaborative improvement. Many MARL papers, e.g. [1], have shown that higher task performance can with less collaboration.\n\nOverall, this paper was challenging to follow. I don't know why you want a MARL system and what you want to distill some knowledge but share other. And I don't have a clear sense of the algorithm. I think you really need an algorithm or a figure that places all the pieces together so that one can understand how they relate to each other\n\n[1] Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination"}, "questions": {"value": "why did you compare against CodeGPT instead of GPT5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dN4Gg3eD0u", "forum": "9ITquDr1G1", "replyto": "9ITquDr1G1", "signatures": ["ICLR.cc/2026/Conference/Submission25606/Reviewer_v45Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25606/Reviewer_v45Z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762899635876, "cdate": 1762899635876, "tmdate": 1762943491270, "mdate": 1762943491270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}