{"id": "qYcZVezPW7", "number": 3677, "cdate": 1757495890782, "mdate": 1759898075570, "content": {"title": "Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models", "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable performance across a variety of real-world tasks. However, existing VLMs typically process visual information by serializing images, a method that diverges significantly from the parallel nature of human vision. Moreover, their opaque internal mechanisms hinder both deeper understanding and architectural innovation. Inspired by the dual-stream hypothesis of human vision, which distinguishes the \"what\" and \"where\" pathways, we deconstruct the visual processing in VLMs into object recognition and spatial perception for separate study. For object recognition, we convert images into text token maps and find that the model's perception of image content unfolds as a two-stage process from shallow to deep layers, beginning with attribute recognition and culminating in semantic disambiguation. For spatial perception, we theoretically derive and empirically verify the geometric structure underlying the positional representation in VLMs. Based on these findings, we introduce an instruction-agnostic token compression algorithm based on a plug-and-play visual decoder to improve decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning. Through rigorous experiments, our work validates these analyses, offering a deeper understanding of VLM internals and providing clear principles for designing more capable future architectures.", "tldr": "", "keywords": ["multimodal language models", "interpretability", "spatial reasoning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea237c7d26e286d3c2844330d158bec8ba409016.pdf", "supplementary_material": "/attachment/6b9a18981651aa202853270fd9c4978898bfb9ea.zip"}, "replies": [{"content": {"summary": {"value": "The paper examines how vision-language models (VLMs) encode and reason with visual information within a one-dimensional token sequence. It addresses two core questions: how VLMs associate positionally discontinuous tokens representing the same object to identify its category (\"what\"), and how they infer 2D spatial relationships between objects (\"where\"). The authors use logit lens and visualizations to uncover a two-stage visual processing pattern. They theoretically and empirically analyze how RoPE-based encoders represent spatial relationships, revealing their underlying geometric structure. Building on these insights, the paper proposes two practical methods: a token compression approach that shortens image sequences with minimal performance loss, and a RoPE scaling technique that enhances spatial reasoning while maintaining overall model performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* The analysis section is thorough and compelling, offering valuable insights into how visual encoders and visual tokens are processed within VLMs. This contributes to a deeper understanding of the inner mechanisms driving object and spatial reasoning.\n\n* The proposed applications are well grounded in the paper’s analytical findings and serve as effective validations of the authors’ observations, demonstrating both conceptual soundness and practical relevance.\n\n* The theoretical and empirical analysis of RoPE-based spatial processing highlights a real limitation in the current VLM architecture. By identifying and addressing this limitation, the work provides a meaningful direction for future research and improvements in spatial reasoning within VLMs."}, "weaknesses": {"value": "__Paper structure:__\n\nThe paper’s structure could be improved. Much of the core analysis is deferred to the appendix, making it difficult to follow the main findings from the main text alone. At the same time, several sections—particularly the Related Work—contain substantial repetition of content already covered in the Introduction. Streamlining the exposition and integrating key analyses into the main body would make the paper more cohesive and readable.\n\n__Comparison to prior work__:\n\nThe paper lacks a sufficient comparison to other recent token compression methods, such as [1]. Since prior methods already identify salient tokens and achieve significant token reduction, it is unclear whether (1) the proposed method identifies similar or distinct subsets of tokens, and (2) whether it provides any advantage in compression efficiency or downstream task performance. A quantitative or qualitative comparison with these approaches would strengthen the paper’s claims.\n\n__Training-free RoPE scaling experiment:__\n\nIn the training-free RoPE scaling experiment, the authors report the best results achieved by tuning the hyperparameters \\alpha and p. It is unclear whether these parameters were optimized using the test set. If so, this constitutes data leakage and invalidates the experiment. The authors should clarify the procedure used to select these parameters (e.g., validation split, held-out set) and ensure that the evaluation remains unbiased.\n\n\n[1] Kaduri et al., What's in the Image? A Deep-Dive into the Vision of Vision Language Models, CVPR 2025"}, "questions": {"value": "My only requests/questions on top of the previously mentioned points are about newer architectures and higher-dimensional RoPEs:\n\n* Will this analysis transfer to models that use DeepStack [2], like Qwen3? \n\n* Can you say something similar about 4D RoPEs in videos (i.e., improving the \"where\" pathway for video frames, or extending it to a \"when\" pathway)?\n\n[2] Meng et al., DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs, NeurIPS 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SDC9sjJGhK", "forum": "qYcZVezPW7", "replyto": "qYcZVezPW7", "signatures": ["ICLR.cc/2026/Conference/Submission3677/Reviewer_fdDb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3677/Reviewer_fdDb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761433299004, "cdate": 1761433299004, "tmdate": 1762916914149, "mdate": 1762916914149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how Vision-Language Models (VLMs) process visual information by decomposing their mechanisms into object recognition and spatial perception, inspired by the dual-stream hypothesis of human vision. The study reveals that VLMs recognize image content in two stages—progressing from low-level attribute detection to high-level semantic understanding—and uncovers the geometric structure underlying their positional  representation. Building on these insights, the authors propose a token compression algorithm and a RoPE scaling technique to enhance decoding efficiency and spatial reasoning. Overall, the work deepens understanding of VLM internals and offers principled guidance for future model design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper conducted comprehensive theoretical and experimental analyses. \n* The paper connects theoretical analysis with applications. Based on findings on the visual processing and spatial perception characteristics of VLMs, this paper introduces an instruction-agnostic token compression method, which reduces image sequence length during decoding, and RoPE scaling, which enhances the spatial reasoning capabilities of VLMs."}, "weaknesses": {"value": "* The paper does not quantitatively compare the proposed token compression method with other existing compression techniques, such as [1] and [2], in terms of inference efficiency and training cost. The proposed method also requires additional training of the visual decoder, which may limit its application scenarios.\n* In Table 1, methods 2 and 3 exhibit a more significant performance decline on TextVQA than on other tasks. What accounts for this difference? \n* There are several instances in the paper where LaTeX quotation marks are incorrectly formatted. For example, ”In which direction is A relative to B?” in line 316.\n\n[1] Lin, Zhihang, et al. \"Boosting multimodal large language models with visual tokens withdrawal for rapid inference.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 5. 2025.\n\n[2] Zhu, Yuke, et al. \"Focusllava: A coarse-to-fine approach for efficient and effective visual token compression.\" arXiv preprint arXiv:2411.14228 (2024)."}, "questions": {"value": "Please refer to the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h1g8jkAETs", "forum": "qYcZVezPW7", "replyto": "qYcZVezPW7", "signatures": ["ICLR.cc/2026/Conference/Submission3677/Reviewer_RfD1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3677/Reviewer_RfD1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903008488, "cdate": 1761903008488, "tmdate": 1762916913858, "mdate": 1762916913858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a deep interpretability analysis of Vision-Language Models (VLMs) through the lens of the brain’s dual-stream hypothesis, examining how models achieve both object recognition (“what”) and spatial perception (“where”). For the “what” pathway, logit lens decoding reveals a two-stage process in token representations: early detection of low-level features and later semantic refinement. For the “where” pathway, theoretical and empirical investigations of 2D RoPE demonstrate that spatial relations are geometrically encoded in attention maps. Leveraging these findings, the authors introduce (1) an instruction-agnostic token compression algorithm using run-length encoding, and (2) RoPE scaling to enhance positional signals and spatial reasoning. Experiments on LLaVA and Qwen2.5-VL validate both the insights and practical effectiveness of the proposed methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers a systematic and mechanistic analysis of visual processing in VLMs. The application of the logit lens to visual tokens is particularly effective, converting otherwise opaque embeddings into interpretable token maps and enabling fine-grained examination of object recognition dynamics.\n\n2. The paper provides a rigorous theoretical analysis of how 2D RoPE encodes spatial relationships, with empirical validation via PCA, object erasure, and intervention experiments. The findings on collinearity of “left/right” and orthogonality of “left/behind” directions convincingly illustrate the geometric structure in the learned representations.\n\n3. The paper features extensive ablation studies, comprehensive training protocols, and thoughtful handling of edge cases, such as top-1 versus top-2 token filtering for compression. Evaluation across multiple models (LLaVA, Qwen2.5-VL, InternVL) and datasets (GQA, POPE, What’s Up) further enhances the generalizability of the findings."}, "weaknesses": {"value": "1. The study's focus on four basic directional relationships (“left,” “right,” “front,” “behind”) excludes more complex spatial configurations (e.g., “top-left,” “surrounding,” “partially occluded”), thereby limiting the scope of the “where” pathway analysis. \n\n2. The logit lens approach assumes that visual representations can be linearly decoded into semantic tokens using the language model’s unembedding matrix. However, this assumption may not hold for earlier ViT layers, whose features are not yet aligned with the final modality connector, as noted in Appendix A. Employing per-layer projection heads could offer a more robust alternative.\n\n3. Although the compression method is instruction-agnostic, it necessitates training a visual decoder through knowledge distillation, which introduces additional engineering overhead. Furthermore, the assertion that \"random selection outperforms mean pooling\" is made without theoretical justification, indicating a need for more thorough investigation in future work."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cmHPXCsUKh", "forum": "qYcZVezPW7", "replyto": "qYcZVezPW7", "signatures": ["ICLR.cc/2026/Conference/Submission3677/Reviewer_ywbf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3677/Reviewer_ywbf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916254773, "cdate": 1761916254773, "tmdate": 1762916913661, "mdate": 1762916913661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "First, they investigate _how_ the vision encoder of the VLM encodes what an object is, and they do so by skipping ViT layers and using logit lens on the LLM to see what the LLM processes. They show that early layers encode high-level attributes and later layers encode specific repesentations of the objects. Then, they study at how VLMs perceive spatial relation through a theoretical and empirical analysis. Lastly, they propose two applications based on their findings: token compression and enhancing spatial reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* They use logit lens on the LLM to study how the Vision Encoder works: this is a particularly insightful idea that I had not previously considered. And it seems to work extremely well, especially with further validation from POPE polling.\n* It is timely to study how VLMs do spatial perception as they are becoming better at it, and the paper does a sound, comprehensive study of it. While the theoretical study is limited to only two objects, I find this to be a necessary constraint."}, "weaknesses": {"value": "* The segmentation map keywords seem to be chosen by the authors. This may mean that the results could be due to human-confirmation bias here, if the authors first saw the logit lens results then came up with the keywords.\n* The applications and results don't seem very strong, but nevertheless serve as further empirical validation of their findings."}, "questions": {"value": "* What was the method used to come up with the keyword set? If it was what I mentioned, is there some more principled way to do it?\n* For the token compression application, the tokens embeddings only become text-like in the later layers of a LM. I assume you are compressing the tokens at the input to the LM (i.e. replacing or truncating the visual embeddings.) How are you compressing the tokens, or what are you replacing the visual embeddings with? I understand it to be text tokens from the logit lens. If so, have you tried merely replacing all visual embeddings with logit lens tokens? (I'm not sure if this is what \"original decoding\" refers to, or whether original decoding is referring to merely using the original visual embeddings)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xDP8Ii7lM9", "forum": "qYcZVezPW7", "replyto": "qYcZVezPW7", "signatures": ["ICLR.cc/2026/Conference/Submission3677/Reviewer_fYWu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3677/Reviewer_fYWu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998462264, "cdate": 1761998462264, "tmdate": 1762916913492, "mdate": 1762916913492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}