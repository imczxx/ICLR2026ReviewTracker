{"id": "60VgwdzxDM", "number": 8548, "cdate": 1758090650788, "mdate": 1759897777401, "content": {"title": "One-Step Flow Q-Learning: Addressing the Diffusion Policy Bottleneck in Offline Reinforcement Learning", "abstract": "Diffusion Q-Learning (DQL) has established diffusion policies as a high-performing paradigm for offline reinforcement learning, but its reliance on multi-step denoising for action generation renders both training and inference slow and fragile. Existing efforts to accelerate DQL toward one-step denoising typically rely on auxiliary modules or policy distillation, sacrificing either simplicity or performance. It remains unclear whether a one-step policy can be trained directly without such trade-offs. To this end, we introduce One-Step Flow Q-Learning (OFQL), a novel framework that enables effective one-step action generation during both training and inference, without auxiliary modules or distillation. OFQL reformulates the DQL policy within the Flow Matching (FM) paradigm but departs from conventional FM by learning an average velocity field that directly supports accurate one-step action generation. This design removes the need for multi-step denoising and backpropagation-through-time updates, resulting in substantially faster and more robust learning. Extensive experiments on the D4RL benchmark show that OFQL, despite generating actions in a single step, not only significantly reduces computation during both training and inference but also outperforms multi-step DQL by a large margin. Furthermore, OFQL surpasses all other baselines, achieving state-of-the-art performance in D4RL.", "tldr": "We introduce OFQL, a robust and efficient diffusion-based offline RL method that enables one-step action generation during training and inference via flow matching with average velocity modeling.", "keywords": ["Reinforcement learning", "Diffusion Model", "Flow Matching", "Offline Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/449443003a7b2a0783f1d48610c75de8964f08b1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To address the slow multi-step denoising and unstable optimization inherent in diffusion-based policies, this paper introduces One-Step Flow Q-Learning (OFQL). OFQL reformulates the diffusion denoising process within the Flow Matching (FM) framework and learns an average velocity field that enables direct one-step action generation. Experiments across diverse D4RL tasks demonstrate that OFQL achieves the highest average normalized scores among all compared methods. Moreover, its one-step sampling design substantially improves both training and inference efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "OFQL is an efficient reinforcement learning algorithm that introduces average velocity fields within the Flow Matching framework, enabling it to model complex policy distributions without relying on distillation procedures or auxiliary networks. The method offers a unified training–inference pipeline, using the same one-step model consistently in both phases. Empirically, OFQL outperforms DQL and other strong baselines in terms of both policy performance and computational efficiency."}, "weaknesses": {"value": "1.\tOFQL relies on the MeanFlow Identity to enable one-step sampling for the learned policy. However, the Jacobian–vector product computation in Eq. (11) may become computationally demanding for large-scale models.\n\n2.\tAs acknowledged in the paper’s limitations, it remains unclear whether OFQL can scale to high-dimensional action spaces (e.g., humanoid control or vision-based RL). Moreover, the stability of the proposed one-step policy under non-stationary or online settings has not been investigated.\n\n3.\tThe paper provides no formal analysis establishing the expressive equivalence between the average-velocity one-step formulation and traditional multi-step diffusion policies."}, "questions": {"value": "1.\tDoes learning an average velocity field constrain the representational power compared to DDPM’s full reverse process?\n\n2.\tHow sensitive is OFQL to flow ratio and time-sampling distribution?\n\n3.\tDoes one-step flow matching better handle out-of-distribution states than diffusion-based DQL?\n\n4.\tHow exactly does the Q-gradient interact with flow learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ldoQrzIobr", "forum": "60VgwdzxDM", "replyto": "60VgwdzxDM", "signatures": ["ICLR.cc/2026/Conference/Submission8548/Reviewer_43m1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8548/Reviewer_43m1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866679873, "cdate": 1761866679873, "tmdate": 1762920401632, "mdate": 1762920401632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Concerns on Experimental Fairness and Potential Bias in Baseline (FQL) Comparison"}, "comment": {"value": "1. **Experimental reliability.** The experimental results reported in this paper appear unreliable. OFQL is evaluated on several environments that are not part of the official FQL testbed, which undermines the fairness of the comparison. The authors should run on `ogbench` to ensure parity. In addition, they should release all hyperparameters and, ideally, the Weights & Biases (wandb) logs to substantiate the findings. For example, on `HalfCheetah-Medium-Expert`, with $\\alpha \\in {3, 10, 100, 200}$, we readily obtain a normalized score of $98.6 \\pm 0.8$, with similar trends on other tasks. We therefore recommend either (i) using the same evaluation suite as FQL, or (ii) publicly disclosing key hyperparameters (e.g., `alpha`) and logs to clarify the source of performance gains.\n\n2. **Positioning relative to FQL.** This work is closely related to FQL rather than DQL, which is also a one-step flow Q-learning method; therefore, the title “Addressing the Diffusion Policy Bottleneck” may be misleading, since FQL likewise employs a one-step Q-learning formulation. The paper should provide a deeper comparison with FQL—e.g., explaining why a MeanFlow-based, inherently one-step approach would be preferable to FQL’s distillation-based one-step design.\n\nThis review is entirely objective, carries no conflict of interest, and is made solely in the interest of promoting a healthier and more constructive development of the academic community."}}, "id": "EJ13I8kDfa", "forum": "60VgwdzxDM", "replyto": "60VgwdzxDM", "signatures": ["~Flow_King1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Flow_King1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8548/-/Public_Comment", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762965315995, "cdate": 1762965315995, "tmdate": 1762966558509, "mdate": 1762966558509, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes One-Step Flow-Q-Learning, an offline RL algorithm that enables one-step action generation during both training and inference. The method is closely related to DQL but leverages the average velocity parametrization following MeanFlow. The action sampling in this parametrization can be done in a single step, which reduces inference cost and avoids the huge computation and memory costs of backpropagation through the multi-step sampling chain. Experiments on the D4RL benchmark show that OFQL surpasses all other included baselines in overall performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method shows empirical advantages in policy performance, training speed and inference time. \n2. The paper is easy to follow."}, "weaknesses": {"value": "1. The proposed method lacks novelty. The only main difference between the proposed method and DQL is replacing the diffusion loss in actor training with a MeanFlow loss.\n2. The experiments are not adequate. Only results on state-based D4RL tasks are included, and no visual observation task results are reported.\n3. The argument in Lines 262-264 is not clear. Flow matching cannot \"in principle, enable one-step generation\", as the sampling trajectory is straight only when the target distribution is a delta distribution or when rectification or similar techniques have been used. The following sentences in this paragraph are accurate."}, "questions": {"value": "1. How many diffusion steps are used for the multi-step diffusion policy baselines? Is the number aligned with the original papers?\n2. Can the proposed method be extended to visual observation tasks? Are there any challenges for the method in high-dimensional input scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mfxPpPDIci", "forum": "60VgwdzxDM", "replyto": "60VgwdzxDM", "signatures": ["ICLR.cc/2026/Conference/Submission8548/Reviewer_QtuL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8548/Reviewer_QtuL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966047663, "cdate": 1761966047663, "tmdate": 1762920401105, "mdate": 1762920401105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **One-Step Flow Q-Learning (OFQL)**, a novel framework for offline reinforcement learning that reformulates Diffusion Q-Learning (DQL) within the **Flow Matching (FM)** paradigm. By learning an **average velocity field** instead of a marginal one, OFQL enables accurate **one-step action generation** during both training and inference—eliminating the need for multi-step denoising and recursive backpropagation. This design substantially improves training and inference efficiency while maintaining, and even improving, performance. The authors demonstrate strong results across D4RL benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Clear conceptual advancement:** Reformulating DQL under the flow-matching framework and introducing an average velocity field is a novel and elegant idea that directly addresses the core inefficiency of multi-step denoising.\n* **Simplicity and effectiveness:** Unlike prior one-step approaches that depend on auxiliary modules or policy distillation, OFQL remains conceptually clean while achieving superior results.\n* **Strong empirical results:** The method outperforms DQL and other diffusion-based baselines by a significant margin on D4RL, demonstrating both **efficiency** and **robustness**.\n* **Illustrative toy example:** The toy experiment effectively clarifies the intuition behind the average velocity field and supports the main claim.\n* **Readable and well-organized:** The paper is well-written, clearly structured, and easy to follow even for readers not deeply familiar with flow-matching methods."}, "weaknesses": {"value": "* The theoretical justification for why learning an **average velocity field** leads to better one-step performance could be elaborated further. Currently, the paper provides an intuitive explanation but lacks a deeper analytical connection to diffusion dynamics."}, "questions": {"value": "1. Could the authors provide a more formal justification for why **average velocity learning** preserves accuracy in one-step action generation?\n2. Are there scenarios (e.g., highly multimodal action distributions) where the **average velocity** assumption might underperform?\n\nTypo: citation in line 151"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZYqzH3nxrW", "forum": "60VgwdzxDM", "replyto": "60VgwdzxDM", "signatures": ["ICLR.cc/2026/Conference/Submission8548/Reviewer_8FUi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8548/Reviewer_8FUi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135808218, "cdate": 1762135808218, "tmdate": 1762920400455, "mdate": 1762920400455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To overcome the limitations of DQL, the paper proposes replacing the multi-step denoising policy used in training and inference with a one-step denoising policy. Unlike other one-step approaches that require an auxiliary teacher network for distillation, the paper adopts a mean-flow policy that directly approximates the denoising process. The proposed method demonstrates strong empirical performance and improved efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method is simple, clear, and effective. By replacing only the diffusion policy component with the mean-flow policy, the approach achieves both higher sampling efficiency and competitive performance. The toy example nicely illustrates the advantage of reparameterizing from $v$ to $u$, providing a clearer intuition for the underlying mechanism."}, "weaknesses": {"value": "Given that mean-flow generative modeling has already shown strong one-step FID results on image generation tasks, it would be valuable to see this approach applied to more complex environments beyond D4RL, such as robotic control or high-dimensional decision-making settings."}, "questions": {"value": "The model performs worse on the Kitchen and AntMaze-Large-Diverse tasks, which are relatively more challenging within the D4RL benchmark. Do the authors have any insights into these results?\nCould it be that the mean-flow policy limits exploration during training, leading to reduced performance on tasks requiring greater stochasticity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IM1RfFoaSK", "forum": "60VgwdzxDM", "replyto": "60VgwdzxDM", "signatures": ["ICLR.cc/2026/Conference/Submission8548/Reviewer_opYF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8548/Reviewer_opYF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762165272067, "cdate": 1762165272067, "tmdate": 1762920399942, "mdate": 1762920399942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}