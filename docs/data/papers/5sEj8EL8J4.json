{"id": "5sEj8EL8J4", "number": 25586, "cdate": 1758369304944, "mdate": 1759896714212, "content": {"title": "Cross-Modal Syntax-NL Attention for Multi-Agent Reinforcement Learning in Collaborative Coding", "abstract": "We suggest a new communication protocol for multi-agent reinforcement learning (MARL) in collaborative coding where agents have to coordinate in coordinate (both structured code syntax and natural language (NL) messages). Conventional ways to treat these modalities separately, the result was suboptimal alignment between the communicational and the code semantic. The proposed method introduces a cross-modal attention framework that is able to dynamically bridge abstract syntax trees (ASTs) of code and NL messages in a jointly learned embedding space. A graph neural net encodes artistic of syntactic elements of code while a pretrained Transformer processes NL messages which are then aligned in the direction of weakly supervised contrastive learning making use of implicit training sign for execution outcome of code to guide the alignment without requirement of manual annotation. Also, the framework uses syntax-aware attention gates to select which message tokens are relevant to particular code nodes, which can result in more precise coordination during collaborative tasks.", "tldr": "", "keywords": ["Syntax-NL Attention"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a71ac8f7fa5f22c51930fd557350c8b8411dd47.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "# Problem:\n\nCollaborative coding involves multiple agents cooperating to align their individual code contributions towards a common goal, via NL communication. This paper addresses the research question of ‘How to effectively coordinate agents in collaborative coding task?’\n\n# Contributions:\n\nThe paper proposes a new communication protocol for MARL in the context of collaborative coding. This communication protocol is cross-modal, between the code modality and the NL communication modality, for each collaborating agent.\n\nThe paper proposes empirical results that has their proposed approach perform better than baseline approaches.\n\nMoreover, an ablation study of the different components of the approach is presented."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "## Quality:\n\nSQ1: The ablation study on the framework components is insightful."}, "weaknesses": {"value": "## Quality:\n\nWQ1: While Figure 3 provides valuable insights, it is difficult to know what is the significance of the data being reported. Moreover, it is unclear how the data are obtained: is it aggregated over multiple trajectories or only on one trajectory? How does the x-axis relate to a trajectory, if at all?\n\nIn order to provide more insights about the significance of the data presented, I would like to invite the author to consider comparing this heatmap with the heatmap obtained in the context of relevantly-ablated runs, in order to highlight how the relevant components of the framework impact this attention heatmap, possibly?\n\nWQ2: Limited analysis of failure cases: The paper does not discuss when or why the method fails. I think that it would increase the quality of the paper if some valuable insights into the method's limitations could be provided and discussed.\n\n## Clarity:\n\nWC1: I think it would ease the reading experience further if some visual examples from the datasets and benchmarks used could be presented in the main paper or appendix, in order for the reader to get a better sense of what kind of collaborative coding task is being targeted here.\n\nWC2: I could not figure out several implementation details, such as:\n\n1. How exactly is the \"shared codebase\" structured in the multi-agent environment? What are the observations of each agents, at each timestep? What are the actions of each agents at each timestep?\n    \n2. What constitutes a \"training step\" in the 500K step training regime reported on in Figure 1? Maybe an algorithm of the whole approach would help clarifying this.\n    \n\n## Originality:\n\nSO1: The proposed approach is only incremental novelty, given the lack of insights into the failure cases. \n\n## Significance:\n\nWS1: None of the results in the paper report statistics that could enable statistical significance evaluation. I would advise the authors to perform experiments on randomised seeds (>=5) and report error of the mean  as much as possible, starting with Figure 1 and 2 (e.g. mean=line+std.error=shaded area) and Tables 1 and 2.\n\nIndeed, as it stands, it is unclear whether, for instance, the current results are not the consequence of a lucky, unintentional cherry-picking of the right (default) random seed.\n\nWS2: Moreover, running some statistical significance test on e.g. Table 1 and Table 2 would strengthen the claims made."}, "questions": {"value": "Please see weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "48PKNGxg42", "forum": "5sEj8EL8J4", "replyto": "5sEj8EL8J4", "signatures": ["ICLR.cc/2026/Conference/Submission25586/Reviewer_i1oQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25586/Reviewer_i1oQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942060451, "cdate": 1761942060451, "tmdate": 1762943486200, "mdate": 1762943486200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-agent reinforcement learning (MARL) framework to improve coordination performance in collaborative coding tasks. The core idea involves using a cross-modal attention mechanism to align representations of abstract syntax trees (ASTs) and natural language. The framework also employs a Graph Neural Network (GNN) and weakly supervised contrastive learning to reduce the reliance on manual annotations. The authors report that their method outperforms existing baselines in both task performance and the quality of the representation alignment."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The primary strength of this paper is its novel approach. The idea of aligning AST and natural language representations within a MARL framework to improve agent coordination is a new and interesting direction for this problem space."}, "weaknesses": {"value": "The paper in its current state, is not ready for publication due to significant issues with writing clarity. The poor presentation makes it extremely difficult to evaluate the technical soundness and novelty of the contributions.\n1. The quality of the writing is a major obstacle. The paper suffers from numerous grammatical errors, typos, and awkward sentence constructions that severely impact comprehension. For example (from page 1): \"coordinate in coordinate\" (line 016) and \"the harmful effect of such work is\" (line 052). The entire manuscript requires thorough proofreading and editing.\n\n2. The model's architecture and the formal problem setting are not clearly defined. It is difficult for the reader to understand the precise mechanisms of the proposed framework, how the agents interact, or how the components (GNN, attention, MARL) are integrated.\n\n3. The description of the experimental setup is too brief to allow for reproduction. Key details regarding the dataset, implementation specifics, and hyperparameter tuning are missing.\n\n4. The paper's central thesis, that aligning AST and natural language representations improves coordination, is underdeveloped. The authors need to provide a clearer theoretical justification for why this alignment should lead to better multi-agent performance. Furthermore, the metrics used to \"verify\" the alignment quality itself are not well-explained, making it hard to distinguish this from the downstream task performance."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QZiza4g8sJ", "forum": "5sEj8EL8J4", "replyto": "5sEj8EL8J4", "signatures": ["ICLR.cc/2026/Conference/Submission25586/Reviewer_gL1u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25586/Reviewer_gL1u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945816269, "cdate": 1761945816269, "tmdate": 1762943486006, "mdate": 1762943486006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a cross-modal syntax–NL attention framework for multi-agent collaborative coding, leveraging weak supervision from code execution to align structured and unstructured modalities. The idea is conceptually novel and shows empirical promise. However, the MARL experimental setup is underpowered: key baselines such as QMIX and other recent and advanced MARL methods are missing, and the results primarily reflect internal ablations rather than comparative performance. From the collaborative coding task perspective, this paper lacks comparison with other methods and does not show any comparison with other modern LLM-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The proposed framework that combines AST-based syntax modeling and Transformer-based NL embeddings into a unified attention mechanism is logically structured. The idea of grounding communication in code syntax is intuitive and consistent with how humans coordinate in coding tasks.\n\n+ Syntax-gated attention adds to the interpretability.\n\n+ Although the baselines are limited, the experiments clearly show that each architectural component (particularly syntax gating) contributes non-trivially to performance. The consistency of these improvements across two datasets (CodeReviewNet, CollabCode) strengthens the empirical narrative."}, "weaknesses": {"value": "- **Baseline selection is narrow and structurally biased.** The comparison set includes only MARL variants with superficial architectural differences, lacking diversity in methodological approaches. There are no comparisons against other LLM-based collaborative coding systems, such as agentic coding frameworks built on recent foundation models (e.g., Qwen-Coder model, or Qwen-Code framework ), nor against non-RL coordination baselines like supervised communication models or imitation-based collaboration systems. As a result, the reported improvements may only reflect internal architectural tuning rather than true algorithmic or paradigm-level advancement.\n- **Novelty in cross-modal design is overstated.** The cross-modal attention between code syntax and NL messages reuses well-established architectural patterns (AST-GNN + Transformer + contrastive alignment). While the domain application is new, the technical novelty is modest. The paper positions this as a breakthrough, but the mechanism largely mirrors existing cross-modal retrieval and code-language embedding frameworks.\n- **Lack of comparison with modern MARL baselines.** The paper only includes weak baselines (Independent MARL, Shared Critic MARL, Syntax-NL Heuristics). Stronger, widely recognized cooperative MARL methods, such as QMIX, VDN, QTRAN, QPLEX, and Transformer-based MARL architectures, are absent. This omission severely limits the credibility of the claimed performance improvements."}, "questions": {"value": "1. How is the “alignment reward” distributed among agents, and can its effect on coordination be visualized or decomposed?\n2. What is the computational cost as agent count or code complexity increases, and do you observe qualitative improvements in coordination or communication?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vu4PWLlEAi", "forum": "5sEj8EL8J4", "replyto": "5sEj8EL8J4", "signatures": ["ICLR.cc/2026/Conference/Submission25586/Reviewer_hYhs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25586/Reviewer_hYhs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762196963302, "cdate": 1762196963302, "tmdate": 1762943485684, "mdate": 1762943485684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}