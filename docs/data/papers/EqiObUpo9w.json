{"id": "EqiObUpo9w", "number": 15783, "cdate": 1758255179723, "mdate": 1759897282532, "content": {"title": "An Effective and Efficient Generation Framework for Condensing the Graph Repository", "abstract": "Graph repositories with multiple graphs are increasingly prevalent in various applications. As the amount of data increases, training neural networks on graph repositories becomes increasingly burdensome. However, existing condensation methods focus more on reducing the size of a single graph, they fail to address the challenges of efficiently and effectively compressing multiple data graphs. In this work, we propose a novel end-to-end graph repository condensation framework (GRCOND) that effectively condenses a large-scale graph repository with multiple graphs, while preserving task-relevant structural and feature information. Unlike traditional methods, our approach pretrains a dataset-specific GNN model to create and optimize synthetic graphs so that we can capture both intra-graph structures and inter-graph relationships, enabling a more holistic representation of the repository. Through experiments, our proposed approach consistently delivers higher accuracy and feature retention with different compression ratios, which highlights the potential of our framework to accelerate GNN training and expand the applicability of graph-based machine learning in resource-constrained environments.", "tldr": "", "keywords": ["Condensation for the graph repository", "Graph Algorithm", "Neural Network Model"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a627cd0e610beb298bfc71e7f945039aceb6b260.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GRCOND for condensation of large graph repositories. The method learns a compact set of latent codes that are decoded into synthetic graphs and features. A frozen pretrained encoder provides representational priors, and two dedicated decoders reconstruct topology and attributes. The synthetic set is optimized with class-wise gradient matching so that training on the condensed data yields update trajectories that closely follow those induced by the full corpus. The study evaluates multiple benchmarks across multiple compression ratios, showing consistent improvements over sampling and prior condensation methods, together with notable reductions in training cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The studied problem is important for large-scale graph research and applications, where full training cycles are costly.\n\n2. The document-to-latent pipeline is clear from pretraining through initialization to trajectory matching and decoding.\n\n3. Cost and speed evidence is provided, which strengthens the real-world usefulness of the framework."}, "weaknesses": {"value": "1. Missing important and strong recent baselines. The paper should align budgets and hyperparameter tuning across all methods, add at least one recent distribution matching approach adapted to graphs, and document model selection using a shared validation protocol.\n\n2. Reported averages lack a consistent statement on seeds and confidence intervals. Please provide three to five seeds, report mean and percentile confidence intervals, and run standard significance tests on headline gains.\n\n3. All tasks concern graph classification on molecules and proteins. Results on social or heterogeneous graphs would help test transfer. A small out-of-domain repository would improve external validity.\n\n4. Several symbols are introduced without complete definitions, including $\\alpha$, $\\beta$, $\\psi$, $f$, and $\\sigma$. The structure of the distance term Dis($\\cdot$) and the batching scheme for class-wise gradient matching are not fully specified. The paper should present the complete loss composition, normalization choices, and batch formation rules in one place."}, "questions": {"value": "1. Are the encoder and both decoders trained strictly on training graphs?\n\n2. What exact distance is used in Dis($\\cdot$) and how is it normalized across layers?\n\n3. What is the effect of removing class-wise matching in favor of a global objective?\n\n4. How sensitive is GRCOND to the chosen frozen encoder family?\n\n5. Can you show results on a repository with more classes and significant class imbalance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qO9BjM5AKK", "forum": "EqiObUpo9w", "replyto": "EqiObUpo9w", "signatures": ["ICLR.cc/2026/Conference/Submission15783/Reviewer_pHNi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15783/Reviewer_pHNi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761087746404, "cdate": 1761087746404, "tmdate": 1762926017203, "mdate": 1762926017203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the computational overhead in GNN training on large graph repositories, which is not met by existing single-graph condensation methods. It proposes GRCOND, a novel end-to-end framework designed to effectively condense multiple graphs to a small synthetic set while preserving structural and feature information. GRCOND uses a pre-trained GNN decoder to optimize synthetic graphs in a continuous latent space to bypass the discreteness of graphs. Empirical results demonstrate that GRCOND substantially reduces training costs while maintaining high classification accuracy across various real-world benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "GRCOND shows a novel application of dataset condensation to multiple graphs. The paper solves the challenge of graph data's discrete structure by using a pretrained decoder, making the condensation technique effective for multiple graph samples. A thorough ablation study and comprehensive evaluation that checks the method from diverse perspectives."}, "weaknesses": {"value": "- The paper suffers from poor clarity and inconsistent presentation, making it very difficult to read and understand. It is not self-contained; mathematical notations (e.g., $L, t, Dis$) are used without formal definition, some notations are duplicated (e.g., $D$ for set vs decoder), inconsistent (e.g., $D_o$ vs $D_O$), confused (e.g., $J$ vs $\\mathcal{J}$), or abused (e.g., minus operation on synthetic graph $S$ generated by the decoder. How can we define minus on graphs?) The connection between the main text and the figures is weak (e.g., CE loss in Figure 1 is absent from the main text), and the pretraining loss is not clearly specified. There should be a full review (not only examples I described) and a cleanup of all notations, descriptions, and illustrations for better readability\n- A significant weakness is the lack of a public code release or placeholder. Accepting a paper without code is not acceptable.\n- The paper needs to provide a stronger justification and a detailed ablation study for using the pretrained decoder, which is the largest contribution. The paper must clarify precisely how the variants in Table 4 were implemented: whether the trainable autoencoder variant was optimized during the condensation process, and whether the other two variants (\"We directly perform gradient descent on the node features and structural features of the graph to optimize graph data. The other variant used the untrained model to test\") were emplyed using frozen parameters. A more detailed ablation focusing purely on the state of the decoder (Frozen Pretrained vs. Frozen Untrained vs. Trainable during Condensation) is required.\n- There is a lack of benchmark domains (chemical and biological domains). There is doubt that it works on other domains like ogbg-code2 (also an example)."}, "questions": {"value": "."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EPU38r594f", "forum": "EqiObUpo9w", "replyto": "EqiObUpo9w", "signatures": ["ICLR.cc/2026/Conference/Submission15783/Reviewer_A5oG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15783/Reviewer_A5oG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899707096, "cdate": 1761899707096, "tmdate": 1762926016453, "mdate": 1762926016453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the scarcity and low efficiency of existing graph-level graph condensation methods. The proposed graph-level compression model that determines the parameters of its encoder and decoder through a pre-training phase. And to optimize the learning process, the model employs a gradient matching technique between the original and synthetic graphs to adaptively adjust the sampling strategy for the original dataset. Experiment results demonstrate the superior efficiency of the method. The main contributions of this work are the proposal of an optimization strategy based on gradient matching and end-to-end condensation framework for graph repositories."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Pre-training is utilized to freeze key parameters within the model, which reduces the training workload and enhances the overall compression efficiency.\n\n2. The authors use the concept of \"graph repository\" to interpret an intrinsic, yet previously overlooked, means that graph data originating from the same dataset possess inherent correlations. This approach concurrently establishes the distributional relationships among the condensed graphs.\n\n3. The method demonstrates excellent efficiency."}, "weaknesses": {"value": "1. The number of baseline models included in the state-of-the-art (SOTA) comparison, generalization performance tests, and computational cost analysis is insufficient, and the majority of them are outdated. It is highly recommended to include more recent graph-level models to better substantiate the claimed superiority of the proposed method in terms of both performance and efficiency. *e.g., KDD2024- Wang Y, Yan X, Jin S, et al. Self-supervised learning for graph dataset condensation.*\n\n\n2. The abstract and introduction do not specify the concrete problems in prior algorithms that the proposed graph-level condensation method aims to solve. Instead, its contribution is broadly summarized as an efficient solution for multi-graph dataset condenstaion."}, "questions": {"value": "Q1： The paper only mentions random sampling. Have other sampling strategies been considered? Would including them in the ablation study potentially yield better results?\n\nQ2：Could the generalization performance be validated on a broader range of datasets beyond just PROTEINS, similar to the experimental setup in Table 4? Furthermore, for improved readability, it is suggested to bold or highlight the best-performing results in both tables.\n\nQ3：How exactly does the matching loss optimize the sampling process, which is currently based on random sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fkpnGoNAQn", "forum": "EqiObUpo9w", "replyto": "EqiObUpo9w", "signatures": ["ICLR.cc/2026/Conference/Submission15783/Reviewer_njsr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15783/Reviewer_njsr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958540408, "cdate": 1761958540408, "tmdate": 1762926015536, "mdate": 1762926015536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an end-to-end Graph Repository Condensation (GRCOND) framework that effectively condenses a large-scale graph repository with multiple graphs while preserving task-relevant structural and feature information. Unlike traditional methods focusing on a single graph, the approach pretrains a dataset-specific GNN model to create and optimize synthetic graphs, capturing both intra-graph structures and inter-graph relationships for a more holistic representation. Experiments show that GRCOND achieves higher accuracy and retains features across different compression ratios, highlighting its potential to accelerate GNN training and enhance applicability in resource-constrained environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces an end-to-end framework that effectively condenses large-scale multi-graph repositories while preserving both structural and feature information.\n2. It leverages a pretrained, dataset-specific GNN to generate and optimize synthetic graphs, capturing intra- and inter-graph relationships comprehensively.\n3. Experiments demonstrate strong performance across compression ratios, showing high accuracy and scalability for resource-constrained GNN training."}, "weaknesses": {"value": "1. The paper lacks experiments on the training time of the graph generation model and the total time combining both generation and condensation processes.\n2. The motivation for condensation is unclear—although efficiency is mentioned, if the overall training time is long, direct training on downstream tasks might be more practical.\n3. While the paper claims that one condensed graph can support multiple downstream tasks, it does not provide experiments to validate this claim."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EvDagnkaEa", "forum": "EqiObUpo9w", "replyto": "EqiObUpo9w", "signatures": ["ICLR.cc/2026/Conference/Submission15783/Reviewer_kvET"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15783/Reviewer_kvET"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762183819228, "cdate": 1762183819228, "tmdate": 1762926013897, "mdate": 1762926013897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}