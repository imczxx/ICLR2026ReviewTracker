{"id": "n6kIYzPRPL", "number": 14281, "cdate": 1758231952366, "mdate": 1759897379183, "content": {"title": "Who Measures What Matters? An Analysis of Social Impact Evaluations in Foundation Model Reporting", "abstract": "As generative models become central to high-stakes AI systems, governance frameworks increasingly rely on evaluations to assess risks and capabilities. While general capability evaluations are common, social impact assessments, such as bias, privacy, and environmental effects remain fragmented, inconsistent, or absent. To characterize this landscape, we conduct the most comprehensive cross-provider analysis to date, examining 99 first-party release-time reports and 187 post-release sources to measure evaluation prevalence and strength, complemented by developer interviews to contextualize gaps. We find that first-party reporting is uneven, often superficial, and has declined over time. Third-party evaluations address some gaps but cannot cover critical areas such as data, content moderation labor, or environmental costs without provider disclosure. We further find that some developers deprioritize social impact evaluations unless tied to adoption or compliance, and that meaningful reporting is limited by resource constraints, reputational concerns, and the lack of standardized, practical frameworks. Our findings highlight that current evaluation practices leave major gaps in assessing foundation models' societal risks, underscoring the urgent need for more systematic, comparable, and transparent frameworks.", "tldr": "We analyze first- and third-party reports on foundation models, complemented by stakeholder interviews, to reveal major gaps and barriers in social impact evaluations such as bias, privacy, and environmental effects.", "keywords": ["social impact", "evaluations", "foundation models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80c06cfa524f1742d3a9d86e8c2d54ae8742aa5a.pdf", "supplementary_material": "/attachment/6b8e864aed912f930477def8e903715ec286d67c.zip"}, "replies": [{"content": {"summary": {"value": "The paper offers a large‐scale, cross‑provider analysis of how social impact evaluations for foundation models are reported. The authors examine seven base‑level dimensions, bias/representational harms, sensitive content, performance disparity, environmental costs, privacy/data protection, financial costs, and data/moderation labo, drawing on first‑party release‑time reports and post‑rlease third‑party sources. They complement the quantitative analysis with semi‑structured interviews of developers to explain observed gaps and incentives.\n\nContributions:\n(1) The first broad, quantitative snapshot of social‑impact evaluation reporting across providers and categories; (2) qualitative context via developer interviews; (3) commitment to release an annotated dataset post‑publication (Sec. 1, bullets; pp. 1–2)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1- Timely and policy‑relevant question with clear framing around seven base‑level social‑impact dimensions (Sec. 4. This focuses the analysis on what model developers control pre‑deployment. \n\n2 - Cross‑provider, multi‑region, multi‑sector dataset; per‑category scoring distinguishes mere mentions from methodologically detailed reporting (Fig. 1–6). \n\n3 - Methodological care. Bayesian hierarchical ordinal regression is an appropriate choice for 0–3 scales; reporting of priors, cutpoint parameterization, and diagnostics inspires confidence (App. 8.7; Table 2). \n\n4 - Strong empirical evidence that third‑party evaluations carry much of the social‑impact load and that environmental and bias reporting has decreased since 2023Q3 (Fig. 2; Fig. 14d). \n\n5 - Mixed‑methods triangulation. Interviews provide plausible explanations for gaps (organizational incentives, reputational/legal risk, lack of usable frameworks) that align with the quantitative patterns."}, "weaknesses": {"value": "Major:\n\n- Annotation reliability not quantified. The paper explicitly states no formal inter‑annotator agreement was computed (App. 8.3, p. 21) \n- Counting inconsistency in source totals. The abstract reports 99 first‑party and 187 post‑release sources; Sec. 1 lists 132 first‑party and 174 third‑party/leaderboards (pp. 1–2). This raises questions about inclusion criteria, deduplication, and whether leaderboards are counted as “post‑release sources.” Actionable: reconcile counts, define each bucket precisely (tech report, model card, system card, blog, leaderboard, peer‑reviewed paper), and provide a consolidated PRISMA‑style flow diagram. \n\n- Third‑party papers are restricted to peer‑reviewed works from 2024 onward (Sec. 3), potentially under‑representing earlier or non‑archival evaluations, and privileging Anglophone venues. This matters for temporal claims (spikes/declines in 2024–2025) and for regional equity."}, "questions": {"value": "1 Reliability. Did you pilot the rubric on a shared subset and, if so, what was the observed IAA? If not, could you compute α/κ on 10–20% of items and report robustness of key conclusions when restricting to items with coder agreement? \n\n2 Temporal analyses. How sensitive are the “decline since 2023Q3” findings (Fig. 2; Fig. 14) to (a) collapsing variants to base models, and (b) including non‑peer‑reviewed third‑party sources pre‑2024? \n\n3 Provider weighting. Beyond hierarchical intercepts, did you try provider‑level weighting (e.g., one vote per provider‑release) to mitigate large‑family effects (Gemini/Llama)? If tried, did conclusions change? (Sec. 3)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lOP9z2SeXi", "forum": "n6kIYzPRPL", "replyto": "n6kIYzPRPL", "signatures": ["ICLR.cc/2026/Conference/Submission14281/Reviewer_q1un"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14281/Reviewer_q1un"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623392341, "cdate": 1761623392341, "tmdate": 1762924730251, "mdate": 1762924730251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a nice paper that presents an annotated database of model evaluations in the context of evaluation for social impact. The database is a valuable contribution, but I believe the analysis could be improved, and the figures should be improved."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Important topic.\n- The annotated dataset is a valuable contribution."}, "weaknesses": {"value": "Figures: I found the figures to be very suboptimal for conveying the desired information. For example, red is usually used to highlight something undesirable, but in some figures, it shows the best score for evaluations. In some of the figures, the test is too small and isn't clear even when you zoom in (and will be unreadable if printing). I think the figures / tables showing progress over time will be much more effective if presented as a figure with 't' on the x-axis. \n\nSummary statistics: I really think the main body of the paper will benifit from summary statistics of the dataset. \n\n Analysis: the two main conclusions 1) the third-party analysis tends to focus better on social impact, and 2) certain factors, such as bias, are more investigated in the climate impact and moderation -- both somewhat unsurprising. This does not mean that there is no value in verifying them, but I am certain the data contains more novel insights as well, which could be highlighted.  \n \nPolicy: the paper does not say anything about policy -- which is very related to the quality of 1st party evaluations."}, "questions": {"value": "Please state if and how you think you can improve the paper with respect to the weaknesses highlighted."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ywbf7gjVwu", "forum": "n6kIYzPRPL", "replyto": "n6kIYzPRPL", "signatures": ["ICLR.cc/2026/Conference/Submission14281/Reviewer_yzxU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14281/Reviewer_yzxU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993044325, "cdate": 1761993044325, "tmdate": 1762924729581, "mdate": 1762924729581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic empirical study in the field of AI governance and evaluation, analyzing how foundation models are currently assessed and reported with respect to their social impacts. Building on the seven-dimensional taxonomy proposed by Solaiman et al. (2023), the authors adopt a standardized empirical methodology that combines rigorous statistical modeling (Bayesian hierarchical ordinal regression) with cross-source data comparison. The study examines 99 first-party release-time reports and 187 third-party post-release sources to measure the prevalence and strength of social impact evaluations. Developer interviews further contextualize organizational incentives, constraints, and unobservable factors shaping reporting practices, thereby illuminating the gaps between first-party and third-party evaluations.\n\nResults show that current evaluation practices leave major gaps in assessing the societal risks of foundation models, particularly in critical dimensions such as privacy, environmental costs, and data or content moderation labor, highlighting the need for more systematic, transparent, and comparable frameworks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper focuses on the social impact evaluation of foundation models, an area of increasing importance within AI governance and Responsible AI research, yet still lacking systematic, quantitative evidence. The authors provide a clear empirical answer to the question \"Who measures AI's social impact?\" Current practices rely heavily on a small number of providers' voluntary disclosures and on third-party studies with limited access and scope. This makes the paper both policy-relevant and academically valuable.\n\n2. Methodology combining quantitative and qualitative analyses.\n    -  Quantitative component: the authors uniformly code and score 132 first-party reports and 187 third-party studies, enabling cross-provider comparability.\n    -  Qualitative component: semi-structured developer interviews (from both for-profit and non-profit organizations) provide valuable contextualization, revealing the organizational incentives and constraints that shape current social impact evaluation practices.\n\n3. It further disaggregates reporting strength across organization types (industry, academia, government, non-profit) and geographic regions (North America, Europe, East Asia, MENA, etc.), offering a fine-grained picture of provider-level variation.\n\n4. The results are communicated with clarity:\n  - First-party reporting quality is generally low and declining over time. \n  - Sensitive content is the only dimension consistently evaluated, while privacy, environmental costs, and content moderation labor are largely neglected.\n  - Interviews uncover the underlying incentive mechanisms, e.g., evaluations are often conducted \"only when they support product adoption or compliance\".\nThe authors propose constructive reforms such as standardized templates, incentive alignment, safe-harbor policies, and multi-stakeholder coordination.\n\n5. The manuscript is easy to follow, and the figures and tables effectively convey the empirical trends."}, "weaknesses": {"value": "1. The developer interview component includes only 5 participants, primarily based in the United States and France, which restricts the representativeness of the qualitative insights.\n\n2. The paper evaluates whether social impact dimensions are reported and how detailed they are (0 - 3 scale), but does not assess whether these evaluations are scientifically valid, methodologically sound, or ethically adequate.\nAs a result, it primarily reveals an information disclosure gap, rather than an evaluation capability gap.\n\n3. The study is positioned as an empirical analysis and does not propose a new theoretical framework or evaluative paradigm.\n\n4. There appears to be a numerical inconsistency between the counts of first- and third-party reports: 99/187 in the abstract versus 132/174 in the main text.\nThe authors should clarify whether these refer to different units (e.g., models vs. reports) or to differences in data filtering or deduplication."}, "questions": {"value": "See Weaknesses Section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vKATIR6cjX", "forum": "n6kIYzPRPL", "replyto": "n6kIYzPRPL", "signatures": ["ICLR.cc/2026/Conference/Submission14281/Reviewer_8YkR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14281/Reviewer_8YkR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995086874, "cdate": 1761995086874, "tmdate": 1762924728904, "mdate": 1762924728904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}