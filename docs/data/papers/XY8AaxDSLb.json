{"id": "XY8AaxDSLb", "number": 24167, "cdate": 1758353492032, "mdate": 1759896779009, "content": {"title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents", "abstract": "Modern language agents often need to solve tasks requiring long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries. Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance. This leads to un-bounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths due to LLM forgetting the context. We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant context size when solving long multi-turn tasks. At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning. Leveraging reinforcement learning (RL) and rollout trajectory truncation, we train a MEM1 agent to develop internal states that integrate prior memory with new observations from the environment while strategically discarding irrelevant or redundant information. Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5$\\times$ while reducing memory usage by 3.7$\\times$ compared to Qwen2.5-14B-Instruct on an augmented multi-hop QA dataset with 16 objectives in each task, and generalizes beyond the training horizon. Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon task-solving agents that involve multiple interactions, where both efficiency and performance are optimized.", "tldr": "We propose a RL-based learning framework that enables interactive agents to solve long-horizon tasks with constant context", "keywords": ["deep research", "reasoning", "context compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/466ad9525f263439e4270eb95fe2a2e331920403.pdf", "supplementary_material": "/attachment/39a1f8e5918a1566c15e510bcba7edf4bb129ce8.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce MEM1, an end-to-end reinforcement learning (RL) framework that enables agents to update a compact, shared internal state at each turn. This state jointly supports memory consolidation and reasoning by integrating essential prior information with new observations while strategically discarding irrelevant data. The framework is trained using RL with a 2D attention mask to manage the dynamic context during policy optimization. Experiments were conducted in three domains: internal retrieval QA, open-domain web QA, and WebShop navigation. The authors introduced a \"multi-objective QA\" task, augmenting datasets like HotpotQA by composing multiple questions. Results shows that Mem1 achieves competitive performance across QA and web navigation benchmarks with substantially reduced memory usage and inference latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written, well-structured, and strongly motivated.\n2. On extra-long horizon tasks (16 objectives), MEM1 demonstrates both efficiency and strong performance, indicating that it effectively mitigates the “forgetting” problem in long-context scenarios. Its ability to generalize from training on 2-objective tasks to 16-objective tasks is particularly impressive, suggesting that the model has acquired a universal memory management capability rather than task-specific heuristics.\n3. A key novelty of this paper lies in integrating memory management directly into the model’s reasoning process, instead of relying on external memory modules. This design allows the agent to learn what to remember as part of its inheent decision-making strategy, which represents an elegant and conceptually unified approach to handling memory within large reasoning models."}, "weaknesses": {"value": "1. The paper lacks evaluation on several representative multi-hop reasoning benchmarks, such as BrowseComp, GAIA, and WebWalker QA. Including these datasets would provide a more comprehensive assessment of MEM1’s capabilities, particularly in extra-long-horizon tasks that require complex, multi-step reasoning.\n\n2. Incomplete analysis of the Internal State.\n\n(1)\tThe paper does not provide sufficient analysis of the Internal State mechanism. It would be helpful for the authors to report statistics such as the average length of the Internal State during inference, and to clarify whether its size can be explicitly controlled, or whether it grows dynamically with the number of interaction turns. \n\n(2)\tFurthermore, the authors should discuss how the model mitigates hallucinations or inconsistencies in the Internal State representation. Presenting a few failure cases or qualitative examples where the Internal State leads to incorrect reasoning would provide valuable insight into the model’s limitations and potential areas for improvement.\n\n3. The authors claim O(1) context efficiency, but this only accounts for the attention cost. Internal State at step t must include all historical information, so its length and generation cost increase with the task complexity at each step. The authors should report the total FLOPS of MEM1 and compare with other methods (DeepResearcher, SearchR1)."}, "questions": {"value": "1. The tasks used in this paper are based on a final outcome reward setting. It would be interesting to explore how the model’s performance changes when applying RLVR algorithm, such as GRPO. \n2. If a model incorrectly summarizes or omits a key fact, that fact is permanently lost. How to solve this problem?\n3. It could guide the model to use a summarization tool to condense all historical information into a fixed context window. What advantages does MEM1 offer compared to this method?\n4. Do different search providers (such as Google Seach and Bing Search) have an impact on performance? What is the search parameter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "moJ9NypwVy", "forum": "XY8AaxDSLb", "replyto": "XY8AaxDSLb", "signatures": ["ICLR.cc/2026/Conference/Submission24167/Reviewer_CDp4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24167/Reviewer_CDp4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761473519759, "cdate": 1761473519759, "tmdate": 1762942970937, "mdate": 1762942970937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MEM1, a reinforcement learning framework that jointly learns memory consolidation and reasoning for long-horizon LLM agents. Instead of storing full interaction histories or relying on external memory, MEM1 maintains a compact internal state updated at each step and discards the previous context. The framework is trained end2end via PPO using masked trajectories that enables stable and accurate policy optimization under MEM1’s memory-constrained execution. Experiments on multi-objective question answering and WebShop navigation demonstrate that MEM1 improves long-horizon task performance while significantly reducing inference memory and latency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes to treat memory as part of the policy and learn it jointly with reasoning, instead of relying on an external memory module or retrieval system. This is a refreshing take on long-horizon agents and feels conceptually meaningful: the model actually learns what to remember rather than being manually engineered to store past information.\n\n2. Existing LLM agents don’t scale well over long contexts because their memory grows linearly (or worse) with interaction steps. MEM1 tackles this by enforcing a constant memory budget using an internal state. This is a practical and relevant contribution for real world agent deployment where inference cost matters.\n\n3. The experimental analysis is relatively thorough and covers both quantitative and qualitative dimensions. On the quantitative side, the paper reports multiple evaluation metrics, including task accuracy, long-horizon generalization performance, peak memory usage, and inference efficiency. In addition, the paper goes beyond standard benchmark reporting by including behavioral analysis of the learned policy. The qualitative inspection of internal states and step-by-step trajectories provides useful insight into how MEM1 operates."}, "weaknesses": {"value": "1. EM reward design lacks ablation study and maybe limits real world applicability . Although the paper adopts EM as the sole reward signal during RL training for QA tasks, it does not use ablation study to analyze or justify this specific reward choice. For example, they do not compare EM with other potential reward signals such as token-level F1, partial matching, or step-wise retrieval rewards that might better capture intermediate reasoning quality.  Also, the assumption of verifiable rewards such as EM restricts the scope of applicability. In most real world long-horizon tasks e.g., scientific literature review, legal case analysis, or customer support, ground-truth answers are ambiguous, subjective, thus EM-style rewards are unavailable.\n\n2. Task Design lacks interdependent objectives. The multi-objective QA benchmark introduced in Section 3.3 constructs tasks by simply concatenating independent questions from HotpotQA and Natural Questions (e.g., “Which magazine was started first…?” and “The Oberoi family is part of a hotel company…?”). Crucially, these sub-questions share no entities, context, or logical dependencies, meaning the agent can solve them in isolation without cross-question reasoning or memory integration. As a result, the task primarily measures multi-task efficiency rather than true synergistic reasoning across interdependent objectives. One possible design can be referenced in M3Agent [1].\n\n3. The evaluation lacks comparison to strong, lightweight memory compression baselines that do not require external modules or reinforcement learning. Notably absent is a simple LLM-based summarization baseline, where the same Qwen2.5-7B model summarizes the full history into a fixed-length context at each turn—a strategy that could achieve similar efficiency gains without RL. The “Truncation (prompt only)” ablation (Table 1) already achieves 0.396 EM on 16-objective QA—more than double the full-history Qwen2.5-7B-Instruct—suggesting that much of MEM1’s gain may stem from its prompt and rollout design rather than the RL policy itself. \n\n[1] Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory"}, "questions": {"value": "1. Can the MEM1 framework be adapted to offline datasets (e.g., human or expert trajectories) and combined with RL algorithms like GRPO? What are the key challenges in doing so? For instance, would the unobserved internal state in offline data pose a fundamental identifiability issue?\n\n2. The system prompt is excluded from token counting，but it’s part of the deployed model’s memory. Isn’t this misleading for real-world efficiency claims?\n\n3. Training only on 2-objective tasks but testing up to 16, does performance gain come from generalization or simply from avoiding context collapse? Baseline models (e.g., Qwen2.5-14B) collapse on 16-objective tasks (Table 1: EM drops to near zero), likely because their context exceeds practical limits or attention dilution occurs. MEM1 avoids this by design. But is MEM1 truly reasoning better, or just not failing catastrophically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2kCdXrLHyC", "forum": "XY8AaxDSLb", "replyto": "XY8AaxDSLb", "signatures": ["ICLR.cc/2026/Conference/Submission24167/Reviewer_pi4p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24167/Reviewer_pi4p"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761531481894, "cdate": 1761531481894, "tmdate": 1762942970716, "mdate": 1762942970716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MEM1, an end-to-end RL framework for language agents that maintains constant memory over arbitrarily long multi-turn tasks by updating a compact shared internal state instead of appending full histories. Training integrates memory management with reasoning, and a scalable task-augmentation scheme builds multi-objective, multi-hop environments. On internal retrieval QA, open-domain web QA, and web shopping, MEM1 delivers strong—often SOTA—accuracy with markedly improved memory and inference efficiency, especially at long horizons."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear, Well-Structured Presentation**: The technical motivation, algorithm, and evaluation methodology are clearly described. Figures such as Figure 1 (“RL pipeline“) and Figure 2 (\"performance and efficiency scaling\") directly help crystallize the approach and the empirical insights.\n\n- **Sound, End-to-End RL Optimization**: The use of reinforcement learning to train both reasoning and memory management jointly is well argued and empirically shown to benefit generalization to longer, more complex tasks. The masking-based trajectory construction and objective computation address the non-trivial technical challenge of dynamically changing agent context during rollouts."}, "weaknesses": {"value": "- **Evaluation Reflects Synthetic Compositions, Not Open-Ended Dialogue.** Benchmarks are mainly constructed by composing QA subsets, which exercises multi-turn reasoning but biases toward compositional templates. Even WebShop, though interactive, is governed by predefined tasks, constrained action spaces, and scripted reward assumptions, so the current setup under-represents genuinely open-ended interactions with ambiguous goals or shifting task boundaries.\n\n\n- **Insufficient Direct Comparison to Hierarchical and Structured Memory Architectures**: While the related work section covers many approaches, no empirical or conceptual comparison is offered with recent hierarchical/structured working memory proposals such as HiAgent [1], Zep[2] or [3]. These works explicitly address long-horizon memory management and could reveal functional or performance trade-offs.\n\n\t\n - **Lack of Ablations on Memory-Reasoning Coupling**: The core contribution is the tight integration of memory and reasoning, but no detailed ablation quantifies the benefit of this coupling versus traditional staged (e.g., memory-summarize-then-reason) approaches. Including such an ablation would clarify to what extent performance gains are due to this integration versus reinforcement learning or prompt design alone.\n\n[1] Hu, Mengkang, et al. \"Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model.\" _arXiv preprint arXiv:2408.09559_ (2024).\n\n[2] Rasmussen, Preston, et al. \"Zep: a temporal knowledge graph architecture for agent memory.\" _arXiv preprint arXiv:2501.13956_ (2025).\n\n[3] Sun, Haoran, and Shaoning Zeng. \"Hierarchical memory for high-efficiency long-term reasoning in llm agents.\" _arXiv preprint arXiv:2507.22925_ (2025)."}, "questions": {"value": "- What are the expected limitations or bottlenecks when scaling MEM1 to hundreds or thousands of objectives/turns? Are there inherent trade-offs between memory consolidation, retrieval accuracy, and forgetting?\n\n- To what extent can MEM1’s method be positioned as an advance in lifelong or continual learning memory management (rather than purely RL agent efficiency) ?\n\n- What specific performance gains (if any) are attributable to tightly integrating reasoning/working memory within a single update, versus staged or modular designs where reasoning and memory summarization are decoupled? A targeted ablation would aid understanding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "udQ4iW00Qn", "forum": "XY8AaxDSLb", "replyto": "XY8AaxDSLb", "signatures": ["ICLR.cc/2026/Conference/Submission24167/Reviewer_Dxgb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24167/Reviewer_Dxgb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986523898, "cdate": 1761986523898, "tmdate": 1762942970136, "mdate": 1762942970136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MEM1, a reinforcement learning (RL) framework that trains LLM agents to handle long-horizon tasks by managing its own contextual memory. Instead of suffering from context exploding in the conventional approaches that append all history in the context, MEM1 trains, via RL, an agent to autonomously and iteratively consolidate its existing context, and learn to synergise its reasoning and information compression. The paper conducts extensive and delivers promising experimental results, where a 7B model trained with MEM1 outperforms a much larger 14B model as well as existing approaches like Search-R1 with significantly improved performance and memory usage on 16-objective tasks. The paper also demonstrates generalizability of the method on WebShop task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The problem is well motivated, targeting a critical bottleneck of using LLM agents for real-world problems. Curbing the unbounded context growth finds many applications in AI agent applications, including deep research, web agents, game playing agents, etc..\n- The proposed method is simple but effective. MEM1 encourages the AI agent to synergise reasoning and memory consolidation by designing a clever rollout mechanism in an RL pipeline. The simplicity and end-to-end nature also implies the potential scalability to solving highly complicated long-horizon tasks.\n- The paper demonstrates impressive empirical results, where MEM1 reliably outperforms baseline methods as the tasks become more long-horizon by increasing the number of objectives."}, "weaknesses": {"value": "- MEM1’s reliance on verifiable and dense reward signals limits its applicability to open-ended or subjective tasks. Many realistic LLM-agent settings (e.g., creative reasoning and open-ended QA) lack such clear supervision. It is interesting how it can be extended to cases where the rewards are more implicit.\n- Some presentation issues: (1)The naming is a bit messy. The paper has used “long-turn”, “long-horizon”, and “multi-turn” throughout. Do they mean the same thing? If so, the authors should standardize the term. (2) The paper repeatedly sells “constant context size,” but the evidence is proxy metrics (peak token count excluding system prompt) that still increase with objectives (e.g., 6.40 -> 10.4×100 tokens from 2->16 objectives). There is no formal upper bound on internal state (IS) length or a proof that memory is constant w.r.t. horizon. I suggest the authors restate claims as “near-constant peak tokens under our rollout policy” to make it more rigorous."}, "questions": {"value": "- What are some of the failure cases when MEM1’s context consolidation fails to be effective?\n- On Figure 2, why is the peak token for 8x objectives more than 16x objectives for some models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I don't have any ethics concerns."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xTscvBQOct", "forum": "XY8AaxDSLb", "replyto": "XY8AaxDSLb", "signatures": ["ICLR.cc/2026/Conference/Submission24167/Reviewer_zPnP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24167/Reviewer_zPnP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762103099498, "cdate": 1762103099498, "tmdate": 1762942969636, "mdate": 1762942969636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}