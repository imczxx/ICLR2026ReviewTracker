{"id": "ckfNLA1Dxy", "number": 7406, "cdate": 1758020472431, "mdate": 1759897854568, "content": {"title": "Δ-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation", "abstract": "Visual Instruction Finetuning (VIF) is pivotal for post-training Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in plain-text large language models, which mainly requires instruction datasets to enable model instruction-following ability, VIF also requires multimodal data to enable joint visual and textual understanding; therefore, it typically requires more data. Consequently, VIF imposes stricter data selection challenges: the method must scale efficiently to handle larger data demands while ensuring the quality of both visual and textual content, as well as their alignment. Despite its critical impact on performance, data selection for VIF remains an understudied area. In this paper, we propose Δ-AttnMask. This data-efficient framework quantifies sample quality through attention-guided masking of the model's hidden states, jointly evaluating image-text pairs without requiring domain labels, auxiliary models, or extra training. By computing loss differences (Δ) between the original states and states masked using high-attention regions, Δ-AttnMask intrinsically assesses sample quality. Experiments across multiple VLMs and datasets show that Δ-AttnMask achieves state-of-the-art performance with just 20% of data, accelerating training by 5× while surpassing full-dataset baselines by +10.1% in overall accuracy. Its model-agnostic and data-agnostic design ensures broad applicability across modalities and architectures.", "tldr": "We propose Δ-AttnMask, a data-efficient framework that quantifies sample quality through attention-guided masking of hidden states, jointly evaluating image-text pairs without domain labels, auxiliary models, or extra training.", "keywords": ["Visual Instruction Finetuning (VIF)", "Vision-Language Models (VLMs)", "post-training", "data selection", "Multimodal Learning", "NLP", "Efficient Machine Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/14948ed9cd80363caed1e091ec764ff6bc1cd4eb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents $\\Delta$-AttnMask, a novel, lightweight method for data selection in the context of Visual Instruction Finetuning (VIF) for Vision-Language Models (VLMs). The core problem it addresses is the inefficiency and poor data quality often present in large-scale VIF datasets. The proposed method quantifies sample quality by measuring the model's output sensitivity to an internal perturbation. Specifically, it performs a forward pass to identify high-attention hidden states in a late-stage transformer block, masks these states, and then measures the resulting increase in loss ($\\Delta$). The authors hypothesize that high-quality, well-aligned samples will be more sensitive to this targeted masking, resulting in a higher $\\Delta$ score. The paper provides strong empirical evidence to support this method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the critical and practical challenge of data efficiency and quality in VIF. As VLM datasets grow, efficient data selection becomes an important bottleneck, and this work provides a well-motivated solution.\n2. A major strength of $\\Delta$-AttnMask is its computational efficiency. By being \"inference-only,\" it avoids costly gradient computations and, crucially, does not rely on other external, proprietary models for scoring, making it a self-contained and practical approach.\n3. The method's heuristic—that sensitivity to masking in late-stage, fused representations correlates with cross-modal alignment and quality—is an intuitive and novel way to approximate data quality with a single, scalar score."}, "weaknesses": {"value": "1. The method's design relies on several key heuristics that are not fully justified. For example, the choice of the \"second-to-last\" transformer block is presented as an empirical optimum but lacks a deeper analysis of why this layer is ideal, or how this choice might change with different model architectures or scales.\n2. The experimental validation would be much stronger with a more comprehensive set of baselines. The paper primarily compares against random selection and other simple baselines. To convincingly demonstrate the advantages of $\\Delta$-AttnMask, it is essential to benchmark it against other established data selection methods from related literature, rather than relying on less competitive comparisons.\n3. The computational cost may be underestimated. While \"inference-only\" is more efficient than back propagation, the scoring process still requires at least one full forward pass for every sample in the original dataset. For a dataset with hundreds of millions of samples, this pre-processing step is still expected to represent a substantial, non-trivial computational cost. A deeper analysis of such additional cost is necessary.\n4. The idea of using $\\Delta$-AttnMask as a data augmentation technique (by re-training with the mask activated) is an interesting side-note but feels underdeveloped. This claim is not sufficiently benchmarked against other established augmentation techniques, and its mechanism is not deeply explored, making it feel more like an afterthought than a core contribution."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "biU6rkCfq2", "forum": "ckfNLA1Dxy", "replyto": "ckfNLA1Dxy", "signatures": ["ICLR.cc/2026/Conference/Submission7406/Reviewer_ZHFx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7406/Reviewer_ZHFx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761201227350, "cdate": 1761201227350, "tmdate": 1762919528496, "mdate": 1762919528496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a data selection method for visual instruction finetuning. It quantifies data quality through attention-guided masking of the model’s hidden states without requiring domain labels, auxiliary models, or additional training. Experimental results show that the proposed method achieves SOTA performance and using just 20% of the data can surpass full-dataset baselines by 10.1% in overall accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method addresses the data-efficient problem in VLM finetuning, achieving enhanced performance using only 20% of the data compared to full-dataset training, which is somewhat novel.\n2. The proposed method is mostly well-written.\n3. The algorithm is easy to follow and understand, which is a strength in my opinion\n4. The experiment results are good and promising, validating the performance across architectures and ablation studies."}, "weaknesses": {"value": "1. As introduced in Sec. 3.3, the proposed method applies masking at the deepest transformer layer before the final prediction head. So, is the proposed method robust to the layer that applies masking?\n\n2. More related works in this domain need to be discussed [1,2,3].\n\n2. Authors are suggested to reduce the space of experiment setup (4.1) and include a new section to discuss some potential limitation and future directions of the proposed methods.\n\n3. Some typos should be carefully revised, e.g., caption of Figure 1 “n”, “k”, and “p” denote variables rather than letters.\n\n4. The appendix should be reorganized as the order they occur in the main paper.\n\n[1] Wei, Lai, et al. \"Instructiongpt-4: A 200-instruction paradigm for fine-tuning minigpt-4.\" arXiv preprint arXiv:2308.12067 (2023).\n\n[2] Bi, Jinhe, et al. \"Prism: Self-pruning intrinsic selection method for training-free multimodal data selection.\" arXiv preprint arXiv:2502.12119 (2025).\n\n[3] Wang, Weizhi, et al. \"Finetuned multimodal language models are high-quality image-text data filters.\" arXiv preprint arXiv:2403.02677 (2024)."}, "questions": {"value": "1. The proposed method quantifies the quality of samples by measuring the model’s sensitivity\nto attention-guided perturbations of its hidden states. However, noisy data points may inevitably exist and exhibit fluctuations. Could this cause inaccurate evaluation on sample quality? More discussions and potential future directions are suggested to include. \n\n2. Can this method be applied to LLMs?\n\n3. This method is model-specific. Can we use the selected data from model A to fine-tune another model B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fsSSSfZrEg", "forum": "ckfNLA1Dxy", "replyto": "ckfNLA1Dxy", "signatures": ["ICLR.cc/2026/Conference/Submission7406/Reviewer_anGF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7406/Reviewer_anGF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537837046, "cdate": 1761537837046, "tmdate": 1762919528084, "mdate": 1762919528084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces $\\Delta$-AttnMask, a data selection framework for visual instruction finetuning in Vision-Language Models. The key idea is to use attention-guided hidden state masking to estimate the quality of multimodal samples. By comparing losses between original and masked states, the method quantifies how sensitive the model is to important regions of the input, thereby identifying high-quality, informative examples. The approach is model-agnostic, requires no additional training or auxiliary models, and can also be used as a data augmentation mechanism. Experiments on several VLMs and datasets show that $\\Delta$-AttnMask achieves up to 5× faster training and +10.1% accuracy improvement using only 20% of the data, outperforming baselines like SELF-FILTER and PreSel."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a novel theoretically grounded idea, which uses loss deltas from attention-guided masking to estimate data utility, bridging efficiency and quality in multimodal data selection.\n\n- The approach is lightweight, requiring only two forward passes per sample and no gradient computation or external models.\n\n- The proposed method demonstrates consistent performance gains across multiple models, datasets, and tasks, achieving better results with 20% of the data."}, "weaknesses": {"value": "- While the proposed method is conceptually appealing, the experimental evidence is somewhat limited. The paper would be strengthened by including comparisons with a broader range of baselines, such as random selection, EL2N [1], D2-Pruning [2], COINCIDE [3], and ICONS [4]. \n\n- The data selection experiments are conducted primarily on moderate-scale datasets, and it remains unclear how well the approach scales to larger scale multimodal instruction tuning, such as LLaVA-1.5 665K or comparable datasets. Demonstrating scalability on such datasets would make the paper’s claims about efficiency and general applicability more convincing.\n\n- The data augmentation results are limited to the small MiniGPT-4 dataset with the Qwen2-VL 2B model. The absence of results on larger models or datasets makes it difficult to assess whether the proposed augmentation strategy generalizes beyond small-scale scenarios.\n\nOverall, I would be inclined to increase my score if the authors could provide additional experiments covering more baselines, larger datasets, and stronger evidence of generalization across model scales.\n\n[1] Paul, Mansheej, Surya Ganguli, and Gintare Karolina Dziugaite. \"Deep learning on a data diet: Finding important examples early in training.\" Advances in neural information processing systems 34 (2021): 20596-20607.  \n[2] Maharana, Adyasha, Prateek Yadav, and Mohit Bansal. \"$\\mathbb {D}^ 2$ Pruning: Message Passing for Balancing Diversity & Difficulty in Data Pruning.\" The Twelfth International Conference on Learning Representations.  \n[3] Lee, Jaewoo, Boyang Li, and Sung Ju Hwang. \"Concept-skill Transferability-based Data Selection for Large Vision-Language Models.\" Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 2024.  \n[4] Wu, Xindi, et al. \"Icons: Influence consensus for vision-language data selection.\" arXiv preprint arXiv:2501.00654 (2024)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "134BMdMU9o", "forum": "ckfNLA1Dxy", "replyto": "ckfNLA1Dxy", "signatures": ["ICLR.cc/2026/Conference/Submission7406/Reviewer_SzTa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7406/Reviewer_SzTa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595371305, "cdate": 1761595371305, "tmdate": 1762919527744, "mdate": 1762919527744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a data selection strategy, $\\Delta$-AttnMask, for Vision-Language models (VLMs).\n\nSpecifically, with attention scores, it first identifies the important tokens.\nThen, measure the loss differences between with and without masking important tokens. Samples with large loss differences will be selected.\n\nExperiments on MiniGPT4 data, LLaVA Instruction 158K, and Vision Flan 191K along with Qwen2-VL models (2B/7B) show that the models trained with 20% original data even achieve better performance than the models trained on the whole data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The method is simple and easy to implement.                                \n(2) It is interesting to use attention-guided loss perturbation for data selection."}, "weaknesses": {"value": "(1) For stage 1 of $\\Delta$-AttnMask, how to derive the unmodified model?     \n    What's the data the unmodified model is trained on?\n    For example, if the unmodified models are the pretrained Qwen2-VL models, then the data selection process would implicitly leverage prior knowledge of extra data used for the unmodified model pretraining. \n \n(2) Suppose we use models A and B to do data selection separately, what's the performance of model C trained on the two selected datasets individually?\n\n(3) The paper validates the effectiveness of the proposed method by experiments on dataset compression. Would it be possible to show the advantage of the proposed method on mining new, informative data that could further improve model performance?\n\n(4) How to aggregate attention scores across layers in Figure 1?\n\n(5) How to mask tokens for each layer of VLMs?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EObbIK1brj", "forum": "ckfNLA1Dxy", "replyto": "ckfNLA1Dxy", "signatures": ["ICLR.cc/2026/Conference/Submission7406/Reviewer_MExr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7406/Reviewer_MExr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930460151, "cdate": 1761930460151, "tmdate": 1762919527313, "mdate": 1762919527313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}