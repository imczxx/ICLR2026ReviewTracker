{"id": "oZJFY2BQt2", "number": 16498, "cdate": 1758265193145, "mdate": 1763192122644, "content": {"title": "Decentralized Attention Fails Centralized Signals: Rethinking Transformers for Medical Time Series", "abstract": "Accurate analysis of Medical time series (MedTS) data, such as Electroencephalography (EEG) and Electrocardiography (ECG), plays a pivotal role in healthcare applications, including the diagnosis of brain and heart diseases. MedTS data typically exhibits two critical patterns: **temporal dependencies** within individual channels and **channel dependencies** across multiple channels. While recent advances in deep learning have leveraged Transformer-based models to effectively capture temporal dependencies, they often struggle to model channel dependencies. This limitation stems from a structural mismatch: ***MedTS signals are inherently centralized, whereas the Transformer's attention is decentralized***, making it less effective at capturing global synchronization and unified waveform patterns. To bridge this gap, we propose **CoTAR** (Core Token Aggregation-Redistribution), a centralized MLP-based module tailored to replace the decentralized attention. Instead of allowing all tokens to interact directly, as in attention, CoTAR introduces a global core token that acts as a proxy to facilitate the inter-token interaction, thereby enforcing a centralized aggregation and redistribution strategy. This design not only better aligns with the centralized nature of MedTS signals but also reduces computational complexity from quadratic to linear. Experiments on five benchmarks validate the superiority of our method in both effectiveness and efficiency, achieving up to a **12.13%** improvement on the APAVA dataset, with merely 33% memory usage and 20% inference time compared to the previous state-of-the-art. Code and all training scripts are available in this [**Link**](https://anonymous.4open.science/r/TeCh-24).", "tldr": "We propose a centralized module to replace decentralized attention in Transformer for centralized medical time series like EEG and ECG.", "keywords": ["EEG", "ECG", "Deep learning", "Transformer"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9f758ff7764fff5d49b184ac7755ea5f3beceaa.pdf", "supplementary_material": "/attachment/3e26e2c64b5e90905bfa6b5f44d0c4b32611cd34.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes CoTAR (Core Token Aggregation–Redistribution), a centralized alternative to self-attention for modeling medical time series (MedTS) such as EEG and ECG. The authors argue that MedTS signals are centrally coordinated (e.g., brain or heart acting as a signal source), while Transformer attention is inherently decentralized, making it ill-suited to capture the global dependencies between channels.\n\nTo address this, CoTAR introduces a central “core token” that aggregates global information from all tokens (channels) and redistributes it back via a lightweight MLP mechanism, achieving linear computational complexity. Combined with a dual tokenization strategy that separately encodes temporal and channel embeddings, the resulting model (TeCh) jointly captures both temporal and inter-channel dependencies.\n\nExtensive experiments across five MedTS datasets (EEG/ECG) and two human activity recognition (HAR) datasets show that TeCh achieves state-of-the-art accuracy and efficiency"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper convincingly articulates the mismatch between decentralized attention and the centralized nature of many physiological signals. This conceptual framing is both intuitive and novel for the MedTS domain.\n\n- CoTAR is a well-engineered module that reduces the quadratic cost of self-attention to linear, while retaining flexibility in cross-token communication.\n\n- Experiments span seven datasets (five MedTS + two HAR) with six evaluation metrics. The method consistently outperforms ten Transformer-based baselines.\n\n- Code and training scripts are publicly released.\n\n- Provide robust test, i.e., standard deviation."}, "weaknesses": {"value": "- The authors repeatedly assert that MedTS are “centralized” but provide no quantitative validation.\n- The paper omits direct comparisons with recent dual-dependency or TeCh-style models (e.g., GAFormer)\n- The proposed CoTAR conceptually resembles several prior Transformer modifications that employ global or auxiliary tokens to aggregate and redistribute information. The authors should discuss them. e,g,. CATS\n- Different datasets use different hyperparameters.\n\n\n\n\n\n\n\n[1] GAFormer: Enhancing time-series transformers through group-aware embeddings\n\n[2] CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables"}, "questions": {"value": "-  Is there a formal way to distinguish between centralized and non-centralized MedTS?\n- Can the core token be interpreted or visualized to correspond to physiological latent processes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ESdK6YTc2j", "forum": "oZJFY2BQt2", "replyto": "oZJFY2BQt2", "signatures": ["ICLR.cc/2026/Conference/Submission16498/Reviewer_XcRP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16498/Reviewer_XcRP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876608134, "cdate": 1761876608134, "tmdate": 1762926592638, "mdate": 1762926592638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the **CoTAR** (Core Token Aggregation-Redistribution) module to replace the attention module in medical time-series (MedTS) classification. This method is motivated by the assumption that MedTS signals typically originate from a centralized biological source. The design of the CoTAR module is inspired by client-server communication, which uses a core token to aggregate and exchange information between clients, rather than self-attention, where each token attends to all others equally. A new method called **TeCh** is proposed, aligned with the CoTAR module, similar to the Transformer architecture, but with attention replaced by CoTAR. Results are compared against 10 baselines across 5 MedTS datasets and 2 general time series datasets for classification, achieving SoTA performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method is motivated by MedTS's domain knowledge and inspired by server-client communication, which is a good approach. It is interesting to see that the linear complexity CoTAR module performs similarly to, and in some cases even better than, SOTA transformer methods. The comprehensive ablation study and direct comparison with the attention module are good and demonstrate the effectiveness of the CoTAR module."}, "weaknesses": {"value": "It is better to provide more detail in equal (2), as Figure 2 lacks notation for the variables used. I can get the idea of the core token being redistributed to each token, but reading the equal (2) is still a little confusing about the details. The performance on the ADFTD dataset is limited to the F1 score. Sometimes, a fixed subject-independent split makes it hard to demonstrate the superiority of a method, as specific subjects in the training set may contain too much noise and make results across methods similar. You could provide a cross-validated (5-fold or Monte Carlo) subject-independent evaluation result on the dataset, demonstrating the effectiveness of your method, even when performance is limited on a fixed split. Besides, more advanced SOTA methods, such as MedGNN, should be compared with."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hj3WJx0UZs", "forum": "oZJFY2BQt2", "replyto": "oZJFY2BQt2", "signatures": ["ICLR.cc/2026/Conference/Submission16498/Reviewer_KGK1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16498/Reviewer_KGK1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892898186, "cdate": 1761892898186, "tmdate": 1762926592263, "mdate": 1762926592263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a \"structural mismatch\" between Transformer models and Medical Time Series (MedTS) data like EEG and ECG. The authors argue that while MedTS signals originate from a centralized biological source (e.g., the heart or brain), the standard Transformer attention mechanism is decentralized, with all-to-all token interactions. This mismatch, they claim, makes it difficult for Transformers to model channel dependencies effectively.\n\nTo solve this, the paper proposes CoTAR, an MLP-based module designed to replace attention. CoTAR introduces a \"global core token\" that acts as a proxy. All tokens first aggregate information to this core token, which then redistributes the integrated information back to all tokens. This star-shaped architecture mimics the centralized nature of MedTS signals while reducing the computational complexity from quadratic to linear.\n\nThe full model, TeCh, uses CoTAR within a Dual Tokenization framework that processes the input in parallel: once with \"Temporal Embedding\" (patches of time) and once with \"Channel Embedding\" (whole channels as tokens).\n\nExperiments on five MedTS and two HAR datasets show that TeCh achieves state-of-the-art performance, significantly outperforming prior SOTA (Medformer) with large gains in efficiency (e.g., 33% memory and 20% inference time) and robustness to noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong, Intuitive Inductive Bias: The paper's primary strength is the clear and compelling motivation. The argument that decentralized attention is a poor structural match for centralized biological signals is an excellent insight and provides a strong foundation for the CoTAR module.\n\n2. Superior Performance: The proposed model achieves state-of-the-art results on a wide range of MedTS datasets, often by a significant margin over the previous SOTA, Medformer. This demonstrates the empirical effectiveness of the CoTAR module.\n\n3. Massive Efficiency Gains: The paper's most significant practical contribution is the efficiency of CoTAR. By reducing complexity from $O(S^2)$ to $O(S)$, the model achieves up to a 5x speedup in inference and a 3x reduction in memory usage compared to the prior SOTA. This is clearly visualized in Figure 4(a) and is critical for real-world medical applications.\n\n4. Improved Robustness: The experiments in Figure 4(b) provide strong evidence that CoTAR's centralized proxy design makes the model significantly more robust to noise in the input channels. This is a key practical advantage for noisy MedTS data.\n\n5. Thorough Evaluation: The experimental setup is strong, using 5 MedTS datasets (EEG and ECG) and 2 HAR datasets to show generalizability. The authors correctly use a subject-independent splitting protocol, which is crucial for clinically relevant MedTS evaluation."}, "weaknesses": {"value": "1. Misleading \"Dual Tokenization\" Framework: The paper's biggest weakness is the framing of the \"TeCh\" model around \"Dual Tokenization\". The SOTA results in Tables 2 & 3 are NOT achieved by a consistent dual-branch model. As Table 6 reveals, 4 of the 7 datasets (TDBrain, PTB, PTB-XL, FLAAP) use a single-branch model ($M=0$ or $N=0$) to get the reported results. This makes the \"Ablation Study on 'Dual Tokenization'\" (Table 4) highly misleading. The ablation's conclusion that \"combining both yields overall superior performance\" is cherry-picked (it's only true for 2/5 datasets) and contradicted by the final model's own hyperparameters. The paper should be reframed to present \"TeCh\" as a family of CoTAR-based models where the tokenization (Temporal, Channel, or Dual) is a hyperparameter to be tuned, rather than presenting \"Dual\" as the definitive architecture.\n\n2. Confusing Mathematical Notation: The formal definition of CoTAR in Equation (2) is unclear. The notations are a bit confusing, particularly the use of symbols to represent the matrices (e.g., both $O$ and $Co$ represent a single matrix/vector). And the function names do not follow a consistent notation (e.g., upright \"GELU\" vs. italicized \"GELU\" in different parts of the equation). Notably, the equations do not clarify the shape of the matrices/vectors involved, making it hard to follow the operations. A clearer, more consistent notation with explicit shapes would improve clarity."}, "questions": {"value": "1. Clarification of TeCh Architecture: The central framing of the paper is confusing. Table 6 shows that the SOTA results for TDBrain, PTB, PTB-XL, and FLAAP are achieved using a single-branch model ($M=0$ or $N=0$), not the \"Dual Tokenization\" model ($M>0$ and $N>0$) described in Section 4.2 and analyzed in Table 4.\n\n   - Could you confirm that the SOTA results in Tables 2 & 3 are achieved by tuning $M$ and $N$ and often setting one to 0?\n   - If so, why is the paper framed around \"Dual Tokenization\" as the primary architecture? This seems to misrepresent the final model and makes the \"Dual Tokenization\" ablation (Table 4) misleading. Wouldn't it be more accurate to present TeCh as a CoTAR-based model where the tokenization strategy (Temporal, Channel, or Both) is a key hyperparameter?\n\n2. Clarification of CoTAR Math (Equation 2): Could you please provide an unambiguous, step-by-step definition of the CoTAR module's computation? Specifically, what are the dimensions of all the weights and biases? A clear definition would resolve confusion about the module's precise mechanism.\n\n3. Mismatch in Ablation Results: There ablation studies are only performed on 5 of the 7 datasets and in a inconsistent manner. Is there a reason why the ablations were not performed on the other datasets? Including these would provide a more complete picture of the model's behavior across all evaluated datasets.\n\n4. Mismatch in Model Implementation: The paper provides the code link, but the imported names of the TeCh model seems to suggest that it uses the same Transformer encoder layer and not the CoTAR module. Since I cannot access the actual content of the files under the layers directory, could you clarify if the provided code implements the CoTAR module as described in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mdAxbqrEK2", "forum": "oZJFY2BQt2", "replyto": "oZJFY2BQt2", "signatures": ["ICLR.cc/2026/Conference/Submission16498/Reviewer_3453"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16498/Reviewer_3453"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978679536, "cdate": 1761978679536, "tmdate": 1762926591898, "mdate": 1762926591898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}