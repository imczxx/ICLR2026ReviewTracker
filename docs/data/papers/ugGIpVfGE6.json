{"id": "ugGIpVfGE6", "number": 12517, "cdate": 1758208356649, "mdate": 1759897504433, "content": {"title": "OWLEYE: ZERO-SHOT LEARNER FOR CROSSDOMAIN GRAPH DATA ANOMALY DETECTION", "abstract": "Graph structured data is commonly used to represent complex relationships such as transactions between accounts, communications between devices, and dependencies among machines or processes. Correspondingly, graph anomaly detection (GAD) plays a critical role in identifying anomalies across various domains, including finance, cybersecurity, manufacturing, etc. Facing the large-volume and multi-domain graph data, recent efforts aim to develop foundational generalist models capable of detecting anomalies in unseen graphs without retraining. To the best of our knowledge, the different feature semantics and dimensions of cross-domain graph structured data heavily hinders the development of graph foundation model, and leaves the further in-depth continual learning and inference capabilities in the evolving setting a quite nascent problem. To address these above challenges, we propose OWLEYE, a novel zero-shot GAD framework that learns transferable patterns of normal behavior from multiple graphs. Systematically, OWLEYE first introduces a cross-domain feature alignment module to harmonize feature distributions, which preserves domain-specific semantics during aligning more than the simple but widely-used Principle Component Analysis. Second, with aligned features, to enable method with continuous and scaling-up learning and inference capabilities, OWLEYE designs the multi-domain pattern dictionary learning to encode shared structural and attribute-based patterns. Third, for achieving the in-context learning ability, OWLEYE presents a truncated attention-based reconstruction module to robustly detect anomalies without requiring labeled data for unseen graph structured data. Extensive experiments on real-world datasets demonstrate that OWLEYE achieves superior performance and generalizability compared to state-of-the-art baselines, establishing a strong foundation for scalable and label-efficient anomaly detection.", "tldr": "", "keywords": ["anomaly detection", "zero-shot", "Multi-domain"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d808ab24a1e830dbca71c3dfbd9c83a14bd9f9ea.pdf", "supplementary_material": "/attachment/69e5d5d3a74237018efa34482a0fe6a0ce042225.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes OWLEYE, a framework for zero-shot graph anomaly detection across multiple domains. OWLEYE consists of three key modules: (1) cross-domain feature alignment to harmonize representations from graphs with heterogeneous features, (2) a multi-domain dictionary learning scheme to store and transfer both attribute and structure-based graph patterns, and (3) a truncated attention-based reconstruction module for robust anomaly inference in unlabeled, unseen graphs. Experiments demonstrate consistent performance gains over recent baselines, including state-of-the-art generalist and few-shot GAD methods, across a suite of diverse real-world graph benchmarks. The paper further investigates continual pattern augmentation and efficiency, positioning OWLEYE as a scalable, label-efficient, and extensible approach for cross-domain GAD."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**S1** The paper directly tackles the problem of cross-domain, zero-shot graph anomaly detection (GAD), a critical and timely challenge as graph datasets become more diverse and distributed in real applications.\n\n**S2** The proposed pipeline—starting with feature projection/normalization (Section 2.1), followed by hierarchical pattern learning (Section 2.2), and concluding with a detailed in-context truncated attention mechanism (Section 2.3)—is thoughtfully constructed and mathematically grounded. The technical exposition of each module is presented with clarity and includes relevant equations (e.g., Equation for cross-domain normalization, attention, loss terms), supporting reproducibility.\n\n**S3** The inclusion of comprehensive experimental results (see Table 1, Table 2, Table 6, Table 7), as well as meaningful visualizations (notably Figure 1), clearly demonstrate the effectiveness of OWLEYE in aligning cross-domain graphs and preserving crucial anomaly-detection patterns, something recent models like ARC and UNPrompt struggle with."}, "weaknesses": {"value": "**W1**While the method is described mathematically, there is little depth to the theoretical guarantees for convergence or for the effectiveness of the normalization and attention truncation mechanisms. For example, the normalization equations on Page 4 define a cross-domain scaling scheme, but the paper does not provide a formal argument or sufficient empirical diagnostic to show that this process consistently preserves key structural semantics across arbitrary domains, beyond the handful of visual examples (Figure 1). Similarly, the attention-truncation approach is intuitively appealing but lacks any statistical justification concerning which anomalies could be missed or mistakenly preserved when top-k truncation is used, particularly as graph scale increases or distributions shift.\n\n**W2**The core pattern storage and reconstruction process relies on “random sampling” nodes and computing truncated attention across dictionaries, but the sensitivity and robustness to this random selection are not systematically analyzed. Figure 4 visualizes attention distributions, but there is no supporting analysis as to how much predictive performance can vary across different random seeds or pattern samples, especially given the use of random support set extraction for the dictionary.\n\n**W3**The method is described as storing “interpretable patterns” in dictionaries, but no concrete analysis or visualization is provided showing what these patterns are, how they differ across domains, or whether they correspond to genuine “normal” behaviors as claimed. Figure 2 shows the architecture, and Figure 4 provides a heat map, but neither supplies users with diagnostics to interpret failures or understand the significance of stored patterns, this is especially important in high-stakes GAD applications.\n\n**W4**Although a broad set of baselines are used, the paper does not explain why several recently proposed cross-domain learning GNNs (many from the missing related work below) were not attempted as baselines, or even as ablation trims to OWLEYE.\nHyperparameter Sensitivity: While Table 10 (Appendix) selects some hyperparameter sweeps, there is little qualitative discussion of failure modes or special behavior, e.g., what happens if the size of the pattern dictionary is much smaller, or how performance degrades with heavy domain shift or label imbalance between training and test graphs.\n\n**W5**Some mathematical definitions, e.g., in the attribute-level and structure-level pattern extraction (Section 2.2 equations), could use more explicit notation (e.g., variable range explanations) for clarity, especially for readers less familiar with GCN-based operations.\n\n**W6**The cross-domain normalization claims median-based scaling (Equation Page 4) avoids domination by single datasets, but this is not shown in controlled experiments (e.g., single domain greatly divergent in distribution), how robust is this alignment when a domain is highly atypical? No “stress tests” or outlier cases are given."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i1IkojbUB0", "forum": "ugGIpVfGE6", "replyto": "ugGIpVfGE6", "signatures": ["ICLR.cc/2026/Conference/Submission12517/Reviewer_fjeF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12517/Reviewer_fjeF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761387253235, "cdate": 1761387253235, "tmdate": 1762923385488, "mdate": 1762923385488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel zero-shot graph anomaly detection framework called OWLEYE. The core idea of ​​OWLEYE is to learn and store representative patterns of normal behavior from multiple source graphs into a structured dictionary, thereby effectively detecting anomalies in unknown graph data without retraining."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a dynamic dictionary to store attribute-level and structure-level normal patterns, supporting incremental knowledge updates.\n\n2. Avoiding noise pollution from spurious normal node sampling is a crucial issue. Truncated attention mechanisms, as a soft filtering method, are more robust than random sampling or hard thresholding.\n\n3. Visualized feature analysis demonstrates a deep understanding of the data."}, "weaknesses": {"value": "1. This paper claims that the runcated attention mechanism \"filters out potential abnormal nodes,\" but it doesn't provide theoretical analysis or comparisons with other attention mechanisms.\n\n2. Dictionary learning mechanisms implicitly assume \"pattern transferability,\" and I have concerns about the general applicability of this assumption.\n\n3. PCA is an unsupervised linear dimensionality reduction method. It can only guarantee dimensionality uniformity, but it cannot guarantee deep semantic alignment of features from different domains. This seems to imply the same assumption as the previous question: \"normal patterns in different domains are similar.\""}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qqIS7lWPfk", "forum": "ugGIpVfGE6", "replyto": "ugGIpVfGE6", "signatures": ["ICLR.cc/2026/Conference/Submission12517/Reviewer_aCfo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12517/Reviewer_aCfo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921102638, "cdate": 1761921102638, "tmdate": 1762923385165, "mdate": 1762923385165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose OWLEYE, an in-context graph anomaly detection model. OWLEYE’s design is based on three main components: (1) a PCA-based projection with normalization to align features across graphs; (2) a multi-domain dictionary learning approach that extracts attribute and structural patterns from graphs using GNNs; and (3) an anomaly score computation method based on reconstructing input graph patterns using the dictionary. The experiments compare OWLEYE against multiple supervised, unsupervised, and in-context (ARC and UNPrompt) graph anomaly detection methods using eight traditional benchmarks. The results show that OWLEYE achieves the best results on 3/8 datasets in the zero-shot setting and 4/8 datasets in the 10-shot setting. Additional results include an ablation study, an efficiency analysis, and case studies on pattern augmentation, fine-tuning, and dictionary size."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a relevant and recent problem\n\n- The idea of using dictionary learning for in-context GAD is interesting\n\n- The proposed solution outperforms the baselines in most settings"}, "weaknesses": {"value": "- The paper writing could be greatly improved: Several parts of the paper concern me. Figure 1 is hard to read because of the small font and markers, and the overwhelming amount of information overall. The feature alignment method proposed in Section 2.1 is not well justified (why is it better than other alignment approaches?).  The loss from Equation 13 is also not well motivated (is it novel? Why is each element in the loss needed?). In general, Section 2 should include citations for all ideas that were inspired by previous work. Figure 3 can be misleading because some of the differences seem to be lower than 1%. The related work is very brief and doesn’t cover relevant literature, such as cross-domain graph anomaly detection, which can be unsupervised. \n\n- The problem setting is not completely clear: It is not clear what information about the test graph is available. In NLP, in-context learning usually assumes that only examples are given (not the entire dataset). However, this paper seems to assume that the entire graph is available during testing. The availability of the test graph makes a big difference, even without labels, as shown in the research on unsupervised GAD. My intuition is that in this setting, it makes more sense to start from an unsupervised method and then try to improve it by incorporating additional information from other labeled datasets (the graph topology is likely more informative than the 10 examples given as in-context input). Notice that if the test graph is not fully observed, the problem becomes much harder, and the approach I described does not work. \n\n- The experimental results need better insights: In the ablation study, it is not clear what the replacement is for each component of OWLEYE. For instance, ARC applies SVD and ordering instead of PCA and normalization for feature alignment. Anomaly score is computed using attention over in-context examples in ARC. The paper should make a case that each component of ARC is better than the alternative. It is also unclear why 200 patterns are sufficient to identify the anomalies; visualizing the patterns and anomalies could be helpful to understand what is happening (not clear if that is what is done in Fig. 1). There is no discussion on training time, which could be a deciding factor for practitioners."}, "questions": {"value": "1) Why is the feature alignment method proposed in Section 2.1 better than existing approaches?\n\n2) Is the loss from Equation 13 novel? What is the motivation?\n\n3) What are the values (numbers) in Figure 3?\n\n4) What information about the test graph is available in the problem setting considered?\n\n5) What is the replacement used for each component of OWLEYE in the ablation study? Are these components the state of the art?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rJE4vjQJks", "forum": "ugGIpVfGE6", "replyto": "ugGIpVfGE6", "signatures": ["ICLR.cc/2026/Conference/Submission12517/Reviewer_fw57"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12517/Reviewer_fw57"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938670692, "cdate": 1761938670692, "tmdate": 1762923384858, "mdate": 1762923384858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OWLEYE, a zero-shot cross-domain graph anomaly detection (GAD) model designed to establish a universal GAD framework. The core contribution lies in designing a novel cross-domain feature alignment module to address the limitations of existing generalist models when aligning graph data across different domains. Additionally, the model identifies anomalies through dictionary learning and a truncated attention-based reconstruction mechanism. Experimental results demonstrate superiority over baseline methods across multiple datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Zero-shot cross-domain graph anomaly detection is a significant and practical problem. The paper decomposes this challenge into three stages—feature alignment, pattern learning, and anomaly detection, addressing each sequentially to ensure a technically sound overall solution.\n2. The paper clearly identifies the shortcomings of existing general GAD models in feature alignment. It proposes a well-motivated, novel, and effective solution.\n3. Extensive experiments across multiple datasets demonstrate that OWLEYE significantly outperforms various state-of-the-art baselines. The case study on continuous learning also preliminarily shows the model's potential to absorb new knowledge in a plug-and-play manner."}, "weaknesses": {"value": "1. The overall innovation of the framework is incremental. Its technical pipeline from feature alignment and multi-hop residual aggregation to a multi-pattern dictionary similar to “context learning” largely follows the established paradigm of generalist GAD models.\n2. Section 2.2 thoroughly argues for using “only structural similarity” (Equation 10) to address “camouflaged” anomalies. However, the final reconstruction formula (12) employs an undefined attribute-based similarity `sim(G, Dict_H)` during attribute reconstruction, contradicting prior descriptions (its own design rationale).\n3. The paper fails to explicitly describe the specific implementation of the “standard attention” variant in ablation study, nor does it provide a concrete analysis of its significant performance decline."}, "questions": {"value": "1. The paper employs PCA for feature projection but offers no justification for choosing this method over other nonlinear or domain adaptation techniques. Furthermore, how is the situation handled if the feature dimension of the original graph is smaller than the preset projection dimension `d`?\n\n2. During inference, the dictionary size and the `k` value in truncated attention are critical hyperparameters. Could the paper provide guiding principles or sensitivity analysis for selecting these parameters for new tasks? Were inference hyperparameters unified across all test sets in comparative experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U8d75OqLNg", "forum": "ugGIpVfGE6", "replyto": "ugGIpVfGE6", "signatures": ["ICLR.cc/2026/Conference/Submission12517/Reviewer_wzbY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12517/Reviewer_wzbY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954288987, "cdate": 1761954288987, "tmdate": 1762923384220, "mdate": 1762923384220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}