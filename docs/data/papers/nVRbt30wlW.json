{"id": "nVRbt30wlW", "number": 22780, "cdate": 1758335357669, "mdate": 1759896846539, "content": {"title": "LVstyler: LoRa-enhanced Varied High-Quality Texture Generation with Text Alignment", "abstract": "We introduce LVstyler, an innovative generative framework that extends beyond the conventional 0-to-1 UV texture synthesis, specializing in a more varied style transfer for 3D meshes while preserving geometric fidelity. The core challenge lies in maintaining style consistency across complex 3D surfaces without introducing style-agnostic artifacts. Other methods leverage a pre-trained texture generation model, which primarily relies on a diffusion model, producing an initial texture map. However, due to the limited styles that a simple pre-trained diffusion-based model can generate for objects, these methods can only handle short and object-based prompts, rather than styling prompts. Therefore, we integrate an optimization-based texture generation model in the image space, specifically modifying it with two LoRA extensions for shape consistency and UV map space adaptation. Through this technique, LVstyler can produce varied high-quality UV textures that allow more imagination through the detailed styling text guidance, significantly advancing the state-of-the-art in texturing 3D objects.", "tldr": "We introduce LVstyler, an innovative generative framework that extends beyond the conventional 0-to-1 UV texture synthesis, specializing in a more varied style transfer for 3D meshes while preserving geometric fidelity.", "keywords": ["Computational Photography", "Image & Video Synthesis", "3D Texture", "Continuous Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8b2a1163c9b60bae77fc05e5ebf2b9ebd7ab62d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes LVstyler, a LoRA-enhanced framework for generating varied UV textures for 3D meshes guided by text prompts. The method introduces a two-stage training pipeline, where a Shape LoRA module learns style transfer in the image space, and a UV LoRA module adapts it to UV map space for consistent 3D texture generation. The goal is to enable high-quality, style-consistent text-driven texture synthesis for 3D assets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper explores the integration of LoRA fine-tuning into texture generation, which is lightweight and potentially reusable.\n\n2. The two-stage training strategy (Shape LoRA + UV LoRA) is conceptually clean and modular"}, "weaknesses": {"value": "1. Experimental results are weak: qualitative samples show blurry textures and inconsistent styles, and quantitative evaluations are based on very limited data. Claims such as “high-quality” and “state-of-the-art” are not supported by strong baselines or convincing comparisons.\n\n2. The paper’s writing and structure are unfocused, with unclear motivation and overemphasis on engineering details (loss terms, LoRA configurations) rather than conceptual insights."}, "questions": {"value": "The visual results in Figures 4 show noticeable style inconsistency and blurriness. Could the authors provide quantitative or perceptual metrics (e.g., CLIP-Score or user studies) to justify the claimed “high-quality texture generation”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lsmroEbqPo", "forum": "nVRbt30wlW", "replyto": "nVRbt30wlW", "signatures": ["ICLR.cc/2026/Conference/Submission22780/Reviewer_hfHd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22780/Reviewer_hfHd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918361245, "cdate": 1761918361245, "tmdate": 1762942384674, "mdate": 1762942384674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents LVStyler, a method for producing high-quality UV textures which are closely aligned with the guiding prompts. This is achieved by performing stylization directly on the UV maps. The method integrates two LoRA modules (a shape LoRA and a UV LoRA) on top of a base styling model. This allows the method to achieve stylization while maintaining shape consistency. The method produces textures that are more aligned with the styling prompts as compared to the selected baseline methods. The method is evaluated qualitatively with comparison figures. The quantitative evaluation consists of the reported metrics and a user study."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed approach improves the alignment to the specific styling prompts.\n- Since the method is designed to take an initial UV map and output a stylized UV map, the approach is independent of the texture generation method and can be used on top of any existing or future texture generation method to more precisely style UV maps.\n- The method achieves the highest scores in almost all metrics when compared with the selected baselines methods and the qualitative comparisons also show better performance."}, "weaknesses": {"value": "Some newer baseline methods are missing from the comparisons.\n- SyncMVD [1] and TEXGen [2] both perform well and should be included in the comparisons. The authors note that TEXGen was not compared to because its code was not open sourced, however, from what I can tell, the code has been open sourced since December 2024 (https://github.com/CVMI-Lab/TEXGen) so it should be possible to compare with their method.\n- Additionally, while it is not required to compare to MVAdapter [3] (ICCV 2025) and Hunyuan3D-Paint-v2-1 [4] (ArXiv) since they were not officially published at the time of ICLR submission, both have released open source code before the submission deadline and I think that the paper would be strengthened by comparing to these methods as well.\n- Since LVstyler works as an \"add-on\" to some existing UV texture generation (taking poorly style aligned UV map and improving its style alignment) comparisons to these newer, better methods is important as it is not clear that the quality of the newer methods still needs this refinement.\n\nWhen calculating the inference time for LVstyler, my interpretation is that the time to computing the initial texture produced by the existing texture inpainting method is not included. If this is the case, it seems unfair to compare inference time with other methods that start from an image or text.\n\nReferences:\n[1] Liu, Yuxin, et al. \"Text-guided texturing by synchronized multi-view diffusion.\" SIGGRAPH Asia 2024 Conference Papers. 2024.\n[2] Yu, Xin, et al. \"Texgen: a generative diffusion model for mesh textures.\" ACM Transactions on Graphics (TOG) 43.6 (2024): 1-14.\n[3] Huang, Zehuan, et al. \"Mv-adapter: Multi-view consistent image generation made easy.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025.\n[4] https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1"}, "questions": {"value": "Could the authors provide further explanation as to why they choose to perform the stylization on the UV map after the multi-view images are already generated as opposed to directly within the multi-view generation component?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g7CgnKneXe", "forum": "nVRbt30wlW", "replyto": "nVRbt30wlW", "signatures": ["ICLR.cc/2026/Conference/Submission22780/Reviewer_fCJ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22780/Reviewer_fCJ1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988561735, "cdate": 1761988561735, "tmdate": 1762942384189, "mdate": 1762942384189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LVstyler, a novel generative framework designed to apply varied and high-quality stylistic textures to 3D meshes by operating on their existing UV maps. The method extends a 2D image stylization model (inspired by CLIPstyler) with two specialized Low-Rank Adaptation (LoRA) modules. The training is conducted in two stages: first, a \"Shape LoRA\" is trained to preserve geometric details and structural consistency using a Laplacian-based loss. Second, a \"UV LoRA\" is trained to adapt the stylization process to the specific domain and layout of UV texture maps. The authors conduct experiments against several baselines, demonstrating superior results in both quantitative metrics and user studies, and also contribute a new dataset (LVstyle) for this task."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is highly efficient in terms of computational cost, demonstrating significantly lower GFLOPs (16.25) and a much faster inference time (65.21s) compared to all diffusion-based baselines.\n\n2. The two-stage LoRA optimization framework is an intelligent way to adapt a 2D model to the 3D UV domain, successfully disentangling the goals of shape preservation and style adaptation.\n\n3. The paper is supported by a strong and thorough experimental evaluation, including both quantitative metrics and a user study, which validates its superior performance on text fidelity and visual quality.\n\n4. The authors have collected and are releasing a new dataset, LVstyle, which comprises raw, masked, and styled UV maps to facilitate future research in this specific area."}, "weaknesses": {"value": "1. The primary weakness is the limited significance of the problem itself. The method focuses on re-styling or refining existing UV textures, which is a niche and incremental problem. This feels more like a clever engineering solution for a specific post-processing step rather than fundamental research addressing a core challenge in generative modeling.\n\n2. The model's performance is fundamentally dependent on the capabilities of its base 2D stylization model, CLIPstyler. Any limitations of this base model in terms of style diversity or content preservation are directly inherited.\n\n3. The reliance on CLIP for textual guidance serves as a bottleneck, as the authors concede. The model may struggle with styling prompts that are highly abstract, culturally specific, or require complex spatial reasoning that falls outside CLIP's training distribution.\n\n4. The paper introduces a large number of loss weights (e.g., $\\lambda_{dir}$, $\\lambda_{patch}$, $\\lambda_{c}$, $\\lambda_{tv}$, $\\lambda_{Laplacian}$) but provides no hyperparameter ablation study. The sensitivity of the model to these many parameters is unknown, and their chosen values are not justified."}, "questions": {"value": "1. The paper focuses on an optimization-based approach. How would this method compare to a feed-forward approach, such as fine-tuning a diffusion model (e.g., ControlNet) on the LVstyle dataset you collected, using the original UV map as a condition? This seems like a more direct path to solving the same problem.\n\n2. The paper mentions CLIP's limitations but does not provide qualitative failure cases. What happens when a styling prompt is semantically very distant from the original object's geometry (e.g., styling a \"trolley cart\" with the prompt \"a cart made of flowing water and light\")?\n\n3. The two-stage training process is well-motivated. Did you experiment with a simpler, joint-training approach where both the Shape LoRA and UV LoRA modules are optimized simultaneously? How critical is the sequential, two-stage process to the final quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pSexcHjP2c", "forum": "nVRbt30wlW", "replyto": "nVRbt30wlW", "signatures": ["ICLR.cc/2026/Conference/Submission22780/Reviewer_UvCE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22780/Reviewer_UvCE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015075508, "cdate": 1762015075508, "tmdate": 1762942383846, "mdate": 1762942383846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LVstyler, a two-stage, LoRA-enhanced framework for UV texture stylization on 3D meshes. It first performs CLIP-guided online learning in the image space (ISS) to acquire style weights, then injects them into a lightweight network equipped with Shape LoRA for edge/structure preservation and UV LoRA for UV-space adaptation, enabling varied, text-driven styles while maintaining geometric fidelity. Experiments on Objaverse/ModelNet40 with a curated UV dataset report improved FID/LPIPS and Aesthetic scores, competitive CLIP alignment, and markedly lower GFLOPs and inference time versus Latent-Paint, TEXTure, Text2Tex, and Paint3D; user/VLM studies further support the perceived quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. Operates directly in UV space with LoRA constraints, reducing reliance on heavy diffusion back-projection and improving efficiency.\n\nS2. The split of Shape LoRA and UV LoRA is well-motivated, offering a clear mechanism to balance structure preservation and stylistic diversity.\n\nS3. Provides a practical pipeline and dataset for industry-style workflows, with strong runtime and compute advantages alongside competitive quality."}, "weaknesses": {"value": "W1. The method appears to presuppose clean, fixed UV mapping and to operate purely in texture space. Any substantive geometric modification would typically invalidate the original UV parameterization and thus the training and inference assumptions. Please clarify whether the approach is strictly a stylization pipeline or if you have a mechanism to remain valid under geometry changes.\n\nW2. I wonder how the authors obtain the GT data for stylization. Please clarify what you consider “ground truth,” how it is produced or curated, and whether it is true pixel-level GT or pseudo-GT derived from renderings.\n\nW3. Please explain why PSNR decreases after the second stage, as shown in Table 2. Specify the reference used for PSNR (original texture vs. stylized target vs. rendered views) and discuss the trade-off between style strength and reconstruction fidelity."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dl8OTIxTmS", "forum": "nVRbt30wlW", "replyto": "nVRbt30wlW", "signatures": ["ICLR.cc/2026/Conference/Submission22780/Reviewer_TpZN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22780/Reviewer_TpZN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762226473125, "cdate": 1762226473125, "tmdate": 1762942383594, "mdate": 1762942383594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}