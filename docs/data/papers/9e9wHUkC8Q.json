{"id": "9e9wHUkC8Q", "number": 7461, "cdate": 1758023155891, "mdate": 1759897851554, "content": {"title": "MANAR: Memory-augmented Attention with Navigational Abstract conceptual Representation", "abstract": "Transformers - and their multi-head attention (MHA) core - power today's leading models across a broad application spectrum. Yet MHA contextualizes each token through explicit, pair-wise interactions with every other token, yielding quadratic time/space cost and an unbounded, linearly-growing context. This \\textit{direct all-to-all} modeling is both the source of attention's expressiveness and a barrier to scaling. We address this bottleneck by augmenting attention with a trainable external memory that stores both conceptual and relational general representations learned during training. For every input, lightweight scalable retrieval produces a fixed-size set of memory retrieved concepts whose values are fused into a compact Abstract Conceptual Representation (ACR). Tokens then attend jointly to (i) the global, concept-level ACR and (ii) a short local context, completely sidestepping all-to-all token interactions. The result is a non-convex pathway that provide the model with an \\textit{out-of-the-box thinking} contextualization - i.e., beyond the convex hull spanned by the input values - while reducing complexity to linear time and memory. Integrated as a drop-in replacement for MHA, our layer (MANAR) preserves accuracy on ImageNet-1K (82.3\\% top-1 with a DeIT-B backbone) and LibriSpeech (2.9/6.8\\% WER on test-clean/other) yet cuts inference latency by up to 14.8x and peak GPU memory by up to 9.3x as sequence length grows to 4K. A simple weight-copy knowledge-transfer procedure trims training cost by $\\approx$99\\% versus training from scratch. Finally, Convex Hull Membership (CHM) tests show that $>$50\\% of MANAR’s outputs lie outside the convex span of the input values, quantitatively confirming its out-of-the-box contextualization.", "tldr": "", "keywords": ["brain inspired", "memory-augmented architecture", "attention", "out-of-the-box thinking", "imagenet", "librispeech", "quadratic complexity"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63ea5e4ceb3595103636cc7984d57c2115ee47b1.pdf", "supplementary_material": "/attachment/b499e46c9b45ebe11b398880ad3ec4a736cb07ca.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces MANAR, a drop-in replacement for MHA that combines local token attention with a retrieved, fixed-size Abstract Conceptual Representation (ACR) from a trainable external memory. The goal is to mitigate the quadratic time/space cost of standard attention. Reported results include up to 14.8× latency and 9.3× peak-memory reductions at 4k tokens, “linear” scaling for fixed windows, ImageNet-1K top-1 = 82.3% (DeiT-B capacity), and LibriSpeech WER 2.9/6.8.\n\nAlthough MANAR sounds promissing, the empirical evidence is not convincing. Experiments were conducted only on two tasks, including ImageNet classification and speech recognition. Missing comparison with other baselines e.g., sparse transformers, Linformer, or other linear attention methods. Experiments in the language domain would substantially strengthen the claim."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea is clearly present: a unification of retrieved global context (ACR) with local attention to avoid all-pairs attention.\n- Efficiency: Substantial wall-clock and HBM savings in microbenchmarks and end-to-end DeiT-S at large resolutions, with improvements growing with sequence length.\n- MANAR enables quick adoption and a large reduction in trainable parameters/steps while retaining accuracy on vision and speech."}, "weaknesses": {"value": "- **Modest accuracy gains:** On ImageNet-1K, improvements over DeiT-B are small (82.3% vs. 81.8%). For ASR, the paper claims SOTA, but test-clean 2.9 trails data2vec (2.8) and test-other is tied at 6.8.\n- **Related work gaps:** While Linformer/Performer and long-sequence families (Mamba/RetNet, KV-cache management) are cited, several key lines are missing or under-discussed: sparse attention baselines, Swin/local-window ViTs, Transformer-XL/Compressive Transformer, standard retrieval-augmented modeling for LMs, and Memory Transformer.\n- **Lack of empirical experiments:** See questions below."}, "questions": {"value": "- How is the accuracy vs latency curve across different values of $L$?  How does the number of retrieved memory concepts affect the performance?\n- The speedups rely on a custom fused Triton kernel. There is no ablation quantifying how much of the gain comes from kernel engineering rather than from the MANAR design itself. \n- The modified paper layout appears to create extra space; please adhere to the venue’s formatting rules."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uMwSctM0l8", "forum": "9e9wHUkC8Q", "replyto": "9e9wHUkC8Q", "signatures": ["ICLR.cc/2026/Conference/Submission7461/Reviewer_6pDP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7461/Reviewer_6pDP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812725653, "cdate": 1761812725653, "tmdate": 1762919574211, "mdate": 1762919574211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper not reviewed due to formatting issues."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "-"}, "weaknesses": {"value": "-"}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "fAlxHyFP6d", "forum": "9e9wHUkC8Q", "replyto": "9e9wHUkC8Q", "signatures": ["ICLR.cc/2026/Conference/Submission7461/Reviewer_dhQL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7461/Reviewer_dhQL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762665018655, "cdate": 1762665018655, "tmdate": 1762919573493, "mdate": 1762919573493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "N/A, see ethics comment & 'Weaknesses' section"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "N/A, see ethics comment & 'Weaknesses' section"}, "weaknesses": {"value": "As pointed out at the beginning of the reviewing phase, the margins of the paper unfortunately appear to have been significantly altered, which allows more space than the original template.\n\nI have to therefore recommend desk-rejection / rejection due to misuse of format."}, "questions": {"value": "N/A, see ethics comment & 'Weaknesses' section"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)", "Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "As pointed out at the beginning of the reviewing phase, the margins of the paper unfortunately appear to have been significantly altered, which allows more space than the original template.\n(See 'official comments' and corresponding exchange with AC and other reviewer.)\n\nI have to therefore recommend desk-rejection."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8pNV1pWdnS", "forum": "9e9wHUkC8Q", "replyto": "9e9wHUkC8Q", "signatures": ["ICLR.cc/2026/Conference/Submission7461/Reviewer_LXgz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7461/Reviewer_LXgz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762668512286, "cdate": 1762668512286, "tmdate": 1762919572971, "mdate": 1762919572971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}