{"id": "q0UEl3xAIZ", "number": 8890, "cdate": 1758101435389, "mdate": 1759897756508, "content": {"title": "Goal-oriented state reduction of unknown game dynamics to produce effective strategies", "abstract": "Recent reinforcement learning agents perform excellently by generating an internal representation of information crucial for predicting outcomes through huge experience, but do not clarify what essential information (core) is extracted in their representation. A model-based reinforcement learning algorithm, Goal-Oriented Environment Inference (GOEI), has been proposed to solve this issue, and its ability to explicitly learn such core states has been demonstrated in an abstract environment. Here, we validated the ability of GOEI in a more realistic environment, i.e., a competitive card game “Hol’s der Geier (The Vulture Gets It).” To our surprise, it achieves a nearly optimal strategy equivalent to the Nash equilibrium by using core states reduced only to 2.9\\% (452 states) of all possible observations (15,542). These results demonstrate that GOEI effectively excludes information irrelevant to game outcomes, thereby significantly reducing the memory burden.", "tldr": "", "keywords": ["Competitive Games", "Model-Based Reinforcement Learning", "Core State Extraction", "Variational Inference"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb4df785d8dcc73c174da0f5d029e30e30e058df.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores application of Goal-Oriented Environment Inference to state space reduction problem in card boardgames, especially in two-player game called \"Hol’s der Geier (The Vulture Gets It)”. Authors test how well will GOEI perform against Nash equilibrium random opponents under different set of hyperparameters."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Experiment that were conducted on \"Hol’s der Geier (The Vulture Gets It)” are solid and shows results for different sets of hyperparameters as well as performance trends for these sets."}, "weaknesses": {"value": "- Work uses only single environment to conduct its experiments, which do not provide any evidence on how GOEI will perform on other environments of the same type or other types of environments.\n- Conducted experiments compare GOEI only with DQN out of all RL algorithms. While GOEI is model-based RL algorithm and especially focused on state representation learning, it's crucial to compare it with other MBRL algorithms (i.e. [Dreamer v3](https://arxiv.org/abs/2301.04104), [DreamerPRO](https://proceedings.mlr.press/v162/deng22a.html) and others)  \n- Work do not propose either novel algorithms nor experimental benchmarks. The algorithm (GOEI) were already [published](https://doi.org/10.1016/j.neunet.2024.106246) and used as-is with citation."}, "questions": {"value": "- What is the contribution of the paper aside from applying GOEI to \"The Vulture Gets It\"?\n- Is there any experiments conducted on other environments or comparison with other model-based algorithims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jRUBi54bCG", "forum": "q0UEl3xAIZ", "replyto": "q0UEl3xAIZ", "signatures": ["ICLR.cc/2026/Conference/Submission8890/Reviewer_PD4Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8890/Reviewer_PD4Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394947101, "cdate": 1761394947101, "tmdate": 1762920647215, "mdate": 1762920647215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates Goal-Oriented Environment Inference (GOEI), a model-based reinforcement learning algorithm designed to extract minimal, goal-relevant state representations (“core states”) from redundant observations while preserving the agent’s ability to predict outcomes. Previous work demonstrated GOEI’s capacity for abstraction in simplified environments.\nThis paper extends GOEI to a realistic competitive game setting, specifically the two-player card game Hol’s der Geier (The Vulture Gets It). The authors show that GOEI can reduce the number of state representations to only 2.9% of all possible observations while achieving a near-Nash-equilibrium performance, outperforming Q-learning in both efficiency and interpretability. The results suggest that GOEI can identify causally important features and drastically reduce representational complexity in competitive settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Demonstrates GOEI’s effectiveness on a non-trivial, stochastic, competitive game\n2. Presents clear comparative experiments with baselines such as Q-learning and Nash equilibrium strategies, along with ablations over hyperparameters (α, β).\n3. The evaluation includes entropy-based measures and mutual information analyses, providing a multi-faceted view of representational compactness."}, "weaknesses": {"value": "1. The evperiment is confined to a simplified (five-card) version of Hol’s der Geier, which restricts claims about scalability and general applicability to larger, more complex environments.\n2. While GOEI achieves state compression, the interpretability of the resulting “core states” remains opaque. The paper acknowledges this but does not propose methods to make reduced representations human-comprehensible.\n3. While the experimental validation is thorough and provides useful empirical insights, this work is primarily an application and validation of the existing GOEI method in a new, more realistic environment (the competitive card game Hol’s der Geier). So it is lack of methodological novelty."}, "questions": {"value": "No question for the work done in this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yH0dzW5C65", "forum": "q0UEl3xAIZ", "replyto": "q0UEl3xAIZ", "signatures": ["ICLR.cc/2026/Conference/Submission8890/Reviewer_sNkK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8890/Reviewer_sNkK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944285192, "cdate": 1761944285192, "tmdate": 1762920646884, "mdate": 1762920646884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The reviewed work evaluates a model-based reinforcement learning algorithm, \"Goal-Oriented Environment Inference\" on a two player zero sum game called \"Hol's der Geier\" (The Vulture Gets It). \n\nMuch of the paper is concerned with explaining the game and common strategies for it. Then, it quickly introduces the problem of state reduction and describes the previously introduced Goal-oriented environment inference (GOEI) methodology."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The authors seem to do a thorough job in investigating the dependence of the proposed methodology on the different parameters."}, "weaknesses": {"value": "- The writing is overall not sufficient to convey the technical ideas. While a lot of real estate is spent on introductoy material, the core of the evaluated methodology is introduced hastily and with excessive amounts of vague language (\"this approach is ensurred by Bayes theory in which Bayesian\ninference is independent of the true causal direction.\" or \"where P (·) represents true probability distribution, while the agent model is differently denoted by p(·) in the following.\"\n\n- The evaluation is limited, partly by being restricted to just a single game and partly by claiming success by virtue of achieving nominal performance only when playing against the Nash Equilibrium\n\n- Even if it were executed perfectly, the contribution of investigating the performance of a known method on a single new game seems insufficient for an ICLR paper."}, "questions": {"value": "1. What do you mean by \"this approach is ensurred by Bayes theory in which Bayesian\ninference is independent of the true causal direction.\" ? \n\n2. What exactly is $p(s_t, o_t)$? What is \"The state reduction model'? \n\n3. You write \n\"If an agent based on a reduced state representation could compete equally\nwell against the NE opponent, then the agent is considered to have successfully learned core information to win the game.\"\n\nMany Nash Equilibrium strategies amount to neutralizing the opponent's agency over the average outcome of the game (think, the 50-50 strategy in matching pennies). Thus, this claim seems fundamentally unjustified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xFSm6P3Xj3", "forum": "q0UEl3xAIZ", "replyto": "q0UEl3xAIZ", "signatures": ["ICLR.cc/2026/Conference/Submission8890/Reviewer_dA92"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8890/Reviewer_dA92"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762029920858, "cdate": 1762029920858, "tmdate": 1762920646588, "mdate": 1762920646588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}