{"id": "2LcxmMKURb", "number": 13523, "cdate": 1758218899196, "mdate": 1759897431125, "content": {"title": "Power and Limitations of Aggregation in Compound AI Systems", "abstract": "When designing AI systems for complex tasks, it is becoming increasingly common to aggregate multiple copies of the same model to create a compound AI system. Given that the homogeneity of these models, this raises the question of when aggregation unlocks greater performance than querying a single model. In this work, we mathematically study the power and limitations of aggregation within a stylized principal-agent framework. This framework models how the system designer can partially steer each agent's output through reward specification, but still faces limitations in prompt engineering ability and model capabilities. To analyze the power of aggregation, we characterize when an aggregation operation expands the set of outputs that the system designer can elicit. We also analyze the limitations of aggregation, showing tight conditions under which aggregation does not expand the set of elicitable outputs, regardless of the level of limitations of prompt engineering ability. Finally, we apply these characterizations to simple intersection-based and union-based aggregation rules. Altogether, our results take a step towards characterizing when compound AI systems can overcome limitations in model capabilities and in prompt engineering.", "tldr": "We theoretically study the power and limitations of aggregation in compound AI systems by building on a principal-agent framework.", "keywords": ["Principal–agent games", "Multi-agent reward design", "Theory of compound AI systems"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f688775c9b08335907dcabfc6c22f21665c9bf56.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Bad paper.  This paper attempts to present a theoretical framework for understanding why aggregating multiple responses from the same language model can lead to improved performance compared to using a single response.  The authors propose a formalism where each response is modeled as an argmax of a hidden reward function under a set of constraints, representing the model’s internal optimization.  They claim that aggregation leads to better outcomes through one of three mechanisms: **feasibility expansion**, **support expansion**, or **binding set contraction**—that is, the aggregated response lies outside the feasible set of any single response, activates more “coordinates,” or relaxes binding constraints.  The paper also provides several definitions, a “limitation-characterizing condition,” and theorems that restate these mechanisms as necessary or sufficient conditions for aggregation to yield improvement.\n\nBecause the paper lacks grounding, insight, and professional presentation, I recommend **rejection**.  It may be salvageable only if the authors (i) re-motivate the setting with realistic assumptions about LMs, (ii) connect their abstract constructs to measurable phenomena, and (iii) include experiments or simulations to demonstrate empirical relevance.  Otherwise, the work remains a purely definitional exercise detached from both theory and practice."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Ambition and clarity of intent.**  \n  The paper aims to address an important conceptual question: *why* multi-sample aggregation often improves large language model outputs.  In doing so, it attempts to explain test time compute scaling, a known power empirical LLM phenomena.\n\n- **Formal framing.**  \n  The authors present clear mathematical definitions and formal statements.  The structure (definitions, theorems, proofs) is coherent.\n\n- **Topic relevance.**  \n  Aggregation and inference-time scaling are active areas of research in 2025 (e.g., Huang et al., *Is Best-of-N the Best of Them?*, 2025).  Exploring these through theory could be valuable if grounded in actual LM behavior."}, "weaknesses": {"value": "This paper has many problems and reflects bad research taste.\n\n**1. Implausible assumptions and circular abstractions.**  \nThe biggest hindrance to one's comprehension of the paper is that the core modeling assumptions are not defensible:\n\n- Each user prompt is assumed to encode a *hidden reward function* that the model optimizes.  \n  This is unrealistic both for human communication and for current LM architectures.  Tell me: what is the reward function that I am implicitly implying to you when I'm writing this sentence?\n- Each model response is assumed to be the *argmax* of that hidden reward.  \n  But language models are stochastic samplers, not deterministic solvers of constrained optimization problems.  Furthermore, there's no reason to think LMs are able to always find those argmax. \n- Each response is represented as an $M$-dimensional “feature vector” whose coordinates correspond to abstract attributes such as *truthfulness* and *politeness*.  The authors then propose operations such as coordinate-wise min or convex combination over these vectors.  \n  In other words, if we have two responses, one .9 polite but .1 truthful and the other .1 polite but .9 truthful, then the authors think that there is a way to combine the two responses into something scoring .5 on both truthful and polite dimensions or a response that is .1 polite and .1 truthful.  I am not sure how this combination can be done in practice.  The reason why I am asking ChatGPT a question is obviously that I don't know the true answer to the question.  But in order to precisely take out the truthful portions from the two responses selectively and combine them into one with the right ratio, one probably needs to have some ground truth access to the actual answer in the first place.\n- Overall, it's unclear to me how the principal-agent framework (which I personaly do like a lot) can possibly be relevant in this context.  \n\nTogether these assumptions detach the framework from how LLMs actually operate.  \n\n---\n\n**2. No empirical or synthetic validation.**  \nThe paper claims to model mechanisms of improvement in “compound AI systems,” but as one can expect from reading the unrealistic definitions, it presents no data, experiments, or even toy simulations.  This is a pathology with entirely imaginary theoreticians who are not grounded in the real practice of LMs.\nNo synthetic example demonstrates that the proposed mechanisms reproduce empirically observed best-of-$N$ effects or other aggregation algorithms.\nWithout grounding, it is impossible to judge whether the theoretical constructs correspond to measurable LLM behavior.\n\n---\n\n**3. Weak or trivial insight.**  \nOnce the readers managed to convince themselves, with the suspension of disbelief, into reading the actual theory, it turns out that the paper contains trivialities.  The main takeaway—“aggregation helps by expanding feasibility/support or relaxing constraints”—really cannot be the most insightful or surprising thing in the world.  The proofs are essentially *definition chasing* and do not reveal deep or counterintuitive explanations about the empirical phenomena.  The framework offers no testable predictions, substantive implications, or connections to practical design of aggregation algorithms.\n\n---\n\n**4. Weird and uncheckable theoretical condition making the proofs easy.**  \nThe “limitation-characterizing condition” (Def. 4.2) is opaque and lacks algorithmic interpretability.  There is no procedure to determine whether real models (e.g., GPT-4o, Gemini-2.5-Pro) satisfy it.  This makes the results unverifiable and undermines the claim that the theorems capture meaningful necessary conditions.  In fact, it appears the condition was reverse-engineered to make the proofs straightforward rather than to capture real constraints.  That also aggreviates the problem that the math here is neither elegant nor hard.\n\n---\n\n**5. No connection to existing aggregation literature.**  \nThe paper ignores prior work that analyzes best-of-$N$ and inference-time alignment empirically and theoretically, such as Huang et al. (2025) *Is Best-of-N the Best of Them?* and the references therein.\nThese learning theory works model aggregation via *coverage* and *sharpening*, grounded in data.  The mechanisms proposed here—“feasibility expansion,” “support expansion,” “binding contraction”—bear no relationship to known empirical phenomena.  What exactly is the support or the conic constraint when you run Llama3.1 on GSM8K or TruthfulQA?  Indeed, the authors do not appear to be familiar with the relevant literature or the professional norms in learning theory research or AI research.\n\n---\n\n**6. Poor writing and presentation quality.**  \nThe presentation is unprofessional.  The numerous grammatical and English issues distract the reading experience of an otherwise intellectually unchallenging paper.  **Even in the abstract**, there are glaring mistakes suggesting that **the authors didn't read through the manuscript** prior to submission.  The third sentence (“Our analysis uncovers three natural mechanisms—feasibility expansion, support expansion, and binding set contraction—through which aggregation provides benefit to the system designer.”) and fourth sentence (“Our analysis identifies three mechanisms—feasibility expansion, support expansion, and binding set contraction—through which aggregation can expand the set of elicitable outputs.”) are the repetitive rephrases of each other, with a classic double em dash. This happens when someone is over-depenedent on LMs for rephrasing but does not read the resultant content.  There is also a missing space in line 24 before “Altogether.”  If you just read through the abstract once, this wouldn't have happened.  Are you not even reading the abstract before submission now?  Seriously?\n- line 58: \"We *capture* prompt engineering *limitations* as the *rewards* operating over a coarser M-dimensional feature space.\"  How can limitations of something be captured as rewards?  The sentences are not making sense.\n- line 134 typo on  \"hallucinations\"\n- line 471 subject verb disagreement \"Our results offer a theoretical insights\"\n- line 1009 typo on \"elicitability\"\nThe list goes on.  It's no excusable to be so bad with English when the authors have all the language models in the world to help with proofread and edit for expression.  The authors' unprofessionalism hurts readability.  \n\nIt has not been a productive use of my time to read a paper which the authors didn't care enough to properly prepare.\n\n---\n\n**7. Lack of contribution.**  \nThe paper overstates its significance.  The authors made various baseless claims.\n- \"...our results uncover key mechanisms that underpin the power and limitations of an aggregation in compound AI systems.\"\nNo.  I don't think one knows more about these after reading the paper than before.\n\n- \"Our results suggest conditions for aggregation to add no power to a system, regardless of the level of prompt engineering limitations.\"\nThe condition is uncheckable, un-verifiable, and has no predictive power.  Can the authors design an experiment where aggregating multiple responses from an LM does NOT help performance, as predicted by the theory?  \n\n- \"Moreover, our results illustrate how the power of an aggregation depends on the interplay between prompt engineering ability and model capabilities.\"\nThere's no clean characterisation of the interplay.  In fact, what even is the quantifiable way to measure prompt engineering ability?  This is once again just some abstract imaginary phrases being thrown around. \n\n- \"More broadly, our results take a step towards understanding when aggregation of multiple copies of the same model provides benefits to system designers.\"\nThis statement is false.  I will change my mind if the authors can provide three examples of real-world AI system designers who have been benefitted from reading the paper.\n\n\nIn short, there is no experiment, simulation, or practical implication to demonstrate progress toward the stated goals; judging this from a pure mathematics perspective, this would be at the level of math REUs (research experience for undergraduate).  As a pure theory paper, the mathematics are elementary and the results offer little novelty or depth."}, "questions": {"value": "1. How do the authors justify modeling user prompts as reward functions and responses as exact argmaxes?  \n\n2. Can the authors provide a concrete mapping between all of their theoretical variables and observable quantities in real LM systems?\n\n3. Why were the aggregation operations restricted to coordinate-wise min and convex combination?  \n   Have the authors considered realistic aggregation methods such as majority voting, reward-model ranking, self-reward scoring, or log-prob mixture?\n\n4. How could one empirically test whether a model satisfies the “limitation-characterizing condition”?  \n   Can the authors provide pseudo-code or an algorithmic diagnostic?\n\n5. Would the authors consider adding even a small-scale simulation to demonstrate that the claimed mechanisms can occur in a controlled environment?\n\n6. How do the proposed mechanisms relate to known empirical theories like *sharpening* or *coverage* (Huang et al., 2025)?  \n   If the relationship is orthogonal, what unique insight does this framework offer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2wsTaglAvu", "forum": "2LcxmMKURb", "replyto": "2LcxmMKURb", "signatures": ["ICLR.cc/2026/Conference/Submission13523/Reviewer_FvLY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13523/Reviewer_FvLY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761064411905, "cdate": 1761064411905, "tmdate": 1762924129928, "mdate": 1762924129928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical analysis of aggregation in compound AI systems—cases where multiple instances of the same model are prompted differently and their outputs combined. Using an extended principal–agent framework, the authors formalize when aggregation can enlarge the set of elicitable outputs beyond what a single model can produce. They identify three mechanisms through which aggregation may add power: (1) feasibility expansion; (2) support expansion; (3) binding-set contraction. The paper presents necessary and sufficient conditions (Theorems 3.7, 4.1–4.4) describing when aggregation helps or is provably useless."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Important question: Understanding the theoretical limits of compound AI systems is timely and conceptually valuable, given the growing use of ensembles, debate, and multi-agent LLM systems.\n2. Theoretical clarity: The framework is clean, extending Kleinberg et al. (2019) to include multiple agents and conic feasibility constraints. The three-mechanism taxonomy is intuitive and could guide future empirical work on ensemble design.\n3. Useful characterizations: The necessary-and-sufficient conditions in Theorems 4.1–4.4 provide a clear geometric interpretation of when aggregation can and cannot expand elicitability."}, "weaknesses": {"value": "1. Limited novelty of insight: While the formalization is careful, most conclusions reiterate intuitive ideas (“aggregation helps only if it changes feasibility or coverage”). The paper extends prior principal–agent analyses rather than offering genuinely new conceptual understanding of compound AI.\n2. Unrealistic assumptions: The theory models models as deterministic optimizers of smooth reward functions, with linear or conic capability constraints and monotone concave rewards. These abstractions are far from how modern LLMs behave (stochastic, discrete, context-dependent). The results therefore may not meaningfully predict behavior of real systems.\n3. No empirical validation: There are no simulations or examples showing that these mechanisms actually appear in practice. Even toy numerical demonstrations would help connect the theory to LLM ensembles or debates.\n4. Questionable generality. The analysis assumes identical, non-communicating agents. It is unclear whether the results generalize to more realistic compound systems (e.g., heterogeneous models, interactive agents, stochastic sampling)."}, "questions": {"value": "1. Under what real conditions do your key assumptions—coarse reward spaces ($N < M$) and linear conic capability constraints ($C \\neq 0$)—actually hold for large language models or other AI systems?\n2. How robust are the theorems to relaxing these assumptions (e.g., stochastic agents, nonlinear constraints, interactive aggregation)?\n3. Empirical research in multi-agents systems with LLMs usually have really complex aggregators. Are the results invariant under different choices of aggregation operators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D8dzDoVikB", "forum": "2LcxmMKURb", "replyto": "2LcxmMKURb", "signatures": ["ICLR.cc/2026/Conference/Submission13523/Reviewer_qfm4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13523/Reviewer_qfm4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866310269, "cdate": 1761866310269, "tmdate": 1762924129296, "mdate": 1762924129296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a theoretical framework to analyze the aggregation in compound AI systems, where multiple identical AI agents (e.g., LLMs) produce outputs that are then combined. Using a principal–agent model, the paper  studies how aggregation can expand the set of elicitable outputs, i.e., outputs that can be generated by designing suitable reward specifications (e.g., prompts).\nThe paper identifies three mechanisms through which aggregation may increase expressive power: Feasibility Expansion, Support Expansion, and Binding Set Contraction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a mathematical treatment of compound AI aggregation under a principal–agent model.\nThe theoretical derivations are strong and have clear logical structure.\nThe proofs look rigorous and complete.\nThe paper provides  theoretical basis for analyzing emergent properties of LLM ensembles and multi-agent systems. It also provides conceptual tools to assess the limits of prompt ensembling."}, "weaknesses": {"value": "The framework presentation is more theoretical and it is not clear to me how directly the results map to practical LLM systems with complex interactions. The paper could benefit from some experimental validations.\nThe mathematical exposition looks precise but may not be accessible to readers. The paper could also benefit from more geometric intuition of the three mechanisms."}, "questions": {"value": "1. How do the results change if some assumptions don’t hold, e.g., that rewards are monotone and concave?\n2. Does the framework suggest any concrete guidelines for designers on how to ensemble prompts or design rewards in real-world situations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YMTf8owzaQ", "forum": "2LcxmMKURb", "replyto": "2LcxmMKURb", "signatures": ["ICLR.cc/2026/Conference/Submission13523/Reviewer_tEsp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13523/Reviewer_tEsp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962826957, "cdate": 1761962826957, "tmdate": 1762924128831, "mdate": 1762924128831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies compound AI systems through a principal-agent framework, analyzing when aggregating outputs from multiple copies of the same model can elicit a broader set of outputs than querying a single model. The authors extend Kleinberg et al.'s (2019) framework by introducing conic constraints to model capability limitations and characterize three mechanisms through which aggregation provides benefits: feasibility expansion, support expansion, and binding set contraction. The main theoretical contributions include necessary and sufficient conditions for when aggregation expands the set of elicitable outputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a current and growing trend: leveraging multiple (often identical) copies of LLMs in a system to increase power, robustness, or reliability, which is relevant in both research and industry.\n- Provides clear formalizations and mathematical analysis, precisely characterizing when aggregation of model outputs can or cannot expand the set of desired system-level outputs.\n- Identifies three distinct mechanisms (feasibility expansion, support expansion, binding set contraction) and connects these directly to improvements from aggregation, offering useful conceptual tools.\n- The paper is clear about which settings aggregation is strictly beneficial, and is precise about which mechanisms are merely necessary versus sufficient."}, "weaknesses": {"value": "- The principal-agent model, as used in Kleinberg et al., are used for classification settings (like job applicants and employers) assumes agents are strategic (with natural game-theoretic incentives, e.g., job applicants maximizing personal outcome). In contrast, LLM “agents” in this paper are neither strategic nor adversarial—they are collaborative and operate according to deterministic/reward-guided rules specified by a system designer. This difference is significant: the economic intuition motivating principal-agent theory (such as strategic effort and misaligned incentives) is not clearly transferred to the compound LLM setting. It is not clear whether all insights from principal-agent theory apply, or if a simpler optimization framework would suffice.\n\n- Homogeneous Setting Only (Limited Generality):\nThe study focuses almost exclusively on aggregating identical models (“copies of the same model prompted differently”). Many real-world compound systems employ heterogeneity, i.e., varying model sizes, architectures, or capabilities. The theoretical results, as currently framed, may not inform those important cases. The work would benefit from discussion or analysis of how aggregation behaves with heterogeneous agents, particularly as “limitations” may be non-uniform.\n\n- The paper treats reward as a general, abstract “specification” that influences outputs. In practical LLM systems, “reward” is often implicit in prompts, user feedback, or RL-from-human feedback, but not explicit or directly adjustable as in standard economic settings. The discussion would benefit from concrete examples or instantiations of how such “reward” appears in real LLM deployments. Also, bigger/better models will face fewer limitations even with the same prompt/reward specification—the model class is not fixed in practice, undermining a key modeling assumption.\n\n- While the paper is mathematically elegant, it is not clear how the theoretical results translate into actionable guidance for practitioners designing compound LLM systems. For example, are there concrete aggregation rules or mechanisms suggested by the results that outperform naive ensembling or majority voting? Does the result motivate any new kind of prompt engineering, or guidance on when aggregation won’t help? The paper should provide illustrative empirical examples or real-world case studies since otherwise the practical impact is highly limited.\n\n- The main mathematical model assumes that limitations (in reward/prompt and in model output) are all linear or conic for tractability. However, LLM behaviors (like hallucinations and edge-case logical errors) are highly non-linear and context-dependent. Modeling these complex sources of error and capacity via simple linear constraints may not capture the true limitations of deployed LLMs, and might mislead system designers about when aggregation will actually help.\n\n- Not clear this captures real prompt engineering challenges like ambiguity in natural language prompts, inability to specify complex constraints, Prompt brittleness.\n\n- While the paper provides a taxonomy of aggregation mechanisms, it mainly offers justification for why existing conceptual ideas (like “prompt diversity” or “ensemble voting”) sometimes work. It does not provide novel principles or mechanisms for designing better compound systems; rather, it mainly delineates when basic strategies succeed or fail.\n\n\nTypos:\n- repeated line in the abstract\n- missing space after comma multiple times in the paper (line 482\n- hallunications -> hallucinations"}, "questions": {"value": "In addition to the ones above in the weaknesses section:\n\n- Can you provide a concrete example mapping real LLM outputs (e.g., text completions) to your R^M output space? How would one extract the constraint matrix C and feature weights \\alpha from a real LLM?\n\n- Can results extend to heterogeneous models? What difference does it make when the agents are strategic vs collaborative (like argmax formulation for aggregation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5rqbrhrxa7", "forum": "2LcxmMKURb", "replyto": "2LcxmMKURb", "signatures": ["ICLR.cc/2026/Conference/Submission13523/Reviewer_H3s3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13523/Reviewer_H3s3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020633446, "cdate": 1762020633446, "tmdate": 1762924128544, "mdate": 1762924128544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}