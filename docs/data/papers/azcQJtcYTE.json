{"id": "azcQJtcYTE", "number": 3849, "cdate": 1757554576676, "mdate": 1763695894021, "content": {"title": "OmniSTVG: Toward Spatio-Temporal Omni-Object Video Grounding", "abstract": "We introduce spatio-temporal omni-object video grounding, dubbed $\\textbf{OmniSTVG}$, a new STVG task aiming to localize spatially and temporally all targets mentioned in the textual query within videos. Compared to classic STVG locating only a single target, OmniSTVG enables localization of not only an arbitrary number of text-referred targets but also their interacting counterparts in the query from the video, making it more flexible and practical in real scenarios for comprehensive understanding. In order to facilitate exploration of OmniSTVG, we propose $\\textbf{BOSTVG}$, a large-scale benchmark dedicated to OmniSTVG. Specifically, BOSTVG contains 10,018 videos with 10.2M frames and covers a wide selection of 287 classes from diverse scenarios. Each sequence, paired with a free-form textual query, encompasses a varying number of targets ranging from 1 to 10. To ensure high quality, each video is manually annotated with meticulous inspection and refinement. To our best knowledge, BOSTVG, to date, is the first and the largest benchmark for OmniSTVG. To encourage future research, we present a simple yet effective approach, named $\\textbf{OmniTube}$, which, drawing inspiration from Transformer-based STVG methods, is specially designed for OmniSTVG and demonstrates promising results. By releasing BOSTVG, we hope to go beyond classic STVG by locating every object appearing in the query for more comprehensive understanding, opening up a new direction for STVG. Our benchmark and code will be released.", "tldr": "", "keywords": ["Spatio-Temporal Video Grounding", "Spatio-Temporal Omni-Object Video Grounding", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/20bb5092537dcb0e3e9b26eebc5cdab809813ba5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the OmniSTVG task where multiple objects mentioned in the text query have to be grounded spatially and temporally. The BOSTVG dataset is collected, consisting of 10K videos manually annotated with spatio-temporal tubes for 1-10 objects, including 287 object classes in total. The OmniTube model is presented, including separate spatial and temporal decoders, text-guided query generation and multi-tube prediction. This model outperforms single-object methods adapted to the new task, and is also about 4x fater than naive approaches of running a single-object model for each object in the query."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A new dataset that addresses an important limitation of previous STVG approaches\n- OmniTube performs well compared to prior approaches and has been ablated extensively"}, "weaknesses": {"value": "- Assuming all objects share the same temporal segmentation is a strong assumption\n- It is confusing what the “baseline” is meant to show in the paper: it excludes various features present in prior work, e.g. text-guided query generation in https://arxiv.org/abs/2502.11168 or alternative spatial and temporal blocks in https://arxiv.org/abs/2203.16434. Note that these features are rightfully not being presented as contributions in the introduction."}, "questions": {"value": "- Out of curiosity, is the architecture able in practice to ground “zero-shot” objects not seen in the training set?\n- How much is each of ResNet and VidSwin feature helpful for the STVG performance?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "A dataset has been collected hence any potential bias/responsible research practice could require more attention."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qcWazWIgp6", "forum": "azcQJtcYTE", "replyto": "azcQJtcYTE", "signatures": ["ICLR.cc/2026/Conference/Submission3849/Reviewer_81gP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3849/Reviewer_81gP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761096617888, "cdate": 1761096617888, "tmdate": 1762917064528, "mdate": 1762917064528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing Spatio-Temporal Video Grounding (STVG) tasks are impractical, as they are conventionally trained to localize only a single target, even when multiple targets are referenced in the text.\n\nThis paper proposes a more practical new task, termed OmniSTVG (Omni-target Spatio-Temporal Video Grounding), which aims to localize all objects mentioned in the textual query. The primary contributions include:\n\nNew Task (OmniSTVG): Defining the aforementioned new direction of \"omni-target\" localization.\n\nNew Dataset (BOSTVG): To support this task, a large-scale, high-quality dataset comprising 10,000 videos was constructed, which has undergone multiple rounds of manual refinement.\n\nNew Baseline (OmniTube): An effective Transformer-based model is provided, which utilizes \"text-guided queries\" to concurrently localize all targets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Pioneering a New Direction: The work identifies the critical limitation of existing STVG tasks (i.e., single-target grounding) and defines a more complex and practical new research direction (i.e., multi-target grounding).\n\nContribution of a Core Resource (BOSTVG): It provides the field's first large-scale (10,000 videos), high-quality benchmark dataset specifically dedicated to \"omni-target\" localization, establishing a core asset for advancing subsequent research.\n\nProvision of a Strong Baseline (OmniTube): The study does not merely pose the problem but also delivers a well-designed (e.g., \"text-guided queries\") and empirically validated solution, offering a solid baseline for future work.\n\nRigorous and Solid Experimentation: The effectiveness of the model's constituent components is validated through exhaustive ablation studies, while comparative experiments underscore the uniqueness and necessity of the new BOSTVG dataset.\n\nThe manuscript is clearly and normatively written; the figures are highly comprehensible and meticulously detail the proposed methodology."}, "weaknesses": {"value": "**Insufficient Analysis**:Regarding the benchmark, an analysis of the task's inherent difficulties is absent. The inclusion of illustrative examples to demonstrate these challenges would be beneficial.Furthermore, a deeper analysis is required as to why existing algorithms, including those compared in this study, cannot be directly or effectively applied to this benchmark.In the ablation study, the paper merely enumerates the functional contributions of individual algorithmic components without providing further in-depth analysis.\n\n**Insufficient Representativeness of \"Omni\" (Data Distribution Imbalance)**:In Section 3.4 (Dataset Splits), the test set is partitioned into three groups: BOSTVG-Low (1-3 targets, 1566 samples), BOSTVG-Medium (4-6 targets, 273 samples), and BOSTVG-High (>7 targets, only 73 samples).The core objective of the paper is to address the \"omni-target\" problem; however, the proportion of high-density, genuinely complex \"omni-target\" samples (73) within the test set is extremely low (approximately 3.8% of the test set).This implies that the model's capability in handling complex, multi-target scenarios has not been sufficiently validated. The reported overall average performance (e.g., $m\\_vIoU$) is likely dominated by the simpler \"Low\" group.\n\n**Questionable Model Generalizability and Data Compatibility (Poor Generalizability)**:In Section 5.2 (Table 8: Ablation on Training Data), when attempting to merge BOSTVG with the existing single-target dataset HCSTVG-v2 for training (Row 3), performance paradoxically decreases slightly compared to training on BOSTVG alone (Row 1).Typically, augmenting the training data, even with related datasets, is expected to enhance model robustness.This performance degradation (which the authors attribute to \"data inconsistency\") may suggest that the model (OmniTube) or the annotation style of the dataset (BOSTVG) is prone to overfitting, hindering its ability to generalize or maintain compatibility with other data sources."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gSptecjtXl", "forum": "azcQJtcYTE", "replyto": "azcQJtcYTE", "signatures": ["ICLR.cc/2026/Conference/Submission3849/Reviewer_CK93"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3849/Reviewer_CK93"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761152606217, "cdate": 1761152606217, "tmdate": 1762917064322, "mdate": 1762917064322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new video grounding task, OmniSTVG, which differs from the traditional STVG task in that it can localize all targets mentioned in the textual query as well as the interactive relationships existing among these targets. To this end, the paper constructs a large-scale dataset BOSTVG and presents a simple yet effective model named OmniTube."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The task proposed in the paper expands the scope of the traditional STVG task. Moreover, the proposed dataset has a wide range of sources and is built using a relatively rigorous manual annotation method, combining both scale and quality.\n- The paper puts forward a simple and effective baseline, and has implemented and compared several public models.\n- The paper provides detailed descriptions of details, making it easy to follow."}, "weaknesses": {"value": "The paper does not mention the performance of multimodal large language models (MLLMs) on this task."}, "questions": {"value": "Why is the performance of any multimodal large language models on this dataset not provided?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DEGice23iu", "forum": "azcQJtcYTE", "replyto": "azcQJtcYTE", "signatures": ["ICLR.cc/2026/Conference/Submission3849/Reviewer_nfHN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3849/Reviewer_nfHN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905559483, "cdate": 1761905559483, "tmdate": 1762917064086, "mdate": 1762917064086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}