{"id": "PTzByqd0aJ", "number": 13488, "cdate": 1758218527607, "mdate": 1759897433462, "content": {"title": "CoEmoGen: Towards Semantically-Coherent and Scalable Emotional Image Content Generation", "abstract": "Emotional Image Content Generation (EICG) aims to generate semantically clear and emotionally faithful images based on given emotion categories, with broad application prospects. While recent text-to-image diffusion models excel at generating concrete concepts, they struggle with the complexity of abstract emotions. There have also emerged methods specifically designed for EICG, but they excessively rely on word-level attribute labels for guidance, which suffer from semantic incoherence, ambiguity, and limited scalability. To address these challenges, we propose CoEmoGen, a novel pipeline notable for its semantic coherence and high scalability. Specifically, leveraging multimodal large language models (MLLMs), we construct high-quality captions focused on emotion-triggering content for context-rich semantic guidance. Furthermore, inspired by psychological insights, we design a Hierarchical Low-Rank Adaptation (HiLoRA) module to cohesively model both polarity-shared low-level features and emotion-specific high-level semantics. Extensive experiments demonstrate CoEmoGen’s superiority in emotional faithfulness and semantic coherence from quantitative, qualitative, and user study perspectives. To intuitively showcase scalability, we curate EmoArt, a large-scale dataset of emotionally evocative artistic images, providing endless inspiration for emotion-driven artistic creation. The dataset and code will be available on GitHub.", "tldr": "", "keywords": ["Emotional Image Content Generation", "Semantically-Coherent Sentence-level Guidance", "Hierarchical LoRA"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d2067f0f1294b02d17bf8066ca73237ea25cb79.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces CoEmoGen, a diffusion-based approach to generate emotion-conditioned images by utilizing hierarchical LORAs."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel approach of using hierarchy for emotions by having two levels, positive/negative for level one and Sad/Happy, etc. for level 2.\n2. The results show the superior performance of the proposed approach compared to the baselines."}, "weaknesses": {"value": "1. The final adapted weight is simply a sum of these LoRAs, which means both adapters are applied in parallel rather than sequentially. This design is more of a multi-branch or compositional LoRA, rather than a truly hierarchical one. A hierarchical LoRA would involve one LoRA’s output feeding into another, or a structured hierarchy where adapters are activated conditionally or at different network depths.\n2. A simple baseline where the emotion label is fed as input to an LLM, which generates a caption for that emotion, and then is fed to a T2I model is missing. I tried this with Flux and the results were reasonable. The results from this experiment would confirm if the problem requires a specific training approach, thereby strengthening the paper's claim. \n3. Line 425: \"we construct the first large-scale emotional art image dataset\". This is incorrect, as [1,2] have done it before, and the data is manually verified. Maybe better to use this dataset.\n\n[1] ArtEmis: Affective Language for Visual Art: Panos Achlioptas, Maks Ovsjanikov, Kilichbek Haydarov, Mohamed Elhoseiny, Leonidas Guibas\n\n[2] It is Okay to Not be Okay: Overcoming Emotional Bias in Affective Image Captioning by Contrastive Data Collection: Youssef Mohamed, Faizan Farooq Khan, Kilichbek Haydarov, Mohamed Elhoseiny"}, "questions": {"value": "1. How many images were evaluated via the human study?\n2. How is 'targeted emotion transfer' in Fig. 8 achieved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ydlEGaYbKW", "forum": "PTzByqd0aJ", "replyto": "PTzByqd0aJ", "signatures": ["ICLR.cc/2026/Conference/Submission13488/Reviewer_LzoM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13488/Reviewer_LzoM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761306137861, "cdate": 1761306137861, "tmdate": 1762924104666, "mdate": 1762924104666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CoEmoGen, a semantically coherent and scalable framework for emotional image generation. It leverages MLLM-generated sentence-level captions to provide rich emotional supervision and introduces a psychologically inspired HiLoRA architecture that integrates polarity-level and emotion-specific representations. Experiments show consistent improvements across quantitative metrics and human evaluations, confirming enhanced emotional fidelity and semantic coherence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The introduction of MLLM-generated sentence-level captions offers an effective and scalable framework for automated emotional annotation.\n\n2. The proposed HiLoRA architecture elegantly integrates shared polarity-level and emotion-specific representations to achieve psychologically grounded emotional modeling.\n\n3. By constructing the EmoArt dataset, the work successfully extends emotional image generation beyond photographic realism into artistic and creative domains."}, "weaknesses": {"value": "1. The MLLM-generated captions may introduce subtle semantic biases or hallucinations, yet there is no human annotation or validation to assess their linguistic accuracy or emotional authenticity.\n\n2. The use of a frozen CLIP text encoder stabilizes training but limits emotional adaptability, as CLIP’s language space is not optimized for affective or psychological semantics.\n\n3. The visual–emotion bias inherent in EmoSet (e.g., fear = terrifying faces) is amplified by multimodal captions and further reinforced through independent emotion-specific LoRA modules. Without textual correction during inference, these biases are directly reflected in the generated results, leading to symbolic and monotonous emotional expressions.\n\n4. Although sentence-level captions from MLLMs aim to enhance semantic coherence, the frequent appearance of neutral phrases (e.g., mouth open) in the word cloud suggests that the model primarily learns low-level visual co-occurrences rather than abstract emotional semantics, which weakens CoEmoGen’s theoretical contribution and generalization ability in genuine affective generation.\n\n5. The figures and tables in the paper use very small fonts, which significantly hinders readability and clarity of presentation."}, "questions": {"value": "1. How do the authors ensure the semantic validity and emotional reliability of the MLLM-generated captions, given that no human verification or annotation was conducted?\n\n2. Why did the authors choose not to include the CLIP text encoder in training, or alternatively, to constrain the Visual-Perception Encoder’s output to align with the text embedding space?\n\n3. Given that EmoSet’s visual–emotion bias (e.g., “fear = terrifying faces”) and the frequent presence of neutral phrases in captions (e.g., “mouth open”) may reinforce low-level correlations, how might the authors mitigate such biases to encourage abstract and contextually rich emotional understanding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1gC6wfRx5d", "forum": "PTzByqd0aJ", "replyto": "PTzByqd0aJ", "signatures": ["ICLR.cc/2026/Conference/Submission13488/Reviewer_2zyk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13488/Reviewer_2zyk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814559200, "cdate": 1761814559200, "tmdate": 1762924104190, "mdate": 1762924104190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CoEmoGen, a novel pipeline for Emotional Image Content Generation (EICG) that addresses the shortcomings of existing text-to-image and EICG models. The authors identify that prior methods, which rely on word-level attribute labels for guidance, suffer from semantic incoherence, ambiguity, and limited scalability. CoEmoGen tackles this by making two primary contributions: (1) It utilizes Multimodal Large Language Models (MLLMs) to generate context-rich, sentence-level captions that serve as semantically coherent guidance. (2) It proposes a Hierarchical Low-Rank Adaptation (HiLoRA) module, inspired by psychology, which decouples emotion modeling into polarity-shared LoRAs (for common low-level features) and emotion-specific LoRAs (for high-level semantics)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper correctly identifies a critical flaw in prior EICG work: the reliance on word-level attribute labels leads to semantic incoherence (e.g., unnatural \"collage-like\" images). The shift from isolated word-level guidance to sentence-level semantic guidance is a significant and logical paradigm shift that directly addresses this core problem, resulting in more natural and contextually sound images.\n\n2. The design of the HiLoRA module is well-motivated by the psychological observation that emotions of the same polarity (e.g., positive/negative) share low-level visual features (like brightness) while differing in high-level semantics. This hierarchical decoupling is elegant and is shown to be effective via strong ablation studies, where removing either the polarity-shared or emotion-specific LoRAs leads to performance degradation."}, "weaknesses": {"value": "1. The entire \"coherent semantic acquisition\" pipeline is fundamentally bottlenecked by the quality of the MLLM used for captioning. The authors acknowledge the risk of MLLM hallucinations and use a heuristic CLIP-based filtering method (discarding the bottom 20% ) to mitigate this. However, this filtering may not be robust enough to catch subtle semantic or emotional inaccuracies, and the model's performance is intrinsically tied to the chosen MLLM's capabilities.\n\n2. While the creation of EmoArt is a strength in terms of scalability, its curation methodology appears flawed. The authors use a classifier pre-trained on EmoSet (natural images) to predict emotions for artistic images from WikiArt. This introduces a significant domain gap. A classifier trained on photos is unlikely to accurately capture the emotional expression in abstract, impressionist, or other non-photorealistic art styles, potentially biasing the resulting dataset."}, "questions": {"value": "The choice of prompt is ablated, but the choice of MLLM is not. Were different MLLMs (e.g., LLaVA, GPT-5) experimented with for caption generation? How sensitive is the model's final performance (e.g., Emo-A or Sem-C) to the quality, verbosity, and style of the captions generated by different MLLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TTc0uJSizd", "forum": "PTzByqd0aJ", "replyto": "PTzByqd0aJ", "signatures": ["ICLR.cc/2026/Conference/Submission13488/Reviewer_xnJi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13488/Reviewer_xnJi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911521089, "cdate": 1761911521089, "tmdate": 1762924103818, "mdate": 1762924103818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work aims to address two core challenges in the field: first, the difficulty general text-to-image models face in accurately capturing and generating abstract emotional concepts like 'awe' or 'contentment'; and second, the limitations of existing EICG-specific model (EmoGen) that rely on word-level attribute labels for guidance. This reliance not only leads to semantically incoherent results (e.g., illogical combinations of elements) and ambiguous emotional expression.\n\nTo overcome these challenges, CoEmoGen proposes a two-pronged solution. First, it leverages Multimodal Large Language Models (MLLMs) to automatically generate context-rich sentence-level captions, replacing the flawed word-level labels to achieve more coherent and rich semantic guidance. Second, inspired by psychological observations, it designs a novel Hierarchical LoRA (HiLoRA) module. This module refines emotion modeling by using \"polarity-shared LoRAs\" (to capture common features of positive/negative emotions) and \"emotion-specific LoRAs\" (to capture the unique semantics of each emotion).\n\nAccording to the experiments, the advantages of this method are the improvements in both semantic coherence (producing more natural and logical images) and emotional faithfulness. Furthermore, its standardized data construction pipeline (using MLLMs) is proven to be highly scalable, allowing it to be effortlessly applied to entirely new domains, such as artistic paintings—a task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A substantive assessment of the strengths of the paper, touching on each of the following dimensions: originality, quality, clarity, and significance. We encourage reviewers to be broad in their definitions of originality and significance. For example, originality may arise from a new definition or problem formulation, creative combinations of existing ideas, application to a new domain, or removing limitations from prior results.\n\nThe paper begins by defining a clear objective: precisely identifying the core weaknesses of current EICG methods, namely their semantic incoherence and poor scalability.\n\nFinally, the claims are substantiated by comprehensive and well-designed experiments. The authors go beyond standard quantitative and qualitative comparisons by including crucial ablation studies to justify their architectural choices, a well-executed user study to validate the perceptually-driven claim of \"semantic coherence,\" and a practical demonstration (the EmoArt dataset) to prove the tangible benefits of their scalable pipeline."}, "weaknesses": {"value": "CoEmoGen doesn't completely escape label dependency. It merely replaces 'fine-grained attribute labels' with a 'coarse-grained emotion label'. Therefore, its scalability is relative; it still requires a dataset pre-annotated with emotions as a starting point, rather than being able to learn from completely unsupervised images."}, "questions": {"value": "**Question 1 (On the Definition and Focus of EICG):**\n\nHow exactly do the authors define Emotional Image Content Generation (EICG)? The examples provided in the paper (e.g., the word clouds figure) suggest that 'emotional images' encompass at least two distinct categories:\n\n1.  Images that **evoke** a specific emotion *in the viewer* (e.g., serene landscapes for 'Awe').\n2.  Images that **depict** a subject *expressing* an emotion (e.g., a person or animal 'showing teeth' for 'Anger').\n\nDoes the CoEmoGen framework demonstrate a particular focus or preference for one of these aspects (evocation or expression), or does it treat both as equally valid means of achieving an \"emotionally faithful\" generation?\n\n**Question 2 (Critique on the Novelty and Scalability of the MLLM Pipeline):**\n\nThe paper presents its MLLM-based pipeline for generating 'sentence-level captions' as a key advantage for scalability, positioning it as a superior alternative to manual 'word-level labels.' However, the MLLM prompt itself still requires a high-level `emotion` label as a prior. This suggests the method is not fully independent of labels.\n\nThis leads to two concerns:\n\n1.  Could this same \"label-to-caption\" generation process also be applied to the EmoGen baseline with minor adaption, like 'label-to-more-labels'? If so, this would imply the novelty lies in the *guidance signal* (captions) rather than the *pipeline's architecture*.\n2.  Furthermore, the pipeline of using a few high-level labels (like an emotion category) to prompt an LLM to generate richer, descriptive captions is an increasingly common practice in the generative field. Does this component of CoEmoGen truly represent a significant and novel advancement in scalability, or is it more of an effective application of an existing paradigm?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "As stated in the paper, EmoArt was collected from WikiArt, and the authors seem to have plans to open-source it. Please check the copyright status."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jwjE2MlP84", "forum": "PTzByqd0aJ", "replyto": "PTzByqd0aJ", "signatures": ["ICLR.cc/2026/Conference/Submission13488/Reviewer_QKK7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13488/Reviewer_QKK7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961369111, "cdate": 1761961369111, "tmdate": 1762924103477, "mdate": 1762924103477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}