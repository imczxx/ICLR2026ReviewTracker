{"id": "DU5eTaQSHT", "number": 5236, "cdate": 1757873409704, "mdate": 1759897986492, "content": {"title": "OpenStamp: A Watermark for Open-Source Language Models", "abstract": "With the growing prevalence of large language model (LLM)-generated content, watermarking is considered a promising approach for attributing text to LLMs and distinguishing it from human-written content. A common class of techniques embeds subtle but detectable signals in generated text by modifying token sampling probabilities. However, such methods are unsuitable for open-source models, where users have white-box access and can easily disable watermarking during inference. Existing watermarking methods that support open-source models often rely on complex or compute-intensive training procedures. In this work, we introduce OpenStamp, a simple watermarking technique that implants detectable signals into the generated text by modifying just the final projection, or unembedding, layer. Through experiments across two models, we show that OpenStamp achieves superior detection performance, with minimal degradation in model capabilities. The implanted watermarking signal is harder to scrub off through post-hoc fine-tuning compared to previous methods, and offers similar robustness against paraphrasing attacks. We have shared our code through an anonymized repository to enable developers to easily watermark their models.", "tldr": "We propose a simple, training-free method to watermark open-source LLMs by modifying their unembedding layer, enabling reliable detection with minimal impact on performance.", "keywords": ["watermarking", "LLM", "opensource"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a48fb9e38af33bc1eb0c85f76d81abae8a8e6d2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors propose a watermarking method for open-source language models. The key idea is to embed the watermarking logic directly into the weights of the unembedding layer, allowing the watermark to influence the output distribution without modifying the model architecture. For detection, the authors introduce a length-normalized log-likelihood ratio that compares the likelihoods between the watermarked and base models. The proposed watermark demonstrates strong effectiveness with minimal degradation in model performance, and it remains robust against fine-tuning attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clear, engaging, and provides well-explained motivations and methods.\n- I found the discussion of limitations particularly valuable and appreciate the authors’ transparency in acknowledging them.\n- The method is easy to implement and train, and it appears to be readily applicable to a wide range of language models.\n- The watermark demonstrates strong robustness against fine-tuning attacks. Notably, even under targeted adaptive attacks, it remains effective after 2,500 fine-tuning steps."}, "weaknesses": {"value": "- The benchmark performance is slightly lower than that of the baseline methods.\n- The detection process is relatively expensive.\n- As the authors also discussed in the paper, the proposed test is not a formal statistical test, so it does not provide an interpretable metric such as a p-value, unlike some previous works."}, "questions": {"value": "- What happens if the model uses tied word embeddings? Could an attacker simply copy the embedding layer’s weights to completely remove the watermark?\n- In the targeted fine-tuning attack, did the authors update the entire unembedding layer, or was LoRA also applied in this case? Can the attacker reinitialize the unembedding layer randomly and do perform the fine-tuning attack?\n- Could we use another open-source model (assuming it is not watermarked and shares the same tokenizer) as the base model for testing, so that the detection can be performed by any party?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fnsyVUxJfu", "forum": "DU5eTaQSHT", "replyto": "DU5eTaQSHT", "signatures": ["ICLR.cc/2026/Conference/Submission5236/Reviewer_3Bfm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5236/Reviewer_3Bfm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667910786, "cdate": 1761667910786, "tmdate": 1762917967025, "mdate": 1762917967025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a watermarking method for LLMs that can be open sourced, a known limitation of decoding based schemes that are trivially avoidable in that setting by omitting the decoding scheme when a user runs the model. Their approach embeds an additional transformation within the final lm head/unembedding layer of the model that approximates a soft selection operation over a precomputed set of \"greenlists\" resulting in a biased token sampling process similar to decoding based watermarks. They evaluate their approach against other open source-able watermarking approaches as well as non-open sourceable decoding based schemes including under paraphrasing and the finetuning attacks more relevant to the open source setting. They demonstrate promising TPR@FPR in both unattacked and attacked settings outperforming the baselines considered."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Methodology and experimental design are generally clearly presented in S3 and S4; figures are easy to interpret.\n2. Evaluation settings are mostly adequate in terms of baselines considered, datasets, and types of ablations on robustness.\n3. Method appears to perform competitively and survive attacks (that aren't provided key insider knowledge of the scheme being deployed)."}, "weaknesses": {"value": "### Missing evidence for certain design choices\n\n### 1. \nUse of a soft selection operation for greenlists makes sense, but has obvious drawbacks. The \"fit\" process performed is based on a L-way clustering over precomputed hstates to map them to L candidate greenlists, regularized for sparseness. However, the chosen loss is a squared error and the sparseness penalty is the Frobenius norm, an unstructured size minimization penalty. What if this optimization were reworked to more directly minimize the entropy of $Sh_i$? Overall, while a simple choice, which is fine, some evidence for why this particular optim problem is the correct one to showcase the scheme is lacking.\n\nUsing cross entropy like $CE(Sh_i,e_i)$ or actually even the reverse $CE(e_i,Sh_i)$ more directly penalize making $Sh_i$ as close to a 1-hot as possible. The reversed version even decomposes into two terms, one of which is precisely the selector entropy one might want to minimize. The decomp referred to is $CE(P,Q) = H(P) + KL(P||Q)$, where canonically P is the ground truth distribution and Q is the predicted or estimated distribution, but the reverse usage is permitted.\nIt is possible this would improve sample complexity while allowing large numbers of distinct lists to choose from to limit reverse engineering\n\n### 2. \nThe LLR test appears principled but could the authors explain why the original type of \"frequency-based detection method\" is not considered as a detection option? L242 motivates that the use of the full logprob ratio test by sensitivity, but the reviewer's intuition is that a better explanation is that the approximate selection via embedded weights method causes predictions to follow mixtures of greenlists not single lists, and thus, it is this mixture that the test is implictly comparing to. Here is an ablation that could help understand the precise way in which the technique is working. \n\nThe standard setting is as described in S3.4, however, two alternates are also run. Alternate detector 1) is that p_wm is instead constructed by using \\delta W broken into two steps, first applying S to the hstates, then applying an argmax to discretize the selection, then applying the resulting 1-hot to select the greenlist from G, and finally, adding this to the original U but otherwise running Algo 1 the same way. Alternate detector 2) runs the standard setting, the proposed alternate with an argmax in the middle, but simultaneously uses the results of the argmax selection as approximations of a single per token greenlist, and uses this to compute the standard \"frequency-based\" test. It's not clear precisely how either of these would compare, but the implict expectation based on the author's intuition is that they will be poorer and providing direct evidence for this set of choices in S3.3 and 3.4 would improve the soundness, and interpretability of the proposed method. \n(The reviewer notes that this is related to the ablation in Fig 5 which uses argmax overlap, the comment is simply that this same process considered there suggests an important detector performance ablation rather than just an interpretability section at the end)\n\n\n### Limitations in the robustness evaluation\n\n### 3. \n\nTPR@1%FPR might not be an acceptable threshold in practice as 1/100 error rates are probably too high to deploy and hence it would be helpful if 0.1%FPR or more realistically 0.01%FPR are evaluated. Of course, because the current results and charts use the same target FPR for all methods, the current results are sound, but whether or not each method performs the same at lower values of expected FPR is not clear. One way to show this would be ROC curves highlighting the low FPR region. In that presentation we see whether the open source methods can achieve detection performance at stricter error rates; if ROC curves for different methods ever cross in this region (though they might not) then it indicates that Fig 2 does not reveal the entire story.\n\n### 4. \n\nThe experiments in S5.3 regarding the finetuning attack need to be clarified: what distribution is Fig 3 computed over? The referenced App F does not appear to provide the missing details. In these experiments, finetuning is performed on the OWT data for up to 2500 steps, and TPR@FPR is measured throughout. The proposed method conditions the soft watermarking rule implemented by $h_t(U+\\Delta W)^T$, on the hstate $h_t$ which is a function of the entire previous prompt context rather than the specified k token window used by schemes like KGW, Aaronson etc. For this reason, it is expected to be more/differently \"context dependent\" than the n-gram based decoding schemes. Perhaps, the impact of the finetuning attack is localized to just the subdomain of OWT samples considered, so, is  Fig 3 evaluated on those samples? or on the same prompt distributions as in Fig 2 and Table 1? \n\nAlso, a note in the writing should be made that the proposed intuition behind the \"targeted\"-ness of the last layer tuning attack appears to not hold, so it should caveat that immediately at L373 as full finetuning is more effective. This begs the question that there may exist stronger tuning attacks specific to OpenStamp. The point is that iif the experiment had shown major degradation under this notion of targeting, then it would evidence that this targeted attack was indeed properly targeted, but it does not show this. See later comment for a sketch of an alternate targeted attack though of course it is not necessarily a \"weakness\" to not have considered other options in an initial method proposal paper."}, "questions": {"value": "### 1. \nCan the results for the two decoding time baselines included in Fig 2 also be included in Table 1?\n\n### Misc Comments\n\n### 2. \nWhile a reasonable choice of topline number in Table 2, L357 mischaracterizes why \"unigram\" is more robust than the original KGW. The original method considers context windows for PRF seeding starting at 2 tokens but in followup work considers longer lengths such as 4. The reason for this is that the number of unique PRF seeds, and therefore greenlists is V^k where k is the context window for seeding. This is good for defending against spoofing as the attacker needs to estimate a much larger set of random partitions to learn the watermarking rules from just observed data, however, a wider context means that edits anywhere in that context window change the outcome of the hash and therefore the greenlist selection degrading the watermark. \"Unigram\" simply selects the most \"robust\" but simultaneously easiest to \"spoof\" setting of KGW, and this ease of spoofing is borne out in experiment as at the unigram, context width=0 setting, the watermark is much more rapidly learned than when the width is 1 or 2 tokens; see Figure 2 in Gu et al. 2024, https://arxiv.org/abs/2312.04469.\n\n### 3. \nProposed attack on OpenStamp. Armed with the information that the technique is based on a decomposition between an original $U$ and $\\Delta W$ would it be possible to pose this problem directly and learn the linear decomposition? As the size of said matrix is simply the unembedding layer, this is not necessarily an expensive problem to run relative to normal finetuning. Complementarily, what if the adversary simply decides to relearn the unembedding layer (perhaps initialized from the un watermarked embedding layer) by either continuing to train the otherwise frozen model? The point here is that for an open watermarking method to be practical, removing it needs to present a non-trivial computational investment. Since the detection process is not black box, the open source release is expected to include some if not all information about how the watermark test works and so a truly targeted attack will use this information. If the threat model is to assume that only the releaser even knows that the watermark perturbation _exists_, then this is of course a fine academic assumption, but security via obscurity is an undesireable precondition for security that we want to rely on in the wild."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sx9oHHSs8d", "forum": "DU5eTaQSHT", "replyto": "DU5eTaQSHT", "signatures": ["ICLR.cc/2026/Conference/Submission5236/Reviewer_4SG8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5236/Reviewer_4SG8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845803722, "cdate": 1761845803722, "tmdate": 1762917966594, "mdate": 1762917966594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OpenStamp, a weighted language model watermarking method designed to embed detectable watermark signals into open-source large language models. Unlike previous decoding-based watermarking approaches that modify the decoding logic during inference, OpenStamp directly integrates the watermarking mechanism into the model’s weights by altering only the final unembedding layer. Specifically, the authors introduce a Linearized Green-List Biasing mechanism, which uses a low-rank matrix factorization $\\Delta W = G S$ to approximate the PRF-based random green-list selection process in KGW. Here, $G$ stores $L$ fixed green lists (each containing approximately $\\gamma \\cdot |V|$ tokens), while $S$ maps the hidden state $h_t$ to a near one-hot selection vector. During generation, the model’s output logits are modified as $v_t' = (U + \\Delta W) h_t$ where the term $\\Delta W h_t$ serves as a context-dependent bias. For detection, the authors propose a statistical test based on the length-normalized log-likelihood ratio (LLR), which compares the conditional probability of the text under the watermarked and original models to distinguish watermarked outputs from unwatermarked ones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a clear and well-structured idea of embedding decoding-based watermarking behavior directly into model weights."}, "weaknesses": {"value": "1. The application scenario is confusing. In Section 3.4, the proposed LLR-based detection requires computing per-token conditional probabilities $p(x_t|x_{<t})$, which implies that the full prompt of the generated text must be known. If the prompt is unavailable (e.g., only the continuation text or human-written text is given), detection cannot be performed. Therefore, this method is more suitable for ownership verification scenarios, where the model owner audits its own generations, rather than for general content detection for open-source LLMs. If the intended goal is to distinguish between human-written and LLM-generated text, it is unclear how one can obtain the prompt for an arbitrary text (or how ``prompt'' applies to human text). The paper should clearly define its target use case, as the current presentation makes the intended scenario ambiguous. If it is used for ownership verification of model, what is the difference between it and the backdoor techniques for model watermarking?\n\n2. The attack model is weak. The robustness evaluation only considers mild parameter updates (finetune the last ten decoder layers together with the unembedding layer). In realistic and academic adversarial settings, attackers are far more capable. For example, distilling the watermarked model into a new student model, or retraining or replacing the output head $U$, directly removing the $\\Delta W$ bias. These operations are computationally affordable and potentially effective for a professional attacker. Hence, the paper's claim of being ``robust to fine-tuning'' only holds under a narrow and weak threat model.\n\n3. OpenStamp’s imitation of a key is extremely limited. In my mind, I understand L as the number of vocabulary permutations in KGW's watermark algorithm. However, in KGW, the number of vocabulary permutations is $|V|!$. OpenStamp predefines only $L=235$ lists, far smaller than the vocabulary ($|V|\\approx32, 000$ in llama models). The small $L$ reduces dynamic variability.  Because $S h_t$ is not perfectly one-hot, multiple green lists can be softly activated at once, diffusing bias energy across many tokens, which increases PPL (in Table 3, RealNewslike PPL$\\approx$14). Since $S h_t$ is a linear and learnable mapping, an attacker with access to $(h_t, logits)$ pairs could reconstruct the pattern of $G$ or the seed-dependent structure. Therefore, OpenStamp ``mimics'' a key but lacks its cryptographic protection. In decoding-based watermarking, the seed can remain private (embedded in the decoder only). In OpenStamp, the seed is fixed during training and embedded into $G \\rightarrow \\Delta W \\rightarrow$ model weights. Therefore, for open-source models, this key is fully exposed, making the watermark both visible and reversible. An attacker could estimate the $G$ pattern and perform a reverse or spoofing attack [1].\n\n[1] Watermark Stealing in Large Language Models, Nikola Jovanović, Robin Staab, Martin Vechev, ICML 2024"}, "questions": {"value": "1. What is the application scenario?\n\n2. What is the perplexity of nonwatermarked text? \n\n3. How is its resilience against spoofing attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5U3H1YyKV7", "forum": "DU5eTaQSHT", "replyto": "DU5eTaQSHT", "signatures": ["ICLR.cc/2026/Conference/Submission5236/Reviewer_Uqzi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5236/Reviewer_Uqzi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880407433, "cdate": 1761880407433, "tmdate": 1762917964800, "mdate": 1762917964800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OpenStamp, a watermarking technique for open-source LLMs that modifies the unembedding (output projection) layer to embed a detectable signal in generated text. Detection uses a log-likelihood ratio test comparing the watermarked and base models. Unlike decoding-time watermarking, this method remains active even when users modify decoding, making it suitable for open-source release scenarios. Experiments on LLaMA-2-7B and Mistral-7B show strong detection performance, minimal perplexity increase, and resistance to post-hoc fine-tuning and paraphrasing attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Practical, simple idea: modifying the final projection layer is lightweight and training-free, lowering barriers to adoption.\n\n- Strong empirical performance: consistently high TPR at 1% FPR across datasets, outperforming baselines in most settings.\n\n- Relevant problem setting: watermarking for open-source models is increasingly important as white-box access grows."}, "weaknesses": {"value": "- Limited theory: The paper does not provide a deeper analysis of why the perturbation remains stable across long-range generation or how the perturbations interact with model dynamics.\n\n- Model scale: All experiments are on ~7B models. Performance and safety tradeoffs at larger scales (e.g., 34B/70B) are not explored, limiting confidence in generalization.\n\n- Paraphrasing robustness at high lexical diversity: While performance at LexDiv=20 is strong, the drop at LexDiv=60 suggests the approach remains vulnerable under aggressive rewriting.\n\n- Attack model coverage: Beyond simple fine-tuning, other relevant attacks (e.g., structured pruning, low-rank weight replacement, extraction/inversion attacks) are not evaluated.\n\n- Incremental novelty: Conceptually close to existing logit-bias watermarking approaches; contribution is largely engineering efficiency + practical insight, rather than algorithmic novelty.\n\nOverall, the work is practically relevant, but feels one step short of readiness for a top-tier venue, primarily due to limited evaluation scope."}, "questions": {"value": "- How does accuracy degrade for larger models? Have you tested 13B/34B or 70B variants?\n\n- Are there conditions on model architecture or token distribution under which the watermark signal may fail to propagate?\n\n- How resilient is the watermark to common deployment transformations such as quantization, pruning, or weight merging"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SJeIvTNMJZ", "forum": "DU5eTaQSHT", "replyto": "DU5eTaQSHT", "signatures": ["ICLR.cc/2026/Conference/Submission5236/Reviewer_2c5R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5236/Reviewer_2c5R"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762034303781, "cdate": 1762034303781, "tmdate": 1762917964330, "mdate": 1762917964330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}