{"id": "UtEY8lF8q2", "number": 8090, "cdate": 1758060887763, "mdate": 1759897808414, "content": {"title": "Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents", "abstract": "Effective interactive tool use requires agents to master Tool Integrated Reasoning: a complex process involving multi-turn planning and long-context dialogue management. To train agents for this dynamic process, particularly in multimodal contexts, we introduce a sandbox environment for reinforcement learning (RL) that supports tool calling and speech-based user simulation. Our core strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses the challenge of credit assignment in long-horizon tasks by employing a Large Language Model (LLM) as a judge to provide turn-level evaluation. To enhance exploration, we integrate a mixed-task training curriculum with mathematical reasoning problems. This unified approach boosts the task pass rate on the text-based $\\tau$-bench by over 6% compared to strong RL baselines. Moreover, we demonstrate our framework's suitability for fine-tuning a multimodal LLM for agentic tasks. By training a base multimodal LLM on interleaved speech-text rollouts, we equip it with tool-use abilities, paving the way for more natural, voice-driven interactive agents.", "tldr": "", "keywords": ["Interactive Tool-Use Agents", "Multimodal Agents", "Tool-Integrated Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/12df17e280c544c4bef5040194fdeb77c0a0c484.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides several interesting contributions to the community, namely: (1) a sandbox for text and speech training with RL and (2) ablations showing that training with *speech and text* with RL is more effective on both domains than training with text alone, (3) a method using trajectory level rewards designed by an LLM-as-a-judge annotating turn-level rewards that improves performance significantly on tao-bench, and (4) results demonstrating that mixed-task training and curriculum learning are an effective strategy for RL."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. Introduce a sandbox environment combining speech and text in the tao bench environment. \n2. Perform multimodal training on speech and text and find that speech and text training using RL is a powerful tool for improving performance on both the text alone set up and the multimodal configuration of the environment\n3. The paper presents a mixed-task training curriculum wherein the authors mix together math problems with tao agent and multi-modal rollouts. The paper shows that including these math tasks significantly improves performance, an interesting finding for the community.\n4. The paper provides a detailed analysis of using a trajectory level reward from turn level reward metrics and demonstrates that using this can yield substantial improvements on tao bench. \n5. Ablate against other entropy based methods for turn level credit assignment and find them to not substantially improve performance on this domain. \n6. Formulate an LLM-as-a-judge reward assignment paradigm, though the details of how the LLM-as-a-judge is implemented are lacking"}, "weaknesses": {"value": "1. The abstract, introduction & title of the algorithm all seem to indicate that turn-level (e.g. token level) reward assignment is critical to algorithm success. However, the most successful variation of this algorithm involves summing up all of the token level rewards and placing it at the end of the trajectory. I find it hard to believe that if we are summing up the reward and placing it at the end of the trajectory we are receiving the fine-grained, turn-level credit assignment claimed in the third bullet point of the introduction\n2. The second critical component of this algorithm is the LLM-as-a-judge, but there is limited details on how we verify that the judgements made by the LLM-as-a-judge are truly useful other than the downstream RL performance\n3. It seems that the LLM-as-a-judge requires a ground truth trajectory something that could be hard to find in many other domains (such as coding where there are many correct solutions)"}, "questions": {"value": "1. How is the confidence paradox addressed by the trajectory level rewards proposed in this paper? Do you observe any qualitative or quantitative results indicating \n2. How did you verify that the LLM-as-a-judge was providing reliable answers on the turn level? \n3. From the findings of this paper, do you believe that turn-level rewards are viable future research direction and we simply need to engineer these rewards to be more useful and the credit assignment algorithm to be better or should practictioners and researchers focus on more useful trajectory-level rewards and mixed task training? \n\nThis reviewer would be willing to give a higher score if the authors reframe the paper away from turn-level rewards, or some suitable explanation from the authors on why they chose to frame the paper in this way."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FCXo5KZRPM", "forum": "UtEY8lF8q2", "replyto": "UtEY8lF8q2", "signatures": ["ICLR.cc/2026/Conference/Submission8090/Reviewer_y6Kx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8090/Reviewer_y6Kx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761432405992, "cdate": 1761432405992, "tmdate": 1762920075833, "mdate": 1762920075833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers (Part 1)"}, "comment": {"value": "We sincerely appreciate the thoughtful feedback from all reviewers and are encouraged that they recognize the strengths of our work. In particular, reviewers M7U7 and dX8P highlighted the value of our initial effort to integrate RL with multimodal voice agents, and reviewers M7U7 and y6Kx noted that our mixed-task training setup yields meaningful and promising insights. We also notice some common questions/concerns raised by reviewers, and we will address them in this general response below.\n\n----\n\n### (1) Reliability of LLM-as-Judge for Multi-Turn Trajectory Evaluation\nReviewers questioned whether an LLM-as-Judge can reliably assess multi-turn trajectories in τ-Bench. To verify this, we conducted a human–LLM agreement study using our evaluation schema (Section 3.2). We sampled trajectories from PPO, GRPO, and RLOO (20 failed + 10 successful task trajectories per method, with a total of 90 trajectories and 1460 turns), collected human turn-level annotations, and compared them to LLM-judge outputs. The results are presented below:\n\n**(a) Strong trajectory-level consistency**\n\nWe compare the summed trajectory scores and evaluate the correlation between LLM and human (after rescaling following our schema in section 3.2):\n- Pearson correlation: 99.4%\n- Spearman correlation: 87.6%\n- Failed-only Pearson: 88.9% (correlation of trajectory-level scores on failed trajectories)\n\nThese results demonstrate that the LLM-judge closely aligns with both the absolute scale and relative ranking used by human evaluators, making it a reliable signal for RL training.\n\n**(b) High turn-level agreement**\n\nWe measure the each turn label's prediction accuracy and LLM achieves a turn-level accuracy of **93.5%**. The LLM-judge reliably identifies good, neutral, and bad turns, demonstrating strong alignment with human fine-grained judgment.\n\n\n**(c) Accurate major deviation detection**\n\nWe evaluate if LLM-Judge can reliably identify the major deviation turns (score = –1). We have the following result:\n- Exact match: 77.6%\n- Fuzzy (±1 turn): 100%\n\nThe fuzzy accuracy allows gpt score to be one turn off from the human label. We see that the LLM-judge can identify the correct region of the error, reinforcing its reliability for pinpointing faulty reasoning in multi-turn interactions.\n\n\n**(d) Human-labeled failure mode analysis** \nFollowing Reviewer dX8P’s suggestion, we performed a human analysis of failure modes. The distribution is:\n\n| Fault Type         | Count |\n|--------------------|-------|\n| Wrong Reasoning    | 31    |\n| Policy Violation   | 15    |\n| User Hallucination | 8     |\n| No Action          | 5     |\n\nMost failures stem from reasoning errors, suggesting that improved summarization and information integration would yield the largest performance gains. Policy violations remain non-trivial in long-horizon interactions, and a small number of failures come from user hallucinations or failing to take necessary actions. This categorization also helps us understand what the LLM-Judge is evaluating and could be useful for developing more finegrained evaluation rubrics.\n\n**Conclusion**\n\nAcross trajectory-level scoring, turn-level labeling, and deviation-turn identification, the LLM-as-Judge shows high alignment with human judgment. These results provide strong evidence that our LLM-based evaluator is reliable and appropriate for driving multi-turn RL in τ-Bench.\n\n---\n\n### (2) On the Generalizability of the Proposed Schema to Other Multi-Turn Tasks\n\nBoth Reviewer M7U7 and Reviewer y6Kx raise questions about whether the proposed turn-level assessment from LLM-as-Judge can be applied to other multi-turn tasks.\nHere are comments about the generalizability of turn-level adjudication:\n1. Our proposed turn-level evaluation can be easily extended to other tasks such as computer-use, math-solving, code-generations, etc., as we target a class of environments where (i) user goals are explicit, (ii) tool execution produces verifiable outcomes, and (iii) turn-level evaluation is feasible. Some recent work [1,2,3] has already adopted various turn-level signals into RL post-training pipeline.\n2. The scoring mechanism will be adjusted per task, but the high-level design philosphy is the same — that you want to adjust turn-level scores such that outcomes are properly separated into a distinct groups with meaningful difference (in our case, we have 4 groups as discussed in section 3.2, line 244 - 258) so that model observes more finegrained signal beyond binary judgement (with 0/1 outcome reward).\n\n--- \n[1] Chang et al., 2025. THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning\n\n[2] Dai et al., 2025. Process Supervision-Guided Policy Optimization for Code Generation\n\n[3] Tang et al., 2025. SEA: Self-Evolution Agent with Step-wise Reward for Computer Use"}}, "id": "s2KsrMXfIT", "forum": "UtEY8lF8q2", "replyto": "UtEY8lF8q2", "signatures": ["ICLR.cc/2026/Conference/Submission8090/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8090/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8090/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763581342296, "cdate": 1763581342296, "tmdate": 1763581342296, "mdate": 1763581342296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on training interactive tool use agents by creating (i) a sandbox environment with text based and audio based user simulators, (ii) database of tasks from retail and airline domain (based on τ-bench) and (iii) utilizing turn based and outcome based rewards. The authors utilize a mixed training strategy by interleaving medium difficulty math problems with the domain tasks to encourage the models to reason, explore, self-critique and correct. Authors show text-based τ -BENCH improvements by over 6% compared to strong RL baselines (using this mixed task and turn based rewards strategy). While the authors show various experiments with the text based system, they also experiment with multimodal models to show voice based interactions with the agents. The multimodal models improve by over 20% with the mixed training and comprehensive rewards strategy. Authors fine-tune Qwen3-8B and Qwen2.5-Omni-7B models and show improvements compared to strong baselines (PPO, GRPO and Reinforce-leave-one-out algorithms). Authors also conduct experiments to study how different reward strategies affect training performance as well as studying how incentivizing exploration involves both mixed task training and fine-grained reward schemes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-  this paper interleaves speech and text modalities for RL pipeline for improving tool use (necessary for a successful outcome), showing improvements on an agentic benchmark dataset. Authors claim it’s the first demonstrated training of a multimodal voice agent via RL on interleaved speech–text tool use. Although most ingredients of the system exist, but the combination of several of these ingredients is the main strength and contribution of this work. \n- other aspects such as RL for tool use within a sandbox environment and turn level supervision with LLM judges is incremental strength of this research; there are multiple systems, models and frameworks that exist today (eg. Kimi-K2 has very impressive tool use results on multiple tool-use benchmarks); this work focuses on enabling smaller models (Qwen 7B variants) which is a welcome direction for many researchers in the field who can easily build up research on top of these model sizes and systems."}, "weaknesses": {"value": "- the paper shows limited benchmark validations (primarily on the τ -BENCH Retail domain); the baseline experiments show Airline domain results are degrading generally and the main content of the paper does not focus on trying to show generalization improvements using their approach. (authors callout that their focus is on optimizing RL strategies for in-domain performance; however instead of focusing on analyzing and improving RL strategies for one set of results, this research is more applied where authors refocus on showing multimodal systems improvement)\n- authors talk about this problem of reduced exploration with RL training, trying to understand how to recover from errors and building strategies for error recovery would be very interesting to analysis (what happens if judges give incorrect rewards, trying to understand these failure modes would make this work more interesting)\n- it isn't clear how real and robust is the speech pipeline; SeeTTS based clean speech (without noise, accents, messy speech) isn't really tested and would further degrade the multimodal performance (already poor in a controlled sandbox environment compared to SOTA larger models)"}, "questions": {"value": "- Can we replace the LLM judges with verifiable sub-goals where possible? \n- Do TARL gains hold with real world noisy speech? \n- What's the impact on latency in the multimodal pipeline with TTS models in the loop? \n- Testing on policy adherence will clarify if the RL based training system retain various refusal or other policy driven behaviors under TARL based training - did the authors test this aspect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NvHvC5ik8N", "forum": "UtEY8lF8q2", "replyto": "UtEY8lF8q2", "signatures": ["ICLR.cc/2026/Conference/Submission8090/Reviewer_dX8P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8090/Reviewer_dX8P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097052445, "cdate": 1762097052445, "tmdate": 1762920075416, "mdate": 1762920075416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of training LLM agents for tool-use in multi-turn environments. The authors propose the following modifications: (1) training on a mixture of task-specific and general reasoning problems (specifically in the math domain) to encourage exploration, and (2) using LLM-as-a-judge for turn-level rewards. The authors evaluate their approach on base textual \\tau-bench, and then on a multi-modal version using speech."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors are (to my knowledge), the first to evaluate a multimodal LLM agent trained on speech. \n\n2. The mixed-training objective, while simple, proves to be an effective method at overcoming the LLM agent becoming too overconfident during RL training. \n\n3. The authors demonstrate that turn-level rewards perform more effectively than trajectory-level ones, which will prove useful for the development of general LLM agents in multiturn environments."}, "weaknesses": {"value": "1. The method relies heavily on LLM-as-a-judge, specifically the identify the most important mistake made in a trajectory. This seems like a limiting heuristic, and it is unclear if LLMs are capable of analyzing multiturn trajectories in this manner, when they contain multiple, cascading errors. My biggest concern with the method is that the current way turn-level rewards are labeled does not seem generalizable to other multiturn domains.  \n\n2. The evidence that mixed-task training improves exploration seems inconclusive. It would be more convincing if the number of training samples were kept consistent between mixed-task and single-task training. Namely, if some samples of training on \\tau-bench were instead co-opted with training on Math, would the benefit due to improved exploration still be visible?\n\n3. Overall, the proposed method itself seems very specific, with the heuristic reward function as well as the training on math. The results would be stronger with an ablation on the reward function, as well as a more comprehensive analysis of how important mixed-training is, e.g. by considering more tasks other than math."}, "questions": {"value": "1. What is the performance impact when the LLM is allowed to assign a score of -1 to more than one turn in a conversation? This seems like a limiting design choice, but I am curious if it is important for performance. \n\n2. How important is the warm-up curriculum? If it is important, it would be an interesting result and probably can be included in the proposed approach. \n\n3. The final trajectory-level reward involves 10x scaling of the turn-level scores. What is the sensitivity of performance to this particular scaling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QCxTusUt8z", "forum": "UtEY8lF8q2", "replyto": "UtEY8lF8q2", "signatures": ["ICLR.cc/2026/Conference/Submission8090/Reviewer_M7U7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8090/Reviewer_M7U7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762286660090, "cdate": 1762286660090, "tmdate": 1762920074942, "mdate": 1762920074942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}