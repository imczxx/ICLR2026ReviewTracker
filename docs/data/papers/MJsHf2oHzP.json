{"id": "MJsHf2oHzP", "number": 4086, "cdate": 1757598320440, "mdate": 1763746107303, "content": {"title": "UniSVD: Unilateral Weight Decomposition for Attention-based Vision Models", "abstract": "Transformers have achieved remarkable success across diverse domains, but their ever-growing scale results in prohibitive computational and memory costs. Low-rank matrix decomposition with Singular Value Decomposition (SVD) has emerged as an effective compression technique. Recent studies, such as ASVD, SVD-LLM, and FLAR-SVD, have improved decomposition quality by incorporating activation-aware method. However, these methods do not consider the unique mechanism of MHA, where query-key ($Q$-$K$) and value–output ($V$-$O$) computations are linear and allow pre-computation. To effectively leverage this mechanism, we propose Unilateral Singular Value Decomposition (UniSVD), a novel framework that applies decomposition to only one side of the $Q$-$K$ or $V$-$O$ weight pairs in a head-wise manner.  Since $Q$-$K$-$V$-$O$ weights exhibit varying sensitivities to low-rank approximation across heads and layers, UniSVD adaptively selects which side to be decomposed according to the rank sensitivity, thereby preserving the important information of weights. Extensive experiments demonstrate that UniSVD seamlessly integrates with existing decomposition methods and consistently achieves superior trade-offs between parameter reduction, FLOPs, and model performance.", "tldr": "In this paper, we propose Unilateral Singular Value Decomposition (UniSVD), a novel and effective method that applies decomposition to only one side of the $Q$-$K$ and $V$-$O$ weights in a head-wise manner.", "keywords": ["Model Compression", "Singular Value Decomposition", "Low-rank Approximation"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/21230ef9fcccdf625f7b804a709b136e65d0de19.pdf", "supplementary_material": "/attachment/59d8d2fd76c981118dee9c9af7512b7ac03529c8.zip"}, "replies": [{"content": {"summary": {"value": "The key idea is that to decompose only one side of (Q,K) and (V,O) per head, chosen by rank-sensitivity, to improve the accuracy–efficiency trade-off vs per-weight and combined decompositions and positions against per-weight SVD inefficiency and COMCAT’s combined scheme. \n\nThe paper claims broad plug-in gains across FWSVD, ASVD, SVD-LLM, FLAR-SVD without fine-tuning. This is backed up by some impressive results - notably in table 2 - beats per-weight and combined methods across 20/40/60% MHA reduction (Table 2); large gains at 60%. \n\nError analyses across layers support the mechanism"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Simple idea which results in some good results. \n\nThe idea is simple, training-free, and plugs into multiple SVD families, improving both Top-1 and latency without fine-tuning. Under heavy MHA reduction, unilateral consistently outperforms combined by large margins. I think this is a nice result which is presented in a very clear way."}, "weaknesses": {"value": "End-to-end efficiency reporting -  provide VRAM and throughput across batch/sequence lengths (besides latency) for each baseline + UniSVD; include wall-clock for the selection pass. \nThe following would improve the paper I feel \n\nCompare Frobenius-error selection with alternatives (spectral norm, activation-aware criteria like ASVD/SVD-LLM whitening) to confirm robustness.\n\nAdd ViT-L/16 or Swin-T, and a second dataset to show generality beyond DeiT/ImageNet-1K. \n\nGranularity of ranks. Release per-layer/head ranks and selection maps; add sensitivity curves (Top-1 vs rank) to guide practitioners. \n\nTheory–performance link. The appendix notes combined-space error may not track accuracy; expand analysis on why individual-space optimisation aligns better with performance. \n\nAblations on latency sources. Separate gains from fewer sublayers vs cache/matmul effects to clarify where the speedup comes from."}, "questions": {"value": "Can you provide full VRAM/throughput and batch/sequence sweeps, and quantify the one-off cost of computing head-wise SVD errors for selection? \n\nDo you have results on non-DeiT architectures or another dataset to confirm portability? \n\nCould unilateral decomposition extend to MLP blocks (e.g., only decompose one of in/out projections) or to LLM attention with RoPE? Any problems you foresee? \n\nThe appendix shows combined-space error isn’t predictive of accuracy. Can you formalise why individual-space optimisation is better aligned, or give counter-examples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CZrP9A6s7E", "forum": "MJsHf2oHzP", "replyto": "MJsHf2oHzP", "signatures": ["ICLR.cc/2026/Conference/Submission4086/Reviewer_Rygs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4086/Reviewer_Rygs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576718168, "cdate": 1761576718168, "tmdate": 1762917171881, "mdate": 1762917171881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniSVD, a novel low-rank decomposition method for efficient compression of Transformer-based models. UniSVD exploits the linear nature of Multi-Head Attention by selectively decomposing only one side of each Q–K or V–O weight pair, while dynamically determining which side to factorize based on Frobenius-norm-based rank sensitivity analysis at the head and layer levels. This approach preserves information in rank-sensitive weights while significantly reducing parameter count and computational cost. Experiments on DeiT-Small and DeiT-Base models with the ImageNet-1K dataset demonstrate that UniSVD consistently improves accuracy and reduces latency compared to prior SVD-based methods such as FWSVD, ASVD, SVD-LLM, and FLAR-SVD, achieving over 40% parameter reduction with only a 2–3% accuracy drop. These results validate UniSVD as a simple yet generalizable low-rank decomposition framework that effectively balances efficiency and performance for Transformer architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed UniSVD maintains a level of parameter and computational efficiency comparable to existing combined decomposition methods, while introducing a unilateral decomposition strategy and a dynamically sensitivity-aware weight selection mechanism. Through this design, the method effectively preserves model accuracy without requiring any fine-tuning process.\n\n2. The proposed UniSVD demonstrates strong practicality and applicability in real deployment and model compression scenarios by maintaining stable performance without any fine-tuning process. This highlights the proposed method’s contribution not only as a theoretically efficient approach but also as a training-free, low-cost compression framework that can be readily applied in real-world applications.\n\n\n3. The theoretical explanation of the proposed method is presented in a logical and convincing manner. The mathematical formulation and analysis are well aligned with the underlying intuition, effectively supporting the validity and reliability of the proposed approach."}, "weaknesses": {"value": "1. While Table 1 reports reductions in latency and GFLOPs, the paper does not provide sufficient analysis of the underlying causes. A more detailed explanation is needed to clarify which computational characteristics of the proposed method lead to reduced operations and latency. In addition, the paper lacks discussion on whether such acceleration effects are specific to certain hardware environments or can be consistently observed across different device types.  \n  \n  \n2. The proposed method leverages the characteristics of the MHA structure. Accordingly, it would be important to discuss whether this method can be extended to large-scale generative models such as Vision-Language Models (VLMs) or Large Language Models (LLMs) that also rely on MHA. Furthermore, the experiments are confined to the relatively simple DeiT–ImageNet setting, which limits the evaluation of the method’s generality and effectiveness for large-scale models. In particular, restricting the experiments to DeiT models does not sufficiently demonstrate the proposed method’s generalization capability, representing a critical limitation that diminishes the overall contribution of the work. As recent research trends emphasize validating applicability and generalization across diverse model architectures and datasets, broader experiments and validation are necessary to reinforce the paper’s claims.\n\n\nOverall, the proposed method presents a novel and technically interesting idea that deserves recognition. However, empirical validation regarding the range of applicable models and the generalization performance remains limited. As mentioned in the future work, extending the approach to large-scale models such as LLMs or VLMs and demonstrating consistent generalization would significantly strengthen the contribution of this study. Incorporating additional experiments or analyses addressing these limitations in a future revision would greatly enhance the completeness and overall impact of the paper."}, "questions": {"value": "1) The paper reports in Figure 4 that $W^{K}$and $W^{O}$are frequently selected due to their low rank sensitivity, and Table 3 shows that a fixed-selection strategy using these weights achieves nearly identical accuracy to the dynamic UniSVD variant. Given this, could the authors clarify whether the additional overhead of performing per-head SVD and loss computation twice in the dynamic selection process is practically justified, considering the marginal accuracy improvement observed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9mosj0VgF6", "forum": "MJsHf2oHzP", "replyto": "MJsHf2oHzP", "signatures": ["ICLR.cc/2026/Conference/Submission4086/Reviewer_B938"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4086/Reviewer_B938"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638493948, "cdate": 1761638493948, "tmdate": 1762917171640, "mdate": 1762917171640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response 3 (for Reviewers 3Es2, 1opc, B938)"}, "comment": {"value": "**Additional Overhead of Dynamic Selection Compared to Fixed Selection**\n| Model  | DINOv2| ViT-Large | Swin-Large| DeiT-Large|\n|------ |--------:|----:|----:|----:|\n| w/o selection (sec) | 0.6 |  1.3 |  1.1 | 1.3 |\n| w/ selection (sec) | 1.2 | 2.6 | 2.4 | 2.6 |\n\n*Table B. Wall-clock time comparison for the dynamic selection. Each time is the cumulative time for all layers. Results are measured on a single 3090 GPU.*\n\nFirst, we would like to clarify that the proposed dynamic selection method does not introduce additional overhead at inference time compared to the fixed selection method. Because the selection is defined on the decomposed weights, all selected components can be precomputed and initialized once, and then reused for multiple inference runs without extra per-sample computation. Therefore, in our dynamic method, the selection is not adaptively recomputed for each input sample. Instead, our method operates dynamically over the per-head weights of each model. \n\nTo further alleviate the reviewers’ concern about the overhead of this dynamic process, we measured the processing time in a setting where the pre-computation step is explicitly included, as reported in Table B. This measurement covers the entire procedure, including weight decomposition and the selection required to obtain the precomputed weights. The results show that the dynamic selection method incurs only a minor overhead of up to 1.3 seconds compared to the fixed selection baseline, which we believe sufficiently addresses the concern about the additional overhead."}}, "id": "0xIMar9K2X", "forum": "MJsHf2oHzP", "replyto": "MJsHf2oHzP", "signatures": ["ICLR.cc/2026/Conference/Submission4086/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4086/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4086/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763742464058, "cdate": 1763742464058, "tmdate": 1763743502107, "mdate": 1763743502107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniSVD, a simple and training-free low-rank compression method for attention-based vision transformers. Instead of decomposing all attention weights, UniSVD decomposes only one side of each Q/K or V/O pair, based on which side is less sensitive to rank reduction. This “unilateral” strategy keeps more information from the important weights and reduces FLOPs and parameters. Experiments on DeiT-Small and DeiT-Base show consistent improvements over several existing SVD-based decompositions, without any fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "UniSVD offers a novel insight into attention decomposition: discovering and utilizing the asymmetric sensitivity of Q/K and V/O matrices to improve compression without retraining.\nThe method is conceptually simple and easy to follow, requiring only SVD and a dynamic selection rule based on Frobenius loss."}, "weaknesses": {"value": "The experimental results mainly highlight two aspects:\n(1) UniSVD is orthogonal to existing SVD-based decompositions and consistently improves upon them;\n(2) it outperforms simpler variants such as Per-Weight Decomposition and Combined Weight Decomposition.\nHowever, the evaluation lacks comparison against stronger and more recent low-rank decomposition baselines from the literature, making it unclear whether UniSVD achieves SOTA performance beyond the specific SVD variants tested here.\n\nFrom a methodological perspective, while the core intuition is reasonable, the technical novelty is modest. The method essentially adds a dynamic selection between Q/K and V/O before applying SVD. The results suggest that decomposing K over Q and O over V tends to be optimal, and the additional head or layerwise dynamic selection provides only marginal benefit, since Table 3 shows negligible differences. \n\n|        | 20%  | 40%  | 60%  | 20%  | 40%  | 60%  | 20%  | 40%  | 60%  |\n|--------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n| K-O  | 78.3  | 74.2  | 65.3  | 79.6  | 76.1  | 69.3  | 81.7  | 80.3  | 71.7  |\n| DynamicSelection | 78.4  | 74.4  | 65.4  | 79.9  | 76.5  | 69.3  | 81.7  | 80.4  | 72.1  |\n\nThus, the main contribution appears to lie in identifying the relative sensitivity of Q/K/V/O weights to decomposition rather than introducing a fundamentally new decomposition framework.\n\nFinally, the method is evaluated only on DeiT models and ImageNet classification. It remains uncertain whether UniSVD generalizes to other ViT architectures e.g. Swin Transformer, downstream tasks such as detection or segmentation, or also transformer-based LLMs. If demonstrated, such generalization could substantially strengthen the paper’s contribution and broader impact."}, "questions": {"value": "q1: In your experiment, why do you use the distilled version of DeiT-S only but not the other two model sizes?\nq2: In figure 4, it seems that in most cases, one component will dominate the other choice. so I'm curious how the following settings works: just calculate per-layer error and always choose the dominant choice regardless of the head. \nq3: I'm curious about what is the ratio of choosing Q/K and choosing V/O, it seems that K is dominating Q and O is dominating V.\nq4: What is the computational cost of the head-wise dynamic selection during inference"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xqkEpbB64V", "forum": "MJsHf2oHzP", "replyto": "MJsHf2oHzP", "signatures": ["ICLR.cc/2026/Conference/Submission4086/Reviewer_1opc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4086/Reviewer_1opc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787987708, "cdate": 1761787987708, "tmdate": 1762917171472, "mdate": 1762917171472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response 2 (for Reviewers 3Es2, 1opc, B938, Rygs)"}, "comment": {"value": "**Discussion of UniSVD Limitations and Extensibility**\n\nWe appreciate the reviewer’s concerns regarding the generality of the proposed method. To address this point, we added experiments on diverse architectures and tasks in Table 2,4,5 and Section 4.7 of the revised draft, including additional vision backbones, VLMs, semantic segmentation, object detection and semantic segmentation. Beyond these empirical results, we provide a more detailed discussion of the applicability of our approach below. The proposed method is built on the linearity between the Q–K and V–O projections in the attention module. Therefore, it is in principle applicable to any architecture that follows this attention mechanism.\n\nFirst, this includes widely used vision encoders, such as Vision Transformers, DeiT, Swin, and PVT, where the attention blocks are implemented with linear Q, K, V, and O projections.\n\nSecond, the method can be applied to VLMs such as EVA and LLaVA-1.5, which employ these vision encoders as their visual backbone.\n\nThird, the method is also applicable to ROPE-based attention when the input sequence length is fixed. For example, in video generation models such as Hunyuan [1], the input sequence length is fixed, so the RoPE transformation applied to Q and K can be absorbed into a fixed linear transformation. In this setting, one can treat the rotation matrices as constant and perform weight decomposition accordingly.\n\nAt the same time, we acknowledge an important limitation. For LLMs using RoPE with variable input lengths, it is not feasible to precompute optimal decompositions for all possible rotary matrices, which makes direct application of our method difficult. We believe that clearly stating this applicability condition and limitation better reflects the true generality of the proposed approach.\n\nHowever, Even for LLMs, there are promising directions for extending our method. A very recent work [2] reports that layer normalization is not strictly required at inference time for GPT-2–style models, and this submission received very strong preliminary scores. This study replaces all layer normalization layers in the QKV projections and MLP blocks with equivalent linear transformations. Under such architectures, many additional linear relationships between adjacent weights naturally arise, and our method can be applied to these linear layers as well. Therefore, we believe that the proposed approach has substantial potential to be further extended and applied to LLMs as these architectures become more widespread. \n\nOverall, we believe that the additional experiments and the above clarifications demonstrate that the proposed method is broadly applicable to a wide range of attention-based vision and vision-language models, while also honestly acknowledging its current limitations for LLM settings. If the perceived generality of the method was a major factor in your evaluation, we hope that this response helps alleviate your concerns, and we would be grateful if you could take these points into consideration when updating your score.\n\n[1] Kong, Weijie, et al. \"Hunyuanvideo: A systematic framework for large video generative models.\" arXiv preprint arXiv:2412.03603 (2024).\n\n[2] Baroni, Luca, et al. \"Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability.\" arXiv preprint arXiv:2507.02559 (2025)."}}, "id": "CSqM84X8db", "forum": "MJsHf2oHzP", "replyto": "MJsHf2oHzP", "signatures": ["ICLR.cc/2026/Conference/Submission4086/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4086/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4086/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763742493135, "cdate": 1763742493135, "tmdate": 1763746355541, "mdate": 1763746355541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UniSVD, a novel weight decomposition method for compressing attention modules in vision transformers. Unlike conventional approaches that decompose all weight matrices or combined weight products, UniSVD selectively decomposes only one side of the Q-K and V-O weight pairs based on their sensitivity to low-rank approximation. The selection is performed dynamically at the head and layer level using Frobenius norm-based approximation error. The authors demonstrate consistent improvements when UniSVD is integrated with existing SVD-based compression methods on DeiT models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 - The unilateral decomposition idea cleverly exploits the linear nature of Q-K and V-O operations in attention mechanisms.\n\n2 - Table 1 shows substantial gains across multiple baseline methods (e.g., 8.5% improvement for ASVD on DeiT-Small).\n\n3 -  The paper includes thorough ablation studies examining dynamic vs. fixed selection strategies and head-wise selection patterns."}, "weaknesses": {"value": "1. **Limited scope and evaluation**: \n   - Only evaluated on DeiT models with ImageNet-1K\n   - No experiments on larger/recent models (ViT-Large, DINOv2, etc.)\n   - Cannot be applied to MLP layers, limiting overall compression potential\n\n2. **Lack of theoretical analysis**: No formal analysis of why unilateral decomposition preserves more information than alternatives. The singular value analysis in Figure 2 is superficial and doesn't rigorously support the claims.\n\n3. **Computational overhead concerns**: The dynamic selection requires computing SVD for both weights to make decisions, potentially negating efficiency gains. This overhead is not analyzed.\n\n4. **Missing critical comparisons**: \n   - No wall-clock time measurements for the selection process\n   - Limited baseline comparisons (only SVD-based methods)\n\n5. **Inconsistent experimental details**:\n   - Table 1 shows different parameter counts for FLAR-SVD (49.2M) vs others (44.1M) without explanation\n   - The \"hierarchical strategy\" for rank assignment is mentioned but not detailed\n\n6. **Weak empirical analysis**: The claim that combined weights have \"noticeably larger singular values\" (Figure 2) lacks quantitative support and statistical testing."}, "questions": {"value": "1. What is the actual computational overhead of the dynamic selection process? Computing SVD for both weights seems expensive. Can you provide wall-clock time comparisons?\n\n2. Why limit evaluation to DeiT models? Have authors tested on ViT-Large, Swin Transformers, or recent models like DINOv2? How does the method scale?\n\n3. Can authors please provide a theoretical analysis?  Why should unilateral decomposition preserve more information than combined decomposition from an information-theoretic perspective?\n\n4. How does UniSVD compare with structured pruning or quantization methods in terms of accuracy-efficiency trade-offs?\n\n5. What is the sensitivity to rank selection? How robust is the dynamic selection to different target ranks?\n\n6. Why do parameter counts differ in Table 1 between FLAR-SVD and other methods for the same model?\n\n7. Can you provide quantitative analysis for Figure 2? Statistical tests comparing singular value distributions would strengthen your claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MtrgDVuwm2", "forum": "MJsHf2oHzP", "replyto": "MJsHf2oHzP", "signatures": ["ICLR.cc/2026/Conference/Submission4086/Reviewer_3Es2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4086/Reviewer_3Es2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962727529, "cdate": 1761962727529, "tmdate": 1762917171235, "mdate": 1762917171235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response 1 (for Reviewers 3Es2, 1opc, B938, Rygs)"}, "comment": {"value": "We sincerely appreciate all reviewers for their careful reading of our paper and for the constructive comments and suggestions. We have carefully addressed all concerns raised in the reviews and updated our draft where appropriate to reflect the corresponding clarifications and additional experiments. We hope that our responses sufficiently resolve the raised issues, and if there are any further points that require clarification, we would be willing to address them in detail during the rebuttal period.\n\n**Extensibility of UniSVD on Recent and Larger Models**\n- DINOv2\n\n| Method | MHA Params (M) | Top-1 | Latency (ms) |\n|-----|------:|------:|------:|\n| Baseline  | 28.3  | 81.2  | 38.6 |\n| Per-Weight Decomposition| 22.5   | 65.5  | 37.3 |\n| Combined Weight Decomposition | 22.5  | 64.2  | 36.0 |\n| UniSVD  | 22.5  | **73.2**  | 35.7 |\n\n- ViT-Large\n\n| Method | MHA Params (M) | Top-1 | Latency (ms) |\n|-----------|-------:|------:|--------:|\n| Baseline  | 100.7 | 84.3  | 100.6 |\n| Per-Weight Decomposition | 50.3 | 75.0  | 91.4 |\n| Combined Weight Decomposition | 50.3  | 74.9  | 83.6 |\n| UniSVD | 50.3  | **77.7** | 81.1 |\n\n- Swin-Large\n\n| Method | MHA Params (M) | Top-1 | Latency (ms) |\n|-----------|-----------:|------:|--------:|\n| Baseline  | 62.8  | 86.3  | 67.8    |\n| Per-Weight Decomposition | 31.4  | 80.4  | 63.9    |\n| Combined Weight Decomposition | 31.4 | 79.2  | 58.3    |\n| UniSVD    | 31.4       | **81.2**  | 57.3    |\n\n- DeiT-Large\n\n| Method | MHA Params (M) | Top-1 | Latency (ms) |\n|--------|-----:|------:|--------:|\n| Baseline  | 100.7 | 86.8  | 99.1  |\n| Per-Weight Decomposition | 50.3  | 82.3  | 91.7    |\n| Combined Weight Decomposition | 50.3  | 81.7  | 84.5    |\n| UniSVD    | 50.3 | **83.0**  | 81.9    |\n\n*Table A. Comparison on various vision models.*\n\nTo address the constructive comments from Reviewers 3Es2, 1opc, B938, and Rygs, we conducted additional experiments on larger and more recent vision models in Table A. Specifically, we applied UniSVD to DINOv2, ViT-Large, Swin-Large, and DeiT-Large. The results show that our method can be effectively applied to these diverse architectures, while preserving performance more effectively than both per-weight and combined weight decomposition strategies. We believe these experiments help to better assess the scalability and extensibility of UniSVD. A more detailed discussion of the scalability of UniSVD is provided in ‘Discussion of UniSVD Limitations and Extensibility’ Section of our rebuttal."}}, "id": "WD4pVbdjC8", "forum": "MJsHf2oHzP", "replyto": "MJsHf2oHzP", "signatures": ["ICLR.cc/2026/Conference/Submission4086/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4086/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4086/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763742623606, "cdate": 1763742623606, "tmdate": 1763745210903, "mdate": 1763745210903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}