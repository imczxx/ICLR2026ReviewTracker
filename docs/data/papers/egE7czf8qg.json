{"id": "egE7czf8qg", "number": 7521, "cdate": 1758025743968, "mdate": 1759897848163, "content": {"title": "Path Matters: Unveiling Geometric Implicit Bias via Curvature-Aware Sparse View Optimization", "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful approach for novel view synthesis by reconstructing scenes as sets of Gaussian ellipsoids. Despite its success in scenarios with dense input images, 3DGS faces critical challenges in sparse view settings, often resulting in geometric inaccuracies, inconsistencies across views, and degraded rendering quality. In this paper, we uncover and address two key implicit biases of 3DGS reconstruction algorithm in sparse-view : (1) the model has a stronger demand for supervision signal toward regions of high curvature, and (2) the model is sensitive to the smoothness of the trajectory of the input views. To tackle these issues, we propose a novel framework that optimizes camera trajectories to maximize curvature coverage while enforcing smooth motion, and we further enhance the informativeness of data through a synthetic view generation process. Extensive experiments on Mip-NeRF 360, DTU, Blender, Tanks & Temples, and LLFF datasets show that our method substantially outperforms state-of-the-art solutions in sparse-view scenarios, both in rendering quality and geometric fidelity. Beyond these empirical gains, our investigation uncovers the subtle ways in which data representation and trajectory planning interact to shape 3DGS performance, offering deeper theoretical insights into the algorithm’s inherent biases.", "tldr": "", "keywords": ["3D Gaussian Splatting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ad8bd881c26849b79ebf5025a130a786ccd32ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a framework to improve 3D Gaussian Splatting (3DGS) under sparse-view inputs. By maximizing curvature coverage while maintaining smooth camera motion, and synthesizing new views along the optimized path via depth-guided parallax-based interpolation, it improves the rendering quality and geometry fidelity of 3DGS under sparse-view inputs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a methodical camera path optimization problem which seems effective and significantly enhances the performance of vanilla 3DGS in sparse-view settings.\n- The algorithm is clearly explained and easy to reproduce.\n- The ablation studies show the benefits of each of the proposed components."}, "weaknesses": {"value": "- Insufficient qualitative results: Apart from the toy examples in Figures 1 and 2, the main paper includes only a single set of qualitative comparisons (Figure 4) on LLFF and Mip-NeRF 360, without specifying the number of input views used. While additional qualitative results are provided in the supplementary material, comparisons on the Tanks & Temples dataset are still missing, leaving an incomplete qualitative evaluation.\n\n- Limited quantitative gains: The main issue I see in this paper is that the reported improvements over other sparse-view baselines appear marginal or inconsistent. For example, in the 3-view DTU results (Table 3), the proposed method performs slightly better than SCGaussian on PSNR (20.56 vs. 20.65) and SSIM (0.86 vs. 0.89), and significantly worse on LPIPS (0.12 vs. 0.24). Similarly, in the 3-view LLFF comparison, the method performs slightly better than MVPGS on PSNR (20.65 vs. 20.93), but performs notably worse on SSIM (0.88 vs. 0.78) and LPIPS (0.10 vs. 0.23). On 12-view Mip-NeRF360 comparison, the proposed method also only achieves very marginal improvement over MVPGS. These quantitative results make it unconvincing that the proposed approach consistently outperforms prior methods under sparse-view settings.\n\n- Missing baselines: Two strong and relevant baselines for sparse-view NVS are missing. (1) MAtCha Gaussians (CVPR 2025): this method integrates monocular depth priors to achieve high-quality novel view synthesis from sparse inputs. (2) Difix3D+ (CVPR 2025): this work tackles sparse-view NVS by progressively augmenting the training set using diffusion-based image enhancement. Including these baselines would provide a more complete and fair comparison of the proposed method’s effectiveness."}, "questions": {"value": "As noted in the weaknesses, I strongly encourage the authors to include additional qualitative results in the main paper for all datasets reported with quantitative metrics. Each figure should include clear captions indicating the number of input views. In addition, I recommend the authors to provide a side-by-side video comparison against all competitive baselines to better access the rendering qualities. Furthermore, I recommend adding both qualitative and quantitative comparisons with MAtCha Gaussians and DIFIX3D+, which are highly relevant recent methods for sparse-view novel view synthesis. I would consider raising my score if these added results and visualizations demonstrate clear and consistent improvements over the baselines.\n\nI also have two additional questions:\n- Would the proposed method benefit further from an iterative, on-the-fly pseudo-view generation strategy similar to DIFIX3D+ (CVPR 2025)? In DIFIX3D+, the authors progressively (over several iterations) render intermediate novel views from the current 3D Gaussian Splats, enhance them using a single-step diffusion model, and then add these enhanced views back into the training set to improve under-constrained regions. Could a similar iterative scheme, where synthetic views are dynamically regenerated and re-used throughout 3DGS training, instead of being pre-interpolated before training, further improve the performance of the proposed method?\n- Could the proposed method be formulated as a plug-in module applicable to any sparse-view Gaussian Splatting pipeline? For example, if integrated into more competitive baselines such as MVPGS or SCGaussian, would it yield a comparable relative improvement over those baselines as it does over vanilla 3DGS? Demonstrating such consistency in quantitative gains across different pipelines would make the contribution far more convincing, showing that the method benefits a broad class of Gaussian-based pipelines in a plug-and-play manner."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SETLKLBgAj", "forum": "egE7czf8qg", "replyto": "egE7czf8qg", "signatures": ["ICLR.cc/2026/Conference/Submission7521/Reviewer_cTEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7521/Reviewer_cTEf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760977364898, "cdate": 1760977364898, "tmdate": 1762919625067, "mdate": 1762919625067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on the reconstruction degradation issue of sparse-view 3DGS, and points out two implicit biases of it, i.e., \"stronger demand for supervision signal toward regions of high curvature\", and \"sensitive to the smoothness of the trajectory of the input views\". To this end, this work proposes a method to optimize the camera trajectory for maximizing curvature coverage. Experimental results on different datasets show that this work achieves improvements compared to several baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper is well-written, and the readers can easily get the points this work aims to propose.\n* The perspective of optimizing the camera trajectory of this work is interesting to me. However, I still have some questions for it. See more details in the Questions part.\n* The experimental results show that this work has better reconstruction quality compared to the previous methods."}, "weaknesses": {"value": "* More detailed ablations are required, like the ablations of different components in Table 4(a).\n* It is recommended to report the SSIM of Table 2 at the same time."}, "questions": {"value": "* I notice that the computatoinal costs in A.2. \"Training one model on an NVIDIA A100 takes ∼80 min; the entire sweep (5 trajectories × 4 scenes) required 2 GPU-days.\" I am confuse that why this work requires so much computational resources, because it seems that this work only optimizes the camera trjectories. If I misunderstand anything, please correct me.\n* I notice that this work has better reconstruction quality compared to the previous state-of-the-art methods, which often utilize the external pre-trained priors, like DUST3R/MAST3R for InstantSplat, monocular depth predictors for DNGaussian, etc. I also want to figure out that what external pre-trained priors this work utilizes. I only notice that this work utilizes the monocular depths on the LLFF/Mip-NeRF 360/Blender dataset, as demonstrated in A.4. I hope the authors can provide more clarity on the use of external pre-trained models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LGTNPB38wp", "forum": "egE7czf8qg", "replyto": "egE7czf8qg", "signatures": ["ICLR.cc/2026/Conference/Submission7521/Reviewer_QAJ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7521/Reviewer_QAJ4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551712607, "cdate": 1761551712607, "tmdate": 1762919624311, "mdate": 1762919624311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores implicit biases in sparse-view 3D Gaussian Splatting (3DGS): regions of high surface curvature need disproportionately more supervision, and the layout/smoothness of camera poses (summarized as a continuous trajectory) affects reconstruction accuracy. To address this, they (i) fit a B-spline camera path and optimize it to maximize curvature coverage under smoothness and proximity constraints to original camera poses, and (ii) synthesize intermediate views sampled along the optimized path using depth-based warping with a two-stage depth pipeline (COLMAP or monocular bootstrap; then 3DGS-consistent fusion). On Mip-NeRF 360 and DTU/LLFF/Tanks&Temples, they show consistent gains vs. recent sparse-view methods; they also correlate “curvature coverage” with PSNR/SSIM and geometric errors to justify the trajectory objective. Ablation analyses highlight the importance of proposed components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written overall, with the insights motivated upfront leading to the proposed method\n2. The insights pertaining to surface curvature and smoothness of pose trajectories affecting reconstruction accuracy is new and interesting\n3. The optimized trajectory leading to synthetic generation of views is a clean formulation\n4. Qualitative and quantitative results show superior performance over a broad range of baselines\n5. Ablation studies highlight the importance of the various proposed components"}, "weaknesses": {"value": "1. Given that the sparse views are combatted through synthetic view generation, it seems the proposed method trains a 3dgs with more views than other compared methods, potentially leading to an unfair comparison.\n2. The number of synthetic views generated per scene is not clearly specified: it is difficult to visualize the impact of the proposed method without this relevant information.\n3. The analysis in section 3 Figure 2 is done on only one scene, reducing the potential empirical validity of the insights"}, "questions": {"value": "1. In the ‘w/o synthetic views’ row of ablation, do you keep trajectory optimization/smoothness? would this not be naive 3DGS in terms of reconstruction and accuracy (or do you change the previously sampled poses in any way)\n2. The blending uses a z-buffer mask “with a small tolerance.” How is the tolerance chosen, and how does it trade off ghosting vs. holes across datasets?\n3. Is there any analysis of the impact of the accuracy of novel view generation (for example, sensitivity to depth estimation accuracy) on final accuracy?\n4. What’s the wall-clock overhead for trajectory optimization and synthetic view generation compared to vanilla 3DGS training? The efficiency table is helpful but doesn’t isolate these stages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pb8L5Dlsql", "forum": "egE7czf8qg", "replyto": "egE7czf8qg", "signatures": ["ICLR.cc/2026/Conference/Submission7521/Reviewer_FqdL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7521/Reviewer_FqdL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762199718424, "cdate": 1762199718424, "tmdate": 1762919623861, "mdate": 1762919623861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the behavior of 3D Gaussian Splatting (3DGS) in sparse-view settings and reports that performance degradation is linked to geometric and trajectory-related factors. The authors identify two types of implicit bias—dependence on curvature coverage and sensitivity to camera trajectory smoothness—and propose an optimization framework that adjusts camera paths and augments supervision through synthetic views. Experiments on multiple benchmarks show consistent improvements in both visual quality and geometric accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1 The paper explores a relevant and timely topic in the area of 3D reconstruction and view synthesis. The problem of sparse-view degradation is practically important and underexplored.\n\n2 The analysis of geometric and trajectory effects provides a new perspective on 3DGS behavior and contributes useful insights to understanding implicit bias in reconstruction models.\n\n3 The proposed optimization framework is technically sound and well motivated, combining curvature-aware path planning with synthetic view construction in a coherent manner.\n\n4 Experimental results are extensive and demonstrate consistent improvements over several recent baselines on diverse datasets. The ablation study also supports the contribution of individual components.\n\n5 The paper is clearly written and well organized, making the methodology and findings easy to follow."}, "weaknesses": {"value": "1 The analysis of implicit bias remains primarily empirical. Providing more formal or quantitative evidence could make the claims more rigorous and generalizable.\n\n2 The quality and reliability of the synthetic views are not deeply discussed. It would be helpful to include a brief evaluation or visualization to show their effect on reconstruction stability.\n\n3 The computational cost of trajectory optimization is not clearly presented. Additional runtime or complexity analysis would make the evaluation more complete.\n\n4 Some details in the appendix may be overly technical, while certain conceptual explanations in the main text (e.g., how curvature coverage improves supervision) could be emphasized more clearly."}, "questions": {"value": "1 How sensitive is the proposed optimization to errors in initial camera poses obtained from SfM or COLMAP?\n\n2 Does the method depend heavily on accurate curvature estimation, and how robust is it in low-texture or noisy regions?\n\n3 Could the synthetic view generation module be integrated into an end-to-end differentiable pipeline in future work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BNqqnMxtvK", "forum": "egE7czf8qg", "replyto": "egE7czf8qg", "signatures": ["ICLR.cc/2026/Conference/Submission7521/Reviewer_2dz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7521/Reviewer_2dz9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762272531296, "cdate": 1762272531296, "tmdate": 1762919623364, "mdate": 1762919623364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies two implicit biases limiting 3D Gaussian Splatting (3DGS) under sparse-view conditions: a geometric detail bias, where high-curvature regions require denser supervision, and a trajectory smoothness bias, where irregular camera paths degrade reconstruction. To mitigate these, the authors propose a curvature-aware trajectory optimization framework that maximizes curvature coverage and enforces smooth motion, coupled with synthetic view generation along optimized paths to enrich supervision. Using B-spline–based optimization, the method achieves consistent gains of  PSNR across DTU, Mip-NeRF360, LLFF, Blender, and Tanks & Temples, reaching state-of-the-art sparse-view performance. This could be potentially used for path planning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The first work to identify and thoroughly validate previously unrecognized implicit biases in 3DGS, namely curvature prioritization and trajectory smoothness.\n\n2. The paper is well written, clearly structured, and enjoyable to read.\n\n3. It suggests a promising direction for optimizing camera path planning to improve 3DGS performance, with potential applications in interactive or guided data collection for building high-quality 3DGS datasets."}, "weaknesses": {"value": "1. The current analysis is conducted primarily on synthetic datasets. It would strengthen the paper if the authors could include a real-world evaluation—capturing scenes with different camera trajectories, then re-capturing them using the proposed optimized paths. Such a practical comparison would better demonstrate the method’s effectiveness and validate its impact under real capture conditions.\n\n2. Synthetic-view generation may amplify rendering artifacts or propagate depth estimation errors, yet the paper does not analyze robustness under such noise.\n\n3. While the implicit bias analysis is insightful, parts of the implementation—trajectory smoothing and synthetic interpolation—rely on well-established engineering heuristics rather than fundamentally new learning paradigms."}, "questions": {"value": "1. How do you ensure that synthetic views generated along the optimized trajectory do not introduce photometric or geometric artifacts that might bias the training of 3DGS?\n2. Does the proposed optimization generalize across scenes with very different geometric complexity (e.g., indoor vs. outdoor, man-made vs. natural environments)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0FUqB3cqaq", "forum": "egE7czf8qg", "replyto": "egE7czf8qg", "signatures": ["ICLR.cc/2026/Conference/Submission7521/Reviewer_8JLf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7521/Reviewer_8JLf"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762894782392, "cdate": 1762894782392, "tmdate": 1762919623091, "mdate": 1762919623091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}