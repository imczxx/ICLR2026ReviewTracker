{"id": "zcniV3DRZJ", "number": 21552, "cdate": 1758318889201, "mdate": 1759896916160, "content": {"title": "Distribution-Calibrated Inference Time Compute for Thinking LLM-as-a-Judge", "abstract": "Thinking Large Language Models (LLMs) used as judges  for pairwise preferences remain noisy at the single-sample level, and common aggregation rules (majority vote, soft self-consistency, or instruction-based self-aggregation) are inconsistent when ties are allowed. We study inference-time compute (ITC) for evaluators that generate $n$ independent thinking--rating samples per item, and propose a principled, distribution-calibrated aggregation scheme. Our method models three-way preferences with a Bradley–Terry-Davidson formulation on rating counts, leveraging both polarity (margin among non-ties) and decisiveness (non-tie rate) to distinguish narrow margins from strong consensus. \nAcross various evaluation benchmarks, our approach consistently reduces MAE and increases pairwise accuracy versus standard baselines, and when evaluated against human-consensus meta-labels, matches or exceeds individual human raters. These results show that carefully allocating ITC and aggregating with distribution-aware methods turns noisy individual model judgments into reliable ratings for evaluation.", "tldr": "RLM judges are noisy; we allocate inference-time compute to sample n ratings and aggregate with a calibrated model, matching/exceeding baselines and human raters.", "keywords": ["Thinking LLM", "LLM-as-a-Judge", "Inference Time Compute"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e6314d4cae293702758445885717b57b9160cbbc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel distribution-calibrated aggregation strategy for LLM-as-a-Judge in pairwise preference tasks. The authors first highlight the importance of allowing \"tie\" judgments to mitigate LLM biases and then demonstrate that tie-rates are highly sensitive to prompt and model variations. The core proposal is a principled aggregation scheme based on a Bradley–Terry-Davidson model that leverages the full distribution of votes from multiple inference-time samples. By considering both the margin between preferences and the rate of non-ties, the method aims to create more reliable and robust evaluations that align better with human judgment."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is well-motivated and effectively presented. It clearly articulates the problem with existing aggregation methods, particularly their failure to handle ties and distributional information gracefully. The manuscript, while concise, is complete and easy to follow.\n\n- The proposed method is well-grounded in established statistical principles, using a Bradley–Terry-Davidson formulation to model three-way preferences. This provides a strong theoretical foundation for the approach.\n\n- The theoretical claims are supported by a thorough empirical evaluation across several benchmarks. The consistent improvements in MAE and pairwise accuracy over standard baselines effectively demonstrate the value of the proposed aggregation scheme."}, "weaknesses": {"value": "See questions."}, "questions": {"value": "- In Section 3, the authors convincingly demonstrate that the rate of \"tie\" judgments is highly sensitive to minor variations in prompt wording. However, it is unclear if the main experiments in Section 5 were conducted using a single, fixed prompt. To fully validate the robustness of the proposed distribution-calibrated method, it would be beneficial to show its performance across the different prompt variations introduced earlier. This would demonstrate that the calibration process can successfully normalize for prompt-induced shifts in the voting distribution.\n\n- The proposed method relies on a calibration set to fit the parameters. The size of this set is a critical hyperparameter that likely affects the final performance. The paper currently uses a fixed percentage of the data for calibration but does not explore how performance changes with varying sizes. It is recommended to conduct an ablation study on the size of the calibration set. This would provide valuable insight into the method's data efficiency and offer practical guidance for users on how large a calibration set is required to achieve stable and reliable results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rpYX7BpqFz", "forum": "zcniV3DRZJ", "replyto": "zcniV3DRZJ", "signatures": ["ICLR.cc/2026/Conference/Submission21552/Reviewer_1jJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21552/Reviewer_1jJ9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760779327401, "cdate": 1760779327401, "tmdate": 1762941832887, "mdate": 1762941832887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Distribution-Calibrated Aggregation method to improve the aggregation of thinking LLM-as-a-Judge. They first address two interesting observations that motivate their research: (1) ties are important to reduce LLM biases, and (2) Tie decisions are not stable. Based on the observations, they draw a conclusion that tie is important for LLM-as-a-judge and aggregation is needed as tie is not stable. Then, they leverage the Bradley-Terry-Davidson framework to estimate aggregation parameters based on a calibrated subset. They perform empirical studies across MT and Reward Bench tasks to show that their method outperforms baselines. \nOverall, this paper could be a nice contribution to LLM-as-a-Judge research, with the clarifications on the importance of tie, a parameter fitting method based on BT, and experiments across datasets. I have some concerns about the experiments and analysis (see weakness), but I would be willing to increase the score with the author's clarification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear research motivation addressing the importance of tie decisions in reducing LLM biases and their instability for LLM-as-a-Judge.\n2. The method is theoretically rigorous and easy to understand.\n3. Sufficient and broad experimental validation across MT and Reward Bench tasks."}, "weaknesses": {"value": "1. In comparison to the baselines, the paper uses an additional calibration set (5%-10% of test data). This indicates additional human effort when using the method for different tasks. Is it possible to fit BTD on one task and test on the other task? Or will the parameter of BTD be very sensitive to the selected task?\n2. Based on argument 1, a sensitive analysis should be presented if the proposed method is not fitted with the best parameters.\n3. Is thinking important? The proposed method is applied to the Thinking LLM-as-a-Judge; however, a general LLM-as-a-Judge may encounter the same issue. Is this because the Thinking LLM-as-a-Judge typically outperforms the general LLM-as-a-Judge? Or does the Thinking LLM-as-a-Judge introduce greater bias, thereby creating a stronger need for the proposed aggregation method?\n4. To address the second motivation, prompt template 3 naturally leads to different results as it gives the definition of accuracy and fluency, e.g.,  Accuracy can refer to both Semantic Fidelity and Lexical Fidelity. \n5. The proposed method lacks analysis with different variants, e.g., what if the model is not optimized based on accuracy instead of MAE?\n\nThings to improve the paper that did not impact the score:\n1. Lack of citations about pairwise LLM-as-a-Judge and its bias:\n[1] Liu Z, Wang P, Xu R, et al. Inference-time scaling for generalist reward modeling[J]. arXiv preprint arXiv:2504.02495, 2025.\n[2] Ye Z, Li X, Li Q, et al. Learning LLM-as-a-judge for preference alignment[C]//The Thirteenth International Conference on Learning Representations. 2025.\n[3] Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. Offsetbias: Leveraging debiased data for tuning evaluators. arXiv preprint arXiv:2407.06551, 2024.\n[4] Zheng C, Zhou H, Meng F, et al. Large language models are not robust multiple-choice selectors[J]. arXiv preprint arXiv:2309.03882, 2023.\n2. Presentation quality, e.g., lack of colon in line 247, strange italic in line 245; the algorithm is difficult to read without the context."}, "questions": {"value": "1. Note that stratification of the splits empirically did not change the results. -> Not clear.\n2. According to Figure 7, ties are very common for BTD. Will BTD tend to tie based on its optimization target? Could you provide a confusion matrix of BTD and compare it with other methods?\n3. Why do you use different models in Table 1 and Table 2?\n4. What's the selection of the fitting parameters in different datasets, and what can we observe from these selections?\n5. Will the temperature affect the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O00vMNrgdo", "forum": "zcniV3DRZJ", "replyto": "zcniV3DRZJ", "signatures": ["ICLR.cc/2026/Conference/Submission21552/Reviewer_FtNF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21552/Reviewer_FtNF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477965484, "cdate": 1761477965484, "tmdate": 1762941832632, "mdate": 1762941832632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the inconsistency of existing aggregation methods for LLM-as-a-judge systems by introducing a distribution-calibrated aggregation scheme that models the full distribution of multiple reasoning–rating samples. Using a Bradley–Terry–Davidson formulation, the method explicitly accounts for both preference margins and tie propensity, aligning inference with the MAE evaluation metric through maximum-likelihood calibration. Experiments across translation and reward evaluation benchmarks show that this approach consistently improves pairwise accuracy and matches or exceeds human rater performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is good in presentation, though it seems very obvious that the authors are trying very hard to stretch their content to 9 pages. \n- It is quite novel to reframe LLM judgement aggregation using a Bradely Terry Model. \n- The experiments show improvements over baselines."}, "weaknesses": {"value": "- In the motivation section, the author appears to make a very strong claim that 'Ties are important to reduce LLM biases. ' Though it reduces the score in numbers, it doesn't really solve the fundamental problem of the model computation mechanism that directly related to position bias. As noted by Wang et al.  [1], the core problem is the mechanism of position bias. Because of this, the claim of 'Ties are important to reduce LLM biases' is way too strong. Instead of reducing it, it seems more likely to hide the problem of position bias. \n- The author claims that  'In other baselines, in order to overcome the positional bias, we draw n/2 samples in an A-then-B response order and the remaining n/2 samples via a B-then-A order.', however, there is really no experiments nor results to validate the effect of this setup. Does this actually work? \n- For table 3, and table 4, it only shows the results from one model. I wonder how other models performed? And only three models are tested unfortunately. How about Claude and deepseek models? And how about Judgebench[2] that is specifically designed for benchmarking llm judges. \n\n[1] Eliminating Position Bias of Language Models: A Mechanistic Approach\n\n[2] JudgeBench: A Benchmark for Evaluating LLM-based Judges"}, "questions": {"value": "Please refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kuwpRLmLSC", "forum": "zcniV3DRZJ", "replyto": "zcniV3DRZJ", "signatures": ["ICLR.cc/2026/Conference/Submission21552/Reviewer_jhV9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21552/Reviewer_jhV9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615924210, "cdate": 1761615924210, "tmdate": 1762941832345, "mdate": 1762941832345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical limitation of Thinking LLM-as-a-Judge—the noise in single-sample pairwise preference judgments and the statistical suboptimality of existing aggregation methods (e.g., majority vote, soft self-consistency) when ties are allowed. The core contribution are:  \n(1) Demonstrating suboptimality of existing ITC aggregation methods for LLM judges;  \n(2) Proposing an ERM-based BTD aggregation scheme that outperforms baselines across tasks;  \n(3) Developing a consensus-based meta-evaluation for noisy labels, enabling fair human-LLM comparisons."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1, The paper identifies and addresses two understudied gaps in LLM-as-a-Judge research—tie instability across models/prompts and the loss of evidential strength in simple aggregation. The BTD-based distribution-calibrated scheme is a creative adaptation of statistical preference modeling to LLM evaluation, filling a critical methodological gap.\n\n2,  Experiments are rigorous and comprehensive. Testing across 3 LLMs (gemini-2.5-flash, qwen3-next-80b, gpt-oss-120b) and 8 benchmarks ensures generalizability.\n\n3, The paper balances technical depth with accessibility.\n\n4, Theoretically, it establishes a principled framework for ITC aggregation that aligns with evaluation metrics."}, "weaknesses": {"value": "1, More related works should be discussed. e.g. https://aclanthology.org/2024.findings-emnlp.135.pdf, https://arxiv.org/abs/2401.02009, https://arxiv.org/abs/2308.00436. For example, at the same cost, does the proposed method perform better than mirror-consistency, self-contrast & self-check?\n\n2, Generating  samples and fitting the BTD model adds computational cost. The paper does not quantify this overhead (e.g., inference time, token usage) relative to baselines e.g., Self-Consistency with n=12, making it hard to assess practicality for real-time applications.\n\n3,  The method requires a labeled calibration set. The paper does not explore scenarios where calibration data is scarce (e.g., few-shot/zero-shot settings) or domain-shifted."}, "questions": {"value": "1, What's the performance comparison between the consistency-based methods and other inference-time methods? e.g. multi-agent systems or other prompting methods like step-back https://arxiv.org/abs/2310.06117. Or let me ask in another way, why should we keep optimizing consistency-based methods, given all other prompting strategies?\n\n2, The method is mainly a prompting engineering work. Can the llm be trained to be better at calibration?\n\n3, Have you tested if calibration parameters learned on one task (e.g., RB2-Factuality) can be transferred to another (e.g., WMT ZH→EN)? If so, how much performance is lost compared to task-specific calibration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UPii6fYzlb", "forum": "zcniV3DRZJ", "replyto": "zcniV3DRZJ", "signatures": ["ICLR.cc/2026/Conference/Submission21552/Reviewer_C1Rb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21552/Reviewer_C1Rb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963489529, "cdate": 1761963489529, "tmdate": 1762941831945, "mdate": 1762941831945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}