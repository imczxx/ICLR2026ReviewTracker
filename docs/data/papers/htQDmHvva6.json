{"id": "htQDmHvva6", "number": 10434, "cdate": 1758171290500, "mdate": 1763702028434, "content": {"title": "Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning", "abstract": "Reinforcement learning with verifiable rewards has significantly advanced the reasoning capabilities of large language models, yet how to explicitly steer training toward exploration or exploitation remains an open problem. We introduce Token Hidden Reward (THR), a token-level metric that quantifies each token’s influence on the likelihood of correct responses under Group Relative Policy Optimization (GRPO). We find that training dynamics are dominated by a small subset of tokens with high absolute THR values. Most interestingly, tokens with positive THR strengthen confidence in correct outputs, thus favoring exploitation, while tokens with negative THR preserve probability mass for alternative outputs, enabling exploration. This insight suggests a natural intervention: a THR-guided reweighting algorithm that modulates GRPO’s learning signals to explicitly bias training toward exploitation or exploration. We validate the efficacy of this algorithm on diverse math reasoning benchmarks. By amplifying tokens with positive THR value and weakening negative ones, our algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse strategy yields consistent gains in Pass@K accuracy, favoring exploration. We further demonstrate that our algorithm integrates seamlessly with other RL objectives such as GSPO and generalizes across architectures including Llama. These findings establish THR as a principled and fine-grained mechanism for dynamically controlling exploration and\nexploitation in RL-tuned LLMs, providing new tools for targeted fine-tuning in reasoning-intensive applications.", "tldr": "", "keywords": ["Large language model", "Math Reasoning", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a5f35f9e735429d68d61859fec4022db885d8ff.pdf", "supplementary_material": "/attachment/591d0eb7b4c8c36e7c911833547bc42041dda670.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces Token Hidden Reward (THR), a token-level metric that quantifies how each generated token contributes to the change in the likelihood of correct responses under GRPO-style RL with verifiable rewards. The key idea is to decompose the learning dynamics into per-token “hidden rewards” whose magnitude identifies a small set of dominant tokens driving updates, and whose sign aligns with the exploration–exploitation trade-off: positive THR tends to strengthen confidence in correct outputs (exploitation), while negative THR tends to preserve probability mass for alternative outputs (exploration). Building on this insight, the authors propose a simple token-level advantage reweighting scheme that:\n\n- masks low-influence tokens (dominant-token training),\n- and amplifies tokens by sign to bias toward exploitation (p>0) or exploration (p<0)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality\n\n- A clear, token-level decomposition of learning dynamics under GRPO that cleanly ties per-token influence to exploration vs. exploitation via the sign of THR. This is a novel perspective beyond question-level hardness reweighting or per-token self-entropy methods, and extends prior analysis (e.g., negative-gradient focus) to both signs and cross-token effects \n\n- The framing of “dominant tokens” and the empirical finding that masking to high-|THR| tokens retains performance is a crisp, interpretable contribution that could inform more efficient training \n\nQuality\n- Theoretical grounding connects THR to likelihood dynamics and relates it to entropy regularization; the analysis explains why the sign of THR modulates exploration–exploitation \n\n- Broad experimental sweep across models (Qwen 0.5B/1.5B/7B; Llama3.2-3B) and RL objectives (GRPO, GSPO-token). The p>0 vs p<0 behavior is consistently demonstrated; THR compares favorably to Pass@K-mixed and COV-KL on Pass@K \n\n- Practicality: THR’s compute overhead is small relative to data generation, suggesting the technique scales. The paper explicitly reports module-wise runtimes, which is valuable for practitioners \n\nClarity\n\n- Clear definition of THR and reweighting, with equations and intuitive illustrations (e.g., sign interpretation, density plots of THR, overlap with high-entropy tokens). The narrative links theory → algorithm → empirical behavior in a readable way \n\nSignificance\n\n- A fine-grained knob to trade-off greedy accuracy vs. Pass@K with minimal engineering changes can be valuable in RLVR pipelines, where one may want different behaviors by domain or deployment constraints (e.g., high-confidence single-shot vs. BoN sampling) \n\n- The “dominant tokens” lens might inspire further token-level curricula, diagnostics, or adaptive schedules.\nAdditionally reflecting the user’s perspective: the method is appealing because computing THR is relatively lightweight compared to rollouts, making it amenable to scaling and combination with existing RLVR loops."}, "weaknesses": {"value": "Magnitude and consistency of gains\n\n- While some settings show non-trivial improvements (e.g., up to ~4 points Pass@1 on Qwen-7B), in several cases improvements over strong baselines like GRPO are modest and sometimes within typical training variance bands, especially at smaller model sizes or across certain datasets. Emphasize variance estimates (e.g., CI/error bars across seeds) to quantify significance of the deltas \n\n- The exploration benefits at large K are clear, but the practical importance of very large K (e.g., 128–256) may be less relevant for many real-world budgets. Provide a sharper focus on K in the 4–16 range and on success-vs-cost trade-offs \n\nGenerality beyond verifiable domains\n\n- The method and analysis are anchored in RLVR with binary/verifiable rewards and GRPO/GSPO-style group structures. It remains unclear how to port THR to non-verifiable domains (e.g., preference models, open-ended generation) where correctness signals are noisy/subjective. The paper mentions generalization across RL objectives but not across reward types or unverifiable tasks \n\nInteraction with error-correction behavior\n- The paper interprets negative THR as “preserving probability mass for alternative (than the correct) responses” to encourage exploration. It is less clear how this interacts with error-correction/repair behaviors, where one typically wants to actively down-weight error-inducing trajectories and prioritize corrective steps. More analysis is needed on whether amplifying negative-THR tokens inadvertently reinforces patterns that impede self-correction or verifier-guided repair \n\nScope of analysis and ablations\n\n- Threshold τ selection follows a prior influence-based heuristic. Sensitivity analyses on τ, the fraction of tokens retained, and p schedules are limited in the main text. Given the centrality of these choices, ablate:\nabsolute vs. percentile thresholds for |THR|,\ndynamic τ over training, adaptive p tied to q (group accuracy) or per-question difficulty \n\n- Baselines could include token-level reweighting heuristics that don’t require THR (e.g., top-|grad| tokens, per-token loss magnitude, or simple entropy-top-k), to isolate THR’s unique value beyond “focus on hard tokens.” Some overlap with entropy is discussed, but stronger head-to-head controls would increase confidence \n\nReporting and diagnostics\n\n- The paper makes strong claims about cross-token interactions; more direct diagnostics (e.g., intervention studies that swap token subsets, or causal tests across positions) would bolster this claim beyond correlation/overlap with entropy \n\n- Calibration and stability analyses are missing. Since THR aims to shape confidence, report calibration metrics, entropy dynamics over training, and instabilities (e.g., length collapse, variance across seeds) more prominently for each setting"}, "questions": {"value": "Interesting work, I wonder how this ties to goal-conditioned RL and if there some applications there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "W9Qt9AJUcp", "forum": "htQDmHvva6", "replyto": "htQDmHvva6", "signatures": ["ICLR.cc/2026/Conference/Submission10434/Reviewer_on7i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10434/Reviewer_on7i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781142276, "cdate": 1761781142276, "tmdate": 1762921739623, "mdate": 1762921739623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Token Hidden Reward (THR), a token-level metric that quantifies how individual tokens influence the change in the likelihood of correct responses in LLM-RL training. First, the paper analyzes how THR is distributed in correct and incorrect responses and it shows that majority of tokens have 0 THR values and small subset of tokens have large positive and negative values. Based on this insight, the paper proposes two training scenarios with THR, THR-only, where advantages of only tokens with sufficiently large THR values are used, and THR with exploration or exploitation, where reweighting advantages based on THR values. In experiments, THR-only shows comparable performance to the original GRPO variant, which indicates that only small subset of tokens influence training performance. The experiments also show that reweighting advantages based on THR values can steer exploration and exploitation and lead to improved performance in math tasks. Lastly, the paper shows connection between THR and entropy, which indicates that high THR tokens and high entropy tokens are overlapped."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and comprehensive. The concept of THR is interesting and generic to LLM-RL settings. And, the nice thing is that the THR values can be computed without additional training components so that it can be essentially used in a plug-and-play style. Also, the paper shows that the THR has an interesting property, which is that small subsets of tokens with large THR values dominate training performance. This fact is a new insight along with how token entropies involve LLM-RL training. The paper provides extensive experiments to show how THR can steer exploration and exploitation and how it can improve performance with math tasks."}, "weaknesses": {"value": "Although I couldn't find obvious flaws in this paper, there some comments that should be addressed:\n- How much does THR calculation add computation cost? I imagine that computing THR every time is not a trivial thing to do. The authors should provide information about this.\n- This is a fundamental question about the proposal in the paper. Since THR and entropy are high correlated, can we just use the entropy as a proxy of THR? If so, the implementation of algorithms with THR could be simplified. Is there a practical reason to keep using THR over entropies? This seems clarified in the manuscript."}, "questions": {"value": "`Weakness` section includes questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xkbaEeArxa", "forum": "htQDmHvva6", "replyto": "htQDmHvva6", "signatures": ["ICLR.cc/2026/Conference/Submission10434/Reviewer_zvLX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10434/Reviewer_zvLX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963311224, "cdate": 1761963311224, "tmdate": 1762921738633, "mdate": 1762921738633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes THR, a token-level signal derived from the training dynamics of GRPO-style RLVR that estimates whether a token will increase or decrease the likelihood of correct responses. The method operationalizes THR in two ways: (i) dominant-token selection that keeps only tokens with large |THR| and (ii) sign-based reweighting that tilts updates toward positive-THR tokens (exploitation) or negative-THR tokens (exploration). The stated goal is to provide a principled, fine-grained knob to trade off exploitation vs. exploration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is anchored in a theoretical motivation (training-dynamics view under GRPO) rather than ad-hoc heuristics. It is intuitive that upweighting positive-THR tokens should push the model toward already promising trajectories, while emphasizing negative-THR tokens should diversify candidates.\n- The structure of the paper is clean and logical, which makes it straightforward to map claims to evidence. Notation is compact, figures/tables are readable, and the roles of the sign-weighting scalar are explained sufficiently to re-implement. The exposition makes the method feel accessible to practitioners who already run GRPO/GSPO."}, "weaknesses": {"value": "- The paper mentions training for ~40 steps, which is typically short for convergence in RLVR settings. From an evaluator’s perspective, this makes it hard to judge stability and whether gains persist or are transient. It would strengthen the paper to include training curves.\n- Prior work (e.g., DAPO) shows that increasing clipping threshold (clip higher) can implicitly encourage exploration by preserving higher-entropy tokens. Given the paper’s token-level framing, a “clip-higher” baseline in Table 2 would be an informative."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jsKB8zBoxP", "forum": "htQDmHvva6", "replyto": "htQDmHvva6", "signatures": ["ICLR.cc/2026/Conference/Submission10434/Reviewer_H57a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10434/Reviewer_H57a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762096353241, "cdate": 1762096353241, "tmdate": 1762921738304, "mdate": 1762921738304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Token Hidden Reward (THR), a token-level metric that quantifies each token’s impact on correct-response likelihood in popular GRPO frameworks. The authors show that a small subset of high-THR tokens dominates training, enabling fine-grained control of the exploration–exploitation trade-off: positive THR drives exploitation, while negative THR promotes exploration. The proposed THR-guided reweighting steers learning toward exploration/exploitation, improving greedy accuracy (p > 0)/Pass@K (p < 0) on math reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The introduced THR framework is principled and fundamental, and connects learning dynamics in GRPO-tuned LLMs to the exploration–exploitation trade-off. This formulation offers a conceptually grounded and theoretically principled perspective that deepens understanding of how token-level interactions shape model behavior during reinforcement learning.\n\n- The paper is generally well written, logically structured, and clearly motivated, making it easy to follow. The authors conduct extensive experimental studies across multiple model scales (0.5B–7B) and architectures (Qwen and Llama), complemented by additional analyses and ablation experiments. Collectively, these results provide solid empirical support for the proposed approach and its underlying claims (while there are some inconsistencies and potential concerns; see weakness section)."}, "weaknesses": {"value": "- The empirical improvements reported between variants and baselines are relatively modest. Incorporating statistical significance analyses would help validate the robustness of these gains and ensure that the observed effects are not attributable to stochastic randomness.\n\n- The reported results indicate that the optimal choice of the hyperparameter p varies across model sizes and model families (e.g., Qwen vs. Llama), suggesting that the method’s efficacy may depend strongly on careful tuning. A more systematic discussion or ablation study on how p is selected—and whether its value generalizes across architectures or tasks—would strengthen the methodological soundness (please correct me if I missed such discussion).\n\n- In Table 1, the observation that p > 0 fails to consistently outperform p < 0 for smaller models (0.5B and 1.5B) appears inconsistent with the claimed interpretation that positive p promotes exploitation. This discrepancy weakens the theoretical linkage between the sign of p and behavioral control.\n\n- While experiments on Llama are included to demonstrate the generalizability of the proposed THR method, the reported results appear to hold primarily for the exploration configuration (p < 0). The exploitation experiments on Llama (Table 8) show that THR with p > 0 underperforms the GRPO baseline, suggesting that the proposed mechanism may not generalize robustly across architectures."}, "questions": {"value": "- Why does training on THR-dominant tokens yield comparable, instead of better performance compared to GRPO given its potential effectiveness in proving stronger learning signals to guide the training process? Alternatively, does it lead to faster convergence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9BZ1Ep59tq", "forum": "htQDmHvva6", "replyto": "htQDmHvva6", "signatures": ["ICLR.cc/2026/Conference/Submission10434/Reviewer_KUZ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10434/Reviewer_KUZ2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762211050999, "cdate": 1762211050999, "tmdate": 1762921737751, "mdate": 1762921737751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}