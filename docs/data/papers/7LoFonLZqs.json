{"id": "7LoFonLZqs", "number": 22257, "cdate": 1758328498575, "mdate": 1763697805764, "content": {"title": "Greater than the Sum of Its Parts:  Building Substructure into Protein Encoding Models", "abstract": "Protein representation learning has achieved major advances using large sequence and structure datasets, yet current models primarily operate at the level of individual residues or entire proteins. This overlooks a critical aspect of protein biology: proteins are composed of recurrent, evolutionarily conserved substructures that mediate core molecular functions. Despite decades of curated biological knowledge, these substructures remain largely unexploited in modern protein models. We introduce Magneton, an integrated environment for developing substructure-aware protein models. Magneton provides (1) a large-scale dataset of 530,601 proteins annotated with over 1.7 million substructures spanning 13,075 types, (2) a training framework for incorporating substructures into existing models, and (3) a benchmark suite of 13 tasks probing residue-, substructure-, and protein-level representations. Using Magneton, we develop substructure-tuning, a supervised fine-tuning method that distills substructural knowledge into pretrained protein models. Across state-of-the-art sequence- and structure-based models, substructure-tuning improves function-related tasks while revealing that substructural signals are complementary to global structural information. \nThe Magneton environment, datasets, and substructure-tuned models are all openly available.", "tldr": "Prevailing protein models ignore the fact that proteins composed of recurrent, modular substructures that are functional and evolutionarily-conserved. We think that should change.", "keywords": ["Protein", "biology", "representation learning", "benchmark", "multiscale protein models", "structure representation learning", "substructures", "motifs", "domain", "protein function", "protein structure", "protein representation learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7331a201e007f3d1e8e6b9e014b221ee3168d6bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Magneton, an integrated environment for developing substructure-aware protein representation models. Magneton comprises three components:\n\n(1) a large-scale dataset of 530 k proteins annotated with 1.7 M substructures spanning 13 k types;\n\n(2) a training framework that incorporates these substructures into existing sequence- or structure-based encoders; and\n\n(3) a benchmark suite of 13 tasks covering residue-, substructure-, protein-, and interaction-level evaluations.\n\nBuilding upon this environment, the authors propose substructure-tuning, a supervised fine-tuning strategy that distills substructural information into pretrained protein models. Experiments across six state-of-the-art encoders (150 M–650 M parameters) show consistent ≈5 % improvements in function-related tasks (e.g., EC and GO prediction) and demonstrate that substructural signals complement global structural information."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies an under-explored gap—modern protein language models neglect conserved substructures, despite their well-known biological importance. The proposed Magneton environment and “substructure-tuning” paradigm offer a novel, systematic way to integrate decades of curated biological knowledge into modern representation learning.\n\n2. The dataset construction and methodology are rigorous. The authors curate and filter substructures from major biological databases (Pfam, InterPro, DSSP, SwissProt), perform consistent train/validation/test splitting, and benchmark on 13 tasks. The experimental setup spans multiple model architectures and modalities, lending credibility to the reported gains.\n\n3. The manuscript is well-written and logically structured. Figures 1–3 clearly illustrate the motivation, architecture, and effects of substructure-tuning.\n\n4. The work lays a foundation for multi-scale protein modeling by linking residue-, motif-, and domain-level representations. This contribution is both scientifically meaningful and practically useful, likely to influence future developments in protein representation learning, function prediction, and design."}, "weaknesses": {"value": "1. Substructure information is only injected through supervised fine-tuning; the model architecture itself remains unchanged. As the authors note, this approach can be brittle under task-specific fine-tuning and may not fully exploit hierarchical dependencies. Exploring architectural biases (e.g., graph-hierarchical encoders) would strengthen the contribution.\n\n2. The dataset filters out substructures with < 75 occurrences. While this simplifies training, it biases learning toward frequent structural motifs and limits generalization to rare but functionally critical substructures.\n\n3. Substructure-tuning improves global functional prediction but sometimes degrades residue-level metrics. This trade-off warrants deeper analysis—e.g., are low-level features being overwritten, or are substructure objectives misaligned with certain downstream signals?\n\n4. The paper could better contextualize gains against concurrent structure-aware fine-tuning strategies (e.g., S-PLM, ESM-S). A direct quantitative comparison or ablation with those would clarify the incremental benefit of substructure-tuning.\n\n5. While performance tables are comprehensive, confidence intervals or variance over multiple runs are missing, which is important given modest absolute improvements."}, "questions": {"value": "1. Could the authors discuss whether hierarchical or graph-based encoders (e.g., multi-resolution GNNs) might integrate substructures more stably than fine-tuning alone? A comparison would help isolate the source of gains.\n\n2. Have the authors evaluated whether the tuned models can generalize to rare or unseen motifs not present in the training subset? Such analysis would demonstrate the learned “substructure awareness” beyond supervised classes.\n\n3. Since different substructure classes use separate heads, how correlated are their learned embeddings? Could joint attention or shared bottlenecks yield better cross-scale consistency?\n\n4. One potential advantage of substructure-aware representations is improved interpretability. Can the authors provide examples where substructure-tuned embeddings correlate with known functional motifs or mechanistic insight?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mydzJOanmN", "forum": "7LoFonLZqs", "replyto": "7LoFonLZqs", "signatures": ["ICLR.cc/2026/Conference/Submission22257/Reviewer_w8Xw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22257/Reviewer_w8Xw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761564138194, "cdate": 1761564138194, "tmdate": 1762942139034, "mdate": 1762942139034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Protein models are typically supervised with individual residues or full structures. However, there are various labeled substructures of active sites, binding sites, etc. that have not been previously used to train models. This paper introduces a dataset and framework to train sequence or structure-sequence models on these datasets. They show that the fine-tuned models improve performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Substructures are intuitive abstractions that captures various levels of protein modeling that are not immediately apparent from either structure or sequence.\nThe dataset developed by processing Swissprot is large and diverse, and is often overlooked in training protein models."}, "weaknesses": {"value": "While there is some analysis of the dataset in Table 1, it is insufficient for a paper which proposes a dataset.\nThe results are impressive but raises concerns as to how much of the test set's labels are related or derived from the SwissProt annotations.\nThe authors finetune the model using elastic weight consolidation (EWC). However, it is not clear empirically how much EWC affects downstream performance."}, "questions": {"value": "1. Beyond Table 1, what does the dataset consist of? What do the unique types consist of?\n2. How do the authors address potential data leakage between the training data present in the Magneton substructure dataset and the evaluation datasets? Are there sequence similarity cutoffs to filter the training set to ensure that the training and testing proteins are sufficiently different?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dZ1ime5BwU", "forum": "7LoFonLZqs", "replyto": "7LoFonLZqs", "signatures": ["ICLR.cc/2026/Conference/Submission22257/Reviewer_RdsB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22257/Reviewer_RdsB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887156413, "cdate": 1761887156413, "tmdate": 1762942138766, "mdate": 1762942138766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Magneton, a framework that brings protein substructure knowledge into representation learning in a systematic way. The authors build a large annotated dataset with over half a million proteins and around 1.7 million substructures from six categories. They propose a substructure-tuning method that fine-tunes pretrained protein language models on multi-scale substructure classification and test its impact on 13 downstream tasks at different levels, from residues to protein interactions. The results show consistent improvements on function-related tasks such as EC, GO-MF, and thermostability, while the effects on localization and residue-level predictions are neutral or slightly negative. Overall, the work shows that substructure information complements global structural signals and offers a reproducible benchmark for studying hierarchical protein representations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper takes a fresh approach to protein representation by explicitly modeling mid-level substructures that connect sequence and overall structure. While earlier work has explored structural supervision, few have looked at substructure information at this scale or systematically tested its impact. The authors run solid and wide-ranging experiments across several model families, including ESM2, SaProt, Cambrian, and ProSST, covering six types of substructures and thirteen downstream tasks. The dataset is clearly described and easy to reproduce, and the figures and tables make the framework intuitive to follow.\n\nOverall, the work is clearly written and well-executed. It provides a practical and scalable benchmark that shows how substructure-level fine-tuning can bring meaningful gains on function-related tasks, even for models that already use 3D information. The results highlight the value of adding explicit substructure knowledge to protein models and point toward new directions for improving interpretability and representation learning in protein science."}, "weaknesses": {"value": "Lack of theoretical motivation:\nThe paper shows that substructure-tuning works, but it doesn’t really explain why. There’s no clear reasoning or framework to describe why it helps with function prediction but hurts residue-level performance. The discussion feels more like observation than real analysis.\n\nUnbalanced and oversimplified supervision:\nThe six types of substructures are not only uneven in number—some, like catalytic or binding sites, are much rarer—but also differ in biological meaning. Many of these categories, such as catalytic sites, cannot be treated as simple binary labels because they involve diverse biochemical functions rather than just “active vs inactive.” Similarly, tasks like PPI prediction or contact prediction represent complex relational signals that go beyond binary classification. The paper filters and simplifies these cases, but it doesn’t assess how this affects the model’s ability to capture functional diversity or relational structure."}, "questions": {"value": "Mechanistic validation:\nCould the authors include attention-map or embedding-similarity analyses (for example, t-SNE or clustering of substructure embeddings) to check whether substructure-tuning actually makes the model group residues or motifs by function? Comparing pre- and post-tuning representations on catalytic or binding site residues would make the mechanism more convincing.\n\n\nGeneralization and transferability:\nTo show robustness, the authors could run few-shot or zero-shot transfer experiments — for instance, testing on unseen protein families or on new substructure types not used during tuning. Measuring how performance drops as family divergence increases would make the generalization claims stronger.\n\n\nFunctional specificity of catalytic sites:\nInstead of treating all catalytic sites as one binary label, the authors could run an additional experiment focusing on specific catalytic functions (for example, hydrolases vs oxidoreductases). This would test whether substructure-tuning helps the model distinguish between different catalytic mechanisms rather than just recognizing “catalytic vs non-catalytic” regions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IIvhPY6Gw5", "forum": "7LoFonLZqs", "replyto": "7LoFonLZqs", "signatures": ["ICLR.cc/2026/Conference/Submission22257/Reviewer_DsWe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22257/Reviewer_DsWe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976192158, "cdate": 1761976192158, "tmdate": 1762942138556, "mdate": 1762942138556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Magneton, a new environment that incorporates protein substructures into protein language models. It contains (1) a large-scale dataset annotated with million-scale substructure annotations, (2) a suite of evaluation tasks,  and (3) a training method called substructure-tuning, a supervised fine-tuning method for distilling substructural information into pretrained models. Substructure-tuning shows substantial improvement on models’ ability to represent protein function."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Compiling curated substructure annotations at scale and providing packaging code + benchmark tasks is a valuable community resource. \n\n2. The suite of 13 tasks spans from residue, substructure, protein, and interaction levels. \n\n3. The paper reports consistent improvements on function-related tasks after substructure tuning, suggesting substructural priors are useful."}, "weaknesses": {"value": "1. The authors restrict to substructures that occur at least 75 times in the SwissProt dataset, corresponding to retaining only the top 10% most frequently occurring domains. Although the histogram in Appendix A supports this, more experiments on this may be beneficial. Also, this practice might limit claims about the general utility of rare/novel substructures.\n\n2. Have you tried other structure pooling techniques more than the mean pool?  E.g. learnable aggregation, or attention?"}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wX0C8C4y1d", "forum": "7LoFonLZqs", "replyto": "7LoFonLZqs", "signatures": ["ICLR.cc/2026/Conference/Submission22257/Reviewer_MQMA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22257/Reviewer_MQMA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042943369, "cdate": 1762042943369, "tmdate": 1762942138304, "mdate": 1762942138304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response (2/2)"}, "comment": {"value": "### P3: **Mechanistic understanding of task-dependent effects of substructure-tuning** (DsWe, x8Xw)\n\nTo investigate the trade-offs introduced by substructure-tuning, particularly the improvement in function prediction but degradation in residue-level tasks, **we performed a gradient conflict analysis**. Using the ESM-C 300M base model, we computed gradients for the substructure classification task and for a set of evaluation tasks, including protein-level function prediction and residue-level classification. We measured cosine similarity between gradients across batches within each task and across different tasks. While gradients for all evaluation tasks were highly consistent across batches, gradients for substructure classification had lower, although still positive, within-task similarity and were close to orthogonal to gradients for the evaluation tasks.\n\n|GO:MF|EC|Binding residue|Functional site prediction (binding)|Functional site prediction (catalytic)|Substructure classification|\n|---|---|---|---|---|---|\n|0.957 ± 0.004|0.956 ± 0.004|0.961 ± 0.007|0.964 ± 0.004|0.973 ± 0.022|0.095 ± 0.077|\nWithin task gradient similarity (mean $\\pm$ std deviation)\n\n|GO:MF|EC|Binding residue|Functional site prediction (binding)|Functional site prediction (catalytic)|\n|---|---|---|---|---|\n|-0.006 ± 0.047|0.003 ± 0.027|0.022 ± 0.056|0.002 ± 0.055|-0.005 ± 0.038|\nGradient similarity of evaluation task with substructure classification gradient (mean $\\pm$ std deviation)\n\nThese results do not fully explain the task-specific effects of substructure-tuning, but they **suggest that the behavior is not due to a simple global misalignment between the substructure objective and certain downstream tasks.** Instead, we hypothesize that our current instantiation of substructure-tuning biases the model against fine-grained residue-level distinctions, because it explicitly encourages residues within the same substructure to share similar representations. As discussed in our Future Work section, this motivates the development of methods that incorporate substructure information at the architectural level or training objectives that operate jointly across spatial and functional scales. **We thank the reviewers for raising this point, which resulted in a better understanding of the mechanisms underlying substructure-tuning.** We provide a discussion of this analysis at the end of Section 4.2 and full details in Appendix A.3.2.\n\n## Additional experiments\nIn addition to the experiments described above, we also conducted 6 additional experiments to investigate comments from individual reviewers:\n1. Alternate pooling methods, including max pooling and a learnable attention aggregation. We found that substructure-tuning is robust to the choice of pooling method.\n2. Verification of lack of data leakage.\n    - We constructed new stringent dataset split that remove from training all proteins similar to those in the test set of any evaluation benchmark. This results in no change in substructure-tuning effects, indicating the lack of data leakage. \n    - We also confirmed that improvements in GO MF prediction were not driven by representation of associated domains in the substructure training set.\n3. Direct comparison against methods for tuning sequence-based models with global structure information (ESM-S and S-PLM). Substructure-tuning compares favorably to ESM-S. Results for S-PLM are pending and will be added as soon as available.\n4. Exploration of a single, shared classification head across substructure types rather than separate heads per type. This configuration is consistent with separate heads.\n5. An explainability analysis identifying correlations between substructure-tuned embeddings and known functional motifs. In GO:MF classification, substructure-tuning increases attribution to residues within associated domains by 17% on average.\n\nWe have also improved the overall presentation by adding additional context to our work:\n1. Appendix section with detailed descriptions of Magneton dataset processing and contents \n2. Motivation and experiments supporting inclusion of elastic weight consolidation\n3. Bootstrapped uncertainty estimates for performance metrics\n\n## Limitations discussed\n\nWe have also transparently discussed remaining limitations including: (1) architectural constraints of the fine-tuning approach, (2) dataset imbalance across substructure types, and (3) trade-offs between protein-level and residue-level performance. These points are now addressed in expanded \"Limitations and Future Work\" sections.\n\n---\n\nWe believe these substantial additions, comprising 9 new experimental analyses, have significantly strengthened the scientific rigor, mechanistic understanding, and practical value of our work. We are committed to incorporating all feedback into the camera-ready version and look forward to further discussion."}}, "id": "cFB8hD91nY", "forum": "7LoFonLZqs", "replyto": "7LoFonLZqs", "signatures": ["ICLR.cc/2026/Conference/Submission22257/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22257/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission22257/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763697637440, "cdate": 1763697637440, "tmdate": 1763697637440, "mdate": 1763697637440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response (1/2)"}, "comment": {"value": "We genuinely appreciate the careful reading of our manuscript and detailed feedback from all reviewers. \n\nWe are encouraged by the reviewers’ positive assessment of our work as **“a valuable community resource\"** (MQMA), a **“point toward new directions for improving interpretability and representation learning in protein science”** (DsWe), incorporating data that **“is often overlooked in training protein models”** (RdsB), and as **“both scientifically meaningful and practically useful, likely to influence future developments in protein representation learning, function prediction, and design”** (w8Xw). The feedback has helped improve the depth, rigor, and overall value of the work.\n\nWe provide a shared response here with experiments that are applicable to points raised by multiple reviewers. We itemize these points as \"P{number}\" and refer to these within the individual reviewer replies. We have also updated the PDF of our submission with all new additions indicated with blue text.\n\n## Common points of feedback\n\n### P1: **Consequences of restricting dataset to substructures that occur at least 75 times in SwissProt dataset** (MQMA, x8Xw)\n\nThis was a very helpful suggestion, as relaxing the count cutoff used for our substructure dataset presents an opportunity for expanding the breadth of substructures represented. **We generated new versions of our dataset using more permissive cutoffs (minimum counts of 25 or 10)**, performed substructure-tuning with these datasets, and evaluated the resulting models on our benchmark suite. This experiment used ESM-C 300M with substructure-tuning using active, binding, and conserved site annotations: \n\n|Model|EC ($F_\\max$)|GO:BP ($F_\\max$)|GO:CC ($F_\\max$)|GO:MF ($F_\\max$)|Binary Localization (Accuracy)|Subcellular Localization (Accuracy)|Thermostability (Spearman's $\\rho$)|Human PPI (AUROC)|\n|---|---|---|---|---|---|---|---|---|\n|ESM-C 300M|0.688|0.307|0.416|0.429|0.871|0.703|0.648|0.917|\n|+ST (original,>=75)|0.761|0.325|0.403|0.488|0.879|0.681|0.66|0.933|\n|+ST (>=25)|0.802|0.32|0.399|0.526|0.851|0.693|0.64|0.917|\n|+ST (>=10)|0.792|0.327|0.401|0.514|0.89|0.728|0.64|0.852|\n\n\nThe table shows that the **performance of substructure-tuning is consistent across the different cutoffs used to construct the dataset. Reducing the cutoff down to 10 also greatly expands the number of substructure types included in the dataset:**\n\n|Substructure type|Full SwissProt set|Count >=75|Count >=25|Count >=10|\n|---|---|---|---|---|\n|Homologous superfamily|3511|1133|1685|2159|\n|Domain|15868|917|1964|3506|\n|Conserved site|748|356|573|691|\n|Binding site|76|48|72|74|\n|Active site|133|82|114|127|\n\nThese results are presented in Appendix A.1.2 and we have **added these expanded label sets to our dataset for public use** and believe the increased number of substructure types will be appreciated by the community.\n\n### P2: **Generalization to rare or novel substructures** (MQMA, DsWe, x8Xw)\n\nThe question of whether substructure-tuning generalizes to unseen substructures is a great one. To explore this, we used our original dataset split to construct two sets of substructure types:\n- “Seen” substructure types that were included during substructure-tuning (>= 75 occurrences in the SwissProt dataset)\n- “Unseen” substructure types that were excluded during substructure-tuning, and further filtered to remove extremely low sample size types (< 75 and >= 10 occurrences in the SwissProt dataset)\n\nWe computed embeddings for all examples of these substructure types in the Magneton test set, before and after substructure-tuning, and calculated silhouette scores to measure clustering of the embeddings of each substructure type (higher score indicates tighter grouping of substructures of the same type). We used ESM-C 300M and SaProt as representative sequence and sequence-structure models for this analysis.\n\nAverage silhouette score across classes:\n|Model|Homologous superfamily||Domain||Conserved site||Binding site||Active site||\n|---|---|---|---|---|---|---|---|---|---|---|\n| |Seen|Unseen|Seen|Unseen|Seen|Unseen|Seen|Unseen|Seen|Unseen|\n|ESM-C|-0.183|0.180|-0.184|0.201|0.279|0.466|0.378|0.641|0.490|0.476|\n|+ ST|0.339|0.584|0.486|0.652|0.830|0.747|0.882|0.894|0.933|0.816|\n|SaProt|0.079|0.301|0.122|0.412|0.534|0.623|0.613|0.796|0.714|0.701|\n|+ ST|0.478|0.684|0.554|0.717|0.796|0.764|0.843|0.938|0.912|0.866|\n\n**Substructure-tuning results in more consistent representations of both seen and unseen substructures, despite never training on any examples of the unseen substructure types**. These results indicate that **substructure-tuning encourages the models to learn general features of functional substructures, rather than just signatures of specific substructure types**. In some cases, unseen substructures cluster more tightly than seen substructures, which we hypothesize is due to the distinctiveness of these rarer substructure types. We include these results in the main text at the end of Section 4.2."}}, "id": "XMbRLXGJ9Y", "forum": "7LoFonLZqs", "replyto": "7LoFonLZqs", "signatures": ["ICLR.cc/2026/Conference/Submission22257/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22257/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission22257/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763697707973, "cdate": 1763697707973, "tmdate": 1763697707973, "mdate": 1763697707973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}