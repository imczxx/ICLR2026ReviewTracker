{"id": "TRVLwnAY4n", "number": 4033, "cdate": 1757588279123, "mdate": 1763619699716, "content": {"title": "Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized Zero-Order LLM Fine-Tuning", "abstract": "Fine-tuning Large Language Models (LLMs) is essential for adapting pre-trained models to downstream tasks. Yet traditional first-order optimizers such as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and computational costs that scale poorly with model size. In this paper, we investigate zero-order (ZO) optimization methods as a memory- and compute-efficient alternative, particularly in the context of parameter-efficient fine-tuning techniques like LoRA. We propose $\\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO SignSGD, requiring the same number of parameters as the standard ZO SGD and only $\\mathcal{O}(1)$ function evaluations per iteration. To the best of our knowledge, this is the first study to establish rigorous convergence guarantees for SignSGD in the stochastic ZO case. We further propose $\\texttt{JAGUAR Muon}$, a novel ZO extension of the Muon optimizer that leverages the matrix structure of model parameters, and we provide its convergence rate under arbitrary stochastic noise. Through extensive experiments on challenging LLM fine-tuning benchmarks, we demonstrate that the proposed algorithms meet or exceed the convergence quality of standard first-order methods, achieving significant memory reduction. Our theoretical and empirical results establish new ZO optimization methods as a practical and theoretically grounded approach for resource-constrained LLM adaptation. \n  Our code is available at https://anonymous.4open.science/r/zo_jaguar", "tldr": "We introduce new memory efficient zero-order optimizers for fine tuning LLM", "keywords": ["Optimization", "Peft", "Zero-order optimization", "Deep learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3de9ec3520dc2c766a90537e143eb4253bb18dd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies memory-efficient zero-order (ZO) optimization for LLM fine-tuning. It introduces two methods: **JAGUAR SignSGD** (a momentumized, coordinate-difference ZO variant of SignSGD with the first convergence guarantee in the stochastic ZO setting) and **JAGUAR Muon** (a ZO version of Muon that updates matrices via a Newton–Schulz–based orthogonalized direction). Each iteration uses $O(1)$ function queries, avoiding backprop and thus saving memory. Experiments on SST-2 (FT and LoRA) and on COPA/WinoGrande (LoRA; Llama-2-7B and OPT-13B) show accuracy competitive with or better than prior ZO baselines at similar or lower memory cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Theory with clear novelty:** Provides nonconvex stochastic ZO convergence guarantees for momentum SignSGD and extends analysis to a ZO Muon update; explicitly models smoothing $\\tau$ and oracle noise $\\Delta$.\n* **Practicality:** Requires only forward passes; simple to bolt onto SignSGD/Muon; compatible with both FT and PEFT (e.g., LoRA); constant query complexity per step.\n* **Empirical signals:** On several LLM backbones, results match or surpass ZO baselines while keeping GPU memory low; the coordinate-difference estimator is an attractive alternative when memory is tight."}, "weaknesses": {"value": "1. **Speed not addressed.** While this ZO estimator performs well in accuracy and memory, it often pays a notable price in wall-clock time. The paper appears to sidestep this trade-off and provides no discussion or measurements of time.\n2. **Experiments are too sparse.** Even pioneering ZO work like MeZO paired theory with extensive empirical study. As a follow-up, this paper’s experimental breadth feels insufficient (e.g., Table 2 only reports SST-2 on OPT-1.3B and RoBERTa-Large; Table 3 has just two models and two tasks). This raises concerns about robustness and generality.\n3. **Presentation could be improved.** The paper only includes tables and no figures. While figures aren’t strictly required, *convergence curves* (or equivalent trend visualizations) are essential. If figures are omitted, some other form of trend presentation should be provided."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ogm4O0q8iv", "forum": "TRVLwnAY4n", "replyto": "TRVLwnAY4n", "signatures": ["ICLR.cc/2026/Conference/Submission4033/Reviewer_vUu1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4033/Reviewer_vUu1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720861290, "cdate": 1761720861290, "tmdate": 1762917143603, "mdate": 1762917143603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes JAGUAR SignSGD and JAGUAR Muon, new zero-order (ZO) momentum algorithms addressing the memory bottleneck in large-scale LLM fine-tuning. Traditional first-order (FO) methods like SGD or Adam require excessive memory for gradient computation and storage. In contrast, this study integrates advanced momentum techniques into ZO optimization—where updates rely only on model outputs—achieving both low memory usage and stable convergence. Specifically, JAGUAR SignSGD adds coordinate-wise accumulated momentum to ZO-SignSGD, maintaining O(1) oracle calls and the same 2d+1 parameter cost while providing the first proven convergence guarantees for stochastic non-convex ZO optimization. JAGUAR Muon extends this principle to matrix-structured parameters (e.g., LoRA) through a ZO adaptation of the Muon optimizer, also with formal convergence proofs. Extensive experiments on SST-2, COPA, and WinoGrande show faster convergence, higher accuracy than previous ZO methods, and significant GPU memory savings—sometimes achieving Adam-level accuracy with several-fold less memory. Code is publicly released for reproducibility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Technical novelty: Integrates momentum mechanisms into ZO optimization without increasing parameter or oracle cost. First to combine SignSGD-style updates with coordinate momentum and to extend Muon’s matrix form into a ZO setting.\n\nTheoretical rigor: Provides full convergence analysis for stochastic non-convex ZO problems—unlike prior heuristic ZO methods—under standard Lipschitz and variance assumptions. Establishes the first theoretical guarantee for ZO-SignSGD with momentum and for ZO-Muon’s convergence rate.\n\nPractical impact: Enables memory-efficient fine-tuning on large LLMs, achieving FO-level stability with a fraction of the memory. Coordinate-wise updates maintain the momentum benefit while keeping memory linear in d.\n\nEmpirical robustness: Validated across multiple models (OPT-1.3B, OPT-13B, LLaMA2-7B, RoBERTa-Large) and tasks (SST-2, COPA, WinoGrande), consistently outperforming prior ZO baselines (MeZO, ZO-AdaMM, ZO-SignSGD). Demonstrates clear memory reduction without sacrificing accuracy.\n\nReproducibility and structure: Algorithms, proofs, and assumptions clearly presented; supplementary experiments and hyperparameter details are provided. Code availability enhances transparency."}, "weaknesses": {"value": "Baseline coverage: The most significant shortcoming is missing comparison with modern memory-efficient FO optimizers. Since these are the de facto baselines for low-memory training, quantitative benchmarks (accuracy, convergence rate, GPU memory usage, runtime) are required to validate true efficiency gains. Current experiments mainly compare to older ZO variants, leaving unclear whether JAGUAR methods outperform or merely match advanced FO approaches.\n\nNovelty scope: The algorithmic innovation—adding momentum to existing ZO-SignSGD and Muon—is incremental conceptually, though non-trivial in analysis. It represents an extension and unification rather than a new optimization paradigm.\n\nPerformance ceiling: Empirical results show convergence close to FO optimizers, but not superior. The contribution thus lies in maintaining accuracy under strict memory limits, not in achieving new SOTA performance.\n\nLoRA dependence: Experiments rely mainly on LoRA fine-tuning. The benefit in full fine-tuning remains limited; LoRA already reduces memory, so additive gains from ZO methods are smaller and not clearly separated.\n\nTheory–practice gap: Some theoretical assumptions (bounded oracle noise Δ) are strong; their practical relevance is unclear. The resulting convergence bound (ε ≥ d√(ΔL)) may constrain attainable accuracy for large d. The paper would benefit from quantitative discussion of Δ and τ–β tuning influence.\n\nScalability and delay: Coordinate-wise momentum may cause stale updates in very high-dimensional settings; analysis of multi-coordinate or mini-batch variants could strengthen generality.\n\nClarity and completeness: Notation (‖·‖S1, δ₀, τ, etc.) introduced abruptly. Limited intuition provided for τ–Δ–β trade-off. Concrete memory and runtime figures are missing—quantified GB comparisons would substantiate claims."}, "questions": {"value": "Have the authors benchmarked JAGUAR methods against AdaFactor, 8-bit Adam, or GaLore under the same settings?\n\nHow large is the runtime overhead from two forward evaluations per step, and how does total training time compare to FO optimizers?\n\nCould multi-coordinate or low-rank perturbation estimators improve convergence speed without major memory loss?\n\nHow sensitive are results to τ and β? Any recommended default values for stable tuning?\n\nIs full-parameter fine-tuning feasible for JAGUAR Muon, and what is the computational cost of the Newton–Schulz projection at scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TkSdbuYpab", "forum": "TRVLwnAY4n", "replyto": "TRVLwnAY4n", "signatures": ["ICLR.cc/2026/Conference/Submission4033/Reviewer_nh4s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4033/Reviewer_nh4s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749454187, "cdate": 1761749454187, "tmdate": 1762917143261, "mdate": 1762917143261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces memory-efficient ZO fine-tuning methods by incorporating momentum and coordinate accumulation. Author proposed ZO-SignSGD and combine it with JAGUAR, and proposes a ZO variant of the Muon optimizer. The authors establish convergence guarantees in stochastic non-convex settings, and demonstrate through experiments that these methods achieve strong performance while reducing memory usage compared to first-order training approaches."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Strength\n\n* Motivation of this paper is clear, by introducing ZO to significantly reduce memory cost of FO method.\n* This paper provide good theoretical guarantee for the proposed method."}, "weaknesses": {"value": "Weaknesses\n\n* Paper writing is confusing, introduction lists many methods, but lacks explanations and clarifies the role of the proposed method within them, It lists many technical terms but lacks brief explanations, making it somewhat difficult to read.\n* The experiments were limited in scale, mainly focusing on simple classification tasks on small datasets, and the variety of models tested was also limited, making it difficult to evaluate the generalizability of the proposed method. It's better to at least include generation tasks, or more challenging benchmarks like MMLU or MT-Bench.\n* Memory savings are shown, but training wall-clock overhead from sampling, Momentum update, Newton–Schulz steps is not reported. It's better to show a wallclock time breakdown.\n* JAGUAR Muon performs worse in full FT, paper claims it's due to non-matrix parameters, could you provide more details related to it?"}, "questions": {"value": "Please refer to the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3k2sgC8R9Q", "forum": "TRVLwnAY4n", "replyto": "TRVLwnAY4n", "signatures": ["ICLR.cc/2026/Conference/Submission4033/Reviewer_bXjE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4033/Reviewer_bXjE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881263624, "cdate": 1761881263624, "tmdate": 1762917143037, "mdate": 1762917143037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces innovative zero-order (ZO) optimization methods for memory-efficient fine-tuning of Large Language Models (LLMs), addressing the critical challenge of high memory requirements in traditional backpropagation-based approaches. The paper combines Jaguar gradient estimation with Momentum Sign-SGD and Muon."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes two extension of zeroth-order optimizers by combining the Jaguar gradient estimate wtih sign-sgd and Muno. \n2. The paper provides the convergence analysis for the proposed algorithms under mild assumptions.\nThe numerical results shows that the proposed method can achieve higher accuracy than the existing ZO algorithms in different LLM training tasks while using similar memory. The reported results show a statistical significance."}, "weaknesses": {"value": "1. In the discussion after Lemma 3.4, in line 287, the authors claims that Guassian random perturbation results in non-convergence of ZO-Sign-SGD even with momentum. However, no further justification is provided. It is hard to tell whether this is correct or not. It is unclear why using the Jaguar peturbation is better than other perturbations.\n\n2. It is also not clear to me why sign operation is required in Algorithm 1, since Momentum SGD should hanve better convergence property than Momentum SignSGD, and their implementation on a single machine should result in the same memory usage. SignSGD is only favorable when communication is required across multiple machines.\n\n3. The numerical experiment results ont reports then final accuracy and overall memory consumption. However, the convergence rate w.r.t. the oracle number is also an important result. Since the paper claims that the algorithm has good oracle efficiency, we would expect the numerical results reflecting that aspect of the proposed algorithm."}, "questions": {"value": "Please address the above weaknesses\n\nAlso, \n1. in line 1064, the last term does not match the last term in the following equation.\n2. Directly starting from a lemma of another paper is hard to follow in App. D.2.\n3. The proof steps are hard to follow and has missing steps, e.g., how line 1085 can be derived from line 1083 by using Lemma 3.4 is not quite clear. \n4. Thge summation in eq(13) should be from 0 to T-1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "t5DqFqF82Z", "forum": "TRVLwnAY4n", "replyto": "TRVLwnAY4n", "signatures": ["ICLR.cc/2026/Conference/Submission4033/Reviewer_Bwkm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4033/Reviewer_Bwkm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931936040, "cdate": 1761931936040, "tmdate": 1762917142798, "mdate": 1762917142798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces innovative zero-order (ZO) optimization methods for memory-efficient fine-tuning of Large Language Models (LLMs), addressing the critical challenge of high memory requirements in traditional backpropagation-based approaches. The paper combines Jaguar gradient estimation with Momentum Sign-SGD and Muon."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes two extension of zeroth-order optimizers by combining the Jaguar gradient estimate wtih sign-sgd and Muno. \n2. The paper provides the convergence analysis for the proposed algorithms under mild assumptions.\nThe numerical results shows that the proposed method can achieve higher accuracy than the existing ZO algorithms in different LLM training tasks while using similar memory. The reported results show a statistical significance."}, "weaknesses": {"value": "1. In the discussion after Lemma 3.4, in line 287, the authors claims that Guassian random perturbation results in non-convergence of ZO-Sign-SGD even with momentum. However, no further justification is provided. It is hard to tell whether this is correct or not. It is unclear why using the Jaguar peturbation is better than other perturbations.\n\n2. It is also not clear to me why sign operation is required in Algorithm 1, since Momentum SGD should hanve better convergence property than Momentum SignSGD, and their implementation on a single machine should result in the same memory usage. SignSGD is only favorable when communication is required across multiple machines.\n\n3. The numerical experiment results ont reports then final accuracy and overall memory consumption. However, the convergence rate w.r.t. the oracle number is also an important result. Since the paper claims that the algorithm has good oracle efficiency, we would expect the numerical results reflecting that aspect of the proposed algorithm."}, "questions": {"value": "Please address the above weaknesses\n\nAlso, \n1. in line 1064, the last term does not match the last term in the following equation.\n2. Directly starting from a lemma of another paper is hard to follow in App. D.2.\n3. The proof steps are hard to follow and has missing steps, e.g., how line 1085 can be derived from line 1083 by using Lemma 3.4 is not quite clear. \n4. Thge summation in eq(13) should be from 0 to T-1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t5DqFqF82Z", "forum": "TRVLwnAY4n", "replyto": "TRVLwnAY4n", "signatures": ["ICLR.cc/2026/Conference/Submission4033/Reviewer_Bwkm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4033/Reviewer_Bwkm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931936040, "cdate": 1761931936040, "tmdate": 1763660756509, "mdate": 1763660756509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal Revision"}, "comment": {"value": "We sincerely thank all four reviewers for their thorough reading, constructive feedback, and insightful comments. Your feedback has been invaluable in strengthening our paper. Below, we provide a comprehensive summary of the major revisions and experimental additions we have made in response to your concerns.\n\n**SUMMARY OF KEY EXPERIMENTAL CHANGES AND ADDITIONS**\n\nMotivated by the reviewers’ comments on experimental scope and practical efficiency, we substantially strengthen the empirical evaluation of our methods:\n\n1. **More challenging tasks (Main part Section 4 + Appendix C.1).**  \nWe add experiments with Gemma-7B on the HumanEval code-generation benchmark. HumanEval is a widely used dataset that tests program synthesis and functional correctness, going beyond ZO benchmark and showing that our methods remain effective on harder, generation-oriented tasks. In the camera-ready version we will also include more experiments based on the wide-spread GSM8K benchmark and more challenging MMLU and MT-Bench. We expect these additional experiments to further clarify the presented optimization algorithms capabilities and strengthen the empirical claims made in the paper.\n\n2. **Wall-clock time benchmarking (Table 5 + Appendix C.1).**  \nWe now report wall-clock time and performance accuracy for JAGUAR-SignSGD, JAGUAR-Muon, and ZO-Muon versus ZO-SGD, ZO-SignSGD and FO SGD. The results show a favorable accuracy-time trade-off for JAGUAR compared to ZO baselines and overhead from Newton-Schulz operation.\n\n3. **Ablation on hyperparameters (Appendix A.1 + A.2).**  \nWe add an ablation over the scaling factor $\\tau$ (in addition to the existing $\\beta$ ablation), showing that the method is reasonably robust and providing guidance on default choices.\n\n**ADDRESSING OTHER CONCERNS**\n\n1. **JAGUAR momentum vs standard ZO perturbations.**  \nWe clarify that the JAGUAR momentum enjoys a bound of the form $||m_t^{\\text{JAGUAR}} - \\nabla f(x^t)|| \\leq \\sigma_{\\text{ZO}}$ that does not scale with $||\\nabla f(x^t)||$, unlike standard Gaussian perturbation estimators. Table 2 supports this: ZO-SignSGD baselines perform significantly worse than JAGUAR-SignSGD under identical conditions.\n\n2. **Relation to prior works and choice of SignSGD / Muon.**  \nWe explain more clearly that JAGUAR-SignSGD extends LeZO-type methods: in addition to sparse MeZO, it uses a coordinate-wise momentum $m_t$ that keeps information in non-sampled directions. We also spell out why we build on SignSGD and Muon specifically: sign-based and Muon-style LMO optimizers show strong empirical performance for LLM training and fine-tuning.\n\n3. **Bounded oracle noise assumption.**  \nWe clarify that the bounded oracle-noise assumption on $\\Delta$ (Assumption 3.3) is standard in the ZO literature and reflects numerical noise. We show, by providing concrete examples, that this assumption is commonly used in recent works published in top-tier A* conferences and Q1 journals."}}, "id": "LvNAByydG2", "forum": "TRVLwnAY4n", "replyto": "TRVLwnAY4n", "signatures": ["ICLR.cc/2026/Conference/Submission4033/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4033/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission4033/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763619559943, "cdate": 1763619559943, "tmdate": 1763619559943, "mdate": 1763619559943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}