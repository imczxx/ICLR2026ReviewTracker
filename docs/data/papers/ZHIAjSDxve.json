{"id": "ZHIAjSDxve", "number": 11998, "cdate": 1758205128309, "mdate": 1763094604231, "content": {"title": "Beyond Markov Assumption: Improving Sample Efficiency in MDPs by Historical Augmentation", "abstract": "Under the Markov assumption of Markov Decision Processes (MDPs), an optimal stationary policy does not need to consider history and is no worse than any non-stationary or history-dependent policy. Therefore, existing Deep Reinforcement Learning (DRL) algorithms usually model sequential decision-making as an MDP and then try to optimize a stationary policy by single-step state transitions. However, such optimization is often faced with sample inefficiency when the causal relationships of state transitions are complex. To address the above problem, this paper investigates if augmenting the states with their historical information can simplify the complex causal relationships in MDPs and thus improve the sample efficiency for DRL. First, we demonstrate that a complex causal relationship of single-step state transitions may be inferred by a simple causal function of the historically augmented states. Then, we propose a convolutional neural network architecture to learn the representation of the current state and its historical trajectory. This representation learning compresses the high-dimensional historical trajectories into a low-dimensional space to extract the simple causal relationships from historical information and avoid the overfitting caused by high-dimensional data. Finally, we formulate Historical Augmentation Aided Actor-Critic (HA3C) algorithm by adding the learned representations to the actor-critic method. The experiment on standard MDP tasks demonstrates that HA3C outperforms current state-of-the-art methods in terms of both sample efficiency and performance.", "tldr": "This paper investigates if augmenting the states with their historical information can simplify the complex causal relationships in MDPs and thus improve the sample efficiency of DRL.", "keywords": ["Markov Decision Process", "Deep Reinforcement Learning", "Historical Augmentation", "Sample Efficiency"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c0b756fde412a0d07aa85929f2e653d112f57e46.pdf", "supplementary_material": "/attachment/94cc3461652539025b9ae4359cf09b587f84404e.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates whether leveraging historical information can improve performance in deep reinforcement learning (DRL), even under the Markov property assumption of MDPs. The authors design a representation learning architecture that abstracts historical state information using CNN-based models. Building on this, they propose the HA3C algorithm, which integrates these history-augmented representations into a TD3-style off-policy framework. Experimental results show that HA3C outperforms several off-policy DRL algorithms, including TD7, on Mujoco continuous control tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Provides a theoretical attempt to analyze the benefit of history augmentation.\n\n- Demonstrates promising empirical results in Mujoco environments."}, "weaknesses": {"value": "1. Motivation\n\nThe necessity of history augmentation under the Markov property assumption is not convincingly justified. The paper claims that “the causal function in Fig. 1(b) can be simpler than the causal function in Fig. 1(a)” and that “historical information can simplify complex causal relationships in MDPs,” but these statements are unclear.\n\n- Could the authors explain these two claims in easier words?\n\n- Do the authors believe that the lack of history augmentation (under the Markov property assumption) is the main bottleneck limiting the sample efficiency of off-policy DRL algorithms?\n\n- Why can’t similar benefits be achieved through careful neural network design without explicitly augmenting history in DRL?\n\n- Can the authors provide a simple empirical demonstration showing that history augmentation improves representation learning, apart from Figure 5 (which only reflects final return performance)?\n\n&nbsp;\n\n2. Literature Review\n\n- If the authors believe that history augmentation is crucial for improving sample efficiency, the related work section should include prior studies that address sample efficiency improvements in DRL.\n\n- Several existing works have incorporated causal inference into (deep) RL. The authors should compare HA3C with these studies, both in terms of (i) methodological differences and (ii) the contexts or issues being addressed.\n\n&nbsp;\n\n3. Organization and Writing\n\nThe writing and organization can be improved. In particular, the motivation should be strengthened, as discussed above. Additionally, many experimental results are deferred to the Appendix, but key results should appear in the main paper.\n\nThe experimental results in the Appendix include fewer baselines than those in the main text, please clarify this inconsistency.\n\n&nbsp;\n\n4. Clarifications Needed\n\n- Can this approach also be effective in POMDP settings? If so, can the authors provide supporting experiments?\n\n- Did the authors compare HA3C with conventional DRL algorithms (e.g., TD3 or TD7) using the same number of stacked input frames ($k$)?\n\n- In Figure 3, how is the CNN used to process $k$-multiple input frames?\n\n- Is the architecture in Figure 3 applicable to discrete action spaces?\n\n- What is the distinction between fixed encoders and target encoders in Section 4.2?\n\n- Can the authors provide the ablation with $k=2, 3$?\n\n&nbsp;\n\n5. Additional Questions\n\n- Could the algorithm be extended with an adaptive adjustment mechanism for $k$?\n\n- Can this architecture be adapted for online DRL algorithms such as PPO?"}, "questions": {"value": "Please see Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9ZCoPnfeQS", "forum": "ZHIAjSDxve", "replyto": "ZHIAjSDxve", "signatures": ["ICLR.cc/2026/Conference/Submission11998/Reviewer_Uzax"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11998/Reviewer_Uzax"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760907114325, "cdate": 1760907114325, "tmdate": 1762922989953, "mdate": 1762922989953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents HA3C, a deep reinforcement learning algorithm designed to tackle the critical issue of sample inefficiency in Markov Decision Processes  with complex transition dynamics. The central premise is that augmenting the current state with historical information can simplify the learning of these complex causal relationships. To achieve this without suffering from high-dimensional inputs, HA3C employs a Convolutional Neural Network to compress the state trajectory into a concise, low-dimensional representation. This learned historical context is then integrated into a powerful actor-critic framework, leading to significant improvements in both sample efficiency and final performance across a suite of challenging continuous control benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-organized. \n2. The paper's central contribution rests on its compelling and counter-intuitive argument for leveraging history in MDPs. This core premise is rigorously validated through comprehensive empirical evidence.\n3. Experimental results show promising results on MuJoCo and DMC tasks."}, "weaknesses": {"value": "1. On several MuJoCo tasks, such as HalfCheetah, the performance improvement over TD7, while positive, may not be statistically significant when considering the reported standard deviations. The authors should provide a more detailed analysis of the statistical significance of their results.\n2. The paper's core premise that history simplifies future prediction is not critically examined for its limitations. The authors' own finding that long histories introduce noise (for k=24) suggests potential failure modes, yet there is no broader discussion of which environmental properties might render historical information detrimental.\n3. The theoretical justification for why the proposed method reduces sample complexity is underdeveloped. The paper provides an intuitive motivation but lacks a formal analysis (e.g., sample complexity bounds) to explain the mechanism, positioning the contribution primarily as a strong empirical finding rather than a new theoretical framework.\n4. The algorithm's performance is highly sensitive to the historical window size k, a crucial hyperparameter that requires manual, task-specific tuning. The absence of a principled method for selecting k limits the algorithm's practicality and makes it difficult to apply to new environments without extensive tuning."}, "questions": {"value": "1. What mechanisms within HA3C are designed to prevent the model from overfitting to spurious correlations within historical trajectories, and can you formalize the conditions under which history might become detrimental to learning?\n2. Instead of a fixed k, have the authors considered dynamic architectures, such as using an attention mechanism, that would allow the agent to adaptively learn the relevant temporal dependencies for a given task or state?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8VmdNzYAEF", "forum": "ZHIAjSDxve", "replyto": "ZHIAjSDxve", "signatures": ["ICLR.cc/2026/Conference/Submission11998/Reviewer_L1Ki"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11998/Reviewer_L1Ki"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836375012, "cdate": 1761836375012, "tmdate": 1762922989557, "mdate": 1762922989557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "CaGvGX3H5X", "forum": "ZHIAjSDxve", "replyto": "ZHIAjSDxve", "signatures": ["ICLR.cc/2026/Conference/Submission11998/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11998/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763094603459, "cdate": 1763094603459, "tmdate": 1763094603459, "mdate": 1763094603459, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes historical augmentation to address sample inefficiency in DRL for complex MDPs. The manuscript is well-structured and easy to follow, with clear motivation (e.g., Fibonacci sequence examples), coherent theoretical-algorithmic design (Theorem 4.1, CNN-based representation learning, HA3C), and comprehensive baseline comparisons on MuJoCo and DMC tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear Problem Definition: The paper effectively highlights the sample inefficiency of existing DRL methods under complex causal relationships in MDPs, even when the Markov assumption holds.\n2. Theoretical and Algorithmic Coherence: Theorem 4.1 (existence of a k-order stationary deterministic policy) provides theoretical justification for historical augmentation, while the CNN-based representation learning (compressing high-dimensional historical trajectories to avoid overfitting) and HA3C algorithm (integrating TD3, historical augmentation, and checkpoints) form a coherent technical pipeline.\n3. Comprehensive Baseline Comparisons: Validates performance against 5 SOTA algorithms and extends to HA3C-SAC, demonstrating cross-algorithm/task adaptability."}, "weaknesses": {"value": "1. Missing key baseline: No comparison with raw concatenation of current and historical states (a standard POMDP practice). Without this, it’s unclear if HA3C’s gain comes from historical information or CNN compression—add this baseline to validate representation learning value.\n2. Unreported parameters: Table 1 lacks network parameter counts for HA3C and baselines. HA3C’s extra encoders may increase capacity; report parameters to rule out \"parameter overcapacity\" as a performance driver.\n3. Predictive loss is not new. The Section 4.1 prediction loss (predicting future state representations) replicates prior work (e.g., SPR, ICLR 2021). Explicitly compare with SPR or add ablations for unique components (e.g., pooling) to highlight insights.\n4. Unjustified focus on causal inference: In practical RL scenarios, the core goal is task solving, not explicit causal relationship inference. Neural networks may implicitly learn causal patterns (or not) as long as the task is solved—there is no evidence that explicit causal understanding improves state representation. The paper fails to justify why causal inference is necessary for the proposed method."}, "questions": {"value": "The term \"complex MDP\" is actually not very specific. The paper provides no clear criteria for identifying which MDP scenarios benefit from causal understanding, making it hard to generalize the method’s applicability. In what RL scenario does causal understanding benefit most?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hjnTq9y20e", "forum": "ZHIAjSDxve", "replyto": "ZHIAjSDxve", "signatures": ["ICLR.cc/2026/Conference/Submission11998/Reviewer_YzSK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11998/Reviewer_YzSK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996181970, "cdate": 1761996181970, "tmdate": 1762922989101, "mdate": 1762922989101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is motivated by the observation that a stationary Markov policy utilising single-step state transitions can often result in inefficient reinforcement learning, especially when the underlying causal relationships of state transitions are complex. To address this, the authors propose to augment the state with historical information, and hence learn a policy of the historically augmented state. To this end, they use a CNN to learn a compressed low-D representation of the high--D historical trajectory, which only focuses on the underlying causal relationships and avoids the overfitting due to having high-D data. The authors introduce a new algorithm, HA3C, which is a variant of the A3C method that additionally uses the learned representation. The experimental study demonstrates improved sample efficiency and performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper has strong motivation and intuition - we indeed expect that including historical information can result in better sample complexity and more efficient reinforcement learning, because it can better capture long-term causal relationships in the transition function.\n- The proposed framework is theoretically sound.\n- The experiments on various benchmarks show strong performance compared to the baselines. Overall, the authors provide strong evidence in favour of using history-augmented policies."}, "weaknesses": {"value": "- The novelty is not significant in my personal view. Self-predictive historical RL representations have been investigated in prior work. Check for instance the recent ICLR 2024 paper \"Bridging state and history representations : Understanding self-predictive RL\" by Ni et al. The idea of employing encoders together with an L2 loss or even a probabilistic f-divergence metric has been already studied in the prior literature (see, e.g., Section 4.1 of aforementioned paper and references therein). So, I feel the representation learning part covered in the current paper is not particularly new or novel. \n- Parts of the theory are already known, to the best of my understanding. Assume for instance Theorem 4.1, for whose proof the authors allocate quite some space. Isn't it a fundamental result in MDP theory that \"For any MDP, there exists an optimal Markov (stationary + deterministic) policy that achieves the same or higher expected return as any history-dependent policy.\"? In that case, isn't it obvious that a history-dependent policy will be as good as the best possible policy on the entire history? I mean, $s_{k, t}$ includes $s_t$ and possibly many more states, so it will be at least as good as using $s_t$ only; but the latter can already result in an optimal policy. To me, Theorem 4.1 looks extremely obvious, unless the authors were trying to same something different.\n- Theorem 4.2 seems correct, but I feel what would really be interesting would be to include some theory on how exactly history-dependent policies can accelerate reinforcement learning. Theorem 4.2 shows convergence for the modified Bellman operator, but seems a rather obvious result."}, "questions": {"value": "- How do the authors position their work compared to prior works on self-predictive historical RL representations? Please check for instance references in paper mentioned above. \n- Why do the authors include a proof for Theorem 4.1? Isn't this a completely obvious result for first-order MDPs?\n- Why do the authors limit their approach to A3C? Is there a particular reason why they decided to focus on the A3C variant? Can't the proposed framework be integrated with any RL algorithm in principle? I feel this would indeed be a strength of this work.\n\nOverall, I feel this is a sound framework with solid performance, but there are various things that must be clarified, in particular with respect to prior work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hhLSjYilIs", "forum": "ZHIAjSDxve", "replyto": "ZHIAjSDxve", "signatures": ["ICLR.cc/2026/Conference/Submission11998/Reviewer_AmU1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11998/Reviewer_AmU1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762197244130, "cdate": 1762197244130, "tmdate": 1762922988670, "mdate": 1762922988670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}