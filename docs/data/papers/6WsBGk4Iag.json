{"id": "6WsBGk4Iag", "number": 7385, "cdate": 1758019246178, "mdate": 1759897856023, "content": {"title": "A Comprehensive Information-Decomposition Analysis of Large Vision-Language Models", "abstract": "Large vision-language models (LVLMs) achieve impressive performance, yet their internal decision-making processes remain opaque, making it difficult to determine if the success stems from true multimodal fusion or reliance on unimodal priors. To address this attribution gap, we introduce a novel framework using partial information decomposition (PID) to quantitatively measure the ``information spectrum'' of LVLMs—decomposing a model’s decision-relevant information into redundant, unique, and synergistic components. By adapting a scalable estimator to modern LVLM outputs, Our model-agnostic pipeline profiles 26 LVLMs on four datasets across three dimensions—\\emph{breadth} (cross-model \\& cross-task), \\emph{depth} (layer-wise information dynamics), and \\emph{time} (learning dynamics across training). Our analysis reveals two key results: (i) two task regimes (synergy-driven vs. knowledge-driven) and (ii) two stable, opposing family-level strategies (fusion-centric vs. language-centric). We also uncover a consistent three-phase pattern in layer-wise processing and identify visual instruction tuning as the key stage where fusion is learned. Together, these contributions provide a quantitative lens beyond accuracy-only evaluation and offer guidance for analyzing and designing the next generation of LVLMs.", "tldr": "", "keywords": ["Interpretability", "Multimodal Learning", "Large Vision-Language Models", "Partial Information Decomposition", "Information Theory"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4cc96ef7a111b3c28737cd2ef230e9ae9fe2d32.pdf", "supplementary_material": "/attachment/d27cfc8fa1d88407427e5946ca95402769ebc597.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel framework utilizing Partial Information Decomposition (PID) to quantitatively analyze the internal decision-making strategies of Large Vision-Language Models by breaking down their decision-relevant information into redundant, unique, and synergistic components. The comprehensive analysis reveals two major task regimes (synergy-driven vs. knowledge-driven) and two opposing family-level strategies (fusion-centric vs. language-centric), providing key insights into how multimodal fusion evolves during training"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "## Strengths\n\n1.  **S1: Rigorous Information-Theoretic Framework for Quantifying Multimodality.**\n    The paper introduces Partial Information Decomposition (PID)—a sophisticated information-theoretic tool—to the domain of Large Vision-Language Model (LVLM) interpretability.\n    * This represents a significant step beyond common interpretability methods (like attention map visualization or simple ablation) by offering a **rigorous, quantitative measure** of multimodal fusion (Synergy, $S$) and unimodal reliance (Uniqueness, $U$).\n    * The PID framework provides a conceptually elegant method to precisely measure the **attribution gap** (i.e., whether success stems from fusion or unimodal priors), addressing a fundamental problem in LVLM research.\n\n2.  **S2: Comprehensive Empirical Scope and Detailed Learning Dynamics Analysis.**\n    The paper executes a highly comprehensive empirical study, analyzing **26 diverse LVLMs** across various families and capabilities (e.g., LLaVA, MiniGPT-4, BLIP-2).\n    * The scale of the analyzed models and tasks is commendable, lending strong statistical weight to the observed findings regarding task regimes (synergy-driven vs. knowledge-driven) and model strategies (fusion-centric vs. language-centric).\n    * Crucially, the analysis of **learning dynamics** (Tracing PID components across training checkpoints, e.g., LLaVA-1.5 7B vs 13B) provides novel, quantitative insights into how fusion and language priors evolve during specific training stages (Pre-training vs. Visual Instruction Tuning). This is a valuable contribution to understanding the VLM development pipeline."}, "weaknesses": {"value": "### Weaknesses\n\n1.  **W1: Crippling Restriction to Multiple-Choice VQA (Severe Lack of Generalizability).**\n    The framework's core reliance on the BATCH PID estimator restricts the analysis \\textbf{exclusively} to tasks where the output $\\mathcal{Y}$ is discrete and finite (i.e., multiple-choice VQA).\n    * This is a critical flaw because modern LVLMs are primarily valued for their \\textbf{generative, open-ended capabilities} (e.g., free-form captioning, complex dialogue).\n    * A \"comprehensive analysis\" that cannot be applied to the dominant and most challenging LVLM tasks has severely limited scope and impact on the field. The analysis is confined to only a small subset of the model's true functionality.\n\n2.  **W2: Unreliable Approximation of Unimodal Information via Noise Masking.**\n    To isolate unimodal components ($U_V, U_L$), the authors employ a \\textbf{modality masking technique}, replacing one modality's embeddings with a statistically calibrated noise sequence.\n    * This is a strong approximation that \\textbf{fundamentally alters the input distribution} to the cross-modal layers. The model's behavior under this unnatural, perturbed input may not accurately reflect its true unimodal capability on natural inputs.\n    * This raises serious doubts about the reliability and quantitative validity of the derived unique ($U$) and synergistic ($S$) information measures, which form the basis of the paper's conclusions.\n\n3.  **W3: Core Findings Lack Substantial Novelty.**\n    The paper's main conclusions largely serve as a \\textbf{quantitative validation of widely established domain knowledge}, rather than offering fundamental new insights.\n    * The distinction between *Synergy-Driven* tasks (high $S$) and *Knowledge-Driven* tasks (relying on LLM priors, high $U_L$) is an intuitive and expected outcome based on task design.\n    * The finding that Visual Instruction Tuning is the critical stage for synergy ($S$) growth merely \\textbf{quantifies the intended effect} of this alignment stage. The framework successfully describes \\textbf{what} is happening but fails to provide novel insight into \\textbf{why} it is happening.\n\n4.  **W4: Lack of Actionable Guidance and Mechanistic Explanation.**\n    The analysis is primarily \\textbf{descriptive} (e.g., identifying \"fusion-centric\" vs. \"language-centric\" model families) but fails to provide \\textbf{mechanistic explanations} for these differences (e.g., how specific architectural choices or pre-training data compositions cause this strategy). Crucially:\n    * The paper offers \\textbf{no intervention experiments} to demonstrate that the PID metrics can be used prescriptively (e.g., in a loss function) to guide or improve model design.\n    * The work remains at the \"post-hoc explanation\" level, limiting its value as a tool for engineering next-generation LVLMs."}, "questions": {"value": "## Questions\n\n1.  **Q1: Method Generalizability to Generative Tasks (Core Limitation).**\n    The paper concedes that the PID framework is restricted to multi-choice VQA tasks. Given that the primary function and value of modern LVLMs lie in **generative** and open-ended tasks (e.g., complex dialogue, zero-shot captioning), can the authors provide strong evidence or a theoretical argument that the observed information-flow strategies (e.g., the distinction between \"synergy-driven\" and \"knowledge-driven\") **reliably generalize** to these generative tasks? If the framework cannot analyze the most common usage of LVLMs, what is its practical prescriptive utility?\n\n2.  **Q2: Quantitative Rigor of the Unimodal Approximation (OOD Concern).**\n    The estimation of unimodal components ($U_V, U_L$) relies on replacing the \"missing\" modality with noise embeddings. This is an Out-of-Distribution (OOD) input to the model's language backbone. Has this OOD approximation been rigorously validated to ensure it faithfully reflects the model's behavior when a modality is truly absent? Since the values of $U_V$ and $U_L$ are contingent on this approximation, doesn't this fundamentally compromise the quantitative reliability of the derived synergy ($S$) component, which is the core subject of the analysis?\n\n3.  **Q3: Prescriptive Utility via Intervention (Guidance for Design).**\n    The paper identifies descriptive strategies like \"fusion-centric\" and \"language-centric.\" For this work to be truly impactful, it must be prescriptive. Suppose a model is diagnosed as \"language-centric.\" Can the authors propose a **specific training intervention** (e.g., modifying the loss function or cross-attention mechanism) directly based on their PID findings, and empirically demonstrate this intervention successfully **shifts the model strategy toward \"fusion-centric\"** while maintaining or improving performance? If the metric is not actionable, its value is limited to post-hoc explanation.\n\n4.  **Q4: Interpretation of Low Redundancy ($R$) across Models.**\n    In many experiments, the Redundancy ($R$) component—which measures the mutual overlap of knowledge between the two modalities—remains consistently at a low level. Does this low $R$ value imply: a) that the **knowledge overlap between modalities is genuinely negligible** even in highly-trained large models; or b) that the PID estimator (BATCH) has an **inherent systematic bias to underestimate redundancy** when dealing with high-dimensional feature representations? How do the authors reconcile and explain the persistently low $R$ values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lTt86ma1HE", "forum": "6WsBGk4Iag", "replyto": "6WsBGk4Iag", "signatures": ["ICLR.cc/2026/Conference/Submission7385/Reviewer_LJBz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7385/Reviewer_LJBz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911936649, "cdate": 1761911936649, "tmdate": 1762919510927, "mdate": 1762919510927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel analysis of Large Vision Language Models through the lens of PID, offering a quantitative framework to examine how different modalities contribute to model predictions. The authors systematically evaluate 26 models across multiple datasets, investigating cross-model and cross-task behaviors, layer-wise information dynamics, and training trajectories. Key findings include the identification of distinct model families characterized by either fusion-centric or language-centric strategies, as well as task regimes ranging from synergy-driven to knowledge-driven scenarios. These results reveal how architectural and training choices shape multimodal integration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Conceptual Novelty:** The adaptation of PID to analyze LVLMs is both innovative and timely. This approach moves beyond performance-centric evaluation by providing a principled methodology to quantify the composition of information used in model decisions. It specifically separates redundant, unique, and synergistic contributions, creating a valuable diagnostic tool for understanding multimodal fusion.\n\n2. **Extensive and Systematic Evaluation:** The scale and scope of the experimental analysis represent a significant strength. The study encompasses 26 models of varying scales and families, four diverse datasets, and multidimensional evaluations including cross-model, cross-task, layer-wise, and training dynamics analyses. This comprehensive approach enables the authors to draw robust conclusions about model strategies and behavioral patterns.\n\n3. **Actionable Insights:** The paper delivers valuable empirical findings with practical implications. The consistent distinction between fusion-centric and language-centric model families, maintained across tasks and scales, provides a new dimension for model comparison and design. Furthermore, identifying visual instruction tuning as the critical phase for emergent synergy offers concrete guidance for future training strategies."}, "weaknesses": {"value": "1.  **Validity of Unimodal Approximation:** The use of Gaussian noise to mask a modality is a significant methodological approximation. This creates a distribution shift between the training data distribution `P_train(X₁, X₂, Y)` and the evaluation distribution `P_test = P(X₁, Y) × N(μ, σ²)`. The model's behavior under this out-of-distribution `P_test` may not faithfully represent true unimodal reasoning and could introduce artifacts into the PID estimates.\n\n2.  **Limited Task Scope:** The PID framework's requirement for a discrete, finite output variable `Y` restricts the analysis to multiple-choice VQA. This is a severe limitation, as it prevents the study from probing the information-use strategies of LVLMs in their core capabilities of open-ended generation. The findings may not generalize beyond discriminative tasks.\n\n3.  **Black-Box Analysis of Reasoning:** The PID analysis provides an end-to-end, input-output decomposition (`X → Y`) but offers no insight into the *internal reasoning process*. It quantifies the information in the mapping `P(Y|X)` but ignores the latent multi-step reasoning `X → Z₁ → Z₂ → ... → Y` that may occur within the model's layers, leaving the \"chain of thought\" as a black box."}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M5hdFyKFck", "forum": "6WsBGk4Iag", "replyto": "6WsBGk4Iag", "signatures": ["ICLR.cc/2026/Conference/Submission7385/Reviewer_99um"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7385/Reviewer_99um"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762191660883, "cdate": 1762191660883, "tmdate": 1762919510284, "mdate": 1762919510284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose using partial information decomposition (PID) to analyze VLMs (such as Qwen and InternVL). Based on this PID analysis, the authors draw several conclusions. The findings include: (1) Some datasets focus more on knowledge, while others focus more on synergy. (2)Fusion-centric model families consistently prioritize cross-modal reasoning, while language-centric families rely more heavily on language priors. (3) Larger models benefit from combining inputs more effectively rather than relying heavily on language priors. (4) Information emerges in the middle-to-late layers, shifting from language-based representation building in the later layers to a decisive, synergistic fusion of modalities in the final layer. (5) Multimodal fusion is unlocked during visual instruction tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors get many findings based on the analysis tool.\n- The authors evaluate the performance of several different models.\n- The motivation of this paper is clear."}, "weaknesses": {"value": "- I noticed that Y/X1 is estimated with a uniform distribution. I suspect this is because, when using only the vision tokens, the probability for all four options is very low (perhaps the EOS token has a high probability instead). However, does this imply that the visual tokens provide no information? Intuitively, this seems incorrect, but the resulting metric might suggest it. I think a good alternative would be to block the question text and only provide the \"A, B,C, D\" choices to the model. This approach might provide a better estimation of the information derived purely from the visual tokens.\n\n- The authors state that the Partial Information Decomposition (PID) framework and the BATCH estimator are existing, well-established methods, not a novel contribution of this paper . While the goal of moving \"beyond accuracy-only evaluation\" is commendable, the paper does not sufficiently justify why this complex, information-theoretic approach is superior to simpler, more direct ablation metrics. Intuitively, one could measure fusion by calculating the delta between the full-model accuracy and the accuracy of language-only input. The authors should provide a clearer argument or a baseline comparison demonstrating what novel insights PID provides that cannot be captured by such simpler, accuracy-based metric.\n\n- Some conclusions may not be very reliable.\n\n(1) Findings 1 & 2: The paper's first findings—that VLM behavior is governed by \"synergy-driven vs. knowledge-driven\" regimes —appears to be an overly complex description of a basic dataset property. A \"knowledge-driven\" task simply seems to indicate that many questions in that benchmark can be answered correctly using only the text, without the image. This feels less like a deep insight and more like a dataset deficit or design. Furthermore, the conclusion in Finding 2 that \"model performance is dictated by S\" on synergy-driven tasks  is somewhat tautological. A \"synergy-driven\" task is defined by its need for fusion, so it is expected that a metric measuring fusion (S) would correlate with accuracy.\n\n(2) Finding 4: The analysis of scaling effects in Finding 4 and Section 4.1.2 seems to overlook a critical confounding variable: model distillation. The analysis assumes that models of different sizes (e.g., Qwen2.5-VL-3B vs. 72B) are trained independently. However, for many state-of-the-art families like Qwen2.5-VL and InternVL3, it is common practice for smaller models to be distilled from larger, more capable ones. If this is the case, the interpretation is reversed. The paper frames the results as \"gains\" in S when \"scaling\" up. But if the smaller models are distilled, the data in Table 3 would represent information \"lost\" during distillation, not \"gained\" during scaling. This confound is not addressed and could fundamentally change the interpretation of how model scale affects multimodal fusion."}, "questions": {"value": "My biggest question is that since the Y/X1 is usually a uniform distribution and the metrics calculated are based on the difference between Y/X2 and Y/(X1,X2). Why not just use the accuracy delta to measure the model performance and draw the conclusion. This paper seems to introduce another layer of abstraction and makes it harder to find the real insight from the data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oAc4f3QgTe", "forum": "6WsBGk4Iag", "replyto": "6WsBGk4Iag", "signatures": ["ICLR.cc/2026/Conference/Submission7385/Reviewer_mp9C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7385/Reviewer_mp9C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762232820698, "cdate": 1762232820698, "tmdate": 1762919509729, "mdate": 1762919509729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework based on Partial Information Decomposition (PID) to analyze the internal decision-making processes of Large Vision-Language Models (LVLMs). The authors adapt a scalable PID estimator (BATCH) to decompose model predictions into four components: redundancy ($R$), vision uniqueness ($U_1$), language uniqueness ($U_2$), and synergy ($S$). They conduct analysis across 26 LVLMs, four datasets, and three dimensions: cross-model/task comparison, layer-wise information dynamics, and training-stage evolution. The paper argues that this PID-based framework provides a more principled understanding of LVLM behavior beyond accuracy-based evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Overall, this paper is easy to follow.\n2. Introducing PID into MLLMs might be new."}, "weaknesses": {"value": "1. The analysis is restricted to multiple-choice VQA tasks with a small set of predefined answers. This narrow focus limits the applicability of the findings to open-ended generation, captioning, or reasoning tasks, which are central to LVLM capabilities. The use of a finite output space is a methodological convenience that may not reflect real-world multimodal understanding, where outputs are often continuous and compositional, typically in recent reasoning models.\n2. The PID framework is correlational, not causal. It quantifies statistical associations but does not explain how or why certain layers or training stages lead to synergy.\n3. The authors claim that model families exhibit “stable, opposing strategies,” but this is only demonstrated on four datasets.\n4. The theoretical contribution is modest: while PID is elegantly applied, the core framework is not novel, and the findings are largely empirical and descriptive."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UbTA7CLXuW", "forum": "6WsBGk4Iag", "replyto": "6WsBGk4Iag", "signatures": ["ICLR.cc/2026/Conference/Submission7385/Reviewer_dfu9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7385/Reviewer_dfu9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762317368381, "cdate": 1762317368381, "tmdate": 1762919509351, "mdate": 1762919509351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an analysis framework based on Partial Information Decomposition (PID), using a BATCH estimator to quantify the redundancy (R), visual uniqueness (U1), linguistic uniqueness (U2), and synergy (S) of decision-related information in LVLMs. The authors conduct a three-dimensional analysis across models/tasks, within-layer information flow, and training stages on 26 open-source LVLMs across four multiple-choice VQA datasets. The main findings are: the models exhibit two information usage patterns, “synergy-driven” and “knowledge-driven”; the model families display two stable strategies, “fusion-centric” and “language-centric”; a three-stage information pattern appears within typical model layers; multimodal synergy primarily forms during the visual instruction tuning stage."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novelty: It is the first systematic application of a scalable PID estimator to large-scale LVLMs, providing a multidimensional quantitative analysis.  \n\n2. Scale and Coverage: The workload is substantial, and the experimental matrix is quite comprehensive, supporting family-level induction.  \n\n3. Discoveries with explanatory value: Quantitative insights into task attributes, model strategies, within-layer flow, and training stages provide references for future architectural design/evaluation.  \n\n4. Method engineering: No model modifications are necessary; inference alone suffices, accompanied by confidence gating and soft aggregation to reduce estimation noise."}, "weaknesses": {"value": "1. Task limitations: All experiments are confined to “multiple-choice VQA,” with a very small answer space. The authors acknowledge that BATCH requires discrete Y but should still discuss the feasibility of extending the framework to open generative tasks (e.g., discretization strategies, sample efficiency).  \n\n2. The authors do not conduct interventions to high S (e.g., occluding images or rearranging text) for validation. Adversarial ablation experiments could be added to support causal conclusions.  \n\n3. This paper is merely an exploratory study of existing conclusions and does not propose any new algorithms."}, "questions": {"value": "1. What effects would appropriate interventions have on cases with high S? \n\n2. I am more concerned about whether the authors have conducted more fine-grained experiments — for example, in multimodal scenarios where answering the question necessarily requires image information, such as geometry problems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GbhESYPSCo", "forum": "6WsBGk4Iag", "replyto": "6WsBGk4Iag", "signatures": ["ICLR.cc/2026/Conference/Submission7385/Reviewer_Das5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7385/Reviewer_Das5"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762886278987, "cdate": 1762886278987, "tmdate": 1762919508966, "mdate": 1762919508966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}