{"id": "2za3iNkwXn", "number": 19167, "cdate": 1758294050352, "mdate": 1759897055262, "content": {"title": "When Reasoning Meets Compression: Understanding the Effects of LLMs Compression on Large Reasoning Models", "abstract": "Compression methods, including quantization, distillation, and pruning, improve the computational efficiency of large reasoning models (LRMs). However, existing studies either fail to sufficiently compare all three compression methods on LRMs or lack in-depth interpretation analysis. In this paper, we investigate how the reasoning capabilities of LRMs are compromised during compression, through performance benchmarking and mechanistic interpretation. To uncover the effects of compression on reasoning performance, we benchmark quantized, distilled, and pruned DeepSeek-R1 models on four reasoning datasets (AIME 2024, FOLIO, Temporal Sequences, and MuSiQue). To precisely locate compression effects on model weights, we adapt difference of means and attribution patching techniques, focusing on the activation of every linear component in compressed LRMs, to interpret fine-grained causal relationships between weights and various reasoning capabilities. This fine-grained interpretation addresses a fundamental question of compression: which weights are the most important for reasoning? Overall, we find dynamically quantized 2.51-bit R1 reaches close-to-R1 performance. With empirical verification, we present three main findings that generalize across both Llama and Qwen: (1) Weight count has a greater impact on LRMs' knowledge memorization than reasoning, highlighting the risks of pruning and distillation; (2) The MLP up projection in the final layer of distilled LRMs is one of the most important components, offering a new perspective on locating critical weights — a fundamental problem in model compression; and (3) Current quantization methods overly compress the final-layer modules and MLP gate projections, so protecting just 2% of all weights that are excessively compressed can raise average accuracy by 6.57%, greatly surpassing the state-of-the-art.", "tldr": "", "keywords": ["LLMs Compression", "LRMs Compression", "Quantization", "Pruning", "Distillation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ac48e4c1cb96d8e318784a3c9cf569964e61390.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how compression methods including quantization, distillation, and pruning, affect the reasoning abilities of large reasoning models (LRMs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper is clearly written, with a well-structured presentation that is easy to follow.\n\n(2) The motivation is articulated in a clear and convincing manner.\n\n(3) The study provides a comprehensive analysis and offers valuable new insights."}, "weaknesses": {"value": "(1) The chosen reasoning models are all based on DeepSeek-R1 or its distilled variants, which may constrain the generality of the claims. It is unclear whether the findings would also hold for other reasoning models such as GPT-OSS-20B or GPT-OSS-120B.\n\n(2) The proposed layer-importance locating method is not entirely convincing. For example, Figure 2 suggests that the first-layer weights are relatively unimportant, yet Table 3 shows that quantizing the 1_up component causes the largest performance drop on AIME 2024. This apparent contradiction raises concerns about the reliability of the identified importance scores.\n\n(3) Minor concerns: the tick labels in Figure 2 are not clearly visible, and there is a typo in line 483 (“imrpving” → “improving”)."}, "questions": {"value": "(1) Can the findings in this paper also hold for other reasoning models such as GPT-OSS-20B/120B?\n\n(2) Why quantizing the 1_up component causes the largest performance drop on AIME 2024 but still keeps relatively good performence on other tasks.  \n\n(3) Does the identified important component also depend on the task type/task difficulties?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aam2QtVjtN", "forum": "2za3iNkwXn", "replyto": "2za3iNkwXn", "signatures": ["ICLR.cc/2026/Conference/Submission19167/Reviewer_G6hk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19167/Reviewer_G6hk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675175792, "cdate": 1761675175792, "tmdate": 1762931175282, "mdate": 1762931175282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how compression methods affect large reasoning models (LRMs), specifically focusing on DeepSeek-R1 and its variants. The authors benchmark compressed models on four reasoning datasets and employ mechanistic interpretability techniques (difference of means and attribution patching) to identify which weights are most important for reasoning capabilities. Key findings include: (1) weight count impacts knowledge memorization more than reasoning, (2) the MLP up-projection in the final layer is critically important for distilled LRMs, and (3) current quantization methods overly compress final-layer modules and MLP gate projections."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "**Comprehensive scope.** The paper systematically evaluates three major compression paradigms (quantization, distillation, and pruning) on LRMs, addressing a timely and important research gap.\n\n**Easy to read and well-structured.** The paper maintains a clear narrative flow with consistent notation, provides context for key design choices, and connects results to takeaways, which significantly improves readability."}, "weaknesses": {"value": "**Limited model coverage.**\nThe analysis centers almost exclusively on DeepSeek-R1 and its distilled variants, which makes several findings read as DeepSeek-specific behaviors rather than properties of compression that generalize across LRMs. Validating the conclusions on additional open-sourced LRM families (e.g., QwQ variants) would strengthen the generalizability claims and help disentangle model-specific effects from compression-induced phenomena.\n\n**Imbalanced treatment across compression methods.**\nThe paper allocates uneven coverage across the three compression families. Distillation is represented by off-the-shelf distilled checkpoints for black-box open-source models, quantization is explored with four distinct methods, while pruning is evaluated only with SparseGPT. This asymmetry makes the comparisons look method-specific rather than family-level and may hurt perceived fairness.\n\n**Coarse knowledge vs. reasoning disentanglement.**\nThe paper infers that parameter count chiefly affects knowledge based largely on lower MuSiQue EM/F1 compared to other tasks. However, this single contrast is not sufficient to attribute effects to knowledge retention versus reasoning capability. MuSiQue itself blends multi-hop reasoning with retrieval-like knowledge and is sensitive to prompt/context choices. As a result, the “parameter count affects knowledge” conclusion feels somewhat overstated. More fine-grained experiments would strengthen the claim, e.g., RAG vs. closed-book ablations across model sizes or other synthetic tasks that decouple reasoning from memorized facts.\n\n**Minor presentation issues.** There’s a typo (“imrpving”) in Section 6."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LNtDIZ5ADQ", "forum": "2za3iNkwXn", "replyto": "2za3iNkwXn", "signatures": ["ICLR.cc/2026/Conference/Submission19167/Reviewer_ja42"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19167/Reviewer_ja42"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918051950, "cdate": 1761918051950, "tmdate": 1762931174823, "mdate": 1762931174823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically studies the effective of different model compression strategies on the reasoning capability of LRM, by applying reasoning-diagnosis metrics such as the steering vectors. From extensive empirical studies the authors propose certain properties and locate certain modules in LRM that are most critical to reasoning, providing insights into future designs on compression methods.\n\nAfter reading the paper, I am under the impression that the paper has meaningful motivation, supported by well-designed empirical studies with strong results, and the claims are well-presented. However, the paper \n\n1. Lacks methodological novelty and diversity in the analytical framework, using only one previously-proposed metrics. Some of the main claims of the paper, such as the localization of critical modules.\n2. Could be further supported by more rigorous controlled study and evaluation metrics. \n\nTherefore I recommend weak reject, but with room for further improvement after obtaining more insights from the authors during discussions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of the paper is meaningful, as the authors focus on the effect of compression methods to (hard) reasoning tasks, which is both critical and not well-studied.\n2. From comprehensive experiments the authors provides valuable insights on the design of compression methods on LRMs, that is to pay attention to certain modules important to reasoning."}, "weaknesses": {"value": "1. The paper makes, in my opinion, a quite strong claim on localizing the reasoning ability to certain model layers. The claim that later layer is more critical to reasoning/performances is intuitive, as they have greater impact on the final output, and could be sensitive to perturbations/compressions. However the authors do not go deeper into why certain deep layers are important to reasoning. \n2. For a systematic study, the authors should consider using a more diverse set of diagnosing tools, rather than only using one framework, as it would put the general applicability of the main claim under question.\n3. In the paper, the locating of tokens related to certain reasoning behaviors seems not rigorous enough, as the authors only use GPT-4o to identify related tokens, which itself may have certain biases.\n4. Some missing discussions on the layer/module-wise compression methods [1, 2], and identifying weights important to reasoning [3]\n\n[1] Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models, https://arxiv.org/abs/2410.10912\n\n[2] Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity, https://arxiv.org/abs/2310.05175\n\n[3] Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning, https://arxiv.org/abs/2506.00772"}, "questions": {"value": "1. Could the authors add more discussions on recent works [1] on the principal weights of model weights that are related to reasoning performances. \n2. Since the choosing of tokens related to reasoning behaviors may be critical in the determination of reasoning-related modules, I wonder whether the authors have tried using other models (other than GPT-4o) to determine the tokens. In those cases, would the overall claims still hold? If so, it will strengthen the robustness of the findings.\n3. The authors proposes that certain layers/modules are more important in reasoning process, which relates to the discussion on the imbalanced quality among layers/modules. Do the authors believe that module-specific compression methods could potentially address the problem of over-compressing the important layers? There are recent works discussing module-wise compression [1, 2]\n4. For empirical evidence of layer importance (Takeaway 4.1 & 4.3, Fig. 2 & 3, etc.) it would be more intuitive if the authors could present the $I_{ml}^c$ metric for the base model (before distillation) to give the reader a better idea on how the importance of each module changes, rather than only presenting the $RI_{ml}^c$.\n\n[1] Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models, https://arxiv.org/abs/2410.10912\n\n[2] Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity, https://arxiv.org/abs/2310.05175\n\n[3] Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning, https://arxiv.org/abs/2506.00772"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZXNQd6Duys", "forum": "2za3iNkwXn", "replyto": "2za3iNkwXn", "signatures": ["ICLR.cc/2026/Conference/Submission19167/Reviewer_SXNL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19167/Reviewer_SXNL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977006841, "cdate": 1761977006841, "tmdate": 1762931174408, "mdate": 1762931174408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a two-fold analysis of the effects of compression on Large Reasoning Models (LRMs), using DeepSeek-R1 as its primary case study. First, it provides a comprehensive benchmark of three major compression families—quantization, distillation, and pruning—evaluating their impact on performance across a diverse set of reasoning datasets (AIME 2024, FOLIO, Temporal Sequences, and MuSiQue). Second, and more notably, the paper employs mechanistic interpretability techniques (adapting difference of means and attribution patching) to identify which specific model components are causally important for reasoning. The authors find that current compression methods, particularly quantization, disproportionately harm these critical components. They validate this finding by showing that selectively protecting these components (e.g., the final-layer MLP) during quantization significantly recovers lost performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novelty of Interpretation: The paper's primary strength is its application of mechanistic interpretability to the problem of model compression. It moves beyond standard \"accuracy vs. bits/size\" tables to provide a causal analysis of why and where performance degrades, which is a valuable contribution.\n\nActionable Findings: The analysis yields clear, actionable insights. The identification of the final-layer mlp.up_proj as a critical component for reasoning (in R1-distilled models) and the finding that popular quantization methods (AWQ, GPTQ) overly compress these final layers are important discoveries.\n\nStrong Validation: The experiment in Section 5.2 is compelling. Demonstrating that protecting just 2% of weights (the final-layer MLPs) from quantization can recover 6.57% in average accuracy on a 3-bit model provides strong validation for the paper's entire interpretability pipeline."}, "weaknesses": {"value": "Generalizability: The analysis is heavily centered on DeepSeek-R1 and its specific distilled variants (R1-Distill-Llama, R1-Distill-Qwen). It is unclear if the central findings (e.g., the high importance of the final-layer mlp.up_proj) are a general feature of all LRMs or an artifact of the specific distillation-with-SFT process used to create the R1 models.\n\nSubjectivity in Methodology: The interpretability analysis (Section 2.2) relies on prompting GPT-4o to locate token sequences corresponding to four specific reasoning behaviors. This labeling process seems subjective and could introduce noise or bias. The robustness of the resulting steering vectors is highly dependent on the quality of this heuristic.\n\nUnderdeveloped Pruning Analysis: While pruning is introduced as one of the three main compression methods, it is quickly dismissed after benchmarking shows it performs poorly (e.t., at 50% sparsity). The subsequent mechanistic analysis focuses almost exclusively on distillation and quantization, making the pruning aspect of the paper feel incomplete."}, "questions": {"value": "1. Can the authors comment on whether the \"final-layer importance\" finding is specific to the R1-distillation process? Have you tried applying your interpretability analysis to a standard, non-distilled model (e.g., base Llama 3) that has been fine-tuned for reasoning? Would you expect to see the same components identified as critical?\n\n2. Could you elaborate on the validation process for the GPT-4o labeling of reasoning behaviors? How sensitive are the final importance scores (and the resulting conclusions) to potential inaccuracies or inconsistencies in this automated labeling process?\n\n3. The paper notes (Takeaway 3.3) that pruning/distillation (reducing parameter count) hurts knowledge memorization (MuSiQue) more severely than reasoning (AIME, FOLIO). Quantization (reducing precision) seems to have a less detrimental effect on knowledge. Could you expand on this distinction? Why do you hypothesize that parametric knowledge is so much more sensitive to parameter count than to parameter precision?\n\nI would like to improve my scores if authors can solve my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WTsgosGljx", "forum": "2za3iNkwXn", "replyto": "2za3iNkwXn", "signatures": ["ICLR.cc/2026/Conference/Submission19167/Reviewer_PAMX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19167/Reviewer_PAMX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979005854, "cdate": 1761979005854, "tmdate": 1762931174071, "mdate": 1762931174071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how compression (quantization, distillation, pruning) impacts reasoning in LRMs (DeepSeek-R1 and distilled Llama/Qwen) via (i) a broad benchmark on AIME-2024, FOLIO, Temporal Sequences, and MuSiQue, and (ii) mechanistic analyses that compute behavior-specific importance for every linear module using difference-of-means and attribution-patching. Key claims include: 2.51-bit dynamic quantization of R1 attains near-R1 performance; the final-layer MLP up-projection is consistently most reasoning-critical; and protecting ~2% of weights (final-layer MLPs) recovers +6.57% average accuracy for 3-bit AWQ."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1、 Comprehensive scope across three compression families with clear head-to-head tables and multiple distilled sizes (70B/32B/8B/7B).\n\n2、Fine-grained mechanistic lens (module-level DoM + attribution-patching) tied to four reasoning behaviors (backtracking, uncertainty estimation, example testing, adding knowledge).\n\n3、 Actionable insight: the final-layer up-projection is most critical; selectively quantizing it alone (≈0.7% of weights) causes a large drop, while protecting final-layer MLPs at 16-bit raises a 3-bit model by +6.57% on average.\n\n4、 Useful observations on collapse points (e.g., pruning ≥50% collapses; 3-bit baselines stress on harder tasks) and knowledge vs. reasoning separation (MuSiQue)."}, "weaknesses": {"value": "1、Robustness & Statistics. Behavior labeling robustness is under-specified (prompt/threshold/seed); key results lack rank-stability and uncertainty (variance, 95% CIs, significance).\n\n2、Metric & Visualization Choice. RI plots zero out increases, risking masked compensations; provide justified alternatives (report both ↑/↓ and net change).\n\n3、Coverage & Generalization. Pruning analysis is shallow (no mechanistic view beyond collapse); external validity is limited (mainly R1-distilled 8B/7B, few non-R1 families)."}, "questions": {"value": "1) Behavior-label robustness.\nVary the behavior taxonomy/prompt/thresholds and report rank stability (Kendall’s τ / Spearman ρ) with 95% CIs; include a leave-one-behavior-out analysis.\n2) RI visualization choice.\nJustify zeroing positive RI deltas and provide an alternative view that reports both increases and decreases, plus a net-change summary.\n3) Selective protection trade-offs. Provide an accuracy vs. protected-ratio (0.5–5%) curve and the latency/memory overhead; compare protecting final-layer gate vs. up vs. both.\n4) 2.51-bit dynamic quantization details.\nSpecify the skip policy (which layers stay high-precision), calibration set, and per-module bit-allocation histograms; compare to AWQ/GPTQ under identical calibration on quantization error, attribution-preservation, and end-task metrics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lqw0KfgUQW", "forum": "2za3iNkwXn", "replyto": "2za3iNkwXn", "signatures": ["ICLR.cc/2026/Conference/Submission19167/Reviewer_ST2S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19167/Reviewer_ST2S"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762105673382, "cdate": 1762105673382, "tmdate": 1762931173645, "mdate": 1762931173645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}