{"id": "5ZSfNAvyzB", "number": 12160, "cdate": 1758206054855, "mdate": 1763717464501, "content": {"title": "CREDIT: Certified Defense of Deep Neural Networks against Model Extraction Attacks", "abstract": "Machine Learning as a Service (MLaaS) has become a widely adopted method for delivering deep neural network (DNN) models, allowing users to conveniently access models via APIs. However, such services have been shown to be highly vulnerable to Model Extraction Attacks (MEAs). While numerous defense strategies have been proposed, verifying the ownership of a suspicious model with strict theoretical guarantees remains a challenging task. To address this gap, we introduce CREDIT a certified defense against MEAs. Specifically, we employ mutual information to quantify the similarity between DNN models, propose a practical verification threshold, and provide rigorous theoretical guarantees for ownership verification based on this threshold. We extensively evaluate our approach on several mainstream datasets and achieve state-of-the-art performance. Our implementation is publicly available at: \\url{https://anonymous.4open.science/r/CREDIT}.", "tldr": "", "keywords": ["Deep Neural Networks", "Model Extraction Defense"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a2470a98ed402c0e7715c9ff5c588d96259a9a8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CREDIT, a mutual information–based model ownership verification method. By injecting Gaussian noise into model embeddings and computing the mutual information between a suspicious model and the original model, CREDIT determines whether model theft has occurred. CREDIT is the first framework to provide rigorous theoretical guarantees for model extraction attack scenarios. It establishes an upper bound on mutual information, proves its tightness, and derives probabilistic bounds for verification errors, where both Type I and Type II errors decay exponentially with the size of the verification set. Experimental results show that CREDIT achieves perfect verification accuracy (100% AUROC) with minimal utility loss (≈1%), significantly outperforming existing watermarking and fingerprinting approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is the first to provide rigorous theoretical guarantees for ownership verification under model extraction attacks, including probabilistic bounds on Type I and Type II errors.\n2. Theoretical analysis is complete and mathematically sound, encompassing the derivation of a mutual information upper bound, proof of its tightness, and a certified verification guarantee.\n3. The use of mutual information as a similarity metric between models is a novel and well-founded idea, grounded in information theory and offering a principled alternative to existing heuristic defenses.\n4. The experimental evaluation is comprehensive, covering multiple modalities (images and graphs) and eight different backbone architectures, demonstrating both generalization ability and empirical superiority over existing watermarking and fingerprinting methods."}, "weaknesses": {"value": "1. Core issue: The paper claims to provide a \"defense\" but in fact presents an \"ownership verification\" method. These are fundamentally different: a true defense should cause attacks to fail (degrade the surrogate model quality, leading to low mutual information), whereas ownership verification relies on a successful attack (the surrogate resembles the original, producing high mutual information) in order to prove ownership. The paper should clearly position itself as a model ownership verification method rather than a model extraction defense.\n2. Conceptual inconsistencies throughout the manuscript:\n   + Problem formulation is inconsistent: Definition 1 labels the object as \"Certified Defense\" but the described goal is to \"correctly distinguish surrogate from independent models\" — that is verification, not defense.\n   + Methodology contains contradictory claims: Section 3.1 states the method \"defends against MEA\", yet Theorem 1 and its discussion acknowledge that a surrogate model inherits a highly similar embedding distribution (high MI) — the method depends on a successful attack (high similarity) to verify ownership, which is incompatible with the goal of making the attack fail. Section 3.2 and Theorem 3 explicitly address \"Ownership Veri fication\" rather than \"Defense\".\n   +  Experiments follow the standard evaluation pipeline for verification methods (accuracy, utility, efficiency) and demonstrate CREDIT’s strength as a verification scheme, but the paper mislabels and markets these results as evidence of a \"defense\".\n3. Missing or unclear threat model. The attacker’s capabilities and constraints (e.g., allowed query strategies, access to auxiliary data, architecture knowledge) are not clearly specified. This omission undermines the assumptions used to design the verification threshold and makes it hard to judge effectiveness across realistic attack scenarios.\n4. As a defense, the work lacks defense-specific evaluations (e.g., attack success rates, degradation of surrogate performance under the mechanism). From the verification perspective, although Theorem 3 provides γ₁/γ₂ bounds, empirical reporting focuses on AUROC; the paper does not connect the theoretical bounds to observed γ₁/γ₂ under different settings.\n5. Baseline selection and comparison framing are misleading. The paper frames CREDIT as a defense but compares only to watermarking/fingerprinting baselines (verification methods) and does not include true defensive baselines (e.g., output perturbation, query-rate limiting, prediction poisoning). Table 3’s \"defense stage\" actually measures verification preparation (computing thresholds, building watermarks) rather than an executed defense mechanism, so the comparison dimension is inappropriate."}, "questions": {"value": "1. The paper claims to provide a “defense against model extraction attacks (MEA),” yet the proposed method relies on the surrogate model being highly similar to the original one (as stated in Theorem 1: “inherits embedding distribution highly similar to g”) in order to verify ownership. If the surrogate has already successfully replicated the original model, the attack has succeeded — how can this still be considered a “defense”?\n2. A true defense should ensure that $I(h_{sur}, g)$ → 0 (the surrogate model becomes unusable), whereas the proposed verification relies on $I(h_{sur}, g)>\\tau$  (the surrogate remains similar). How can these two opposing objectives coexist under the same definition of “defense”?\n3. Table 2 reports 100% AUROC, implying high mutual information between the surrogate and the original model. Please report the actual accuracy of the surrogate models in your experiments. If their accuracy is close to that of the original model (Table 1 shows 94.67%), doesn’t this indicate that the attack has already succeeded?\n4. The paper only compares CREDIT with ownership verification methods (watermarking and fingerprinting), without including active defense techniques such as output perturbation or query limitation. If CREDIT is claimed to be a “defense method,” why is there no comparison of defensive effectiveness (e.g., attack success rate) with such defense baselines?\n5. As a “defense method,” why does the paper not evaluate (1) the attack success rate and (2) the trade-off between defense strength and model utility? Focusing solely on verification accuracy (AUROC) seems to indicate that this is a verification-oriented work rather than a defense.\n6. Table 3 refers to the computation of the threshold τ as the “defense stage,” but this appears to be a preparation step for verification rather than an actual defense mechanism. What is the true defense component of CREDIT? How does CREDIT actively prevent or weaken the attack during the extraction process?\n7. The proposed method appears conceptually similar to existing watermarking and fingerprinting approaches: (1) it does not prevent model extraction success, (2) it performs ownership verification *after* an attack, and (3) it provides some form of theoretical or empirical guarantee. Beyond introducing mutual information and theoretical bounds, what is the fundamental difference between CREDIT and prior verification-based methods?\n\nIf the paper were reframed as *“Certified Ownership Verification for Model Extraction Detection”* instead of *“Defense Against Model Extraction Attacks,”* would this resolve most of the our concerns? Would your core technical contributions still hold under this reframed title? Why insist on using the term *defense*, which appears conceptually misleading in this context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "24Rx1vo7bN", "forum": "5ZSfNAvyzB", "replyto": "5ZSfNAvyzB", "signatures": ["ICLR.cc/2026/Conference/Submission12160/Reviewer_fnV4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12160/Reviewer_fnV4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761047591057, "cdate": 1761047591057, "tmdate": 1762923115760, "mdate": 1762923115760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CREDIT, a defense against Model Extraction Attacks where adversaries steal machine learning models via API access. The core contribution is a certified defense mechanism that provides strong, theoretical guarantees of its effectiveness. The method works by injecting a subtle, statistically-verifiable fingerprint into the model's outputs using Gaussian noise and then using a mathematically-derived threshold based on mutual information to reliably determine if a suspicious model is a stolen copy. This approach requires no extra model training for verification and systematically balances the trade-off between the model's performance and its security."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Strong theoretical foundation\n- Does not require training extra models\n- A thorough and comprehensive set of experiments"}, "weaknesses": {"value": "CREDIT relies on injecting a certifiable statistical fingerprint into model outputs via a Gaussian noise. However, an adversary could collect noisy outputs from the defended API and train a secondary model, such as a denoising autoencoder, to learn and remove the injected noise. The adversary could then use this denoiser as a pre-processing step to create a 'clean' training set for their surrogate model. Such an attack would aim to preserve utility while removing the fingerprint used for verification. How resilient is the CREDIT verification scheme to this type of noise filtering attack?\n\nThe security of the verification process depends on the adversary's inability to estimate the verification threshold, $\\tau$, due to the unknown noise parameter ($\\sigma$), verification set ($\\mathcal{V}$), and error tolerances ($\\gamma_1, \\gamma_2$). However, a sophisticated adversary could approximate this threshold since $\\sigma$ must lie within a narrow utility-preserving range, and $\\gamma_1, \\gamma_2$ are typically small (e.g., < 0.01). Could the adversary not perform a grid search over these limited parameters? While the adversary does not know the exact verification set $\\mathcal{V}$, they could use their own hold-out set $\\mathcal{V'}$ from a similar distribution to estimate a threshold $\\tau'$. How sensitive is the mutual information calculation and resulting verification outcome to the exact composition of the verification set? Have the authors considered an adaptive attack where the adversary approximates the threshold and tunes their surrogate to bypass this estimated boundary?\n\nThe verification mechanism relies on the KSG estimator for mutual information. Non-parametric estimators like KSG degrade in accuracy and increase in computational cost as dimensionality grows. The experiments use moderate embedding sizes, but modern architectures, especially in NLP or large-scale vision, use embeddings with thousands of dimensions. How does the proposed defense, and specifically the KSG estimation, scale to high-dimensional settings? Have the authors evaluated the computational overhead and reliability of the mutual information estimate for much larger embeddings?"}, "questions": {"value": "- How resilient is the CREDIT verification scheme to this type of noise filtering attack?\n- Have the authors considered an adaptive attack where the adversary approximates the threshold and tunes their surrogate to bypass this estimated boundary?\n- Have the authors evaluated the computational overhead and reliability of the mutual information estimate for much larger embeddings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W3B6vm3ZaI", "forum": "5ZSfNAvyzB", "replyto": "5ZSfNAvyzB", "signatures": ["ICLR.cc/2026/Conference/Submission12160/Reviewer_JkN2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12160/Reviewer_JkN2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761078143886, "cdate": 1761078143886, "tmdate": 1762923115369, "mdate": 1762923115369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CREDIT, a certified defense framework against model extraction attacks (MEAs). It leverages mutual information (MI) to quantify similarity between neural network models and introduces a theoretically grounded threshold (CREDIT threshold) to verify ownership. The approach provides rigorous guarantees on false positive and false negative rates, and is empirically validated across image and graph modalities, consistently outperforming baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.\tThe paper provides a theoretical guarantee of the ownership verification, which is sound and novel.\n2.\tMutual information is introduced to define the similarity between models.\n3.\tThe proposed method achieves superior performance among baseline methods."}, "weaknesses": {"value": "1.\tThe threat model is not clearly defined. What information can the defender have access to? The proposed method assumes access to the embedding representations of the suspicious model to estimate mutual information. This implies a white-box or at least gray-box setting. It is unclear whether CREDIT can operate effectively in a pure black-box scenario (e.g., only labels/soft labels are available). \n2.\tDefinition 2 only shows how the functionality is replicated in a model. It does not directly answer the key question: “to what extent, does this similarity certify that h_sur has been extracted from the protected model?” This definition needs further clarification. \n3.\tThe proposed framework relies on mutual information-based similarity measurement. However, mutual information may not fully capture the neural network behaviors. It would be great if the paper could discuss the scope and limitations of mutual information as a similarity measure in the defense."}, "questions": {"value": "1.\tWhat level of access does the defender require to the suspicious model?\n2.\tHow does the CREDIT threshold quantitatively certify that a surrogate model has been extracted from the protected model, rather than merely being functionally similar?\n3.\tWhat are the limitations of using mutual information as a similarity metric between models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CHdgoFHceR", "forum": "5ZSfNAvyzB", "replyto": "5ZSfNAvyzB", "signatures": ["ICLR.cc/2026/Conference/Submission12160/Reviewer_mf7r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12160/Reviewer_mf7r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853839499, "cdate": 1761853839499, "tmdate": 1762923114677, "mdate": 1762923114677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a certified defense against MEAs, which formally formulates the problem of certified defense for MEAs and introduces the CREDIT method. The proposal aims to enable practical ownership verification while providing rigorous theoretical guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The first one that use the mutual information to quantify the dependency between two models. \n2. The paper evaluates its method across two different data modalities and deploys CREDIT on four distinct backbone models within each modality to implement the defense methods. \n3. A theoretical framework and sufficient experiments are proposed to support its conclusion."}, "weaknesses": {"value": "1. The authors employ the Knowledge Distillation method (Romero et al., 2014) as the primary attack strategy. While this is a foundational technique, it is now considered relatively outdated. It would strengthen the paper's contribution to evaluate CREDIT against more recent and advanced model extraction attacks. A critical question remains: if challenged by these newer methods, can CREDIT still achieve reliable ownership verification while maintaining its rigorous theoretical guarantees?\n2. In the model utility evaluation experiment, the authors did not provide detailed information on the number of suspicious models used when distinguishing between independent models and target models based on the same backbone models."}, "questions": {"value": "1.What is the connection between mutual information and KSG?\n2. How to calculate τ(σ, Q)?\n3.Why does the threshold within the range [0, β] effectively separate the surrogate model hsur from independently trained models hind?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7pSqLftEjX", "forum": "5ZSfNAvyzB", "replyto": "5ZSfNAvyzB", "signatures": ["ICLR.cc/2026/Conference/Submission12160/Reviewer_ZRkj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12160/Reviewer_ZRkj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916857575, "cdate": 1761916857575, "tmdate": 1762923114036, "mdate": 1762923114036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CREDIT (Certified Defense of Deep Neural Networks against Model Extraction Attacks), a certified defense framework designed to protect models from Model Extraction Attacks (MEAs) in the Machine Learning as a Service (MLaaS) setting. The work adds Gaussian noise to model embeddings, which enables the derivation of a mutual information (MI) upper bound between the protected model and any surrogate model. Based on this bound, the authors define a CREDIT threshold to verify model ownership with formal probabilistic guarantees. Finally, theoretical analysis provides provable bounds on Type I and Type II errors that decay exponentially with the verification set size."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1 Quantifying model similarity is crucial for model protection, and this work is built on solid, established theory by employing Mutual Information as an information-theoretic metric for model similarity and the Gaussian Mechanism from differential privacy for provable bounds. \n\n2 Unlike traditional approaches based on model watermarking or fingerprinting, this work has minimal impact on the model's performance. \n\n3 Furthermore, this work rigorously derives the theoretical upper bound of mutual information, which substantiates the reliability and credibility of the proposed work."}, "weaknesses": {"value": "First, in the MLaaS setting, services only expose the final outputs, and introducing noise perturbations into intermediate layer embeddings represents a non-standard and highly demanding assumption. \n\nSecond, the experimental validation is limited, lacking comparisons with other metrics which can quantify model similarity, and performance on large language models (LLMs). \n\nFinally, mutual information estimation is critical for this work’s effectiveness, but the KSG estimator is known to be unstable and computationally intensive in high-dimensional spaces."}, "questions": {"value": "1.Choice of σ：Does the σ need to be selected differently for different task models or tasks? If so, would this lead to a high search cost? When considering efficiency in experiments, is this time cost taken into account? \n\n2.How robust is CREDIT to query-averaging/denoising attacks? If an adversary issues m repeated queries per input and averages responses to reduce Gaussian noise variance, can they increase MI and evade detection? What defenses (rate-limit, per-user noise, query budget accounting) do you recommend?\n\n3.KSG estimator hyperparameter: How is the number of nearest neighbors set in the KSG estimator, and how significantly does this parameter affect mutual information computation? \n\n4.Lemma 5 upper bound: How is the upper bound in Lemma 5 derived?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HyHKZzTTCX", "forum": "5ZSfNAvyzB", "replyto": "5ZSfNAvyzB", "signatures": ["ICLR.cc/2026/Conference/Submission12160/Reviewer_d192"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12160/Reviewer_d192"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762305342510, "cdate": 1762305342510, "tmdate": 1762923113568, "mdate": 1762923113568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}