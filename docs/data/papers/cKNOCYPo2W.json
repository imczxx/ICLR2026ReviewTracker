{"id": "cKNOCYPo2W", "number": 25196, "cdate": 1758365190031, "mdate": 1759896730809, "content": {"title": "Conditioned Initialization for Attention", "abstract": "Transformers are a dominant architecture in modern machine learning, powering applications across vision, language, and beyond. At the core of their success lies the attention layer, where the query, key, and value matrices determine how token dependencies are captured. While considerable work has focused on scaling and optimizing Transformers, comparatively little attention has been paid to how the weights of the queries, keys and values are initialized. Common practice relies on random initialization or alternatives such as mimetic initialization, which imitates weight patterns from converged models, and weight selection, which transfers weights from a teacher model. In this paper, we argue that initialization can introduce an optimization bias that fundamentally shapes training dynamics. We propose **conditioned initialization**, a principled scheme that initializes attention weights to improve the spectral properties of the attention layer. Theoretically, we show that conditioned initialization can potentially reduce the condition number of the attention Jacobian, leading to more stable optimization. Empirically, it accelerates convergence and improves generalization across diverse applications, highlighting conditioning as a critical yet underexplored area for advancing Transformer performance. Importantly, conditioned initialization is simple to apply and integrates seamlessly into a wide range of Transformer architectures.", "tldr": "", "keywords": ["spectral conditioning transformers", "spectral properties of attention"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0cefa1eb32ef625b997cb5d4d3b2c49ccfbe99ba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the initialization of transformers, specifically the initialization of the attention mechanism. Specifically, arguing that for better optimization of the network, the initialization should bound the condition number of the Jacobian matrix of the attention computation.\n\nThe paper suggests a practical implementation for this by initializing the $\\mathbf{W}_q$ and $\\mathbf{W}_k$ matrices as orthogonal matrices, and $\\mathbf{W}_v$ matrix as an identity one.\n\nThrough a series of experiments across language and vision tasks, the initialization is shown to achieve better performance, while achieving the same accuracy as baselines with significantly less compute, highlighting its efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper is clear and can be followed easily.\n\nS2: The practical implementation of the initialization is easy to use, making it conducive to wide adoption.\n\nS3: The method shows consistent performance gains compared to the baselines across different tasks and modalities (language and vision).\n\nS4: The suggested initialization method is highly efficient, achieving the same performance as baselines with much less compute. This is an advantage that could be highlighted further by the authors, perhaps through showing detailed training loss curves of the different models and adding more figures like Figure 2 for all tasks."}, "weaknesses": {"value": "W1: The main weakness is that while the paper demonstrates the proposed initialization bounds the condition number of the Jacobian matrix of the attention computation, it does not sufficiently explain the theoretical basis for why this is a desired property for better optimization. Although better performance is shown in practice, a proof or a more robust theoretical explanation for this optimization benefit is expected, especially since the paper outlines how to achieve the bound.\n\nW2: This is a minor point, but using the notation $\\mathbf{A}(\\mathbf{X})$ for the output of the attention computation is potentially confusing, as this symbol is often reserved for the attention matrix itself. A symbol change is suggested for clarity."}, "questions": {"value": "See W1"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wy698R8rgX", "forum": "cKNOCYPo2W", "replyto": "cKNOCYPo2W", "signatures": ["ICLR.cc/2026/Conference/Submission25196/Reviewer_jfhc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25196/Reviewer_jfhc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761140118045, "cdate": 1761140118045, "tmdate": 1762943358528, "mdate": 1762943358528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to initialize the query and key weights of self-attention layers as semi-orthogonal matrices, and the value weights as rectangular identity matrices. This scheme is based on an analysis of the condition number of the Jacobian of self-attention layers, supported by previous research suggesting that bounding this condition number leads to smoother/faster convergence."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Comprehensive empirical validation on a variety of vision tasks, showing clear positive benefit for large and small scale image classification, detection and segmentation\n- (Somewhat less) comprehensive empirical validation on language tasks, showing improved performance on LRA and on 100m-param-scale language modeling\n- Demonstrated benefit over mimetic initialization\n- Addresses the ViT small-scale-data issue as well or better than mimetic initialization while being similarly simple\n- Theoretically grounded\n- Works for a variety of attention mechanisms"}, "weaknesses": {"value": "- Relatively small-scale language model evaluation (e.g., experiments requiring one GPU while training ViT-B presumably took multiple GPUs). But I still find the results convincing and promising. I know it's hard to do this on an academic budget. And the vision results alone justify my score."}, "questions": {"value": "Did you try anything at the intersection of conditioned and mimetic initializations? For example, you could maybe pick a close orthogonal pair of matrices. Or you could use conditioned init for W_Q, W_K and mimetic init for W_V, W_O. \n\nDid you try any ablations? (Like only initializing W_Q, W_K or only initializing W_V?)\n\nDid you consider the output projection at all? It seems like it would be closely tied to W_V."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sASb52klwU", "forum": "cKNOCYPo2W", "replyto": "cKNOCYPo2W", "signatures": ["ICLR.cc/2026/Conference/Submission25196/Reviewer_44Pd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25196/Reviewer_44Pd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768958360, "cdate": 1761768958360, "tmdate": 1762943358280, "mdate": 1762943358280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes conditioned initialization, an initialization scheme for attention weights that explicitly targets the conditioning of the attention Jacobian. The authors present a theoretical analysis showing that the condition number of Jacobian can be upper-bounded in terms of the condition numbers of the query, key, and value matrices. Making these matrices well-conditioned at initialization is expected to stabilize optimization. Extensive experiments on various downstream tasks demonstrate that this initialization improves final accuracy and accelerates convergence compared to baseline methods on small models. The approach is simple to apply and does not require changes to existing training pipelines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a clear theoretical motivation and analysis, deriving an explicit upper bound related to attention optimization stability. This is both novel and well justified.\n2. The paper is well written and easy to follow. The proposed method is straightforward to implement and architecture-agnostic.\n3. Experimental results across various downstream tasks are promising. The method consistently improves performance and accelerates convergence."}, "weaknesses": {"value": "1. The theoretical analysis optimizes an upper bound on the condition number of the Jacobian rather than the condition number itself. Although empirical results support the approach, the gap between the bound and the actual condition number is not fully characterized. It remains unclear whether a tighter bound would yield further improvements.\n2. All experiments are conducted on relatively small models. It is unclear whether the benefits of conditioned initialization extend to large-scale models.\n3. There is a typo on line 869: “The implementation of the ViTs were” should be “The implementation of the ViTs was.”"}, "questions": {"value": "1. Does conditioned initialization continue to provide performance gains and faster convergence when hyperparameters such as learning rate, warmup steps, and weight decay are re-tuned for each baseline? Could baseline methods catch up with light hyperparameter tuning?\n2. Is the proposed method still effective for large-scale models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3BaP2APKmQ", "forum": "cKNOCYPo2W", "replyto": "cKNOCYPo2W", "signatures": ["ICLR.cc/2026/Conference/Submission25196/Reviewer_sReQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25196/Reviewer_sReQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828076860, "cdate": 1761828076860, "tmdate": 1762943358110, "mdate": 1762943358110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a spectrum inspired approach to initialize the attention matrices to reduce the conditioning number of the attention Jacobian matrices with partial theoretical analysis. Extensive experiments demonstrate that using the presented initialization approach leads to consistently superior performance over the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper proposes a simple but effective intialization approach, which might have a better condition number of the Jacobian of the attention matrix with respect to the parameter matrices $W_Q$, $W_K$ and $Q_V$.   \n+ Extensive experiments demonstrate consistent improvements over the two baseline methods."}, "weaknesses": {"value": "- The results in Lemma 3.1 and Theorem 3.1 cannot be listed as the (theoretical) contributions of the paper. These results are merely the formal expression of the gradients of a matrix function with respect to the parameter matrices via the Kronecker product. Similar results can be found in prior work, e.g., the appendix in [a]. \n\n- The results for Jacobian matrices developed in the paper might not complete due to ignorance of the stablization structure, e.g., Layer Norm (LN), RMSNorm, QKNorm. Thus, it is also questionable whether the upper bound of the condition number makes any sense in practical. If either LN, or RMSNorm, or QKNorm is introduced, can the proposed approach still yield improved performance comparing to the counterpart baseline methods? \n\n- While the condition number of the proposed initialization strategy is reduced, it is merely a heuristic way to form the initialization for the attention matrices. Does it enable the training process stable? How about the effects of using the conditioned initialization on the learning curves?  In practice, when one of LN, or RMSNorm, or QKNorm or some combination of them is introduced, what aobut the learning curves? \n\n- In previous work, there are many attempts to design stablized optimization algorithm for training Transformer. It would be more interesting if some evaluations to connect the proposed initialization strategy with the stablized algorithms. The reviewer is curious about that whether or not the proposed initialization still works when stablized optimization algorithms (or stablized structure, e.g., LN, RMSN, QKNorm, etc.) used.\n\n\n[a] Taming Transformer Without Using Learning Rate Warmup, ICLR'25. \n\n[b] Learning deep transformer models for machine translation. arXiv preprint arXiv:1906.01787, 2019.\n\n[c] Query-key normalization for transformers. EMNLP 2020\n\n[d] Scaling vision transformers to 22 billion parameters. ICML 2023.\n\n[e] Root mean square layer normalization. NeurIPS 2019."}, "questions": {"value": "- Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VkKlNBW6nC", "forum": "cKNOCYPo2W", "replyto": "cKNOCYPo2W", "signatures": ["ICLR.cc/2026/Conference/Submission25196/Reviewer_ip1U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25196/Reviewer_ip1U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998439768, "cdate": 1761998439768, "tmdate": 1762943357933, "mdate": 1762943357933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}