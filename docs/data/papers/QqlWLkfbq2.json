{"id": "QqlWLkfbq2", "number": 5599, "cdate": 1757921968307, "mdate": 1759897965645, "content": {"title": "EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation", "abstract": "Mixed-Integer Linear Programming (MILP) is fundamental to solving complex decision-making problems. The proliferation of MILP instance generation methods, driven by machine learning's demand for diverse optimization datasets and the limitations of static benchmarks, has significantly outpaced standardized evaluation techniques. Consequently, assessing the fidelity and utility of synthetic MILP instances remains a critical, multifaceted challenge. This paper introduces a comprehensive benchmark framework designed for the systematic and objective evaluation of MILP instance generation methods. Our framework provides a unified and extensible methodology, assessing instance quality across crucial dimensions: mathematical validity, structural similarity, computational hardness, and utility in downstream machine learning tasks. A key innovation is its in-depth analysis of solver-internal features --particularly by comparing distributions of key solver outputs including root node gap, heuristic success rates, and cut plane usage -- leveraging the solver's dynamic solution behavior as an `expert assessment' to reveal nuanced computational resemblances. By offering a structured approach with clearly defined solver-independent and solver-dependent metrics, our benchmark aims to facilitate robust comparisons among diverse generation techniques, spur the development of higher-quality instance generators, and ultimately enhance the reliability of research reliant on synthetic MILP data. The framework's effectiveness in systematically comparing the fidelity of instance sets is demonstrated using contemporary generative models. The code is available in \\url{https://github.com/iclr2026evamilp/EVA-MILP}.", "tldr": "", "keywords": ["Mixed-Integer Linear Programming", "Instance Generation", "Graph Neural Networks"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abb5ba25f3ea1f369a12971767ed55fbbedbd5cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents EVA-MILP, a comprehensive evaluation framework for MILP instance generation. The framework establishes two complementary perspectives for assessment: solver-dependent and solver-independent features. Furthermore, the authors conduct extensive experiments to demonstrate the effectiveness and robustness of EVA-MILP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well written and easy to understand.  \n\n2. I find it interesting that the authors formalize the evaluation from both solver-independent and solver-dependent perspectives. The hardness of a MILP instance is influenced not only by the intrinsic characteristics of the instance itself but also by the behavioral features of the solver. So, establishing a two-dimensional evaluation mechanism is both reasonable and necessary.\n\n3. The authors further strengthen their contribution by conducting extensive experiments on various MILP instance generation methods, which effectively demonstrate the comprehensiveness and methodological soundness of EVA-MILP."}, "weaknesses": {"value": "1. EVA-MILP primarily relies on synthetic datasets, lacking real-world MILP instances such as those from MIPLIB. MILP instances often exhibit locally optimal behavior, where various methods quickly converge to local optima and then stagnate, making it unclear how such cases should be evaluated.\n\n2. I find that the experiments in this paper make extensive use of high-thread Gurobi configurations, which may reduce the distinguishability of results. The parallel heuristics and scheduling mechanisms in such settings could diminish performance differences across instances, thereby lowering the evaluation sensitivity of the framework."}, "questions": {"value": "1. Could you please include an analysis of the evaluation on MILP instances from MIPLIB?\n\n2. Could you set the Gurobi thread number to 1 for the analysis?\n\n3. Regarding the GNN downstream tasks, there are several other mainstream frameworks, such as HEM and Predict-and-Search. Could you also analyze the evaluation of EVA-MILP within these frameworks?\n\n4. I am a bit confused about the solver-dependent features. Although EVA-MILP successfully establishes stable statistical characteristics of solver-internal behaviors, its analysis remains limited to a macroscopic distribution level. In real-world or high-dimensional synthetic MILP scenarios, distributional similarity alone may not reveal the dynamic differences in solver behavior during critical phases. For example, the gap improvement ratio after each cut addition."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VuIMOzMoLI", "forum": "QqlWLkfbq2", "replyto": "QqlWLkfbq2", "signatures": ["ICLR.cc/2026/Conference/Submission5599/Reviewer_P4EP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5599/Reviewer_P4EP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761564325745, "cdate": 1761564325745, "tmdate": 1762918153383, "mdate": 1762918153383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EVA-MILP, a unified framework for evaluating MILP instance generation methods. It defines both solver-independent and solver-dependent metrics, incorporating internal solver signals (root node gap, heuristic success count, cut plane usage) to characterize computational behavior, and downstream tasks such as hyperparameter tuning and initial-basis prediction to measure practical utility. The authors benchmark several existing generators and discuss interesting findings such as the mismatch between structural and behavioral similarity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This work identifies an important problem: the lack of a standardized, reproducible evaluation protocol for MILP instance generation. It is beneficial to the community.\n\n1. In introduces solver-internal statistics to measure behavioral similarity, which is novel and clearly useful beyond coarse metrics like runtime. It also Includes downstream tasks to measure the real-world utility rather than synthetic difficulty alone.\n\n1. Experiments cover several generators with detailed analyses, and the discussion section provides genuinely interesting insights, especially the finding 3."}, "weaknesses": {"value": "1. Each generator is only tested on selected datasets with rare explanations. This limits the comparability across different methods. The authors may want to test these methods on all datasets, or detail the reasons why some methods cannot run on the specific datasets.\n\n1. The chosen datasets are mostly simple synthetic ones. Harder or real-world MILPs would make the conclusions stronger. Moreover, I recommand the authors to include some real challenging datasets with only a few instances to demonstrate the effectiveness of generation techniques under data sparsity.\n\n1. Some conclusions (e.g., simple outcome metrics not reflecting true difficulty or structures) are supported only qualitatively. It would be better to provide some correlation analysis or case studies.\n\n1. The choice of metrics is intuitive but not fully justified, and the authors may want to provide further explanations. See questions."}, "questions": {"value": "1. Why is initial basis prediction chosen as the downstream task? Would other tasks like initial solution prediction or learning2branch also be appropriate?\n\n1. For the internal metrics, why focus on these three ones (root gap, heuristic count, cut usage)? How comprehensive are they in representing solver dynamics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mp87zGjQTR", "forum": "QqlWLkfbq2", "replyto": "QqlWLkfbq2", "signatures": ["ICLR.cc/2026/Conference/Submission5599/Reviewer_YQ2i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5599/Reviewer_YQ2i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644695060, "cdate": 1761644695060, "tmdate": 1762918153147, "mdate": 1762918153147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EVA-MILP, a benchmark framework to evaluate MILP instance generation methods. It combines solver-independent metrics and solver-dependent metrics. Experiments cover Ecole-style datasets (SC/CA/CFL/IS) and ML4CO, and compare several generators (G2MILP, ACM-MILP, DIG-MILP)."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Important problem: the community needs more principled evaluation for synthetic MILP instances."}, "weaknesses": {"value": "- Trivial “copying” generators can score highly by reproducing the original data distribution. The framework lacks novelty/diversity/anti-duplication checks, so high scores may not reflect actual usefulness.\n\n- Heavy solver dependence. Are Gurobi-based evaluations necessarily correct or representative? Branching-node counts and “internal features” can differ across solvers (e.g., Gurobi vs. SCIP). If they are not consistent across solvers, why should “similar branching nodes under Gurobi” be taken as evidence of instance similarity?"}, "questions": {"value": "Same as weaknesses: overall, this feels like a one-sided evaluation framework with limited practical value—rewarding copying without novelty/diversity checks, and relying on Gurobi-specific behavior without cross-solver justification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uXKS34Yvyu", "forum": "QqlWLkfbq2", "replyto": "QqlWLkfbq2", "signatures": ["ICLR.cc/2026/Conference/Submission5599/Reviewer_14M5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5599/Reviewer_14M5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963878191, "cdate": 1761963878191, "tmdate": 1762918152856, "mdate": 1762918152856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EVA-MILP, a benchmarking framework designed to evaluate existing MILP instance generation methods. EVA-MILP assesses generated MILP instances from two perspectives—Fidelity and Utility—and categorizes evaluation metrics into solver-internal and solver-external features. Furthermore, the paper proposes using solver-internal features as a novel approach to evaluate the quality of generated MILP instances."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Establishing a systematic and standardized evaluation framework for MILP instance generation is crucial for advancing research in combinatorial optimization data synthesis.\n2. EVA-MILP provides a comprehensive organization of existing evaluation metrics and proposes a clear and structured categorization.\n3. The paper offers valuable insights into the design of evaluation metrics, which may inform future work in MILP instance generation and benchmarking."}, "weaknesses": {"value": "1. While the paper reviews and classifies many evaluation metrics, most of them are derived from previous studies. The only original contribution appears to be the introduction of solver-internal features.\n2. The experiments are conducted solely on the CA, IS, and SC datasets from ACM-MILP and DIG-MILP. The framework’s performance on more challenging MILP problems—such as TSP, Graph Coloring, VRP—or on real-world benchmarks like MIPLIB remains unexplored, which is crucial for assessing generalizability.\n3. The authors argue that generative models are inherently specialized for specific problem types, and therefore only evaluate each model on a limited set of problems (as shown in Tables 1 and 2). This experimental design prevents an objective comparison among different generation methods.\n4. The definition of the challenges in MILP instance generation (lines 088–097) is incomplete. For instance, if a generator simply reproduces training data, it provides no real utility—undermining all similarity-based evaluation metrics between generated and reference instances.\n5. I agree with the idea of using downstream tasks to evaluate generated instances, but such tasks must have real practical value (e.g., accelerating ML-based solvers). The current choice of hyper-parameter tuning lacks meaningful application: if one intends to tune solver parameters for a problem set, it is more direct to tune them on the original data rather than on newly generated ones.\n6. The authors should consider incorporating a wider variety of practically relevant downstream tasks to more comprehensively demonstrate the framework’s effectiveness."}, "questions": {"value": "1. Building upon W2, can the authors extend the benchmark to include more diverse problem types and provide results showing which generation methods are suitable for each?\n2. The current experiments involve only small-scale instances solvable within one second. Can the authors evaluate how these methods perform on larger MILP instances?\n3. In Section 4, the paper evaluates instance similarity by solving for 120 seconds and comparing solver-internal features. How sensitive is the stability of these metrics (measured by W-1 distance) to the choice of time limit (e.g., increasing or decreasing 120s)?\n4. Were the experiments in Section 4 repeated to rule out randomness introduced by data partitioning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3fWW5sasTP", "forum": "QqlWLkfbq2", "replyto": "QqlWLkfbq2", "signatures": ["ICLR.cc/2026/Conference/Submission5599/Reviewer_mu6D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5599/Reviewer_mu6D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989169502, "cdate": 1761989169502, "tmdate": 1762918152622, "mdate": 1762918152622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}