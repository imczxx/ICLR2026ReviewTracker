{"id": "6iZO20TrjG", "number": 6974, "cdate": 1758003944547, "mdate": 1759897880624, "content": {"title": "FaithThinker: Dialectical Reasoning for Noise-Robust LLMs", "abstract": "Large Language Models (LLMs) have shown strong capabilities across a wide range of tasks. However, they remain vulnerable to noisy or adversarial contexts, often producing unfaithful or hallucinatory outputs. To address these weaknesses, recent work has integrated LLMs with Retrieval-Augmented Generation (RAG) and external tools. While effective, these approaches still suffer from error propagation, as existing structured reasoning methods cannot reliably detect and correct mistakes during intermediate steps.\nWe propose FaithThinker, a reasoning framework designed to improve contextual faithfulness. At its core is Self-Questioning and Verification (SQV), a reasoning paradigm inspired by dialectical thinking. SQV allows models to question, verify, and revise intermediate reasoning steps in a single pass. To extend this capability, we introduce SQV-Alignment, an adversarial context‚Äìaugmented fine-tuning method that efficiently transfers SQV from large to smaller models.\nExperiments demonstrate that FaithThinker achieves state-of-the-art robustness under both clean and noisy conditions. SQV reduces hallucinations by up to 30.6\\% compared with Chain-of-Thought, and generates reasoning paths 4√ó shorter than iterative methods such as Self-Refine. These results highlight FaithThinker‚Äôs ability to enhance contextual faithfulness, mitigate hallucinations, and improve efficiency in challenging environments.", "tldr": "", "keywords": ["Large Language Models", "Contextual Faithfulness", "Hallucination Mitigation", "Structured Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da730e7bd967b9d9f48e0be9d18adfd16a81eb74.pdf", "supplementary_material": "/attachment/8de821a64244127d36f6a449b0ba5d9e5ff96de3.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents FaithThinker, a new reasoning framework for large language models (LLMs) designed to improve contextual faithfulness‚Äîthe model‚Äôs ability to remain consistent with retrieved evidence and avoid hallucinations, even in noisy or adversarial contexts. The framework is built on the Self-Questioning and Verification (SQV) paradigm, which encourages models to question, verify, and refine their reasoning steps to enhance accuracy and reliability. In addition, the study introduces a fine-tuning approach that integrates this reasoning ability directly into the model‚Äôs inherent capabilities, ensuring more faithful and robust reasoning performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a highly original and well-motivated framework, FaithThinker, that addresses a critical limitation in current reasoning systems‚Äîmaintaining contextual faithfulness in the presence of noisy or adversarial contexts. Its central innovation, the Self-Questioning and Verification (SQV) paradigm, represents a creative and conceptually elegant integration of dialectical reasoning into LLM inference. Unlike existing structured reasoning frameworks such as Chain-of-Thought, Self-Refine, or Tree-of-Thought, SQV achieves verification within a single forward pass, effectively balancing reasoning depth with computational efficiency.\n\nIn terms of originality, the work stands out by redefining how reasoning reliability can be formalized and improved. The formulation of Input-to-Trajectory and Intra-Trajectory Hallucinations provides a new lens for understanding reasoning failures. The proposed SQV-Alignment method further extends this contribution by offering a practical and scalable approach to transferring reasoning capabilities from large to smaller models through adversarial context fine-tuning.\n\nThe quality of the work is strong, supported by comprehensive experiments across multiple model scales and benchmarks. Results consistently demonstrate substantial reductions in hallucination rates and improvements in efficiency, indicating both technical soundness and empirical robustness.\n\nThe clarity of the paper is commendable. The structure is logical, with motivating examples, clear mathematical formalization, and detailed illustrations (e.g., Figure 2) that make the methodology easy to follow.\n\nRegarding significance, the work is highly relevant to both academic and applied research in reasoning, retrieval-augmented generation, and trustworthiness of LLMs. The proposed dialectical reasoning paradigm has potential to influence future directions in robust AI reasoning design and knowledge alignment, making the paper an important step toward self-corrective LLMs."}, "weaknesses": {"value": "While the conceptual contribution is novel, there are a few areas that could be strengthened. First, the paper would benefit from more detailed ablation studies on the role of each SQV component (thesis, antithesis, verification, synthesis) across different model sizes to better isolate which stages contribute most to performance gains.\n\nSecond, the evaluation scope, though broad, focuses primarily on QA-style reasoning benchmarks. The paper could explore more complex multi-step reasoning domains (e.g., mathematical reasoning, scientific inference, or code understanding) to demonstrate broader generalization of SQV.\n\nThird, while SQV-Alignment is introduced as a scalable fine-tuning approach, the paper does not provide computational cost comparisons or training efficiency metrics relative to standard fine-tuning methods. A clearer analysis of scalability and training stability would strengthen the claim of efficiency.\n\nIn addition, more models should be tested and validated to better demonstrate the generalizability of the proposed method and to provide broader and more comprehensive benchmark results for the study.\n\nFinally, some theoretical parts (e.g., Equations 2‚Äì4) could benefit from tighter connections to empirical findings‚Äîfor instance, showing how the probabilistic formulation directly informs prompt design or evaluation metrics."}, "questions": {"value": "1. How does the SQV framework interact with retrieval quality? Specifically, can SQV detect when the retrieved evidence itself is misleading, and to what extent does it mitigate such failures without additional retrieval filtering?\n\n2. Can the authors provide a more detailed breakdown of computational efficiency‚Äîboth in terms of reasoning path length and wall-clock inference time‚Äîcompared to iterative methods like Self-Refine?\n\n3. How sensitive is SQV-Alignment to the choice of teacher model and noise type during adversarial fine-tuning? Would the framework generalize if the teacher model were significantly smaller or trained on a different domain?\n\n4. Have the authors considered combining SQV with reinforcement learning or process supervision methods to further strengthen long-horizon reasoning? If so, what were the observed trade-offs?\n\n5. For interpretability and reproducibility, could the authors share more qualitative examples that illustrate how SQV reasoning evolves differently from standard Chain-of-Thought under adversarial input conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J10m7fSINo", "forum": "6iZO20TrjG", "replyto": "6iZO20TrjG", "signatures": ["ICLR.cc/2026/Conference/Submission6974/Reviewer_JMUb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6974/Reviewer_JMUb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761283921889, "cdate": 1761283921889, "tmdate": 1762919193907, "mdate": 1762919193907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FaithThinker which is a reasoning framework aiming at improving the contextual faithfulness of the LLMs under noisy contexts. The core pattern which is Self questioning and verification is broadly a loop of thesis, antithesis, verification and finally sinthesis in a single forward pass to detect reasoning errors. \nThe authors perform a good study by defining two intraprocess hallucination types, input to trajectory and intra trajectory, with binary predicates over reasoning traces. They also propose SQV-alignment, a lora based sft process where training data is generated by strong teacher model to transfer the SQV pattern to smaller students. They experiment and show significant gains on multiple datasets and also show lower hallucination rates compared to CoT, SC, Self-refine methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) Most important strength of the paper is the clear problem formulation of intra-process halluincations, where they explicitly distinguish hallucinations relative to context vs within trajectory and formalize both with simple indicators Hcr, and Hrr. This is a useful conceptual lens beyond final-answer hallucinations\n2) The single pass dialectical control which is via the 4 stage SQV embeds verficiation into each micro steps, promising better error containment without the cost of iterative loop, the formulation and figure 2 are clear. \n3) Data efficient alignment process, that is well motivated by using a teacher to synthesize SQV and adverserial contexts, Lora also keeps the compute low, making this widely useable. \n\nThe paper also shows minor but important contributions like token length differences and shows large savings versus multi sample methods."}, "weaknesses": {"value": "1) Current statistical rigor is insufficient, inference is run twice and averaged, no confidence intervals. A statement of statistical significance appears only for the entropy analysis without test specifications or p values. \n2) Ablations compare ‚Äúw/o SQV format alignment‚Äù and ‚Äúw/o dialectical components,‚Äù but they still rely on format-specific supervision. There is no variant that keeps identical prompts/format while disabling only the verification decision.\n3) The set includes CoT, Self-Consistency, ToT, and Self-Refine, but no retrieval centric methods like chain of verification, self-rag with critique. this makes it hard to place SQV againsts state of the art approaches."}, "questions": {"value": "1) In appendix E hallucination rate is defined as Nn ‚Äã /N, while accuracy is ùëÅùëù / ùëÅ  and  since ùëÅ ùëù + ùëÅ ùëõ = ùëÅ doesnt the hallucination metric collapse into error rate, these two are perfectly anti-correlated (Hal = 1 - Acc), so this doesnt seem to measure faitfulness independent of correctness.\n2) The much re-iterated claim of 4x shorter than iterative methods seems an overstretch, looking at table 5, self-refine is 1957 vs SQV is 596, this makes it more of like 3x (it holds for self consistency - but the abstract mentions self-refine which is untrue) for reference snippet from the abstract \"4√ó shorter than iterative methods such as Self-Refine\" this is false. \n3) Typo: Line 338 says authors used Qwen2.5-3B and then \"for brevity\" they say on line 340 Qwen2.5-7B, minor point for correction.\n4) Can you also report wall-clock and GPU hours for inference to quantify single pass benefits apart from token length"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jZ8Xr6Bpw1", "forum": "6iZO20TrjG", "replyto": "6iZO20TrjG", "signatures": ["ICLR.cc/2026/Conference/Submission6974/Reviewer_7zNZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6974/Reviewer_7zNZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800911889, "cdate": 1761800911889, "tmdate": 1762919193526, "mdate": 1762919193526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FaithThinker introduces Self-Questioning and Verification (SQV), a reasoning paradigm inspired by dialectical thinking, to enhance contextual faithfulness and robustness in large language models (LLMs). Traditional reasoning methods (e.g., Chain-of-Thought or Self-Refine) suffer from error cascades when input contexts are noisy or adversarial. SQV mitigates this by embedding questioning, verification, and refinement at each reasoning step, enabling single-pass correction without iterative loops."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work embeds the dialectical self-critique within a single reasoning pass.\n2. The work shows consistent improvements across multiple models and datasets.\n3. This work enables small models to gain reasoning robustness from large teacher models without high RL cost."}, "weaknesses": {"value": "1. The authors claim \"structured reasoning under noisy context,\" but do not test on the math reasoning task.\n2. The reliance on a strong SQV teacher model seems like a type of knowledge distillation.\n3. Since LLMs process all steps, there is no analysis on whether each step can introduce new hallucinations, such as SQV turns correct steps to incorrect steps."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CKBlmHB0f7", "forum": "6iZO20TrjG", "replyto": "6iZO20TrjG", "signatures": ["ICLR.cc/2026/Conference/Submission6974/Reviewer_u4ka"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6974/Reviewer_u4ka"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062271489, "cdate": 1762062271489, "tmdate": 1762919193142, "mdate": 1762919193142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a prompting procedure based on self-questioning for enhancing the faithfulness of reasoning models to the provided context. This leads to notable improvements on QA benchmarks with noisy contexts (FaithEval). The paper also shows how reasoning traces from stronger models can be distilled into smaller ones."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method leads to notable improvements over CoT and other baselines, across a range of models\n- The ablation study suggests that the dialectical approach of self-questioning and verification does in fact lead to strong improvements (at least on the tasks considered)."}, "weaknesses": {"value": "- The presentation of the paper could be improved. It's not clear if the unnecessary math introduced for the technique adds any insight (Eqs 1-4). The key idea is an improved prompting technique, but its presented as something more complex.\n- The concepts of input-to-trajectory hallucination and intra-trajectory hallucination are discussed at length, but there is no empirical evidence about how prevalent these are, and whether the SQV technique actually reduces it. There is pretty much no analysis of what the SQV prompt does in practice for the problems studied here.\n- There are terms used throughout like top-10% group entropy, token efficiency without any explanation in the main paper. Looking at the appendix these are actually simple ideas (e.g., inverse of the response length) -- so why not directly use simple terminology?\n- I find it very strange that the main table presents two metrics hallucination and accuracy, which always sum up to 1. There is no need to present two metrics -- this is like reporting both accuracy and error rates together.\n- At a high-level, the paper fails to make it clear what should be the overall objective in cases where the RAG context is noisy. Should the model rely on its internal knowledge in this case, or should it identify the problems in the context and report it? What is being measured by the benchmarks used here?\n- Some important citations are missing, e.g., an ICLR 2025 paper [1].\n\n[1] Huang, Yukun, et al. \"To Trust or Not to Trust? Enhancing Large Language Models' Situated Faithfulness to External Contexts.\" arXiv preprint arXiv:2410.14675 (2024)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "S0ewrU1N2e", "forum": "6iZO20TrjG", "replyto": "6iZO20TrjG", "signatures": ["ICLR.cc/2026/Conference/Submission6974/Reviewer_ckNu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6974/Reviewer_ckNu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098989289, "cdate": 1762098989289, "tmdate": 1762919192656, "mdate": 1762919192656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}