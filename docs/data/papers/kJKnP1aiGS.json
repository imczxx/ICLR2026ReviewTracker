{"id": "kJKnP1aiGS", "number": 17270, "cdate": 1758274074173, "mdate": 1759897186142, "content": {"title": "Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting", "abstract": "Adapting language models (LMs) to new tasks via post-training carries the risk of degrading existing capabilities---a phenomenon classically known as *catastrophic forgetting*. In this paper, we set out to identify specific guidelines to mitigate this phenomenon, by systematically comparing the forgetting patterns of supervised fine-tuning (SFT) and reinforcement learning (RL), two widely adopted post-training methods. Our experiments reveal a consistent trend across LM families (Llama, Qwen) and tasks (instruction following, general knowledge, and arithmetic reasoning): RL leads to less forgetting than SFT while achieving comparable or higher target task performance.\nTo investigate the cause for this difference, we consider a simplified setting in which the LM is modeled as a mixture of two distributions, one corresponding to prior knowledge and the other to the target task. We identify that the *mode-seeking* nature of RL, which stems from its use of *on-policy* data, enables keeping prior knowledge intact when learning the target task. We then verify this insight by demonstrating that the use on-policy data underlies the robustness of RL to forgetting in practical settings, as opposed to other algorithmic choices such as the KL regularization or advantage estimation. Lastly, as a practical implication, our results highlight the potential of mitigating forgetting using *approximately* on-policy data, which can be substantially more efficient to obtain than fully on-policy data.", "tldr": "RL forgets less than SFT due to its mode-seeking, on-policy nature, motivating the use of approximately on-policy data for SFT to reduce forgetting", "keywords": ["post-training", "catastrophic forgetting", "supervised finetuning", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/42f2caae389196592a1e8617643cf03313dff3b7.pdf", "supplementary_material": "/attachment/2c7aac31fbd5f14500d944871c024b213e78a367.zip"}, "replies": [{"content": {"summary": {"value": "This paper systematically compares catastrophic forgetting in language models during post-training via supervised fine-tuning (SFT) and reinforcement learning (RL). Through experiments across multiple model families and tasks, the authors demonstrate that RL exhibits significantly less forgetting than SFT while achieving comparable or better target task performance. They attribute this robustness to RL's mode-seeking behavior, which stems from its use of on-policy data, and further show that even approximately on-policy data can mitigate forgetting in SFT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a comprehensive and rigorous empirical evaluation across diverse tasks, making the findings highly robust and generalizable.\n2. The authors offer an intuitive yet formal explanation for the observed phenomenon by modeling the policy as a mixture of distributions and linking forgetting behavior to the mode-seeking nature of reverse KL minimization.\n3. The practical implication that approximately on-policy data can significantly reduce forgetting is a valuable and efficient alternative to full on-policy RL, offering a useful guideline for real-world model adaptation."}, "weaknesses": {"value": "1. Forgetting is measured via average accuracy drops; other forms of degradation (semantic drift, safety loss, calibration changes) are not quantitatively explored.\n2. The experiments are limited to models of up to 8B parameters, and it is unclear whether the same trends hold for significantly larger or smaller models, limiting the scalability claims.\n3. While the Gaussian mixture analogy provides valuable intuition, it may oversimplify the complex, high-dimensional, and often non-Gaussian nature of real-world language model distributions."}, "questions": {"value": "1. How closely must data match the current policy to be considered \"approximately on-policy,\" and what are the precise thresholds for its effectiveness in mitigating forgetting?\n2. Does the observed robustness of RL hold for capabilities beyond knowledge and reasoning, such as in multimodal or conversational tasks, where forgetting patterns might differ?\n3.  Is the reduced forgetting in RL achieved at the cost of slower convergence or higher sample complexity compared to SFT, and what are the implied trade-offs for practical deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TdSlmzJQ1B", "forum": "kJKnP1aiGS", "replyto": "kJKnP1aiGS", "signatures": ["ICLR.cc/2026/Conference/Submission17270/Reviewer_uGfs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17270/Reviewer_uGfs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17270/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761225603677, "cdate": 1761225603677, "tmdate": 1762927219172, "mdate": 1762927219172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates catastrophic forgetting of tow prevalent LLM post-training paradigms: supervised fine-tuning and reinforcement learning. Empirical results across several model families and tasks show that RL (particularly on-policy algorithms like GRPO) achieves comparable or better target-task performance while exhibiting less forgetting on non-target tasks. To explain this, the authors propose that RL’s mode-seeking behavior (reverse KL), rooted in its use of on-policy data, helps preserve prior knowledge. A simplified mixture-of-Gaussians analysis further illustrates how SFT and reverse RL behave differently under uni-modal vs multi-modal assumptions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper articulates a concrete and important question, i.e., why RL fine-tuning forgets less than SFT, and provides extensive experimental results supporting the finding across architectures and datasets.\n\n- The writing and figures are well-organized. In particular, the “gain–drop” metric and visualization (Figure 2) make results intuitive, and the toy Gaussian analysis offers a didactic explanation.\n\n- The study disentangles potential confounders (KL regularization, advantage estimation) and clearly isolates the effect of data policy."}, "weaknesses": {"value": "- The finding that on-policy learning mitigates forgetting better than off-policy learning has already been explored in both RL and alignment literature. Notably, “Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data” (Tajwar et al., 2024) also frames on-policy vs off-policy updates as mode-seeking vs mode-covering, drawing the same connection between reverse KL and improved retention. Thus, while this paper extends that reasoning to an explicit forgetting study, its conceptual contribution over the established framework appears incremental.\n\n- Given that on-policy methods (iterative SFT, DPO, rejection-sampling, GRPO, PPO) are already standard practice, the practical insight, i.e., “on-policy mitigates forgetting”, may have limited influence on future method design unless coupled with deeper theoretical grounding or new algorithmic proposals.\n\n- The mixture-of-Gaussians setting helps intuition but does not rigorously establish the link between KL directionality, multimodality, and empirical forgetting in high-dimensional LLMs."}, "questions": {"value": "I noticed that the statement “Conventional wisdom presumes that the mode-seeking nature of reverse KL enables faster learning … while the mode-covering forward KL should maintain probability mass across modes.” cites (Chan et al., 2022; Tajwar et al., 2024b). However, upon reviewing Tajwar et al. (2024b), I couldn’t find an explicit discussion of the latter claim regarding forward KL preserving mode coverage. Could the authors please clarify whether this interpretation is directly supported by that work or derived from general understanding in the literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1umxqwLLUq", "forum": "kJKnP1aiGS", "replyto": "kJKnP1aiGS", "signatures": ["ICLR.cc/2026/Conference/Submission17270/Reviewer_rYjr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17270/Reviewer_rYjr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17270/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761300806663, "cdate": 1761300806663, "tmdate": 1762927218539, "mdate": 1762927218539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of catastrophic forgetting in the post-training of LLMs. It finds that reinforcement learning suffers almost no forgetting because it uses on-policy data, whereas SFT easily forgets due to off-policy training. The authors propose Iterative-SFT, which generates new data with the current model at the beginning of each epoch, achieving approximately on-policy learning. This method avoids the complexity of RL training while significantly mitigating forgetting."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper conducts both experimental evaluations and theoretical analyses of catastrophic forgetting in SFT and RL, and the conclusions are convincing.\n- It clearly shows that the reason RL resists catastrophic forgetting lies in its on-policy nature of data, rather than KL regularization or advantage estimation, which is an observation of notable value."}, "weaknesses": {"value": "- The practicality of Iterative-SFT may be limited for two reasons: 1) Since the policy model generates its own training data, the generated examples may not be sufficiently challenging compared to data produced by a stronger teacher model; 2) It requires a reward model or rule-based verification methods to score and filter the data; however, because the policy model itself may not be well-versed in the target domain, the proportion of high-quality samples could be low, placing high demands on the reward model/rule-based verification methods."}, "questions": {"value": "- Why are the Self-SFT and SFT curves shown as straight lines, rather than plotted after each epoch like Iterative-SFT with a stepwise curve?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fLQkV0tXtY", "forum": "kJKnP1aiGS", "replyto": "kJKnP1aiGS", "signatures": ["ICLR.cc/2026/Conference/Submission17270/Reviewer_toGA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17270/Reviewer_toGA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17270/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721730620, "cdate": 1761721730620, "tmdate": 1762927218227, "mdate": 1762927218227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}