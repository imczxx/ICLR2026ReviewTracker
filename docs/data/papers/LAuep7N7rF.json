{"id": "LAuep7N7rF", "number": 15028, "cdate": 1758247033552, "mdate": 1763713240634, "content": {"title": "Incomplete Multi-View Multi-Label Classification via Shared Codebook and Fused-Teacher Self-Distillation", "abstract": "Although multi-view multi-label learning has been extensively studied, research on the dual-missing scenario, where both views and labels are incomplete, remains largely unexplored. Existing methods mainly rely on contrastive learning or information bottleneck theory to learn consistent representations under missing-view conditions, but relying solely on loss-based constraints limits the ability to capture stable and discriminative shared semantics. To address this issue, we introduce a more structured mechanism for consistent representation learning: we learn discrete consistent representations through a  multi-view shared codebook and cross-view reconstruction, which naturally align different views within the limited shared codebook embeddings and reduce redundant features. At the decision level, we design a weight estimation method that evaluates the ability of each view to preserve label correlation structures, assigning weights accordingly to enhance the quality of the fused prediction. In addition, we introduce a fused-teacher self-distillation framework, where the fused prediction guides the training of view-specific classifiers and feeds the global knowledge back into the single-view branches, thereby enhancing the generalization ability of the model under missing-label conditions. The effectiveness of our proposed method is thoroughly demonstrated through extensive comparative experiments with advanced methods on five benchmark datasets.", "tldr": "", "keywords": ["multi-label classification", "dual incomplete multi-view multi-label classification", "representation learning", "label correlations", "multi-view consistent representation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dca19d8a02cf03820b1434d9403312f0245494f3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel framework, SCSD, designed to address incomplete multi-view data scenarios that involve both missing views and missing labels. The approach incorporates three core components: (1) a shared codebook that facilitates the learning of discrete and consistent representations by aligning heterogeneous views within a compact latent space; (2) an adaptive view-weight estimation mechanism that leverages label correlations to enhance the reliability of fused predictions; and (3) a fusion-guided self-distillation strategy that transfers holistic knowledge from the fused predictions back to individual view branches, thereby improving generalization and strengthening inter-view collaboration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The SCSD framework enhances the learning process from three complementary aspects—representation learning, decision-level fusion, and training strategy—resulting in a well-organized and conceptually coherent design.\n2.The model exhibits strong scalability, and its discrete representation learning module can be easily adapted or extended to other tasks and architectures.\n3.The ablation studies clearly demonstrate that the fusion-teacher self-distillation mechanism substantially boosts classification accuracy, validating its effectiveness."}, "weaknesses": {"value": "1.While the framework integrates several functional modules that could affect computational efficiency, the paper does not provide a time complexity or efficiency analysis. Including such an evaluation would offer readers a clearer picture of the model’s practical scalability.\n2.The codebook utilization analysis in the appendix lacks details on how the utilization rate is quantified, which reduces reproducibility.\n3.In the experimental section, the authors primarily report superior results over baselines but do not offer deeper performance interpretation or insight. A more thorough discussion would enhance the paper’s contribution.\n4.Since SCSD jointly addresses both missing views and missing labels, it would be helpful to discuss whether any trade-off exists between these two factors—specifically, whether the model exhibits higher sensitivity to one form of incompleteness than the other."}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VakGFS0qQz", "forum": "LAuep7N7rF", "replyto": "LAuep7N7rF", "signatures": ["ICLR.cc/2026/Conference/Submission15028/Reviewer_gK1A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15028/Reviewer_gK1A"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726522867, "cdate": 1761726522867, "tmdate": 1762925355775, "mdate": 1762925355775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge that existing incomplete multi-view multi-label classification methods fail to effectively capture stable and discriminative shared semantic representations. To overcome this limitation, the authors propose a novel framework named Incomplete Multi-View Multi-Label Classification via Shared Codebook and Fused-Teacher Self-Distillation (SCSD). The SCSD model learns discrete and consistent representations across multiple views through a shared codebook and a cross-view reconstruction mechanism. Moreover, a view-weight estimation strategy is introduced to adaptively assess the relative importance of different views, thereby improving the quality of the fused prediction. In addition, a fused-teacher self-distillation framework is designed to enhance the generalization capability of the model. Extensive experiments conducted on multiple benchmark datasets demonstrate the superior performance and effectiveness of SCSD compared with existing state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe proposed approach effectively integrates a shared codebook and cross-view reconstruction to learn consistent discrete representations across multiple views. The method is conceptually well-founded and supported by solid experimental evidence.\n2.\tThe weight estimation strategy and a teacher self-distillation framework are introduced to improve the quality of view fusion and the generalization of the proposed model.\n3.\tComprehensive experiments on five publicly available benchmark datasets show that SCSD achieves superior performance across various evaluation metrics, confirming its robustness and effectiveness."}, "weaknesses": {"value": "1.\tIn Figures 3(a) and 3(b) (page 8), the axis labels are too small, which negatively affects readability. Enlarging the font size would improve the presentation quality.\n2.\tIn the overall optimization objective (Eq. 10), four different loss terms are included, but a weighting coefficient is applied only to the third one. The reasoning behind this particular design choice should be further clarified.\n3.\tThe process for determining the size of the shared codebook is not clearly explained. It would be valuable to discuss whether increasing the codebook size consistently improves performance or whether the gains plateau beyond a certain point.\n4.\tThe manuscript would benefit from a more detailed discussion of the limitations of the proposed approach, which would make the evaluation of SCSD more balanced and comprehensive."}, "questions": {"value": "Refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1E3QjzhvHU", "forum": "LAuep7N7rF", "replyto": "LAuep7N7rF", "signatures": ["ICLR.cc/2026/Conference/Submission15028/Reviewer_Z7Gz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15028/Reviewer_Z7Gz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786776928, "cdate": 1761786776928, "tmdate": 1762925355120, "mdate": 1762925355120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the SCSD framework, which aims to learn consistent multi-view representations under missing-view scenarios. The method enforces representation consistency through a shared cross-view codebook, strengthens semantic alignment via cross-view reconstruction, and adaptively refines view-specific weights based on label correlations. By integrating these components into a unified architecture, the proposed approach achieves robust and reliable performance across various benchmark datasets, demonstrating both novelty and promising experimental potential."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe study addresses the dual-missing problem in multi-view multi-label learning, a challenging and practically significant research direction that has received limited prior exploration.\n2.\tThe proposed shared codebook mechanism effectively mitigates instability in semantic consistency learning under missing-view conditions, outperforming existing methods theoretically and empirically.\n3.\tThe experimental design is rigorous and systematic, and the ablation studies clearly validate the contribution of each component within the SCSD framework."}, "weaknesses": {"value": "1.\tAlthough Table 2 presents extensive quantitative results, the corresponding analysis in Section 3.4 is relatively concise. A more detailed discussion from multiple perspectives (e.g., robustness, scalability, or dataset characteristics) would improve the interpretability of the findings.\n2.\tThe overall loss function in the method section comprises several components. Adding a concise explanatory paragraph summarizing their roles would help readers grasp the optimization objective more intuitively.\n3.\tWhile the use of a shared codebook to learn discrete representations is effective, the paper offers insufficient intuitive explanation or conceptual justification for why discretization enhances performance. Providing such insight would strengthen the theoretical grounding of the approach.\n4.\tGiven the potential heterogeneity of feature distributions across different views, it would be valuable to discuss whether the shared codebook design might unduly restrict view-specific representations, potentially leading to semantic compression or information loss."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7AmGnsMPsj", "forum": "LAuep7N7rF", "replyto": "LAuep7N7rF", "signatures": ["ICLR.cc/2026/Conference/Submission15028/Reviewer_3pou"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15028/Reviewer_3pou"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799699218, "cdate": 1761799699218, "tmdate": 1762925354531, "mdate": 1762925354531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the dual-missing problem in multi-view multi-label learning by introducing the SCSD model. The proposed framework leverages a shared codebook to learn consistent representations across views, employs an adaptive weighted fusion mechanism to aggregate multi-view predictions based on their reliability, and adopts a self-distillation strategy to enhance model generalization. Experimental results on five benchmark datasets consistently demonstrate that SCSD outperforms existing state-of-the-art methods under various incomplete-view settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe proposed weighted fusion strategy, grounded in the label correlation structure, effectively assesses the contribution of each view without introducing extra parameters, thus maintaining model efficiency.\n2.\tThe model exhibits robust and stable performance under different levels of missing data, indicating strong adaptability to scenarios with both missing views and missing labels.\n3.\tThe paper is clearly presented and well-structured, with coherent notation and logical flow, making it accessible and easy to follow."}, "weaknesses": {"value": "1.\tThe parameter sensitivity analysis lacks a systematic evaluation of how varying the codebook size and embedding dimension affect model performance—two crucial factors that directly influence representational capacity.\n2.\tThe problem formulation in Section 2.1 is somewhat lengthy, and the accompanying notations and descriptions could be further streamlined to improve clarity and readability.\n3.\tThe hyperparameter tuning ranges for $\\alpha$ and $\\lambda$ are provided ($[1e^{-2}, 2e^{1}]$ and $[1e^{-3}, 5e^{-1}]$, respectively), but the rationale behind these specific search intervals is not discussed.\n4.\tIt remains unclear whether the shared codebook mechanism introduces constraints related to the number of views or the feature dimensionality, and a discussion on this point would strengthen the paper’s technical completeness."}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0i50ggTXLW", "forum": "LAuep7N7rF", "replyto": "LAuep7N7rF", "signatures": ["ICLR.cc/2026/Conference/Submission15028/Reviewer_Jx7D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15028/Reviewer_Jx7D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897488476, "cdate": 1761897488476, "tmdate": 1762925354081, "mdate": 1762925354081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their valuable comments and for recognizing the novelty and completeness of our work. The reviewers’ feedback is highly meaningful for improving the quality of the paper. We revise the manuscript accordingly and describe the changes in our responses.\n\nBelow are the additions and modifications we make based on the reviewers’ suggestions:\n\n- We adjust the problem definition in Section 2.1 and refine several expressions to improve clarity and readability.\n- We add a description of the overall objective function $\\mathcal{L}$ in Section 2.5.\n- We include an analysis of the time complexity of the SCSD model at the end of Section 2.5.\n- We expand the analysis of the comparative experimental results in Section 3.4, adding multiple perspectives to better explain the performance of the SCSD model.\n- We add experiments evaluating different codebook sizes and embedding dimensions, including results on both the Corel5k and Pascal07 datasets (see Figures 3c and 3d), and we provide corresponding analysis at the end of Section 3.5.\n- We adjust the font size in all subfigures of Figure 3 and re-optimize the overall layout to improve clarity and readability.\n- We add a detailed analysis of the limitations of the SCSD model in the conclusion section (Section 4).\n- We add an additional explanation of the computation in the codebook utilization analysis at the end of Appendix A.3."}}, "id": "9g0JWJHLS1", "forum": "LAuep7N7rF", "replyto": "LAuep7N7rF", "signatures": ["ICLR.cc/2026/Conference/Submission15028/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15028/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission15028/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763712469557, "cdate": 1763712469557, "tmdate": 1763712469557, "mdate": 1763712469557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}