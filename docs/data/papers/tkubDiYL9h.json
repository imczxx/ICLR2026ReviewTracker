{"id": "tkubDiYL9h", "number": 2416, "cdate": 1757078797517, "mdate": 1759898149629, "content": {"title": "Constrained Stochastic Multi-Objective Optimization", "abstract": "This paper aims to address the constrained stochastic multi-objective optimization (CSMOO) problem, where both objectives and constraints involve expectations over random variables. Firstly, to tackle the computational challenge of exact expectation evaluations, we propose two approximation schemes: stochastic approximation, which updates the entire problem using new samples at each iteration, and block stochastic approximation, which updates only subsets of variables iteratively. Secondly, to handle potential infeasibility in the surrogate problems, we develop two strategies: a feasible update reformulation and a rigorously justified penalty scheme equivalent to the original problem. Our framework provides asymptotic convergence guarantees to stationary points that satisfy Fritz John conditions. Experiments on synthetic and real-world wireless communication benchmarks demonstrate superior convergence, stability, and constraint satisfaction over state-of-the-art methods.", "tldr": "", "keywords": ["Multiobjective optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11643e15911400b47dfdf9f4adfb40c4efb91621.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work studies constrained stochastic multi-objective optimization. To handle the randomness in the objective/constraints, the authors introduce different asymptotically consistent estimators. Based on them, the authors propose two algorithms, CSMOO-1 and CSMOO-2, which also adopt the surrogate quadratic program. Lastly, numerical experiments demonstrate the effectiveness of the new algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow.\n\n1. The experiments are detailed."}, "weaknesses": {"value": "1. Please add a reference for Definition 3.\n\n1. The notion $\\xi_t$ in $\\mathbb{E}\\_{\\xi_t}[\\cdot\\mid\\mathcal{F}_t]$ is redundant.\n\n1. In Line 116, the authors use $d$ to denote the dimension of the variable $x$. However, in many other places (e.g., Line 212), the authors change the notation to $k$. Please unify it.\n\n1. In Line 222, the authors use $p_r^t$ to denote the probability of selecting the $r$-th coordinate at the $t$-th iteration. However, in the proof of Theorem 1, it changed to $p_{r,t}$. Please unify it.\n\n1. I cannot find any convergence results for the two newly proposed algorithms. Could the authors provide/say any of them?\n\n1. For Theorem 3, from the current proof, the authors only show that the Fritz John condition is satisfied. However, it is only a necessary condition for a point to be weakly Pareto optimal, but not sufficient. I don't understand why the authors can claim $x^*$ is weakly Pareto optimal.\n\n1. For both algorithms, according to Line 160, I assume the authors default to $z_i=\\inf_{x\\in\\mathcal{X}}f_i(x)$. However, finding $z_i$ may not be easy in many cases. This largely limits the practicality of both algorithms.\n\n1. For CSMOO-2, how does penalty parameter $\\beta$ affect the algorithm? Adding more discussions on it will benefit the work.\n\n1. In Lemma 2, what are the second order sufficiency conditions? In the proof of Theorem 2, does the Lagrange multiplier $\\eta$ satisfy them?\n\n1. In many places in the proof of Theorem 1 (e.g., Line 684), $\\gamma_t^2$ is redundant, and $\\bar{x}_t$ should be $x_t$.\n\n1. Line 755, it should be $t=\\ell k+r-1$ according to the definition in equation (6).\n\n1. Lines 750 and 812, $f_{i,t}$ should be $\\hat{\\nabla}f_i(x_t)$.\n\n1. Lines 797 to 802, these steps only hold for the specified algorithm but not in general, meaning that they are not true under the current statement of Theorem 1. Please revise them."}, "questions": {"value": "See **Weaknesses**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KmuQsomXKG", "forum": "tkubDiYL9h", "replyto": "tkubDiYL9h", "signatures": ["ICLR.cc/2026/Conference/Submission2416/Reviewer_z557"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2416/Reviewer_z557"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760815828483, "cdate": 1760815828483, "tmdate": 1762916228737, "mdate": 1762916228737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors studied CSMOO problems and proposed stochastic and block stochastic approximation schemes to efficiently approximate the original formulation. To handle potential infeasibility in the surrogate problems, they further introduced feasible-update reformulations and a penalty-based strategy with theoretical guarantees. Experiments have been conducted for testing their proposed algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper effectively mitigates the computational burden of exact expectation evaluations by introducing two approximation schemes: (i) a stochastic approximation method that updates all variables with fresh samples each iteration, and (ii) a block stochastic approximation method that updates variable subsets iteratively. The distinction between these schemes makes them suitable for variables of different dimensions.\n2. The paper also addresses potential infeasibility in surrogate problems through two well-motivated strategies: a feasible-update reformulation and a rigorously justified penalty scheme that is theoretically equivalent to the original formulation."}, "weaknesses": {"value": "1. The literature review is not comprehensive. A more thorough discussion of prior work on CSMOO is needed, including existing stationarity conditions and algorithms for solving such problems. The current related-work section focuses primarily on deterministic settings.\n2. The metric, FJ condition, is relatively weak. Are there stronger stationarity guarantees applicable to this class of problems? For example, could the proposed method be shown to converge to KKT points instead?\n3. The presentation of the experimental results lacks clarity. For instance, in Figure 1, the three curves are heavily overlapped—what is the intended takeaway from this plot? Moreover, additional baselines beyond projected SGD would help strengthen the empirical evaluation."}, "questions": {"value": "1. How is subproblem (14) solved in Algorithm 2? Please clarify the exact solution procedure or any approximations used.\n2. What are the advantages of using block stochastic approximation? Does it lead to improved convergence rates or better sample complexity compared to the standard stochastic approximation?\n3. How do you choose $\\rho_t, \\gamma_t, \\beta$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rbh5v57jjx", "forum": "tkubDiYL9h", "replyto": "tkubDiYL9h", "signatures": ["ICLR.cc/2026/Conference/Submission2416/Reviewer_dvpP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2416/Reviewer_dvpP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774162282, "cdate": 1761774162282, "tmdate": 1762916228539, "mdate": 1762916228539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the **constrained stochastic multi-objective optimization (CSMOO)** problem, where both objectives and constraints are expectation-valued. The authors introduce a general algorithmic framework based on **Tchebycheff scalarization**, **stochastic (and block-stochastic) approximations**, and two strategies to handle infeasibility of surrogate problems:\n\n-   **CSMOO-1:** A feasible-update formulation that ensures progress even under inaccurate estimates.\n    \n-   **CSMOO-2:** A penalty-based reformulation proven equivalent to the original problem and always feasible.\n    \n\nThey provide asymptotic consistency results for both function and gradient estimates and show that their algorithms converge to stationary points satisfying the **Fritz John** conditions. Empirical validation includes synthetic experiments and a **wireless communication (physical-layer security)** benchmark, demonstrating better constraint satisfaction and convergence versus baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. CSMOO sits at the intersection of constrained optimization and stochastic multi-objective learning — a setting with growing importance in ML and communications. The formulation is rigorous and well-motivated.\n    \n    \n2. The two proposed methods (feasible-update vs. penalty-based) address key implementation bottlenecks — infeasibility and non-differentiability — in a principled way.\n    \n3. Synthetic and real-world wireless tasks show clear convergence and feasibility improvements. Visualization of Pareto fronts and constraint violation trends are helpful.\n    \n4. The paper is clearly organized, with detailed notation, assumptions, and comparison to prior MOO and CMOO work."}, "weaknesses": {"value": "1. Both algorithms need the **z** vector which is the set of optimal values of the multiple objective functions, which requires minimizing all the m objective functions in advance.\n    \n2. The assumption 1 is somewhat strong since it requires the Lipschitz continuity and smoothness of stochastic function $f_i(\\cdot, \\xi)$ and $g_j(\\cdot, \\xi)$.\n\n   \n3. Provide only asymptotic convergence guarantee. A non-asymptotic analysis would be better.\n\n4. Lack of novelty. Both the moving average estimator for function values and gradients and the quadratic surrogate functions are well-known ideas in stochastic optimization. And I believe that the asymptotic consistency(theorem 1) for moving average estimator is also a well-known result.\n\n5. Evaluation is restricted to low-dimensional synthetic and a single wireless-security case. No large-scale or ML-relevant benchmarks are tested, limiting generality.\n \n \n6. The baseline in wireless communication experiments is a simple projected SGD. Missing comparisons to modern stochastic MOO methods (e.g., stochastic MGDA variants) weakens empirical claims."}, "questions": {"value": "1.  Regarding weakness 2, can you relax assumption 1 to the Lipschitz continuity and smoothness of the expected function $f_i(\\cdot)$ and $g_j(\\cdot)$?\n\n2.  In definition 3 you mention that Fritz John condition is only a **necessary** condition to weak pareto optimality, then how do you conclude in the proof of theorem 3 that $x^*$ is weak Pareto solution by only verifying the Fritz John condition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "r2UJKkQseg", "forum": "tkubDiYL9h", "replyto": "tkubDiYL9h", "signatures": ["ICLR.cc/2026/Conference/Submission2416/Reviewer_fre1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2416/Reviewer_fre1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762081305311, "cdate": 1762081305311, "tmdate": 1762916228368, "mdate": 1762916228368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses constrained stochastic multi-objective optimization (CSMOO) problems where both objectives and constraints involve expectations over random variables. The authors propose two approximation schemes (stochastic approximation and block stochastic approximation) to handle the computational challenge of exact expectation evaluation, and two strategies (feasible update reformulation and penalty scheme) to handle infeasibility in surrogate problems. Theoretical analysis provides asymptotic convergence guarantees to Fritz John stationary points. Experiments on synthetic and wireless communication benchmarks are presented."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper focuses on constrained stochastic multi-objective optimization which has applications in many domains including wireless communications and industrial design.\n- The paper proposes two complementary strategies (CSMOO-1 and CSMOO-2) to handle infeasibility issues that arise from inaccurate expectation estimation, where CSMOO-2 overcomes the increased computational overhead and potentially slower convergence of CSMOO-1.\n- Theoretical analysis are provided to show convergence of proposed methods and the equivalence between the reformulated problem and original problem."}, "weaknesses": {"value": "- The novelty in the paper seems to be limited at the reformulation from original MOO formulation into the constrained stochastic optimization problem. The techniques used after that are rather standard including the surrogate function approximation and gradient estimators. Also, the theoretical results look similar to results in [1] for CSMOO-1 as well.\n- The numerical experiments are not illustrating the complexity of MOO problems as both examples only contain 2 objectives. Overall they do not show how the proposed method can scale up to more complex settings.\n- It looks like $\\beta$ is an important parameter for CSMOO-2 but I do not see discussion on how to specify it. Also, it would be great to have ablation study on how $\\beta$ affects the performance of CSMOO-2.\n- There is only a gradient-based baseline in the experiments and no discussion on why not including other related methods on MOO, e.g. preference-based methods...\n- There are no information provided on how to solve (11), (13) or (14). Even though it might be standard, it would benefit the readers if the authors provide the full details.\n\n[1] Liu, An, Vincent KN Lau, and Borna Kananian. \"Stochastic successive convex approximation for non-convex constrained stochastic optimization.\" IEEE Transactions on Signal Processing 67.16 (2019): 4189-4203."}, "questions": {"value": "- In the reformulated problem such as (9) or (13), should we consider individual slacks for each constraints instead of having only one $y$ or $delta$?\n- What is the complexity of solving problems (11), (13), or (14)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EBQ6E9abrG", "forum": "tkubDiYL9h", "replyto": "tkubDiYL9h", "signatures": ["ICLR.cc/2026/Conference/Submission2416/Reviewer_owf2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2416/Reviewer_owf2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762209394158, "cdate": 1762209394158, "tmdate": 1762916227935, "mdate": 1762916227935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}