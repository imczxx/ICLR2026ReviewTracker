{"id": "tmTpi5swZ7", "number": 12350, "cdate": 1758207211899, "mdate": 1759897515533, "content": {"title": "Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations", "abstract": "Fine-tuning large language models (LLMs) using zeroth-order (ZO) optimization has emerged as a promising alternative to traditional gradient-based methods due to its reduced memory footprint requirement. However, existing ZO methods suffer from high variance in gradient estimation, leading to slow convergence and suboptimal performance on large-scale models. In this work, we propose P-GAP,\na fast LLM fine-tuning approach through zeroth-order optimization with Projected Gradient-Aligned Perturbations. Specifically,  we first estimate a low-dimensional gradient space and then align perturbations in projected gradients' direction within the space. This approach enables reduced the number of perturbed parameters and decreased variance, therefore accelerated convergence for LLM finetuning. Experiments on LLMs show that P-GAP consistently surpasses the baselines, achieving up to 6% increase in accuracy on classification tasks and up to 12% higher accuracy on generation tasks, with up to about 81% less training iterations and 70% less GPU hours. These results demonstrate that P-GAP enables fast, scalable, and resource-efficient ZO LLM fine-tuning.", "tldr": "", "keywords": ["Zeroth-order optimization", "Efficient AI", "LLMs"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef550d40e21a1ce2268ecc0094f9ba70253e2a07.pdf", "supplementary_material": "/attachment/ba9b30b8eb0b4402b02c35e27acc19ebdf7fda0c.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes P-GAP, a zeroth-order optimization method for LLM fine-tuning that reduces gradient estimation variance by projecting perturbations into a low-dimensional subspace and aligning them with approximate gradient directions. The method is motivated by efficiency constraints in large-scale fine-tuning without backpropagation and aims to improve convergence speed and memory efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed projection-based, gradient-aligned perturbation design is useful for this task.\n\n- The paper provides thorough theoretical analysis to support the method."}, "weaknesses": {"value": "- The paper lacks sufficient ablation studies on hyperparameters (e.g., subspace dimension, update frequency, perturbation scale). How sensitive is the method to these choices, and what guidelines should practitioners follow?\n\n- Table 3 and Table 4 only report partial experiments. Could the authors provide results on more datasets to confirm robustness and generality?\n\n- The experiments mainly focus on OPT models, which are relatively outdated. It would strengthen the claims to include newer LLMs to validate the generalization ability of the proposed method.\n\n- The writing feels rushed. Important algorithm are only in the appendix; maybe these should be brought into the main text for clarity and accessibility."}, "questions": {"value": "Seen in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4qhPfT1lpX", "forum": "tmTpi5swZ7", "replyto": "tmTpi5swZ7", "signatures": ["ICLR.cc/2026/Conference/Submission12350/Reviewer_yw6x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12350/Reviewer_yw6x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760766892908, "cdate": 1760766892908, "tmdate": 1762923267182, "mdate": 1762923267182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes P-GAP (Projected Gradient-Aligned Perturbation), a new  fine-tuning method for LLM based on zeroth-order (ZO) optimization. P-GAP reduces the variance of ZO gradient estimation by finding proper perturbation directions in the space of high-dimensional matrices and further decreases the parameter space through low-rank decomposition. Specifically, it performs SVD to estimate a low-dimensional gradient subspace and aligns Gaussian perturbation in that subspace. Empircal experiments on models including RoBERTa-large, OPT 2.7B/6.7B/13B, and LLaMA 3B/8B show that P-GAP achieves faster convergence and higher accuracy compared to baselines across diverse classification and generation benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents an interesting extension of directionally aligned perturbations from vectors to matrices to reduce variance in ZO gradient estimation. The authors conduct comprehensive experiments to demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "The paper argues that P-GAP reduces variance through low-dimensional perturbation spaces, but there are no explicit variance measurements or visualizations supporting this claim. The main algorithm (Algorithm 1) is only briefly referenced in the main text, and the appendix lacks a detailed explanation of their theoretical results."}, "questions": {"value": "- The authors claim that working in a low-dimensional space helps reduce variance, but no empirical evidence is provided to support this claim. How can we be sure that the  performance improvements in the experiments are due to variance reduction rather than other factors? Since the central motivation of the paper is variance reduction, the authors should provide more detailed evidence to support this claim.\n\n- The appendix is just a list of technical theoretical statements without any explanations. It is difficult to read and follow, and additional context or intuition would help readers better understand the results.\n\n- Algorithm 1 introduces many hyperparameters but the paper provides little explanations about how to choose them in practice. The authors should include practical guidance or sensitivity analysis to help practitioners set these hyperparameters effectively.\n\n- The descriptions of the algorithms have several notation errors. For example, what does $S_{\\ell}^{r}$ in eq (15) represent? The notation used in Algorithm 1 and Algorithm 2 is inconsistent (e.g., $S_{\\ell}^{r}$ and $S_{r}^{\\ell}$, $\\ell_+, \\ell_-$ in line 16). What is $z$ in line 15?\n\n- Typo in line 6 of Algorithm 2 in the expression $\\mathcal{L}^j_+ -$.\n\n- In line 12, if the parameter is not a matrix, does the algorithm just perform standard ZO gradient estimation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KWV1CUhh85", "forum": "tmTpi5swZ7", "replyto": "tmTpi5swZ7", "signatures": ["ICLR.cc/2026/Conference/Submission12350/Reviewer_pUhR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12350/Reviewer_pUhR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761701265814, "cdate": 1761701265814, "tmdate": 1762923266836, "mdate": 1762923266836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **P-GAP**, a zeroth-order (ZO) fine-tuning scheme for LLMs that samples perturbations in a **low-dimensional gradient subspace** and then **aligns** them with a projected gradient direction. The subspace is obtained via a low-rank factorization (e.g., SVD) and is updated lazily to amortize cost; ZO gradient estimates are then formed using corrected low-dimensional perturbations mapped back to the full parameter space. The authors provide variance and convergence analyses arguing that restricting/aligning perturbations lowers estimator variance relative to standard ZO. They evaluate on encoder (RoBERTa-large) and decoder models (OPT/LLaMA) across classification and QA tasks, and report faster convergence, reduced GPU hours, and higher accuracy compared to ZO baselines; they also report results with LoRA and include wall-clock and memory analyses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Clearly targets the known high-variance issue of ZO by combining dimensionality reduction with directional alignment of perturbations.\n* The projection/alignment step is simple, uses standard linear algebra, and can be dropped into existing ZO pipelines (incl. PEFT/LoRA) with minimal changes.\n* Provides analysis on variance scaling with dimensionality and argues why the proposed projection reduces estimator variance; includes a convergence discussion under stated assumptions.\n* Uses a lazy-update strategy to limit subspace estimation overhead; focuses on wall-clock and memory implications that matter for resource-constrained fine-tuning.\n* Evaluations span multiple model families and parameter scales; the paper reports gains over representative ZO baselines and  reports additional improvements when combined with LoRA."}, "weaknesses": {"value": "1. **Near-duplicate figures/tables vs. #12282.** The plotting/table template and ordering are *almost identical* to **#12282**, with colors changed: **#12350 Fig.3 / Fig.2 / Fig.4 ≈ #12282 Fig.1 / Fig.2 / Fig.3** (axes, legends, and layouts closely match).\n\n2. **If shared templates are acceptable, how to explain drifting baselines?** Under ostensibly comparable settings, baselines diverge between the two papers in ways that **systematically favor each paper’s method**.\n\n   * Example (**Table 2 · DROP**): **#12350** MeZO-LoRA=19.2 / HiZOO-LoRA=18.3 / **P-GAP-LoRA=22.5** **vs.** **#12282** MeZO-LoRA=13.4 / HiZOO-LoRA=13.9 / KerZOO-LoRA=14.7.\n   * Example (**Table 3**): similar column-wise discrepancies under matched configurations.\n\n3. **Even within this paper, baselines are modified without explanation.** Several MeZO entries (and MeZO-LoRA in particular) differ from the MeZO paper, while others (e.g., the **Zero-shot** row) track the original exactly. The paper does not state which baselines were re-run vs. quoted, nor justify the changes—differences consistently benefit the proposed method.\n\n4. **Reproducibility gap.** The code release lacks **requirements.txt / environment specs, hyperparameters and seeds, preventing independent reproduction and verification of the baseline inconsistencies."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ws9nsG6Qhn", "forum": "tmTpi5swZ7", "replyto": "tmTpi5swZ7", "signatures": ["ICLR.cc/2026/Conference/Submission12350/Reviewer_gySz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12350/Reviewer_gySz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969185220, "cdate": 1761969185220, "tmdate": 1762923266457, "mdate": 1762923266457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}