{"id": "NgJDFIJF5c", "number": 16665, "cdate": 1758267441319, "mdate": 1759897226153, "content": {"title": "Strata-Sword: A Hierarchical Safety Evaluation towards LLMs based on Reasoning Complexity of Jailbreak Instructions", "abstract": "Large language models (LLMs) have gained widespread recognition for their superior performance and have been deployed across numerous domains. Building on Chain-of-Thought (CoT) ideology, Large Reasoning models (LRMs) further exhibit strong reasoning skills, enabling them to infer more accurately and respond appropriately. However, strong general reasoning capabilities do not guarantee a safety response to jailbreak instructions requiring even more robust reasoning capabilities. A model with strong general reasoning capabilities but lacking corresponding safety capabilities can create serious vulnerabilities in the real application.\nTherefore, a comprehensive benchmark needs to be established to evaluate the safety performance of the model in the face of instructions of different reasoning complexity, which can provide a new dimension of the safety boundaries of the LLMs. This paper  quantifies \"Reasoning Complexity\" as an evaluable safety dimension and categorizes 15 jailbreak attack methods into three different levels according to the reasoning complexity,  establishing a hierarchical Chinese-English jailbreak safety benchmark for systematically evaluating the safety performance of LLMs. Meanwhile, to fully consider reasoning complexity brought by unique language characteristics, we first propose some Chinese jailbreak attack methods, including the Chinese Character Disassembly attack, Lantern Riddle attack, and Acrostic Poem attack. A series of experiments indicate that current LLMs and LRMs show different safety boundaries under different reasoning complexity, which provides a new perspective to develop safer LLMs and LRMs", "tldr": "", "keywords": ["Large language models", "Jailbreak Attack", "Reasoning Complexity"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8f046cbe288d8ab79723b2fccbc24f4722b0f9b.pdf", "supplementary_material": "/attachment/f7b5431d7d0625dd7579a8dffcea39c8cfbdfb52.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a hierarchical benchmark which is designed to evaluate LLMs under jailbreak attacks of varying reasoning complexity. It introduces reasoning complexity as a new safety dimension composed of logical depth, linguistic ambiguity, and task overhead . Fifteen types of jailbreak attacks are categorized into three hierarchical levels. Experimental results demonstrate that model vulnerability increases with reasoning complexity, revealing clear scaling patterns and cross-lingual asymmetries."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction of reasoning complexity as a safety dimension is novel and meaningful.\n2. The writing is easy to follow."}, "weaknesses": {"value": "1. The definition of reasoning complexity is heuristic. It lacks theoretical derivation to be convinced.\n2. The description of the construction, categorization, and other essential details of the 700 attack prompts lacks clarity.\n3. The evaluation relies solely on one classifier, QwQ-32B without justification or comparative testing. Moreover, the model’s performance on relevant tasks is not presented in experiments, which may result in biased or unreliable conclusions.\n4. The use of a single metric, ASR, is overly simplistic and limits the depth of performance analysis.\n5. The multilingual analysis includes only Chinese besides English, making the claimed cross-lingual contribution narrow in scope."}, "questions": {"value": "1. How to ensure the scientific validity of the reasoning complexity hierarchy and its level definitions?\n2. What are the key design factors considered when constructing and categorizing the 700 attack prompts?\n3. Why is QwQ-32B chosen as the evaluator, and have comparative tests been conducted with other evaluators?\n4. Is the classification and annotation process conducted manually or automatically? If manually, how is consistency and fairness ensured among annotators? If automatically, how is model-based annotation accuracy verified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ykHeZsR2yb", "forum": "NgJDFIJF5c", "replyto": "NgJDFIJF5c", "signatures": ["ICLR.cc/2026/Conference/Submission16665/Reviewer_3WVB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16665/Reviewer_3WVB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761221808566, "cdate": 1761221808566, "tmdate": 1762926723674, "mdate": 1762926723674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper highlights that the general reasoning capabilities of large reasoning models (LRMs) are insufficient to ensure safe responses to jailbreak instructions, which demand even more robust and nuanced reasoning. To address this challenge, the authors introduce \"reasoning complexity\" as a quantifiable safety dimension, including factors such as logical depth, linguistic ambiguity, and task overhead. Based on this framework, jailbreak attack methods are classified into three levels: Level 1 (Basic Instruction), Level 2 (Simple Reasoning), and Level 3 (Complex Reasoning). Using this categorization, the paper constructs a hierarchical Chinese-English jailbreak safety benchmark named Strata-Sword, and evaluates current LLMs and LRMs to reveal their varying safety boundaries.\n\nOverall, the paper presents an interesting and potentially useful benchmark for safety evaluation. However, the depth of analysis remains limited, and the research would benefit from a more thorough investigation."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation for building a jailbreak benchmark organized by \"reasoning complexity\" makes sense and is clearly presented.\n\n2. The curated Strata-Sword benchmark contributes to evaluating the safety boundaries of existing LLMs and LRMs."}, "weaknesses": {"value": "1. The primary concern is that this paper lacks research depth. The three predefined elements of \"reasoning complexity\" and the corresponding three-tier safety evaluation appear intuitive but are not supported by any guiding principles or theoretical justification to validate the completeness of the categorization.\n\n2. The experiments are somewhat weak. The authors primarily report attack success rates of various LLMs and LRMs across different model families and sizes, without deeper analysis. The five so-called \"insights\" presented are more akin to surface-level observations derived directly from the evaluation results, which makes the paper resemble an experimental report rather than a substantive research study. It would be much better to involve a deeper investigation into the underlying causes of model behaviors and a discussion of potential strategies to enhance safety alignment.\n\n3. Regarding the proposed Strata-Sword benchmark, the paper lacks a comprehensive comparison with existing safety evaluation benchmarks. This makes it difficult to assess the relative value or novelty of the benchmark within the broader landscape."}, "questions": {"value": "1. In the evaluation setup, why do the authors choose the QwQ-32B model only to assess the safety?\n\n2. For the \"Insight 3\", what does the \"temporal trends\" mean? Where's the evidence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "muQy1TGgvc", "forum": "NgJDFIJF5c", "replyto": "NgJDFIJF5c", "signatures": ["ICLR.cc/2026/Conference/Submission16665/Reviewer_YtyB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16665/Reviewer_YtyB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771537679, "cdate": 1761771537679, "tmdate": 1762926723211, "mdate": 1762926723211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on evaluating model safety—specifically, a model’s ability to consistently refuse malicious instructions. It introduces the Strata-Sword benchmark, which classifies 15 existing jailbreak techniques into three complexity levels based on three dimensions: Logical Depth, Linguistic Ambiguity, and Task Overhead. The study also designs several jailbreak methods tailored to Chinese, demonstrating strong attack effectiveness. By systematically evaluating models on this benchmark, the paper synthesizes the current state of model safety and highlights avenues for improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The study makes an important point: jailbreak techniques should be stratified by complexity to help us better understand the trajectory of model safety research.\n\n2. It conducts comprehensive experiments, providing systematic ratings of mainstream open-source and closed-source models across 15 jailbreak methods.\n\n3. It summarizes developments in model safety and offers suggestions for future work."}, "weaknesses": {"value": "Main limitation. The evaluation scheme is built on static, heuristic intuition, and the presented results do not always align with the proposed levels. For example, in Table 2, for Vicuna-7B in English settings, the attack success rates for L1 and L2 are very close; similar issues appear for DS-Distill-Qwen2-7B. In Figure 2, the OPPOSING method (categorized as L2) outperforms some L3 methods. These observations suggest that the proposed framework only coarsely reflects attack complexity and does not always withstand closer scrutiny.\n\nPreferred approach. Ideally, we should adopt a dynamic evaluation: mix all jailbreak prompts together and score each prompt using the proposed metrics to produce a prompt-level ranking. This would directly validate the effectiveness of the complexity framework, rather than relying on coarse, method-level categories. It also addresses a practical need: a robust evaluation system must be able to classify all existing jailbreak methods. Heuristic definitions struggle to scale to that goal, whereas a dynamic scoring approach can."}, "questions": {"value": "Refer to our proposed weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x5TCWyhbMW", "forum": "NgJDFIJF5c", "replyto": "NgJDFIJF5c", "signatures": ["ICLR.cc/2026/Conference/Submission16665/Reviewer_2Yr1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16665/Reviewer_2Yr1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988161870, "cdate": 1761988161870, "tmdate": 1762926722816, "mdate": 1762926722816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}