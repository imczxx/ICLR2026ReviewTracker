{"id": "9upf6JVssk", "number": 9200, "cdate": 1758114940864, "mdate": 1759897738056, "content": {"title": "MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive Text Sources", "abstract": "We present MixtureVitae, an open‑access pretraining corpus built to minimize copyright risk while preserving strong downstream performance. MixtureVitae follows a risk‑mitigated sourcing strategy that combines public‑domain and permissively licensed text (e.g., CC‑BY/Apache) with carefully justified low‑risk additions (e.g., government works and EU TDM‑eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi‑stage pipeline for license‑aware filtering, safety and quality screening, and domain‑aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open‑sci‑ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb‑Edu and approach DCLM in the later stages of training. Performance is particularly strong on MMLU and competitive on QA tasks. These results demonstrate that permissive‑first, risk‑mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness.", "tldr": "MixtureVitae is a legally mitigated pretraining dataset built from open and civic sources. It matches or beats mixed-license baselines, proving high performance doesn’t require risky web scrapes.", "keywords": ["Large Language Models", "Pretraining Datasets", "Permissive Licensing", "Open Data for AI", "Text and Code Corpora"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58901f3e3642a627f2285cb87c3288b1b8b81d47.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper curates a large-scale open and permissive pretraining corpus. In comparison to its permissive counterparts, Mixture Vitae has a special focus on reasoning/math/code- specific data and synthetic data, giving it superior performance in corresponding downstream tasks. The authors have also conducted rigorous experiments in an open-sci-ref manner, which shows that Mixture Vitae serves as a strong permissive baseline and sheds insights on which part of the data brings in the biggest performance improvement. Overall, the paper makes an important contribution to the open-source community."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "-The authors make a timely and commendable contribution toward creating legally safer and more reproducible pretraining data amid increasing copyright litigation risks. \n\n- The paper shows that a permissive-first data approach can produce competitive models, with the proposed dataset outperforming all other permissive alternatives."}, "weaknesses": {"value": "- It seems that a major strength of the MixtureVitae dataset is to minimize copyright risk; however, there are no demonstrative experiments to support this. For example, it would be interesting to see if memorisation over New York Times articles instead gets reduced training with MixtureVitae. Similar verbatim experiments like in [1,2] can further strengthen this paper.\n\n- As a pretraining dataset, its size is relatively small compared to the 8TB used in [3]."}, "questions": {"value": "- Regarding the web portion of your dataset, how did you check the licenses of the crawled dataset? In a recent paper by Fan et al [2], the authors did a retroactive robots.txt filtering to make sure web-scale data is compliant. Have similar efforts been done here?\n\n- I think it is pretty impressive how much improvement one can get by curating more code/math/reasoning data. However, is it necessarily the goal of pretraining to arrive at a reasoning-capable base model? A lot of models only enhance such capabilities in post training, and it seems to me that they just used such data in their post training phases. Thus it is unfair to compare the performances on GSM-8k, MBPP, IF-eval such benchmarks.\n\n\n\n[1] Exploring Memorization and Copyright Violation in Frontier LLMs: A Study of the New York Times v. OpenAI 2023 Lawsuit\n\n[2] Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs\n\n[3] The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NoMvbiI9aT", "forum": "9upf6JVssk", "replyto": "9upf6JVssk", "signatures": ["ICLR.cc/2026/Conference/Submission9200/Reviewer_otHz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9200/Reviewer_otHz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770511245, "cdate": 1761770511245, "tmdate": 1762920868921, "mdate": 1762920868921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MixtureVitae, a pre-training corpus of 211B permissively licensed data, and thus minimize the risk of copyright. To enrich the diversity and quality of MixtureVitae, they aggregate existing permissive high-quality data resources, synthetic data, instruction-following data, and reasoning data. They conduct pre-training experiments using MixtureVitae and baseline datasets on 4 sizes of LLMs (from 130M to 1.7B), finding that MixtureVitae leads to better pre-training outcome than other permissive pre-training corpus. The performance is even competitive to high-quality, non-permissive baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality and significance: the paper's creativity can be reflected from the data curation pipeline. It strategically combines data from various sources and published datasets, including web data, synthetic data, instruction-following, and reasoning data. More importantly, such aggregation is under a license-permissive manner to minimize the potential legal risk. It addresses a critical problem in the field: how to train high-quality open LLMs on permissive data only. To my knowledge, only concurrent work Apertus tries to address this problem despite its importance.\n\n- Quality: The paper conduct comprehensive evaluation and comparison across difference LLM sizes, training data scale, and evaluation benchmarks. It also involves robustness check through decontamination and ablation. The permission filtering is particularly novel compared to related work.\n\n- Clarity: The data processing pipeline -- the major contribution of this pre-training corpus paper -- is well documented and clearly understandable."}, "weaknesses": {"value": "I'm not aware of major weaknesses hurting the soundness this paper. One minor weakness:\n\n- Lack of discussion on the potential scalability of the data collecting pipeline: current LLMs need ~10T level tokens to achieve SOTA results. However, MixtureVitae only has 211B tokens. The size is fair considering its permissive feature, but would be good to discuss whether the quantity can further grow using the proposed data curation pipeline. The limited datasize also restrict the experiments to LLMs of relatively small scales."}, "questions": {"value": "- Differences between MixtureVitae and Apertus: It is fair enough not including Apertus dataset as a baseline, given that it is a concurrent work. But the related work discussion would be much more thorough, if the paper can discuss more about how the data collection of Apertus and MixtureVitae differs, and how the advantages of both can be combined together."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "I am not a legal expert in copyright and licensing, which is a main claim of the paper. So it would be good if someone else can check it."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lzFt0EzvAb", "forum": "9upf6JVssk", "replyto": "9upf6JVssk", "signatures": ["ICLR.cc/2026/Conference/Submission9200/Reviewer_1Bso"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9200/Reviewer_1Bso"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919988250, "cdate": 1761919988250, "tmdate": 1762920868475, "mdate": 1762920868475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Comment for all Reviewers"}, "comment": {"value": "We thank the reviewers for their constructive feedback and for recognizing the significance of a permissive‑first corpus. We will revise the manuscript to incorporate the reviewers’ suggestions and clarify points that were previously unclear. We are encouraged by the consensus on our core strengths:\n\n* **Timely Contribution**: Described as \"timely and commendable\" (otHz) and addressing a \"critical problem\" (1Bso) by demonstrating high-performance training with \"legally safer\" (otHz) sources.\n* **Transparent Curation**: The pipeline was noted as \"thorough, transparent, and literature-grounded\" (NdNQ) and \"well documented\" (1Bso), with a decontamination analysis that \"convincingly addresses leakage concerns\" (NdNQ).\n* **Strong Validation**: Reviewers praised the \"rigorous experiments\" (otHz), noting MixtureVitae \"outperforms all other permissive alternatives\" (otHz) and \"approaches parity\" (NdNQ) with non-permissive baselines.\n\nAcross the reviews, we identified a few shared themes, which we address below: **Dataset Scalability, Curation Strategy & Licensing, and Evaluation Integrity: Contamination and Memorization**. \n\n### Dataset Size and Scalability\n\nRegarding the observed scale difference (211B vs 10T+ in frontier runs), our primary goal was a proof-of-concept for data efficiency and strong performance within a permissive‑first, risk‑mitigated licensing framework. We see several avenues to scale this approach:\n* **Clarification on Baselines**: Common Pile is an 8TB pool; the derived training set (Comma-0.1) is ~600B unique tokens.\n* **Subset Upsampling**: Unlike common recipes (e.g., Llama3) that upsample high-quality subsets 4-10x, MixtureVitae does not assign special upsampling factors to individual shards. Upsampling our highest-value subsets would immediately scale their contribution.\n* **Multilingual Expansion**: Expanding to multilingual data, via either sourcing additional datasets or translation, represents an order-of-magnitude opportunity for scaling.\n* **Synthetic Expansion**: Our synthetic subsets could be expanded by generating more samples using permissively licensed models.\n* **Web Data Rephrasing**: Rephrasing of web data, subject to the same allowlist andlegal‑risk framework we use in MixtureVitae, can help expand the corpus.\n\n### Curation Strategy & Licensing\n\nReviewers (OtHZ, 1Bso) discussed our data curation and licensing methodology. We wish to clarify our \"Permissive-First, Risk-Mitigated\" design philosophy, which distinguishes MixtureVitae from large-scale crawls like Common Corpus or Apertus.\n* Pipelines such as Apertus operate on broad web scrapes and then apply negative filtering (e.g., discarding data based on current `robots.txt` rules).\n* **Our Approach**: MixtureVitae uses a positive inclusion (allowlist) strategy. We positively selected safe sources derived from Common Crawl. In regard to opt-out, we leverage Common Crawl’s inherent adherence to `robots.txt` at the time of capture.\n* **Benefit**: By filtering the web data component of MixtureVitae for permissive and `.gov` (or similar) domains, and leveraging Common Crawl’s inherent adherence to opt-out protocols, we believe we comply with text data mining opt-out regimes and guard against the ingestion of copyrighted content (like the New York Times) from the start. This proactive approach substantially reduces reliance on retroactive decontamination and, together with our 13-gram decontamination checks, establishes a cleaner legal profile for users.\n* **Tiered Legal‑Risk Framework and Synthetic Data**: As described in Section 2.1.4, we additionally organize all sources, including synthetic instruction/reasoning data, into tiers with distinct risk profiles. Most of MixtureVitae’s data has a clearly permissive provenance and falls into our lowest‑risk tier (Fig. 2(b)). \n\n### Evaluation Integrity: Contamination and Memorization\nReviewers emphasized the importance of ruling out test leakage and, more broadly, the risk of memorization on restricted sources (e.g., NYT). We performed a 13‑gram decontamination sweep over all benchmarks: document‑level overlap is negligible for most tasks, and performance is stable when we (i) re-evaluate on decontaminated test sets (Table 9) and (ii) retrain after removing shards that account for most of the contamination signal (Fig. 9). \n\nBecause >90% of MixtureVitae consists of code, math, synthetic reasoning, and curated scientific and civic sources, and the remaining web component is built via positive inclusion and license‑keyword filtering, we expect the presence of paywalled or opted‑out publishers to be very low. As a result, NYT‑style memorization tests are less diagnostic for this dataset than for broad web scrapes, though we agree that such tests are valuable future work.\n\nWe are thankful for the reviewers for their time and constructive engagement. We will use the feedback to strengthen our manuscript and we hope that our responses have addressed all outstanding concerns."}}, "id": "rYIFjfu6qa", "forum": "9upf6JVssk", "replyto": "9upf6JVssk", "signatures": ["ICLR.cc/2026/Conference/Submission9200/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9200/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9200/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763673192958, "cdate": 1763673192958, "tmdate": 1763673192958, "mdate": 1763673192958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MixtureVitae, a 211.1 B token permissive-first, risk-mitigated pretraining corpus that integrates explicitly licensed sources (CC-BY, Apache, public domain), curated permissive code, civic/governmental content, and reasoning-oriented synthetic data with documented provenance. Using a controlled open-sci-ref training recipe, the authors show that MixtureVitae (i) outperforms existing permissive datasets and (ii) approaches parity with non-permissive corpora such as Nemotron-CC-HQ and DCLM, particularly on MMLU, GSM8K, and MBPP. The dataset composition, license tiers, filtering pipeline, and decontamination analyses are fully documented. The main contribution is an empirically validated demonstration that a carefully constructed, permissive-first mixture can achieve competitive downstream performance without relying on legally ambiguous data and release/documentation of the dataset artifacts."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "(1) Thorough, transparent, and literature-grounded curation pipeline. \\\nThe paper provides a clear and well-justified account of how the dataset was constructed, with each design choice motivated by established best practices in large-scale data curation and LLM training. The authors’ approach is thoroughly grounded in prior literature and consistent with contemporary standards, reflecting a careful, research-driven methodology rather than ad-hoc engineering.\n\n(2) Careful empirical validation, principled source selection, and clear legal rationale. \\\nThe authors conduct rigorous and systematically controlled experiments, maintaining fixed architectures and hyperparameters to isolate the effects of data quality and composition. More broadly, the experimental design reflects an academic level of care in controlling confounding factors and articulating how scaling interacts with dataset construction. The dataset sources are thoughtfully selected, with explicit reasoning about what constitutes “permissive” or “low-risk” content. The tiered legal framework is well documented and well-reasoned, reflecting a careful effort to maximize the research utility of the data while appropriately managing potential legal and compliance risks.\n\n(3) Careful decontamination analysis that convincingly addresses leakage concerns. \\\nAppendix D.3 and Table 9 show downstream test set exact-match filtering, overlap statistics, and evaluation with highly contaminated shards/documents removed. This analysis alleviates the most natural skepticism, that high performance stems from test overlap rather than data quality.\n\n(4) High-quality writing and presentation. \\\nThe paper is readable, well-structured, and supported by comprehensive experiments and reproducibility documentation."}, "weaknesses": {"value": "(1) Synthetic-data provenance and generation details remain somewhat opaque. \\\nIf I understand correctly, the authors do not themselves generate new synthetic data but instead aggregate existing permissively licensed datasets that may include synthetically generated data. However, some of these sources (e.g., Nemotron-CC, MagaCorpus) were produced by rephrasing or re-expressing web text from non-permissive sources. It would help to clarify exactly how these synthetic subsets were created—whether they consist primarily of paraphrases of non-permissive text, newly generated reasoning or math/code problems, or continuations of conversational data. More broadly, is there existing evidence (either from the authors or prior literature) that such rephrasing or rewriting pipelines reliably avoid leakage of protected content? Clarifying these points would strengthen confidence in the “permissive-only” guarantee, though this is more a request for detail than a critique of the current results.\n\n(2) Organization and emphasis of test data contamination results. \\\nThe ablation analysis in Section 3.4 is somewhat coarse, focusing primarily on Full, without Web, without Instructions. Given the breadth of the results, this section could be condensed or moved to the Appendix, with more space in the main body dedicated to the data contamination analysis (Appendix D.3 and Table 9). The possibility of test set leakage is one of the most plausible alternative explanations for the paper’s strong capability results, and the contamination analysis directly and convincingly addresses that concern. Bringing more of this evidence into the main text would highlight how the authors empirically address one of the strongest counter-hypotheses, thereby strengthening the credibility of the central claims.\n\n(3) Contamination detection focuses on exact n-gram matches. \\\nThe 13-gram decontamination method used is standard but may overlook paraphrased or semantically similar examples. It would be helpful for the authors to comment on whether they explored or plan to explore approximate or embedding-based methods (e.g., LSH, SimHash, ANN) for identifying near-duplicates or semantic overlaps. Even a brief discussion of these trade-offs would further strengthen the already careful analysis in Appendix D.3.\n\n(4) Limited granularity in reasoning/instruction ablations. \\\nIf the dataset is already organized into domain-aware shards, there may be value in providing a finer-grained ablation within the reasoning and instruction components themselves. As currently presented, the without Instructions ablation yields an intuitive result, performance on MMLU and related reasoning benchmarks drops sharply, whereas without Web has a much smaller effect. A more granular analysis of which shards within the reasoning/instruction data (e.g., math, coding, general instruction, synthetic Q&A) contribute most to downstream performance would be more informative and would better illuminate what kinds of reasoning data are most valuable."}, "questions": {"value": "1. Could the authors clarify how the synthetic subsets were actually produced? From the description, it seems they are drawn from existing datasets with synthetically generated data rather than newly generated data. Were these primarily rephrasings of non-permissive text, continuations of such text in a chat or QA style, or newly constructed reasoning and math examples designed to resemble those domains? Understanding this distinction would help clarify how these datasets were created and why they are considered permissive.\n\n2. Am I correct in understanding that the dataset shards are formed based on source-domain groupings or other dataset-level partitions (e.g., those listed in Table 11)? While I understand that additional ablations may be resource-intensive, it struck me that it would be extremely valuable for the research community if the released dataset included clear documentation of how these shards are defined and organized—along with their relative sizes, license tiers, and token counts. Making this shard-level metadata available would enhance transparency and enable future research/ablations on data mixtures.\n\n3. The downstream results show that the chosen reasoning, math, and coding datasets were remarkably effective in driving strong benchmark performance, even though the mixture appears to rely on a relatively focused set of high-quality permissive sources. Could the authors elaborate on how these particular datasets were selected? Were they chosen to be representative of the domains emphasized in downstream benchmarks, or based on prior evidence or community popularity in other pretraining mixtures?\n\nSmall line-level edits\n- Line 110: \"It also contain synthetic data\" -> \"It also contains synthetic data\"\n- Line 122: I think the Saxton in-line citation is misformatted.\n- Line 142: \"as well domain-specific datasets\" -> \"as well as domain-specific datasets\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GAbL9XZmSy", "forum": "9upf6JVssk", "replyto": "9upf6JVssk", "signatures": ["ICLR.cc/2026/Conference/Submission9200/Reviewer_NdNQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9200/Reviewer_NdNQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031495185, "cdate": 1762031495185, "tmdate": 1762920868082, "mdate": 1762920868082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}