{"id": "6YTEDVd80T", "number": 18248, "cdate": 1758285622978, "mdate": 1763671521659, "content": {"title": "Semantic Calibration in Media Streams", "abstract": "Current generative models can produce synthetic media that is visually indistinguishable from real content. As a result, traditional detection methods rely mostly on subtle artifacts introduced during generation. However, we show that such methods could eventually become ineffective. Anticipating this, we suggest that the main risk lies not in whether a media sample is synthetic or real, but in whether its semantic content is deceiving, that is, whether it distorts the information distribution in a way that misrepresents reality. To capture this, we formally introduce the notion of deception in the context of online media streams. Complementing standard detection approaches, we introduce semantic calibration to mitigate deception directly by processing semantic content using captioning and large language models, rather than relying on artifacts introduced by generative models. Our method is explainable, transparent, and modality agnostic, providing a rigorous foundation for developing new tools to combat online misinformation. We offer both theoretical justification and empirical evidence for its effectiveness.", "tldr": "", "keywords": ["moderation", "semantic calibration"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd36da35b8709b906dafc8f295b5e593f0ff76f4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper argues that instead of merely detecting whether media content is synthetic or not, one should directly calibrate its semantic distribution to mitigate potential semantic deception. The authors theoretically demonstrate that conventional deepfake detection methods based on non-semantic artifacts will ultimately fail as generative models approach perfection. Accordingly, they propose a Semantic Calibration framework, which employs captioning models and language models to perform rejection sampling in the semantic space, thereby enabling cross-modal content filtering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The authors are the first to formally define semantic deception and introduce a distribution-level metric for it, providing a perspective that transcends the traditional binary paradigm of authenticity detection.\n2.The proposed pipeline‚Äîextracting semantics via captioning models, estimating semantic distribution ratios with two language models, and performing rejection sampling‚Äîis concise, transparent, and interpretable through token-level saliency analysis."}, "weaknesses": {"value": "1.The image experiments are conducted only on COCO, CIFAR-10, CIFAR-100, and ImageNet. It is recommended that the paper include evaluations on datasets more representative of security-related applications, such as those in the deepfake or AIGC-generated content detection domains.\n2.In practical scenarios, a single image may correspond to multiple valid descriptions (e.g., ‚Äúa man playing guitar‚Äù vs. ‚Äúa musician performing on stage‚Äù), and a text segment may have several semantically equivalent paraphrases. Such multi-description diversity introduces representational ambiguity in the semantic space, which may cause the model to misinterpret natural linguistic variability as semantic shift or deception.\n3 the method assumes captions are an (almost) lossless stand-in for semantics (aiming for ùêª(ùëç‚à£ùëç^)=0) and then operates entirely in caption space; this is acknowledged but not validated rigorously, and failures of the captioner (omissions, hallucinations, bias) directly impact decisions. \n4 By design the method may pass factually wrong but distribution-typical content, and reject surprising yet true items; this limits suitability for many moderation goals and shifts risk to how ùëùùëü is chosen."}, "questions": {"value": "1. How often does ùêª(ùëç‚à£ùëç^)materially deviate from zero in practice? Do you have end-to-end ablations showing deception-reduction vs. caption quality (e.g., swapping Florence with a weaker/stronger captioner)?\n2. If an attacker crafts captions (or ASR prompts) to mimic ùëùùëü(ùëß^), can they evade calibration? Any defenses beyond top-œÅ filtering?\n3.Your experiments mostly reweight label distributions. How does the method handle semantic recombination (e.g., rare object‚Äìcontext pairs) or multi-label captions?\n4.Can the framework extend to video (temporal narratives) or mixed media posts where text and image semantics conflict?\n5 In the ‚Äúdisjoint support‚Äù case you match OOD detection and report near-perfect scores on misinformation datasets; how realistic is this assumption outside curated text benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oA0vc08NyD", "forum": "6YTEDVd80T", "replyto": "6YTEDVd80T", "signatures": ["ICLR.cc/2026/Conference/Submission18248/Reviewer_aoBm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18248/Reviewer_aoBm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568450762, "cdate": 1761568450762, "tmdate": 1762927978116, "mdate": 1762927978116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes semantic calibration, a novel framework that shifts deepfake and misinformation detection from artifact-based identification to semantic distribution alignment. Instead of distinguishing real versus synthetic content, the method identifies deceptive shifts in semantic information. It employs captioning models to convert multimodal inputs into text, trains two language models to estimate real versus mixed semantic distributions, and applies a likelihood-ratio‚Äìbased rejection sampling rule to filter deceptive media."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work introduces a distributional view of deception, formalizing it via KL divergence between semantic distributions, and rigorously establishes the limitations of traditional deepfake detection under improved generation quality. This theoretical foundation is both timely and intellectually solid.\n2. The proposed semantic calibration offering transparency rarely seen in media integrity research."}, "weaknesses": {"value": "1. Experiments are limited to classical datasets (CIFAR, COCO, AG-News, UrbanSound8K, etc.) and artificially simulated shifts, lacking tests on realistic AI-generated or manipulated content, such as GenImage [1], DeepfakeBench [2], Loki [3].\n2. No quantitative or qualitative comparison is provided against advanced AI-generated content detection methods such as HAMMER (multi-modal detection) [4] or UniFD (image detection) [5], limiting the evaluation‚Äôs competitiveness.\n3. The framework relies heavily on captioners or semantic encoders to extract text representations. Any bias, hallucination, or semantic drift in these upstream models directly impacts calibration reliability, but this sensitivity is not quantitatively analyzed.\n\n[1] Genimage: A million-scale benchmark for detecting ai-generated image, NeurIPS 2023.\n[2] Deepfakebench: A comprehensive benchmark of deepfake detection, NeurIPS 2024.\n[3] Loki: A comprehensive synthetic data detection benchmark using large multimodal models, ICLR 2025.\n[4] Detecting and grounding multi-modal media manipulation, CVPR 2023.\n[5] Towards universal fake image detectors that generalize across generative models, CVPR 2023."}, "questions": {"value": "4. Details such as model backbone choices, parameter counts, captioning prompts, and training hyperparameters are insufficiently documented."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B1niBVEnaI", "forum": "6YTEDVd80T", "replyto": "6YTEDVd80T", "signatures": ["ICLR.cc/2026/Conference/Submission18248/Reviewer_iQsp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18248/Reviewer_iQsp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654370573, "cdate": 1761654370573, "tmdate": 1762927977756, "mdate": 1762927977756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Semantic Calibration, a theoretical and practical framework that redefines the problem of deepfake detection. Instead of distinguishing real vs. synthetic media through low-level artifacts, the authors argue that the true objective is to reduce semantic deception, which is a distributional distortions in the meaning conveyed by media streams.\n\nThe paper proves that artifact-based deepfake detection will eventually fail as generative models approach perfection. It then formalizes deception as the KL divergence between the semantic distribution of observed media and that of real data, and finally proposes a modality-agnostic mitigation strategy: converting media to text via captioning, then filtering samples using rejection sampling based on semantic likelihood ratios derived from two fine-tuned LLMs.\n\nExtensive experiments across text, image, and audio modalities show consistent reductions in semantic deception and strong explainability via token-level saliency maps. The method is transparent and empirically effective in aligning media semantics with real-world distributions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper makes a paradigm shift from authenticity detection to semantic distribution alignment from simple binary deepfake detection. The notion that misinformation should be treated as a semantic calibration problem is both novel and timely. This framing may become foundational for next-generation media integrity systems as artifact cues disappear\n\n2. The authors formally derive performance bounds (Theorem 1) showing the inevitability of deepfake detection failure under improving generators. The theoretical link between deception and achievable detection accuracy is rigorous and motivates the need for semantic methods\n\n3. The proposed rejection sampling in semantic space is mathematically clean and interpretable. Using captioning models and LLM likelihood ratios provides explainability, which is a key advantage over opaque moderation algorithms. The token level saliency analysis demonstrates further interpretability."}, "weaknesses": {"value": "1. Because semantics are extracted via pretrained captioners (e.g., Qwen-Audio), calibration accuracy inherits their biases and failure modes. Although the authors discuss this limitation, no robustness experiments are shown under noisy or adversarial captions.\n\n2. While experiments simulate semantic shifts via reweighted class distributions, these synthetic setups might be simplified compared to real-world misinformation, which is dynamic, adversarial, and context-dependent. Demonstrating the method on real social media streams or misinformation datasets (beyond tabular datasets) may strengthen the claim of practical viability.\n\n3. The approach depends critically on a \"trusted\" dataset to model the real semantic distribution. As the authors acknowledge, this assumption is strong, where biases or incompleteness in the trusted dataset directly propagate to moderation outcomes. The paper does not provide strategies for ensuring fairness or robustness of the dataset itself."}, "questions": {"value": "1. How would semantic calibration adapt to evolving media semantics (e.g., breaking news or new slang)? Would continual retrainingbe required to maintain the semantic distribution?\n\n2. Given a threat model where an attacker deliberately craft content semantically close to the distribution but factually false, can calibration still filter them?\n\n3. How feasible is deploying semantic calibration as a real-time moderation layer given captioning and LLM inference overhead?\n\n4. Could semantic calibration be viewed as a distributional analogue of LLM alignment (e.g., minimizing semantic divergence instead of reward loss)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RyTzFnCbjq", "forum": "6YTEDVd80T", "replyto": "6YTEDVd80T", "signatures": ["ICLR.cc/2026/Conference/Submission18248/Reviewer_8cL7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18248/Reviewer_8cL7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855457416, "cdate": 1761855457416, "tmdate": 1762927977426, "mdate": 1762927977426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their time spent evaluating our work and for the valuable feedback. \n\nReviewers generally agree that the paper presents a timely, conceptually novel shift from binary deepfake detection to semantic distribution alignment, with a rigorous theoretical foundation. They also highlight that the semantic calibration pipeline is simple, transparent, interpretable, and offers strong explainability.\n\nThe reviewers raised two main concerns. First, the method relies on captioning models, whose imperfections may limit robustness across modalities. Second, several of our experiments use controlled, synthetic semantic shifts rather than real-world media streams. In the following we address both concerns.\n\n### Reliance on captioning models\nAs discussed in Sec. 6, we agree that robust captioning is an important open problem. A large body of research focuses on building strong captioning models. We refer reviewers to existing literature for more details on captioning robustness [1, 2], recent methods aimed at improving it [3], and the current state of captioners [4]. \n\nWhile semantic calibration inherits the limitations of captioning models for non-text modalities, it also benefits from ongoing progress in that area. We also note that the reliance on captioning models is a weakness shared by many recent automatic content moderation algorithms (e.g., [5,6]), not only semantic calibration. For instance, OpenAI's latest content moderation API uses a multimodal model `omni-moderation-latest` to classify images as harmful [6]. \n\nFinally, we emphasize that semantic calibration itself is modality-agnostic: it operates purely on text. While we use captioning models to obtain text from non-textual media, our proposed method applies equally to human-generated captions or textual content, where $Z=\\hat{Z}$ and $H(Z|\\hat{Z})=0$ by definition. The reliance on captioning models is therefore not intrinsic to our work.\n\n### Additional experiments on real-world semantic shifts\n\nSeveral reviewers noted that our experiments focused on simplified, artificial shifts. To address this, we added results on realistic shifts by evaluating semantic calibration on datasets of political opinions from different parties [1,2,3]. The objective is to filter a potentially skewed media stream such that the resulting stream matches a desired balanced distribution. For the binary datasets, we balance between conservative and liberal content. For the three-label dataset, we balance among left, center, and right.\n\nWe run three experiments: one on tweets from U.S. senators, one on political podcast segments, and one on bias-annotated political news (see manuscript for references). **In all cases, the objective is purely distributional: we do not attempt to fact-check or validate the content itself.** The goal is to calibrate the semantic distribution of the stream, which is the core distinction from traditional detection.\n\nAs shown below, semantic calibration consistently reduces deception by more than 80% across real semantic shifts, while introducing negligible bias on the baseline. These results have also been added to Table 1 of the manuscript.\n\nSenator Tweets, target dist.: Conservative \\(C): 50%, Liberal (L): 50%\n\n| Case | C (Initial) | L (Initial) | C (Filtered) | L (Filtered) | Reduction |\n|-|-|-|-|-|-|\n| Baseline | 50% | 50% | 50% | 50% | ‚Äî |\n| Mild | 60% | 40% | 52% | 48% | **96.03%**|\n| Moderate | 20% | 80% | 41% | 59% | **91.55%**|\n| Severe | 90% | 10%| 68% | 32%| **81.99%**|\n\nPolitical Podcasts, target dist.: C: 50%, L: 50%\n\n| Case | C (Initial) | L (Initial) | C (Filtered) | L (Filtered) | Reduction |\n|-|-|-|-|-|-|\n| Baseline | 50% | 50% | 49% | 51% | ‚Äî |\n| Mild | 60% | 40% | 50% | 50% | **99.77%** |\n| Moderate | 70% | 30% | 50% | 50% | **99.99%** |\n| Severe | 19% | 81% | 42% | 58% | **93.79%** |\n\nPolitical Bias Corpus, target dist.: Left (L): 33%, Center \\(C): 33%, Right \\(R): 33%\n\n| Case | L (Initial) | C (Initial) | R (Initial) | L (Filtered) | C (Filtered) | R (Filtered) | Reduction |\n|-|-|-|-|-|-|-|-|\n| Baseline | 33% | 33% | 33% | 33% | 33% | 33% | ‚Äî |\n| Mild | 44% | 11% | 44% | 39% | 24% | 36% | **84.85%** |\n| Moderate | 70% | 20% | 10% | 27% | 36% | 36% | **97.12%** |\n| Severe | 75% | 10% | 15% | 49% | 22% | 29% | **84.52%** |\n\n[1] A. Shirnin et al. Analyzing the Robustness of Vision & Language Models, IEEE/ACM Trans. Audio, Speech, Lang. Process., 2024.\n\n[2] C. Schlarmann, M. Hein, On the Adversarial Robustness of Multi-Modal Foundation Models IEEE/CVF ICCV Workshops, 2023.\n\n[3] S. Lee et al., Toward Robust Hyper-Detailed Image Captioning, ICML, 2025.\n\n[4] Cheng, K. et al. Caparena: Benchmarking detailed image captioning, arXiv:2503.12329, 2025.\n\n[5] Wu, Mengyang, et al. ICM-Assistant: Instruction-Tuning Multimodal Large Language Models for Rule-Based Explainable Image Content Moderation. AAAI, 2025.\n\n[6] OpenAI. Upgrading the Moderation API with Our New Multimodal Moderation Model. OpenAI blog, Sept. 26, 2024."}}, "id": "nFuewHtwEJ", "forum": "6YTEDVd80T", "replyto": "6YTEDVd80T", "signatures": ["ICLR.cc/2026/Conference/Submission18248/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18248/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission18248/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763674563331, "cdate": 1763674563331, "tmdate": 1763674563331, "mdate": 1763674563331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}