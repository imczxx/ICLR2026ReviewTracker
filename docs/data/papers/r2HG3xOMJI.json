{"id": "r2HG3xOMJI", "number": 15651, "cdate": 1758253592860, "mdate": 1759897291109, "content": {"title": "mCLM: A Modular Chemical Language Model that Generates Functional and Makeable Molecules", "abstract": "Despite their ability to understand chemical knowledge, large language models (LLMs) remain limited in their capacity to propose novel molecules with desired functions (e.g., drug-like properties). In addition, the molecules that LLMs propose can often be challenging to make, and are almost never compatible with automated synthesis approaches. To better enable the discovery of functional small molecules, LLMs need to learn a new molecular language that is more effective in predicting properties and inherently synced with automated synthesis technology. Current molecule LLMs are limited by representing molecules based on atoms. In this paper, we argue that just like tokenizing texts into meaning-bearing (sub-)word tokens instead of characters, molecules should be tokenized at the level of functional building blocks, i.e., parts of molecules that bring unique functions and serve as effective building blocks for real-world automated laboratory synthesis. This motivates us to propose mCLM, a modular Chemical-Language Model that comprises a bilingual language model that understands both natural language descriptions of functions and molecular blocks. mCLM front-loads synthesizability considerations while improving the predicted functions of molecules in a principled manner. Experiments on 430 FDA-approved drugs showed that mCLM is capable of significantly improving chemical functions critical to determining drug potentials. mCLM, with only 3B parameters, also achieves improvements in synthetic accessibility relative to 7 other leading generative AI methods including GPT-5. When tested on 122 out-of-distribution medicines using only building blocks/tokens that are compatible with automated modular synthesis, mCLM outperforms all baselines in property scores and synthetic accessibility. mCLM can also reason on multiple functions and iteratively self-improve to rescue drug candidates that failed late in clinical trials (“fallen angels”).", "tldr": "We propose mCLM: a bilingual, modular Chemical-Language Model that understands both natural language descriptions of functions and molecular blocks; mCLM front-loads synthesizability while improving the functions of molecules in a principled manner.", "keywords": ["molecule-language multimodality", "language model", "molecule tokenization", "molecule generation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/42b7b0a9cb0e339fc07200fda3d1c578b199045a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes mCLM, a modular chemical-language model that helps the translation between natural language description and molecular blocks. Different from previous chemical LLMs using SMILES or SELFIES as their inputs, mCLM learns a new molecular language that is more effective in property prediction and better in synthesis. Experimental results show that with 3B parameters, mCLM can achieve much better synthetic accessibility and property scores than LLMs like GPT-5."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of proposing a new molecular language and tokenizing molecules into substructures is novel and helps LLMs better focus on the structural patterns of molecules that will affect the molecular properties.\n2. The performance of mCLM is significantly much better than previous baselines. With only 3B parameters, mCLM is also much more efficient than LLMs like GPT-5.\n3. This paper is overall well-written and easy to follow."}, "weaknesses": {"value": "1. The proposed molecular vocabulary lacks sufficient details regarding its implementation. Furthermore, it is worth investigating whether the decomposition of molecules into these substructures could potentially compromise their structural integrity.\n2. The molecular tokenization in mCLM utilizes GNNs to generate embeddings for the building blocks. However, the study does not explore or compare this approach against more advanced or alternative methods, such as VQ-VAE, which could potentially offer superior performance.\n3. The evaluation of mCLM is confined to the Qwen-3B base model. To assess the scalability and generalizability of the proposed method, it would be beneficial to test it across different models of varying sizes and architectures.\n4.  The experimental section primarily benchmarks against other LLMs. While some of the chemical LLMs used as baselines may be outdated, the evaluation notably omits comparisons with established GNN baselines, which are crucial for a comprehensive assessment.\n5. The proposed molecular property optimization task could be significantly strengthened by incorporating more relevant and diverse datasets into the evaluation."}, "questions": {"value": "Please address my concerns in the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KNkb7oVxrM", "forum": "r2HG3xOMJI", "replyto": "r2HG3xOMJI", "signatures": ["ICLR.cc/2026/Conference/Submission15651/Reviewer_XT6a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15651/Reviewer_XT6a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548210314, "cdate": 1761548210314, "tmdate": 1762925909067, "mdate": 1762925909067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes mCLM, a framework designed to generate small molecules that are both functional and synthesizable. The model introduces a multimodal framework that combines a GNN based representation of molecular building blocks with natural language understanding of molecular functions. The tokenization is performed at the building block level rather than atom-level, aiming to align molecular representation with automated synthesis rules."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The construction of an LLM framework that jointly considers synthesizability and functionality represents an important step toward practical and interpretable molecular generation.\n- The integration of GNN representations with natural language embeddings for modular chemical reasoning is technically novel and well-motivated.\n- The figures are clean, well-structured, and enhance the overall readability and understanding of the method."}, "weaknesses": {"value": "I would consider raising the score if the following weaknesses are resolved.\n- **Comparison to fragment-aware baselines**: While the paper includes comparisons to recent general-purpose and domain-specific molecule LLMs, it omits fragment- or group-aware baselines such as SAFE [1], GROUPSELFIES [2], or Reasyn [3]. Even acknowledging that Reasyn is concurrent, such comparisons (especially against Transformer-based models with other representations, as mCLM itself employs a Transformer backbone) would strengthen the evaluation and effectiveness.\n- **Lack of clarity on reasoning knowledge acquisition**: The process for defining and annotating the molecular functions of building blocks (Figure 2) is insufficiently described. How are functional roles such as “Hinge binder, cell activity promoter” obtained or validated? If a user seeks to optimize a given property, does this require (1) manual annotation of new functions, (2) training of a proxy model, and (3) full re-training of the multimodal LLM? The pipeline for expanding functional knowledge is unclear for me.\n- **Unclear link between function-infused vocabulary and molecular functionality**: Section 2 describes the vocabulary as *function-infused* and *synthesis-friendly*. While the synthesis aspect is well justified, the connection between the decomposed building blocks and their *functional meaning* remains ambiguous, even after reading Section 3.3. It is not evident how these building blocks encode or correlate with molecular functions, since molecular function typically arises from overall structure and context, not from isolated building blocks.\n- **Minor correction**: “thanks to recent” in line 161 is aligned with nothing afterwards.\n\n[1] Noutahi, E., et al. Gotta be SAFE: a new framework for molecular design. Digital Discovery, 3(4), 796-804.\n\n[2] Cheng, A. H., et al. Group SELFIES: a robust fragment-based molecular string representation. Digital Discovery, 2(3), 748-758.\n\n[3] Lee, S., et al. Rethinking Molecule Synthesizability with Chain-of-Reaction. arXiv 2025."}, "questions": {"value": "- **Reason for free from function group conflicts**: The authors claim that resulting building blocks are *free from functional group conflicts* (lines 263–264). How is this ensured when building blocks from different tokenizers or different molecules are mixed? Wouldn’t incompatible functional groups potentially lead to synthesis failures?\n- What could be the reason that the proposed mCLM show relatively weaker performance on HIA and PGP despite superior results on others?\n- **Proxy model reliability**: Since experimental results rely heavily on proxy models for property prediction, how accurate and robust are these models across different molecular classes?\n- **Ablation study**: Could the authors include an ablation comparing the full GNN-based encoding with a baseline using only textual (SMILES or SELFIES) representations? This would clarify the contribution of the graph modality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "z2aRlCyv2I", "forum": "r2HG3xOMJI", "replyto": "r2HG3xOMJI", "signatures": ["ICLR.cc/2026/Conference/Submission15651/Reviewer_81Cp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15651/Reviewer_81Cp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762151998670, "cdate": 1762151998670, "tmdate": 1762925908694, "mdate": 1762925908694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “mCLM: A Modular Chemical Language Model That Generates Functional and Makeable Molecules” introduces mCLM, a new type of chemical language model designed to generate small molecules that are both functionally effective and synthetically feasible. Traditional large language models can understand chemical information, but they often struggle to design molecules that can actually be synthesized in the lab. mCLM addresses this challenge by shifting from atom-level representations (like SMILES strings) to a modular representation, where molecules are described as combinations of chemically meaningful, synthesis-ready building blocks. This allows digital molecular generation to directly correspond to real-world automated synthesis.\n\nmCLM functions as a bilingual model—it understands both natural language descriptions and molecular structures. It uses graph neural networks (GNNs) to encode molecular modules and combines these with text embeddings in a Transformer-based architecture, enabling it to “code-switch” between chemistry and natural language. The model is trained on paired datasets that link chemical properties, synthesis reactions, and textual descriptions. This training allows mCLM not only to generate molecules with desired biological or physical functions but also to ensure that these molecules are makeable through automated synthesis pipelines.\n\nIn experimental evaluations using 430 FDA-approved drugs and 122 out-of-distribution compounds, mCLM demonstrated substantial improvements in key pharmacological properties—including absorption, distribution, metabolism, excretion, and toxicity (ADMET)—while maintaining or improving synthetic accessibility. It outperformed leading AI systems such as GPT-5, Claude 3.5, and Gemini 2.5-Flash in both functional property optimization and synthetic feasibility. Furthermore, mCLM proved capable of “rescuing” failed drug candidates—like Evobrutinib and TNG348—by suggesting minimal structural modifications that reduced toxicity and improved drug-like behavior."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Conceptually Innovative but Incremental in Execution\nThe idea of representing molecules through modular, synthesis-ready building blocks rather than atom-level encoding is conceptually novel and offers a creative bridge between digital design and physical synthesis. This modular approach reflects an original perspective on chemical language modeling. However, the implementation mainly extends existing ideas from reaction-aware and retrosynthesis-based models, making the innovation more incremental than transformative.\n\n- Solid Technical Foundation but Limited Validation\nThe paper demonstrates technical competence in integrating graph neural networks with Transformer architectures and applying them to chemical structure–language fusion. The system design is coherent, and the methodology is explained at a reasonable level of detail. However, the experimental quality is weakened by the absence of real-world synthesis or bioassay validation, and the evaluation remains largely computational and self-referential, lowering the overall scientific robustness.\n\n- Generally Well-Written but Overclaims Results\nThe manuscript is clear and logically structured, with well-organized figures and a coherent narrative linking modular chemistry to AI language modeling. Nevertheless, some claims—especially about outperforming large general-purpose models and enabling autonomous molecular discovery—are exaggerated relative to the presented evidence. The lack of sufficient methodological transparency (e.g., ablations, dataset details) also detracts from clarity and reproducibility.\n\n- Potentially High Impact but Not Yet Realized\nIf validated experimentally, mCLM could have significant implications for automated drug discovery and robotic chemistry. The integration of function-aware reasoning and synthesis feasibility into a unified framework is a meaningful direction for the field. Yet, given the limited empirical support and narrow demonstration scope, its real-world significance remains largely aspirational rather than achieved."}, "weaknesses": {"value": "- Limited Generalization and Chemical Creativity\nThe modular tokenization relies on a fixed library of known reaction building blocks and predefined synthesis rules. While this ensures synthetic feasibility, it severely restricts the model’s ability to explore novel chemical spaces or generate fundamentally new scaffolds beyond existing reaction types. Thus, the model’s creativity is constrained by human-curated chemistry knowledge.\n\n- Lack of Experimental Validation and limited ablation and interpretability analysis.\nThe evaluation is almost entirely computational, based on predicted ADMET properties and synthesis scores rather than actual laboratory synthesis or biological testing. Without experimental confirmation, the model’s claimed functional and pharmacological improvements remain speculative and unverified in practice. It does not clearly isolate the contribution of its modular tokenization, GNN encoder, or reasoning loop. The internal decision-making of mCLM—how it balances function optimization and synthesizability—remains a black box, reducing its scientific transparency and reproducibility.\n\n- Weak Comparative and Analytical Rigor\nThe baselines used (e.g., GPT-5, Claude 3.5, Gemini 2.5) are general-purpose models, making comparisons less meaningful. The paper omits direct evaluation against specialized molecular generative models (like ChemBERTa or retrosynthesis-aware VAEs), and lacks ablation studies to show the unique contribution of each module (tokenizer, GNN, reasoning loop).\n\n- Overstated Multimodal Integration and Transparency Issues\nThe bilingual natural language–molecule framework is conceptually attractive but only superficially demonstrated. The model’s interpretability and internal reasoning are not clearly explained, leaving it as a black box with limited insight into how function and synthesizability are balanced."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CSnBQFRsyU", "forum": "r2HG3xOMJI", "replyto": "r2HG3xOMJI", "signatures": ["ICLR.cc/2026/Conference/Submission15651/Reviewer_XTAL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15651/Reviewer_XTAL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177320909, "cdate": 1762177320909, "tmdate": 1762925908405, "mdate": 1762925908405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents mCLM, a modular chemical language model designed for universal molecular understanding across textual, graphical, and structural modalities. The framework integrates multiple specialized modules—each trained on different molecular representations (e.g., SMILES, graphs, spectra)—and fuses them through a shared latent alignment layer. The authors emphasize scalability, interpretability, and extensibility, demonstrating competitive results on property prediction, reaction reasoning, and cross-modal translation benchmarks such as MolLangBench and MoleculeNet."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "There is novelty in this work. Specifically, there is clear architectural separation between domain-specific encoders and a unified fusion backbone which promotes flexibility and domain transfer.\n\nExperiment results are also promising. This work outperforms strong baselines (MolX, ChemBERTa, GraphMVP) on multimodal reasoning tasks, particularly in low-data and cross-domain settings.\n\nFigure 4 is particularly useful as it shows module-wise attribution analyses for how modality-specific knowledge contributes to chemical reasoning."}, "weaknesses": {"value": "There is however limited novelty at the core LLM level. While modularization is effective, the language model itself is adapted rather than fundamentally redesigned for chemistry.\n\nThere is also lack of validation of the practicality of this approach, say on real world sparse datasets. The evaluation focuses primarily on benchmark datasets, with minimal discussion of noisy experimental spectra or reaction data.\n\nFurther, the computational cost of the work seems infeasible. Training multiple modality-specific experts and fusion layers may hinder accessibility for smaller research groups.\n\nThere are also couple recent relevant research works that have not been referenced which the authors need to cite to improve the comprehensiveness of the related work section:\n\n- Le, Khiem, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, and Nitesh V. Chawla. “MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension.” Proceedings of the 2025 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, AC M, 2025. \n\n- Ju, Jiaxin; Yizhen Zheng; Huan Yee Koh; Shirui Pan. “Uni-MRL: Unified MultiModal Molecular Representation Learning with Large Language Models and Graph Neural Networks.” Advances in Knowledge Discovery and Data Mining (PAKDD 2025), Lecture Notes in Computer Science, vol. 15874, Springer, 2025, pp. 275-287."}, "questions": {"value": "How does mCLM handle conflicts between representations (e.g., inconsistent SMILES vs. graph encodings)?\n\nCould the modular framework support plug-and-play extensions for new data types (e.g., protein–ligand complexes)?\n\nHow stable is the latent alignment layer during joint fine-tuning across highly imbalanced modalities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ku6VaSFORh", "forum": "r2HG3xOMJI", "replyto": "r2HG3xOMJI", "signatures": ["ICLR.cc/2026/Conference/Submission15651/Reviewer_k9Sm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15651/Reviewer_k9Sm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762218429808, "cdate": 1762218429808, "tmdate": 1762925907836, "mdate": 1762925907836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}