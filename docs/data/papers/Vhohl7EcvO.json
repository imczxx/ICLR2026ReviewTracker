{"id": "Vhohl7EcvO", "number": 9602, "cdate": 1758129619620, "mdate": 1763323331488, "content": {"title": "Noise Stability of Transformer Models", "abstract": "Understanding simplicity biases in deep learning offers a promising path toward developing reliable AI. A common metric for this, inspired by Boolean function analysis, is average sensitivity, which captures a model's robustness to single-token perturbations. We argue that average sensitivity has two key limitations: it lacks a natural generalization to real-valued domains and fails to explain the \"junta-like\" input dependence we empirically observe in modern LLMs. To address these limitations, we propose *noise stability* as a more comprehensive simplicity metric. Noise stability expresses a model's robustness to correlated noise applied to *all* input coordinates simultaneously. We provide a theoretical analysis of noise stability for single-layer attention and ReLU MLP layers and tackle the multi-layer propagation problem with a covariance interval propagation approach. Building on this theory, we develop a practical *noise stability regularization* method. Experiments on algorithmic and next-token-prediction tasks show that our regularizer consistently catalyzes grokking and accelerates training by approximately $35$\\% and $75$\\% respectively. Our results establish noise stability as a powerful tool for understanding and improving modern Transformers.", "tldr": "We introduce noise stability in Transformer models as an alternative proxy for explaining simplicity bias and propose a corresponding regularization method that we observe accelerates grokking.", "keywords": ["transformers", "simplicity bias", "noise stability", "regularization methods", "spectral concentration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/077976b821b037be198b3178180ec78259681267.pdf", "supplementary_material": "/attachment/d02b16b083686dffe7463d578419df9fc97cfa49.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a method to investigate the simplicity bias in neural networks. Simplicity bias is the tendency of neural networks of learning simple functions to fit the data: as simpler functions are (i) more interpretable and (ii) more robust, assessing simplicity is important to study model reliability. \nA current trend of research frames the problem of simplicity in terms of spectral decomposition. The underlying idea is that simple functions should show spectral concentration around low-order polynomials. Drawing from Boolean functional analysis, it is possible to study the impact of a single token by \"swapping\" its sign and contrasting the average output change with the total variance. The authors criticise the approach. They claim that neural networks employ functions defined on real numbers -and not on binary coordinates- and they criticise the idea of considering one token at the time as this fails to capture their collective behaviour: (i) the \"junta\"-like concentration, i.e., the sparsity of the influential tokens; (ii) the positional bias; (iii) the non-zero influence of all tokens. \nTo address their concern, they propose to extend the concept of noise stability from Boolean analysis - which is the study of the effect of correlated noise applied to all inputs simultaneously. First, they define the concept of stability as the correlation between the original input and a partially perturbed input - corrupted with Gaussian noise. Then, they express this measure employing the spectral decomposition of the function in terms of its Hermite-Fourier coefficients. Finally, they study (i) a simplified RELU MLP and (ii) a simplified Transformer, finding that the first dampens stability, whereas the latter maintains it. \nTo test the impact of penalising with respect to spectral concentration, they propose a stability regularisation: they train transformers on two artificial tasks that exhibit grokking, i.e., a sharp decrease in the loss function value after an initial over-fitting phase, noting that regularisation cuts the number of iterations requires to attain it."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality:  The authors propose an approach to the study of neural networks that is well-grounded in theory and takes into account the collective behaviour of the tokens in LLMs. They derive analytical solutions for typified examples and introduce a regularisation mechanism. \n Quality:   the derivations are sound; experiments accompany all the claims; a numerical example works as a case study. \n  Clarity: The work is clear.  well-organised and easy to read. The Appendix is organised well and it is easy to follow the derivation of the derivation of the Ornstein-Uhlenbeck Operator. Some relevant entries in the bibliography are incomplete (e.g., Nathan Keller. Elchanan Mossel. Arnab Sen. \"Geometric influences.\" Ann. Probab. 40 (3) 1135 - 1166, May 2012. https://doi.org/10.1214/11-AOP643). The labels of the plots are too tiny and difficult to read. \n Significance The author propose a new method to investigate the simplicity bias that considers all tokens at the same time. Given the importance of collective token dynamics in Transformer, this work goes in the right direction."}, "weaknesses": {"value": "The model criticizes the use of influence as a proxy of sensitivity and propose noise stability as a new method. One of the reasons of their criticism is that the upper bound of the former method are too loose with respect to the latter when estimating spectral concentration. As evidence, they report the values for four different models in Table 1. However, it is not entirely clear if this is the only advantage of their new method. In other words, it is not entirely clear if using influence or, better still, geometric influence it is not possible to gain the insights they obtain using noise stability. The authors obtain interesting results for MLPs and Transformers, but it is not  clear why the results differ and what might be the reason behind this. Similarly, while the example of Section 6 shows that there seems to a be link between simplicity bias and spectral concentration, it is not immediately clear if noise stability captures it better than influence, if it might be possible to use it too, and if there is a way to experimentally detect the issues they already showcase in Figure 1 in this training setting."}, "questions": {"value": "1. Line 107/108. Are you sure the dimensions of this equation are correct? \n    2. Table 1. What would happen if we instead considered the geometric influence here? \n    3. Line 279. Maybe a typo: you see \"(see Section 5.2)\" in Section 5.2. \n    4. Figure 2. This is a bit hard to read: the labels are very small. \n    5. Section 5. Can you comment on the results? Why are the different between Transformers and MLPs? Or how does it show in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jNvzX6if9B", "forum": "Vhohl7EcvO", "replyto": "Vhohl7EcvO", "signatures": ["ICLR.cc/2026/Conference/Submission9602/Reviewer_6UoK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9602/Reviewer_6UoK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723964325, "cdate": 1761723964325, "tmdate": 1762921145869, "mdate": 1762921145869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes noise stability as a measure of simplicity of Transformers.\nUnlike average sensitivity, which captures local perturbations, noise stability measures robustness to correlated, global perturbations across the input sequence. Building on this idea, the authors introduce a noise-stability regularizer and show that it improves training and generalization on algorithmic tasks exhibiting grokking dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The mathematical formulation of noise stability through the Ornstein–Uhlenbeck operator is well presented. The concept of analyzing robustness under global, correlated perturbations rather than local ones is intuitive and sound.\n2. The introduction of a differentiable noise-stability regularizer is a concrete and practical contribution.\n3. The choice of algorithmic tasks is appropriate for studying grokking phenomena. The experiments show correlations between noise stability, validation loss, and generalization behavior."}, "weaknesses": {"value": "1. The paper motivates the use of noise stability over average sensitivity based on the observed “junta-like” dependence in Transformers, arguing that noise stability captures global correlated perturbations rather than single-coordinate ones. However, the connection between robustness to global perturbations and dependence on a few input variables is not clearly established. It remains unclear why capturing correlated noise should correspond to fewer influential tokens.\n2. The authors claim that noise stability provides tighter spectral tail bounds than average sensitivity, but no theoretical comparison or proof is given. The empirical evidence in Table 1 lacks sufficient methodological detail for verification—for example, the meaning of “percentage of weight in degrees ≥ 15” is not explained.\n3. The claim that “average sensitivity is too loose to explain the behavior of modern Transformers” is not convincingly supported by experiments. The paper would be much stronger with a direct comparison of average sensitivity and noise stability as regularizers.\n4. The paper also does not compare against other regularizers known to affect grokking dynamics (e.g., weight decay, $L_1$ norm [1, 2]). The reported grokking acceleration appears to be based on a limited number of runs and lacks confidence intervals.\n5. The proposed regularizer perturbs discrete token inputs via random replacements, whereas the theoretical formulation of noise stability is defined over Gaussian perturbations. The paper would benefit from clarifying how these two settings relate. \n6. Section 5’s analysis of noise stability in Transformers does not meaningfully yield actionable insights for Transformer training or regularizer design. \n\nMinor: \n- The term “junta-like” appears in the abstract before being defined.\n- In Line 397, the authors mention “three key properties” but list only two."}, "questions": {"value": "1. How do the positional-bias and sensitivity patterns in Figure 1 concretely demonstrate the limitation of average sensitivity, and in what way does noise stability resolve it?\n2. Can the authors include a controlled comparison among (i) average-sensitivity regularization, (ii) noise-stability regularization, (iii) weight decay, and (iv) no regularization, to demonstrate that noise stability improves training dynamics beyond heuristic smoothing?\n\n3. What is the additional computational overhead of computing and back-propagating the noise-stability term compared to average sensitivity?\n\n4. In Appendix J.2, the authors show that noise stability correlates with validation loss. Can they further test whether noise stability decreases before validation loss changes, serving as an early indicator of grokking as observed in [3].\n\n[1] Ziming Liu, Eric J Michaud, and Max Tegmark. Omnigrok: Grokking Beyond Algorithmic Data. ICLR 2023.\n\n[2] Kaifeng Lyu, Jikai Jin, Zhiyuan Li, Simon S. Du, Jason D. Lee, and Wei Hu. Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking. ICLR 2024.\n\n[3] Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang, and Vatsal Sharan. Transformers Learn Low Sensitivity Functions: Investigations and Implications. ICLR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nrv2MFIKTh", "forum": "Vhohl7EcvO", "replyto": "Vhohl7EcvO", "signatures": ["ICLR.cc/2026/Conference/Submission9602/Reviewer_QkLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9602/Reviewer_QkLJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752651900, "cdate": 1761752651900, "tmdate": 1762921145611, "mdate": 1762921145611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes noise stability as an alternative to average sensitivity for explaining spectral concentration in Transformers. The authors argue that average sensitivity has theoretical limitations in extending to real-valued domains and empirically fails to capture the \"junta-like\" behavior observed in modern LLMs. They provide theoretical analysis of noise stability for single-layer attention and ReLU MLP components, develop a stability interval propagation technique for multi-layer networks, and introduce a noise stability regularization method that accelerates grokking by approximately 35% on algorithmic tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Substantive theoretical contributions.**\nThe paper gives an exact, closed-form expression for the noise stability of a ReLU layer (Theorem 5.1 / Appendix E) and analyzes single-layer attention under different structural regimes (identity/low-rank and unstructured; Theorems 5.2 and 5.3). It also studies multi-layer propagation, showing weak dampening for deep ReLU MLPs and contrasting behavior for Transformers.\n\n**Thorough and self-contained presentation.**\nThe work includes background on Boolean function analysis and the Ornstein–Uhlenbeck semigroup/Hermite expansion, connecting noise stability to spectral concentration and supporting later results with detailed appendices."}, "weaknesses": {"value": "**Limited comparison with prior works.**\nSection 3 argues that geometric influence (GI) is a more robust alternative to average sensitivity in continuous domains, but then pivots to advocating noise stability as the primary metric without a direct empirical or theoretical head-to-head comparison (e.g., GI vs. noise stability on the same tasks/models). Related work cites Li & Mossel (2025) for noise sensitivity propagation but does not clearly mention what is new beyond the inspiration or compare results. Adding targeted comparisons would sharpen the contribution.\n\n**The regularizer mechanism requires more intuition and ablation.**\nThe regularizer (Def. 6.1) is clearly specified, but the paper offers limited intuition for why this particular form steers learning toward “simpler” functions beyond the variance-level view. Hyperparameter sweeps for $\\gamma$ and $\\rho$ are also missing.\n\n**Narrow experimental scope.**\nAll experiments are on synthetic algorithmic tasks (noisy k-sparse parity, modular addition), and the grokking speed-up is reported only there. It’s unclear whether the regularizer helps on realistic NLP or larger-scale settings.\n\n**Minor presentation issues.**\nThere’s a typo (“doamins”) in Section 4, and some figures/tables could use more consistent visual styling and larger font sizes for readability."}, "questions": {"value": "1. What is the intended use case for S=0 (encouraging instability)? Are there regimes (e.g., parity-like targets) where anti-stability regularization could help avoid degenerate shortcuts? The paper only evaluates S=1.\n\n2. Appendix J.2 analyzes stability evolution *without* regularization. Showing the same trajectories *with* the regularizer would more directly support the “catalyst” claim and illuminate the mechanism.\n\n3. What is the computational cost of noise stability measure and training with noise stability regularization relative to average sensitivity or other alternatives? (E.g., the load of extra forward pass on a correlated Y and scaling properties) A brief cost analysis would be useful for practice, but I didn’t see this discussed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "l3NfZjAPnK", "forum": "Vhohl7EcvO", "replyto": "Vhohl7EcvO", "signatures": ["ICLR.cc/2026/Conference/Submission9602/Reviewer_DkBU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9602/Reviewer_DkBU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905600782, "cdate": 1761905600782, "tmdate": 1762921145229, "mdate": 1762921145229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies noise stability as a quantitative lens on simplicity bias and spectral concentration in Transformers. It formalizes Gaussian noise stability using the Ornstein Uhlenbeck semigroup and Hermite expansions, then proves new results for core components: a closed form for the stability of a ReLU MLP layer, and stability characterizations for attention under identity, low rank, and unstructured regimes. It extends the analysis to depth via a recurrence for MLPs that predicts weak dampening and introduces stability interval propagation to bound stability through deep Transformers. Empirically, the authors propose a noise stability regularizer that is differentiable and data dependent. On grokking style tasks such as modular addition and noisy k sparse parity, the regularizer speeds up emergence by about thirty five percent. They also compare stability based spectral tail bounds with sensitivity based bounds on several pretrained models, finding better agreement with measured concentration when using stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Stability analysis of attention with identity, low rank, and random regimes is new and insightful. The appearance of a permutation consistency factor $s(\\rho)$ in the unstructured case is a neat and interpretable result. Theoretical results are derived from a solid OU semigroup foundation with explicit ReLU formulas and careful discussion of approximations. Interval propagation addresses the realistic case where exact recurrences are brittle. The regularizer is minimal and differentiable, with a clear sampling scheme for $Y$. The experiments control for seeds and hyperparameters and demonstrate a consistent thirty five percent speedup on two canonical grokking tasks. Finally, stability based tail bounds better match measured concentration across common pretrained models than sensitivity based bounds. This supports the relevance of the stability viewpoint for understanding learned spectra in practice."}, "weaknesses": {"value": "- Assumption gaps for deep Transformers. The multi layer recurrence assumes fixed distributions and idealized conditions. The authors later observe full dampening in practice when $\\gamma < 1$, which highlights a gap between proxy analysis and actual behavior. More systematic experiments that probe when interval bounds are tight would strengthen claims.\n\n- Limited task scope. The regularizer is only evaluated on synthetic algorithmic tasks. Evidence that it helps on language modeling or fine tuning tasks is indirect through tail bound comparisons. A small scale LM experiment that tracks validation perplexity, stability, and tail mass together would improve external validity.\n\n- Sensitivity to $\\rho$ and $\\gamma$. The analysis and the regularizer hinge on these hyperparameters. The paper sets them per task but does not map performance and stability as a function of $\\rho$ and the regularization strength $\\gamma$ across a grid. This makes practical tuning less clear."}, "questions": {"value": "1. Role of  $s(\\rho)$ beyond initialization. The unstructured attention result introduces a permutation consistency factor $s(\\rho)$ that is derived under random weights. How does $s(\\rho)$ evolve during training when attention becomes more structured. Can the authors estimate or bound $s(\\rho)$ on trained layers.\n\n2. Tightness of interval bounds. The interval propagation method provides upper and lower bounds. On realistic trained networks, how tight are these intervals per layer. A plot of measured stability with interval envelopes would help assess utility.\n\n3. The regularizer allows both orientations. Have you tried S=0 to discourage stability as a stress test on overfitting or memorization. Does decreasing stability hurt grokking or robustness.\n\n4. From synthetic to text. Can you run a small language modeling experiment that logs stability, spectral tail, and validation perplexity every few thousand steps. Even a 100M parameter model on WikiText or FineWeb subsets would help connect theory to practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dd8PcpnINA", "forum": "Vhohl7EcvO", "replyto": "Vhohl7EcvO", "signatures": ["ICLR.cc/2026/Conference/Submission9602/Reviewer_vNH8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9602/Reviewer_vNH8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946198500, "cdate": 1761946198500, "tmdate": 1762921144670, "mdate": 1762921144670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}