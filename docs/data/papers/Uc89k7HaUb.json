{"id": "Uc89k7HaUb", "number": 17306, "cdate": 1758274461496, "mdate": 1759897183332, "content": {"title": "S2R-HDR: A Large-Scale Rendered Dataset for HDR Fusion", "abstract": "The generalization of learning-based high dynamic range (HDR) fusion is often limited by the availability of training data, as collecting large-scale HDR images from dynamic scenes is both costly and technically challenging. To address these challenges, we propose S2R-HDR, the first large-scale high-quality synthetic dataset for HDR fusion, with 24,000 HDR samples. Using Unreal Engine 5, we design a diverse set of realistic HDR scenes that encompass various dynamic elements, motion types, high dynamic range scenes, and lighting. Additionally, we develop an efficient rendering pipeline to generate realistic HDR images. To further mitigate the domain gap between synthetic and real-world data, we introduce S2R-Adapter, a domain adaptation designed to bridge this gap and enhance the generalization ability of models. Experimental results on real-world datasets demonstrate that our approach achieves state-of-the-art HDR fusion performance.", "tldr": "", "keywords": ["HDR Dataset", "HDR Fusion", "Domain Adaption"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d9eb94a30bbb1a519a8eaace1a2e166d1dcd0092.pdf", "supplementary_material": "/attachment/ff37fe0f55d515f26faa68cab177d673bf465e43.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a method for generating image data in the field of HDR fusion using the Unreal Engine 5. To address the domain gap between synthetic and real data, it also introduces a domain adaptation approach applicable to both labeled and unlabeled datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The arguments in the paper are well-grounded, and the writing is logically rigorous and easy to follow.\n\n\n2. The proposed dataset offers a richer variety of HDR scenes, substantiated by partial visualizations and extensive quantitative comparisons. When combined with the strongly generalized domain adaptation method, it further enhances the performance in the field of HDR fusion."}, "weaknesses": {"value": "The paper does not discuss the limitations of the proposed method. Is it effective in all scenarios? Please specify any potential constraints and provide examples of failure cases, which would help readers understand the method's boundaries."}, "questions": {"value": "1. The authors emphasize the efficiency of their method. Could you provide the additional computational cost (for both training and inference) introduced by the domain adaptor? Specifically, during inference, if the method necessarily requires additional test-time-training from the adaptor for every new scenarios (e.g., day, night, dusk), the claimed efficiency might be compromised. Clarifying this practical overhead is crucial.\n\n\n2. The LDR data in the Unreal Engine 5 synthetic dataset is generated via data augmentation, but the process (e.g., adding noise) is not detailed. For instance, the LDR examples in Figure 3 show no noise variation across exposures. This lack of realistic degradation might be a key reason for the poor generalization when trained solely on the proposed dataset. Enhancing the data synthesis pipeline with a more realistic degradation model could significantly boost the dataset's generalization capability and contribution.\n\n\n3. Abundant HDR data from professional film-grade equipment can be acquired on the internet. What is the specific advantage of the dataset proposed in this work compared to these readily available sources? Please elaborate on its unique value."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bXQ3lrgknP", "forum": "Uc89k7HaUb", "replyto": "Uc89k7HaUb", "signatures": ["ICLR.cc/2026/Conference/Submission17306/Reviewer_cNjM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17306/Reviewer_cNjM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761490264335, "cdate": 1761490264335, "tmdate": 1762927242451, "mdate": 1762927242451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The core contribution of this paper is addressing the generalization challenge in HDR fusion models caused by the scarcity of real-world data. By introducing the first large-scale synthetic dataset (S2R-HDR) containing 24,000 high-quality HDR samples rendered using Unreal Engine 5, which covers diverse dynamic scenes and lighting conditions, along with an innovative domain adaptation method (S2R-Adapter) that balances synthetic and real-world data knowledge through a dual-branch design, the model achieves state-of-the-art performance on real-world datasets. Through comprehensive experiments on real-world datasets, the combined use of S2R-HDR and S2R-Adapter enables models to achieve state-of-the-art performance, significantly reducing motion artifacts like ghosting and enhancing the restoration of details in extreme illumination conditions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This study constructs a novel dataset for multi-exposure HDR fusion tasks, which contributes to the advancement of this research field. This dataset comprises diverse types of image data captured under various environmental conditions and exposure levels, demonstrating considerable diversity. Additionally, to address the discrepancy between synthetic and real data, this paper propose the S2R-Adapter as an effective solution for mitigating the domain gap between synthetic and real-world data. The two branches of this module are particularly noteworthy, as the shared branch and the transfer branch can effectively reduce the domain gap.This paper conducted both supervised and unsupervised experiments, and the experiments appear to be sufficiently comprehensive."}, "weaknesses": {"value": "In the introduction, the author states that the dataset proposed by Barua et al. is designed for single-image LDR to HDR conversion tasks. However, this assertion appears to be inaccurate, as the dataset actually includes corresponding multi-exposure LDR images. In fact, it encompasses variations in color hues, saturation, exposure, and contrast levels. It is worth noting that this dataset is exceptionally large, containing 40K HDR image. Since this dataset is based on GTA, it inherently includes features such as vehicles, humans, and varying lighting conditions. The author only describes the practices of shared branches and transfer branches, but throughout the entire paper, I cannot fully comprehend how these two branches achieve their intended functions. What is the theoretical basis for this? Or is it solely validated through experimental results? In addition, it appears that the author has omitted the UE5 dataset construction flowchart."}, "questions": {"value": "Q1: Is the first claim accurate? The dataset compiled by Barua et al. contains 40,000 HDR images, resulting in a total dataset size of 1M. The volume of this data significantly exceeds that of your dataset.\nQ2:Why is the dataset from Barua et al. unsuitable for multi-exposure HDR image reconstruction? The author should revise the introduction section regarding the description of the Barua et al. dataset, preferably providing a detailed comparison between this dataset and the proposed dataset in the paper. The dataset from Barua et al. includes multi-exposure data.\nQ3: The authors need to elaborate on the specific rendering process using UE5, including scene creation and virtual camera capture. Could a corresponding flowchart be provided? If acceptable, could the process script also be included?\nQ4: Why can the shared branch be utilized to preserve knowledge for rendering data? Why can the transfer branch learn domain-specific knowledge? The authors do not appear to provide detailed theoretical proofs, offering only some explanations in the experimental section. I could not find the basis for the corresponding effects of these two modules in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9YIFQN9sJv", "forum": "Uc89k7HaUb", "replyto": "Uc89k7HaUb", "signatures": ["ICLR.cc/2026/Conference/Submission17306/Reviewer_Ug55"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17306/Reviewer_Ug55"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638086227, "cdate": 1761638086227, "tmdate": 1762927242129, "mdate": 1762927242129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper mainly proposes a large-scale synthetic HDR dataset and a post-processing module for domain adaptation between the synthetic and real-world domains. Specifically, a large-scale HDR dataset is essential for training powerful HDR models, especially in the era of large-scale generative models. Compared with existing datasets, the proposed HDR dataset leverages Unreal Engine 5 to synthesize data, resulting in 24,000 high-quality HDR frames featuring diverse motion types, lighting conditions, and both indoor and outdoor scenes. Current HDR models trained on small real datasets often struggle with large-scale motion and diverse lighting conditions, whereas models trained on the proposed large-scale dataset achieve significantly better performance.\n\nRegarding the proposed post-processing module, called S2R-Adapter, it aims to reduce the domain gap between synthetic and real-world data. This module can be used in two modes: 1). fine-tuning mode, which requires labeled datasets for supervised training; 2). self-supervised mode (or test-time mode), which can be applied to unlabeled data without ground-truth images.\n\nExperimental results demonstrate that the proposed large-scale synthetic HDR dataset effectively improves the performance of existing HDR fusion models. They also show that the proposed dataset and adapter achieve better generalization across different datasets.\n\nOverall, the quality of this paper is above the borderline and it is likely to be accepted. However, several questions and concerns remain are  listed in below sessions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this paper is very clear, which aims to tackles the dataset bottleneck in HDE imaging based on multiple-image fusion.\n2.  It also point out the challenging issue regarding to domain gap between the synthesized and real-world data, and propose S2R-adapter to handle this issue.\n3. The paper demonstrates comprehensive experiment results, which are promising."}, "weaknesses": {"value": "1. One of the major contributions of this paper is the proposed large-scale dataset, but the paper does not clearly describe how this dataset is created. For example, it only mentions that Unreal Engine 5 is used with varying tone-mapping and gamma-correction parameters to render different data, but the explicit pipeline for dataset generation is missing. Why does this synthetic pipeline effectively produce data with diverse lighting conditions and object motions? Is this dataset curation pipeline reproducible?\n\n2. In the S2R-Adapter framework, the paper shows that it uses a diffusion model as the backbone. Which specific diffusion model is used in the experiments? If the S2R-Adapter is employed, what is the resulting model complexity and inference time? During fine-tuning, how large a dataset is required to train the adapter effectively? It would be better if the experimental settings elaborated more on the training details.\n\n3. The S2R-Adapter can be used in self-supervised mode, and the paper mentions that “we dynamically adjust the scale factors using domain shift.” It is unclear how the domain shift is measured and how the model determines whether the domain shift is large or small. The paper refers to data augmentation and uncertainty estimation — but why does this strategy effectively measure domain shift and determine its magnitude?\n\n4. In Table 2, it would be better to include the model complexity and inference time for all compared models. Additionally, how many adaptation steps does the proposed S2R-Adapter use in the experiments? For the self-supervised mode, how does the performance vary across different datasets?"}, "questions": {"value": "I jointly discuss the weakness and questions on this paper in the Weakness section. The authors can prepare their rebuttal referred to the comments in the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qvFWDJCAkl", "forum": "Uc89k7HaUb", "replyto": "Uc89k7HaUb", "signatures": ["ICLR.cc/2026/Conference/Submission17306/Reviewer_2u9g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17306/Reviewer_2u9g"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905030100, "cdate": 1761905030100, "tmdate": 1762927241787, "mdate": 1762927241787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes S2R-HDR, a synthetic dataset of 24,000 HDR images rendered using Unreal Engine 5, representing a large-scale increase over existing datasets. To bridge the synthetic-to-real domain gap, they introduce S2R-Adapter, a dual-branch domain adaptation module with shared and transfer branches. The method demonstrates state-of-the-art performance on the reported benchmarks through both supervised fine-tuning and test-time adaptation"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The major strength of the dataset is definitely its scale.\n- The dataset presents a high variety of environments as opposed to real captures that are often in the same environment, or provide a very small number of examples in various environments.\n- The idea of using the adapter for domain adaptation is sound and seems to be working well.\n- The quantitative and qualitative results look convincing."}, "weaknesses": {"value": "- The authors introduce a shake simulation into camera poses. It would be great to assess whether those are close to the real world. In a fairly easy way, one could record sensor data with a phone capture to compare the two (potentially real patterns could also be transferred to the sim).\n- The authors report the size of the other dataset considering sequences of images, while the presented dataset is reported as the number of frames, which is misleading to the reader.\n- It would be good to see high-resolution images from the dataset; the compressed ones in the paper make it harder to judge quality.\n- I believe there is room for additional experiments to help with assessing the weight of the contributions. In Table 4, we see that training only on S2R-HDR is typically not enough but rather needs fine-tuning on the target dataset. It raises the question whether the scale or data diversity is needed - one could add all datasets to the mix in the comparison. Further, and more importantly, what happens if we take one or more of the real-world datasets to train the approach and then use the adapter on the target data? This would clarify the impact of the dataset a lot.\n- Continuing the argument regarding experimentation with the source data, part of the issues with the real-world dataset is imperfect real ISPs versus the synthetic pipeline. Would the adapter approach work, for e.g. a collection of real-world data (e.g. from YouTube) with a synthetic degradation applied?  \n- It seems that the approach does not work on the Kalantari dataset - Table 11 misses the entry of SAFNet trained on Kalantari (SAFNet reports higher values). It would be better to comment on that instead of skipping completely. Similarly, in Table 10, the authors report 44.13dB on Kalantari, whereas the SCT paper reports 44.49dB."}, "questions": {"value": "- What are the benefits of your approach over the one from (Barua et al., 2025), i.e. could their approach be adjusted to multiple exposures?\n- What kind of features were extracted for t-SNE?\n- Some more statistics on the dataset would be useful, e.g. more analysis on motion (quantifying local and global by presenting optical flow analysis), and some analysis on the number of different environments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BBUEFFW8nY", "forum": "Uc89k7HaUb", "replyto": "Uc89k7HaUb", "signatures": ["ICLR.cc/2026/Conference/Submission17306/Reviewer_rtUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17306/Reviewer_rtUP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961483641, "cdate": 1761961483641, "tmdate": 1762927241455, "mdate": 1762927241455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}