{"id": "DDogB73gS4", "number": 10482, "cdate": 1758173276890, "mdate": 1759897648370, "content": {"title": "PHUMA: Physically-Grounded Humanoid Locomotion Dataset", "abstract": "Motion imitation is a promising approach for humanoid locomotion, enabling agents to acquire humanlike behaviors. Existing methods typically rely on high-quality motion capture datasets such as AMASS, but these are scarce and expensive, limiting scalability and diversity. Recent studies attempt to scale data collection by converting large-scale internet videos, exemplified by Humanoid-X. However, they often introduce physical artifacts such as floating, penetration, and foot skating, which hinder stable imitation.\nIn response, we introduce \\textbf{PHUMA}, a \\textbf{P}hysically-grounded \\textbf{HUMA}noid locomotion dataset that leverages human video at scale, while addressing physical artifacts through careful data curation and physics-constrained retargeting.\nPHUMA enforces joint limits, ensures ground contact, and eliminates foot skating, producing motions that are both large-scale and physically reliable.\nWe evaluated PHUMA in two sets of conditions: (i) imitation of unseen motion from self-recorded test videos and (ii) path following with pelvis-only guidance. In both cases, PHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant gains in imitating diverse motions. PHUMA will be publicly released to support future research.", "tldr": "", "keywords": ["humanoid", "retarget", "dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/01b47257669751c81aca458cee9eb4e24cabff2d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission introduces PHUMA, a physically-grounded humanoid locomotion dataset created from human videos with a physics-constrained retargeting pipeline that enforces joint limits, reliable ground contact, and removes foot-skate. Trained policies on PHUMA outperform counterparts trained on AMASS and Humanoid-X in motion imitation and path-following tests. The authors plan to release the dataset to the community."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A large-scale (73h), curated, physics-aware dataset that  addresses well-known artifacts in video-derived motion (e.g., joint limits, penetration, floating, foot-skate). This provides practical value to the community.\n- The method is sound. The proposed PhySINK extends SINK to deal with joint limits, ground contact, and foot skating.\n- The evaluation is principled and thorough. Evaluations show consistent gains over AMASS and Humanoid-X across two use cases (unseen-video imitation, pelvis-only path following)."}, "weaknesses": {"value": "- Related works: Please compare/relate to recent efforts that also scale human to humanoid data, e.g., H2O/Human2Humanoid, ASAP, Humanoid Policy ~ Human Policy.\n- How does the proposed method handle non-planar ground, such as ground with height changes or stairs?\n- Physically invalid motions are removed but not refined. This could reduce the diversity of the data."}, "questions": {"value": "- what is the data distribution in terms of scene geometry, material, and semantics diversity (e.g., indoor vs outdoor, height variation, ground material)\n- To what extent is retargeting tied to a specific humanoid morphology? What is the distribution over body shape and heights.\n- How is foot contact detected? How reliable is that?\n- It would strengthen the paper to quantify how each physics constraint (contact, joint limits, anti-skate) contributes to downstream policy quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EhdHKQDC3k", "forum": "DDogB73gS4", "replyto": "DDogB73gS4", "signatures": ["ICLR.cc/2026/Conference/Submission10482/Reviewer_nr5V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10482/Reviewer_nr5V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761519679367, "cdate": 1761519679367, "tmdate": 1762921773263, "mdate": 1762921773263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the data scalability issue of humanoid locomotion, the authors proposed PHUMA. With carefully curated physics-aware motion filtering, retargetting, and learning, PHUMA managed to effectively enlarge the available data scale for humanoid. The effectiveness of PHUMA is validated on unseen motion imitation and path following. Impressive performance is achieved."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The curated PHUMA is superior to existing efforts in scale and diversity.\n\n- The proposed pipeline seems reasonable and practical.\n\n- The proposed pipeline and dataset could serve as valuable resources for humanoid learning."}, "weaknesses": {"value": "- A noticeable characteristic of PHUMA is its heterogeneous data sources. Extending the existing data quality analysis and performance analysis to data from different sources would be preferred. A separate evaluation of mocap-sourced and video-sourced PHUMA would be helpful.\n\n- Comparisons between the proposed physink and related works, like GMR, are missing. \n\n- Though the comparison of heuristic metrics in Table 2 is straightforward, a missed chance would be investigating the relationship between these physical artifacts and their influence on the humanoid learning procedure.\n\n- The physics-based filtering hasn't been well analyzed with ablation studies.\n\n- Many details are missing, some of which are closely related to the data quality and algorithm performance.\n\n- No video demonstration is attached, which is important for identifying the data quality. \n\n- No hardware deployment results are provided."}, "questions": {"value": "- In Table 1, most of the video datasets only have a frame rate of 30FPS. How would the low-pass filter with a cutoff frequency of 30Hz, as mentioned in A.1.1, work for these data? \n\n- How were the limits and thresholds in Table 7 decided? How many motion sequences were filtered out for each term? Also, how were the corresponding influences on the performance?\n\n- If the curation pipeline is applied to LAFAN1 and AMASS, would the corresponding performance be improved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8WLKYeiAyJ", "forum": "DDogB73gS4", "replyto": "DDogB73gS4", "signatures": ["ICLR.cc/2026/Conference/Submission10482/Reviewer_QDRY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10482/Reviewer_QDRY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820171158, "cdate": 1761820171158, "tmdate": 1762921772742, "mdate": 1762921772742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents PHUMA, a large-scale physically grounded humanoid motion dataset designed for stable imitation learning in simulation. It addresses the problem that motion data extracted from Internet videos, such as Humanoid-X, often contain artifacts like floating, penetration, and joint-limit violations that degrade physics-based policy training. PHUMA introduces a physics-aware curation process that filters motions for contact and balance consistency, and a physics-constrained retargeting algorithm, PhySINK, that enforces non-floating, non-penetration, and non-skating constraints. The resulting dataset contains about 76k motion clips covering 73 hours of locomotion data. Experiments on the Unitree G1 and H1-2 humanoids in Isaac Gym show that policies trained on PHUMA achieve substantially higher imitation success and physical stability than those trained on LaFAN1, AMASS, or Humanoid-X."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on physically grounded humanoid motion, addressing a gap in large-scale imitation datasets where stability and contact consistency are often ignored.\n\n2. The proposed PhySINK pipeline is technically solid and produces cleaner motion data with fewer artifacts than Humanoid-X.\n\n3. The experiments are comprehensive within simulation, demonstrating clear quantitative improvements on multiple humanoid platforms and providing a useful dataset that can benefit future physics-based imitation learning research."}, "weaknesses": {"value": "1. While the paper presents a clean and useful dataset, its novelty appears somewhat limited relative to recent works that also improve over Humanoid-X through physics-aware retargeting. Methods such as ASAP (RSS 2025), KunfuBot (NeurIPS 2025), and GMR have already introduced substantial innovations in motion retargeting and data cleaning—ASAP integrates RL-based physical simulation during retargeting, KunfuBot applies extensive filtering for realistic contacts, and GMR focuses specifically on retargeting fidelity—whereas PHUMA mainly improves data quality through curation and constraints. Compared with those works, the contribution here lies primarily in dataset refinement rather than methodological advance. \n\n2. In addition, the dataset only covers locomotion, leaving out other interaction behaviors, and all results are limited to simulation without any hardware validation. There is no qualitative simulation videos for reference, which makes it difficult to assess the realism of the resulting motions."}, "questions": {"value": "The imitation results are reported on both Unitree G1 and H1-2. Are the same PHUMA motions directly retargeted to each robot, or are robot-specific shape/scale adjustments applied?\n\nAre there qualitative examples where PhySINK still fails?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JRkCKEDwy1", "forum": "DDogB73gS4", "replyto": "DDogB73gS4", "signatures": ["ICLR.cc/2026/Conference/Submission10482/Reviewer_1eFX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10482/Reviewer_1eFX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943690861, "cdate": 1761943690861, "tmdate": 1762921772404, "mdate": 1762921772404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PHUMA introduces a new dataset of physically-grounded humanoid motions reconstructed then processed from videos of humans. The pipeline extracts human motion from RGB video then retargets it using a proposed Physically-grounded Shape-adaptive Inverse Kinematics (PhySINK) procedure. For policies trained on the PHUMA dataset, the authors observe higher success rates than corresponding policies trained on AMASS and Humanoid-X."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Data curation and processing pipeline is well-justified and thoroughly explained.\n- PhySINK has good quantitative comparison to SINK and Mink's IK as appropriate baselines.\n- Method identifies real problems with existing video-to-humanoid motion pipelines that lead to poor downstream performance during RL training and deployment.\n- The dataset is relatively large, comparing favorably against popular datasets like AMASS\n- Data pipeline validated on multiple humanoid robot form factors."}, "weaknesses": {"value": "- 1.2x success rate improvement over AMASS isn't very dramatic.\n- \"Physically grounded\" in this paper means the motions have kinematic plausibility as defined by heuristics and loosely verified by downstream sim RL training. I think this is somewhat misleading wording, since I would interpret \"physically grounded\" to mean that it is simulated dynamically in the data processing loop.\n- A sim-to-real deployment would be a stronger validation of the data pipeline."}, "questions": {"value": "- Why is VIBE used as opposed to a newer model like VIMO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6evd3gGCBd", "forum": "DDogB73gS4", "replyto": "DDogB73gS4", "signatures": ["ICLR.cc/2026/Conference/Submission10482/Reviewer_h4PJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10482/Reviewer_h4PJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993218686, "cdate": 1761993218686, "tmdate": 1762921772062, "mdate": 1762921772062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}