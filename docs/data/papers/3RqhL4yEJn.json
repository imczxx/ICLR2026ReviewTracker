{"id": "3RqhL4yEJn", "number": 2665, "cdate": 1757188459275, "mdate": 1759898134359, "content": {"title": "UQ: Assessing Language Models on Unsolved Questions", "abstract": "Benchmarks shape progress in AI research. A useful benchmark should be both difficult and realistic---questions should challenge frontier models while also reflecting real-world usage. Yet, current paradigms face a difficulty-realism tension: exam-style benchmarks are often made artificially difficult with limited real-world value, while benchmarks based on real user interaction often skew toward easy, high-frequency problems. In this work, we explore a radically different paradigm: assessing models on unsolved questions. Rather than a static benchmark scored once, we curate unsolved questions and evaluate models asynchronously over time with validator-assisted screening and community verification. We introduce UQ, a testbed of 500 challenging, diverse questions sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi and history, probing capabilities including reasoning, factuality, and browsing. UQ is difficult and realistic by construction: unsolved questions are often hard and naturally arise when humans seek answers, thus solving them yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset and its collection pipeline combining rule-based filters, LLM judges, and human review to ensure question quality (e.g., well-defined and difficult); (2) UQ-Validators, compound validation strategies that leverage the generator-validator gap to provide evaluation signals and pre-screen candidate solutions for human review; and (3) UQ-Platform, an open platform where experts collectively verify questions and solutions, enabling ongoing, asynchronous, and community-driven evaluation. The top-performing model passes UQ-validation on only 15% of questions, and preliminary human verification has already identified correct answers among those that passed. UQ charts a path for evaluating frontier models on real-world, open-ended challenges, where success pushes the frontier of human knowledge.", "tldr": "We explore a radical paradigm for AI evaluation: assessing LLMs on unsolved questions. Instead of contrived exams where progress ≠ real-world value, we test LLMs on organic, unsolved problems via oracle-free LLM validation & community verification.", "keywords": ["benchmark", "evaluations", "language models", "verifiers", "generator-validator gap", "platform"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0924b6d426c1b61028aeca495cda13d773851d3f.pdf", "supplementary_material": "/attachment/80301bf6b342902f137629470ed2ada388234d23.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents UQ, a benchmark assessing LMs on unsolved questions. UQ consists of 500 challenging, diverse questions that do not have solutions yet. Together with these questions, this paper presents UQ-Validators (validation strategies that pre-screen candidate solutions) and UQ-Platform. On SOTA LMs, this paper observes unique insights."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Currently there are a lot of LM evaluation datasets where the answers are already online, and perhaps even already in the LMs' memories. The UQ dataset overcomes that significant drawback and introduces a novel perspective in constructing datasets.  \n- Also, since these questions are unsolved yet, the solution to these problems can yield direct real-world value.  \n- The analyses on the LMs' problem-solving strategies produce interesting findings. Also, the discussion that connect the validation strategies to test-time scaling is interesting.  \n- The UQ-Platform is also a valuable empirical contribution that lays the foundation for future works."}, "weaknesses": {"value": "- The design of the UQ-Validator (especially the mid-level and the high-level strategies) seems a bit off target to me. I think the validator should check for the problem-solving strategy, and then provide an estimation regarding whether the attempted solution is \"on the right path\". Following this design goal, the low-level strategies make sense to me, but I don't think the mid-level and the high-level strategies (which aim at coping with the randomness of LLM-as-judges) reach the correct goal.  \n- Considering the pass rates for SOTA LMs are very low, I am afraid that the UQ-Validator allows too small of a subset of candidate solutions. Perhaps the LLMs don't need to pass these strategies to solve these problems?  \n- While the study that transfers from the HLE to UQ improves the solidness of the UQ-Validator, the granularity of transfer is on the generator-validator answer accuracy gap feels a bit too coarse to me. I would find a more fine-grained (and more direct) evaluation more convincing: to validate the detection of problem-solving strategies."}, "questions": {"value": "Could you respond to the items in the \"weaknesses\" section? I look forward to understanding more about the rationales behind the design."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "b5wnb556SN", "forum": "3RqhL4yEJn", "replyto": "3RqhL4yEJn", "signatures": ["ICLR.cc/2026/Conference/Submission2665/Reviewer_axDw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2665/Reviewer_axDw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589210899, "cdate": 1761589210899, "tmdate": 1762916324459, "mdate": 1762916324459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a benchmark including unsolved problems from CS, math, science, and history. Also, provide a complete pipeline including data filtering, evaluation, and building a platform for experts to continuously update the dataset"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation to design both difficult and realistic benchmarks, which provide challenging practical questions, is novel and meaningful\n2. The paper proposes a reference-free validation method to evaluate the correctness of the LLM response on the unlabeled question-answer datapoints\n3. Focusing on currently unsolved problems stimulates the usage of LLM in answering new research questions or exploring new research directions"}, "weaknesses": {"value": "1. The reason behind the dataset creation criteria is unclear. How the rules set for the rule-based filter and LLM-based filter contribute to the difficulty and realism is not clearly illustrated.\n2. Some processes use the LLM to provide the label. The reliability of the LLM on such tasks is unclear. For example, the “approachable: whether the question is logically sound and solvable in principle”, seems to be a challenging task for LLM to provide a reliable label. \n3. The definition of unsolvable is general. Without further analysis, we can not figure out whether the reason for the LLM’s failure is insufficient knowledge or reasoning capability. Since multiple factors can make questions unsolvable, performance on this dataset does not clearly isolate which capability is being evaluated.\n4. To achieve an oracle-free validator is challenging, but it is required with good performance to guarantee the quality of the benchmarks. The current performance level on the HLE dataset seems to be unsatisfying. Additionally, the validator is not well-connected with the human evaluation part. The paper mentions that the experts rate the responses that passed the validation. The low precision indicates that the expert labels are still significant to control the dataset quality."}, "questions": {"value": "1. In the LLM-based filter stage, what is the accuracy of the LLM labels on each property checking task?\n2. For the validator stage, instead of the binary label, would it be better to provide a numerical probability like a confidence score? A carefully designed threshold could then balance precision and recall, or identify questions requiring further human annotation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YxDflUtSfB", "forum": "3RqhL4yEJn", "replyto": "3RqhL4yEJn", "signatures": ["ICLR.cc/2026/Conference/Submission2665/Reviewer_yanc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2665/Reviewer_yanc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857606738, "cdate": 1761857606738, "tmdate": 1762916323658, "mdate": 1762916323658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces UQ, a benchmark of unsolved questions sourced from Stack Exchange. The benchmark includes 500 questions curated through rule-based filtering, LLM filtering and human review. Model responses are judged by LLM validators that aim to rule out wrong answers via a hierarchical set of strategies such as correctness, consistency, logic structure etc. The dataset is hosted on a platform where domain experts can comment on questions, check model answers and even provide solutions to the problems. The paper argues that this paradigm increases realism while remaining difficult in a non-artificial way, compared to other exam-like benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper explores alternatives to artificial exam-like benchmarks and tries to tackle the problem of benchmarks with highly difficult but unrealistic problems.\n- The collection pipeline is well curated, with several types of filters and explicit filtering criteria such as well-posedness, difficulty etc.\n- The fact that the benchmark is hosted on a live platform helps checking question quality, model answers and even provide solutions to problems\n- Validators are useful to rule out wrong answers, and there is a clear pipeline for that."}, "weaknesses": {"value": "- The paper suggests that unsolved problems are “by construction” realistic. However, there are several unsolved problems that are really hard, but artificial. Conversely, not all difficult solved problems are inherently unrealistic and artificial. As an example, research problems are unsolved but still natural for the context. It’d be better to add some more evidence about the realism claim.\n- Given the absence of ground truth answers and with validators being indicative but not conclusive (according to the authors), comparison between models is not grounded. UQ resembles more a testbed+platform rather than a traditional benchmark.\n- The dataset is heavily weighted towards math-related problems (>60%), with several categories having only 1 problem. This might reflect the Stack Exchange supply, but makes the dataset less diverse."}, "questions": {"value": "- It would be helpful to expand a bit and clarify the results in table 2, in particular the human pass rate.\n- A more detailed discussion on realism and difficulty, with some examples, would help clarify this tradeoff.\n- Given the heavy math presence in the dataset, are you planning on expanding other categories?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mQDVUR41zq", "forum": "3RqhL4yEJn", "replyto": "3RqhL4yEJn", "signatures": ["ICLR.cc/2026/Conference/Submission2665/Reviewer_o1JY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2665/Reviewer_o1JY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957422567, "cdate": 1761957422567, "tmdate": 1762916323452, "mdate": 1762916323452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces UQ, a novel benchmark designed to evaluate frontier AI models on genuinely unsolved questions sourced from communities like Stack Exchange. The authors argue that current benchmarks are either too artificial or too easy. UQ addresses this by curating a dataset of 500 difficult and realistic problems, developing an LLM-based system (UQ-Validators) to pre-screen answers, and launching a public platform for ongoing community verification. This paradigm shifts AI evaluation from solving known problems to tackling the frontiers of human knowledge, making progress on the benchmark directly valuable."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ A significant strength is the creation of the UQ-Platform. The authors acknowledge that automated validation is insufficient for these complex, open-ended problems. By building an ecosystem for human experts to review, rate, and verify AI-generated answers, they create a sustainable, long-term evaluation method that can adapt as models improve.\n\n+ The benchmark's design directly measures a model's ability to solve problems that are currently unsolved by humans. This is a powerful concept because success isn't just about matching human performance on a solved task."}, "weaknesses": {"value": "- The core premise, using unsolved questions, creates a fundamental verification bottleneck. Without ground truth, evaluation relies on the UQ-Validators (which are imperfect) and human experts. Sourcing and compensating a diverse pool of domain experts to verify a large volume of complex answers is costly, time-consuming, and not easily scalable.\n\n- The UQ-Validator pipeline is intricate and depends on LLMs judging other LLMs, a process known to have biases. The paper's own results show the best validator has limited precision (~40%), meaning it incorrectly passes many wrong solutions (false positives) and may discard correct ones (false negatives). This questions the reliability of the automated screening process.\n\n- Because the problems are unsolved, a model's failure doesn't necessarily indicate a flaw. The problem might be intractable. This makes it difficult to distinguish between a model's capability gap and the inherent difficulty of the question, unlike benchmarks with known solutions where failure is a clear signal.\n\n- The entire UQ-Validator system is built on the hypothesis that validating an answer is significantly easier than generating it. While the paper provides evidence for this on the HLE dataset, this assumption may not hold true for all types of unsolved problems. For deeply complex proofs or creative tasks, constructing the verification can be as intellectually demanding as creating the solution itself."}, "questions": {"value": "The paper contrasts \"artificially difficult\" questions with UQ's \"naturally arising\" ones. Could you elaborate on your definition of \"artificial\" and provide more detail on how this leads to the \"distribution shift\" you aim to avoid?\n\nYou define a \"difficult\" benchmark as one that challenges frontier models, yet you critique exam-style benchmarks for being quickly saturated. How do you reconcile these points? What properties of unsolved questions make their difficulty more durable and a better long-term measure of capability?\n\n\nConsidering that researchers often increase the complexity of problems after solving easier ones, what is the core disadvantage of creating progressively harder artificial questions? Why is preserving the \"realism\" of naturally-arising problems prioritized over creating a more scalable and continuously challenging artificial benchmark?\n\n\nIn the LLM-based filtering stage, you mention using three repeated calls to judge each question. Could you provide data or analysis on the variance of these judgments? How did you validate that these scores were stable and reliable enough to serve as a critical filter in your pipeline?\n\nThe final UQ-Dataset contains 500 questions, curated from an initial pool of over 3 million. How did you end up only on exactly 500 questions.\n\nThe paper states that benchmarks based on user queries often skew towards \"easy to articulate\" problems. How does sourcing questions from Stack Exchange, a user-query-based platform, avoid this same potential bias?\n\nDuring the LLM-based filtering stage, a model (GPT-4o) is used to generate a candidate answer to help assess the question's quality. What was the rationale for choosing this specific model? Was there a concern that using a more powerful, state-of-the-art model at this stage might solve too many questions, thereby misclassifying them as \"easy\"?\n\nUQ-Dataset is not a representative sample of \"unsolved questions\" but rather a sample of popular, well-defined, STEM-oriented questions that remain unanswered on a specific set of websites. Progress on this benchmark may not generalize to other forms of frontier problem-solving.\n\nBecause the questions are so difficult, models are expected to solve very few of them. As seen in the initial results, the top model solved only 4 out of 500 questions after human verification. This creates an extremely sparse signal. A model could make significant internal improvements in its reasoning capabilities but still not be powerful enough to solve another UQ question, resulting in a reported score of zero progress.\n\nWhat question is not artificially created? Artificial is well-defined."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VtLhqsC7o6", "forum": "3RqhL4yEJn", "replyto": "3RqhL4yEJn", "signatures": ["ICLR.cc/2026/Conference/Submission2665/Reviewer_JCCv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2665/Reviewer_JCCv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998339111, "cdate": 1761998339111, "tmdate": 1762916323177, "mdate": 1762916323177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}