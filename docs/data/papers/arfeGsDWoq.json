{"id": "arfeGsDWoq", "number": 6736, "cdate": 1757993993039, "mdate": 1763435803539, "content": {"title": "RayI2P: Learning Rays for Image-to-Point Cloud Registration", "abstract": "Image-to-point cloud registration aims to estimate the 6-DoF camera pose of a query image relative to a 3D point cloud map. Existing methods fall into two categories: matching-free methods regress pose directly using geometric priors, but lack fine-grained supervision and struggle with precise alignment; matching-based methods construct dense 2D-3D correspondences for PnP-based pose estimation, but are fundamentally limited by projection ambiguity (where multiple geometrically distinct 3D points project to the same image patch, leading to ambiguous feature representations) and scale inconsistency (where fixed-size image patches correspond to 3D regions of varying physical size, causing misaligned receptive fields across modalities). To address these issues, we propose a novel ray-based registration framework that   first predicts patch-wise 3D ray bundles connecting image patches to the 3D scene and then estimates camera pose via a differentiable ray-guided regression module, bypassing the need for explicit 2D-3D correspondences. This formulation naturally resolves projection ambiguity, provides scale-consistent geometry encoding, and enables fine-grained supervision for accurate pose estimation. Experiments on KITTI and nuScenes show that our approach achieves state-of-the-art registration accuracy, outperforming existing methods.", "tldr": "", "keywords": ["Image-to-Point Cloud Registration"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97953f10df367f47544c872b6f9daf1731fd8a5b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of registration between a 3d point cloud and an image of the same scene. Compared to the problems of 3d registration between two\npoint clouds or image matching between two images, this problem is significantly more difficult. This difficulty comes from the ambiguities introduced by projecting 3d points \nto images and finding the correct matches in the images. Learning based approaches have been dominating the field. The main novelty in this paper is the use of the rays to \nparameterize the pose of the point cloud to the camera coordinate system. The rays are represented using Plücker coordinates, and this has been shown to be beneficial for \ncamera pose estimation and for 6d object pose estimation. This parameterization seems to be easier to learn and brings other benefits, like more precise alignment and \nfewer projection ambiguities, and scale inconsistencies. The paper is well-written and not difficult to follow. The novelty seems to be limited, since all elements of the \nproposed method have been known before and just combined using the ray representation. The overall pipeline has two stages composed of the ray prediction module, which basically does \n3d point cloud and image feature fusion and is fairly standard and doesn't depend on the ray parameterization of the image patches. The second stage is a ray-guided regression module\nthat uses ray parameterization of the image patches, fused multi-modal features, and regresses the pose in terms of rotation and translation. The pipeline is supervised and applied to autonomous driving scenarios, where alignment between LIDAR scans and RGB images has been performed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-motivated and well-written. It was easy to follow it. Using ray parameterization for estimating the pose of the point cloud in the camera coordinate system\nThis makes sense, as it has been demonstrated for related problems, such as image pose estimation [1] and object pose estimation [2]. However, both approaches use this ray parameterization \nfor underlying diffusion models. Here, this is not the case. What I find interesting is the use of reference rays in the ray-guided pose regression. However, this is not well motivated."}, "weaknesses": {"value": "The proposed method has limited novelty. The first part of the feature fusion, known as the ray prediction module, is not new and doesn't benefit from the camera ray parameterization; however, it utilizes fused features and predicts image patch rays. The motivation behind the use of the reference rays and predicted rays is not given. The paper \ncontains numerous ad-hoc design choices that are not clearly motivated or explained. See questions section for more details.\nOn the experimental side, I find the evaluation limited. It has been done only on two autonomous driving datasets. Why is this choice made for only this dataset and not some indoor datasets. Better motivation is needed for readers who are not interested in autonomous driving. I suppose the reason is that LIDAR and RGB images are not registered in the autonomous vehicles."}, "questions": {"value": "Can you give more precise intuition behind using the reference rays? How do the guides and stabilize the regression process?\n\nWhat will happen if you do not predict rays, but matches between 3d points and patches?\n\n\nThe results from Table 1 show that GraphI2P is actually performing very well and it is on par or better than the proposed method. What's the reason? What is the advantage of the proposed approach compared to GraphI2P?\n\nIn the ablation study, the results in Table 3 are a bit confusing. It is unclear what the fused features bring, as the rotation estimates in KITTI are more stable (smaller std) than in NuScenes. How do you explain this? What will happen if you have FPF and CPS only? RR seems to improve rotation estimation. Why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BVCpkSTyiK", "forum": "arfeGsDWoq", "replyto": "arfeGsDWoq", "signatures": ["ICLR.cc/2026/Conference/Submission6736/Reviewer_JaMP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6736/Reviewer_JaMP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869193848, "cdate": 1761869193848, "tmdate": 1762919022551, "mdate": 1762919022551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an image-to-point cloud registration method that estimates the 6-DoF camera pose of a query image relative to a 3D point cloud map. The proposed ray-based registration framework first predicts patch-wise 3D ray bundles connecting image patches to the 3D scene, then estimates the camera pose via a differentiable ray-guided regression module."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed ray-based registration framework for image-to-point cloud registration is interesting.\n- The experimental results verified the effectiveness of the proposed method."}, "weaknesses": {"value": "- Additional discussion comparing the proposed method with other ray-based representation methods [1] should be added.\n- The approach appears to be a direct application of ray-based representation for pose estimation to the image-to-point cloud registration task. The specific challenge this addresses should be further clarified.\n\n[1] Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as rays: Pose estimation via ray diffusion. In *The Twelfth International Conference on Learning Representations*, 2024."}, "questions": {"value": "What is the key challenge in image-to-point cloud registration compared to ray-based representation for pose estimation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "j8evKs18hG", "forum": "arfeGsDWoq", "replyto": "arfeGsDWoq", "signatures": ["ICLR.cc/2026/Conference/Submission6736/Reviewer_RNei"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6736/Reviewer_RNei"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906972355, "cdate": 1761906972355, "tmdate": 1762919022016, "mdate": 1762919022016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a method for image-to-point cloud registration. Matching-based approaches is challenged by projection ambiguity and depth-induced scale inconsistency. To adress these problems, they introduce a differentiable ray-guided regression module which regress camera pose from predicted Plu ̈cker rays, thereby naturally resolving projection ambiguity and depth-induced scale ambiguity. The authors conduct experiments on KITTI and nuScenes. Method are evaluated by Relative Translation Error (RTE), average Relative Rotation Error (RRE), and registration accuracy (Acc). Compared to existing state-of-the-art approaches, the proposed method achieves strong accuracy, while remaining computationally efficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is clearly motivated and well written. The paper intoduces a novel ray-guided pose regression module which addresses projection ambiguity and depth-induced scale inconsistency. Its technical rationale is supported by findings in the generalized camera models. The experimental setup is comprehensive, covering multiple datasets, metrics and baselines."}, "weaknesses": {"value": "- The author did not analyze the causes of errors in the predicted rays. The neural regression module seems more like a compensatory measure; however, its use may raise concerns about generalization.\n- Some of the illustrative figures are not very clear. In Figure 3, does the color of the points represent the depth error or the actual depth value?"}, "questions": {"value": "- Can the ray errors could be mitigated using diffusion-based methods?\n- If the error threshold for Acc is set smaller, can the regression-based method achieve precision comparable to that of matching-based methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TgEsjZrwvP", "forum": "arfeGsDWoq", "replyto": "arfeGsDWoq", "signatures": ["ICLR.cc/2026/Conference/Submission6736/Reviewer_6CqR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6736/Reviewer_6CqR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993969599, "cdate": 1761993969599, "tmdate": 1762919021620, "mdate": 1762919021620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}