{"id": "kbuGvzBJg4", "number": 16133, "cdate": 1758260409519, "mdate": 1759897259745, "content": {"title": "Don’t \"Overthink'\" Pointwise Reranking: Is Reasoning Truly Necessary?", "abstract": "With the growing success of reasoning models across complex natural language tasks, researchers in the Information Retrieval (IR) community have begun exploring how similar reasoning capabilities can be integrated into passage rerankers built on Large Language Models (LLMs). These methods typically employ an LLM to produce an explicit, step-by-step reasoning process before arriving at a final relevance prediction. But, *does reasoning actually improve pointwise reranking accuracy?* In this paper, we dive deeper into this question, studying the impact of the reasoning process by comparing reasoning-based pointwise rerankers (Rank1) to standard, non-reasoning pointwise rerankers (StandardRanker) under identical training conditions, and observe that StandardRanker generally outperforms Rank1. Building on this observation, we then study the importance of reasoning to Rank1 by disabling its reasoning process (Rank1-NoReason), and find that Rank1-NoReason is surprisingly more effective than Rank1. Examining the cause of this result, our findings reveal that pointwise reasoning rerankers are bottlenecked by the LLM's reasoning process, which pushes it toward polarized relevance scores and thus fails to consider the *partial* relevance of passages, a key factor for the accuracy of pointwise rerankers. The source code is in the supplementary materials and will be made public upon acceptance.", "tldr": "In this work, we study whether the generation of a reasoning chain prior to making a relevance prediction actually improves the accuracy of pointwise rerankers.", "keywords": ["passage retrieval", "reranking", "reasoning", "large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7a835ba88e3963a56a65bc644684881e1de4f41.pdf", "supplementary_material": "/attachment/5b712479569f200c27741b7934b463ef75de6267.zip"}, "replies": [{"content": {"summary": {"value": "This paper focus on  investigating wether reasoning actually improve pointwise reranking accuracy by comparing reasoning-based pointwise rerankers (Rank1) to standard, non-reasoning pointwise rerankers (StandardRanker) under identical training conditions, and observe that StandardRanker generally outperforms Rank1. Building on this observation, This paper studys the importance of reasoning to Rank1 by disabling its reasoning process (Rank1-NoReason), and find that Rank1-NoReason is surprisingly more effective than Rank1. Experimental results reveal that pointwise reasoning rerankers are bottlenecked by the LLM's reasoning process, which pushes it toward polarized relevance scores and thus fails to consider the partial relevance of passages, a key factor for the accuracy of pointwise rerankers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper fills this gap by providing a thorough comparison between reasoning-based rerankers and their equivalent non-reasoning counterparts under identical backbones and training data.\n2. This paper is very easy to read and clearly conveys the authors’ intentions, making it easy to follow.\n3. The experiments in this paper are highly rigorous, with all potential confounding factors carefully controlled to ensure the validity and reliability of the results. This paper demonstrate strong experimental discipline, conducting systematic analyses to isolate the effects of reasoning from other influencing variables."}, "weaknesses": {"value": "1. In general, the paper focuses on aligning existing methods under comparable settings and reporting the resulting findings, without introducing new techniques or theoretical contributions. Hence, its novelty is rather limited.\n2. In the implementation of Rank1-NoReason, the authors use <think> Okay, I have finished thinking. </think>. This implementation may not be entirely appropriate. I believe a more precise approach would be to use a blank (empty string) instead. According to existing studies [1][2], using logically invalid Chain-of-Thought (CoT) prompting can improve performance; therefore, merely including a <think> marker when removing reasoning is arguably not entirely justified.  [1] Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting [2] ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization\n3. According to the assumptions in this paper, adding reasoning tends to polarize relevance scores, which reduces the performance of pointwise rerankers. Figure 2 shows that the score distribution of Rank1-NoReason is smoother compared to Rank1. However, despite its smoother distribution, Rank1-NoReason still does not achieve the same performance as StandardRanker."}, "questions": {"value": "1. Can the authors provide a further explanation for the phenomenon observed in the third paragraph of Section 4.2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WnTGKiC5Dx", "forum": "kbuGvzBJg4", "replyto": "kbuGvzBJg4", "signatures": ["ICLR.cc/2026/Conference/Submission16133/Reviewer_JX9a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16133/Reviewer_JX9a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645108907, "cdate": 1761645108907, "tmdate": 1762926302634, "mdate": 1762926302634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether explicit reasoning actually benefits LLM based pointwise rerankers. The experiments are inspired by Rank1, the authors conduct controlled experiments comparing three variants: 1) StandardRanker (direct relevance classification), 2) Rank1 (reasoning-before-prediction), 3) Rank1-NoReason (reasoning disabled). All using the same Qwen2.5 backbones, data, and LoRA fine-tuning. \n\nAcross MS MARCO and BRIGHT datasets, the paper finds that reasoning consistently reduces effectiveness: both StandardRanker and Rank1-NoReason outperform Rank1, with reasoning producing more polarized relevance scores and weaker modeling of partial relevance. The authors further test self-consistency and inverse-training variants but find limited improvement. The authors claim that reasoning, in its current form, is unnecessary or even harmful for pointwise reranking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-controlled empirical setup.\nThe paper carefully eliminates confounding variables with same backbone, training data, and fine-tuning recipe, enabling a fair and reproducible comparison rarely seen in prior reasoning works.\n\n2. Clear exposition and empirical transparency.\nFigures and tables are easy to follow, prompts and code are documented, and reasoning examples (Table 6) illustrate the claimed polarization effect intuitively.\n\n3. Timely contribution.\nThe work addresses a currently popular but under-examined assumption that reasoning always improves LLM performance, making its negative results relevant to both the IR and reasoning communities."}, "weaknesses": {"value": "1. limited novelty. While the empirical setup is rigorous, the overall conceptual novelty is limited. The study primarily revisits the question, whether reasoning-before-prediction improves ranking performance, through replication and controlled comparison, but does not introduce new modeling, training, or theoretical innovations. The work therefore reads more as a careful diagnostic replication of Rank1 rather than a forward methodological contribution or a new hypothesis about how reasoning could be made effective.\n\n2. Limited exploration of alternative reasoning placements or purposes. The study focuses exclusively on the “reasoning-before-prediction” paradigm, leaving unexplored other plausible reasoning designs that may capture complementary benefits, such as reasoning-after-prediction (for explainability) or reasoning-as-regularization (for robustness). Reasoning could hold value beyond accuracy, e.g., by improving model interpretability, calibration under distributional shifts, or user trust. Without examining whether reasoning contributes to robustness (e.g., under adversarial or out-of-domain perturbations) or to stable explanation quality, it remains unclear whether the observed degradation reflects a fundamental flaw of reasoning or an artifact of this specific evaluation setup.\n\n3. Undertrained and potentially biased reasoning models. The negative finding that reasoning reduces performance may partly arise from the tuning strategy rather than an inherent weakness of reasoning itself. First, all models are fine-tuned with LoRA for only one epoch, using a fixed configuration (rank 32, α=64). Such shallow adaptation may under-optimize reasoning sequences, which typically require longer or multi-stage training to align reasoning tokens with downstream labels. Second, the reasoning data themselves, generated by DeepSeek-R1, may contain stylistic bias or overconfident language that systematically distorts supervision. The paper does not quantify how this bias propagates into Rank1’s learning. Without such analysis, it is difficult to determine whether reasoning failure stems from model capacity, data bias, or tuning procedure.\n\n4. Lack of quantitative calibration analysis. The central claim that reasoning polarizes probabilities and harms modeling of partial relevance, is plausible but remains qualitatively supported. Figure 2 visually suggests this polarization, yet no quantitative evidence is presented to substantiate or measure its magnitude. Including calibration or uncertainty metrics such as entropy or KL divergence between score distributions would provide objective validation of this hypothesis. These measures could also reveal whether reasoning affects confidence miscalibration (i.e., overconfident “true” predictions) differently across datasets or model sizes. Without such analysis, the explanation for why reasoning fails remains speculative and descriptive rather than empirically grounded.\n\n5. Limited interpretability and trade-off analysis. While the paper claims reasoning degrades accuracy, it does not explore whether reasoning offers compensatory benefits in interpretability or case-specific generalization. For example, reasoning might improve performance on complex or long queries, even if it hurts on simpler ones. Similarly, reasoning could help explain borderline relevance decisions even if it does not change ranking order. However, the study treats reasoning as monolithic, without differentiating by query difficulty, reasoning length, or reasoning structure. As a result, potential trade-offs between reasoning depth and ranking effectiveness are not analyzed, leaving open whether “turning off reasoning” truly yields a better system overall or simply optimizes for one narrow metric (nDCG@10) at the expense of transparency and task diversity."}, "questions": {"value": "1. Your study convincingly shows that reasoning-before-prediction harms pointwise reranking accuracy. However, could you clarify whether your claim is intended to generalize beyond this paradigm? For instance, do you believe your conclusion extends to reasoning-after-prediction or reasoning-as-regularization frameworks? A clearer articulation of scope would help contextualize how general the “reasoning is unnecessary” statement should be interpreted.\n\n2. The paper fine-tunes each model for only one epoch with fixed LoRA settings. Have you explored whether extending fine-tuning duration, adjusting LoRA ranks, or using multi-stage optimization would change the observed trend? In particular, could undertraining explain why reasoning appears ineffective at all model scales?\n\n3. Since all reasoning chains originate from DeepSeek-R1, how do you assess or control for potential bias or stylistic artifacts in this synthetic data? Have you examined whether reasoning length, verbosity, or lexical patterns correlate with ranking degradation?\nQuantifying how reasoning data quality affects downstream reranker performance would help separate data bias from model behavior.\n\n4. The paper makes extensive use of long dash constructions (“—....—”), a stylistic pattern often characteristic of ChatGPT-generated writing. Could the authors clarify to what extent AI tools were used in manuscript drafting or editing? (i.e., line 50, line 61, line 62, line 189, line 208)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mWvOQryw6S", "forum": "kbuGvzBJg4", "replyto": "kbuGvzBJg4", "signatures": ["ICLR.cc/2026/Conference/Submission16133/Reviewer_d73v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16133/Reviewer_d73v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801773943, "cdate": 1761801773943, "tmdate": 1762926302246, "mdate": 1762926302246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether introducing reasoning processes, such as chain-of-thought (CoT), can genuinely improve the performance of large language model (LLM)-based pointwise reranking tasks. The authors compare a reasoning-enhanced reranker (Rank1) with a standard reranker without reasoning (StandardRanker) under identical model architecture (Qwen2.5), training data, and hyperparameters. The experiments reveal that StandardRanker consistently outperforms Rank1 on both in-domain (MS MARCO) and out-of-domain (BRIGHT) datasets. Interestingly, disabling the reasoning steps in Rank1 during inference (Rank1-NoReason) leads to further performance improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The effectiveness of reasoning training for LLMs in ranking tasks addresses an important research question.\n2. The paper identifies an intriguing phenomenon: CoT knowledge distillation in supervised fine-tuning (SFT) may not necessarily enhance model reasoning capabilities.\n3. The manuscript is well-written with clear and coherent argumentation."}, "weaknesses": {"value": "Lack of statistical significance testing: The paper does not verify the statistical significance of experimental results, leaving the reliability of the findings uncertain.\n\nLimited generalizability: Experiments are conducted solely on the Qwen 2.5 model series. Whether the observed “reasoning harm” phenomenon extends to other mainstream LLMs (e.g., Qwen3, LLaMA series) remains unverified, thereby limiting the universality and persuasiveness of the conclusions.\n\nUnexplored influence of label form: The use of true/false string labels during training could affect the results. It remains unclear whether alternative label formats, such as binary relevance scores (0/1) or yes/no tokens, would alter the conclusions.\n\nUnquantified quality and impact of reasoning data (CoT):\nThe CoT used in training is generated by the Deepseek model, whose quality is difficult to assess. Whether higher-quality CoT generated by more advanced models (e.g., GPT-5 or Gemini 2.5 Pro) would yield different outcomes is unexplored.\nPotential distributional differences between Deepseek’s reasoning patterns and the Qwen 2.5 model’s “thinking style” may impede effective learning of reasoning capabilities, but this aspect is not discussed.\nTable 4 lacks statistical significance testing, raising concerns that observed metric fluctuations may stem from input variance rather than generalizable effects.\nSingle retrieval method: The study employs only one retrieval method per dataset. It is uncertain whether findings are affected by the quality and characteristics of the retrieval pool. For instance, whether similar conclusions hold when reranking documents retrieved via pure BM25 on BRIGHT requires verification.\n\nInsufficient empirical support for the attribution of ‘score polarization’:\n\nThe paper attributes performance degradation to score polarization caused by reasoning, but this remains a logically plausible hypothesis rather than a rigorously validated conclusion.\n\nFundamental questions such as the cause of score polarization, zero-shot score distributions, and whether model training generally promotes polarized or balanced scores remain unaddressed.\n\nObservations that Qwen3 achieves strong zero-shot performance on BRIGHT despite apparently polarized score distributions suggest a discrepancy with the paper’s reasoning.\nMore importantly, causal evidence linking score polarization directly to performance decline is lacking.\n\nMissing related work section: The paper lacks a dedicated related work discussion and does not systematically review or compare with prominent reasoning-based ranking models such as RankR1, REARANKER, and TFRank."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SzokHRMJOo", "forum": "kbuGvzBJg4", "replyto": "kbuGvzBJg4", "signatures": ["ICLR.cc/2026/Conference/Submission16133/Reviewer_KvpF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16133/Reviewer_KvpF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904894089, "cdate": 1761904894089, "tmdate": 1762926301847, "mdate": 1762926301847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors of this paper investigate whether reasoning is beneficial for point-wise reranking performance. They demonstrate that Rank1, a recently proposed reasoning based reranker, performs better when forced to not produce any reasoning tokens. Additionally, the authors hypothesize that reasoning hurts performance because it results in a bimodal probability distribution for the final yes/no token that is used to compute the reranker score."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of the paper are:\n- The authors correctly point out that the the primary non-reasoning baselines used in Rank1 are built on older language models and therefore the baseline comparison is flawed in the Rank1 paper.\n- The proposed model, StandardRanker, outperforms Rank1 without using reasoning traces. \n- The analysis done with Rank1-NoReason, that is the Rank1 model with reasoning disables, is interesting."}, "weaknesses": {"value": "The weaknesses of the paper are:\n- The authors claim that the Rank1 has worse performance because the partial relevance of query-passage pairs is not modeled by Rank1. I agree that the relevance scores from Rank1 are either 0 or 1 most of the time. But I'm not convinced that this is the primary reason for why Rank1 underperforms. In the datasets that the authors use (MSMarco and Bright), the samples are either relevant or non-relevant (and there are no partially relevant samples). So it is important to model partial relevance? It is possible that modeling partial relevance provides a better training signal, but the authors have not clearly demonstrated the causal link between modeling partial relevance and improved performance. \n- In appendix H, Rank1 is trained to produce non-binary relevance labels. The relevance scores from Rank1-hybrid should now be distributed in a less bimodal way. Can we see the distribution of scores from this model? How come this model still underperforms StandardRanker+hybrid? \n- No analysis of whether reasoning helps listwise rerankers. The current scope of the paper is very limited and the analysis is mostly limited to RankR1. \n- Shao et al. (2025) propose using Qwen models directly without training. StandardRanker is not compared to this baseline.\n\nReference:\nRulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, et al. ReasonIR: Training Retrievers for Reasoning Tasks. arXiv preprint arXiv:2504.20595, 2025."}, "questions": {"value": "Here are some questions I have:\n- Which dataset is used in figure 2? \n- Which model and dataset are used for the analysis in section 5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oNmlLgPE61", "forum": "kbuGvzBJg4", "replyto": "kbuGvzBJg4", "signatures": ["ICLR.cc/2026/Conference/Submission16133/Reviewer_ohBn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16133/Reviewer_ohBn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967410717, "cdate": 1761967410717, "tmdate": 1762926301151, "mdate": 1762926301151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}