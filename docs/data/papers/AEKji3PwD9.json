{"id": "AEKji3PwD9", "number": 22374, "cdate": 1758330191835, "mdate": 1759896869638, "content": {"title": "To Distill or Not to Distill: Knowledge Transfer Undermines Safety of LLMs", "abstract": "Training smaller LLMs often relies on fine-tuning with high-quality data or distilling knowledge from a larger teacher model. Fine-tuning is known to improve utility but reduces safety even on harmless data. In contrast, the safety implications of distillation are not well studied. In this study, we systematically evaluate different hard and soft label distillation methods across tasks such as machine translation, arithmetic reasoning and medical instruction following. We then probe these models on safety dimensions covering jailbreaks, faithfulness and toxicity. Our results show that logit-based soft label distillation produces highly capable models but  negatively impacts their safety, with a significantly greater impact (up to 50%) compared to fine-tuning. Post-hoc mechanistic analysis reveals greater token-level uncertainty during safety evaluations and  sporadic semantic drift patterns between teacher and student models, which better explains this amplified effect. As distillation methods continue to improve, our findings show the need to examine their safety consequences.", "tldr": "Knowledge distillation yields less safer modes than supervised fine-tuning.", "keywords": ["Knowledge Distillation", "Safety evaluations", "Post-training Robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fed763a30898a94daf0c79a480b698875f2cf105.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper conducts experiments to show that safety on small models will be undermined with model distillation. It is an intuitive guess, and the authors have verified on both soft and hard labels. Uncertainty and semantic drift have been analyzed in distillation. Three models have been used to justify safety in three tasks: machine translation, arithmetic reasoning, and medical instruction following. Code has been released."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is an intuitive viewpoint that safety will be undermined with model distillation. All types of performance should have been undermined. That is why it is called distillation.\n2. The reviewer appreciate the authors' effort to conduct extensive experiments.\n3. The analysis could benefit future technology of model distillation."}, "weaknesses": {"value": "1. This paper is more of an empirical study paper rather than a research paper proposing new methods. While extensive experiments have been conducted, no new method has been proposed. With these analyses, the reviewer would expect new distillation methods to address the issue. The current state of the paper may fit other tracks, such as position paper tracks or the benchmark track.\n\n2. The impact of this paper could be limited. When fine-tuning with human feedback is dominating in both industry and academia to address safety, safety with model distillation would not excite readers, as it is the inherent weakness of model distillation. Even after model distillation, fine-tuning is still necessary, which may hinder the contribution of the observation.\n\n3. The paper introduces tasks including machine translation, arithmetic reasoning, and medical instruction following. From the perspective of the reviewer, the safety in machine translation and arithmetic reasoning is not the biggest concern. The paper should introduce more safety-related benchmarks, such as [1].\n\n4. Presentation could be improved. For example, Figure 6 and Table 2 are out of the paper border, which could violate the format requirement. \n\n[1] SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hwW8EZAbKE", "forum": "AEKji3PwD9", "replyto": "AEKji3PwD9", "signatures": ["ICLR.cc/2026/Conference/Submission22374/Reviewer_LVSz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22374/Reviewer_LVSz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755222540, "cdate": 1761755222540, "tmdate": 1762942190625, "mdate": 1762942190625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies an under-examined but important question: whether knowledge distillation (KD) erodes the safety of LLMs more than supervised fine-tuning (SFT). The authors compare five post-training regimes: SFT, sequence-level KD (hard labels), and three soft-label / KL-based distillation methods (vanilla KD with forward KL, MiniLLM with reverse KL, and on-policy GKD with JS divergence) - across three benign tasks (Marathi to English translation, MetaMathQA arithmetic reasoning, and MedInstruct medical IF). They then evaluate all resulting students on three safety dimensions: jailbreak robustness (JailbreakBench+PAIR), toxicity (RealToxicityPrompts), and faithfulness/hallucinations (FaithEval). The central empirical finding is sharp: soft-label, logits-based distillation gives the biggest utility gains but degrades safety the most—up to ~50 percentage points more than SFT on the same benign data, and far more than hard-label SFT. The paper further offers a mechanistic story: logit-based KD causes asymmetric uncertainty shifts—students stay confident on the trained utility tasks but become more epistemically uncertain specifically on safety queries, which correlates with more jailbreak successes. A semantic-drift analysis with unbalanced OT shows that being closer to the teacher semantically can paradoxically correlate with worse jailbreak outcomes, suggesting KD may also transfer undesirable lexical/safety patterns. The paper argues that, as KD becomes standard in open LLM pipelines, we need safety-aware distillation objectives."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem formulation. KD is widely used in open-source LLM training, but most safety work has focused on SFT/RLHF; showing that “KD is not safety-free” is a useful corrective.\n\n2. Systematic comparison of multiple KD flavours. Evaluating SFT, SeqKD, FKLD (KD), RKLD (MiniLLM), and JS/on-policy GKD on the same benign tasks and multiple model families (Qwen2, Llama-3.2, Gemma3) makes the conclusion harder to dismiss as “it was just one setup.” Table 3 is especially informative in this regard. \n\n3. Multidimensional safety evaluation. Many papers stop at jailbreaks; this one also measures toxicity and faithfulness, and finds the pattern is fairly consistent—distillation hurts more often than not.\n\n4. Interesting mechanism section. The logit-evidence / Dirichlet-based uncertainty analysis (LogTokU) to explain why safety collapses after KD is novel in this context and helps connect the empirics to a plausible explanation (safety tokens become high-evidence but low-knowledge → brittle refusal). \n\n5. Practical takeaway. The result that pretrained (non-IT) students preserve safety better than IT students after KD is useful guidance for practitioners who want to compress guard-like models."}, "weaknesses": {"value": "1. Limited contribution. Many prior work has already studied and discovered that KD can erode safety, or more broadly trustworthiness. For example, [1][2] have surveyed many work on privacy, safety, etc. There is no significant difference of this work.\n\n2. No baselines for safety-aware KD methods. Some KD methods are safety-aware. This paper only include non-safety-aware KD methods.  Please see [3][4]. The paper does not try the simplest ones (KD + refusal loss; KD + safety LoRA frozen; distill logits only on utility tokens but cross-entropy on safety tokens). Even a small experiment here would strengthen the contribution.\n\n3. Causality of \"soft labels\" leads to \"safety loss\" is not fully pinned down. The paper observes that KL-based KD correlates with stronger safety degradation, but several plausible co-factors are entangled: (i) teacher-student shared tokenizer and lineage; (ii) offline vs on-policy setting; (iii) specific benign tasks (translation is repeatedly noted to be “catastrophic” for jailbreaks); and (iv) warm-up SFT before KD for some methods. It is therefore hard to say “KD causes it,” as opposed to “this very common recipe for KD causes it.”\n\n4. Limited task diversity on the training side. All training data are “benign” and quite structured (translation, math, medical IF). When we distill LLMs, we usually include dialogue or long-form instruction data with mixed safety content. It is possible that KD on mixed-safety or safety-aware corpora would look less negative; the paper only hints at this (Sec. 5.1-5.3) but does not test it.\n\n5. Safety metrics are all output-side. The paper argues in Sec. 5.2 about \"lexical pattern reinforcement\" as a mechanism, but the presented evidence is indirect (semantic UOT correlations and uncertainty shifts). There is no representation-level or probing-level evidence (e.g., safety head activations, refusal-token margin analyses) to directly show that KD overwrites alignment features.\n\n6. Interpretation of the UOT result is a bit speculative. The finding that \"closer to teacher leads to more jailbreaks\" is interesting but could also be an artefact of (a) teacher itself not being perfectly safe; (b) students over-fitting to teacher phrasing that jailbreak benchmarks exploit; or (c) distribution mismatch between evaluation and distillation outputs. A brief ablation where the teacher is more strongly aligned (e.g., a Safety-RLHF-trained teacher) would make this claim more solid. \n\nReferences\n[1] A Comprehensive Survey on Knowledge Distillation, Mansourian et al., TMLR, 2025\n[2] A survey on knowledge distillation: Recent advancements, Moslemi et al., 2024\n[3] DistillSeq: A Framework for Safety Alignment Testing in Large Language Models using Knowledge Distillation, Yang et al., arxiv, 2024\n[4] Complementary KD for robust & privacy-preserving VFL, Gao et al., AAAI, 2024"}, "questions": {"value": "1. How teacher-dependent is the safety drop? If you distill from a more aligned but slightly less capable teacher (e.g. an RLHF’d version of the same model), does KD still degrade safety more than SFT, or does the effect shrink?\n\n2. Is it possible to decouple \"task effect\" from \"KD effect\"? The authors mentioned that translation triggers the worst safety deterioration across all methods. If you SFT on translation but KD on arithmetic (or vice versa) for the same student, do you still see the same pattern? A small cross-task swap experiment would clarify whether some of your results are task-driven.\n\n3. Why does semantic proximity to the teacher predict worse jailbreak resistance when the teacher is better? This is counter-intuitive and central to your argument in Sec. 4.2. Can you show at least one qualitative example where the student “copies” a teacher structure but loses the refusal token?\n\n4. Did you try KD with temperature > 1 at training time specifically to flatten teacher distributions? \"temperature is set to 1\" for student training; but much of the brittleness you observe could be a consequence of over-confident targets.\n\n5. How stable are the results across random seeds and judge models? JailbreakBench + PAIR can be sensitive to the refusal style. Did you verify with a second LLM judge or with a toxicity classifier different from Detoxify?\n\n6. Could safety be restored cheaply post-KD? You discuss RESTA/LISA/SafeMERGE as heavy-weight options. What happens if we just do a small, 2-5k example safety SFT after KD-do we recover SFT-like safety while keeping (some of) KD utility? That would make the paper even more actionable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "99XKwjR6m3", "forum": "AEKji3PwD9", "replyto": "AEKji3PwD9", "signatures": ["ICLR.cc/2026/Conference/Submission22374/Reviewer_NpKh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22374/Reviewer_NpKh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094051703, "cdate": 1762094051703, "tmdate": 1762942190341, "mdate": 1762942190341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the safety implications of knowledge distillation from larger LLMs to smaller ones. The main finding is that supervised fine-tuning (SFT) better preserves safety than distillation. Soft-label KD achieves a better safety–utility trade-off than hard-label KD, yet still shows a significant safety retention gap compared to SFT."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents an analysis across models, KD methods, tasks, and evaluation metrics.\n\nAdditional post-hoc analyses are provided, offering two main explanations (with evidence) for why KD degrades safety more than SFT.\n\nThe takeaway derived from the analysis is interesting."}, "weaknesses": {"value": "The analysis is interesting but not surprising, given prior work showing that SFT can improve utility while reducing safety even on harmless data. This paper confirms that KD tends to improve utility even more but also suffers greater safety degradation. Since the stronger utility of KD is already well known, the main takeaway reduces to \"KD incurs more safety degradation\"\n\nGiven the above weaknesses, I would expect at least a preliminary solution. As the first paper focusing specifically on this problem, even a simple proof-of-concept approach, ideally grounded in the analysis in Sec. 4, would make the contribution more actionable and inspiring for follow-up work.\n\nIn Fig. 2, while the evaluation pipeline is clear, the 2nd and 4th blocks contain redundant information.\n\nFig. 3 is hard to follow; the KD types are not consistently shown across all models.\n\nAll figures and tables: Provide more experimental-setting details and add a concise descriptive sentence to guide the reader."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w5FUgmvWP4", "forum": "AEKji3PwD9", "replyto": "AEKji3PwD9", "signatures": ["ICLR.cc/2026/Conference/Submission22374/Reviewer_UdH2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22374/Reviewer_UdH2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095687320, "cdate": 1762095687320, "tmdate": 1762942189924, "mdate": 1762942189924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that fine-tuning language models using knowledge distillation makes them compromise their safety performance more than fine-tuning them on standard fine-tuning datasets For knowledge distillation the authors utilize different experimental setups: hard label distillation, soft label distillation with forward KL divergence, reverse KL divergence, and JS divergence. Building on this observation, the authors argue that practitioners should be careful while using knowledge distillation for fine-tuning smaller models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors compare multiple baselines of different ways to perform knowledge distillation. They also consider multiple tasks and open source their code for reproducibility."}, "weaknesses": {"value": "It is not clear why distillation should lead to larger decrease in model’s safety performance as compared to standard fine-tuning. It seems that the key underlying reasons might be related to the distribution shift between the distillation and the SFT datasets. For some reason the SFT datasets seem to be difficult to train with (as the utility doesn't increase much on in-distribution datasets in table-3). It is also not clear why authors observe degradations in utility as well as safety performance (in some cases) in Table-3 on training using SFT. This might be because learning the SFT distribution is difficult for the model. Overall, I think the authors should look at the utility vs safety trade-off which they observe (in Fig.1 and Table-3) instead of highlighting the larger degradation in safety performance on using knowledge distillation. A fair comparison should have similar utility.\n\nIt would be great if the authors can bring some more insights on why they think knowledge distillation could hamper safety performance more than SFT. Currently, it is not clear as in case of using hard labels for knowledge distillation (KD), both SFT and KD would use the same training loss. I think deeper investigation is needed in this direction to make the claims of this paper more grounded."}, "questions": {"value": "It would be great if the authors can try to use hard label distillation between different families of models. This can help them give effects similar to SFT.\n\nI would recommend that the authors should try to compare their numbers for the same utility performance. Then their results can give a stronger signal.\n\nPerhaps reframing their results as demonstrating safety-utility trade-off could be more interesting. This has also been observed recently in a few other works [1]\n\n[1] The Jailbreak Tax: How Useful are Your Jailbreak Outputs?  (https://arxiv.org/abs/2504.10694)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IK8sCRQyNZ", "forum": "AEKji3PwD9", "replyto": "AEKji3PwD9", "signatures": ["ICLR.cc/2026/Conference/Submission22374/Reviewer_sdkg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22374/Reviewer_sdkg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762274307178, "cdate": 1762274307178, "tmdate": 1762942189039, "mdate": 1762942189039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}