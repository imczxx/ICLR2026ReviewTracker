{"id": "vIPnw1DiaQ", "number": 19535, "cdate": 1758297033657, "mdate": 1763719166334, "content": {"title": "$\\pi$-CoT: Prolog-Initialized Chain-of-Thought Prompting for Multi-Hop Question-Answering", "abstract": "Chain-of-Thought (CoT) prompting significantly enhances large language models' (LLMs) problem-solving capabilities, but still struggles with complex multi-hop questions, often falling into circular reasoning patterns or deviating from the logical path entirely. \nThis limitation is particularly acute in retrieval-augmented generation (RAG) settings, where obtaining the right context is critical. \nWe introduce **P**rolog-**I**nitialized **C**hain-**o**f-**T**hought ($\\pi$-CoT), a novel prompting strategy that combines logic programming's structural rigor with language models' flexibility.  $\\pi$-CoT reformulates multi-hop questions into Prolog queries decomposed as single-hop sub-queries. These are resolved sequentially, producing intermediate artifacts, with which we initialize the subsequent CoT reasoning procedure. Extensive experiments demonstrate that $\\pi$-CoT significantly outperforms standard RAG and in-context CoT on multi-hop question-answering benchmarks.", "tldr": "", "keywords": ["large language models", "prompting", "retrieval augmented generation", "multi-hop question-answering", "long context", "reasoning", "Prolog", "logic programming"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54951ccb4ea5c81513667bd3f4f8e100a36c0ec5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents \\pi-CoT, a prompting framework that has an LLM first generate Prolog queries from multi-hop natural language questions, then execute each via SLICE to build a KB. SLICE essentially calls LLM for fact extraction or verification. Experiments on multiple benchmarks report some performance improvements over baselines due to the symbolic prolog-guided decomposition and reasoning structures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, the program framing is motivating, and the idea of combining symbolic scaffolding with Prolog queries with LLM for knowledge extraction and reasoning is conceptually straightforward. The intermediate Prolog queries and per-hop passages improve interpretability and reliability over the reasoning trace. Evaluation across multiple datasets establishes the validity of the method. The analysis in Figure 3 particularly indicates the method's stability even as the reasoning step increases."}, "weaknesses": {"value": "1. There already exist many similar approaches generating Prolog from LLMs for improving arithmetic reasoning and multi-hop QA, like \"Reliable Reasoning Beyond Natural Language\", \"ProSLM: A Prolog Synergized Language Model for explainable Domain Specific Knowledge Based Question Answering\", \"Arithmetic Reasoning with LLM: Prolog Generation & Permutation\", \"Thought-Like-Pro: Enhancing Reasoning of Large Language Models through Self-Driven Prolog-based Chain-of-Though\". A comparison or discussion with these works is missing in the paper. \n2. In open-domain results (Table 1, 2) and in-context datasets (Table 4a), \\pi-CoT does not significantly outperform the baselines. Why do the gold passages also diminish the value of Prolog's symbolic guidance?\n3. No ablation studies to isolate contributions of components like Prolog and SLICE chaining. Consider adding an ablation by removing specific components from the prompt when generating the final answer (Section 4.3). \n4. It appears that the predicate definitions, question templates, and statement templates are LLM‑generated with few-shot examples. How often are these definitions incorrect, and how would this affect the downstream performance?"}, "questions": {"value": "1. See weakness for details.\n2. Issues in writing: Tables 1–2 label the proposed method as “Memento (Ours)” rather than π‑CoT; “Tukey BSD” is likely a typo for Tukey HSD; typo \"Quirell\" at line 284"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vKKLUFcAvV", "forum": "vIPnw1DiaQ", "replyto": "vIPnw1DiaQ", "signatures": ["ICLR.cc/2026/Conference/Submission19535/Reviewer_42qM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19535/Reviewer_42qM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761506513178, "cdate": 1761506513178, "tmdate": 1762931424237, "mdate": 1762931424237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes $\\pi$-CoT (Prolog-Initialized Chain-of-Thought), a prompting strategy designed to enhance large language models’ (LLMs) reasoning in multi-hop question-answering tasks.\n\n$\\pi$-CoT first translates natural language questions into Prolog queries that decompose reasoning into a sequence of sub-queries. Each sub-query is executed through a SLICE module (Single-step Logical Inference with Contextual Evidence), which uses the LLM to extract or verify relevant facts from unstructured text and update a symbolic knowledge base. The intermediate results including facts, retrieved passages, and natural-language notes are then used to initialize the final chain-of-thought reasoning step, combining symbolic rigor with neural flexibility.\n\nExperiments on HotpotQA, 2WikiMultiHopQA, MuSiQue, and PhantomWiki show consistent or improved performance over RAG, IRCoT, and HippoRAG baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[S1] Interesting conceptual perspective. The perspective of reliable decomposition with formal method is interesting. It offers a principled way to constrain reasoning trajectories while keep the subtasks manageable for LLMs during multi-hop inference.\n\n[S2] Writing quality is good. The paper is in general well-written and easy to follow. It also appropriately situates itself among prior works (e.g., IRCoT, Self-Ask, GraphRAG, HippoRAG 2).\n\n[S3] Comprehensive evaluation. The evaluation is done on both real-world datasets (HotpotQA, MuSiQue, 2WikiMultiHopQA) and synthetic multi-hop dataset (PhantomWiki), which provides insights from both realistic and controlled scenarios."}, "weaknesses": {"value": "[W1] Improvement can be inconsistent across datasets. On real-world datasets (e.g., HotpotQA, MuSiQue), $\\pi$-CoT performs comparably to baselines, with statistical significance only on certain datasets like 2WikiMultiHopQA. The claimed “significant outperforming” does not hold uniformly. Also, I think it would be good to also make clear that only prompting-based methods are compared in the table. As latest SOTA on the datasets are way higher. E.g., finetuned approaches on HotpotQA is ~10% higher. (https://hotpotqa.github.io)\n\n[W2] Potential high cost. The approach expands every query into multiple Prolog sub-queries, and each step involves a separate LLM call plus intermediate reasoning tokens. Although Tab. 2 reports retriever efficiency, the number of total LLM calls and token usage per question is not quantified. This may make $\\pi$-CoT expensive. It would be great that analyses can be provided on that front. \n\n[W3] Dependency on accurate semantic parsing. The pipeline critically relies on the LLM’s ability to correctly translate NL questions into Prolog queries and definitions (Sec. A.1). When the LLM fails to produce the right predicates or variable bindings, downstream steps will propagate errors, potentially cancelling out the benefits."}, "questions": {"value": "[Q1] How many total LLM calls or tokens are required per question (vs. standard RAG or IRCoT)? Could parts of the pipeline (e.g., deterministic Prolog execution) be done without invoking the LLM?\n\n[Q2] Are all intermediate steps truly necessary in the LLM prompt for final performance? Have you tried skipping fact-verification or compressing intermediate notes?\n\n[Q3] For queries like “Who is the female Polish scientist who won the Nobel Prize in 1903”, if the query was decomposed to q1= woman(X), q2=scientist(X) etc, then the intermediate states could contain too many entities. How is this type of token explosion handled?\n\n[Q4] I think there might be a typo in the method name in Tables 1 and 2, I think the name Memento was not used elsewhere in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Me7Jm1689U", "forum": "vIPnw1DiaQ", "replyto": "vIPnw1DiaQ", "signatures": ["ICLR.cc/2026/Conference/Submission19535/Reviewer_ydFc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19535/Reviewer_ydFc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709368703, "cdate": 1761709368703, "tmdate": 1762931423655, "mdate": 1762931423655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces π-CoT (Prolog-Initialized Chain-of-Thought), a prompting strategy that integrates Prolog into LLM reasoning. It decomposes complex multi-hop questions into Prolog sub-queries, each solved step-by-step using the SLICE (Single-step Logical Inference with Contextual Evidence) module, and uses the resulting symbolic traces to initialise the final CoT reasoning step. Notably, both the generation of Prolog sub-queries and the SLICE procedure (translation and answering of sub-queries) are performed by LLMs. The authors evaluate their method on the HotpotQA, 2WikiMultiHopQA, MuSiQue, and PhantomWiki datasets."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents an interesting approach to addressing the limitations of combining CoT and RAG by incorporating neural-symbolic reasoning through the use of Prolog.\n- The paper is clearly written, with well-organised explanations and helpful illustrations."}, "weaknesses": {"value": "- The experimental setup appears somewhat arbitrary and selective. For example, in Table 3, the retrieval model differs from that used in Tables 1 and 2. Is there a specific reason for this? It seems that the baselines in Tables 1 and 2 could also be evaluated under the retrieval model of Table 3 for a fairer comparison. Similarly, in Section 5.2 / Table 4, why are the PW-S and PW-M datasets not used in the experiments of Tables 1–3?\n\n- The paper lacks robust analysis regarding the Prolog component. Were the processes of generating, translating, and answering Prolog sub-queries executed without errors? Did the authors observe any parsing or execution errors (as they are done by LLMs) during these steps?\n\n- The empirical evidence for Prolog improving multi-hop reasoning is limited. In Table 1, π-CoT’s advantage over IRCoT on HotpotQA diminishes on MuSiQue. As the authors note (line 374), prior work has shown that many HotpotQA examples can be solved with single-hop reasoning, whereas MuSiQue was explicitly designed to mitigate this issue. Given that, is there sufficient evidence that Prolog-based reasoning truly benefits multi-hop reasoning? Alternatively, could one interpret that while Prolog offers structured neuro-symbolic reasoning, it may introduce constraints that limit performance on multi-hop tasks (for instance, through error propagation across sub-query resolutions)?"}, "questions": {"value": "- In Table 2, the authors measure efficiency solely by the number of retriever calls. However, does the proposed method increase the overall computational cost compared to other RAG + CoT baselines, considering the cost of generating Prolog queries and resolving each sub-query through the LLM, as well as the expanded input length and generation overhead? If so, should it perhaps be compared not only to standard CoT but also to inference-time intervention methods with similar computational budgets?\n\n- Why are statements evaluated as false simply ignored? Could incorporating them as additional context in the final CoT reasoning be beneficial?\n\n- (Minor) In Tables 1 and 2, the proposed method is labelled as Memento (Ours)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4EULLA9boS", "forum": "vIPnw1DiaQ", "replyto": "vIPnw1DiaQ", "signatures": ["ICLR.cc/2026/Conference/Submission19535/Reviewer_dNEJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19535/Reviewer_dNEJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965655630, "cdate": 1761965655630, "tmdate": 1762931423089, "mdate": 1762931423089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper improves the multi-hop reasoning abilities of LLMs by using a prolog inspired approach. They propose pi-CoT which first translates the reasoning question into a prolog program and then executes the prolog program in a step-by-step manner using their approach called SLICE which combines LLM-based reasoning with the semantics of prolog execution. They show that pi-CoT outperforms baselines on several datasets due to its ability to better handle multi-hop multi-branch reasoning than baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is clearly written.\n* The method is novel and is a useful way to combine the benefits of a symbolic system (prolog) with the knowledge and natural language reasoning abilities of LLMs.\n* Results are presented with variances and appear mostly significant."}, "weaknesses": {"value": "* The method’s dependence on prolog potentially limits this method to a specific set of problems.\n* It is not clear what Memento refers to in the tables. Bolding in the tables is also confusing. This makes the results very hard to understand.\n* Cost/latency is not evaluated."}, "questions": {"value": "* How does the cost of pi-CoT compare to baselines? Does it perform more RAG lookups or more LLM inference calls than the baselines?\n* How often were errors due to incorrect formulation of the problem as prolog in the initial step? For problems that are harder to formalize as prolog, this method will inevitably fail.\n* How are concepts like negation handled? This seems like it could lead to a blow up of the knowledge base if the prolog program is not written in a smart way."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hbwZxV6rNA", "forum": "vIPnw1DiaQ", "replyto": "vIPnw1DiaQ", "signatures": ["ICLR.cc/2026/Conference/Submission19535/Reviewer_iYvj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19535/Reviewer_iYvj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980474674, "cdate": 1761980474674, "tmdate": 1762931422635, "mdate": 1762931422635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all the reviewers for their detailed and constructive feedback on our work. We summarize our additions as follows:\n\n## Prolog Analysis\n1. **Figure 4: Contributions of the Prolog component to $\\pi$-CoT**  \n\nWe analyze how the Prolog component affects the downstream accuracy of $\\pi$-CoT on the open-domain QA experiment of Table 1.\n\n2. **Table 5: Contributions of passages, notes, and Prolog answer in the final CoT prompt to $\\pi$-CoT**\n\nWe remove one component (passages, notes, Prolog answer) each time in order from the $\\pi$-CoT prompt and observe that the performance drops monotonically as we remove more. This suggests that _all_ the components in our final prompt helps to guide the LLM towards the final answer via CoT reasoning.\n\n| Method | HotpotQA |  | 2WikiMultiHopQA |  | MuSiQue |  |\n|:-------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|  | **EM** $\\uparrow$ | **F1** $\\uparrow$ | **EM** $\\uparrow$ | **F1** $\\uparrow$ | **EM** $\\uparrow$ | **F1** $\\uparrow$ |\n| $\\pi$-CoT | **42.0** | **59.1** | **49.4** | **57.5** | **15.2** | **25.7** |\n| $\\quad$ w/o Passages | 37.2 | 54.4 | 48.4 | 56.3 | 12.6 | 23.3 |\n| $\\quad\\quad$ w/o Notes | 34.0 | 49.6 | 47.8 | 55.7 | 12.4 | 22.5 |\n| $\\quad\\quad\\quad$ w/o Prolog Answer | 19.2 | 24.1 | 1.0 | 1.0 | 3.2 | 4.2 |\n\n3. **Table 9: Detailed analysis of Prolog error modes**\n\n| **Error Type** | **HotpotQA** | **2WikiMultiHopQA** | **MuSiQue** |\n|:-------|:--------:|:--------:|:--------:|\n| Parsing: Prolog query parsing error | 0.8% | 0% | 0.8% |\n| Parsing: Execution parsing error | 0.8% | 0% | 0.8% |\n| Execution: Intermediate predicate existence error | 16.2% | 19.8% | 37.4% |\n| Execution: Final predicate existence error | 9.2% | 0.2% | 0.6% |\n| **Total errors** | **27.0%** | **20.0%** | **39.6%** |\n\n## Computational Cost\n\n1. **Table 2 (updated): BM25 queries, LLM calls, and total tokens in open-domain QA**\n2. **Table 7: Detailed breakdown of total token cost (prompt and completion tokens) on open-domain QA**\n\nWe report mean ± 1 standard error number of prompt tokens (in thousands) and completion tokens (in thousands) for the results. We use the vLLM inference engine with prefix caching enabled and report the number of cached tokens in parentheses next to the prompt tokens.\n\n| Method | HotpotQA |  | 2WikiMultiHopQA |  | MuSiQue |  |\n|:-------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|  | **Prompt** | **Completion** | **Prompt** | **Completion** | **Prompt** | **C** |\n| Standard RAG | 3.46 (0.032) | 0.159 | 3.63 (0.032) | 0.106 | 2.21 (0.073) | 0.136 |\n| Self-Ask | 14.5 (10.5) | 0.402 | 15.7 (10.9) | 0.682 | 11.4 (9.06) | 0.750 |\n| IRCoT | 62.3 (51.5) | 0.047 | 59.2 (44.6) | 0.053 | 45.3 (38.1) | 0.057 |\n| $\\pi$-CoT (Ours) | 18.0 (5.07) | 0.483 | 21.1 (4.21) | 0.543 | 14.4 (3.28) | 0.654 |\n\n3. **Table 8: Computational cost of in-context QA.** \n\nWe report mean ± 1 standard error number of prompt tokens (in thousands), completion tokens (in thousands), and LLM calls for the results. We use the vLLM inference engine and report the number of cached tokens in parentheses next to the prompt tokens when prefix caching was enabled.\n\n(a) Real-world (Wikipedia-based) multi-hop QA datasets\n\n| Method | HotpotQA |  |  | 2WikiMultiHopQA |  |  | MuSiQue |  |  |\n|:-------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|  | **Prompt** | **Completion** | **Calls** | **Prompt** | **Completion** | **Calls** | **Prompt** | **Completion** | **Calls** |\n| _Llama-3.3-70B-Instruct_ | | | | | | | | | |\n| CoT | 2.68 | 0.110 | 1 | 2.10 | 0.0845 | 1 | 3.43 | 0.116 | 1 |\n| $\\pi$-CoT (Ours) | 12.4 | 0.412 | 4.37 | 11.0 | 0.437 | 4.47 | 16.1 | 0.553 | 4.90 |\n| _DeepSeek-R1-Distill-Qwen-32B_ | | | | | | | | | |\n| CoT | 2.77 | 0.305 | 1 | 2.20 | 0.298 | 1 | 3.57 | 0.520 | 1 |\n| $\\pi$-CoT (Ours) | 11.1 | 1.66 | 4.09 | 9.59 | 1.55 | 4.28 | 12.3 | 1.95 | 4.65 |\n\n(b) Synthetic multi-hop QA datasets\n\n| Method | PW-S |  |  | PW-M |  |  |\n|:-------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|  | **Prompt** | **Completion** | **Calls** | **Prompt** | **Completion** | **Calls** |\n| _Llama-3.3-70B-Instruct_ | | | | | | |\n| CoT | 8.12 (8.09) | 0.400 | 1.00 | 68.7 (68.6) | 0.375 | 1 |\n| $\\pi$-CoT (Ours) | 174 (173) | 1.59 | 23.9 | 2800 (2754) | 2.6 | 40 |\n| _DeepSeek-R1-Distill-Qwen-32B_ | | | | | | |\n| CoT | 8.30 (8.26) | 1.42 | 1 | 70.8 (70.6) | 1.13 | 1 |\n| $\\pi$-CoT (Ours) | 234 (207) | 7.28 | 21.0 | 1260 (1230) | 7.09 | 16.8 |\n\n## Improved Presentation\n\n1. We apologize for the typo \"Memento\" in our previous tables. It should be replaced with $\\pi$-CoT.\n2. Appendix G: We include 5 randomly chosen examples of Prolog query and definitions generation from HotpotQA, 2WikiMultiHopQA, MuSiQue, and PhantomWiki.\n3. Appendix H: We include 1 example from HotpotQA, 2WikiMultiHopQA, MuSiQue to illustrate the complete $\\pi$-CoT workflow."}}, "id": "JrcgBm9bcv", "forum": "vIPnw1DiaQ", "replyto": "vIPnw1DiaQ", "signatures": ["ICLR.cc/2026/Conference/Submission19535/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19535/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19535/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763718975796, "cdate": 1763718975796, "tmdate": 1763718975796, "mdate": 1763718975796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}