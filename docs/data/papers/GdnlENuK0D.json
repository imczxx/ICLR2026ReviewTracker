{"id": "GdnlENuK0D", "number": 767, "cdate": 1756817502074, "mdate": 1759898243139, "content": {"title": "BiMoE：Pushing the Limit of Post-Training Quantization for MoE-based LLMs", "abstract": "Large language models (LLMs) with Mixture-of-Experts (MoE) architectures have achieved remarkable progress in natural language processing, yet their massive memory and compute costs hinder practical deployment. Binarization, which compresses model weights to 1 bit, yields extreme efficiency, offers an extreme efficiency advantage. However, existing methods that primarily target dense LLMs are not well suited to address MoE-specific quantization challenges, including redundant expert representations, task-unaware weight-importance scoring, and quantization-induced expert-shift. To this end, we propose BiMoE, the first binarization framework tailored for MoE-based LLMs. BiMoE is built on three core innovations: 1) using joint SVD decomposition to reduce cross-expert redundancy; 2) integrating global loss gradients into local Hessian metrics to enhance weight importance estimation; 3) introducing an error constraint guided by the input null space to mitigate routing distortion. Notably, BiMoE achieves these optimizations while incurring no additional storage overhead, striking a balance between efficiency and model performance. Extensive experiments demonstrate that BiMoE consistently outperforms state-of-the-art binary methods across multiple MoE-based LLMs and benchmarks. For example, on Qwen3-30B-A3B, BiMoE reduces perplexity by 52.2\\%, improves average zero-shot performance by 43.4\\%, achieves over 2 × inference speedup, and further shortens quantization time. The code is available at https://anonymous.4open.science/r/BiMoE-CADF/.", "tldr": "", "keywords": ["MoE-based LLMs", "Binarization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7072122d2ee49045335bbcf06521e49d3c8af0ac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a binarization framework tailored for MoE models called BiMoE. It identifies three major challenges unique to MoE binarization, including 1) cross-expert redundancy, 2) task-unaware importance estimation, and 3) quantization-induced expert shift, and addresses them through three key components: 1) Cross-Expert Joint Decomposition (CEJD), which employs shared SVD to reduce redundant expert parameters; 2) Global Loss-Aligned Saliency (GLAS), which integrates global loss gradients with local Hessians for task-aware weight importance; and 3) Null-space Guided Expert-Shift Suppression (NGES), which constrains quantization errors within routing-insensitive subspaces. Experiments on multiple open-source MoE LLMs and several zero-shot evaluation sets show that BiMoE could achieve over 2x speedup, and remarkably better algorithm performances compared with other baseline MoE binarization methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides novel and well-supported empirical observations on the unique challenges of binarizing MoE-based LLMs, such as expert redundancy and expert-shift. These analyses are both insightful and foundational, offering valuable guidance for future research on quantization for MoE models\n- The proposed BiMoE framework is theoretically grounded and technically rich, integrating principled formulations (joint SVD, gradient-aligned Hessian, and null-space projection) into a coherent and elegant solution. The method is well-presented, with clear motivation, derivations, and intuition throughout."}, "weaknesses": {"value": "While the proposed method is theoretically well-founded and demonstrates strong results on standard benchmarks, the evaluation scope is limited to relatively simple and dated zero-shot tasks (e.g., HellaSwag, LAMBADA, PIQA) with limited context length (4096). These benchmarks, while conventional, do not fully capture the diverse behaviors of quantized MoE-based LLMs. Previous work [[1]](https://arxiv.org/abs/2402.18158) has shown that **quantization effects vary significantly across task types**, and issues may emerge in more challenging settings such as long-context understanding, multi-turn dialogue, or mathematical reasoning, which are not covered in this work. Consequently, the current experimental evaluation may underestimate potential weaknesses of the proposed method, and a more comprehensive assessment across advanced tasks would be necessary to convincingly demonstrate its robustness and general applicability."}, "questions": {"value": "Could you further provide additional evaluation results on more advanced tasks (like long-context tasks and reasoning tasks), to justify the proposed method's general applicability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3lmPir5XBE", "forum": "GdnlENuK0D", "replyto": "GdnlENuK0D", "signatures": ["ICLR.cc/2026/Conference/Submission767/Reviewer_BkZp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission767/Reviewer_BkZp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761191541049, "cdate": 1761191541049, "tmdate": 1762915599659, "mdate": 1762915599659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents BiMoE, an MoE-aware compression framework for LLMs that includes post-training binarization. It addresses three MoE-specific challenges: (i) Cross-Expert Joint Decomposition (CEJD) to remove inter-expert redundancy before quantization, (ii) Global Loss-Aligned Saliency (GLAS) for loss-aware calibration, and (iii) Null-space-Guided Expert-Shift Suppression (NGES) to mitigate routing drift due to the quantization error."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Clear MoE-specific design that jointly tackles redundancy, saliency misalignment, and routing instability (CEJD+GLAS+NGES).\n\nNo fine-tuning required, making the method practical for very large MoE models.\n\nConsistent accuracy improvements over prior 1-bit baselines on the reported zero-shot suite.\n\nSystem-level reporting (memory reduction, inference throughput) and ablations that isolate each component and show combined benefits.\n\nGenerally clear writing and informative figures; easy to follow overall."}, "weaknesses": {"value": "1. **Method relation/novelty:** CEJD appears conceptually close to another SVD-based expert compression work [1]. It will strengthen the paper to clarify the novelty beyond integrating quantization to better highlight the CEJD’s specific contribution.\n\n2. **Effective bitwidth fairness in accuracy comparisons:** With the shared backbone at 8-bit, the effective bitwidth seems to exceed that of 1-bit baselines by a non-negligible margin. To help disentangle accuracy–bitwidth trade-offs, it would be helpful to include configurations with a lower-bit backbone (e.g., 2-bit) that match baseline effective bit budgets; such a matched-budget comparison may be most informative.\n\n3. **Evaluation benchmark:** The results focus on a small zero-shot suite and WikiText-2 perplexity. Given typical MoE use in reasoning/assistant scenarios, task-specific evaluations (math/code/QA; e.g., AIME, GPQA, LiveCodeBench, BFCL) and, where appropriate, instruction-following/chat assessments should be included.\n\n4. **Positioning with respect to minimal-tuning 1-bit methods:** While the method is practical in not requiring fine-tuning, there are binary quantization approaches that allow minimal fine-tuning and report substantially higher scores [2]. Including initialization time and accuracy/latency comparisons against such methods could further substantiate the benefits of the fine-tuning-free setting.\n\n5. **CEJD vs. other SVD-based MoE compression:** Since the quantization methods (GLAS, NGES) can be also implemented to other SVD-based compression that shares a low-rank matrix, a comparison of CEJD with other SVD-based MoE compression methods [3] would have clarify CEJD’s effectiveness and how it integrates with quantization.\n\n[1] Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging, Lujun Li, Zhu Qiyuan et al., https://arxiv.org/abs/2506.23266.\n\n[2] The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, Shuming Ma, Hongyu Wang et al., https://arxiv.org/abs/2402.17764. (or recently, BitNet b1.58 2B4T Technical Report, Shuming Ma, Hongyu Wang et al., https://arxiv.org/abs/2504.12285)\n\n[3] MoE-SVD: Structured Mixture-of-Experts LLMs Compression via Singular Value Decomposition, Wei Li, Lujun Li et al., ICML 2025."}, "questions": {"value": "Mainly listed in the weakness. Below are the additional questions.\n\n1. Have you considered using the backbone as \\Simga V and quantizing the U, where the SVD is applied to the row-wise concatenation of weights (W^{(1)}, ... W^{(n)}), as a reverse option compard to sharing U \\Sigma and quanting the V? \n\n2. Are the models are calibrated per task within the seven zero-shot benchmarks, or only once on a single corpus in table 3?\n\n3. Related to Weakness 5, what is the wall-clock cost for few dozens of billion parameter models (e.g., Qwen3-30B-A3B, GPT-OSS-20B)?\n\n4. Minor notation: I suggest using \\top for transpose instead of T."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1y7MutAkdb", "forum": "GdnlENuK0D", "replyto": "GdnlENuK0D", "signatures": ["ICLR.cc/2026/Conference/Submission767/Reviewer_bxMB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission767/Reviewer_bxMB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553988917, "cdate": 1761553988917, "tmdate": 1762915599521, "mdate": 1762915599521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BiMoE, a binarization framework for Mixture-of-Experts (MoE) large language models. The authors identify three challenges: expert redundancy, task-unaware weight importance scoring, and quantization-induced expert-shift. They propose three components: Cross-Expert Joint Decomposition (CEJD), Global Loss-Aligned Saliency (GLAS), and Null-Space Guided Expert-Shift Suppression (NGES). Experiments show improvements over BiLLM and ARB-LLM baselines on several MoE models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Compressing MoE models is an important practical problem given their memory overhead.\n\n* The paper evaluates on multiple MoE architectures (OLMoE, Qwen, DeepSeek) and provides detailed ablation studies.\n\n* The method is well-motivated and presented with nice figures and tables."}, "weaknesses": {"value": "* While conceptually interesting, the performance of binarized models in this work suffer from significant performance drop (e.g. as shown in Figure 1). This makes the applicability of the proposed method questionable. \n\n* The paper only compares against extreme 1-bit methods and shows these methods fail catastrophically. This might create a strawman comparison. I would love to see comparisons with: 1) 2-bit, 4-bit quantization methods like GPTQ or AWQ, which are more practical; 2) Mixed-precision quantization specifically designed for MoE [1] [2].\n\n* I love Table 8's intuitive demonstration. Would it be possible to scale up such evaluation for testing if the output makes sense? For example, we can use LLMs to systematically evaluate the generated response from the binarized MoEs to see how many of them make sense.\n\n[1] Li, P., Jin, X., Tan, Z., Cheng, Y. and Chen, T., 2024. QuantMoE-Bench: Examining Post-Training Quantization for Mixture-of-Experts. arXiv preprint arXiv:2406.08155.\n\n[2] Duanmu, H., Li, X., Yuan, Z., Zheng, S., Duan, J., Zhang, X. and Lin, D., 2025. MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design. arXiv preprint arXiv:2505.05799."}, "questions": {"value": "See above. Thank the authors for the work. I'm more than happy to increase my rating if my questions are adequately addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "o2ve3w5r9h", "forum": "GdnlENuK0D", "replyto": "GdnlENuK0D", "signatures": ["ICLR.cc/2026/Conference/Submission767/Reviewer_ynLx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission767/Reviewer_ynLx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705160492, "cdate": 1761705160492, "tmdate": 1762915599349, "mdate": 1762915599349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BiMoE, the first binarization framework specifically designed for Mixture-of-Experts (MoE) large language models. The authors identify three key challenges in binarizing MoE models: expert redundancy, task-unaware weight importance scoring, and quantization-induced expert-shift. BiMoE addresses these through three components: (1) Cross-Expert Joint Decomposition (CEJD) using SVD to extract shared high-precision backbones, (2) Global Loss-Aligned Saliency (GLAS) incorporating task-level gradients into Hessian-based importance metrics, and (3) Null-Space Guided Expert-Shift Suppression (NGES) constraining quantization errors to routing-insensitive subspaces. Experiments on six MoE models demonstrate substantial improvements over existing binary PTQ methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This is a study of binarization for MoE-based LLMs, addressing a timely problem as MoE architectures become more prevalent. The identification of three MoE-specific challenges is insightful, particularly the expert-shift problem illustrated in Figure 3 and Table 1. \n2. The CEJD approach cleverly exploits cross-expert similarity through joint SVD decomposition, validated by CKA analysis in Figure 2. The experimental evaluation is comprehensive, covering six diverse MoE models across multiple benchmarks with thorough ablation studies. \n3. The plug-and-play experiments demonstrate good generalizability. The achieved 2× speedup and 90% memory reduction while maintaining reasonable performance is practically significant for deployment."}, "weaknesses": {"value": "1. The evaluation relies on simple zero-shot reasoning tasks (ARC, HellaSwag, PIQA, etc.) that may not fully capture model capabilities. Including more challenging benchmarks like MMLU, MATH, and code generation tasks (MBPP, EvalPlus) would better demonstrate the method's effectiveness across diverse domains.\n2. The manual hyperparameter tuning (λ=0.2) is acknowledged as a limitation, and generalization across architectures is unclear. Please share more results about the hyperparameter.\n3. The evaluation relies on simple zero-shot reasoning tasks (ARC, HellaSwag, PIQA, etc.) that may not fully capture model capabilities. Including more challenging benchmarks like MMLU, MATH, and code generation tasks (MBPP, EvalPlus) would better demonstrate the method's effectiveness across diverse domains. Also, we can evaluate on some instruction following task, if the model is an instruction finetuned model.\n4. Miss some citation related to MoE quant: moequant, Examining post-training quantization for mixture-of-experts: A benchmark, etc."}, "questions": {"value": "See weakness. If weakness can be solved, I will raise scores."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uFCxJpX35j", "forum": "GdnlENuK0D", "replyto": "GdnlENuK0D", "signatures": ["ICLR.cc/2026/Conference/Submission767/Reviewer_ZK89"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission767/Reviewer_ZK89"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794637152, "cdate": 1761794637152, "tmdate": 1762915599181, "mdate": 1762915599181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}