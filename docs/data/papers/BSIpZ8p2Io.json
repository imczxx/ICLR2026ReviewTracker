{"id": "BSIpZ8p2Io", "number": 23285, "cdate": 1758341687895, "mdate": 1759896822736, "content": {"title": "Towards Decision Focused Learning for Sparse and Weakly Supervised Environments", "abstract": "Decision-focused learning (DFL) integrates machine learning and optimisation by training predictive models to directly optimise decision quality.\n        However, DFL typically depends on access to accurate ground-truth targets such as objective function parameters or optimal decisions: This is often infeasible in real-world settings where only sparse, binary feedback on decision outcomes is available.\n        \nThis work takes first steps towards formalising Decision-Focused Learning for settings where observed data is limited to such binary feedback. We propose a preliminary DFL framework that learns latent user preferences from weakly supervised binary feedback on decision outcomes. The novelty of our approach lies is in a) a ground-truth-free, differentiable surrogate loss that maps binary evaluations to decision outcomes, and b) a novel meta-learning mechanism that learns latent user preference patterns and transfers this knowledge between users to mitigate data sparsity.\n        \nOur experiments suggest that this framework can reduce decision regret by 20-fold and achieves convergence with $2.4-4\\times$ fewer data points than standard predict-then-optimise baselines. On a novel hyper-sparse real-world trip-planning feedback dataset, we show the model's ability to extract user-preference clusters from sparse data ($\\approx 1$ interaction/user). We also evaluate our model in cold-start recommendation settings and show that our decision loss correctly prioritises ranking quality, achieving $9.5$\\% higher nDCG@5 than the baseline despite $14.6$\\% higher MSE. \n        The aim of this work is to broaden the applicability of DFL and explore its potential in weakly supervised data-sparse regimes, with future work extending non-linear user-preference structure.", "tldr": "", "keywords": ["Decision Focused Learning", "Meta Learning", "Weakly Supervised Learning", "Personalised Trip Planning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be88c3529e77fbfa37efcb3237b229e614579ff0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper extends the decision-focused learning framework to scenarios involving sparse, binary supervision signals within a multi-agent system. In particular, the authors focus on recommendation tasks aimed at predicting user preferences. Based on several assumptions and approximation adaptations, they design the MPLL loss and BCE loss within a collaborative training and meta-learning framework for optimization. Experiments are conducted on both synthetic data and a trip-planning recommendation dataset to show the effectiveness of the proposed model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem under study is novel.\n2. Mathematical analyses are provided to illustrate the proposed methodology.\n3. The ablation study demonstrates the effectiveness of the proposed model components."}, "weaknesses": {"value": "1. Generally, I feel that the intuition and benefits of the proposed method are not clearly articulated. There exists a substantial body of prior work on personalization/recommendation with sparse supervised learning signals. It is therefore unclear why decision-focused learning is necessary for this problem and what specific advantages it offers. The authors introduce many modifications based on various assumptions for adaptation, and the resulting optimization algorithm appears considerably more complex than standard supervised learning approaches. Without a clear intuitive justification, this added complexity seems difficult to motivate.\n2. Some concepts lack clear and practical definitions. For example, it is not explained what the solution variable  $x$ repsent in real recommender system, or why the utility function can be defined as the sigmoid of a linear combination of the solution and the parameters being optimized. The papar says \"DFL trains the predictor so that the predicted parameters leads to an optimal output from the solver\", yet it remains unclear what the solver specifically refers to and why it is necessary.\n3. The introduction of the multi-agent setting further adds confusion. The motivation and necessity of incorporating this into the decision-focused learning framework are not well justified. It is not evident what benefits or practical implications this multi-agent design brings to recommender systems.\n4. The practical implementation of these concepts in the experiments is not clearly described. Moreover, since the code and data are not released, it is difficult for other researchers to understand, verify, or reproduce the results."}, "questions": {"value": "My main questions are outlined in the weaknesses section. They primarily concern the benefits and necessity of the proposed method, as well as the practical meaning of its underlying concepts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KFgRrCvvSJ", "forum": "BSIpZ8p2Io", "replyto": "BSIpZ8p2Io", "signatures": ["ICLR.cc/2026/Conference/Submission23285/Reviewer_pMCb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23285/Reviewer_pMCb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537950482, "cdate": 1761537950482, "tmdate": 1762942589680, "mdate": 1762942589680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles decision-focused learning (DFL) when only sparse binary feedback (“acceptable” vs “not”) is available and many users have just 1–2 interactions. It proposes an end-to-end surrogate that optimizes the quality of chosen decisions rather than predictive accuracy, combining a modified pairwise logistic loss with a conservative pseudo-label BCE so that the model learns to rank the selected solution above (or below) alternatives depending on the feedback. To handle cold start, it adds a collaborative layer: each user’s score blends their own preference vector with a distance-weighted consensus of neighbors, followed by two-phase updates that adjust both the focal user and their neighbors. The framework is validated on a personalized trip-planning task and a user-level cold-start benchmark (MovieLens-1M), where the simple Linear+DFL+Collaborative variant achieves state-of-the-art ranking (e.g., nDCG@5 ≈ 0.799) despite higher MSE, aligning training with the decision objective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method aligns training directly with the downstream decision objective by optimizing whether the actually chosen option is acceptable, rather than predicting proxy scores; it turns a single binary feedback into rich learning signal via a modified pairwise ranking loss combined with a conservative pseudo-label BCE, yielding data efficiency under sparse supervision."}, "weaknesses": {"value": "### Robustness and Ablations\nEvidence comes mainly from one trip-planning setup and a linear MovieLens variant, so generality is unclear. Reported gains may hinge on specific design choices (surrogate mix, consensus rule, data curation). Does the effect persist with other model classes, alternative decision-focused surrogates, and consensus mechanisms? How sensitive are results to neighborhood size, distance and other hyperparameters?\n\n### Scalability Concerns\nNeighbor search and consensus updates can dominate runtime and memory as users/items/dimensions grow. System behavior under load spikes and rolling updates is not characterized. What are the time/memory scaling laws at large scale? What is the end-to-end latency budget (including neighborhood rebuild frequency) for online inference and updates?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AUKhGgpF8Z", "forum": "BSIpZ8p2Io", "replyto": "BSIpZ8p2Io", "signatures": ["ICLR.cc/2026/Conference/Submission23285/Reviewer_4C8D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23285/Reviewer_4C8D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832962346, "cdate": 1761832962346, "tmdate": 1762942589187, "mdate": 1762942589187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a DFL framework for settings with only binary feedback, learning latent user preferences through a differentiable surrogate loss and meta-learning mechanism. Experiments show that it significantly reduces decision regret and performs well in data-sparse and cold-start scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem of using only binary feedback to solve DFL is interesting and important, which could be a more practical avenue for DFL.\n\n2. It also studies the collaborative training, in which the data from users with similar preferences are shared. Ablation studies show that this collaborative training mechanism helps to get a better performance."}, "weaknesses": {"value": "1. There are many typos and incorrect paragraphs in the paper. For example, the use of the notation $u(x, \\theta)$ is very confusing. In Section 3.1, $u(\\theta)$ is introduced as a general function and $f$ is defined by Eq. (3).  However, in Section 3.2, Eq. (5) provides $u(x,\\theta) = P(f(x,\\theta)=1)$. These two uses are not compatible if Eqs. (3) and (5) are taken together.  It is also unclear what the randomness in $P(f(x,\\theta)=1\\mid x,\\theta)$ refers to. If the $x, \\theta$ is given, the function $f(x,\\theta)$ should be deterministic. The typos and incorrect equations could take much effort to understand the true meaning of this paragraph.\n\n2. Also, the assumption in Section 4.1 is very weird and hard to satisfy. Since $f(x^\\*, \\theta)$ can either be 0 or 1, this assumption implies that for any $x^\\*$, either $u(x^\\*, \\hat{\\theta}) \\ge u(x_j, \\hat{\\theta})$ for all $x_j \\in X_z\\setminus \\\\{x^\\*\\\\}$, or  $u(x^\\*, \\hat{\\theta}) \\ge u(x_j, \\hat{\\theta})$ for all $x_j \\in X_z\\setminus \\\\{x^\\*\\\\}$. This is a very strict assumption. I understand that this is not a true “assumption” but rather a motivation for introducing the modified Pairwise Logistic loss and BCE loss. It would be clearer to present this assumption as a motivation instead of labeling it as an assumption."}, "questions": {"value": "1. Could the author explain why the loss minimization is related to the regret minimization? Is there any theoretical result that can show the relationship between these two goals?\n\n2. In order to achieve collaborative training, the algorithm needs to update $\\hat{\\theta}$ at each round and try to find the neighbors at each time. Is it too costly in time? In the experiment, does the author have any tricks to avoid finding neighbors for each gradient update?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P8T0XH376e", "forum": "BSIpZ8p2Io", "replyto": "BSIpZ8p2Io", "signatures": ["ICLR.cc/2026/Conference/Submission23285/Reviewer_rNw4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23285/Reviewer_rNw4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893562457, "cdate": 1761893562457, "tmdate": 1762942588898, "mdate": 1762942588898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tries to address an important problem for decision-focused learning. Prior DFL methods assume access to rich supervision information, which is not realistic in many real-world settings where only sparse, weak feedback is available. This paper proposed a new framework that DFL can work even when only sparse and binary feedback is available, which is common in many real-world decision systems. Decision regret and ranking quality, as the performance metric, make more sense than the prediction accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The identified problem is important, and the writing of the paper describes it in a natural tone.\n- Have novel theoretical results and well-rounded analysis. \n- The design of the surrogate loss is the highlight of the paper."}, "weaknesses": {"value": "- The model and setup are still confusing in different dimensions.\n- Experiments are not sufficient to address or verify the claim.\nPlease refer to the details in the Questions.\n\nI will consider raising my score after the rebuttal questions have been properly addressed. But I do think this paper requires quite a lot of editing and polishing to reach the publishable stage."}, "questions": {"value": "Literature:\n- The related works seem not to cover enough literature. Although I'm not very familiar with the DFL/PFL framework, this reminds me of a few terms, such as Smart Predict-then-Optimize (PTO), Joint Prediction and Optimization, and end-to-end learning. Would you like to compare and contrast these related literature? Is there any effort from these works in order to address noise and limited feedback/labels?\n- In your setup, you have a latent assumption on the tolerance of decision makers that is a key structure for getting your theoretical results, thus you may also want to mention this compromising/tolerance effect from other literature, such as psychological/social sciences, to support and justify your setup.\n\nModel:\n- You mentioned \"multi-agent\" setting in your introduction, but the parameter $\\theta^*$ is assumed to be fixed for all agents. How does the \"multi-agent\" make the problem more interesting/complex? In other words, what makes it necessary to introduce the multi-agent environment? \n**Add-ons:** I read Section 5, and this question has been partially addressed. But I do recommend introducing multi-agent at the beginning and mentioning that you first start with a single agent.\n- What are the assumptions of the utility function $u(x,\\theta)$ (e.g. shape of $u$ w.r.t. $x$ and $\\theta$, contuity)? \n- In equation (2), I did not quite get it. Is the $\\theta$ the ground-truth parameter? I feel it is quite confusing because $\\theta$ is usually a variable, and $\\theta^*$ is usually the optimal value of parameters. Afterwards, in line 141-142, you also mention $\\theta_i$ as the ground-truth and I become confused -- are there $N$ different ground truths? **typo**: should be $\\{\\dots\\}_{i=1}^N$, and $N$ is not defined.\n- Gradually following your description of the model, I sort of understand this is an online learning setup, but it is not stated clearly in the introduction. Until you define the regret, I have not realized that this is not an offline joint optimization and prediction problem. You may want to clarify somewhere at the beginning.\n- Line 147-148: $f(x, \\theta)$ is defined as a function w.r.t. both $x$ and $\\theta$. Unless you assume $\\theta$ is the ground truth, you need to define what is $\\theta$, otherwise, you may not include $\\theta$ as a parameter of $f$. \n- As you introduced below equation (2), the surrogate loss is to address minimizing the regret in non-trivial cases. Then in line 153 and below, you mention it is the unknown $\\theta$ that makes the regret intractable. This makes the purpose of the introduction of the surrogate loss a bit misleading. Does your setup also consider the discrete action set $\\mathcal{X}$? Does the surrogate loss help both in terms of the $\\arg\\max$ and the unknown $\\theta$?\n- In line 174-179, I am not so convinced the choice of sigmoidal function is the right nonlinear mapping to be chosen. I understand that it may provide a nice structure for the proof, but it would be good to support your choice by citing other literature or mentioning that the empirical performance is sufficient. You may want to check/compare with other mapping empirically. **typo**:  $s,b$ are not defined. I'm confused whether these noise & scaling are tuning hyperparameters.\n- A follow-up question: Is the method sensitive to the design of the surrogate loss or hyperparameters?\n- in line 197 & 206, multiple $\\mathcal{X}_{?}$ are not properly defined.\n- The resolution of Figure 1 needs to be improved. Consider outputting PDF format instead.\n- seems to Section 5.0.1 is a wrong index?\n- Can the approach be extended beyond binary feedback to other objective settings (e.g. ordinal, multiple target)? \n\nExperiments:\n- For your experiments in Section 6.1, I don't think you can produce meaningful conclusions with merely 5 data samples/users. Would you provide more explanation of why you think this suffices, or add a more convincing experiment?\n- In line 103, you mentioned your experiment contributes to \"how lightweight feedback mechanisms empower smaller organisations to adopt DFL\". Could you elaborate more specifically, along with your experiments? I don't find a very direct link there.\n- In line 21, \"transfers this knowledge between users to mitigate data sparsity\": I don't think the data sparsity can be mitigated, but the decision and learning challenges due to data sparsity could be mitigated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WM279SGZJn", "forum": "BSIpZ8p2Io", "replyto": "BSIpZ8p2Io", "signatures": ["ICLR.cc/2026/Conference/Submission23285/Reviewer_rr4b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23285/Reviewer_rr4b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984535911, "cdate": 1761984535911, "tmdate": 1762942588720, "mdate": 1762942588720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}