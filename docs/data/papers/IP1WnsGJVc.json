{"id": "IP1WnsGJVc", "number": 10895, "cdate": 1758184345599, "mdate": 1759897622205, "content": {"title": "Visual Object-Centric Counterfactual Explanations", "abstract": "Generating visually coherent and realistic counterfactual explanations is essential for understanding discriminative visual models. Existing methods often modify images at the pixel-level or within holistic latent spaces, leading to entangled changes that obscure the precise factors influencing model decisions. To address this, we introduce a novel object-centric method for visual counterfactual explanations. Our approach decomposes input images into distinct object-centric latent slots and leverages model's gradients to guide a reverse diffusion process conditioned on these slots. To maintain realism, we propose a Gaussian Mixture Model (GMM)-based regularizer that constrains counterfactuals to remain within the distribution of plausible object states, preventing unrealistic generations. Experiments on three datasets and a user study demonstrate that our object-centric approach yields significantly more interpretable and realistic counterfactuals compared to state-of-the-art baselines. Moreover, our approach shows strong generalization: when trained solely on FFHQ dataset, it successfully generates coherent counterfactual explanations on unseen CelebA-HQ data. Overall, our approach substantially advances visual counterfactual explanations by offering explicit object-level interpretability and improved quality of generation.", "tldr": "We introduce an object‑centric diffusion method that generates visual counterfactual explanations by editing object‑level representations with gradient guidance and a GMM regularizer, outperforming prior approaches and generalizing across datasets", "keywords": ["Interpretability", "Explainability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f68e5d1afff3a46cb1b719e6291ab29dc9ffeb8b.pdf", "supplementary_material": "/attachment/c55abb8a4c13d0ff7eaf0dcf6c042ff560dae8f9.zip"}, "replies": [{"content": {"summary": {"value": "To address the issue of existing methods creating unrealistic or entangled visual counterfactual explanations (CEs), this paper introduces a novel object-centric approach VOCCE. The method aims at decomposing images into distinct object-centric latent slots and uses a gradient-guided reverse diffusion process, constrained by a GMM regularizer, to achieve realistic edits that are localized to specific objects. To assess these claims, the paper presents a series of experiments, including quantitative comparisons and a user study."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper considers an important problem of constraining the changes in CEs to some semantically meaningful subset of image pixels.\n\nS2. The proposed method's novelty lies in combining latent slot diffusion with a GMM-based regularization and gradient-based slot updates within the diffusion reverse process conditioned on the classifier of interest.\n\nS3. In addition to the standard quantitative evaluation, the authors conduct a user study to assess important properties of the generated CEs, and achieve optimistc results."}, "weaknesses": {"value": "W1. My main concern relates to whether the primary premise of VOCCE is actually delivered. While I acknowledge its superiority in terms of the ELS metric, what are the actual guarantees that the resulting CE has changes only within a specific subset of the slots? From what I understand, the changes should be following the distribution of slots in the training data, but this doesn't mean that less significant slots won't be modified at all. Therefore, the changes might be more focused around regions that are more influential on the classifier, but also appear in less important regions, which contradicts locality. This can also be seen in Figure 2, e.g., columns 2, 5 and 6 for VOCCE include visible background changes.\n\nW2. The authors properly mention recent state-of-the-art solutions like DiG-IN, ACE and ECED, but never actually include them in the quantitative evaluation. This directly contradicts the claim that VOCCE *yields significantly more interpretable and realistic counterfactuals compared to **state-of-the-art** baselines*. Moreover, I believe that the paper is also missing another relevant baseline [1], which also considers localized, semantically valid edits performed within predefined regions -- a concept that resembles slots used in VOCCE.\n\nW3. An often overlooked issue in the evaluation of CEs is a lack of explanandum-related metrics. In this paper, the only such metric is the flip rate (validity). Many recent and prior works additionally include COUT [2], which focuses on classifier-related sparsity. Including this metric is important, as its role is largely orthogonal to those included in the authors' work.\n\nW4. I acknowledge that the authors mentioned the challenge of VOCCE's applicability to larger and more diverse datasets as a limitation. I believe that such an example is crucial for a paper that puts object-level semantics at its core. At least an initial study into such an example should be provided, e.g., on BDD100k [3] or ImageNet [4].\n\nW5. Missing ablation studies: what is the influence of $\\gamma$, the number of Gaussian components and the step size for the slot update on the optimization procedure?\n\nW6. Less significant but still important:\n\n1. line 101: counterfactuals refer to images with specific characteristics edited, not conditioned on the classifier. Counterfactual explanations should be mentioned here instead.\n\n2. No qualitative examples for the ClevrTex dataset.\n\n3. It would be beneficial to include qualitative examples presenting the absolute difference between the original samples and their corresponding CEs. These would visually indicate the actual locality and object-centredness of the changes.\n\n\n[1] Sobieski et al., Rethinking Visual Counterfactual Explanations Through Region Constraint, ICLR, 2025\n\n[2] Khorram et al., Cycle-Consistent Counterfactuals by Latent Transformations, CVPR, 2022\n\n[3] Yu et al., BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning, CVPR, 2020\n\n[4] Deng et al., ImageNet: A large-scale hierarchical image database, CVPR, 2009"}, "questions": {"value": "Please refer to the Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zTLMGXqsqd", "forum": "IP1WnsGJVc", "replyto": "IP1WnsGJVc", "signatures": ["ICLR.cc/2026/Conference/Submission10895/Reviewer_8sJi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10895/Reviewer_8sJi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774570493, "cdate": 1761774570493, "tmdate": 1762922103683, "mdate": 1762922103683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **VOCCE**, a method for generating visual counterfactual explanations that act on *object-centric representations* rather than entire images. By decomposing scenes into object slots and modifying only those relevant to a target label change, the approach aims to produce more interpretable and localized counterfactuals. Experiments on datasets such as CLEVRTex, FFHQ, and CelebA-HQ compare VOCCE to existing diffusion-based counterfactual methods, reporting improved realism and faithfulness, along with a user study supporting better perceptual quality."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- **Comprehensive ablation study:** The paper includes an ablation analyzing the contribution of individual loss terms and components, which helps assess design choices.  \n- **User study included:** A small-scale human study comparing generated counterfactuals is a welcome addition, showing effort to evaluate beyond automatic metrics.  \n- **Potentially useful insight:** The approach could, in principle, enable explanations that modify only the relevant object in a scene, which may aid fine-grained interpretability."}, "weaknesses": {"value": "The main limitation of the paper is **how it is written and organized**, which makes it unnecessarily difficult to follow and evaluate.  \n1. **Unclear motivation and intuition**  \n   The paper lacks an intuitive justification for why *object-centric explanations* are particularly needed or advantageous. It would help to include a concrete example (e.g., modifying only one object’s attribute) showing when such explanations outperform traditional pixel- or region-based methods.\n\n2. **Poor presentation and confusing figures.**  \n   - Figure 1 is overly complex and difficult to interpret; subfigure (a) references terms that do not appear in the diagram, and subfigure (b) is too dense, without clear explanation.  \n   - The overall architecture description is fragmented: it is unclear which components are pretrained, fine-tuned, or frozen, and when the classifier is first introduced versus reused in later sections (3.1 vs 3.2).  \n\n3. **Loss definitions are confusing.**  \n   - The slot loss is re-introduced multiple times (Sections 3.3 and 3.4), and it remains ambiguous which terms constitute the final objective versus auxiliary ones.  \n   - The training setup (e.g., what is pretrained on FFHQ, what is optimized jointly) is not clearly specified.\n\n4. **Experimental setup**  \n   - Only two datasets are used (CLEVRTex and FFHQ), and there is no qualitative analysis on CLEVRTex. Including additional datasets or broader discussion would strengthen the empirical claims.  \n   - It remains unclear how the authors chose the set for sFID evaluation.\n\n5. **Quantitative results.**  \n   - The paper does not report results for edits involving multiple labels or multiple object changes simultaneously, which could be a selling point of the approach.  \n   - The user study compares only with L-DVCE and omits other relevant baselines; no statistical significance tests are provided.  \n   - Same for the generalization experiment\n\nOverall, the **writing quality and structure** significantly hinder readability and make it difficult to understand the model, experiments, and contributions."}, "questions": {"value": "1. What specific advantage do *object-centric explanations* provide over region-based or pixel-level counterfactuals? Could you illustrate a concrete scenario where your approach succeeds but standard ones fail?  \n2. Please clarify the training procedure: which models are pretrained, frozen, or fine-tuned? What is the purpose of FFHQ pretraining?  \n3. How is the “slot loss” finally combined with classifier and reconstruction losses? A full loss equation would help.  \n4. Why are only two datasets used, and why is there no qualitative analysis on CLEVRTex? Would the approach generalize to other datasets or object domains?  \n5. How were sets selected for computing sFID?\n6. In the user study, why only compare with \\(L_{\\mathrm{DVCE}}\\)? Were significance tests (e.g., paired t-test, bootstrap) performed?  \n7. Could you simplify or redesign Figure 1 for readability and ensure all terms used in the caption appear in the diagram?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KwDXFRWEN3", "forum": "IP1WnsGJVc", "replyto": "IP1WnsGJVc", "signatures": ["ICLR.cc/2026/Conference/Submission10895/Reviewer_gb21"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10895/Reviewer_gb21"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927150061, "cdate": 1761927150061, "tmdate": 1762922103303, "mdate": 1762922103303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an object-centric method for generating realistic and interpretable visual counterfactual explanations. Unlike prior pixel-level or holistic latent approaches that produce entangled and unclear edits, which are done by firstly decomposing images into object-centric latent slots, isolating meaningful components and later use the model gradients to guide a reverse diffusion process for generating counterfactuals, While appling a GMM-based regularizer to ensure generated counterfactuals to prevent slot drifts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The use of OCL for counterfactual generation is interesting, enabling swapping of entire object rather than some abstract parts, mainting human understandability to an extent\n\n- The idea of GMM regularisation is interesting and novel\n\n- The paper is well written and easy to follow to a large extent"}, "weaknesses": {"value": "- For the GMM prior generation, there exist a few methods, like: (1) SlotVAE-like approach with learnable GMM priors and learn the parameters via training, (2) Probabilistic Slot Attention (PSA) like approach using an aggregate posterior to create an optimal prior, (3) Or the approach the authors have considered using the latents of the entire datasets and manually fitting the GMM, the implications of all three methods or atleast the discussion on three methods would add value to the paper.\n\n- The details of GMM fitting are missing. Do you consider latents from the entire dataset? Or subset? If a subset, how do you select these samples? Discuss the tractability and complexity of such a method.\n\n- More recent works on realistic counterfactual generations are not cited: High Fidelity Image Counterfactuals with Probabilistic Causal Models, Diffusion Counterfactual Generation with Semantic Abduction, and others, as they achieve more realistic-looking counterfactuals, which the authors claim do not exist\n \n- Details about the human survey aren’t clear\n\n- OCL results in isolation are not discussed in qualitative nor quantitative form"}, "questions": {"value": "- The use of OCL is limited to GMM regularisation. I’m curious to hear the author's thoughts on guiding diffusion models to generate appropriate slots themself directly. \n\n- Would like to know if the following OCL approach helped in the case of the FFHQ dataset? based on the types of edits, its bit hard to gauge, as OCL methods are not capable of capturing abstract concepts like young, smile, and so on."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a8VJtidAhV", "forum": "IP1WnsGJVc", "replyto": "IP1WnsGJVc", "signatures": ["ICLR.cc/2026/Conference/Submission10895/Reviewer_UQUx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10895/Reviewer_UQUx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976894602, "cdate": 1761976894602, "tmdate": 1762922102903, "mdate": 1762922102903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VOCCE (Visual Object-Centric Counterfactual Explanations), a diffusion-based method designed to generate realistic and interpretable counterfactual images. The method employs classifier-guided diffusion to steer samples toward a target label, gradient-based slot updates to refine object-level changes, and a Gaussian Mixture Model (GMM) prior to maintain plausibility. Experiments on ClevrTex, FFHQ, and CelebA-HQ show improved closeness, realism, and locality over prior diffusion-based counterfactual methods (DVCE, DiME, FastDiME). A user study further supports that VOCCE produces more meaningful and subtle edits, and cross-dataset tests indicate strong generalization of object-centric representations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is motivated well and I enjoy reading the introduction part.\n- It claims to be the first object-centric framework for generating visual counterfactual explanations. \n- The method is tested on three datasets, showing consistent quantitative improvements over baselines.\n- The visual results in figure 1 look realistic and with good identity preservation.\n- They show generalization of the proposed method by training on FFHQ and tested on CelebA-HQ.\n- They perform human user study which confirms benefit of their method over the baseline."}, "weaknesses": {"value": "- The main difference from prior diffusion-based counterfactual methods appears to be the introduction of slot attention for object-centric representations. Yet there are no experiments focusing on exploring how each slot captures different object and how that help with the counterfactual generation. \n- It is hard to follow the methodology. For example, it is not clear how different components of the method is trained. Are they trained separately or jointly? The methodology section needs to be improved. And maybe an algorithm summarizing the method step by step would be helpful for clarity. Unfortunately, in my opinion, figure 1 is confusing and not very helpful for understanding.\n- One key contribution of this paper is that it operates on slot-based representations, where “each slot encodes a distinct object”. However, this is not experimentally verified in this paper. I would expect some experiments where they visualize which object a slot captures. And how this can help with the counterfactual quality. For example, for mouth open 0->1, is there a slot that represents mouth? When how does that slot representation changes during the counterfactual generation process? And how do other slots change?\n- The paper focuses on generating visual counterfactual explanations, yet it presents only one figure with qualitative results on CelebA-HQ. Since the main contribution is visual interpretability, it would be helpful to include more qualitative examples, ideally across multiple datasets and target attributes, to better demonstrate the quality of the generated counterfactuals.\n- It is unclear whether the same classifier is used both to guide the counterfactual generation and to evaluate validity. If so, the reported success rate may be biased, since the classifier’s own errors or overconfidence could make invalid counterfactuals appear “successful.” It would be important to clarify this setup and, if possible, evaluate validity using an independent or held-out classifier to ensure that the measured success reflects genuine semantic changes rather than the guiding model’s bias."}, "questions": {"value": "- Can you provide more clear description of the methodology, and maybe provide an algorithm summarizing the method step by step.\n- In this paper, “counterfactual” is defined as the minimally modified image that flips a classifier’s prediction. While this aligns with the machine-learning literature on counterfactual explanations (e.g., Wachter et al., 2017; Goyal et al., 2019), it differs from the causal-inference definition by Judea Pearl, where counterfactuals are potential outcomes under explicit interventions in a causal model. Pearl’s formulation requires structural equations and causal mechanisms, not just classifier decision boundaries. Thus, the proposed method generates decision-boundary counterfactuals rather than causal counterfactuals, and this distinction should be clarified to avoid confusion in terminology.\n- It would also be valuable to discuss the difference between this line of work and causal-model-based counterfactual generation methods that explicitly incorporate Pearl’s framework (e.g., Deep Structural Causal Models, Pawlowski et al., 2020). Can such causal generative models also serve as counterfactual explanation tools, and how do their assumptions or objectives differ from the classifier-driven approaches explored here?\n- Can you provide experiments where you can show how each slot capture objects? And how different slot changes for different attribute counterfactuals?\n- Can you provide more visual results? For the other two datasets and for the ablation studies. \n- Did you use the same classifier for guiding the counterfactual generation and to evaluate validity? If so, the reported success rate can be biased.\n- Did you consider the issue of attribute entanglement when evaluating counterfactuals? If two attributes are correlated or causally linked (e.g., Smiling and Mouth open in CelebA-HQ), flipping one may unintentionally affect the other, making it unclear whether the change is truly isolated. It would be useful to discuss how your method handles such entangled attributes, whether through disentanglement in the slot space, targeted conditioning, or evaluation metrics that account for correlated attributes.\n- In section 6, the authors wrote \"our approach is built on DDPM\", but in 4.2, they wrote \"we use DDIM sampling\". So do you use DDIM or DDPM?\n- When generating images, did you first get a noisy version of z and then apply classifier guidance during the reverse process? If so, I think you did use DDIM, right?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IQClAphSn6", "forum": "IP1WnsGJVc", "replyto": "IP1WnsGJVc", "signatures": ["ICLR.cc/2026/Conference/Submission10895/Reviewer_rfdB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10895/Reviewer_rfdB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004422002, "cdate": 1762004422002, "tmdate": 1762922102566, "mdate": 1762922102566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}