{"id": "NvSRYp0oaX", "number": 3156, "cdate": 1757344024720, "mdate": 1763652946192, "content": {"title": "ABBA-Adapters: Efficient and Expressive Fine-Tuning of Foundation Models", "abstract": "Large Language Models have demonstrated strong performance across a wide range of tasks, but adapting them efficiently to new domains remains a key challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by introducing lightweight, trainable modules while keeping most pre-trained weights fixed. The prevailing approach, LoRA, models updates using a low-rank decomposition, but its expressivity is inherently constrained by the rank. Recent methods like HiRA aim to increase expressivity by incorporating a Hadamard product with the frozen weights, but still rely on the structure of the pre-trained model. We introduce **ABBA**, a new PEFT architecture that reparameterizes the update as a Hadamard product of two independently learnable low-rank matrices. In contrast to prior work, ABBA fully decouples the update from the pre-trained weights, enabling both components to be optimized freely. This leads to significantly higher expressivity under the same parameter budget, a property we validate through matrix reconstruction experiments. \nEmpirically, ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, consistently outperforming existing PEFT methods by a significant margin across multiple models. Our code is available anonymously at: https://anonymous.4open.science/r/abba-anonymous-8359.", "tldr": "", "keywords": ["LoRA", "Low-rank adaptation", "PEFT", "Parameter-Efficient Fine-Tuning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9cdc5a66b775d68f982080eb5cdb10a6a9c9007f.pdf", "supplementary_material": "/attachment/f54e046113d919396c61177b63549e6c00f33d91.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed ABBA, a variant of LoRA that relies on the Hadamard product of two learnable low-rank matrices. The key difference compared to HiRA is that both terms in the Hadamard product are learnable, and are decomposed through LoRA. In doing so, the Hadamard product can be calculated through the well-known Khatri–Rao factorization (or equivalently, face-splitting product). Experiments are conducted on commonsense reasoning datasets with LLaMA3.2 1B and 3B, and Arithmetic reasoning with Mistral-7B and Gemma-2 9B, demonstrating the empirical advantages of the advocated approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is easy to follow and the presentation is clear\n2. The Khatri–Rao factorization provides a computationally efficient approach to bypass the Hadamard product. \n3. The rank stability is confirmed theoretically. \n4. Numerical results are promising, which demonstrate a consistent performance boot."}, "weaknesses": {"value": "1. My major concern is the computational cost of ABBA. With the face-splitting (i.e., row-wise Khatri-Rao) product in Theorem 1, the forward and backpropagation cost of $\\Delta W x$ is $\\mathcal{O}((m+n)r_1r_2) = \\mathcal{O}((m+n)r^2)$, which is $\\mathcal{O}(r)$ times higher than vanilla LoRA. In other words, ABBA is be less scalable with the rank increasing. It would be beneficial if experiments can be conducted to showcase how the time and space complexities change with r (e.g., from $r=4$ to $r=512$) under different model sizes. \n2. There is no need to provide a proof for Theorem 1 in Appendix D.1, as it is a well-known result in linear algebra, and the citation (18) could be sufficient. \n3. The \"ABBA vs. LoRA: Reconstruction\" paragraph in Section 2.4 is not convincing enough, given the lack of tight theoretical bounds. Could you please provide some insights regarding how the bound can be improved? \n4. The models used in Section 4 are relatively small (<10B). It would be beneficial if experiments can be scaled to larger models such as Gemma-3-27B, and more complicated tasks such as coding."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0mgbwADhDP", "forum": "NvSRYp0oaX", "replyto": "NvSRYp0oaX", "signatures": ["ICLR.cc/2026/Conference/Submission3156/Reviewer_Z4En"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3156/Reviewer_Z4En"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761083003066, "cdate": 1761083003066, "tmdate": 1762916576079, "mdate": 1762916576079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper proposes an alternative PEFT method to LoRA called ABBA\n- ABBA paramtrizes the residual weights $\\Delta W$ used during the fine-tuning as $\\Delta W = s(B_1 A_1) \\odot (B_2 A_2)$ where $A_1, A_2, B_1, B_2$ are low rank matrices of ranks $r_1$ and $r_2$ respectively, where $s=\\frac{\\alpha^2}{r_1r_2}$ for rank stabilization.\n- $A_1$ and $B_1$ are initialized using the truncated SVD of the pretrained weights $W_o$ by keeping the top $r_1$ singular values and vectors.\n- $A_2$ and $B_2$ are initialized using the the standatd LoRA initialization scheme.\n- ABBA uses Khatri-Rao factorization to rewrite the paramtrization as $(B_1A_1) \\odot (B_2A_2) = (B_1 \\odot_r B_2)(A_1 \\odot_r A_2)$ where $\\odot_r$ is the Khatri-Rao product, makig the computation efficient. Here, $B_1 \\odot_r B_2 = B_{kr} \\in \\mathbb{R}^{m \\times r_1 r_2}$ and $A_1 \\odot_r A_2 = A_{kr} \\in \\mathbb{R}^{r_1 r_2 \\times n}$."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The method a clear theoretical motivation.\n2. The Khatri-Rao formulation makes the method computationally efficient, and hence practically feasible.\n3. The method is easy to implement and can be easily integrated with existing PEFT methods.\n4. The method is evaluated on commonsense reasoning, arithmetic reasoning and outperforms prior PEFT methods across model sizes.\n5. The method is ablated well to study its properties (initialization strategies, scaling factors, layer placement, and chaining more low-rank matrices instead of only two)."}, "weaknesses": {"value": "1. The paper compares ABBA with total ranks of 16 and 32 with rank 32 LoRA (and variants). However, LoRA can have onptimization problems with larger ranks, and many times, using smaller ranks can lead to slightly better performance. Hence, comparison with LoRA should also be done by setting the LoRA rank to 16 for a more thorough comparison.\n2. I find it hard to believe that full fine-tuning lags behind ABBA by such a large margin. While the authors try to justify this on lines 330-332, better hyperparameter tuning (especially setting a low learning rate) can possibly help full fine-tuning perform better. This does not necessarily mean that ABBA does not have merits over full fine-tuning, but rather that the expectations should be tempered.\n3. Continuing the point above, the experiments on arithmetic reasoning have been performed with a 20k subset of MetaMathQA. It's possible that with increasing the dataset size, full fine-tuning and LoRA can perform better. Again, this experiment would provide a more comprehensive comparison and help understand the merits of ABBA in limited data settings (wich are often observed in real-world applications)."}, "questions": {"value": "1. What is the hyperparameter tuning strategy used for full fine-tuning?\n2. Can the authors perfrom the experiments on arithmetic reasoning with a larger dataset, e.g., 40k subset of MetaMathQA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8sR01gLFhK", "forum": "NvSRYp0oaX", "replyto": "NvSRYp0oaX", "signatures": ["ICLR.cc/2026/Conference/Submission3156/Reviewer_FALt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3156/Reviewer_FALt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761532668239, "cdate": 1761532668239, "tmdate": 1762916575869, "mdate": 1762916575869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ABBA, a PEFT adapter that models updates as a Hadamard product of two independently learnable low-rank matrices and provides a Khatri–Rao reformulation so the update can be applied like LoRA without materializing full matrices. Claimed benefits are higher expressivity at a fixed parameter budget and LoRA-like efficiency. The proposed method is tested on commonsense (8 datasets) and arithmetic (GSM8K/MATH) in comparison with rsLoRA, PiSSA, DoRA, LoRA-Pro and HiRA with promising performance obtained."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The proposed method extends HiRA by replacing its fixed modulation using a learnable factor decouples the updated from $W_0$, which is a technical improvement. The Khatri-Rao factorization is a nice implementation detail. \n\n+ The proposed method is shown to have higher expressivity against LORA via matrix-reconstruction experiments and strong accuracy on commonsense/arithmetic."}, "weaknesses": {"value": "- The main concern is the novelty of this paper is incremental relative to HiRA/MoRA/ReLORA/KronA. The core architectural change is to learn both factors in the Hadamard product instead of tying one to $W_0$ (HiRA). While useful, this feels like a straightforward extension in the space of multiplicative/structured adapters already explored (HiRA, MoRA, KronA, ReLoRA), and the paper’s Related Work acknowledges much of this trajectory. The new factorization is an implementation convenience rather than a substantially new theoretical primitive.\n\n- The proposed method is evaluated only on two tasks: commonsense (8 datasets) and arithmetic (GSM8K/MATH). No instruction-tuning, long-context, code, multilingual, or domain-shift tests—settings where higher-rank adapters might matter most, especially considering the proposed method is claimed to be better than the widespread of LORA. \n\n- Compared with the built-in multi-faceted efficiency of LORA,  the efficiency evdience of the proposed method is not sufficiently compared. Figure 4 reports peak training memory at batch size = 1 and seq len = 256; settings are narrow and may not reflect real training regimes (e.g., larger batches/sequences, activation checkpointing, gradient accumulation). Tables show parameter counts matched to LoRA, sometimes halved (r=16), but parameter count alone is not the whole efficiency story; missing are optimizer-state bytes, activation footprints at realistic batch/seq, and per-step throughput. For example, all experiments run on a single A6000 (48 GB); conclusions about scalability and efficiency may not carry to multi-GPU or larger models. \n\n- LoRA variants are included (rsLoRA, PiSSA, DoRA, LoRA-Pro, HiRA), but QLoRA—a common efficiency baseline combining quantization and adapters—is discussed only in related work and not compared in experiments, and some recent work such as WeGeFT (C. Savadikar et al ICML25) are not discussed and compared. \n\n- Based on the the Khatri–Rao theorem, the proposed method can be treated as a special case of LORA with $B=B_1\\odot_r B_2$ and $A=(A_1^\\top\\odot_r A_2^\\top)^\\top$, i.e., introducing structures for B and A in LORA, and the proposed initialization of B_1 and A_1. In terms of this, LORA-One (ICML'25) and LORA-GA should be compared. \n\n- Although there are ablations for $\\alpha, \\{r_1,r_2\\}$, and placement, the “adapter chains” variant underperforms, hinting at optimization fragility when stacking. More analysis of failure modes would help (e.g., gradient scales, curvature, stability vs. rank)."}, "questions": {"value": "please consider to address the questions in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hG5KNoyCkk", "forum": "NvSRYp0oaX", "replyto": "NvSRYp0oaX", "signatures": ["ICLR.cc/2026/Conference/Submission3156/Reviewer_Cd6v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3156/Reviewer_Cd6v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932412514, "cdate": 1761932412514, "tmdate": 1762916575620, "mdate": 1762916575620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ABBA, a new parameter-efficient fine-tuning (PEFT) method that models the weight update as the Hadamard product of two independently learnable low-rank matrices. Unlike HiRA, which ties the update to the pretrained weights $W_0$, ABBA fully decouples both components, allowing them to be optimized freely. The authors further derive an efficient Khatri–Rao factorization so that ABBA can be implemented with the same memory and compute efficiency as LoRA while offering much higher expressivity. Extensive experiments on Llama-3.2 (1B & 3B), Mistral-7B, and Gemma-2 9B demonstrate consistent improvements over LoRA, HiRA, DoRA, and other PEFT baselines on commonsense and arithmetic reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of composing two learnable low-rank modules via a Hadamard product is conceptually clean and represents a clear generalization of LoRA and HiRA. The method increases effective rank while maintaining strict parameter efficiency.\n2. The paper introduces Khatri–Rao reformulation and a well-motivated rank-stability theorem, which explains how scaling should depend on $r_1,r_2$.\n3. Results across four foundation models and multiple reasoning tasks show large and consistent gains. The authors carefully control for parameter count and initialization, making comparisons fair."}, "weaknesses": {"value": "1. The paper proves a scaling law for stability but does not empirically show how optimization behaves under varying $r_1, r_2$ or initialization errors. Gradient norm or loss-landscape visualizations would strengthen claims about stable training.\n2. It remains unclear whether the performance gain comes from the Hadamard structure itself or simply from doubling the number of learnable matrices. An ablation removing the Hadamard product (e.g., summation or concatenation) would clarify this.\n3. While Section 2.4 claims that ABBA is more expressive than LoRA, the analysis relies primarily on empirical reconstruction errors. It will be better that if authors can provide formal proof or analysis showing that ABBA’s representational space strictly contains LoRA’s or HiRA’s."}, "questions": {"value": "Please refer Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2DL4QhebGL", "forum": "NvSRYp0oaX", "replyto": "NvSRYp0oaX", "signatures": ["ICLR.cc/2026/Conference/Submission3156/Reviewer_XkLT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3156/Reviewer_XkLT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934498386, "cdate": 1761934498386, "tmdate": 1762916575417, "mdate": 1762916575417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}