{"id": "NteaYNay7g", "number": 1459, "cdate": 1756884632778, "mdate": 1759898208030, "content": {"title": "RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization", "abstract": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone for image generation, demonstrating strong scalability and superior performance compared to U-Net architectures. However, their deployment remains hindered by substantial computational and memory costs. While quantization-aware training (QAT) has shown promise for U-Net architectures, its application to DiTs introduces unique challenges, primarily due to activation sensitivity and distributional complexity. In this work, we identify activation quantization as the principal bottleneck in pushing DiTs to extreme low-bit settings, and present a systematic QAT study of quantization for DiTs, namely RobuQ. We first establish a strong ternary-weight (W1.58A4) DiT baseline. To achieve robust activation quantization, we then propose RobustQuantizer, supported by theoretical analysis showing the Hadamard transform converts unknown per-token distributions into known normal distributions. In addition, we introduce AMPN, the first Activation-only Mixed-Precision Network pipeline for DiTs, which distributes mixed-precision activation to eliminate information bottlenecks, achieves state-of-the-art performance at W1.58A3, and stably supports average precision as low as W1.58A2 without collapse. Extensive experiments on unconditional and conditional image generation show that our framework consistently outperforms prior state-of-the-art quantization methods, achieving highly efficient DiT quantization while preserving generative fidelity. Together, these contributions substantially advance the practical deployment of DiTs in challenging ultra-low bit quantization scenarios.", "tldr": "We propose RobuQ, the first framework that successfully quantizes DiT to 2-bit activations (W1.58A2) and achieves stable generation on ImageNet-1K.", "keywords": ["Ternarization", "Diffusion Transformer", "Mixed-Precision Quantization", "Quantization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f78e7ed181a618ba0c74200da7b471195e72c99.pdf", "supplementary_material": "/attachment/2c1e70054b1ecc0785b7d88d89dd076833764a76.pdf"}, "replies": [{"content": {"summary": {"value": "This paper investigates Quantization-Aware Training (QAT) for Diffusion Transformers (DiTs) and proposes a systematic framework named RobuQ, which enables ultra-low-bit quantization (down to W1.58A2) while preserving generative quality.  The authors identify activation quantization as the key bottleneck in DiT compression and introduce three main components:  \n\n1.  A baseline established by integrating SVD-initialized low-rank compensation with Hadamard transformation.  \n\n2.  A RobustQuantizer, which theoretically transforms per-token activation values into a normal distribution via Hadamard transformation, enabling distribution-agnostic quantization.  \n\n3.  An Activation-only Mixed-Precision Network (AMPN), which optimizes activation bit allocation under a target quantization level using dynamic programming.  \n\nExperiments on ImageNet-1K and FFHQ demonstrate that RobuQ achieves significant efficiency improvements over existing QAT and PTQ baselines while maintaining competitive image generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces the Hadamard transform and provides a rigorous proof that this transformation can approximately normalize the channel-wise activation distributions, effectively mitigating quantization errors caused by irregular activation distributions. This insight is leveraged to achieve stable W1.58A2 activation precision in DiTs.\n\n2. The proposed AMPN (Activation-only Mixed-Precision Network) strategy dynamically allocates bit-widths across activation layers, offering valuable insights for model compression and efficient deployment.\n\n3. The manuscript is clearly written and enriched with informative figures and tables. The appendix includes detailed mathematical derivations and comprehensive experimental results, enhancing the overall rigor and reproducibility of the work."}, "weaknesses": {"value": "The paper draws heavily on recent quantization techniques such as BitNetV2, SVD-Quant and BinaryDM. While the integration of these methods yields strong results, the overall contribution would be more compelling if it included additional novel components that are clearly attributable to the authors."}, "questions": {"value": "Beyond the integration of existing techniques, which specific components of RobuQ constitute genuine technical novelty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qMVsXmiA6P", "forum": "NteaYNay7g", "replyto": "NteaYNay7g", "signatures": ["ICLR.cc/2026/Conference/Submission1459/Reviewer_fB1t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1459/Reviewer_fB1t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706488748, "cdate": 1761706488748, "tmdate": 1762915774920, "mdate": 1762915774920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on ultra-low-bit quantization for diffusion transformer models. It applies ternary weights across the entire network while assigning different activation precisions to each layer to mitigate information bottlenecks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The formulation of the dynamic programming strategy for AMPN design is novel and technically interesting."}, "weaknesses": {"value": "1. The paper lacks comparisons with several competitive baselines, such as SVDQuant.\n2. The novelty appears somewhat limited — the weight quantization closely follows BitNetV2, and the Hadamard rotation has already been applied to DiT models in prior works such as DvDQuant, LRQ-DiT, and SegQuant.\n3. The meaning and implementation of W1.58A4 quantization are unclear, especially given the lack of specialized kernels for such low-bit GEMM operations."}, "questions": {"value": "1. Please discuss the key differences between your approach and recent DiT quantization methods, such as DvDQuant, LRQ-DiT, and SegQuant.\n2. Could you provide detailed results on GPU speedup and memory usage? The referenced Appendix C is missing from the supplementary materials.\n3. Does the proposed Per-token Gauss Quantizer introduce any additional inference cost?\n4. Is RobuQ applicable to text-to-video generation models? If so, could you include experimental results to support this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jGSMM6Wh90", "forum": "NteaYNay7g", "replyto": "NteaYNay7g", "signatures": ["ICLR.cc/2026/Conference/Submission1459/Reviewer_WCPa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1459/Reviewer_WCPa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933609180, "cdate": 1761933609180, "tmdate": 1762915774310, "mdate": 1762915774310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RobuQ, a QAT framework for ultra-low-bit Transformer quantization, instantiated on DiT-XL/2. The method has three components: \n\n(i) RobustQuantizer—apply an all-layer Hadamard transform to “Gaussianize” per-token activations, then quantize with a per-token Gauss quantizer (uniform and Lloyd–Max); \n\n(ii) Activation-only Mixed-Precision (AMP)—fix weights to ternary (W1.58) and allocate per-layer activation bit-widths via a DP/knapsack objective; \n\n(iii) experiments on ImageNet-1K and FFHQ showing competitive A4/A3 and stable A2 under QAT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Coherent systems integration. A well-engineered composition of established pieces—all-layer Hadamard transformation with per-token Gaussian quantization—that, in concert with QAT, yields stable ultra-low-bit operation on large Transformers.\n\n\n- Activation-only mixed precision. Introduces an activation-only mixed-precision (AMP) (weights fixed ternary; layer-wise activation bits budgeted) that enables ultra-low-bit QAT in practice."}, "weaknesses": {"value": "- The advantages of Hadamard/orthogonal mixing for taming activation distributions and Lloyd–Max for (near-)Gaussian inputs are well established in prior quantization literature (e.g., rotation/incoherence-based LLM quantization such as QuIP). As presented, these ingredients read as strong engineering choices rather than conceptual innovations.\n- From Table 1, the incremental improvement attributable to AMP over the RobustQuantizer is not consistently large, which weakens the claim that AMP is indispensable in practice. Its largest gains appear at A2, Fig, while Fig. 10 indicates noticeable artifacts in A2 generations that may render outputs unusable in practice.\n\n- Although the method is demonstrated on DiTs, none of the three pieces appears diffusion-specific; the contribution is better characterized as a general Transformer quantization recipe. This is fine, but then comparisons to non-diffusion Transformer baselines (e.g., ViTs/LLMs) would strengthen the generality claim."}, "questions": {"value": "- Table 1 has FP DiT-XL/2 FID appearing higher (worse) than the original DiT paper’s numbers. Could you clarify the evaluation protocol differences? A likely factor is that you evaluate with 10k generated images, whereas the common practice for ImageNet FID is 50k; 10k-sample FID is known to be noisy and biased.\n- Table 2b suggest uniform often outperforms Lloyd–Max despite the latter’s optimality for Gaussian inputs. Can you provide analysis of the mismatch (e.g., residual non-Gaussianity, scale drift)?\n- Could you clarify how Hadamard is deployed at inference and provide end-to-end latency on a realistic hardware in Table 2a ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HiYFI85hlA", "forum": "NteaYNay7g", "replyto": "NteaYNay7g", "signatures": ["ICLR.cc/2026/Conference/Submission1459/Reviewer_WvD2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1459/Reviewer_WvD2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953398079, "cdate": 1761953398079, "tmdate": 1762915774106, "mdate": 1762915774106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "RobuQ addresses ultra-low-bit quantization for Diffusion Transformers by identifying activation quantization as the main bottleneck and proposing a robust quantization framework. The key innovation is proving that Hadamard transforms convert arbitrary activation distributions to normal distributions, enabling distribution-agnostic quantization via Lloyd-Max quantizers, combined with SVD-initialized low-rank branches and an activation-only mixed-precision strategy (AMPN). The method achieves state-of-the-art results at W1.58A4 and is the first to demonstrate stable W1.58A2 quantization on ImageNet-1K without training collapse."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Proof that Hadamard transforms convert arbitrary per-token activation distributions to approximately normal distributions, enabling distribution-agnostic quantization with Lloyd-Max optimal quantizers. The proof is novel. \n\n2. Establishes new SOTA at W1.58A4 (IS=103.24, FID=17.97), and achieves first stable W1.58A2 training on ImageNet-1K, avoiding the collapse that affects baselines."}, "weaknesses": {"value": "1. No comparison with SVDQuant (SoTA DiT PTQ method), PTQ4DM, or Q-Diffusion. QueST 4W4A baseline completely collapses, suggesting implementation issues. As the QueST is better than PTQ4DiT. This severely limits ability to assess true contributions.\n\n2. Only 3.5× actual speedup vs. 17.3× theoretical ratio. Mixed-precision quantization is difficult to accelerate on current hardware. No reported speedup for AMPN variants, and memory-bound operations may not benefit proportionally from activation bit reduction.\n\n3. Incomplete ablations (no SVD rank analysis, limited Hadamard alternatives), and substantial training costs (204.5 hours for W1.58A2) not compared to PTQ alternatives."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8GxHJ0Yvox", "forum": "NteaYNay7g", "replyto": "NteaYNay7g", "signatures": ["ICLR.cc/2026/Conference/Submission1459/Reviewer_Uouw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1459/Reviewer_Uouw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762274345690, "cdate": 1762274345690, "tmdate": 1762915773952, "mdate": 1762915773952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}