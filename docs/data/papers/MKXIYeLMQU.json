{"id": "MKXIYeLMQU", "number": 12967, "cdate": 1758212250427, "mdate": 1759897473682, "content": {"title": "Dual-Priv Pruning : Efficient Differential Private Fine-Tuning in Multimodal Large Language Models", "abstract": "Differential Privacy (DP) is a widely adopted technique, valued for its effectiveness in protecting the privacy of task-specific datasets, making it a critical tool for large language models. However, its effectiveness in Multimodal Large Language Models (MLLMs) remains uncertain. Applying Differential Privacy (DP) inherently introduces substantial computation overhead, a concern particularly relevant for MLLMs which process extensive textual and visual data. Furthermore, a critical challenge of DP is that the injected noise, necessary for privacy, scales with parameter dimensionality, leading to pronounced model degradation; This trade-off between privacy and utility complicates the application of Differential Privacy (DP) to complex architectures like MLLMs. To address these, we propose Dual-Priv Pruning, a framework that employs two complementary pruning mechanisms for DP fine-tuning in MLLMs: (i) visual token pruning to reduce input dimensionality by removing redundant visual information, and (ii) gradient-update pruning during the DP optimization process. This second mechanism selectively prunes parameter updates based on the magnitude of noisy gradients, aiming to mitigate noise impact and improve utility. Experiments demonstrate that our approach achieves competitive results with minimal performance degradation. In terms of computational efficiency, our approach consistently utilizes less memory than standard DP-SGD. While requiring only 1.74% more memory than zeroth-order methods which suffer from severe performance issues on A100 GPUs, our method demonstrates leading memory efficiency on H20 GPUs. To the best of our knowledge, we are the first to explore DP fine-tuning in MLLMs. Our code is Our code is avaliable in : https://anonymous.4open.science/r/Dual-priv-pruning-AE7E.", "tldr": "", "keywords": ["Multimodal", "Trustworthy", "Differential Privacy"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f0e7658d6ca362c7b48d5c12b145008cb0824d86.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Dual-Priv Pruning, a new framework for differentially private (DP) fine-tuning of Multimodal Large Language Models. Dual-Priv Pruning addresses the challenges of computational overhead and model degradation due to noise injection in DP-based finetuning, which scales with parameter dimensionality. The proposed Dual-Priv Pruning framework employs two techniques: (1) visual token pruning and fusion to reduce input dimensionality by removing redundant visual information tokens, and (2) gradient-update pruning to apply noisy gradients during DP-SGD optimization selectively. Experimental results show that Dual-Priv Pruning efficiently reduces the computational cost during DP-based finetuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tPioneering Approach. Dual-Priv Pruning is the first framework to address DP fine-tuning specifically for MLLMs, filling a critical research gap.\n\n2.\tEfficiency Gains. Dual-Priv Pruning achieves significant memory reduction (14.34% less peak GPU usage) and computational efficiency compared to standard DP-SGD.\n\n3.\tBetter privacy-utility trade-off.  Dual-Priv Pruning maintains competitive performance under strict privacy budgets (ε ≤ 3) despite noise injection during DP optimization."}, "weaknesses": {"value": "1.\tLack of Novelty. The work doesn’t invent a new approach for DP. It is more like an engineering approach that combines existing pruning methods and DP training methods.\n\n2.\tLimited Generalizability: Relies on specific assumptions (e.g., selection mechanism for visual tokens), which may not apply universally across all MLLM tasks.\n\n3. Doubtful experimental results. Under ϵ = inf in Table 1, non-private performance is reported for DZPO, DP-SGD and Dual-Priv. As it is a non-private performance, why does the overall performance differ so much? For example, under ScienceQA and ϵ = inf setup, DZPO has an accuracy of 22.16, DP-SGD has an accuracy of 81.10, while  Dual-Priv has an accuracy of 84.60. Do they use the same base VLLM? I am so confused about the reported results."}, "questions": {"value": "I am not familiar with the VLLM optimization and the evaluated tasks. So my review comments may not be professional, and I set my confidence score as 2. If Question 1 can be addressed properly, I am willing to raise my rating.\n\n1. Under ϵ = inf in Table 1, non-private performance is reported for DZPO, DP-SGD and Dual-Priv. Why does the overall performance differ so much? Do they use the same base VLLM?\n\n2. Will empirical attacks be conducted to verify the robustness of Dual-Priv Pruning?\n\n3. What is the baseline VLLM performance without fine-tuning? What is the baseline VLLM performance after normal non-private fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7LcUO4zO3y", "forum": "MKXIYeLMQU", "replyto": "MKXIYeLMQU", "signatures": ["ICLR.cc/2026/Conference/Submission12967/Reviewer_fULi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12967/Reviewer_fULi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761498894763, "cdate": 1761498894763, "tmdate": 1762923723420, "mdate": 1762923723420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Dual-Priv Pruning is a novel framework for differentially private fine-tuning of multimodal large language models (MLLMs) that addresses the computational challenges and privacy-utility trade off. The proposed method combines two machines: 1.  visual token pruning using attention mechanisms to select and compress the most informative visual tokens, and 2. gradient-update pruning that selectively updates only the most significant parameter blocks after adding DP noise. The proposed method achieves substantial reductions in memory and computational overhead by reducing the context length of each input. The experimental results show improved accuracy and significant GPU memory savings across a range of benchmarks and privacy budgets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper address an important challenge of privacy utility trade off in a multimodal LLM fine-tuning setup.\n2. The proposed method employs two levels of pruning, one for each a) reducing memory overhead, and b) reducing the impact of DP noise on utility.\n3. The experimental results show the improvements in accuracy resulted by utilizing dual-priv pruning as compared to DP-SGD (first order DP fine-tuning) and DPZO (zeroth order DP fine-tuning) for various benchmarks and privacy settings.\n4. The paper also presents ablation studies on memory usage and empirical privacy results via MIA."}, "weaknesses": {"value": "The proposed mechanism introduces additional hyper-parameters that need to tuned such as 1) selected layers of the vision encoder for computing the importance scores, and 2) K and |C| values for pruning in step-1 (token selection) and step-2 (gradient pruning). Tuning these parameters to get reasonable trade-offs can introduce heavy computational overhead."}, "questions": {"value": "1. \"we first compute the multi-head self-attention maps within a selected layer of the vision encoder.\" How do we decide which layers or how many layers to use for scores computation? Is this a hyper-parameter that needs tuning?\n2. Section 5.5 indicates computational efficiency analysis but only presents the memory usage numbers. Dual-priv pruning reduces computational cost by token pruning but at the same time there are additional computations introduced to select the important tokens and contextual token fusion. What are the compute savings of the end-to-end pipeline of the proposed method in-terms of FLOPs as compared to DP-SGD?\n3. We could potentially reduce the number of parameters by reducing the LoRA rank. How does dual-priv pruning compare to DP-SGD at iso-parameters? For more context, let's say that we train an MLLM with 1) setup 1: Dual-Priv with LoRA rank (r) = 128 with 50% pruning and compare it with, 2) setup 2: DP-SGD with LoRA rank (r) = 64. Assuming both these setups do not have mechanism 1 (token pruning). The goal here is to understand if top-k gradient pruning is significantly better than simply reducing the LoRA rank. \n4. The proposed mechanism seems generic and can be applied to any large model with transformer architecture. Is there any part of the algorithm that is addressing multimodal specific challenges? \n5. How does \"Dominant Token Selection via CLS Attention\" compare with random selection?\n6. In table Table 5, can you add an additional datapoint where you split mechanism 1 into a) w/ token pruning, and b) token pruning + contextual fusion? The proposed algorithm has the following pieces: token pruning + contextual fusion + fusion noise + gradient pruning. To have a comprehensive ablation study, I would suggest adding results on all combinations: (0, 1, 1, 1), (1, 0, 1, 1), (1, 1, 0, 1) and (1, 1, 1, 0). \n7. Token pruning depends on the existence of CLS token. Is the proposed method extendable to generation tasks?\n8. The final set of visual tokens is a concatenation of $v_{cls}$, $V_d$ and $C$. Do the proposed method ensure that the ordering of the tokens is maintained? The wrong ordering of tokens might not impact the performance of models with visual inputs but in my opinion, this will impact performance of models with text inputs. Will the proposed method work equally well for text to text LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yc9VSwITMW", "forum": "MKXIYeLMQU", "replyto": "MKXIYeLMQU", "signatures": ["ICLR.cc/2026/Conference/Submission12967/Reviewer_RBbS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12967/Reviewer_RBbS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761523641049, "cdate": 1761523641049, "tmdate": 1762923722945, "mdate": 1762923722945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies differentially private training of Multi-modal LLMS (vision language models). DP-SGD is the most widely used algorithm for training models with differential privacy. DP-SGD adds noise to the gradients at each step, where the noise scales with The authors improve upon this baseline with two techniques that increase model accuracy:\n\n(1) reduce dimensionality of the input image by selecting the most relevant tokens and fusing the remaining tokens using a clustering technique. \n\n(2) masking the (noisy) gradient update so that the gradient update only occurs for the parameters with strongest signal (about 80% of parameters). \n\nThe authors evaluate accorss several visonal language question answering benchmarks, including benchmarsk in the medical domain where privacy is a more relevant concern. There are consistent accuracy improvements over the DP-SGD baseline (~4 pct points and ~8 pct points for 2 of the tasks, and at about 1-2 pct point in 3 of the tasks). The reduced input dimensionaly also leads to memory improvements of 14%."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- First paper to consider private training of VLLMs, opening up a new avenue for research \n- Paper proposes some interesting techniques for improving the utility of differentially private training when working with high-dimensional data. These techniques might be useful in other settings for differentially private traning beyond VLLMs\n- Comprehehensive evaluation with open source code\n- Consistent improvement over the baseline method. \n- Great, easy to follow presentation"}, "weaknesses": {"value": "The first technique, which reduces dimensionality of the input image by selecting the most relevant tokens and fuses the remaining tokens using an averaging+clustering method,  does not have a differential privacy guarantee. Instead noise is added to the fused tokens heuristically. Unless I am missing something, the E2E algorithm is not technically differentially private and I think this should be emphasized further in the limitations/intro. \n\nI agree that for practical privacy guarantees and as shown by your MIA results this might not matter as much."}, "questions": {"value": "Couuld you provide more intuition for why adding nosie to the fused non-dominant tokens helps with accuracy? You also say that this noise should be of the same magnitude as the noise added to the gradients. Maybe I am missing something but isn't the gradient computed wrt to the fused tokens as well, so why do we need double the noise?\n\nThe sentece in line 199-200 is also confusing. It seems to imply that because the pruning is text-agnostic we do not need to worry about using part of the privacy budget, but your privacy statement is wrt both the image and text.\n\nWilling to increase my score upon clarification of these questions and addressing of the weakness I mention."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "El0ZaklXoz", "forum": "MKXIYeLMQU", "replyto": "MKXIYeLMQU", "signatures": ["ICLR.cc/2026/Conference/Submission12967/Reviewer_Xu4T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12967/Reviewer_Xu4T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704647486, "cdate": 1761704647486, "tmdate": 1762923722359, "mdate": 1762923722359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a dual pruning algorithm to optimize the differential privacy process of MLLMs. The first stage uses visual CLS to discard less important visual tokens, and the second stage uses gradient pruning for parameter updates."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe motivation behind the problem is clear: MLLM computation is computationally expensive. Differential privacy also suffers from reduced utility as dimensionality increases.\n2.\tThe method design is simple and relatively easy to implement.\n3.\tThe experiments and ablation studies are relatively comprehensive."}, "weaknesses": {"value": "1. It is unclear whether importance scores should be used as the evidence for discarding tokens. This seems to be a common technique used in engineering, and the author also seems to have demonstrated the significance of discarding them. However, simply discarding tokens based on the importance of attention seems to lack rigorous justification.\n\n2. It's unclear why the author conducted accuracy experiments on the Q&A dataset: the author proposed a new method for differential privacy, but testing its accuracy on different Q&A datasets seems strange, because differential privacy itself is not designed to achieve higher accuracy. I think the author wanted to convey that the discarded tokens are redundant tokens, and that even after discarding them, the method still maintains high accuracy on Q&A datasets, is that correct?\n\n3. Experiments conducted using only LLAVA-7B and its medical fine-tuned model have limited effectiveness. The authors should consider incorporating other models into the experiment.\n\n4. Anonymous code cannot be accessed.\n\n5. The text contains numerous typos and content that could easily mislead readers. For example, in line 269, \"Mechanism 2\" should be \"Mechanism 1\". Requiring readers to infer these errors from the context increases the reading effort."}, "questions": {"value": "See the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "v4NLHuH6C5", "forum": "MKXIYeLMQU", "replyto": "MKXIYeLMQU", "signatures": ["ICLR.cc/2026/Conference/Submission12967/Reviewer_ZjQL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12967/Reviewer_ZjQL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185428139, "cdate": 1762185428139, "tmdate": 1762923722019, "mdate": 1762923722019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}