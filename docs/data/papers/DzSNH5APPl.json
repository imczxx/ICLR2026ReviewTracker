{"id": "DzSNH5APPl", "number": 8915, "cdate": 1758102289394, "mdate": 1763659403424, "content": {"title": "Learning Explicit Single-Cell Dynamics Using ODE Representations", "abstract": "Modeling the dynamics of cellular differentiation is fundamental to advancing the understanding and treatment of diseases associated with this process, such as cancer. With the rapid growth of single-cell datasets, this has also become a particularly promising and active domain for machine learning.\nCurrent state-of-the-art models, however, rely on computationally expensive optimal transport preprocessing and multi-stage training, while also not discovering explicit gene interactions. \nTo address these challenges we propose Cell-Mechanistic Neural Networks (*Cell-MNN*), an encoder-decoder architecture whose latent representation is a *locally linearized ODE* governing the dynamics of cellular evolution from stem to tissue cells. Cell-MNN is fully end-to-end (besides a standard PCA pre-processing) and its ODE representation explicitly learns biologically consistent and interpretable gene interactions.\nEmpirically, we show that Cell-MNN achieves competitive performance on single-cell benchmarks, surpasses state-of-the-art baselines in scaling to larger datasets and joint training across multiple datasets, while also learning interpretable gene interactions that we validate against the TRRUST database of gene interactions.", "tldr": "We propose Cell-MNN, an encoder–decoder model with a locally linearized latent ODE representation that achieves competitive performance on single-cell dynamics benchmarks, while discovering biologically consistent gene interactions.", "keywords": ["AI4Science", "AI4Biology", "gene interaction discovery", "single-cell dynamics", "dynamical systems"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cdb32ab2776c08023b4646e824ad39626e720ba8.pdf", "supplementary_material": "/attachment/1b9f474d92d35b8b3f5a474f1e5534f64c1324a5.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose to use a variant of Neural ODEs where the velocity is parameterized as a constant linear function of location at every timestep.  They use this parameterization to fit single-cell RNA data over several timepoints, evaluating with interpolation of a heldout timepoint."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors show good results compared to other interpolation methods on several common benchmarks.  The motivation for the method is straightforward and natural, and I appreciate the work in making simpler and more interpretable models competitive with flow-based trajectory learning."}, "weaknesses": {"value": "Projecting to 5d PCA is a considerable limitation, as the method seems to not scale and the dynamics of single cell data is generally not so easily compressible.  Additionally, the actual training of the model is somewhat unclear (see questions).\n\nI’m also not totally convinced by the gene analysis results, as it probably requires a baseline to convince a reader that these top gene interactions were not simply the ones encoded by PCA, and any model that learns a velocity (say, MFM-OT or TrajectoryNet) could linearize its velocity at some timepoint and find the same gene interactions.  The later claim about enforcing a zero in the spectrum seems a bit flimsy; it’s true that many individual genes will not be changing during the dynamics, but at only 5 PCs one would expect all PCs were relevant to the dynamics."}, "questions": {"value": "How exactly is the model trained?  The locally linear ODE maps from z_t to z_{t + \\Delta_t} but I cannot find an indication of how the authors turn this into an explicit loss.  They make no mention of differentiating through an ODE solver so is there a parameter \\Delta_t set in the method and the authors explicitly differentiate through several linearization steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "okxDo4gO6Q", "forum": "DzSNH5APPl", "replyto": "DzSNH5APPl", "signatures": ["ICLR.cc/2026/Conference/Submission8915/Reviewer_JgxX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8915/Reviewer_JgxX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761319274199, "cdate": 1761319274199, "tmdate": 1762920668674, "mdate": 1762920668674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Answer"}, "comment": {"value": "We appreciate the detailed reviews and would like to thank all reviewers for their feedback and time invested. We are happy to hear that the reviewers found the paper to be *“...well-motivated, theoretically sound”* (4yFL), *“...clearly written”* (51Jg) and that it tackles *“...an important problem”* (51Jg). There are two clear concerns that are shared by multiple reviewers and we address them in this general statement. Questions or remarks that are particular to individual reviewers are addressed in direct answers.\n\n# Concern of all reviewers: Low dimensional PCA space \nAll the reviewers raised concern about the use of a low dimensional PCA space to model the gene expression dynamics in. We believe these concerns arise from a misunderstanding of the scope of our contribution:\n- **Low-dimensional PCA is standard in ML-based trajectory inference.** Using 5 principal components is common practice in the ML literature on single-cell interpolation, **see the nine methods we compare with (see Table 1 in the paper)**. This aligns with the assumption in computational biology that scRNA-seq data lie on a low-dimensional manifold. For example, in \\[1] trajectory inference is performed directly on 5 PCs to derive biological conclusions about lineage progression.\n- **Why low-dimensional PCA is biologically meaningful.** Across all datasets we evaluate on, PCA variance plots show that the first few components capture the large part of the variance, see Figure 7. Using 5 PCs yields a cumulative explained variance of above 60% for Cite and Multi and above 40% for the Embryoid dataset. Crucially, the 5 dimensional PCA embedding preserves cell-type information: We compute the KNN classifications performance using 15 neighbors and observe an accuracy of 87 % (Multi) and 90% (Cite) (Embryoid has no cell-type labels). To visualize this, we show the UMAPs computed on the 5 dimensional PCA embedding (Figure 7b & d) clearly cluster by cell-type. This is the essential requirement in scientific studies that investigate lineage bifurcations, where the goal is to predict cell-type transitions and not reconstruct all gene-level nuances.\n- **Our contribution within this setting.** Our paper is **not** about learning a different representation for this problem. We propose Cell-MNN to improve the learning algorithms on top of the commonplace representations used in other deep learning baselines. Cell-MNN achieves SOTA average performance while avoiding strong OT assumptions and providing explicit, local dynamics that are interpretable as gene interactions. \n- **Clarifications in the revision.** We thank the reviewers for pointing out that the low-dimensional PCA embedding was not sufficiently discussed. We added a dedicated section (Appendix C.1)  including (i) motivation (ii) PCA variance plots for all datasets, (iii) a demonstration that cell-type structure is preserved in 5-PC space using KNN classification and UMAP plots, and (iv) comparison of OT-CFM and Cell-MNN with 10-PCs (similar performance).\n\n\\[1]: https://www.nature.com/articles/s41467-019-11493-2\n\n# Concern of all reviewers: GRN discovery baseline is too weak\nAll reviewers were asking for additional baselines for the GRN discovery experiment. We completely agree that additional baselines make the picture more comprehensive and thank the reviewers for the suggested new baselines. We have updated our GRN discovery experiment showing outperformance with respect to SCODE and the Jacobian of OT-CFM. In the following we present our line of reasoning:\n- **Requirement: prediction of signed gene interactions.** Our GRN experiment evaluates whether Cell-MNN can correctly classify regulatory interactions as *activating* or *repressing*. This focuses on signed edges, which substantially restricts the set of meaningful baselines. \n- **Many popular GRN methods do not predict signed edges.** Methods such as Marlene, GRNBoost, and GENIE3 output only unsigned regulatory edges. This means that they cannot serve as baselines for our task.\n- **Velocity-based methods require incompatible data.** Dynamo and related RNA-velocity-based GRN estimators depend on spliced or metabolically labeled RNA counts. The datasets in our study provide only pure UMI counts, making these methods inapplicable.\n- **GRN methods based on scATAC-seq are not applicable.** Methods such as SCENIC+ and Dictys require paired chromatin accessibility (scATAC-seq) and scRNA-seq measurements. These are not available for the datasets used in our study, and hence these methods cannot serve as baselines.\n- **Added competitive baselines.** To strengthen the evaluation we have updated the GRN discovery experiment with additional baselines that do output signed interaction matrices: (i) the Jacobian of OT-CFM, and (ii) SCODE. As reported in Table 2, Cell-MNN outperforms the two baselines by around 16% percentage points on average, supporting the claim that the predicted operators capture biologically meaningful interactions."}}, "id": "0PZ3pHYBGg", "forum": "DzSNH5APPl", "replyto": "DzSNH5APPl", "signatures": ["ICLR.cc/2026/Conference/Submission8915/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8915/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8915/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763656888833, "cdate": 1763656888833, "tmdate": 1763656888833, "mdate": 1763656888833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose Cell-MNN, a new ODE-based method designed to model the single-cell dynamics of cellular differentiation. The method operates in PCA space, and uses a hypernetwork to learn the transition matrix A. The local dynamics are assumed to be linear. The model is trained by optimizing an MMD loss between the predicted and target marginals of gene expression in the latent space. They benchmark their model on single-cell interpolation, amortized training, and gene interaction directionality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method eliminates the need for OT which is typically a computational bottleneck for many methods.\n- The method achieves competitive performance on the cell interpolation task, outperforming multiple baselines on three different datasets.\n- The authors perform a scaling experiment to benchmark the ability of their model to handle large datasets and show that it is scalable enough for typical contemporary large datasets.\n- The ability of the model to learn explicit gene regulatory interactions is useful for interpretability and for learning the explicit gene networks governing differentiation."}, "weaknesses": {"value": "- The authors perform an unsupervised classification task to predict the direction of regulation for a known TF-gene link from the TRRUST database. The paper does not benchmark the method's ability to predict the links themselves, e.g., by evaluating if the interactions with the highest predicted strength are enriched for known links. Furthermore, analysis of the top source or target genes using gene enrichment analysis could shed light into any relevant pathways.\n- The authors mention GRN inference as a complementary line of work but only reference static GRN models. A few works have experimented with learning temporal gene networks for cell differentiation or biological state progression, such as Scenic+, Marlene, Dictys [1-3]. A discussion on how the current work differs from these or a direct comparison is needed to strengthen the contribution.\n- The authors project data into 5 dimensions using PCA. This is too limiting for large single-cell datasets with ~20k genes and likely ignores finer interactions between genes. The authors need to present additional work/analysis that uncovers the type of information lost by doing such a projection and the model's ability to learn fine-grained interactions.\n\n[1] https://www.nature.com/articles/s41592-023-01938-4  \n[2] https://academic.oup.com/bioinformatics/article/41/Supplement_1/i628/8199402  \n[3] https://www.nature.com/articles/s41592-023-01971-3"}, "questions": {"value": "Could the authors comment on the model's capability for extrapolation, that is, predicting a future time point beyond the final observed time in the training set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iqZMODWuTP", "forum": "DzSNH5APPl", "replyto": "DzSNH5APPl", "signatures": ["ICLR.cc/2026/Conference/Submission8915/Reviewer_otWS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8915/Reviewer_otWS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761435918869, "cdate": 1761435918869, "tmdate": 1762920668263, "mdate": 1762920668263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Cell-MNN, a ODE–based framework that learns explicit single-cell dynamics by modeling local linear ODEs in a PCA latent space. The approach avoids explicit OT preprocessing and claims to simultaneously provide interpretable gene regulatory interactions through the learned Jacobian matrices. Experiments on several benchmark single-cell datasets show competitive interpolation accuracy and advantages over OT-based baselines."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and tries to tackle an important problem in single-cell dynamic modeling — learning interpretable continuous-time dynamics."}, "weaknesses": {"value": "1. Latent-space linearization is under-justified and fragile. The method hinges on representing dynamics as locally linear in a PCA latent space, $\\dot z \\approx A_\\theta(z,t)z$, advanced via a matrix exponential. The paper itself acknowledges that evolving too far leaves the validity region and requires re-encoding to refresh the local ODE, but provides no analysis of linearization error growth, step-size control, or robustness to curvature in realistic trajectories. Moreover, the claimed efficiency advantage relies on a very low latent dimension; once d_z grows, computing and applying the operator (incl. eigendecomposition/inversion) becomes the dominant cost (see 4), which does not resolve the core limitations of Neural ODEs for stiff/strongly nonlinear flows— it only replaces numerical integration with piecewise-linear surrogates without guarantees.\n2. GRN novelty is overstated; prior work is insufficiently acknowledged. Inferring gene regulatory relations from a learned ODE/vector field via its Jacobian has been proposed and used before (e.g., Dynamo [1]) and applied in multi-timepoint trajectory settings (e.g., TrajectoryNet [2], TIGON [3]). Cell-MNN can be viewed as the special case where the vector field is constrained to be piecewise linear in a low-dimensional PCA subspace. The “Gene Regulatory Network Discovery” related-work discussion omits or underplays these lines, which is misleading about the methodological novelty. \n3. Scalability claims (vs. OT) are overstated, and the mechanism is unclear. The paper attributes scalability improvements to replacing OT preprocessing with MMD-based distribution matching and reports out-of-memory (OOM) errors for standard OT on inflated datasets. However, many Neural ODE–based or flow-matching methods already alleviate memory issues by computing OT or matching losses in mini-batches, thereby avoiding O(n^2) coupling matrices in practice. From this perspective, it is unclear why Cell-MNN “solves” sample-size scalability more effectively than these existing approaches, given that both rely on batched stochastic training. Moreover, substituting OT with balanced MMD does not directly address unbalanced dynamics (cell proliferation/death), where Unbalanced-SB frameworks provide principled formulations. Thus, the claimed scalability advantage seems more an artifact of implementation choices than a fundamental methodological improvement.\n4. Cubic scaling in the latent dimension threatens practicality beyond very low $d_z$. The paper states $O(T d_z^2)$ for applying the analytical solution, but also a one-time $O(d_z^3)$ per operating point to form the operator (e.g., eigendecomposition/inversion). For $d_z=50–100$, this rapidly dominates and can exceed the cost of Neural ODE field evaluations, undermining the claimed efficiency. The assertion that 5D PCA is “sufficient” is not substantiated by ablations or biological coverage analyses. And I also believe this assertion is not correct.\n5. Wall-clock efficiency is not compelling. Despite a 5D latent space and no OT, the reported runs surprisingly high (one hour) for the stated computational simplicity, and notably slower than many baselines under comparable settings. The paper does not explain where time is spent (e.g., repeated eigendecompositions, kernel MMD computation) or provide profiling.\n6. Redundant/ill-motivated invertibility regularization. The additional term $L_{\\text{inv}}(\\theta)$, encouraging $P_\\theta$ invertibility, appears unnecessary: invertible matrices are dense; a randomly initialized square matrix is almost surely invertible. The paper does not justify why this regularizer is needed, nor analyze its numerical side-effects (e.g., unstable gradients via $\\det(\\cdot)$).\n7. Interpolation results of OT baselines are unclear. In Table 1, several OT-based methods underperform the OT-Interpolate baseline, while Cell-MNN surpasses it. Since many methods eventually regress toward OT-derived supervision, it is unclear why Cell-MNN outperforms those indirect OT-fitting approaches. The paper should articulate the mechanism (e.g., bias/variance advantages of distribution-level MMD vs. velocity-level supervision, regularization effects) and provide controlled comparisons \n8. Amortized training procedure lacks essential details. Section 4.2 merges datasets in PCA space for amortized training and feeds a dataset index to the model, but does not explain the details.\n9. Gene interaction validation lacks competitive baselines. Validation against TRRUST is useful, but the paper does not compare to other vector-field/Neural-ODE methods that could also extract Jacobian-based interactions under the same preprocessing. Without head-to-head comparisons, it remains unclear whether Cell-MNN offers any real advantage for GRN discovery beyond the convenience of an explicit linear operator.\n10. Insufficient ablations/sensitivity analyses. Key design choices—kernel and bandwidth for MMD, discount factor $\\gamma$, regularization weights $\\lambda_{\\text{kin}}$, $\\lambda_{\\text{inv}}$, latent dimension $d_z$, operator parameterization (e.g., fixing one eigenvalue to 0 vs. not ), and $\\Delta t$ sampling—lack systematic ablations. Given that interpretability and stability hinge on these knobs, their impacts should be quantified."}, "questions": {"value": "Due to these concerns, I lean towards rejecting the paper in its current form. Significant theoretical justification, experimental clarification, and comparison to prior ODE-based methods would be needed for a more favorable evaluation. The authors should: \n\n1. Clarify justification for latent-space linearization. (See weakness 1)\n2. Discuss the relation to prior GRN-from-ODE works. (See weakness 2)\n3. Substantiate the scalability argument. (See weakness 3)\n4. Provide complexity and latent-dimension analysis.  (See weakness 4)\n5. Report detailed runtime and efficiency breakdown.(See weakness 5)\n6. Revisit the invertibility regularization. (See weakness 6)\n7. Clarify interpolation results and superiority to OT baselines. Could the authors explain why Cell-MNN achieves superior interpolation despite training toward similar objectives?  (See weakness 7)\n8. Elaborate on amortized training implementation. (See weakness 8)\n9. Enhance GRN validation comparisons (e.g., trajectorynet, TIGON). (See weakness 9) \n10. Include hyperparameter and kernel sensitivity studies. (See weakness 10)\n\nReferences\n1. Xiaojie Qiu, Yan Zhang, Jorge D Martin-Rufino, Chen Weng, Shayan Hosseinzadeh, Dian Yang, Angela N Pogson, Marco Y Hein, Kyung Hoi Joseph Min, Li Wang, et al. Mapping transcriptomic vector fields of single cells. Cell, 185(4):690–711, 2022.\n2. Alexander Tong, Jessie Huang, Guy Wolf, David Van Dijk, and Smita Krishnaswamy. Trajectorynet: A dynamic optimal transport network for modeling cellular dynamics. In International conference on machine learning, pages 9526–9536. PMLR, 2020.\n3.  Yutong Sha, Yuchi Qiu, Peijie Zhou, and Qing Nie. Reconstructing growth and dynamic trajectories from single-cell transcriptomics data. Nature Machine Intelligence, 6(1):25– 39, 2024.\n\nThis review was independently written by the reviewer. An LLM was employed solely for minor phrasing and grammar improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VjFprwbgot", "forum": "DzSNH5APPl", "replyto": "DzSNH5APPl", "signatures": ["ICLR.cc/2026/Conference/Submission8915/Reviewer_51Jg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8915/Reviewer_51Jg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576157465, "cdate": 1761576157465, "tmdate": 1762920665949, "mdate": 1762920665949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Cell-MNN, a ODE–based framework that learns explicit single-cell dynamics by modeling local linear ODEs in a PCA latent space. The approach avoids explicit OT preprocessing and claims to simultaneously provide interpretable gene regulatory interactions through the learned Jacobian matrices. Experiments on several benchmark single-cell datasets show competitive interpolation accuracy and advantages over OT-based baselines."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and tries to tackle an important problem in single-cell dynamic modeling — learning interpretable continuous-time dynamics."}, "weaknesses": {"value": "1. Latent-space linearization is under-justified and fragile. The method hinges on representing dynamics as locally linear in a PCA latent space, $\\dot z \\approx A_\\theta(z,t)z$, advanced via a matrix exponential. The paper itself acknowledges that evolving too far leaves the validity region and requires re-encoding to refresh the local ODE, but provides no analysis of linearization error growth, step-size control, or robustness to curvature in realistic trajectories. Moreover, the claimed efficiency advantage relies on a very low latent dimension; once d_z grows, computing and applying the operator (incl. eigendecomposition/inversion) becomes the dominant cost (see 4), which does not resolve the core limitations of Neural ODEs for stiff/strongly nonlinear flows— it only replaces numerical integration with piecewise-linear surrogates without guarantees.\n2. GRN novelty is overstated; prior work is insufficiently acknowledged. Inferring gene regulatory relations from a learned ODE/vector field via its Jacobian has been proposed and used before (e.g., Dynamo [1]) and applied in multi-timepoint trajectory settings (e.g., TrajectoryNet [2], TIGON [3]). Cell-MNN can be viewed as the special case where the vector field is constrained to be piecewise linear in a low-dimensional PCA subspace. The “Gene Regulatory Network Discovery” related-work discussion omits or underplays these lines, which is misleading about the methodological novelty. \n3. Scalability claims (vs. OT) are overstated, and the mechanism is unclear. The paper attributes scalability improvements to replacing OT preprocessing with MMD-based distribution matching and reports out-of-memory (OOM) errors for standard OT on inflated datasets. However, many Neural ODE–based or flow-matching methods already alleviate memory issues by computing OT or matching losses in mini-batches, thereby avoiding O(n^2) coupling matrices in practice. From this perspective, it is unclear why Cell-MNN “solves” sample-size scalability more effectively than these existing approaches, given that both rely on batched stochastic training. Moreover, substituting OT with balanced MMD does not directly address unbalanced dynamics (cell proliferation/death), where Unbalanced-SB frameworks provide principled formulations. Thus, the claimed scalability advantage seems more an artifact of implementation choices than a fundamental methodological improvement.\n4. Cubic scaling in the latent dimension threatens practicality beyond very low $d_z$. The paper states $O(T d_z^2)$ for applying the analytical solution, but also a one-time $O(d_z^3)$ per operating point to form the operator (e.g., eigendecomposition/inversion). For $d_z=50–100$, this rapidly dominates and can exceed the cost of Neural ODE field evaluations, undermining the claimed efficiency. The assertion that 5D PCA is “sufficient” is not substantiated by ablations or biological coverage analyses. And I also believe this assertion is not correct.\n5. Wall-clock efficiency is not compelling. Despite a 5D latent space and no OT, the reported runs surprisingly high (one hour) for the stated computational simplicity, and notably slower than many baselines under comparable settings. The paper does not explain where time is spent (e.g., repeated eigendecompositions, kernel MMD computation) or provide profiling.\n6. Redundant/ill-motivated invertibility regularization. The additional term $L_{\\text{inv}}(\\theta)$, encouraging $P_\\theta$ invertibility, appears unnecessary: invertible matrices are dense; a randomly initialized square matrix is almost surely invertible. The paper does not justify why this regularizer is needed, nor analyze its numerical side-effects (e.g., unstable gradients via $\\det(\\cdot)$).\n7. Interpolation results of OT baselines are unclear. In Table 1, several OT-based methods underperform the OT-Interpolate baseline, while Cell-MNN surpasses it. Since many methods eventually regress toward OT-derived supervision, it is unclear why Cell-MNN outperforms those indirect OT-fitting approaches. The paper should articulate the mechanism (e.g., bias/variance advantages of distribution-level MMD vs. velocity-level supervision, regularization effects) and provide controlled comparisons \n8. Amortized training procedure lacks essential details. Section 4.2 merges datasets in PCA space for amortized training and feeds a dataset index to the model, but does not explain the details.\n9. Gene interaction validation lacks competitive baselines. Validation against TRRUST is useful, but the paper does not compare to other vector-field/Neural-ODE methods that could also extract Jacobian-based interactions under the same preprocessing. Without head-to-head comparisons, it remains unclear whether Cell-MNN offers any real advantage for GRN discovery beyond the convenience of an explicit linear operator.\n10. Insufficient ablations/sensitivity analyses. Key design choices—kernel and bandwidth for MMD, discount factor $\\gamma$, regularization weights $\\lambda_{\\text{kin}}$, $\\lambda_{\\text{inv}}$, latent dimension $d_z$, operator parameterization (e.g., fixing one eigenvalue to 0 vs. not ), and $\\Delta t$ sampling—lack systematic ablations. Given that interpretability and stability hinge on these knobs, their impacts should be quantified."}, "questions": {"value": "Due to these concerns, I lean towards rejecting the paper in its current form. Significant theoretical justification, experimental clarification, and comparison to prior ODE-based methods would be needed for a more favorable evaluation. The authors should: \n\n1. Clarify justification for latent-space linearization. (See weakness 1)\n2. Discuss the relation to prior GRN-from-ODE works. (See weakness 2)\n3. Substantiate the scalability argument. (See weakness 3)\n4. Provide complexity and latent-dimension analysis.  (See weakness 4)\n5. Report detailed runtime and efficiency breakdown.(See weakness 5)\n6. Revisit the invertibility regularization. (See weakness 6)\n7. Clarify interpolation results and superiority to OT baselines. Could the authors explain why Cell-MNN achieves superior interpolation despite training toward similar objectives?  (See weakness 7)\n8. Elaborate on amortized training implementation. (See weakness 8)\n9. Enhance GRN validation comparisons (e.g., trajectorynet, TIGON). (See weakness 9) \n10. Include hyperparameter and kernel sensitivity studies. (See weakness 10)\n\nReferences\n1. Xiaojie Qiu, Yan Zhang, Jorge D Martin-Rufino, Chen Weng, Shayan Hosseinzadeh, Dian Yang, Angela N Pogson, Marco Y Hein, Kyung Hoi Joseph Min, Li Wang, et al. Mapping transcriptomic vector fields of single cells. Cell, 185(4):690–711, 2022.\n2. Alexander Tong, Jessie Huang, Guy Wolf, David Van Dijk, and Smita Krishnaswamy. Trajectorynet: A dynamic optimal transport network for modeling cellular dynamics. In International conference on machine learning, pages 9526–9536. PMLR, 2020.\n3.  Yutong Sha, Yuchi Qiu, Peijie Zhou, and Qing Nie. Reconstructing growth and dynamic trajectories from single-cell transcriptomics data. Nature Machine Intelligence, 6(1):25– 39, 2024.\n\nThis review was independently written by the reviewer. An LLM was employed solely for minor phrasing and grammar improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VjFprwbgot", "forum": "DzSNH5APPl", "replyto": "DzSNH5APPl", "signatures": ["ICLR.cc/2026/Conference/Submission8915/Reviewer_51Jg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8915/Reviewer_51Jg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576157465, "cdate": 1761576157465, "tmdate": 1763695697311, "mdate": 1763695697311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for learning a locally linear differential equation in latent space that describes gene expression changes during cell differentiation. The latent space itself is also a linear projection from gene space. A key idea of the paper is to make the dynamics analytically tractable so that gene interactions can be explicitly modeled."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths: \n•\tThe idea is well-motivated, theoretically sound, and biologically meaningful.\n•\tThe idea is clearly explained. The paper is well-written and I found it easy to read\n•\tThe evaluations presented are overall sound and sensible."}, "weaknesses": {"value": "Weaknesses:\n•\tHighly similar concept to VeloVAE, ICML 2022. VeloVAE fits a mixture of linear ODEs and uses an encoder-decoder framework to perform Bayesian inference of cell times and ODE parameters. Another highly similar paper is LatentVelo, which fits a neural ODE end-to-end to learn dynamics in the latent space. Dynamo uses a linear ODE to estimate the Jacobian of gene regulation.\n•\tThe method seems more incremental than revolutionary. There are closely related approaches for the same problem. The move to a locally linear ODE doesn't seem that impactful to me.\n•\tBaseline chosen for gene regulatory network evaluation is very weak. There are dozens of gene regulatory network inference algorithms that take single-cell RNA-seq data as input. Comparing performance against these would be more informative. Dynamo (Qiu et al. Cell 2022) seems particularly relevant, because a stated goal of the method is to recover gene-gene interactions by estimating the Jacobian.\n•\tI understand the motivation for interpretability, but it seems that this local linearity would have to come with some loss of predictive power. Approaches like VeloVAE don’t suffer from this limitation while retaining interpretability. I don't understand how this restricted model can outperform less interpretable but more expressive models, apart from scalability concerns.\n•\tEvaluation in terms of rate parameters seems important. The locally linear ODE can be interpreted in terms of gene expression changes (RNA velocity), and thus it's important to benchmark against the class of methods that aim to estimate these rates directly. Ground truth in the form of metabolic labeling data (see Dynamo paper) is available for an increasing number of datasets."}, "questions": {"value": "1. How is your approach different from VeloVAE and LatentVelo?\n2. Why does your less expressive approach outperform more expressive previous models? This seems surprising, because I would expect the local linearity constraint to make the predictions less accurate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "stpct1FlC9", "forum": "DzSNH5APPl", "replyto": "DzSNH5APPl", "signatures": ["ICLR.cc/2026/Conference/Submission8915/Reviewer_4yFL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8915/Reviewer_4yFL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919184527, "cdate": 1761919184527, "tmdate": 1762920665463, "mdate": 1762920665463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}