{"id": "ya00JrKTjp", "number": 8461, "cdate": 1758084898344, "mdate": 1759897782579, "content": {"title": "Reducing Semantic Mismatch in Brain-to-Text Decoding Through Personalized Multimodal Masking", "abstract": "The rapid progress of large vision-language models (VLMs), such as CLIP, has spurred the development of a wide range of neural decoding frameworks. Nevertheless, most existing approaches still suffer from semantic mismatches during representational alignment. This challenge may stem from the fact that the human brain does not distribute attention uniformly across a visual scene, but rather selectively encodes salient or relevant regions. Moreover, such selectivity is closely related to individual interests and varies from person to person. To address this challenge, we propose Yo'Mind, a novel optimal transport (OT)-driven personalized multimodal semantic masking framework designed to bridge the semantic gap between brain and machines in interpreting visual scenes. Technically, Yo'Mind introduces a dynamic semantic pruning and allocation mechanism that adaptively masks redundant visual semantic components in stimulus images based on individual neural responses—without requiring extra human supervision or hyperparameter tuning. This strategy can be used to enhance semantic consensus between brain and machine representations during decoding. Furthermore, the inherent flexibility of OT theory enables Yo'Mind to perform brain-visual-linguistic alignment and cross-subject decoding within a unified end-to-end architecture. Extensive experiments demonstrate that our Yo'Mind offers several advantages, including state-of-the-art brain-to-text reconstruction performance and improved interpretability of the decoding process.", "tldr": "", "keywords": ["Brain-to-text reconstruction", "Neural decoding", "Semantic decoding", "Multimodal learning"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/362760f89532a1329f1ddfa6a598e63373a5fba9.pdf", "supplementary_material": "/attachment/09bfb4c5b53a9e565b5493ab365424540c09857e.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a novel semantic decoding model Yo'Mind that utilizes Optimal Transport (OT) theory to allocate semantic components to fMRI patches. Using this framework, the paper implements cross-subject fine-grained semantic-level brain decoding. Experimental results confirm the powerful capability of the proposed Yo'Mind in semantic decoding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The authors introduce OT theory into the brain decoding field and propose a novel subject-adaptive and fine-grained alignment semantic decoding framework.\n\n(2) Experimental results validate that the proposed model has strong semantic decoding capabilities."}, "weaknesses": {"value": "(1) **Unfair comparison**. Although the proposed model achieves superior performance in Table 1, this comparison is not fair. As shown in Table 2, performance significantly improves when the voxel number is increased. Some baselines, such as UMBRAE [1], used fewer voxels (subj01: 15,724), while the author compared it with more voxels (subj01: 21,721). Additionally, I am curious why the performance of UMBRAE slightly differs from the report in the original paper.\n\n(2) **Lack of low-level decoding ability validation (image reconstruction task)**. The paper only validates the model on the brain caption task, lacking validation on direct image reconstruction. Image reconstruction tasks can assess the model’s performance at the low-level. Note that both low-level (e.g. perceptual) and high-level (e.g. semantic) capture capabilities are equally important for brain decoding [2]. I am concerned that by using OT to allocate different semantic components to fMRI patches, the model may disrupt the spatial information of the original visual stimuli, which could result in a lack of low-level decoding ability. Therefore, the proposed model may have gained high-level decoding ability at the cost of sacrificing low-level decoding ability. The low-level decoding ability needs to be measured through the metrics of image reconstruction tasks (e.g., SSIM, FID, PSNR).\n\n(3) **Biologically unreasonable OT-driven semantic allocation**. The author uses OT to allocate different semantic components to fMRI patches from different brain regions. However, this operation may violate the biological response of the human brain to visual stimuli. In fact, some lower-level brain areas (such as V1-V4) are responsible for processing raw optical information, while higher-level brain areas handle color and texture, and even higher-level areas are responsible for semantic understanding [3]. But the authors allocate semantic components to all brain regions using OT, including those that are biologically not involved in semantic understanding. Therefore, I question the validity of this OT-based approach. In addition, the authors fix the number of fMRI patches at 8, but there is a lack of further explanation (or ablation) for this choice.\n\n(4) **Incomplete validation of the motivation and model efficacy**. The author’s motivation is that different subjects focus on different areas when viewing the same image. Although the author provides a visualization of the OT semantic mask in Figure 5, there is no evidence to prove that the proposed OT-driven method truly captures the subjects’ attention. It would be interesting if this could be confirmed (for example, through eyetracking data provided by NSD).\n\n\n\n(5) **Some expressions in the paper are not clear enough**. (a) In Figure 5a, the author states, \"The first row shows examples of intra-subject consistency, and the second row presents examples of inter-subject variability.\" However, the figure lacks detailed annotations, so we cannot clearly understand what the four images for the same sample represent (four different subjects? or inferences with different seeds from the same subject?). (b) The citation format throughout the paper is incorrect, which makes it difficult to read. Please use the `\\citep` command. (c) The mathematical expressions are confusing. In Line 210, what do you mean by $\\min_{\\mathbf\\Gamma\\in\\Pi}$? What is $\\Pi$? It has not been described earlier in the text. In Equation 2, what is $\\mathbf 1_{N+M}$ and $\\mathbf 1_{K}$? ... These are just examples of the issues I mentioned.\n\n[1] Xia et al., Umbrae: Unified Multimodal Brain Decoding. ECCV, 2024.\n\n[2] Scotti et al., Mindeye2: Shared-subject models enable fmri-to-image with 1 hour of data. ICML, 2024.\n\n[3] Grill-Spector et al., The Human Visual Cortex. Annu. Rev. Neurosci., 2004."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UVf0zJQh5j", "forum": "ya00JrKTjp", "replyto": "ya00JrKTjp", "signatures": ["ICLR.cc/2026/Conference/Submission8461/Reviewer_4kw3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8461/Reviewer_4kw3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760626271548, "cdate": 1760626271548, "tmdate": 1762920345141, "mdate": 1762920345141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the semantic mismatch in brain-to-text decoding and proposes Yo’Mind, an optimal transport-driven personalized multimodal semantic masking framework that dynamically prunes redundant semantic components. Its key innovations include introducing a virtual \"dustbin\" for soft-selection masking, unifying brain-visual-linguistic alignment, and enabling cross-subject decoding without extra supervision or hyperparameter tuning. On the NSD dataset, Yo’Mind outperforms baselines like Mind-SA across metrics and achieves better fine-grained decoding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unlike prior fixed-patch masking methods, Yo’Mind introduces a virtual \"dustbin\" into the OT framework to realize soft, adaptive semantic pruning, which automatically assigns irrelevant visual-textual components to the dustbin based on individual fMRI responses, enabling fine-grained alignment with human selective attention.\n2. This paper creatively integrates CLIP-derived visual patch embeddings and MLLM-generated textual embeddings into OT-driven alignment, extending traditional brain-visual mapping to brain-visual-linguistic alignment.\n3. Rigorous experiments (NSD dataset, 8 metrics, ablations on brain regions/masking) and clear qualitative analyses confirm Yo’Mind’s superiority over SOTA."}, "weaknesses": {"value": "1. The study only used fMRI data from 4 out of 8 subjects, 1, 2, 5, 7, in the NSD dataset, which may not enough to fully prove its cross-subject decoding works well for everyone. It should test the other 4 subjects too, to see if Yo’Mind stays reliable across more diverse people.\n2. It only used one MLLM, Qwen2.5, to make textual semantic cues. We can’t tell if the better performance comes from mixing visual and text data, or just because Qwen2.5 is good. It should try other common MLLMs, better or worse, to show the design works no matter which MLLM is used.\n3. This paper says it captures “brain-preferred semantics,” but there is no check with real physiological data like eye-tracking. If it compared the model’s focused semantic areas to where subjects actually looked, that would make the claim way more believable."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ps15FPMMrI", "forum": "ya00JrKTjp", "replyto": "ya00JrKTjp", "signatures": ["ICLR.cc/2026/Conference/Submission8461/Reviewer_eqj6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8461/Reviewer_eqj6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762756625414, "cdate": 1762756625414, "tmdate": 1762920344571, "mdate": 1762920344571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Yo’Mind, a new framework for brain-to-text decoding using fMRI. The key idea is to reduce the semantic mismatch between brain activity and machine representations. The authors argue that current large vision–language models encode all semantic content in an image, whereas the human brain selects only the salient elements, and this selection differs across individuals. It introduces OT-driven personalized multimodal semantic masking, where an optimal-transport assignment dynamically determines which semantic components (image patches and text attributes) align with brain responses. The method is tested on NSD Dataset and achieves SOTA performance on most captioning metrics (BLEU, METEOR, CIDEr, SPICE)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Motivation is strong and well-founded.\nThe paper explicitly addresses a subtle but fundamental issue identified in Mind-SA (ICCV'25): neural decoding suffers from semantic mismatch because machine representations capture full-scene semantics while the brain encodes only selective ones.\n\n2. Improved interpretability.\nVisualizations of “brain-preferred semantic regions” show intra-subject consistency and inter-subject variability, which is a compelling neuroscientific insight.\n\n3. Extensive quantitative comparison.\nIncludes multiple recent baselines (MindGPT, Mind-SA, UMBRAE, Neuro2Language).\nResults show consistent improvements across most metrics.\n\n4. Novel formulation but simple integration.\nThe OT masking module is differentiable and can be plugged into any brain-to-text model, not just MindGPT."}, "weaknesses": {"value": "1. Evaluation is restricted to one dataset (NSD).\nNSD is the standard benchmark, but the claim of general fine-grained multimodal alignment would be more convincing with additional datasets (other extended fMRI tasks).\n\n2. Dependency on generated textual semantics may bias supervision.\nSince text semantics are produced by Harmon/Qwen2.5, the reconstruction target space partly inherits the biases of that MLLM. No robustness study is provided to show how performance varies across different captioning models or prompts.\n\n3. Statistical significance not reported.\nImprovements over Mind-SA are meaningful (≈2 BLEU / ≈2 CIDEr), but there are:\nno confidence intervals,\nno statistical significance test (e.g., bootstrap / paired permutation test).\n\n4. OT computational cost and scalability not thoroughly analyzed.\nSinkhorn is run for 100 iterations, but no discussion on: efficiency impact when using more image patches, O(N×M) scaling when adding more semantic text attributes.\n\n5. The novelty is incremental, not groundbreaking.\nThe paper improves Mind-SA’s masking strategy, but still inherits the architecture/flow of MindGPT. The conceptual leap may be viewed as methodological refinement rather than a new brain decoding paradigm."}, "questions": {"value": "1. Robustness to textual supervision\nWhat happens if you generate attribute text from a smaller model or random captions?\nIs the gain from text semantics inherent or model-dependent?\n\n2. Can Mind-SA perform competitively with adaptive masking?\nA stronger baseline would be a dynamic masking version of Mind-SA (not fixed patch count).\n\n3. Statistical significance\nPlease provide confidence intervals or hypothesis tests for BLEU/CIDEr results.\n\n4. Scalability\nHow does complexity scale when increasing #patches / #text tokens? Is OT a bottleneck?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ULTyQbwePL", "forum": "ya00JrKTjp", "replyto": "ya00JrKTjp", "signatures": ["ICLR.cc/2026/Conference/Submission8461/Reviewer_AFHn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8461/Reviewer_AFHn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762772051191, "cdate": 1762772051191, "tmdate": 1762920344028, "mdate": 1762920344028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Yo'Mind, an Optimal Transport-driven personalized multimodal semantic masking framework for brain-to-text decoding. The work addresses the critical challenge of semantic mismatch between machine representations and human brain signals during neural decoding. The proposed method introduces a dynamic semantic pruning mechanism that adaptively masks redundant visual semantic components based on individual neural responses, without requiring additional human supervision or hyperparameter tuning. The authors demonstrate state-of-the-art performance on the Natural Scenes Dataset while improving interpretability of the decoding process."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel Theoretical Framework:** The integration of Optimal Transport theory into brain signal decoding represents a significant innovation, providing a mathematically rigorous approach to address semantic mismatch. The virtual dustbin mechanism elegantly resolves the mass conservation constraint in traditional OT, allowing for adaptive semantic filtering.\n2. **Biologically-Inspired Design:** The multimodal semantic set construction combining visual patches and textual descriptions aligns well with human cognitive processing. The soft assignment mechanism better reflects the brain's distributed representation compared to hard masking approaches.\n3. **Comprehensive Experimental Evaluation:** The paper includes thorough comparisons with 8 state-of-the-art methods across multiple standard metrics (BLEU, METEOR, ROUGE, CIDEr, SPICE), demonstrating clear performance advantages. The ablation studies on brain regions and multimodal components provide valuable neuroscientific insights.\n4. **End-to-End Differentiable Architecture:** The seamless integration of OT modules with fMRI encoding and text decoding enables joint optimization while maintaining differentiability through Sinkhorn iterations, supporting both intra-subject consistency and inter-subject variability modeling."}, "weaknesses": {"value": "1. The selection of cosine distance as the cost metric lacks sufficient biological or theoretical justification. Similarly, critical hyperparameters (ε=1 for Sinkhorn, K=8 patches) appear to be set empirically without sensitivity analysis.\n\n2. Results lack statistical significance testing (p-values, confidence intervals), and evaluation is confined to the NSD dataset without cross-dataset validation. The fixed loss weight (10 for alignment loss) seems arbitrary and may not generalize.\n\n3. The computational overhead of Sinkhorn iterations and OT optimization is not quantified, despite being crucial for practical applications. There's no analysis of training/inference time, memory requirements, or computational complexity.\n\n4. While attention visualizations are provided, deeper analysis linking the learned representations to established neuroscientific principles is lacking. The choice of standard ViT as fMRI encoder lacks biological adaptation reasoning.\n\n5. Insufficient details about implementation specifics hinder reproducibility, and the paper lacks discussion of brain privacy concerns and potential misuse of decoding technology."}, "questions": {"value": "Please see the weakness above."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Please refer to the weakness 5."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yAywtnMyZb", "forum": "ya00JrKTjp", "replyto": "ya00JrKTjp", "signatures": ["ICLR.cc/2026/Conference/Submission8461/Reviewer_P2dE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8461/Reviewer_P2dE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762782370803, "cdate": 1762782370803, "tmdate": 1762920343644, "mdate": 1762920343644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the task of reconstructing language from fMRI signals evoked by visual stimuli.\nThe authors focus on the fMRI representation learning stage of this task. Their main idea is that human subjects tend to pay attention to local regions within an image rather than processing the entire image uniformly.\nAccording to the authors, a previous approach enables the removal of unattended image patches under end-to-end optimization. The remaining patches (or tokens) are then used to compute the CLIP loss together with the fMRI embeddings.\nIn this paper, in addition to aligning the fMRI embeddings with a subset of image patches, the authors also introduce alignment between fMRI embeddings and a subset of tokens from the original image caption. Moreover, they improve the process of discarding unattended image and text tokens by leveraging the Optimal Transport (OT) framework.\nThe authors conduct experiments on the Natural Scenes Dataset (NSD), and their results outperform the previous related works they follow."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides a well-organized and comprehensive review of the related work.\n\n2. The method proposed in this paper outperforms the baseline models."}, "weaknesses": {"value": "1. Since the model is optimized in an end-to-end manner, its loss function is computed between the reconstructed text and the ground-truth (GT) captions provided in the MSCOCO dataset. Such a supervision signal seems to encourage the model to extract information from the fMRI signals that is relevant to the GT text, and thus, during the removal of image/text tokens, it appears to discard those unrelated to the GT captions. This training process, therefore, might not actually capture the image regions that the subject’s attention was focused on. In fact, the retained image and text tokens are those that are highly correlated with the ground-truth (GT) captions, which is also consistent with the visualization results shown in Figure 5. Therefore, in my view, the method proposed in this paper does not truly capture the regions that the subjects focused on during the visual perception process."}, "questions": {"value": "1. Why does Figure 5 only show 80% of the retained patches? This kind of selective visualization does not represent the actual results produced by the model.\n\n2. I’m quite curious why the proposed method and the baseline model MindSA were not evaluated on the fMRI-to-image reconstruction task. If the trained fMRI representation model can truly capture the regions that subjects focus their attention on, then the reconstructed images should semantically correspond to the remaining (attended) image patches. Such an evaluation would provide stronger evidence for the effectiveness of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bSZXjzaqlc", "forum": "ya00JrKTjp", "replyto": "ya00JrKTjp", "signatures": ["ICLR.cc/2026/Conference/Submission8461/Reviewer_HQz8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8461/Reviewer_HQz8"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762794875802, "cdate": 1762794875802, "tmdate": 1762920343389, "mdate": 1762920343389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}