{"id": "BScFgd7MGm", "number": 3970, "cdate": 1757576767791, "mdate": 1759898060365, "content": {"title": "Towards Scalable and Consistent 3D Editing", "abstract": "3D editing—the task of locally modifying the geometry or appearance of a 3D asset—has wide applications in immersive content creation, digital entertainment, and AR/VR. However, unlike 2D editing, it remains challenging due to the need for cross-view consistency, structural fidelity, and fine-grained controllability. Existing approaches are often slow, prone to geometric distortions, or dependent on manual and accurate 3D masks that are error-prone and impractical.  To address these challenges, we advance both the data and model fronts. On the data side, we introduce 3DEditVerse, the largest paired 3D editing benchmark to date, comprising 116,309 high-quality training pairs and 1,500 curated test pairs. Built through complementary pipelines of pose-driven geometric edits and foundation model-guided appearance edits, 3DEditVerse ensures edit locality, multi-view consistency, and semantic alignment.  On the model side, we propose 3DEditFormer, a 3D-structure-preserving conditional transformer. By enhancing image-to-3D generation with dual-guidance attention and time-adaptive gating, 3DEditFormer disentangles editable regions from preserved structure, enabling precise and consistent edits without requiring auxiliary 3D masks.  Extensive experiments demonstrate that our framework outperforms state-of-the-art baselines both quantitatively and qualitatively, establishing a new standard for practical and scalable 3D editing. Dataset and code will be released. Project: https://anonymousresearch37.github.io/3DEditFormer/", "tldr": "We introduce 3DEditVerse, the largest paired 3D editing benchmark, and propose 3DEditFormer, a mask-free transformer enabling precise, consistent, and scalable 3D edits.", "keywords": ["3D Editing", "3D Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49c2ac4f43163a2b1568f1d65afaf7a5f6d834b8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents (1) 3DEditVerse, a large paired 3D editing dataset (~118K train pairs, 1.5K curated test pairs) created via two automated pipelines (pose-driven geometric edits and a text-image-3D appearance-edit pipeline) and (2) 3DEditFormer, a conditional transformer that injects multi-stage structural features from a frozen image-3D backbone (Trellis) via a Dual-Guidance Attention Block and a Time-Adaptive Gating mechanism to preserve unedited structure during localized edits. The authors report large quantitative gains over several baselines and ablate key components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed dataset and benchmark is very helpful in this field, and addresses a real bottleneck of supervised 3d editing.\n2. Clear model idea addressing an important failure mode (multi-layer feature fusion, and gating mechanism for better local editing and globally preserve the details)\n3. The experiments are comprehensive, and the final performance clearly outperforms existing baselines (mainly VoxHammer).\n4. The proposed method is very practical to the 3d-aigc research and industry."}, "weaknesses": {"value": "1. The main issue lies in the dataset curation. Though the text-image and image-3D pipeline can facilitate generating a large amount of paired data, I wonder whether this is the right / proper solution for 3D editing, since it will definitely involve error accumulation in the data conversion. For example, can the dynamic 3d object (like dataset used in Diffusion4D, be directly used for 3d editing? No error accumulation will be involved in this way)\n2. The writing is not very coherent, perhaps due to this paper introduces many components (dataset, benchmark, model pipeline, and so on). \n3. This method is based on trellis3d, and for SoTA 3D asset generative pipelines like Hunyuan3D 2.1, I wonder whether the proposed method can be still applied?"}, "questions": {"value": "1. Some native 3d diffusion models discussions are missing in the related work, like 3DTopia-XL (cvpr 25), GaussianAnything (ICLR 25), and also vecset-based (3Dshape2vec, siggraph 23) baselines.\n2. Another line of work that aims for unified 3d generation, understanding, and editing, e.g., ShapeLLM-Omni (NIPS 25). More discussions / comparisons with this line of work should be introduced. Though ShapeLLM-Omni does not introduce 3d editing yet, it is very straightforward following the 2D domain progress (gpt-4o, meta-query, and blip-3o).\n\nI would like to increase the score if the author could address my concerns in the rebuttal stage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sFOsuGR8TB", "forum": "BScFgd7MGm", "replyto": "BScFgd7MGm", "signatures": ["ICLR.cc/2026/Conference/Submission3970/Reviewer_MB98"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3970/Reviewer_MB98"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761492502601, "cdate": 1761492502601, "tmdate": 1762917119087, "mdate": 1762917119087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a complete pipeline for preparing 3D editing pair data, termed 3DEditVerse, and a novel 3D editing method called 3DEditFormer.\nFor the dataset, it consists of two parts: a pose-driven dataset and an appearance-driven dataset.\nFor the editing method, the authors propose a dual-attention mechanism that attends to both the semantic target image instruction and the fine-grained structural details of the source object.\nComprehensive comparisons with existing state-of-the-art 3D editing models demonstrate the effectiveness of both the proposed dataset and the method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a data pipeline for producing 3D-consistent edited data pairs, which is an important contribution to this research area. The pose-driven part demonstrates strong 3D consistency since it is constructed using animation pose data. Although the appearance-driven part still has some limitations in 3D consistency, it achieves higher consistency compared to previous data generation pipelines.\n\n2. The dual-attention module in 3DEditFormer is novel. It simultaneously considers both semantic information from the target image and fine-grained structural details from the source object, enabling the model to accurately edit the target regions while preserving the original geometric features."}, "weaknesses": {"value": "1. The pose-driven part of the dataset applies animation poses to characters, which makes it primarily applicable to human-like objects. Therefore, this type of structural editing is somewhat limited in scope.\n\n2. The appearance-driven data generation pipeline appears overly complex, as it involves many modules and processing stages. Such complexity increases the likelihood of failure at each stage and makes the overall pipeline less stable.\n\n3. The paper does not provide video results to verify whether the edits maintain good quality and align well with source objects and target image, which is particularly important in 3D editing."}, "questions": {"value": "1. There are other types of 3D editing beyond the pose-driven category, such as articulated-object or part-level editing, where ground-truth data could also be used to construct 3D edited pairs. Could the authors provide more discussion on the applicability of their data pipeline to these types of editing tasks?\n\n2. The appearance-driven data generation pipeline appears quite complex, involving multiple stages:\n(1) prompt generation, (2) source image generation, (3) edit prompt generation, (4) target image generation, (5) 3D pair initialization, (6) 2D and 3D mask computation, and (7) 3D inpainting.\nSince each stage may fail and require manual filtering, could the authors report the success rate of each module as well as the overall pipeline?\n\n3. Could the authors provide video demonstrations of the 3D editing results to better illustrate the consistency of the edits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R7nVpt3Dyf", "forum": "BScFgd7MGm", "replyto": "BScFgd7MGm", "signatures": ["ICLR.cc/2026/Conference/Submission3970/Reviewer_41EE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3970/Reviewer_41EE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726611545, "cdate": 1761726611545, "tmdate": 1762917118589, "mdate": 1762917118589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces 3DEditVerse, a large-scale paired 3D editing dataset, and 3DEditFormer, a transformer-based framework that preserves 3D structure consistency during localized editing. The dataset combines pose-driven geometric and text-guided appearance edits, while the model employs a dual-guidance attention mechanism and time-adaptive gating to disentangle editable and preserved regions. Extensive experiments demonstrate improved 3D editing performance compared with previous baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall pipeline is well structured, integrating dataset design and model development in a coherent way.\n\n2. The generation process of 3DEditVerse is reasonable and clearly described. Although current edits are limited to addition/deletion operations without other types of edits, e,g., local deformations, this is acceptable since “editing” itself covers a broad concept.\n\n3. The experiments are comprehensive and reproduce multiple baselines, showing consistent quantitative advantages."}, "weaknesses": {"value": "1. Could the framework be extended to support deformation-like edits, not just add/remove or appearance changes? Such capability would make the system more general and practically valuable.\n\n2. There are two diffusion modules in Trellis (for voxel generation and structured latent modeling). Are both feature hierarchies actually leveraged in 3DEditFormer, or only one of them? \n\n3. How about the inference time consumption?\n\n4. Minor one: the title emphasizes “Consistent”, but the paper does not clearly define or quantify “consistency” — is it geometric consistency, text-geometry alignment consistency, or temporal consistency across views?"}, "questions": {"value": "As with weaknesses,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MJvYkidWhK", "forum": "BScFgd7MGm", "replyto": "BScFgd7MGm", "signatures": ["ICLR.cc/2026/Conference/Submission3970/Reviewer_En71"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3970/Reviewer_En71"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791886868, "cdate": 1761791886868, "tmdate": 1762917118337, "mdate": 1762917118337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to advance 3D editing from both data and model perspectives. It introduces a large-scale paired “before/after” dataset called 3DEditVerse (claiming 116,309 training pairs and 1,500 testing pairs) and proposes 3DEditFormer, which builds upon the frozen Trellis image-to-3D generation framework by adding dual-branch cross-attention and temporal adaptive gating. The method claims to achieve localized and cross-view consistent editing without requiring 3D masks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper combines geometry-editing (pose-driven) and appearance-editing (text-guided) pipelines to produce paired samples with local and consistent changes at a larger scale than prior work.\n2.  The idea of separating fine-grained structural features at late diffusion timesteps from semantic transition features at early timesteps is reasonable and clearly implemented through temporal gating."}, "weaknesses": {"value": "1. The comparison with VoxHammer seems unfair: for methods requiring masks, the authors remove subsets without character-animation masks, while for their own method, they report both full and subset results, including a favorable “radius inflation” test. The comparison setup benefits their approach.\n2. The data pipeline depends on a chain of large pre-trained models (DeepSeek-R1, Flux, Qwen-VL, Trellis, SAM2), introducing heavy model bias and coupling—these models might appear again during evaluation, compromising fairness and generalization to real-world assets (complex backgrounds, non-centered subjects, lighting variations).\n3. The use of clean, white backgrounds for stable lifting amplifies bias and limits applicability to realistic scenes. No real-scene qualitative results are shown."}, "questions": {"value": "1. Please summarize performance under occlusion, cluttered backgrounds, high-frequency textures (e.g., hair, wires), and topology changes.\n2. How does the temporal gate behave across edit types (texture-only, geometry-only, mixed)? Any instability or gradient conflict observed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q3EWmgV2Ba", "forum": "BScFgd7MGm", "replyto": "BScFgd7MGm", "signatures": ["ICLR.cc/2026/Conference/Submission3970/Reviewer_dgbA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3970/Reviewer_dgbA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913921821, "cdate": 1761913921821, "tmdate": 1762917118103, "mdate": 1762917118103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}