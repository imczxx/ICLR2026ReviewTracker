{"id": "0BkvUY61MX", "number": 23289, "cdate": 1758341769702, "mdate": 1759896822547, "content": {"title": "ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality", "abstract": "Scaling laws research has focused overwhelmingly on English—yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R². Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 × 38 = 1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models—beyond English-first AI.", "tldr": "Scaling laws for multilingual pretraining, finetuning, and language transfer.", "keywords": ["scaling laws", "multilinguality"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8058aba8e6566d3587e70df9f688d9784b59a1bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "1. The paper presents scaling laws in the multilingual setup across different axis:\n\n1.1 For repeated epochs in the monolingual setup\n\n1.2 To account for cross lingual transfer in a language mixture setup\n\n1.3. Account for the curse of multilinguality by providing a closed form approximation to account for how much more data to train / how many more parameters to account for to keep the loss on a target language consistent, while adding new languages.\n\n1.4. Address the issue when is it better to pretrain a language model for a target language from scratch vs finetune a multilingual language model.\n\n2. The authors demonstrate that their scaling laws fit better for monolingual and multilingual scenarios compared to other monolingual, data constrained and multilingual scaling laws respectively. In order to account for scaling across different languages with different vocabularies, they fit scaling laws to the vocabulary insensitive loss.\n\n3. The authors also present a very large scale cross lingual transfer study. They propose a cross lingual transfer metric: the number of target language tokens taken by a bilingual model to reach the same loss as that taken by the target language monolingual model. By analysing the transfer matrix, the authors highlight the key factors associated with positive transfer: language script as well as language family. They also demonstrate that these factors additionaly seem predictive of whether a symmetric transfer might occur between the languages. \n\n4. The authors present their results over an impressive number of experiments. If the results (esp. details on number of parameters, tokens, loss convergence values, mixture weights, data subset etc.) are released, it can enable additional analysis especially w.r.t cross-lingual transfer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a new functional form for modeling multilingual setups, accounting for repeated tokens as well as data mixtures. The consequent scaling law has better predictive power compared to other baselines.\n\n2. For me, the biggest contribution is the cross-lingual study for understanding transfer at scale: the significant number of pairwise experiments definitely help identify key factors for cross-lingual transfer, and additionally be an important resource for trying to further understand what factors might actually help influence the degree of the transfer.\n\n3. The paper's answers an important question on how should I scale compute (N,D) to achieve the same loss level while adding new languages. They also provide a good heuristic approximation on when it might be better to fine-tune a pre-trained multilingual language model vs train the lm from scratch. \n\n4. The authors present their results over an impressive number of experiments: 750 independant training runs. Managing experiments at this scale can be and is difficult, and the effort is appreciated."}, "weaknesses": {"value": "While I think the paper make some really good contributions, I do have the following concerns:\n\n1. The scaling law's functional form doesn't explain why it was chosen to be the functional form in the first place, compared to other ways of formulating the interaction. Concretely [1,2], both demonstrate that L(N, D, p) ~ L(N, D)*p exp(\\gamma), which also requires that the term modeling the parameters should also depend on the proportion of language (a similar symmetrization argument was also made in [3]).  Some motivation on why the functional form was chosen would be good to have: both for the general purpose law as well as the target language specific loss (Page 7: L373).\n\n2. For the section on curse of multilinguality, the authors mention that they sample the tokens from each language uniformly: however that is counter to the sampling setup used on most other multilingual experiments (Unimax). For consistancy, it would be good to know if the laws also hold under the unimax distribution setup. Orthogonally, the authors give a recommendation on how to scale compute (N, D) for accommodating new languages. But without actual verification of the scaling law (i.e actually training with the additional compute on the new languages), it is hard to verify the accuracy of the proposed law. Concretely, it would be informative to have a setup to train with languages not used for the fitting, but belonging to either a group with positive transfer, neutral transfer or negative transfer, and then showing how accurate the law is at predicting the performance.\n\n3. The section on pre-train vs fine-tune seems very rushed, and is a bit hard to follow. Additionally, it is quite counter-intuitive that the proposed inflection point is not a function of the compute capacity spent on the pretrained unimax model. Overall, as a meta comment, I think this section requires a bit more of a rigorous setup to compare (eg: varying C for the pre-trained model). In addition to that, there are a number of observations from the cross-lingual setup that are difficult to explain: eg: for the target language of En, why do source languages id and ms have a high BTS score. Having a better understanding of that might be very informative.\n\n\n[1] He, Yifei, et al. \"Scaling laws for multilingual language models.\" arXiv preprint arXiv:2410.12883 (2024).\n\n[2] Akimoto, Kosuke, and Masafumi Oyamada. \"Optimizing Low-Resource Language Model Training: Comprehensive Analysis of Multi-Epoch, Multi-Lingual, and Two-Stage Approaches.\" arXiv preprint arXiv:2410.12325 (2024).\n\n[3] Muennighoff, Niklas, et al. \"Scaling data-constrained language models.\" Advances in Neural Information Processing Systems 36 (2023): 50358-50376."}, "questions": {"value": "1. For leveraging the vocabulary agnostic loss, how did that compare against fitting without the load agonstic setup. Does this design choice have any implication on the results presented in the paper? Likewise, were the baseline MSL fit on the vocabulary agnostic loss or on general log-likelihood? \n\n2. Somewhat related to the previous question: what is your opinion on balancing out the vocabulary at a per language level [1], and leveraging that for loss computation (instead of doing a vocabulary agnostic loss) ?\n\n3. For the MSL evaluations, what was the language grouping that was used for computing the scaling laws for fitting ? Also as a meta point, it would be good to also add details on how the curve fitting was actually carried out.\n\n4. If my understanding of Eqn (3) is correct, the authors make the assumption that the languages decay at the same rate. Intuitively, the decay rate (\\lambda) should be a function of the dataset quality, which varies significantly across languages. So why would the assumption hold true ?\n\n5. For Figure 1, hi, seems like overtraining hurts both the monolingual and the unimax model, however the monolingual model with the multilingual vocabulary seems to be doing okay. Why do the authors think that might be the case?\n\n\n[1] Zheng, Bo, et al. \"Allocating large vocabulary capacity for cross-lingual language model pre-training.\" arXiv preprint arXiv:2109.07306 (2021)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w7Hmgb6E2t", "forum": "0BkvUY61MX", "replyto": "0BkvUY61MX", "signatures": ["ICLR.cc/2026/Conference/Submission23289/Reviewer_Yyfb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23289/Reviewer_Yyfb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762210195486, "cdate": 1762210195486, "tmdate": 1762942591661, "mdate": 1762942591661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the scaling law of multi-lingual models w.r.t. model size, data size and computation budgets."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "• Systematic and comprehensive study of scaling law for multilingual models is an important topic.\n\t• Significant number of experiments are conducted. A few important findings are drawn (the evidence to support the claims requires additional attention though)."}, "weaknesses": {"value": "• Key definitions are missing for a few key concepts and key equations. For example, the symbols in equation (1) are not defined. The grounding of these symbols and equations are not available in this paper. The readers will need to look up the citations with significant efforts of guessing to understand the key idea in an inaccurate manner. \n\t• The formal model of scaling laws are not well defined in this paper.\n\t• The writing and structure of this paper doesn't meet the scientific paper quality."}, "questions": {"value": "1. Line 015, in what sense is this the largest multilingual scaling law study? \n\t2. Line 075, please define N, D, C before they used? I am guessing they are model size, data size, computational buduget? Especially the symbol N is difficult to bet against. \n\t3. Line 093-097, as R^2 is a key concept in this paper, it is worth explaining/re-iterating the intuition behind R^2 on model size, token numbers, computational costs and number of pairs of languages (M --- I am again guessing the meaning of this symbol).\n\t4. Line 104, will the vocab size affect the analysis results? Is 64K vocab size too small for 48 languages especially with glyphic scripted languages.\n\t5. Line 159 - Line 178, please expand the description to include formal definitions, intuitive explanation and other details which can help readers to understand the design of ATLAS. It would be more rigorous if this can be grounded on any statistical models that can draw relationship among the variables in ATLAS and draw relationships on these variables and statistical data collected from model training experiments regarding validity of the proposal.\n\t6. Line 197, how do you choose C=6ND? \n\t7. Line 1208, \"well-validated power-law behavior\", please provide references and evidence in terms of statistical data fitting for experical training tasks. As the LLM training behavior is highly complex, it needs substantial evicence to draw conclusion that simple linear or power-law equations can fit the training behaviors with stats over data size, model size, language number, and computation cost. \n\t8. Line 483-485, any conclusion or summary section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1ArIbUDFMQ", "forum": "0BkvUY61MX", "replyto": "0BkvUY61MX", "signatures": ["ICLR.cc/2026/Conference/Submission23289/Reviewer_taDw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23289/Reviewer_taDw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762765603972, "cdate": 1762765603972, "tmdate": 1762942591382, "mdate": 1762942591382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new multilingual scaling framework that models how performance in multilingual language models scales with model size, data size, and the number of languages during pretraining and finetuning. To understand cross-lingual transfer, the work provides a large-scale empirical study quantifying the pairwise language transfer in a model-based manner. In addition, it models and quantifies the curse of multilinguality, providing scaling rules for maintaining performance as language coverage expands. Finally, the work analyzes when it is more efficient to pretrain from scratch compared with finetuning from a multilingual checkpoint."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The problems the paper wants to tackle are important in the multilingual learning literature. Each section begins with a clear research question, which guides the reader through the narrative logically.\n\n- The work shows significant experimental efforts. Notably, the bilingual transfer table in Figure 2 is a valuable asset to the community of multilingual learning. \n\n- The findings offer actionable insights for multilingual model practitioners, especially the compute-optimal scaling frontier and pretrain-vs-finetune tradeoff. These results help translate empirical observations into practical guidance for scaling multilingual models efficiently."}, "weaknesses": {"value": "- It is unclear which part of the proposed law is a unique contribution of the authors, and which is adapted. For instance, it is known that equation (1) essentially follows the Chinchilla scaling law, but why do equations (2) and (3) take the given specific form? Although the fitting accuracy is high, it would be helpful to provide some justification about why the proposed law organizes those terms in the optimal way. If those constructions are an improvement or combination of previous scaling laws, it would be better to first explicitly write their corresponding forms and name the improvement or generalization of the proposed law mathematically. \n\n- The practical use of the proposed scaling law for predicting model performance or guiding training design remains somewhat implicit. In section 5 and 6, how is the proposed law used? A clearer description of the parameter fitting process would also strengthen the work: for example, what data points are used to estimate the coefficients, what optimization method is applied, and how stable the fitted parameters are across runs. Furthermore, the computational cost of fitting the scaling law is a key practical factor. If estimating these parameters requires comparable or greater resources than training a target model directly, the law’s real-world usefulness would be limited.\n\n- The paper lacks a conclusion section that summarizes the findings or discusses future directions. The narrative ends abruptly after the analysis, making the work feel unfinished."}, "questions": {"value": "- In section 5, the study of the curse of multilinguality, the authors do not seem to explicitly model the effect of inter-language interaction. For instance, if the added languages are highly similar to the existing ones, the curse of multilinguality might not be as evident. On the other hand, if the added language is very different, the effect would be more pronounced. Does the current analysis take this language similarity into account?\n\n- The transfer between pairs of languages is clearly modeled in Figure 2. What about transfer in terms of subsets of languages? The effect of cross-lingual transfer for a subset may not merely manifest as the sum of transfer in each of the constituent languages as multiple languages combined could show redundancy or synergy. How would this be captured by the current scaling law?\n\n- For finetuning discussed in section 6, do the authors actually mean continual pretraining? If so, it might be a better term as finetuning could be confused with SFT in the context of LLM, causing confusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xlZ5IXbVL4", "forum": "0BkvUY61MX", "replyto": "0BkvUY61MX", "signatures": ["ICLR.cc/2026/Conference/Submission23289/Reviewer_gXoZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23289/Reviewer_gXoZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762823984721, "cdate": 1762823984721, "tmdate": 1762942591104, "mdate": 1762942591104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}