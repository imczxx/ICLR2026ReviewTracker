{"id": "vEYRsHoWJ2", "number": 23037, "cdate": 1758338483255, "mdate": 1759896834876, "content": {"title": "Graph-Enhanced EEG-to-Text Decoding: A Spatio-Temporal Relational Embedding Framework for Brain Signal Translation", "abstract": "Despite recent progress in brain–computer interfaces (BCIs), decoding natural language directly from EEG remains a critical challenge. Existing EEG-to-text models primarily treat signals as sequential time series, which severely limits their ability to capture the spatial and temporal relationships among electrodes and limits the possibility of generalization in low-data regimes. To address this challenge, we propose a novel graph-enhanced framework to explicitly model relational information in brain signals. The key idea of our framework is to construct Spectro-Topographic Relational Graphs (STRG) that jointly encode static electrode topology and dynamic inter-channel functional connectivity. From these graphs, we derive Spatio-Temporal Relational Embeddings (STRE), which provide graph-aware representations for downstream sequence-to-sequence decoding. Specifically, (i) STRG captures spatial adjacency and frequency-specific connectivity, (ii) STRE transforms these relational structures into embeddings aligned with text decoding, and (iii) the overall framework integrates these embeddings with a neural decoder to generate natural language outputs. To the best of our knowledge, this is the first graph-enhanced approach for EEG-to-text decoding that explicitly uses graph-based representations of EEG signals. Empirical results show that our framework delivers substantial improvements over strong recurrent and Transformer baselines. In particular, our Graph-Enhanced EEG-to-Text Decoding achieves up to 16% relative gains on BLEU-4, which highlights the effectiveness of relational graph modeling for advancing neural decoding.", "tldr": "We propose the first graph-enhanced EEG-to-text decoding framework that uses spatio-temporal relations among electrodes, which outperforms RNN and Transformer baselines.", "keywords": ["EEG-to-Text Decoding", "Brain–Computer Interfaces", "Graph Neural Networks"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7bd8fd0ea01d53bd5070f674637f9fa14f1e44fb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a graph-enhanced framework to explicitly model relational information in brain signals and decode natural language directly from EEG. Specifically, spectro-topographic relational graphs are constructed, followed by the spatio-temporal relational embeddings for downstream decoding. However, the significance of the work is not clear and the technical details, dataset splitting, and experimental settings are missing. Besides, the technical contributions are limited."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Graph representation learning for a specific domain of EEG: EEG-to-text decoding"}, "weaknesses": {"value": "[1] Graph representation learning – Graph representation learning has been extensively explored in the EEG field. The authors should clarify their unique technical contributions.\\\n[2] Related works – The authors should significantly improve their related works.\\\n[3] Equations – The authors should describe all the notations in their work. For example, the authors should indicate the covariance and standard deviations in Equation (1).\\\n[4] Methodology – The method that the authors presented (Graph – GAT – Transformer – CLIP-like contrastive learning) is unfortunately not novel in the EEG field and is not enough for publication in ICLR.\\\n[5] Baseline – The authors are encouraged to compare their work with models in the field, such as EEG2TEXT\nLiu et al., EEG2TEXT: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and Multi-View Transformer.\\\n[6] Metrics – The authors should indicate which specific BERT model they used for BERTScore. And indicate which ROUGE did they use, e.g., ROUGE-1 or ROUGE-L? More details should be revealed.\\\n[7] Experimental settings – The dataset splitting and experimental settings are missing in the manuscript. The authors should clearly indicate the training, validation, and testing sets. Besides, the specific experimental settings are missing, e.g., subject-dependent cross-session experiments, subject-independent experiments, etc.\\\n[8] Significance – The motivation and significance of the work, as well as the topic, is not convincing."}, "questions": {"value": "[1] How exactly did the authors build the graph with the encoding of the functional connectivity? And how exactly is the “dynamic” functional connectivity built?\\\n[2] What is the significance of the work and the topic?\\\n[3] What is the dataset splitting and experimental setting?\\\n[4] What is the technical contribution of the authors’ work? For example, what is the contribution to the graph representation learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DAHCAO5GzP", "forum": "vEYRsHoWJ2", "replyto": "vEYRsHoWJ2", "signatures": ["ICLR.cc/2026/Conference/Submission23037/Reviewer_E3Wd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23037/Reviewer_E3Wd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761414667581, "cdate": 1761414667581, "tmdate": 1762942488045, "mdate": 1762942488045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores incorporating graph structures into EEG-to-text decoding, aiming to explicitly model spatial and functional relationships among EEG channels. The authors construct an electrode graph based on channel topology and correlation, apply a graph neural encoder to extract spatio-temporal features, and align EEG and text embeddings through contrastive learning for downstream text generation. While the motivation of introducing graph topology into EEG modeling is relevant and well justified, the work appears incomplete in terms of design, experimentation, and presentation."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe motivation is valid and well aligned with the challenges of EEG decoding. Incorporating electrode topology and inter-channel relationships is a meaningful direction that has been underrepresented in EEG-to-text studies.\n2.\tThe idea of using graph representations to capture spatial dependencies could, if properly developed, lead to more neurophysiologically grounded models.\n3.\tThe paper identifies a real limitation of current transformer-based EEG encoders, which generally ignore spatial structure."}, "weaknesses": {"value": "1.\tThe work is clearly incomplete. Figures and tables appear preliminary, and the narrative lacks coherence. Many implementation details are missing, and visualizations are too rough to illustrate the model’s behavior.\n2.\tThe proposed graph modeling is not well justified. The dynamic edge definition is problematic—high correlation between EEG channels may arise from artifacts or shared noise sources rather than genuine functional coupling. Without physiological constraints or validation, the learned graph structure is difficult to interpret.\n3.\tExperimental validation is insufficient. The dataset is small, baselines are limited, and there are no meaningful ablations or cross-subject evaluations. Reported improvements are minimal and may fall within statistical noise."}, "questions": {"value": "1.\tHow are dynamic edges defined in practice, and how do you control for correlations driven by noise or volume conduction rather than functional connectivity?\n2.\tWhat measures did you take to ensure the graph structure reflects physiological plausibility rather than arbitrary channel correlations?\n3.\tDo you plan to extend this work with richer data (e.g., multi-session EEG or MEG) and stronger baselines to validate the approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qR0LmlLoS2", "forum": "vEYRsHoWJ2", "replyto": "vEYRsHoWJ2", "signatures": ["ICLR.cc/2026/Conference/Submission23037/Reviewer_ULZw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23037/Reviewer_ULZw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995829616, "cdate": 1761995829616, "tmdate": 1762942487722, "mdate": 1762942487722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework that explicitly models spatial and spectral relationships in EEG signals to improve natural language generation from brain data. The authors introduce Spectro-Topographic Relational Graphs (STRG), where each node corresponds to an electrode–frequency band pair and edges encode both static electrode topology and dynamic functional connectivity among channels. They then derive Spatio-Temporal Relational Embeddings (STRE) by applying graph neural networks (Graph Attention Networks) on STRGs and feeding the resulting graph-aware features into a temporal Transformer encoder. A Transformer-based decoder finally generates text from these embeddings. The approach is evaluated on the ZuCo dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of Spectro-Topographic Relational Graphs (STRG) is novel and motivation is good. By explicitly modeling electrode adjacency and functional connectivity in each EEG segment, the model captures spatial patterns that sequential models ignore. It's a nice attempt of injecting prior layout knowledge to the encoder. Previous attempts are mostly in sleeping stage prediction."}, "weaknesses": {"value": "- The weakness is also related to graph based encoder, use node to represent node and edges to suggest the layout and spatial relationships are not very novel, at least it has been applied on to other domains of EEG for times, such as sleeping stage prediction and driving drowness prediction. The novelty is somewhat incremental. Meanwhile, considering EEG-to-Text domain has apears a bunch of papers pointing out the alignment and probabaly some training schemas plays more improtant role. \n\n- Experimental results are not strong enough for support the claim. Ablation study is not very convincing, with graph and without graph results are some but not determinastic. Meanwhile, these follow-up works mentioned \"teacher forcing\" setting and the necessity of comparing with random input for two years. The random fluctuation of performance is around that range as well."}, "questions": {"value": "1. When designing the connectivity of the graph, how the edges been defined between nodes, are the edges are desided with human prior knowledge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g4FDyO3Q1o", "forum": "vEYRsHoWJ2", "replyto": "vEYRsHoWJ2", "signatures": ["ICLR.cc/2026/Conference/Submission23037/Reviewer_PdKP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23037/Reviewer_PdKP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023882807, "cdate": 1762023882807, "tmdate": 1762942486830, "mdate": 1762942486830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a graph-enhanced framework for decoding natural language from EEG signals. The authors address a key limitation of existing EEG-to-text models, which treat signals as sequential time series and thereby ignore spatial relationships among electrodes and frequency-specific connectivity patterns. Their approach constructs Spectro-Topographic Relational Graphs (STRG) that jointly encode static electrode topology (based on physical scalp placement) and dynamic functional connectivity (derived from inter-channel correlations). These graphs are then processed through Graph Attention Networks (GATs) to generate Spatio-Temporal Relational Embeddings (STRE), which serve as input to a Transformer-based decoder for text generation. The framework is evaluated on ZuCo datasets and reports improvements of up to 16% in BLEU-4 over baseline methods including BiLSTM, BART, DeWave, and E2T-PTR."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper effectively identifies the limitation of sequential models in capturing spatial relationships, which is a legitimate gap in EEG-to-text decoding.\n2. The approach makes sense since STRG design reflects known spatial and functional EEG properties.\n3. Comparison against four different baseline paradigms (recurrent, Transformer, discretized embedding, contrastive pretraining) provides reasonable coverage."}, "weaknesses": {"value": "1. Only two datasets of reading EEG; unclear if results generalize to spontaneous speech or other subjects.\n2. The paper lacks statistical analysis, e.g. error bars, significance tests, or multi-seed evaluation. Given small sample sizes, results could be statistically insignificant.\n3. No visualization or neuroscientific analysis of learned graphs: interpretability claims remain unsubstantiated.\n4. The hyperparameters $\\alpha$, $\\beta$, $\\lambda_{1}$, and $\\lambda_{2}$ are introduced in Sections 4.2 - 4.4 within Eqs.~(3) and the text following Eq(7), but their specific values or selection procedure are not reported. Moreover, no sensitivity analysis or tuning discussion is provided to assess the impact of these parameters on performance.\n5. Limited reproducibility details. Missing information includes: exact hyperparameters (learning rate, batch size, number of epochs, GAT layers, Transformer layers), data preprocessing pipeline specifics, training procedure (optimizer, scheduling, early stopping criteria). Code availability not mentioned"}, "questions": {"value": "1. How is overfitting controlled given small data? Was early stopping or dropout used?\n2. Please clarify if the contrastive loss uses frozen or jointly trained text embeddings?\n3. How were the hyperparameters $\\alpha$ and $\\beta$ in Equation~(3) chosen? What is their sensitivity to performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R3Wa2IqA9k", "forum": "vEYRsHoWJ2", "replyto": "vEYRsHoWJ2", "signatures": ["ICLR.cc/2026/Conference/Submission23037/Reviewer_T8ih"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23037/Reviewer_T8ih"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762075239744, "cdate": 1762075239744, "tmdate": 1762942486061, "mdate": 1762942486061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}