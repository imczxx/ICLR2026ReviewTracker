{"id": "TOyf8Z4y22", "number": 12930, "cdate": 1758211805302, "mdate": 1763670539081, "content": {"title": "Learning in Prophet Inequalities with Noisy Observations", "abstract": "We study the prophet inequality, a fundamental problem in online decision-making and optimal stopping, in a practical setting where rewards are observed only through noisy realizations and reward distributions are unknown. At each stage, the decision-maker receives a noisy reward whose true value follows a linear model with an unknown latent parameter, and observes a feature vector drawn from a distribution. To address this challenge, we propose algorithms that integrate learning and decision-making via lower-confidence-bound (LCB) thresholding. In the i.i.d. setting, we establish that both an Explore-then-Decide strategy and an $\\varepsilon$-Greedy variant achieve the sharp competitive ratio of $1 - 1/e$. For non-identical distributions, we show that a competitive ratio of $1/2$ can be guaranteed against a relaxed benchmark. Moreover, with window access to past rewards, the optimal ratio of $1/2$ against the optimal benchmark is achieved. Experiments on synthetic datasets confirm our theoretical results and demonstrate the efficiency of our algorithms.", "tldr": "", "keywords": ["Prophet Inequalities", "Learning", "Stopping Time", "Decision-Making"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d167b1eb327c48e876cc890dd1389aa163f7816.pdf", "supplementary_material": "/attachment/0b878fb006a107129c405133093799c5a05ee52e.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies Prophet Inequalities under a novel setting. In such a setting, the rewards at time $i\\in[n]$ are defined as $x_i^\\top \\theta$ where $\\theta$ is an unknown fixed latent vector, while $x_i$ is a stochastic feature vector sampled at each time from a known distribution $\\mathcal{D}_i$ (which can potentially varies over time). At each time, the decision maker observes both $x_i$ and a nosy observation of the rewards and decides whether to accept the reward (and then stop) or continuing the interaction. The objective is to attain a \"good\" competitive ratio w.r.t. the optimum a posteriori over the $n$ time steps. The authors propose two algorithm for the i.i.d. setting ($\\mathcal{D}_i=\\mathcal{D}$) attaining the $1-1/e$ competitive ratio, under a growth condition of the optimum. In the non i.i.d. setting, the author propose two algorithms. The first one attains $1/2$ competitive ratio against an optimum which considers the round from a fixed threshold to the end of the interaction, under a growth condition of the optimum. The second one assumes to have access to past samples (in the decision) and attains $1/2$ competitive ratio against the standard optimum."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, I believe the results attained in this paper are interesting. Assuming that the rewards is linear in a latent vector is reasonable in practice and the authors effectively adapts techniques from contextual bandits to this \"new\" prophet inequality setting. The paper studies both the i.i.d. setting and the non i.i.d. one, thus extending the analysis to non-stationary settings."}, "weaknesses": {"value": "My main concern is about the various assumptions which are made throughout the paper. Specifically,\n\n1. It is not clear to me why the growth assumption should be either mild or reasonable in practice. To me, it seems just an artefact to overcome the theoretical hardness of the problem.\n2. I believe this \"new\" contextual prophet inequality setting is interesting. Nonetheless, I have some concerns on the non i.i.id. setting. Indeed, the distributions can vary over time; still, they are known to the decision maker. Differently, the latent vector is unknown but fixed. Thus, in some sense, the non-stationarity of the problem is known.\n3. Finally, I see the results on the adversarial setting as less meaningful than the i.i.d. ones. To me, the relaxed benchmark in Theorem 5.1. is really weak since it depends on the exploration phase of the algorithm. Moreover, the window access assumption seems again just a way to overcome the weakness of the previous result, since it cancels the inherent difficulty of the prophet inequality problem.\n\nMinor: Line 361-362 I think the $\\max$ should be inside the expectation."}, "questions": {"value": "Could you elaborate on the assumptions?\n\nSince I am not an expert on this literature I will be happy to increase my score if the answers turn out to be pretty positive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xC3aOnfffT", "forum": "TOyf8Z4y22", "replyto": "TOyf8Z4y22", "signatures": ["ICLR.cc/2026/Conference/Submission12930/Reviewer_YYur"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12930/Reviewer_YYur"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760803079368, "cdate": 1760803079368, "tmdate": 1762923695287, "mdate": 1762923695287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the prophet inequality in a novel and practical setting where the underlying reward distributions are unknown, and the decision-maker only observes noisy realizations of the rewards. The authors propose algorithms based on Lower-Confidence-Bound (LCB) thresholding.\n\n1.  For i.i.d. setting, the paper presents two algorithms, Explore-then-Decide (ETD-LCBT) and $\\epsilon$-Greedy-LCBT, that both achieve the sharp $1-1/e$ asymptotic competitive ratio. This matches the optimal bound for the classical known-distribution case and is achieved under a mild non-degeneracy condition on the growth of the optimal prophet's value.\n2.  For non-i.i.d. setting, the paper shows that the optimal $1/2$ competitive ratio can be achieved against a relaxed benchmark that excludes the initial exploration phase.\n3.  By introducing a mild relaxation of window access, i.e., allowing the decision-maker to choose from a small set of past items, the proposed algorithm (ETD-LCBT-WA) achieves the optimal $1/2$ ratio against the standard, non-relaxed prophet benchmark.\n\nEmpirical results on synthetic datasets validate the theoretical findings, showing the algorithms achieve the claimed ratios and are robust to noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.  The paper introduces a compelling new model for prophet inequalities that incorporates unknown distributions, noisy observations, and contextual features. This setting is well-motivated by real-world applications like hiring or ad allocation, where contextual data is available but true reward values are unknown and feedback is noisy.\n2.  In this new, challenging learning setting, the paper achieves the sharp, classical competitive ratios of $1-1/e$ for i.i.d. and $1/2$ for non-i.i.d. with a window. It shows that the linear structure is sufficient to replace full distributional knowledge without sacrificing performance, given a mild non-degeneracy condition.\n3.  The paper is very clear about why its assumptions are necessary. It explicitly provides impossibility results of Proposition 4.1 for noise and Proposition 5.2 for non-i.i.d. learning that motivate the need for its structural assumptions (the linear model) and conditions (the $OPT$ growth condition and the window-access relaxation)."}, "weaknesses": {"value": "1.  The sharp $1-1/e$ bound in the i.i.d. case (Corollary 4.3) hinges on the $OPT = \\omega(1/\\sqrt{f(n)})$ condition. The paper argues this is \"mild\" because $OPT$ is often bounded away from zero. While this is a reasonable assumption, it is the key condition that allows the algorithm to overcome the impossibility from noise shown in Prop 4.1. The paper could be strengthened by discussing this assumption in more detail, perhaps with examples of reward distributions where it might be violated, e.g., if $OPT \\to 0$ quickly.\n2.  To achieve the optimal $1/2$ ratio, the paper relies on either a relaxed benchmark (Theorem 5.1) or a relaxed problem setting (Theorem 5.4, window access). While both are well-justified, it leaves open the question of the standard non-i.i.d. problem against the full benchmark. The paper shows it's hard, but it's not clear if it's proven impossible to achieve $1/2$.\n3.  The experiments are supportive but limited to synthetic data. While this is standard for a theoretical paper, the practical motivation would be bolstered by testing on semi-synthetic data, e.g., using features from a real-world dataset to generate the linear rewards."}, "questions": {"value": "1.  The $1-1/e$ ratio in Corollary 4.3 depends on $OPT = \\omega(1/\\sqrt{f(n)})$. Could the authors elaborate on scenarios where this might not hold? For example, if the rewards $X_i \\sim \\text{Bernoulli}(p_n)$ where $p_n \\to 0$ rapidly, thus $X_i$ are all extremely small, then $OPT$ might shrink too fast. How does this non-degeneracy condition compare to similar assumptions in the learning-to-price or contextual bandit literature?\n2.  The paper shows that for the non-i.i.d. case, a $1/2$ ratio is achieved against a relaxed benchmark or the full benchmark with window access. Proposition 5.2 justifies the difficulty. Is it known or conjectured whether the $1/2$ ratio is impossible for the standard non-i.i.d. problem no window against the full $\\mathbb{E}[\\max_{i \\in [n]} X_i]$ benchmark in this linear learning setting?\n3.  The required exploration length $l_n$ depends on $L$ from Assumption 3.2. Remark 3.3 states $L$ may depend on $n$ and diverge. Remark 4.4 gives an example of $L=\\sqrt{n}$. Could you elaborate on the practical implications of a diverging $L$? If $L=\\sqrt{n}$, $l_n$ must be $\\omega(\\sqrt{n})$, which seems to fit the $l_n=o(n)$ requirement. Is this a common scenario, and does it present any practical challenges for the algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SgQAFbfcEr", "forum": "TOyf8Z4y22", "replyto": "TOyf8Z4y22", "signatures": ["ICLR.cc/2026/Conference/Submission12930/Reviewer_KgWu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12930/Reviewer_KgWu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925582416, "cdate": 1761925582416, "tmdate": 1762923695001, "mdate": 1762923695001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies prophet inequality with linear-bandit-type reward: the $i$th true reward is $X_i = x_i^T \\theta$, where $x_i$ is an observable feature vector distributed according to known distribution $D_{x, i}$, while $\\theta$ is an unknown latent parameter in $R^d$.  The decision maker observes a noisy value $y_i = X_i + \\eta_i$ each round, and needs to irrevocably decide to accept $X_i$ or continue.  The differences with classical prophet inequality are unknown reward distribution (because $\\theta$ is unknown) and noisy observation.  The paper designs algorithms achieving approximation ratios $1 - 1/e$ for the iid setting and $1/2$ for the non-iid setting.  These bounds are claimed to be tight.  The main algorithmic idea is explore-then-decide: use ridge regression to estimate $\\theta$ with samples from the first $l_n$ rounds; then use a classical thresholding algorithm for known-distribution prophet inequality but apply to the lower-confidence bound on the reward, computed from the estimated parameter $\\hat \\theta$.  Simulation results show that this algorithm achieves the desired approximation ratios and is better than heuristic algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "(S1) This paper approaches the problem of prophet inequality with unknown reward distributions (which has been studied by some previous works) from a different angle: a linear reward structure with observable feature and unobservable latent parameter.  This is different from previous works (such as Correa et al, 2019) that assume unstructured unknown distributions.  This linear structure assumption allows the authors to prove very nice positive results: without any additional samples, by just going over the reward sequence once, the algorithm can achieve $1-1/e$ approximation ratio, which was achievable only with at least $\\Omega(n)$ additional samples in the unstructured case.  Such a linear structure also allows noise in the reward observation.  This structural assumption is arguably realistic but has not been widely considered in the literature of learning-based optimal stopping or sequential search problems, so it is a good conceptual contribution to the literature in my opinion. \n\n(S2) Besides the Explore-Then-Decide algorithm, the authors also analyze the performance of $\\epsilon$-Greedy, the classical algorithm that mixes exploration and exploitation.  This makes the picture more complete.  \n\n(S3) The approximation ratios, $1-1/e$ for iid and $1/2$ for non-iid settings, are claimed to be tight.  (While these results seem to be true, I do have a question for the authors regarding formal proofs for these results.)"}, "weaknesses": {"value": "(W1) A small weakness is the lack of formal argument that the $1-1/e$ bound for the iid setting is tight.  The authors claim in Remark 4.4 that $1-1/e$ is tight, because Correa et al (2019) show that one cannot do better than $1 - 1/e$ if the number of samples from the unknown reward distributions is $O(n)$, _if the reward distribution is unstructured_.  However, in this paper, the reward distribution has a linear structure, and ridge regression can estimate the true reward parameter very accurately and efficiently.  So, I am not sure whether the previous $1-1/e$ negative result (for unstructured distributions) can imply the same negative result for the linear structured distributions. @authors: Can you comment on this?"}, "questions": {"value": "## Questions for the authors\n\n(Q1)  See (W1). \n\n(Q2) The iid experiment considers both ETD and $\\epsilon$-Greedy, but the non-iid experiment only has ETD.  Why?  How does $\\epsilon$-Greedy perform in non-iid setting?  \n\n\n\n## Suggestions\n\n* Typo: line 125-126: $X_{\\tau+1}$ should be $X_{n+1}$ ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gO2S3netLB", "forum": "TOyf8Z4y22", "replyto": "TOyf8Z4y22", "signatures": ["ICLR.cc/2026/Conference/Submission12930/Reviewer_g2Ym"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12930/Reviewer_g2Ym"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949007686, "cdate": 1761949007686, "tmdate": 1762923694580, "mdate": 1762923694580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a variant of the bandit problem, which the authors describe as a “prophet inequality.” The agent observes the context $x_i$ and its sampling distribution $D_{x,i}$ at each round $i$, but the only available actions are **go** or **stop**. Once the agent chooses **stop**, the received reward is evaluated based on its ratio against the outcomes of all $n$ rounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "According to the authors’ claim, this paper is the first to study *bandit reward–based prophet inequality* and successfully achieves competitive ratios comparable to related results in the literature. The introduction of the *window access* setting seems conceptually reasonable and somewhat interesting."}, "weaknesses": {"value": "### Weaknesses\n\n1. From a *linear bandit* perspective, the problem seems rather simplistic. Hasn’t the estimation of $\\hat{\\theta}$ been extensively studied for a long time?\n   1.1. The LCB bound appears very similar to what we typically observe in linear bandit results; in fact, it looks almost identical to the classic bound from OFUL.\n   1.2. The setting assumes that the agent knows not only the context distribution (even future distributions $D_{x,i}$) but also that the reward probability $\\alpha$ can be easily computed via the inverse CDF. There is no exploration strategy involved — the agent merely decides whether to **go** or **stop**. If the sampling is sufficiently uniform so that $\\hat{\\theta}$ can be well estimated (or if $\\theta$ were known), the problem seems almost trivial.\n   Particularly, the only dimension-related constant one can rely on under limited exploration is the minimum eigenvalue, and it seems that in Proposition 4.1, the result holds mainly because $\\lambda$ is assumed to be independent of $n$ and $1/\\lambda$ is sufficiently small compared to $n$.\n   Taken together, it seems that the algorithm performs well simply because $n$ is large enough so that the dimension and minimum eigenvalue are no longer critical issues. Therefore, while the algorithm works as expected, it does not appear particularly *novel* or surprising.\n\nOverall, while there is a long line of work suggesting that the topic has intrinsic research value, I personally do not find it very clear what aspect of the problem is *challenging* compared to standard linear bandit analysis.\n\n### Clarity\n\n1. What is $w$? It suddenly appears starting from Theorem 4.2 and seems to play a critical role, yet I could not find a clear definition. Also, the notation involving $l_n = o(n)$ and $l_n = w(\\cdot)$ keeps appearing — could the notation be simplified?\n   For example, in Theorem 4.2, it would be clearer to specify the minimal $n$ (e.g., $n > \\dots$) for which $l_n \\in o(n)$ holds."}, "questions": {"value": "1. **Difficulty compared to linear bandits:**\n   Since the action is only *stop*, the environment handles the exploration, and the estimation method seems nearly identical, in what sense is this problem more challenging than linear bandits?\n   1.1. Why is the ratio necessarily $1 - 1/e$ in the i.i.d. setting?\n   1.2. In the non-i.i.d. setting, why can’t the ratio exceed $1/2$ even with access to offline data? A mathematical explanation would be appreciated.\n\n2. Regarding $\\lambda$ (and $\\lambda'$ in Section 5), what assumptions are made in relation to $n$?\n\n3. Proposition 5.2 claims that a minimal amount of exploration is necessary. However, as $l_n$ grows, the upper bound of the ratio is expected to increase. Is $d$ the best possible scaling? Is there any lower bound result showing that no improvement beyond the authors’ $l_n$ choice is possible?\n   3.1. It is also confusing that the authors effectively use $l_n$ offline data before starting the stopping process, yet in the offline setting the optimal ratio is $1/2$, while the proposed method achieves a higher value. How can this be reconciled?\n\n4. Why must the rewards be non-negative? Is there any trick or condition in the proof that specifically requires “above-zero” rewards?\n\n5. In the *Contributions* section, the authors claim that their setting is “the first scenario that only observes noisy rewards together with feature information and reward distributions are unknown.”\n   5.1. However, according to line 123, the agent does observe $x_i$ in every round, and even $D_{x,i}$ is known. Doesn’t this statement overstate the novelty of the contribution?\n   5.2. Then what exactly did previous works observe in addition to noisy rewards, and how does this work differ in that regard?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9BWIifiVzt", "forum": "TOyf8Z4y22", "replyto": "TOyf8Z4y22", "signatures": ["ICLR.cc/2026/Conference/Submission12930/Reviewer_Y9sN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12930/Reviewer_Y9sN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964626826, "cdate": 1761964626826, "tmdate": 1762923693664, "mdate": 1762923693664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}