{"id": "cFzPNWC9Qb", "number": 5202, "cdate": 1757865003580, "mdate": 1763754200256, "content": {"title": "Stackelberg Coupling of Online Representation  Learning and Reinforcement Learning", "abstract": "Deep Q-learning jointly learns representations and values within monolithic networks, promising beneficial co-adaptation between features and value estimates. Although this architecture has attained substantial success, the coupling between representation and value learning creates instability as representations must constantly adapt to non-stationary value targets, while value estimates depend on these shifting representations. This is compounded by high variance in bootstrapped targets, which causes bias in value estimation in off-policy methods. We introduce Stackelberg Coupled Representation and Reinforcement Learning (SCORER), a framework for value-based RL that views representation and Q-learning as two strategic agents in a hierarchical game. SCORER models the Q-function as the leader, which commits to its strategy by updating less frequently, while the perception network (encoder) acts as the follower, adapting more frequently to learn representations that minimize Bellman error variance given the leader's committed strategy. Through this division of labor, the Q-function minimizes MSBE while perception minimizes its variance, thereby reducing  bias accordingly, with asymmetric updates allowing stable co-adaptation, unlike simultaneous parameter updates in monolithic solutions. Our proposed SCORER framework leads to a bi-level optimization problem whose solution is approximated by a two-timescale algorithm that creates an asymmetric learning dynamic between the two players. Extensive experiments on DQN and its variants demonstrate that gains stem from algorithmic insight rather than model complexity.", "tldr": "", "keywords": ["Reinforcement Learning", "Representation Learning", "Q-learning", "Stackelberg Equilibrium", "Two-timescale"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51a175598c6dfe74a2913dcd6744d850d5bf8d25.pdf", "supplementary_material": "/attachment/32fdb8e76e7459236ef202938d99ad816f41b611.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Stackelberg Coupled Representation and Reinforcement Learning, a novel value-based RL framework inspired by game theory. The key insight is to decouple representation and value learning into a hierarchical leader-follower game: the Q-function (control network) is the leader updated on a slow timescale, while the perception network (state encoder) is the follower learning rapidly to minimize Bellman error variance under the leader’s strategy. This two-timescale design aims to solve the instability issue (deadly triad) seen in monolithic off-policy deep Q-learning, improving sample efficiency and performance without increased model complexity. Experiments on MinAtar, Atari, and MiniGrid show consistent, substantial improvements over standard DQN and its variants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated by the Stackelberg game theory to reformulate the RL optimization problem, separating learning dynamics between representation and value estimation.\n\n2. The role assignments (control as leader, perception as follower) and the choice of Bellman error variance for the follower are both theoretically justified and empirically validated."}, "weaknesses": {"value": "1. While computational cost is minimal, the framework introduces new algorithmic complexity (two-level optimization, choice of timescale schedules) that requires parameter tuning.\n\n2. The method is centered entirely on discrete value-based methods. Extension to actor-critic or continuous-control algorithms is absent."}, "questions": {"value": "1. Do you consider DQN and Dueling DQN as good baselines and the results naturally generalize to strong baselines such as Rainbow? \n\n2. Can you add studies or discussions on actor-critic methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qao58YFbtm", "forum": "cFzPNWC9Qb", "replyto": "cFzPNWC9Qb", "signatures": ["ICLR.cc/2026/Conference/Submission5202/Reviewer_b2sk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5202/Reviewer_b2sk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760533387779, "cdate": 1760533387779, "tmdate": 1762917942967, "mdate": 1762917942967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new framing of value-based deep RL methods in terms of a two-player game between a representation player and a controller player. The motivation stems from the apparent instability that emerges in RL when a learner tries to simultaneously learn its representation, and its control policy. The core technical idea is to treat the learner's estimate of the Q-function as the leader, while a perception network is treated as the follower of a Stackelberg game. This framing yields \"Stackelberg Coupled Representation and Reinforcement Learning\" (SCORER), a simple value-based method for deep RL based on the principles of Stackelberg games. The game is solved using a two-level form of gradient descent, which yields a stable relationship between the representation and the value. Critically, this game only involves the exchange of the standard signals of the RL problem (based around MSBE) and does not allow incorporation of extraneous signals (such as intrinsic motivation, entropy bonuses, and so on). This methodological choice allows the work to focus purely on the game dynamics in a simple setting. The paper's primary evidence to support its central claims comes from a broad experimental study contrasting the performance of SCORER with standard value-based methods (DQN, DDQN, and so on) across a variety of standard tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This is a very clean paper. The scope is well defined, the question is well-posed, all of the needed ingredients are self-contained within the paper, and the solution (two-level optimization of a Stackelberg game) is tidy. In other words, the paper does exactly what it sets out to do in the intro.\n- The experimental study is well designed, with appropriately chosen baselines, a variety of environments, and suitable ablations to explore various design choices (such as which player should be the leader vs. follower in Fig 4).\n- The writing is clear and does a good job of motivating the various design choices. I especially appreciated the discussion around whether extraneous other signals should be incorporated into the study such as standard self-supervised objectives. I found the question, the main method, and the findings to be well-motivated and communicated effectively.\n- The convergence result in the appendix adds a firm theoretical support to the approach.\n- Lastly, the idea at the heart of the paper is relatively simple: if we decouple algorithmic components, we can treat them as players in a game, and solve the game."}, "weaknesses": {"value": "- The general premise of mitigating challenges of deep RL through decomposing the monolithic policy network or value network into different components has been studied extensively. The proposal to treat a two-decomposition sub-system as engaged in a Stackelberg game is, to my knowledge, new, though I do believe a deeper understanding of why this specific decomposition is valuable would strengthen the work. As two concrete examples:\n\n1. In Anand and Precup (2023), they propose to decompose the estimate of the value function into a \"permanent\" and \"transient\" component. These two components yields two separate networks, one for learning the permanent and one for the transient. They key difference between the two is that they are updated at different schedules. Then, the overall value function is determined by taking a combination of the two. I believe there is a heavy conceptual overlap in this approach and SCORER, and I wonder: if someone were to want to deploy a value-based method for RL, when should they use SCORER as approach to the approach of Anand and Precup?\n\n2. Second, the Actor-Critic architecture is naturally decomposed into separate pieces to enhance stability. Indeed, a recent paper by Garcin et al (2025) has explicitly looked at the learned representations in each of the Actor and Critic, and found that the two networks tend to specialize their representation differently. First, I believe applying this kind of analysis to SCORER could be beneficial to shed further light on what specifically is being learned by the representation and policy in each of the ablations (and in the general setup). Second, it is so far again unclear what the precise benefit of this proposed decomposition is compared to something like Actor-Critic. I see this is noted in the discussion (applying SCORER to AC algorithms), though I believe this is an important point to understand about the space.\n\n- Otherwise, no obvious weaknesses stand out. This is a clean paper with a tight focus that delivers on what it sets out to do.\n\n\nSome typos and writing suggestions:\n\n- Some repeated text at line 166 or so: \"The defining characteristic is the leader’s ability to anticipate and influence the follower’s decision by committing to a strategy first. The defining characteristic is the leader’s ability to anticipate the follower’s best response and commit to a strategy that steers the game toward a favorable equilibrium.\".\n\n- Convergence analysis in Appendix J is really nice. If you can find space, I would love to see this in the main text, even it is just a colloquial theorem statement with the assumptions and proof in the appendix. On that note, I would suggest stating the convergence result as a mathematical theorem or proposition, even if it is in the appendix, to give you a chance to succinctly assert the result with all needed quantifiers and variables in one place.\n\nReferences:\n- Anand, N., & Precup, D. Prediction and control in continual reinforcement learning. NeurIPS 2023.\n- Garcin, S., McInroe, T., Castro, P. S., Panangaden, P., Lucas, C. G., Abel, D., & Albrecht, S. V. Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning. ICLR 2025."}, "questions": {"value": "My main question relating to my evaluation of the work is as follows:\n\nMQ.1: For the purposes of understanding, how do you see SCORER in relation to other approaches to deep RL that decompose the monolith into a variety of subcomponents? Can you comment on the merits of framing this as perceiver/actor in a game, rather than something else (such as the Anand and Precup approach)?\n\nSeparately, I have a one other question (OQ.X), though it is less critical to my overall appraisal:\n\nOQ.1: How do you foresee SCORER extending to another similar algorithmic templates, such as Actor-Critic, or even cases when value is not learned at all but perhaps just a policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TkjJn0MAzH", "forum": "cFzPNWC9Qb", "replyto": "cFzPNWC9Qb", "signatures": ["ICLR.cc/2026/Conference/Submission5202/Reviewer_HaiV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5202/Reviewer_HaiV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589970322, "cdate": 1761589970322, "tmdate": 1762917942725, "mdate": 1762917942725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SCORER - a method that splits deep Q-learning into two separate networks that learn at different speeds to address training instability. The representation network updates faster and minimizes Bellman error variance, while the Q-function updates slower for stability. They test on MinAtar and MiniGrid environments where some tasks show good improvements while others show more modest gains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a real issue in deep Q-learning where representation learning and value learning are tightly coupled, causing training instability.\n- The learning curves show SCORER produces smoother, more stable training compared to baselines. The curves don't have as many sudden drops or erratic behavior.\n- The appendix provides mathematical analysis based on two-timescale stochastic approximation. The convergence guarantees and game-theoretic formulation seem reasonable.\n- Results are strong in some cases. MinAtar Breakout shows roughly 3x improvement over baseline DQN. MiniGrid Four Rooms is particularly impressive - baseline R2D2 completely fails while SCORER achieves 97% success"}, "weaknesses": {"value": "- Experiments only cover 4 simple MinAtar environments and basic gridworld tasks. No full-scale Atari with high-resolution images. This is a problem because representation learning challenges are fundamentally different at scale, and we can't tell if benefits hold with deep convolutional networks.\n- Outside Breakout and Four Rooms, gains are small. Asterix is basically the same as baseline, Freeway has identical final performance (just faster), SpaceInvaders shows about 10-15% improvement. The claim of \"consistently improves final performance\" oversells what the data shows.\n- The method uses 5:1 ratio (follower:leader learning rates) but there's no ablation testing 2:1, 10:1, or other values. This is important - we need to know how sensitive the method is and whether 5:1 is actually optimal.\n- A monolithic network with higher learning rate for encoder layers (without network separation or stop-gradients) would test whether the complex game structure is necessary or if just updating representations faster works. This might not be essential since current ablations show hierarchy matters, but it would be useful to rule out simpler explanations."}, "questions": {"value": "1. Why were full-scale Atari experiments not included, and do you have any preliminary results on high-dimensional visual domains?\n2. Can you provide an ablation study on different follower-to-leader learning rate ratios (e.g., 2:1, 10:1, 20:1) beyond the 5:1 used in all experiments?\n3. Have you tested a simpler baseline where a standard monolithic network uses higher learning rates for encoder layers without network separation or stop-gradients?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UpsVt0GsyJ", "forum": "cFzPNWC9Qb", "replyto": "cFzPNWC9Qb", "signatures": ["ICLR.cc/2026/Conference/Submission5202/Reviewer_pMYo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5202/Reviewer_pMYo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717711425, "cdate": 1761717711425, "tmdate": 1762917942495, "mdate": 1762917942495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces a new reinforcement learning method for online representation learning. The core idea is to divide the function approximator into two parts: one for learning a useful representation and another for learning to predict future returns. Each part is treated as an agent playing a Stackelberg game, maximizing a different objective while sharing the same Q-function predictor. This leads to a bi-level optimization problem, which they address by enforcing a faster learning rate for the encoder than for the predictor. Experiments on various tasks are presented to report the method's performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A. The submission focuses on an important problem of reinforcement learning, which is representation learning.\n\nB. The reading flow is good, and the method is clearly presented.\n\nC. The method is simple to implement as it simply uses two different learning rates for the encoder and the predictor, and updates the encoder and the predictor sequentially."}, "weaknesses": {"value": "I. Some choices are suboptimal or unjustified.\n\n   a. The choice of minimizing the variance of the temporal difference error is problematic. Indeed, the proposed method faces the double-sampling issue, yet it is not mentioned. This means the follower's objective function is biased. More specifically, $\\mathbb{E}[\\delta_j^2] \\neq \\mathbb{E}[\\delta_j]^2$. Therefore, the presented algorithm will suffer from this issue in stochastic MDPs, where the variance of the temporal difference error exists, even for the optimal Q-function. Importantly, all experiments are made in deterministic environments. I suggest setting the follower's objective function to the temporal-difference error, which is an unbiased estimate. \n\n   b. It is claimed that \"This promotes stability in the learning process, a trait for mitigating the risk of divergence that was first formally demonstrated in foundational work by Baird (1995)\" in Line 218. However, this claim is never justified by an empirical study on the Baird counter-example. I suggest evaluating the method on this counter-example.\n\n   c. The choice of the follower and the leader is only justified by an empirical comparison. Adding a theoretical or an intuitive argument would strengthen the motivation behind this work.\n\nII. The empirical analysis remains weak and can be improved.\n\n   a. The method relies on multiple parallel environments. While this allows for faster training, it is an unrealistic assumption for real-world applications, and when a simulator is available, on-policy methods will be privileged. To fully assess the potential of the presented method, it would be useful to also have experiments in which only one environment is used at a time.\n\n   b. As mentioned in I. a., it is unlikely that the presented method performs well in stochastic environments. Therefore, I suggest evaluating SCORER in stochastic environments, for example, with sticky actions.\n\n   c. Two cofounding factors can be responsible for the performance increase. (i) The learning rate schedules are not fully disclosed. It would be more convincing to compare the proposed approach with the baselines using the same learning rate schedules (the one of the encoder and the one of the predictor). (ii) The gradients are clipped for the proposed approach but not for the baseline. Ablating this choice would help understand the benefit of SCORER.\n\n   d. An analysis of the learning dynamics would provide more insights. For example, reporting the parameter norm and the srank would help better understand the method.\n\n   e. While the interquantile-mean is reported as suggested by [1], the recommended way of reporting the confidence intervals is not followed, and no justification is provided.\n\n   f. In Table 1, the maximum IQM performance is reported. This choice is uncommon and not motivated. \n\n[1] Agarwal, Rishabh, et al. \"Deep reinforcement learning at the edge of the statistical precipice.\" NeurIPS 2021.\n\n### Remarks\n\n1. The font size of every figure can be larger. Replacing the tables with visual plots would facilitate the analysis of the results.\n \n2. Two citations can be added when speaking about the fact that learning appropriate representations helps with sample efficiency: \n\n   [1] Dabney, Will, et al. \"The value-improvement path: Towards better representations for reinforcement learning.\" AAAI 2021.\n\n   [2] Vincent, Théo, et al. \"Iterated Q-Network: Beyond One-Step Bellman Updates in Deep Reinforcement Learning.\" TMLR 2025.\n\n3. The batch seems to be missing in equation 5.\n\n4. In Figure 4, the left and middle Figures are swapped."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2WZX5o6LtM", "forum": "cFzPNWC9Qb", "replyto": "cFzPNWC9Qb", "signatures": ["ICLR.cc/2026/Conference/Submission5202/Reviewer_7cFX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5202/Reviewer_7cFX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745438370, "cdate": 1761745438370, "tmdate": 1762917942231, "mdate": 1762917942231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}