{"id": "ovJBVWMEwi", "number": 24409, "cdate": 1758356552480, "mdate": 1759896767800, "content": {"title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning", "abstract": "The ability to process information from multiple modalities and to reason through it step-by-step remains a critical challenge in advancing artificial intelligence. However, existing reasoning benchmarks focus on text-only reasoning, or employ multimodal questions that can be answered by directly retrieving information from a non-text modality. Thus, complex reasoning remains poorly understood in multimodal domains. Here, we present MARBLE, a challenging multimodal reasoning benchmark that is designed to scrutinize multimodal language models (MLLMs) in their ability to carefully reason step-by-step through complex multimodal problems and environments. MARBLE is composed of two highly challenging tasks, M-Portal, M-Cube and M-Maze, that require the crafting and understanding of multistep plans leveraging spatial, visual, and physical constraints, which must be executed correctly and in the proper order. We find that current MLLMs perform poorly on MARBLE - all 12 advanced models obtain around 0\\% accuracy performance on M-Cube and M-Maze, while only Grok-4 and GPT-5 slightly outperformed the random baseline on M-Portal. These results indicate that complex reasoning is still a challenge for existing MLLMs. Moreover, we show that perception remains a critical bottleneck to mulitmodal reasoning. By shedding light on the limitations of MLLMs, we hope that \\method will spur the development of the next generation of models with the ability to reason and plan across many multimodal reasoning steps.", "tldr": "", "keywords": ["Multimodal Language Model", "Vision Language Model", "Reasoning", "Benchmarks"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/65e985119dfcdccef00c8c3c16a95c5f35a5bb44.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- This paper present MARBLE, a challenging multimodal reasoning benchmark for MLLMs on multi-step planning and spatial reasoning. \n- The motivation for MARBLE is to fill the gap between previous benchmarks for lacking evaluation of step-by-step reasoning in planning and spatial reasoning domain. \n- MARBLE consistes of three main tasks, M-Portal, M-Cube, M-Maze, in total of 5,024 samples.\n- The evaluation results of MARBLE on 12 leading MLLMs show that most model achieve nearly random or 0% performance on MARBLE except for Grok-4 and GPT-5.\n- The authors claim that according to failure analysis, perception abilities are still major bottlenecks in multimodal reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation of the paper is clear, the paper aims to fill the blank of exisiting multimodal language model benchmarks for lacking the gap of structured reasoning steps\n- The paper present detailed descriptions about how to curate the benchmark category by category and what's the meaning of each task\n- The data curation pipeline emphysize on the reasoning steps instead of the final outcomes of other benchmarks\n- The benchmark contains interaction mechanism with the game applications"}, "weaknesses": {"value": "- Most models achieve very limited scores on MARBLE, it is hard to distinguish which model perform better than others. Especially, In Table2, M-Cube dataset, CUBE category, all the models achieve 0 on this. It is confusing to find the meaning of this benchmark if most of the models will fail and have extremely low scores.\n\n- The paper significantly lacks insight about why MLLMs fail on MARBLE but focus too much on the data curation process, making the paper looks a little bit like a technical report instead of a research article.\n\n- The application domain of this paper is poor, the paper only focus on planning and spatial reasoning domain, which is not enough for complex multimodal reasoning. In the meantime, the data source is very limited in single setting, e.g. cube task, in fact, many tasks and data sources can be considered for the spatial reasoning. The distribution of the benchmark is very limited.\n\n- Although the author focus on the data curation pipeline, some details still are not clear because of lacking of necessary visualizations, e.g. Figure1, What exactly are these steps?"}, "questions": {"value": "In order to get the authors better prepared for the rebuttal, I propose the following questions:\n\n- L77 missing Table reference, which table are you indicating?\n- When designing M-PORTAL and M-CUBE, how did you ensure that the intermediate reasoning traces generated by the model could be accurately evaluated or supervised? Did you consider introducing a human-in-the-loop validation process?\n- How do you control for the difficulty of the task itself so it doesn't become the main reason for model failure? For example, when a model fails, is it because its reasoning ability is weak, or because the task setup is too difficult for the model?\n- Do the incorrect steps still contribute partially to achieving the task goal, or do they completely lead to failure?\n- In the discussion, you mentioned that MARBLE evaluates both the final answer and the reasoning trace. Has any hierarchical evaluation been performed on the model-generated chain-of-thought outputs? \n- How's the human performance on your Benchmark?\n- Does Chain-of-Thought Prompting improve the overall performance?\n- What is the performance improvement of common test-time-scaling methods?\n- There are also many existing works on chain-of-thought evaluation, what is your major differences between them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CT11r4M1Ei", "forum": "ovJBVWMEwi", "replyto": "ovJBVWMEwi", "signatures": ["ICLR.cc/2026/Conference/Submission24409/Reviewer_iioQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24409/Reviewer_iioQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761606120109, "cdate": 1761606120109, "tmdate": 1762943073999, "mdate": 1762943073999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a suite of tasks designed to evaluate the “think-with-image” capability of multimodal large language models (MLLMs). Specifically, the MARBLE benchmark includes three tasks, i.e., M-Portal, M-Cube, and M-Maze. Each targeting different aspects of multimodal reasoning and spatial planning. Most evaluations are conducted offline, comparing the performance of various MLLMs and providing insightful analyses of their reasoning behaviors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This benchmark is distinctive in its design, incorporating video game environments to construct datasets that evaluate the multimodal reasoning abilities of MLLMs. It features long-horizon tasks with large search spaces, providing a challenging testbed.\n\nThe benchmark provides a comprehensive evaluation of various MLLMs, revealing that visual perception and planning remain critical bottlenecks in multimodal reasoning."}, "weaknesses": {"value": "The main focus of this work is on providing a benchmark to evaluate the capabilities of existing MLLMs. It feels largely engineering-oriented, emphasizing data curation rather than introducing new methods to enhance MLLM performance, which limits the conceptual contribution of the paper.\n\nWhile the overall writing quality is good, it could be made more concise.\n\nThe evaluation setup appears somewhat specialized, focusing on a narrow subset of tasks based on simulated images, which may limit the generalizability of the findings."}, "questions": {"value": "Could you provide the image resolution used for each task? For the M-Maze task in particular, if the resolution is too low, tokenized images may lose important fine-grained details that affect spatial reasoning.\n\nSince most tasks are challenging even for humans without prior experience, the major performance gap appears mainly in the maze tasks, while for others, the gap between the best-performing VLMs and humans is relatively small. Although some MLLMs achieve a 0% success rate, how to confidently conclude that perception is the critical bottleneck in multimodal reasoning?\n\nIt seems that the difficulty of these tasks also stems from the challenge of remembering intermediate states. The online evaluation results appear more reliable and often better, but only one case was tested this way. Would it be possible to evaluate more models online, or at least explore alternative approaches to approximate online performance?\n\nAs these tasks rely on simulated images and specific environments, I am curious about the practical potential or applicability of such benchmarks for assessing real-world MLLM capabilities?\n\nCould you describe the evaluation cost and feasibility of this benchmark for different models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xjz9awgRZc", "forum": "ovJBVWMEwi", "replyto": "ovJBVWMEwi", "signatures": ["ICLR.cc/2026/Conference/Submission24409/Reviewer_14V2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24409/Reviewer_14V2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951179152, "cdate": 1761951179152, "tmdate": 1762943073780, "mdate": 1762943073780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MARBLE, a multimodal reasoning benchmark aimed at diagnosing step-by-step spatial reasoning and planning in MLLMs across three tasks: M-PORTAL (Portal-style multi-step planning with visual context), M-CUBE (assembling a 3D cube from six interlocking pieces), and M-MAZE (dynamic 2D labyrinth planning that alternates tile insertion and navigation). Each task has two difficulty settings and emphasizes process-oriented evaluation (e.g., plan correctness, fill-the-blanks, success rate) rather than only final answers. In large-scale evaluations of 12 models, state-of-the-art systems largely fail on the hard settings (≈0% accuracy for M-CUBE and M-MAZE) and only slightly beat random baselines on some M-PORTAL subtasks (with Grok-4 and GPT-5 the best among tested systems), highlighting perception bottlenecks and error accumulation in long-horizon planning. The authors also report an online interactive evaluation loop for M-MAZE that reduces but does not eliminate compounding errors with intermediate feedback."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The three tasks probe different mixtures of perception, spatial reasoning, combinatorics, and rule-driven planning. The two-tier difficulty in each task (e.g., CUBE vs CUBE-easy, MAZE vs MAZE-easy) cleanly exposes where models fail (perception vs search vs dynamics).\n3. The paper isolates perception with a conversion task (image to edge arrays), showing around 70–76% per-cell accuracy and 0% piece-level accuracy across models, which plausibly explains downstream failures even before reasoning over large search spaces.\n4. The action-by-action loop for M-MAZE is practical and closer to agentic usage. It mitigates long-horizon error accumulation, and is helpful for future iterative agents."}, "weaknesses": {"value": "1. Plan-correctness relies on mixing up to five independent mistake steps to produce 2^5 candidates with 1 positive, which is an extreme imbalance that can confound minority-class F1 and encourage shortcut cues in negatives.\n2. Human results are reported from 2–3 experienced players; this small N, without variance/confidence intervals, makes it hard to contextualize model gaps especially on hard tasks.\n4. While the image to array conversion task is informative, it’s still a single proxy. Consider adding: (a) controlled render sweeps (viewpoints, occlusion, lighting) with sensitivity curves; (b) synthetic text-only surrogates (perfect symbolic inputs) for all tasks to quantify pure reasoning headroom per model; (c) vision-only ablations (frozen LLM head) to profile encoder failure modes; and (d) explicit evaluation of template overfitting in 2D-array formats.\n3. Fill-the-blanks and binary plan correctness are valuable, but they do not capture how the plan errs (e.g., spatial consistency violations vs rule misuse vs temporal dependency errors). Consider structured error taxonomies and edit distance between predicted and gold plans (segment-level precision/recall), plus consistency checks (state-update invariants across steps)."}, "questions": {"value": "1. How do you ensure that mistaken steps are independent and don’t introduce superficial artifacts (e.g., phrasing patterns) detectable without reasoning?\n\n2. Do you randomize style/syntax across correct vs incorrect steps to avoid annotation style leakage?\n3. For each task, can you release paired symbolic inputs (perfect parses) so that one can swap in oracle perception and isolate reasoning gaps by model? How do results change under such oracle perception conditions for all models, not just a subset?\n4. What were the token/time budgets per example? Did you enable tool use (e.g., the M-CUBE validator) uniformly across models? If not, could you standardize an evaluation track with validator feedback loops and report Success@k tool calls?\n4. For M-PORTAL, how do models behave under viewpoint perturbations, screenshot subsets, or linguistic paraphrases of map instructions? For M-CUBE, can you provide hardness-graded instances (e.g., controlled number of flips, symmetries), and for M-MAZE, depth-controlled splits with matched visual complexity?\n5. Will it be possible to add plan-level structured metrics (e.g., step-type confusion, state-consistency violations, illegal-move rate) and per-error attribution dashboards?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YCrE1SNaAt", "forum": "ovJBVWMEwi", "replyto": "ovJBVWMEwi", "signatures": ["ICLR.cc/2026/Conference/Submission24409/Reviewer_3WW3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24409/Reviewer_3WW3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973169142, "cdate": 1761973169142, "tmdate": 1762943073604, "mdate": 1762943073604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MARBLE, a benchmark designed to evaluate the ability of multimodal large language models (MLLMs) to perform complex, step-by-step reasoning across modalities. Existing multimodal benchmarks often focus on direct information retrieval from visual inputs or purely text-based reasoning. In contrast, MARBLE targets structured, multi-step planning that integrates spatial, visual, and physical reasoning. The benchmark consists of three tasks—M-Portal, M-Cube, and M-Maze—requiring models to generate and execute multi-step plans in visually grounded environments. Results show that current advanced MLLMs perform poorly on MARBLE, suggesting that complex multimodal reasoning remains an open challenge. The authors further analyze perception bottlenecks and argue that stronger perceptual understanding is essential for progress."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear and well-structured writing.\nThe manuscript is clearly written and easy to follow. The benchmark design and evaluation methodology are all presented in a logical, concise manner, supported by informative figures and examples.\n\nThoughtful dataset and task construction.\nThe benchmark curation process is technically sound. The detailed pipeline for constructing environments and task sequences provides valuable insight for future multimodal benchmark development.\n\nComprehensive evaluation and analysis.\nThe experiments include a broad range of state-of-the-art MLLMs, paired with qualitative analyses of error modes. This thorough evaluation allows the community to clearly understand current limitations and bottlenecks, particularly in perception-driven reasoning."}, "weaknesses": {"value": "Unclear motivation behind task combination.\nWhile each task individually provides meaningful evaluation, the rationale for combining M-Portal, M-Cube, and M-Maze into a single benchmark is not fully articulated. These settings are fairly distinct in format and objective, and similar concepts have appeared in prior embodied and spatial reasoning benchmarks. The paper would benefit from a stronger justification for why combining these tasks yields emergent value beyond scaling and aggregation.\n\nAdditional guidance for future research could enhance impact.\nAlthough the paper identifies perception as a bottleneck, it would be helpful to provide more actionable suggestions or directions—e.g., benchmark variants isolating perception versus planning, curriculum strategies, or evaluation under improved perception modules.\n\nIn general, introducing yet another challenging benchmark should be justified by strong motivation and a clearly demonstrated gap in existing resources. Without compelling reasons or urgent practical need, adding a new benchmark risks contributing to benchmark inflation rather than advancing the field."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "csRHcZGXQg", "forum": "ovJBVWMEwi", "replyto": "ovJBVWMEwi", "signatures": ["ICLR.cc/2026/Conference/Submission24409/Reviewer_GE4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24409/Reviewer_GE4m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997879449, "cdate": 1761997879449, "tmdate": 1762943073410, "mdate": 1762943073410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}