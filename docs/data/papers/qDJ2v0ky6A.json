{"id": "qDJ2v0ky6A", "number": 14371, "cdate": 1758234042103, "mdate": 1759897374517, "content": {"title": "AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans", "abstract": "Fine-tuning large language models for code editing has typically relied on mining  commits and pull requests. The working hypothesis has been that commit messages describe human intent in natural language, and patches to code describe the changes that implement that intent. However, much of the previously collected data is noisy:  commit messages are terse, human-written commits commingle several unrelated edits, and many historical commits come from simple, rule-based bots.\n\nThe recent adoption of software engineering agents changes this landscape. Code changes co-authored by humans and agents tend to be more narrowly scoped and focused on clearer goals. Their commit messages, generated by LLMs, articulate intent and rationale in much greater detail. Moreover, when these changes land in public repositories, the are implicitly filtered by humans: maintainers discard low-quality commits to their projects.\n\nWe present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code, OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August 2025. We describe the identification and curation pipeline, quantify adoption trends of these agents, and analyze the structural properties of the edits.  Finally, we show that models fine-tuned on AgentPack outperform models trained on prior human-only commit corpora, highlighting the potential of using public data from software engineering agents to train future code-editing models.", "tldr": "", "keywords": ["Code Editing", "Code Commits", "Software Engneering Agents", "Fine-Tuning", "AI for Code"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/81691a38005a37a920ca1260d3928fa0b6266a19.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AGENTPACK, a large-scale dataset of 1.3M code edits co-authored by AI agents and humans. The core hypothesis is that this data, curated from public repositories and implicitly filtered by human maintainers, is superior to traditional commit-mined data. The paper includes a comprehensive analysis of the dataset's properties and an evaluation of DeepSeekCoder (1.3B, 6.7B) models, finding that fine-tuning on AGENTPACK yields generally positive results compared to the prior EditCoder dataset."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: The central idea of mining agent-human collaborative edits is novel and highly relevant. As agent use becomes ubiquitous, this data source will only grow.\n\n- Quality & Scale: The resulting 59GB dataset is an order of magnitude larger and more complex than prior work (e.g., EditCoder). It also captures a wide variety of languages and task types, as shown in the analysis.\n\n- Clarity: The paper is well-written. The data collection, filtering, and processing pipeline are all clearly articulated."}, "weaknesses": {"value": "- Marginal Performance Gains: The performance gains over EditCoder are not uniformly strong. While significant on the 1.3B model, the improvement is minimal on the more capable 6.7B model (+6.7% on HumanEvalFix) and even shows a minor regression (-2.4% on CanItEdit). This suggests the dataset's primary value might be in bootstrapping weak models, while stronger models may not benefit as much.\n\n- Outdated Model Evaluation: For a paper targeting a 2026 conference, the choice of base models is a significant weakness. The experiments use DeepSeekCoder V1 (released Nov 2023), while DeepSeekCoder V2 (June 2024) and DeepSeek V3 (Dec 2024) have been available for over a year. This makes it difficult to assess the dataset's relevance. The key open question is whether AGENTPACK provides a meaningful signal to SOTA models or if its benefits are limited to older, less capable models.\n\n- Mismatch in Evaluation Complexity: The paper's evaluation is limited to relatively simple, single-function benchmarks (HumanEvalFix, CanItEdit). This is a missed opportunity IMO, as the AGENTPACK dataset itself contains complex, multi-file edits. Evaluating on more challenging benchmarks like SWE-bench would have been a far more convincing validation of the dataset's utility."}, "questions": {"value": "1. Could the authors provide results on more complex, multi-file editing benchmarks like SWE-bench? This would better align the evaluation with the claimed complexity of the AGENTPACK data.\n\n2. The paper frames the verbose, LLM-generated instructions as a positive. However, does this create a distribution mismatch and potentially hurt model learning? Could training on these hyper-detailed descriptions harm performance when the model is given a concise, or even vague, instruction from a real user? If the model is trained on these \"solution-aware\" descriptions, is it actually learning the difficult tasks of problem-solving, intent inference, and bug localization? Or is it just learning a simpler translation from a detailed commit description to code?\n\n3. The training format (\"ellipsis-hunk\") seems mismatched from the evaluation format (full file output). Why was this format chosen over others more suitable for model generation, such as OpenAI's [diff format](https://cookbook.openai.com/examples/gpt4-1_prompting_guide#reference-implementation-apply_patchpy) or [Aider's search-replace format](https://aider.chat/docs/more/edit-formats.html)? Have other training representations been explored?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1TRRnuDhYd", "forum": "qDJ2v0ky6A", "replyto": "qDJ2v0ky6A", "signatures": ["ICLR.cc/2026/Conference/Submission14371/Reviewer_XjER"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14371/Reviewer_XjER"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761160547165, "cdate": 1761160547165, "tmdate": 1762924789245, "mdate": 1762924789245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AGENTPACK, a large-scale dataset of 1.3M code edits (~60GB) co-authored by LLM-based coding agents (Claude Code, OpenAI Codex, Cursor Agent) and humans, mined from GitHub between April–Aug 2025. The authors argue that agent–human commits exhibit clearer natural language descriptions, more structured changes, and higher quality due to human filtering (merged PRs). They describe a pipeline to detect agent commits, extract diffs, filter dependency files, and align metadata. The paper analyzes code/description properties and task types, and demonstrates that fine-tuning models on AGENTPACK improves HumanEvalFix and CanItEdit performance over CommitPackFT and EditCoder datasets"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Relevant and timely dataset capturing the rise of coding agents.\nSolid motivation: agent-written commits include richer natural language intent than human-only commits.\nCareful filtering: merged commits only; node_modules removal.\nExtensive dataset analysis across languages, file types, patch sizes, and edit tasks.\nDemonstrated performance gains over strong editing baselines (CommitPackFT, EditCoder).\nAddresses an emerging paradigm shift in software engineering: human–LLM collaboration.\nUseful resource for future research in code editing and agent behavior."}, "weaknesses": {"value": "1. **Evaluation scope too limited**\n   - Only Python subset used, despite claiming multi-language strengths.\n   - Benchmarks are relatively small; improvements modest.\n   - No test on real repositories or multi-file editing despite dataset emphasis.\n\n2. **Assumptions not empirically validated**\n   - “Merged commits = high quality”\n   - “LLM commit messages = true intent”\n   - No human study confirming commit message correctness or utility.\n\n3. **Agent attribution uncertain**\n   - Signature-matching may miss cases or misattribute commits.\n   - Cursor backend ambiguity creates label noise.\n\n4. **Missing critical ablations**\n   - Quantity vs quality effect not disentangled.\n   - No filtering strategy ablation.\n   - No comparison to synthetic R1/agent-rationale datasets.\n\n5. **Licensing and usage implications unclear**\n   - Commercial training concerns left ambiguous.\n   - No discussion of potential security or insecure-code contamination.\n\n6. **Underexplored failure cases**\n   - Where does AGENTPACK underperform?\n   - Does training produce verbose or agent-like coding artifacts?"}, "questions": {"value": "1. How reliable is the agent detection pipeline? Did you manually sample for attribution accuracy?\n2. Do LLM commit messages ever hallucinate rationale? Any validation?\n3. Why not evaluate on JS/TS (largest portion of dataset)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oTG7n2AMlv", "forum": "qDJ2v0ky6A", "replyto": "qDJ2v0ky6A", "signatures": ["ICLR.cc/2026/Conference/Submission14371/Reviewer_nPd5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14371/Reviewer_nPd5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905799687, "cdate": 1761905799687, "tmdate": 1762924788746, "mdate": 1762924788746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AgentPack, a valuable dataset that captures code changes co-authored by LLM agents and human developers and successfully merged into production branches. The dataset’s methodology of using human acceptance as a critical quality filter, combined with the LLM’s ability to generate rich context, results in a corpus superior to historical human-only commit data for training advanced code-editing models. The contribution is significant and timely for advancing software engineering agent research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Existing datasets for code changes are known to be noisy, with several tangled commits. In this work, the authors explicitly focus on commits created by coding agents with humans as gatekeepers, including only those that successfully merged into the main/master branch. As a result, the dataset benefits from an inherent human-in-the-loop filtering, ensuring that the data captures production-ready changes for training reliable code editing models. \n\n2. The LLM-generated commit messages are not merely longer (median ~10x), but provide a qualitatively superior, explicit rationale for the change. This rich context is invaluable for teaching models the complex mapping between a detailed, single-purpose instruction (the \"intent\") and the corresponding code implementation.\n\n3. The dataset captures structurally multi-hunk, multi-file code changes, often spanning large patch sizes (median 70 lines). The authors correctly point out that these are often logically unitary changes (e.g., implementing one feature across several architectural layers) that older datasets wrongly filtered out, providing a more realistic and challenging benchmark for agents."}, "weaknesses": {"value": "1. The main limitation, IMO, is that the dataset is defined by the outcome (i.e., the merged commit) rather than the process (i.e., the full agent-human interaction history). As a result, it is not possible to track the agent trajectory, as well as the actual contribution of the human developers in refining it. This means the dataset trains models on a \"successful collaborative output\"rather than pure agent autonomy, limiting how much can be inferred about the agent’s standalone capability.\n\n2. The claim that AgentPack commits are inherently \"less tangled\" compared to human-only data is hard to fully accept given their median size (70 lines across 2-3 files). A commit of this magnitude has a high structural possibility for introducing logical tangling, even if the agent's intent was singular. The paper’s reliance on the length and detail of the commit message to define quality and untangledness is a strong, yet potentially insufficient, assumption. A qualitative analysis categorizing the logical unity of a sample of these large commits would significantly strengthen this claim."}, "questions": {"value": "1. What criteria or methodology were used to conclusively determine that a long, multi-file commit is logically focused rather than merely a combination of several documented changes?\n\n2. Given the risk of LLM hallucination, how did the authors verify or ensure that these detailed rationales are factually correct and accurately describe the changes made to the code, rather than fabricating justifications? Is there a metric provided for the correctness of the commit messages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pdVGim3EsL", "forum": "qDJ2v0ky6A", "replyto": "qDJ2v0ky6A", "signatures": ["ICLR.cc/2026/Conference/Submission14371/Reviewer_vtF5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14371/Reviewer_vtF5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950525381, "cdate": 1761950525381, "tmdate": 1762924788299, "mdate": 1762924788299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AgentPack, a corpus of 1.3M code edits co-authored by Claude Code, OpenAI Codex, and Cursor Agent across public GitHub projects. This corpus can be used to fine-tune large language models for code editing. Empirical results in this paper show that models fine-tuned on AgentPack can outperform models trained on prior human-only commit corpora."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper proposes a new dataset co-authored by agents and humans for fine-tuning LLMs on code editing tasks. \n2. Empirical results show that models like DeepSeekCoder fine-tuned on the proposed corpus outperform models trained on prior human-only commit corpora.\n3. This paper is easy to follow."}, "weaknesses": {"value": "1. There are limited empirical results. For example, in Table 4, this paper only fine-tuned one LLM, i.e., DeepSeekCoder, and only compared with one prior training dataset EditCoder. More LLMs like CodeLlama and Mixtral can be included in the experiments, to make the results more solid.\n2. Neither codebase nor dataset has been released to ensure reproducibility.\n3. Some settings are unclear. Which exact model does Claude Code and Codex use?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xBS9vhUHFx", "forum": "qDJ2v0ky6A", "replyto": "qDJ2v0ky6A", "signatures": ["ICLR.cc/2026/Conference/Submission14371/Reviewer_7ETF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14371/Reviewer_7ETF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974545431, "cdate": 1761974545431, "tmdate": 1762924787778, "mdate": 1762924787778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}