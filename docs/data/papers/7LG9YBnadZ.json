{"id": "7LG9YBnadZ", "number": 8475, "cdate": 1758085560734, "mdate": 1759897781956, "content": {"title": "Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models", "abstract": "Large vision–language models (VLMs) achieve strong benchmark performance, but controlling their behavior through instruction tuning remains difficult. Reducing the budget of instruction tuning dataset often causes regressions, as heuristic strategies treat models as black boxes and overlook the latent capabilities that govern learning. We introduce Capability-Attributed Data Curation (CADC), a framework that shifts curation from task-specific heuristics to intrinsic capability analysis. CADC discovers intrinsic capabilities in an unsupervised manner from gradient-based learning trajectories, attributes training data to these capabilities via influence estimation, and curates capability-aware curricula through balanced selection and staged sequencing. This transforms black-box instruction tuning into a controllable, capability-driven process. With as little as 5\\% of the original data, CADC surpasses full-data training on multimodal benchmarks. These results validate intrinsic capabilities as the fundamental building blocks of model learning and establish CADC as a principle paradigm for instruction data curation.", "tldr": "CADC discovers intrinsic capabilities from learning dynamics, maps training data to them, and curates balanced curricula; with only 5% data, it surpasses full-data baselines in multimodal benchmarks.", "keywords": ["Vision-Language Models", "Supervised Fine-Tuning", "Data Curation", "Intrinsic Capabilities", "Curriculum Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b7be02f91014548c892386e412d4a2908df2ab3b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of data curation for instruction tuning in large vision-language models (VLMs). The authors argue that conventional heuristic-based data reduction strategies treat models as \"black boxes\" and often lead to performance degradation. To solve this, the paper introduces the CADC framework. CADC shifts the paradigm from task-specific heuristics to an analysis of the \"intrinsic capabilities\" that the model develops during the learning process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. CADC actually introduces a novel method for guiding model training and evolution by analyzing gradient-based learning trajectories on multi-task benchmarks to understand and leverage a model's intrinsic capabilities.\n\n2. CADC can achieve comparable or even superior performance to training on a full dataset while using as little as 5% of the original instruction tuning data."}, "weaknesses": {"value": "1. Questionable Generalizability to Large-Scale Models: The framework's core experiments are conducted on a very small model (SmolVLM-256M). While showing benefits on this scale, the claim of achieving superior performance with only 5% of data may not hold for current state-of-the-art, large-scale Vision-Language Models (e.g., LLaVA-OV, Qwen-VL 2.5), whose learning dynamics and data requirements are substantially different.\n\n2. Although the final training set is smaller, the process to create it is computationally expensive. The framework requires calculating gradient-based trajectories and influence scores for the entire dataset, a process far more costly than simpler data selection heuristics. This high upfront cost of curation might offset the efficiency gains made during the final fine-tuning stage.\n\n3. The framework distills 162 distinct subtasks from MMT-Bench into just three broad intrinsic capabilities. This high level of abstraction risks oversimplification, potentially masking crucial, finer-grained skills. For instance, grouping dozens of diverse tasks under \"Perceptual Recognition\" may prevent the specialized curation needed for distinct sub-skills within that category, leading to a less nuanced training curriculum."}, "questions": {"value": "1. On Model Generalizability: How does the framework's effectiveness, particularly the 5% data efficiency, generalize to current state-of-the-art models significantly larger than 7B parameters (e.g., the Qwen-VL 2 / 2.5 series)? Does the performance advantage of CADC diminish as the base model becomes more capable?\n\n2. Could you provide a detailed analysis of the computational overhead for the data selection stage? More importantly, what is the underlying reason for the performance degradation observed when the data subset was increased from 15% to 20%?\n\n3. What is the justification for selecting exactly three intrinsic capabilities (K=3)? Could you provide an ablation study on K to show how performance varies with different numbers of clusters? Furthermore, are these discovered capabilities a stable property of the model, or an artifact of using MMT-Bench as the target dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pSFLTJZb9K", "forum": "7LG9YBnadZ", "replyto": "7LG9YBnadZ", "signatures": ["ICLR.cc/2026/Conference/Submission8475/Reviewer_xEJf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8475/Reviewer_xEJf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578389207, "cdate": 1761578389207, "tmdate": 1762920354177, "mdate": 1762920354177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a data selection method for MLLMs that uses gradient-based learning trajectories of MMT-Bench as an influence metric. The method clusters validation data into three high-level categories (grounding, perception, reasoning) based on the cosine similarity of these trajectories. It then uses this clustering to adapt the data budget allocation and the training data sequence, framing this as a score-based curriculum learning approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction of a score-based curriculum learning framework for the MLLM data selection task is a promising direction to the field.\n\n2. The proposed data selection pipeline is comprehensive, addressing the full process from data scoring and attribution to data budget allocation and the final learning scheme design."}, "weaknesses": {"value": "1. The motivation for the paper feels insufficiently justified, and the novelty of the proposed method appears limited. For example, the manuscript fails to clearly differentiate its core concept of \"capability\" from existing concepts, such as the \"concepts\" or \"skills\" discussed in prior work (e.g., COINCIDE). While the specific score formulation may differ, the fundamental method of performing data attribution via clustering is highly similar. The proposed approach could be interpreted as a simplified/coarse instance of this existing framework, with a K=3 clustering. The claim that this K=3 clustering makes the method \"white-box\" or \"intrinsic\" to the model dynamics is not well-supported. Furthermore, the three identified categories (perception, reasoning, etc.) are already widely acknowledged in the literature (e.g., in the MME benchmark's level-1 categories) and do not offer new insight.\n\n2. There appears to be a contradiction in the paper's positioning. The authors claim their method is distinct from previous heuristic and task-driven approaches, yet they utilize MMT-Bench as a validation set to discover capabilities. This discovery process itself appears to be heuristic, which undermines the initial claim.\n\n3. The authors' end-to-end pipeline includes a Sequence training stage. It is unclear if the baseline methods used for comparison also benefit from this sequencing stage. If they do not, the reported performance gains may stem from the curriculum learning scheme rather than the novel data selection metric itself. This must be clarified to validate the paper's core claims.\n\n4. The paper is missing a computational analysis of the entire pipeline. Calculating gradient-based learning trajectories is known to be computationally expensive. This is a critical omission for a data selection method, especially one based on influence functions. The authors should provide a thorough analysis of the computational overhead (e.g., similar to Figure 7 in COINCIDE) to assess the method's practical feasibility.\n\n5. The presentation of benchmark results is inconsistent across the manuscript's tables and figures. This inconsistency makes it difficult for the reviewer to follow the experiments and accurately assess the method's efficacy."}, "questions": {"value": "Heuristic-based finer-grained capabilities works with the advantage of being more scalable and are demonstrably effective and generalizable. For example, Harvesting Rich, Scalable and Transferable Multi-Modal Data for Instruction Fine-Tuning (2025). Can we reasonably assume that existing heuristic approaches remain a more practical solution regarding computational cost? The authors should provide a detailed computational analysis to correct this assumption if it is wrong."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "82fbFix9nT", "forum": "7LG9YBnadZ", "replyto": "7LG9YBnadZ", "signatures": ["ICLR.cc/2026/Conference/Submission8475/Reviewer_KJpr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8475/Reviewer_KJpr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761586923104, "cdate": 1761586923104, "tmdate": 1762920353824, "mdate": 1762920353824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Capability-Attributed Data Curation, a framework for instruction tuning in vision–language models that replaces heuristic, task-specific curation with a capability-centric approach. CADC works in three stages, Discovery, Attribution and Curation. Experiments on LLaVA and SmolVLM across benchmarks show that CADC achieves full-data or superior performance using only 5% of training data, outperforming baselines such as COINCIDE, ICONS and TIVE. Ablations and transferability tests further demonstrate robustness and interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors propose a novel algorithm that recasts data curation as capability-driven rather than task-driven, providing a principled foundation for model control and interpretability.\n  \n- The proposed method demonstrates cross-model and cross-dataset generalization, with small models generating reusable subsets for larger ones.\n\n- The ablation study exhibits the necessity of each component and the benefit of balanced sequencing."}, "weaknesses": {"value": "- In Table 1, most baseline results (e.g., TIVE, ICONS, COINCIDE) are reported using LLaVA-v1.5-7B, while CADC is evaluated on SmolVLM-256M. Since model architectures and capacities differ substantially, performance gains may partially stem from these discrepancies rather than the proposed method itself. For a fair comparison, baselines and CADC should be evaluated using the same model and training setup.\n\n- The main experiments primarily focus on SmolVLM-256M trained on LLaVA-1.5 Mix665K, which is relatively small-scale. Results on larger and more diverse models (e.g., LLaVA-v1.5-7B, SmolVLM-2.2B, Qwen2-VL/Qwen2.5-VL families) and datasets would strengthen the empirical evidence and demonstrate the scalability of CADC.\n\n- The proposed framework involves computing gradient trajectories, influence estimation, and community detection, all of which appear computationally intensive. The paper lacks a clear analysis of time and memory costs compared to existing data selection baselines and to training on the full dataset, which is essential given the method’s stated goal of improving efficiency."}, "questions": {"value": "1. Table 2 shows that CADC’s improvements diminish for larger models. Could the authors clarify why CADC is less effective in this regime? Additionally, why are only CADC-T results reported on Vision-Flan, while direct CADC results are missing?\n\n2. The experimental design for Table 1 is difficult to interpret. Specifically:\n- Why do the benchmarks differ between the 15% data and 20% data settings?\n- Why are the baselines not consistent across these two settings?\n- Why is the 20% CADC result shown only in Table 1(b) but not in Table 1(a)?  \nClarifying these choices would help assess the comparability of results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aOQIZNkSJa", "forum": "7LG9YBnadZ", "replyto": "7LG9YBnadZ", "signatures": ["ICLR.cc/2026/Conference/Submission8475/Reviewer_i7e7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8475/Reviewer_i7e7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601528310, "cdate": 1761601528310, "tmdate": 1762920353471, "mdate": 1762920353471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Capability-Attributed Data Curation (CADC), a framework for selecting and sequencing instruction-tuning data for vision–language models based on their intrinsic capabilities, which are discovered in an unsupervised manner from gradient-based learning dynamics. The work addresses efficiency and controllability issues in instruction tuning by clustering tasks according to how the model learns them, mapping training data to these latent capabilities via influence analysis, and designing balanced, staged curricula. \n\nMain contributions include: (1) a method for intrinsic capability discovery, (2) an influence-based attribution technique to map data to capabilities, (3) a curriculum arrangement and sequencing strategy for balanced capability growth, and (4) empirical results showing CADC surpasses full-data baselines with only 5% of the data; LLM baselines and pruning methods are considered."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces an interpretable capability-based framework for data curation grounded in the model’s own learning dynamics, rather than heuristic or task-based selection.  \n2. Demonstrates good efficiency, outperforming baselines and full-data training while using small subsets.  \n3. Provides detailed experimental validation, transferability studies, and ablations that clarify the contribution of each component."}, "weaknesses": {"value": "1. The definition and interpretation of “intrinsic capabilities” may be subjective, and the clustering method may be sensitive to hyperparameters.  \n2. Heavy reliance on specific experimental settings may limit generalizability outside those regimes.  \n3. Some methodological steps, such as curriculum sequencing based on self-influence trends, could be under-motivated or require more rigorous statistical justification.  \n4. Lack of scalability experiments on larger LMs (7b / 72b); Why are benchmarks inconsistent in Table 1 (a) and (b)? Do these results achieve improvement or a drop?"}, "questions": {"value": "1. How sensitive is the intrinsic capability discovery process to the choice of similarity threshold τ and the community detection parameters in the Leiden algorithm?  \n2. What happens when the number of discovered capabilities K is misestimated—how robust is CADC to over- or under-clustering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rJHgZL1WkP", "forum": "7LG9YBnadZ", "replyto": "7LG9YBnadZ", "signatures": ["ICLR.cc/2026/Conference/Submission8475/Reviewer_RTQv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8475/Reviewer_RTQv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000152147, "cdate": 1762000152147, "tmdate": 1762920353177, "mdate": 1762920353177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}