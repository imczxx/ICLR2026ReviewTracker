{"id": "eu3PwSle8J", "number": 13573, "cdate": 1758219333503, "mdate": 1759897427717, "content": {"title": "Enforcing Instruction Hierarchy via Augmented Intermediate Representations", "abstract": "Indirect prompt injection attacks are a critical security vulnerability in large language models (LLMs), allowing attackers to hijack model behavior by injecting malicious instructions within the input context. Recent defense mechanisms have leveraged an Instruction Hierarchy (IH) Signal – often implemented through special delimiter tokens or additive embeddings – to denote the privilege level of input tokens. However, these prior works typically inject the IH signal exclusively at the initial input layer, which we hypothesize limits its ability to effectively distinguish the privilege levels of tokens as it propagates through the different layers of the model. To overcome this limitation, we introduce a novel approach that injects the IH signal into the intermediate token representations within the network. Our method augments these representations with layer-specific trainable embeddings that encode the privilege information. Our evaluations across multiple models and training methods reveal that our proposal yields between $1.6\\times$ and $9.2\\times$ reduction in attack success rate on gradient-based prompt injection attacks compared to state-of-the-art methods, without significantly degrading the model's utility.", "tldr": "Injecting privilege signals deeper within the LLMs significantly improves their robustness against indirect prompt injection attacks.", "keywords": ["prompt injection", "defense", "instruction hierarchy"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37b34d52817044777c839761d83ddfca78e8fd16.pdf", "supplementary_material": "/attachment/712109fa93b5ed1de06591aa5ece435b7b3d4d61.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Augmented Intermediate Representations (AIR), a defense mechanism against prompt injection attacks in LLMs. Existing defenses introduce an Instruction Hierarchy (IH) signal, indicating the privilege level of input tokens, but inject it only at the input layer. The authors hypothesize that this limits the model’s ability to maintain hierarchical distinctions as information propagates through layers. To address this, AIR injects layer-specific, trainable embeddings encoding IH signals into every decoder block of the model. This recurrent injection ensures that privilege information remains accessible throughout the network."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper proposes a new architectural idea that injects Instruction Hierarchy signals at every layer of the model rather than only at the input, offering a fresh approach to prompt-injection defense distinct from earlier input-based methods.\n2.\tThe experiments cover different model sizes and both SFT and DPO training setups, and consistently show gains in robustness.\n3.\tThe problem addressed, protecting models from prompt injection in settings involving untrusted data and agent workflows, is timely and highly important."}, "weaknesses": {"value": "1.\tUtility is measured mostly via AlpacaEval win rates. There is no assessment of factual accuracy or reasoning (e.g., MMLU), so it is difficult to judge whether AIR affects model quality in benign settings. Including standard benchmarks would strengthen the claims.\n2.\tAlthough the paper suggests AIR can be applied to direct prompt attacks and agent settings, no experiments verify this. Multi-turn, retrieval-augmented, and user-as-attacker (jailbreak) scenarios remain unexplored.\n3.\tThere is no visualization of how AIR changes attention patterns across layers. Such analysis could clarify whether AIR genuinely preserves hierarchical separation or merely adds regularization noise.\n4.\tThe SFT models are fully fine-tuned, while DPO models use LoRA. This mismatch may explain the utility drop seen in Figure 8. A controlled comparison (full-FT vs LoRA in both settings) would clarify this."}, "questions": {"value": "1.\tQwen required a much larger initialization scale for the IH embeddings. Can the authors provide a systematic study of initialization and stability? Otherwise, AIR’s robustness may depend on model-specific tuning rather than a generalizable method.\n2.\tUtility is measured mainly via AlpacaEval win rate. Can the authors report additional evaluations (e.g., MMLU, BLEU, factual consistency, human preference) to confirm that AIR does not subtly degrade model quality in benign settings?\n3.\tSince the paper reimplements prior defenses, can the authors verify that the reproduced baselines match the original papers’ reported performance, or provide an error margin? This would ensure a fair comparison and strengthen the empirical claims.\n4.\tThe method uses trainable embeddings for privilege levels. Could the authors verify that learning these embeddings is necessary? For example, how does AIR perform if fixed random vectors are used instead? A comparison would clarify whether the improvement comes from the learned hierarchy signal or simply from adding noise/perturbations at each layer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wQsVchfiC4", "forum": "eu3PwSle8J", "replyto": "eu3PwSle8J", "signatures": ["ICLR.cc/2026/Conference/Submission13573/Reviewer_Pn8n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13573/Reviewer_Pn8n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849250638, "cdate": 1761849250638, "tmdate": 1762924170244, "mdate": 1762924170244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new defense against indirect prompt injections, a type of attack where the attacker injects malicious content into the input context. The authors propose Augmented Intermediate Representations (AIR), which, unlike prior approaches that apply Instruction Hierarchy (IH) information only at the input of the transformer (e.g. using delimiter tokens or IH embeddings), adds IH embeddings at every transformer block. They hypothesize that previous methods lose IH signal strength as it propagates through model layers, which motivates their method. The paper benchmarks AIR on the AlpacaFarm and SEP datasets using Llama-3.2-3B, Qwen-2.5-7B, and Llama-3.1-8B, trained with either SFT or DPO, under both static and gradient-based attacks. On these benchmarks, AIR achieves favorable results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Relevant topic**: The paper addresses a relevant, timely, and practical problem.\n- **Clear motivation and presentation**: The paper is well-written and clearly explains why the method is needed.\n- **Simple approach**: The method is easy to implement and introduces only a relatively small number of additional parameters.\n- **Strong empirical section**: The evaluation is extensive and covers multiple base models, attack types, and training regimes (SFT, DPO). See Weaknesses for comments regarding datasets."}, "weaknesses": {"value": "- **Validation of motivating hypothesis**: The hypothesis requires stronger validation. It remains unclear whether existing methods fail due to IH signal degradation, as claimed in section \"Limitations of Existing Defenses\". Measuring cosine similarity across layers is not sufficient, especially for delimiter-based methods. A simple linear probing experiment (as done e.g. in ASIDE) to test IH separability would strengthen the claim substantially. This is particularly important since identifying this limitation of input-only methods is listed as one of the papers three main contributions.\n\n- **Static attacks fail**: The reported improvements come mainly from gradient-based attacks. Static attacks appear to fail even for the naive baseline, so robustness improvements there seem less meaningful. Including more difficult or diverse attack benchmarks would make the results more convincing."}, "questions": {"value": "- The authors may want to look into ASIDE, which proposes a closely related defense method, also addressing a similar “IH signal degradation” issue in ISE. While not being obligatory, a comparison between AIR and ASIDE would be scientifically valuable, as they both target a similar goal but try to achieve it with different methods: ASIDE enforces IH separation via orthogonal rotations at the input layer, whereas AIR reinforces the IH signal throughout the network with IH embeddings.\n- L135 Spelling error: \"Ig nore\"\n- Figure 4 seems to be wrong as a single decoder block contains two masked self-attention computations. \n\n **References**:\n- Zverev et al. “ASIDE: Architectural Separation of Instructions and Data in Language Models.” ICLR 2025 Building Trust in LLMs and LLM Applications Workshop (non-archival), 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iNT1ePrhRe", "forum": "eu3PwSle8J", "replyto": "eu3PwSle8J", "signatures": ["ICLR.cc/2026/Conference/Submission13573/Reviewer_kjgc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13573/Reviewer_kjgc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900877864, "cdate": 1761900877864, "tmdate": 1762924169473, "mdate": 1762924169473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses prompt injection attacks in LLMs by proposing Augmented Intermediate Representations (AIR), a defense mechanism that injects instruction hierarchy (IH) signals across all decoder layers rather than only at the input layer. The authors argue that existing defenses (using delimiters or input segment embeddings) suffer from signal degradation through the network. Experiments across three models show AIR reduces attack success rates by 1.6-9.2× on gradient-based attacks compared to"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation and well-identified limitation: The observation that IH signals degrade through decoder layers (Figure 3) is compelling and provides solid motivation for the proposed approach. The parallel to positional embeddings (RoPE) is insightful.\n\n2. Comprehensive experimental evaluation: The paper evaluates multiple models (3B, 7B, 8B parameters), training methods (SFT, DPO), and attack types (static and gradient-based), demonstrating thoroughness.\n\n3. Minimal overhead: The additional parameters (0.005% for Llama3.1-8B) and inference compute are negligible, making the approach practical.\n\n4. Well-structured presentation: The paper is clearly written with good use of figures and tables to convey results."}, "weaknesses": {"value": "1. Limited theoretical justification: While Figure 3 shows cosine similarity increases across layers, this alone doesn't conclusively prove that IH signal degradation is the limiting factor. Alternative explanations could include:\n- The difficulty of learning from input-only signals during training\n- The specific architecture's tendency to homogenize representations\n- The paper would benefit from ablation studies showing what happens with AIR at only some layers, or from analyzing attention patterns to demonstrate that AIR helps maintain privilege distinctions.\n\n2. Inconsistent performance across training methods: AIR-SFT sometimes shows lower utility than the None baseline (Figure 8b), particularly for Qwen-2.5-7B and Llama-3.1-8B. This is concerning and inadequately explained. The paper should:\n- Investigate why this degradation occurs specifically with SFT\n- Provide guidance on when to use DPO vs. SFT with AIR\n- Discuss potential mitigation strategies\n\n3. Model-specific hyperparameter sensitivity: The need for different initialization strategies for Qwen (σ=0.1 vs. σ=0.02 for Llama) raises concerns about generalization:\n- How sensitive is performance to this choice?\n- What guidance can be provided for applying AIR to new model families?\n- The lack of hyperparameter tuning \"due to computational constraints\" is unsatisfying for a defense mechanism intended for practical deployment.\n\n4. Limited evaluation scope:\n- Only single-turn interactions are tested (acknowledged in limitations)\n- No evaluation on real-world prompt injection scenarios beyond AlpacaFarm and SEP\n- No comparison with detection-based defenses mentioned in Appendix D\n- Static attack evaluation less informative: Since all three IH mechanisms achieve near-perfect defense against static attacks (Table 1), these results don't effectively differentiate approaches. More emphasis should be placed on adaptive attacks."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7i3jZvtrxI", "forum": "eu3PwSle8J", "replyto": "eu3PwSle8J", "signatures": ["ICLR.cc/2026/Conference/Submission13573/Reviewer_PM5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13573/Reviewer_PM5M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964788673, "cdate": 1761964788673, "tmdate": 1762924168791, "mdate": 1762924168791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}