{"id": "tMRTMdi5Hz", "number": 5827, "cdate": 1757938084190, "mdate": 1759897951004, "content": {"title": "Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment", "abstract": "Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity (O(1/ε) vs O(1/ε2)) and empirically validate a 4.5$\\times$ noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models. Results demonstrate a clear performance hierarchy: point-based methods $<$ static pairwise training $<$ Elo-Evolve across Alpaca Eval 2.0 and MT-Bench, validating the progressive benefits of pairwise comparison and dynamic opponent selection for LLM alignment.", "tldr": "", "keywords": ["Co-evolutionary", "Alignment", "Language Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4dd72288c26d7fa4274de1d331b87ebb9a20825e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces a novel method of preference optimization by using a dynamic elo as the reward for RLHF. The idea is clear and the method is clearly presented. Experiments have shown significant improvement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method is simple while surprisingly effective in practice with Qwen base models. The ablation study is sufficient."}, "weaknesses": {"value": "- The first one of the and main issues is that the contribution is claimed to be \"eliminating Bradley-Terry model dependencies\". But this isn't new at all. Bunch of previous works have addressed this or propose this as contribution such as [1,2,3,4]. Especially INPO [4] is exactly directly learning from binary win/loss outcomes. These are missing literatures as well, the author should include these related works, compare and clarify the contribution of this work. \n\n- Following the above issue, although it is interesting to see a significant improvement in AlpacaEval, it's unclear how to compare with INPO [4].\n\n- Another main issue is that experiments are only conducted with single type of base model Qwen. It's necessary to test the methods on other base models to demonstrate the effectiveness.\n\nWritings:\n- I personally think using symbols like \"<\" in writing especially abstract is informal.\n- Some notational issues around Eq. (3), such as unclear what is R in the definition of E. Should that be R_t?\n\nIn general, there are notable limitations of current manuscript.\n\n\n\n[1] Munos, Rémi, et al. \"Nash learning from human feedback.\" Forty-first International Conference on Machine Learning. 2024.\n[2] Wang, Mingzhi, et al. \"Magnetic preference optimization: Achieving last-iterate convergence for language model alignment.\" arXiv preprint arXiv:2410.16714 (2024).\n[3] Tang, Xiaohang, et al. \"Game-Theoretic Regularized Self-Play Alignment of Large Language Models.\" arXiv preprint arXiv:2503.00030 (2025).\n[4] Zhang, Yuheng, et al. \"Iterative nash policy optimization: Aligning llms with general preferences via no-regret learning.\" arXiv preprint arXiv:2407.00617 (2024)."}, "questions": {"value": "- This method compared to self-play-based methods is significantly more expensive since typically a larger model is required to act as opponent for Elo-Evolve. Given that, is it more worthy investing computing in scaling base model (i.e. using a larger base model) and conduct self-play or even normal RLHF (e.g. Point GRPO)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EWaTJlmqES", "forum": "tMRTMdi5Hz", "replyto": "tMRTMdi5Hz", "signatures": ["ICLR.cc/2026/Conference/Submission5827/Reviewer_nZun"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5827/Reviewer_nZun"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761253720469, "cdate": 1761253720469, "tmdate": 1762918286358, "mdate": 1762918286358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing alignments methods typically distill human preference data into reward models. The paper proposes an alternative using a dynamic multi-agent framework to bypass traditional bradley-terry approaches. They show strong performance as a Qwen-2.5-7B model trained with this approach is able to improve on AlpacaEval 2.0 and MT Bench."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe adaptive curriculum learning approach used in Elo-Evolve is interesting where a different reference opponent model is used at each stage of the training allowing the model to progressive improve against stronger opponents."}, "weaknesses": {"value": "1.\tThe claim in Lines 40-45 seems un/under-substantiated. Claim 1 is not supported by any literature and there has been evidence such as HelpSteer2-preference [1] that shows only 10 thousand samples is enough for training high quality reward models. For claim 2, it’s not clear what sub-optimal sample complexity is and claim 3 is supported by 1 paper from 2020, even though the post training field has evolved substantially since then.\n2.\tThe results in Table 1 don’t seem to be very strong. For instance, the Elo-Evolve performs at 38.03 on AlpacaEval 2 LC while the Point GRPO is at 37.41, which is presumably within one SD. On MT Bench, the performance of Elo-Evolve is at 8.04 while DNO is at 7.97, which is also unlikely to be substantially different. The “vs. Qwen xxx” baselines should be interpreted as ablations but in this case, some ablations outperform the Elo-Evolve algorithm which suggest that maybe a static opponent might be good enough.\n3.\tMetrics are slightly outdated – with AlpacaEval 2 and MT Bench both from late 2023, with known flaws such as length bias. AlpacaEval 2 Length Control is the most recent (from early 2024) and should be used instead of AlpacaEval 2 WR (rather than in addition to it). Furthermore, I think more up-to-date metrics such as Arena Hard [5] or WildBench [6] should be used (both from late 2024) since they reflect capabilities of recent models better (e.g. Qwen 2.5 was released in late 2024).\n4.\tI think a missing baseline is to use the same LLM-Judge (Qwen3-14B-Instruct) and just use the generated responses against one another in GRPO. This can help us to understand whether the improved performance is due to using a better judge (compared to the WorldPM point-RM) or because of the reference responses from the (adaptive) opponents. Using the LLM-Judge by itself has been done many times e.g. [3] and is likely to be useful without needing for external model responses.\n\n[1] HelpSteer2-Preference: Complementing Ratings with Preferences https://arxiv.org/abs/2410.01257\n\n[2] AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback https://arxiv.org/abs/2305.14387\n\n[3] Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena https://arxiv.org/abs/2306.05685 \n\n[4] Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators https://arxiv.org/abs/2404.04475\n\n[5] From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline https://arxiv.org/abs/2406.11939\n\n[6] WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild https://arxiv.org/abs/2406.04770"}, "questions": {"value": "1.\tWhy is there a need to report performance at steps 100/300/500? My understanding is that only the optimal step across a run should be reported since different methods might have different optimal training steps. Table 1 and 2 currently looks confusing with too many values and bolded values."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qRMRVT3UJ4", "forum": "tMRTMdi5Hz", "replyto": "tMRTMdi5Hz", "signatures": ["ICLR.cc/2026/Conference/Submission5827/Reviewer_3CYn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5827/Reviewer_3CYn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698044325, "cdate": 1761698044325, "tmdate": 1762918286044, "mdate": 1762918286044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a game-theoretic alignment algorithm for large language models that leverages a pool of opponents. In Elo-Evolve, the proposed method, an LLM is trained by playing against opponents matched based on their ELO scores. ELO-based matching provides a natural learning curriculum, enabling the resulting LLM to achieve competitive performance across various benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The presented idea of using ELO rating for opponent matching is reasonable and presented clearly.\n- Performance gain seems consistent.\n- Clever length bias mitigation is used."}, "weaknesses": {"value": "1. The paper lacks comparison and discussion regarding self-play alignment methods. In recent years, significant attention has been devoted to alignment algorithms based on self-play.\n    - Such methods do not rely on the Bradley–Terry model and often leverage game-theoretic ideas. Ideally, the current manuscript could be much stronger by providing a discussion of self-play methods (e.g., how Elo-Evolve could outperform self-play methods) and including empirical comparisons.\n2. Compared to self-play methods, Elo-Evolve requires additional pre-trained opponent models. While a self-play method typically requires a pre-trained generalized preference model, Elo-Evolve additionally depends on pre-trained opponents.\n3. The paper allocates a non-trivial amount of space to the discussion of the benefits of relative reward signals (e.g., Sections 3 and 5.2). Although these are interesting results, the claims do not specifically support Elo-Evolve, but rather broadly support all methods that use a generalized preference model. The limitations of the Bradley–Terry model have already been discussed several times in the self-play literature (although I do not think the exact argument has been presented before).\n\nFor self-play alignment methods, see, for example,\n\n[1] Wu, Yue, et al. \"Self-play preference optimization for language model alignment.\" arXiv preprint arXiv:2405.00675 (2024).\n\n[2] Tang, Xiaohang, et al. \"Game-Theoretic Regularized Self-Play Alignment of Large Language Models.\" arXiv preprint arXiv:2503.00030 (2025).\n\n[3] Munos, Rémi, et al. \"Nash learning from human feedback.\" Forty-first International Conference on Machine Learning. 2024."}, "questions": {"value": "1. Can Elo-Evolve be used to train a model that is significantly better than the provided opponents? How can we push the state-of-the-art of language models using Elo-Evolve beyond the strongest provided opponent?\n2. What would be the advantages of Elo-Evolve over self-play-based alignment methods, such as SPPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "17P1o9R6GN", "forum": "tMRTMdi5Hz", "replyto": "tMRTMdi5Hz", "signatures": ["ICLR.cc/2026/Conference/Submission5827/Reviewer_jDP1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5827/Reviewer_jDP1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762523877, "cdate": 1761762523877, "tmdate": 1762918285549, "mdate": 1762918285549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Elo-Evolve, a co-evolutionary framework that aligns LLMs through dynamic multi-agent competition. Instead of static reward models, the policy learns from binary win/loss signals in pairwise matches. An Elo-based opponent selection introduces automatic curriculum learning: the model faces similar-strength opponents early and stronger ones later. Experiments on UltraFeedback with Qwen models show consistent gains on AlpacaEval 2.0 and MT-Bench over point-based and static pairwise baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The dynamic opponent selection is interesting. The temperature parameter offers a clean way to balance focus and diversity—small T for close-strength opponents, large T for variety. This forms an automatic curriculum where training difficulty grows with model ability.\n\n2. Each prompt selects its own opponent, leading to smoother and more stable training.\n\n3. Replacing scalar rewards with binary win/loss is well-motivated; both the PAC-theoretic analysis and experiments support its efficiency and robustness."}, "weaknesses": {"value": "1. The framework introduces several components, which makes the system design a little complex. It would be helpful to include a simple baseline, where the model is trained sequentially against Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B as progressively stronger opponents. The prompts can be divided into three groups either randomly or based on their difficulty, for example using a reward model to estimate complexity. Such a baseline would help clarify how much the dynamic Elo scheduling improves over a manually designed curriculum.\n2. Because Elo ratings are continuously updated, opponent strength may fluctuate during training. When a main opponent weakens, as observed at Step 500 on MT-Bench, the policy appears to over-adapt to easier adversaries. This may affect measured progress and limit further improvement."}, "questions": {"value": "1. The use of Qwen3-14B-Instruct as the judging model instead of a specialized reward model is not fully discussed. A short explanation of this choice would improve the clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6fYW3WGx5G", "forum": "tMRTMdi5Hz", "replyto": "tMRTMdi5Hz", "signatures": ["ICLR.cc/2026/Conference/Submission5827/Reviewer_oaiY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5827/Reviewer_oaiY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778092949, "cdate": 1761778092949, "tmdate": 1762918284942, "mdate": 1762918284942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}