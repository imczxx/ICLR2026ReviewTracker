{"id": "bKoZoUk0Th", "number": 15114, "cdate": 1758247952046, "mdate": 1759897327598, "content": {"title": "Geometric Embedding Alignment via Curvature Matching in Transfer Learning", "abstract": "Geometrical interpretations of deep learning models offer insightful perspectives into their underlying mathematical structures. In this work, we introduce a novel approach that leverages differential geometry, particularly concepts from Riemannian geometry, to integrate multiple models into a unified transfer learning framework. By aligning the Ricci curvature of latent space of individual models, we construct an interrelated architecture, namely Geometric Embedding Alignment via cuRvature matching in transfer learning (GEAR), which ensures comprehensive geometric representation across datapoints. This framework enables the effective aggregation of knowledge from diverse sources, thereby improving performance on target tasks. We evaluate our model on 23 molecular task pairs and demonstrate significant performance gains over existing benchmark models—achieving improvements of at least 14.4% under random splits and 8.3% under scaffold splits.", "tldr": "A novel transfer learning algorithm leverages Riemannian differential geometry, boosting performance by aligning latent representations across tasks through Ricci scalar curvature matching.", "keywords": ["Geometrical Deeplearning", "Transfer Learning", "Molecular Property Prediction"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/210f4548bce6713813a56124ee581c327436f5e7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes GEAR, a transfer learning framework that uses Riemannian geometry to align the Ricci curvature of latent spaces across multiple models. Evaluations are performed to show effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "a) The paper is well organized."}, "weaknesses": {"value": "**(a) Limited novelty**  \nThe proposed method appears to build heavily on GATE, with Fig. 1 in this paper closely resembling Fig. 2 from GATE. The new approach, GEAR, seems to primarily add an additional term to GATE rather than introducing fundamentally new ideas. This raises concerns about the extent of technical novelty and distinct contribution.\n\n**(b) Poor readability**  \nSections 3.1 and 3.2 contain extensive derivative formulations that appear unnecessary for understanding the method and make the paper harder to follow. Many of these formulas could be moved to the appendix to reduce cognitive load for readers. In practice, the most relevant parts are the loss definitions — which again are similar to those used in GATE.\n\n**(c) Limited applicability**  \nThe proposed transfer learning strategy is presented as a generic framework, yet its evaluation is restricted to molecular property prediction tasks. Without experiments on a broader range of domains, the generality and potential impact of the method outside the molecular setting remain unclear.\n\n**(d) Insufficient model details**  \nThe paper does not clearly describe the backbone network architectures used for the tasks. For a claimed general-purpose algorithm, it would be important to validate performance across more model architectures and report these details explicitly.\n\n**(e) Incomplete ablation analysis**  \nThe overall loss function contains six distinct terms with associated weighting parameters, making training potentially complex and sensitive to hyperparameter choice. The paper lacks analysis of how these weighting parameters influence performance or how each loss component contributes to the final results. A thorough ablation would be necessary to understand the method’s robustness."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UFGQZS3XmL", "forum": "bKoZoUk0Th", "replyto": "bKoZoUk0Th", "signatures": ["ICLR.cc/2026/Conference/Submission15114/Reviewer_6FvH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15114/Reviewer_6FvH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570898237, "cdate": 1761570898237, "tmdate": 1762925436651, "mdate": 1762925436651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GEAR, a novel transfer-learning framework designed to enhance the robustness and efficiency of knowledge transfer, particularly for data-scarce regression tasks.\n\nThe work is motivated by a critique of existing geometric transfer-learning methods. The authors posit that current approaches are limited by a \"local\" and myopic alignment strategy, which focuses on matching infinitesimal distances within the latent space. This, they argue, fails to capture and align the intrinsic global geometric structure of the latent spaces, thereby hindering the potential for fundamentally robust and efficient knowledge transfer.\n\nTo address this perceived limitation, the paper proposes GEAR, a framework grounded in Riemannian differential geometry. The architecture is built upon three core components: (1) independent task encoders that map source and target data into their respective curved latent-space manifolds; (2) a central geometric transfer module that acts as a bridge, mapping these distinct manifolds onto a shared, globally flat common manifold; and (3) a curvature-matching function that serves as the guiding principle for the framework. This loss term is designed to analytically compute and align the Ricci scalar curvature of the two latent manifolds, steering the transfer process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method proposed in this paper apply the idea of matching the Ricci scalar curvature from Riemannian differential geometry to the problem of transfer learning. Compared with prior approaches that rely on local perturbation-based alignment, GEAR captures and aligns the latent space’s global geometric structure by matching curvature.\n\n2. The authors conduct extensive comparisons of GEAR against multiple baseline models and demonstrate its superior performance. Detailed ablation studies clearly validate the crucial role of key model components—particularly the curvature loss—in mitigating overfitting and improving performance."}, "weaknesses": {"value": "1. The discussion of the loss-function design is insufficient, particularly with respect to sensitivity analysis for the multiple hyperparameters. The final total loss (Equation 23) comprises five terms that must be balanced and is controlled by five hyperparameters (α, β, γ, δ, ε). Such a complex design requires careful tuning, yet the paper provides no analysis of how variations in these hyperparameters affect model performance. A method that is highly sensitive to hyperparameters raises questions about its robustness and generalization in practical applications. I therefore recommend that the authors include, in the revised manuscript, a description of the hyperparameter tuning procedure and a sensitivity analysis for these key hyperparameters to demonstrate the practicality and stability of their approach.\n2. The paper’s theoretical motivation is built on the assumption of a “shared manifold M,” i.e., that the latent spaces of different tasks are mapped from a common underlying manifold. However, in the model’s concrete implementation this shared manifold M is not explicitly parameterized or constructed. The alignment operation is realized by computing curvature within each latent space and imposing a loss that forces these curvature measures to match. This alignment appears to occur in a shared locally flat framework rather than on the theoretical manifold M, producing a gap between theory and practice: does matching pointwise-computed Ricci curvature alone guarantee that two latent spaces are effectively aligned onto a unified, coherent underlying manifold? The authors should clearly explain the practical role and instantiation (if any) of the shared manifold M in their implementation.\n\n\nMinor revision:\n1. In Figure 2, the annotated RMSE (root mean square error) is a “lower is better” metric, but it is currently labeled as if “higher is better.”"}, "questions": {"value": "1. The loss function involves five hyperparameters, but the paper lacks a sensitivity analysis, raising questions about the method's practical robustness. Could you provide an analysis of how performance is affected by these hyperparameters to demonstrate the stability of your approach?\n\n2. The theory is based on a \"shared manifold M,\" but the implementation appears to only match Ricci curvatures. Could you clarify how matching curvature practically achieves alignment onto this theoretical shared manifold and why this is a sufficient condition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oiwDPy84nY", "forum": "bKoZoUk0Th", "replyto": "bKoZoUk0Th", "signatures": ["ICLR.cc/2026/Conference/Submission15114/Reviewer_oNjo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15114/Reviewer_oNjo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803086717, "cdate": 1761803086717, "tmdate": 1762925436191, "mdate": 1762925436191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a transfer learning algorithm based on tools from differential geometry, specifically Ricci curvature, for molecular property prediction (regression task). The main idea is to align the latent spaces of a source and a target task by learning mappings into a shared, locally flat frame, and matching their Ricci scalar curvature. The proposed algorithm is based on multiple loss functions, including mapping, curvature, and autoencoder reconstruction losses. The authors evaluate their method on 23 molecular task pairs across 14 molecular properties."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- I think the use of Ricci curvature for global latent space alignment is a novel and interesting contribution to the field of transfer learning and molecular property prediction.\n\n- The authors support their idea with an ablation study showing that the proposed curvature and mapping losses are important for model performance, as removing them could lead to overfitting."}, "weaknesses": {"value": "* The experiments are mainly focused on molecular property prediction. It would be valuable to see how these results extend to other domains/ tasks and data modalities. \n* The presentation of the paper could be improved further, as the underlying math is sometimes advanced, which might limit the accessibility of the method to a broader audience.\n* The paper focuses on a two-task setting (source and target). The authors claim the framework is extensible to scenarios with more than two tasks, but this is not shown. It is unclear, e.g., how the curvature matching would be managed in a multi-task setting."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BOkL6LTmVi", "forum": "bKoZoUk0Th", "replyto": "bKoZoUk0Th", "signatures": ["ICLR.cc/2026/Conference/Submission15114/Reviewer_uBNt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15114/Reviewer_uBNt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956388980, "cdate": 1761956388980, "tmdate": 1762925435665, "mdate": 1762925435665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new transfer learning (TL) framework, named GEAR (Geometric Embedding Alignment via cuRvature matching), designed for regression tasks, with a focus on molecular property prediction. The method interprets the latent spaces of deep learning models as Riemannian manifolds. Building upon prior work like GATE, which aligns latent spaces by matching local infinitesimal distances, GEAR aims to overcome the limitations of local alignment by enforcing a global geometric consistency. The core contribution is a novel loss term that encourages the Ricci scalar curvatures of the source and target latent spaces to match."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The primary conceptual contribution—using the matching of Ricci scalar curvature as an explicit objective for aligning latent spaces in transfer learning—is novel and intellectually stimulating. While the use of Riemannian geometry in deep learning is an active research area, the direct analytical computation and optimization of a global curvature property like the Ricci scalar for transfer learning(TL) is a creative  extension of existing ideas.\n\nSignificance: The paper addresses the important and practical problem of transfer learning in data-scarce domains, particularly for regression tasks in the molecular sciences. The empirical results presented are compelling. Achieving average RMSE improvements of 14.4% (random split) and 8.3% (scaffold split) over a strong predecessor like GATE is significant. If these results are robust and reproducible, the method could have a substantial impact on the field.\n\nQuality & Effort: The technical effort invested in this work is substantial. The appendices contain extensive mathematical derivations, attempting to analytically compute the first, second, and third derivatives of the network's mapping to formulate the Ricci scalar curvature (e.g., Eq. 102, 107). The experimental evaluation is also comprehensive, comparing GEAR against seven different baselines across 23 task pairs and two different data splitting strategies (random and scaffold)."}, "weaknesses": {"value": "My main concerns with this paper revolve around its profound lack of clarity, questionable technical justifications, which collectively undermine the confidence in its impressive results.\n\n1. Severe Clarity and Presentation Issues: The paper is exceptionally dense and challenging to parse, to the point of being almost impenetrable.\n\nOverwhelming Equations: The core technical sections (Sec 3.2, Appendix C) present monolithic equations (e.g., Eq. 96, 102, 107) that span many lines. These are provided with minimal intuition or step-by-step explanation, making them hard to verify or build upon. \n\nConfusing Notation and Logic: The logic behind key components is convoluted. For instance, the $l_{metric}$ loss (Eq. 19-21) is crucial for regularizing the common space, but its formulation and the accompanying text are opaque. The definition of ẑ′ is confusing, and the justification for how minimizing MSE against the identity matrix $η_ij$ enforces local flatness is absent.\n\nUnexplained Complexity in Pseudocode: Algorithm 1 introduces an inner loop (for step k = 1, . . . , K) for iteratively computing the metric. This procedure is not mentioned or justified anywhere in the main text. This is a major omission that introduces unexplained computational complexity and makes the algorithm difficult to understand.\nInaccessible to the Target Audience: The paper fails to bridge the gap between differential geometry and transfer learning. The introduction immediately invokes terms like \"diffeomorphism invariance\" and \"Ricci curvature\" without providing the necessary high-level intuition for an audience expert in transfer learning but not necessarily in geometry. A reader should not have to be a specialist in both fields to grasp the core motivation of the paper. This failure in communication creates an unnecessarily high barrier to entry and signals poor exposition.\n\n\nWhy Ricci Scalar? The paper argues for global geometric alignment but settles for matching the Ricci scalar R. This scalar is a single number representing a highly simplified trace of the full curvature information contained in the Riemann tensor $R^i_{ljk}$. Two geometrically very different manifolds can share the same Ricci scalar at a point. The paper makes the strong claim that this is sufficient for \"accurate alignment\" without providing any justification or considering the trade-offs of not using the richer Ricci or Riemann tensors.\n\nMotivation is Asserted, Not Proven: The central motivation is that local alignment methods like GATE are insufficient. However, this is presented as a given limitation rather than being demonstrated. There is no analysis or illustrative example showing a concrete scenario where GATE fails and GEAR succeeds due to its global perspective.\n\nLack of Justification for Complexity / Omission of Simpler Alternatives: The paper does not justify why the immensely complex machinery of Ricci curvature is the most parsimonious solution. The goal is to capture non-local, second-order properties of the latent space. A much simpler approach would be to align the second-order derivatives (i.e., the Hessians) of the transfer mappings directly. This would also capture the \"bending\" of the space but would be vastly simpler to compute (requiring only second derivatives of the network, not third) and implement. The authors provide no ablation or argument for why the significant leap in complexity to Ricci curvature is necessary or superior to such a simpler alternative.\n\nPotential Weaknesses in Experimental Protocol:\n\nHyperparameter Complexity: The model has a complex loss function with five weighting hyperparameters (α, β, γ, δ, ϵ). Furthermore, Table 2 shows that the encoder architecture itself is a hyperparameter that varies for each of the 23 task pairs. The paper provides no details on how these numerous hyperparameters were tuned. This raises the concern that the strong performance may be the result of exhaustive, task-pair-specific tuning, which would limit the method's general applicability and make the comparison to baselines less controlled.\nUnconventional Robustness Test: The noisy data experiment in Section 5.2 involves corrupting a subset of the test set, adding these corrupted points to the training set, and then evaluating on these same corrupted points. This is not a standard test of robustness to noisy labels. It primarily tests the model's ability to correct/memorize specific noisy examples it has seen during training, rather than its ability to generalize in the presence of a noisy training distribution."}, "questions": {"value": "Questions\n\n\nRegarding Algorithm 1: Please explain the purpose and motivation for the inner loop over k from 1 to K. Is this an iterative procedure performed at each training step? What is its computational cost, and why is it necessary?\n\nRegarding $l_metric$ (Eq. 19-21): Could you provide a clear, step-by-step derivation and intuition for this loss term? The current explanation is difficult to follow. Specifically, how does the proposed formulation mathematically ensure that the manifold M is driven towards being locally flat?\n\n\nRegarding the Choice of Curvature: Why did you choose to match the Ricci scalar R instead of a more informative quantity like the Ricci tensor $R_ij$? Since R is a trace, it loses significant geometric information. Have you explored matching tensor components, and if so, how did it affect performance and computational cost?\n\nRegarding Hyperparameters: How were the loss weights (α to ϵ) and the 23 different task-specific encoder architectures (Table 2) selected? Was a systematic and consistent validation strategy employed? Please comment on the sensitivity of GEAR's performance to these choices.\n\nRegarding Comparison to GATE: The motivation for GEAR rests on the purported limitations of GATE's local alignment. Can you provide a more direct and concrete analysis (e.g., a qualitative visualization of latent spaces for a specific task pair) that demonstrates a failure case of local alignment that is successfully addressed by your global curvature matching approach?\n\nRegarding Method Complexity: Could you justify the use of Ricci curvature over simpler alternatives for capturing second-order geometric information? For instance, have you considered aligning the Hessians of the transfer maps? Please provide an argument or an ablation study showing that the added complexity of computing Ricci curvature is warranted by a significant performance gain over such simpler methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vHLzJ9olJF", "forum": "bKoZoUk0Th", "replyto": "bKoZoUk0Th", "signatures": ["ICLR.cc/2026/Conference/Submission15114/Reviewer_Lito"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15114/Reviewer_Lito"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762139343853, "cdate": 1762139343853, "tmdate": 1762925434516, "mdate": 1762925434516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}