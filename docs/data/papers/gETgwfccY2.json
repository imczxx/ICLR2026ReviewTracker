{"id": "gETgwfccY2", "number": 16601, "cdate": 1758266626467, "mdate": 1758806378378, "content": {"title": "CLEP: Contrastive Language-Pose Pretraining", "abstract": "Aligning natural language descriptions with precise 3D human poses remains a big challenge due to the scarcity of large-scale, semantically rich datasets and effective pose representation techniques. To overcome these limitations, we introduce **CLEP-2M**, a novel large-scale dataset comprising two million high-quality 3D pose-language pairs, substantially expanding the previously released PoseScript dataset through strategic sampling and enhanced annotation procedures. Building upon this dataset, we propose **CLEP**, a contrastive pretraining framework designed for language-pose alignment, which incorporates a spatial multi-scale modeling architecture and cross-scale attention fusion mechanism to capture hierarchical pose semantics. Extensive experimental evaluations on both CLEP-2M and PoseScript demonstrate that our method consistently outperforms existing approaches across a range of downstream tasks, underscoring the strong zero-shot generalization capabilities of CLEP and the efficacy of multi-scale representation learning in improving pose-language alignment.", "tldr": "", "keywords": ["Contrastive Learning", "Human Pose", "Multimodal"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "", "supplementary_material": ""}, "replies": [], "withdrawn": true}