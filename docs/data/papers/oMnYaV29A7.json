{"id": "oMnYaV29A7", "number": 7060, "cdate": 1758006418350, "mdate": 1763357883901, "content": {"title": "TFGNet: Target Face Generation from Low-Quality Images via Textual Guidance", "abstract": "Recent advances indicate that text-guided face generation has attracted considerable attention in the field of computer vision. However, most existing methods assume high-quality (HQ) face inputs, while how to generate HQ and identity-preserving faces from low-quality (LQ) images is still an open problem. In this paper, we propose a novel face generation approach named TFGNet, which generates HQ face images from LQ inputs guided by textual descriptions. Unlike most existing methods that depend on HQ inputs, TFGNet leverages external textual descriptions as semantic guidance to directly generate HQ and identity-preserving faces from degraded images. First, we design a unified framework that integrates a Transformer-based encoder, a codebook mechanism, and multimodal representations extracted from contrastive language-image pretraining (CLIP) model to produce enhanced cross-modal embeddings, which are then decoded by a diffusion model to generate target face images with both high visual fidelity and accurate identity retention. Second, we propose a masked diffusion loss that emphasizes identity-related regions and incorporate it into a dynamically weighted total loss, enabling a balanced trade-off among visual fidelity, semantic coherence, and identity consistency. Third, we build a multimodal dataset comprising LQ face images, HQ targets, and manually annotated textual descriptions to address the scarcity of suitable text-image pairs for this task. Extensive experimental results demonstrate that the proposed TFGNet approach outperforms many state-of-the-art techniques in face generation in terms of both objective metrics and perceptual quality.", "tldr": "", "keywords": ["Multimodal Learning", "Face Generation", "Text-to-Image Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f703e70bfdc6ae6bcec8f7a7b804090dc7fad176.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TFGNet, a novel framework for generating high-quality, identity-preserving faces from a single low-quality image and a textual description. The paper proposes a masked diffusion loss to strike a balanced trade-off between visual quality and identity-consistent semantics. The paper also proposes a dataset for the task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses the important problem of restoring high-fidelity, identifiable faces from low-quality images.\n2. The paper proposes valuable benchmark T2F500."}, "weaknesses": {"value": "1. The paper does not clarify whether the text prompt is used for restoration (filling in lost details lost) or for editing (incorporating new attributes).\n2. What happens if the text prompt contradicts the HQ target's attributes?\n2. There are no ethical considerations mentioned in the paper."}, "questions": {"value": "What happens if the text prompt contradicts the HQ target's attributes?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cu2WUxz9eW", "forum": "oMnYaV29A7", "replyto": "oMnYaV29A7", "signatures": ["ICLR.cc/2026/Conference/Submission7060/Reviewer_wHno"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7060/Reviewer_wHno"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761049369572, "cdate": 1761049369572, "tmdate": 1762919252148, "mdate": 1762919252148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work targets the task of text-guided face image generation with low-quality reference, aiming to combine identity information from the low-quality input with specific descriptions from the text prompt for human face image synthesis. Specifically, the authors employ a codebook-based VQGAN-like model to restore the low-quality input image, then integrate the restored image features with text prompt features into stacked identity embeddings for diffusion-based generation. A masked diffusion loss is further proposed to balance identity fidelity and visual quality, and a curated dataset aligned with this task - pairing low-quality images with corresponding text prompts as inputs - is also proposed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work presents a relatively unexplored task of text-guided face generation from low-quality reference images, conducting an exploration in this specific area. The proposed model demonstrates the capability to produce results with relatively high visual quality that align with the provided text prompts. Furthermore, it achieves superior performance compared to existing baseline methods across the evaluation metrics established by the authors."}, "weaknesses": {"value": "1. The task setting possesses inherent ambiguity - a single low-quality input can correspond to multiple plausible high-fidelity ground truths, which complicates a definitive evaluation and challenges the clear validation of the method's superiority. \n\n2. The proposed pipeline can be essentially decomposed into the straightforward sequential combination of two well-established sub-tasks: face image restoration and text and reference-guided face generation, both of which have been extensively studied. Consequently, the methodological design, which directly cascades two models as mentioned above, lacks integration and novelty. \n\n3. For specific model design, the initial restoration stage closely resembles existing Vector Quantization-based models like VQGAN [1], while the subsequent stacked identity embedding strategy bears strong similarity to the approach introduced in PhotoMaker [2].\n\n\n[1] Esser, Patrick, Robin Rombach, and Bjorn Ommer. \"Taming transformers for high-resolution image synthesis.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n\n[2] Li, Zhen, et al. \"Photomaker: Customizing realistic human photos via stacked id embedding.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024."}, "questions": {"value": "1. Please elaborate on the practical significance and value of the proposed setting in this work, and clearly distinguish its contributions from a straightforward, sequential pipeline that simply combines \"face image restoration\" followed by \"text/reference-based image generation.\"\n\n2. Please discuss the specific distinctions and potential superiority between the proposed restoration model and existing Vector Quantization-based generative models, as well as the advantages of the stacked ID embedding used in the second stage compared to the one proposed in PhotoMaker.\n\n3. Please justify the rationale behind using ArcFace feature similarity between the generated images and the ground-truth images for evaluating model performance in this specific setting, considering the inherent ambiguity of the task.\n\n4. The article mentions the contribution of a multimodal dataset. However, detailed descriptions are lacking. Please introduce this dataset and discuss its specific value or advantages compared to existing Text-to-Image datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PYe18M2vwD", "forum": "oMnYaV29A7", "replyto": "oMnYaV29A7", "signatures": ["ICLR.cc/2026/Conference/Submission7060/Reviewer_5ejV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7060/Reviewer_5ejV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761196499423, "cdate": 1761196499423, "tmdate": 1762919251533, "mdate": 1762919251533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TFGNet, a text-guided face generator that takes a low-quality (LQ) face image and a text description as input to produce a high-quality (HQ), identity-preserving face. The framework integrates a codebook learned through HQ self-reconstruction, a Transformer structure that maps LQ features into code sequences, and CLIP-based multimodal embeddings injected into a diffusion model after constructing stacked ID embeddings together with image encoder information. The framework is trained using a two-stage strategy: first, the discrete codebook is optimized via HQ self-reconstruction with L1 loss, perceptual loss, adversarial loss, and code-level loss; second, the entire model is optimized with an additional masked diffusion loss focusing on identity-salient regions. The authors build two test sets (Face500 and T2F500) and report consistent improvements, particularly on Face Similarity, DINO, CLIP-I, LPIPS, and FID."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Explicit focus on **LQ→HQ** text-guided face generation, a realistic surveillance setting underexplored by current methods which tipically focus on HQ images as input.\n- A brandnew multimodal dataset for face generation is constructed.\n- On both Face500 and T2F500, TFGNet tops Face Sim./DINO/CLIP-I/LPIPS vs many recent baselines.\n- This work also provides ablation experiments to show each component contributes."}, "weaknesses": {"value": "- **W1 typo**: e.g. caption in figure 1 line 27 five not four state-of -the-art methods\n- **W2  motivation or startpoint of this work**: This work highlights the lack of research on text-guided face generation with LQ images and introduces a new challenge along with corresponding datasets. However, an important question arises: can text-guided face generation with LQ inputs be decomposed into two parts—super-resolution (SR) of LQ images followed by text-guided face generation? It is evident that directly generating from LQ images leads to degraded image quality, but the authors do not discuss whether applying state-of-the-art SR methods to LQ faces first, and then processing them with state-of-the-art text-guided face generation methods, would also suffer from significant quality limitations. Furthermore, the proposed architecture seems to be essentially a sequential combination of an image enhancement module (acting as SR of LQ images) and a text-guided face generation module, without demonstrating deeper integration or fusion of information. Both parts rely on structures already established in their respective tasks `[1][2]`. While it is undeniable that generation from LQ images presents unique challenges, existing SR techniques already offer partial solutions to these challenges.\n- **W3 insufficient details**：There remain uncertain details, as neither the main body nor the appendix provides sufficient clarification or proper citations. e.g., the structural details (e.g., HQ/LQ encoder, …), training details (e.g., fine-tuning of LQ encoder, …), text info for image.\n- **W4 novalty**: 1) Regarding the framework, as noted in W2, the Image Enhancement Module and the subsequent components appear to be a concatenation of two tasks—super-resolution (SR) and text-guided face generation. Our perspective on this newly defined task has also been discussed in W2. 2) Both of the main structural components of the framework are directly adopted from state-of-the-art methods in these two respective tasks `[1][2]`. 3) As for the masked diffusion loss, it is highly similar to the Cropping and Segmentation in [2] with binary masks generated by Mask2Former[3].\n[1]Learning Image-Adaptive Codebooks for Class-Agnostic Image Restoration\n[2]Photomaker: Customizing realistic human photos via stacked id embedding.\n[3]Masked-attention mask transformer for universal image segmentation.\n- **W5 unclear application**: It is difficult for me to imagine a scenario where generating a high-quality image from both low-quality one and  text description would be necessary. Could the authors clarify what practical applications the proposed method could be used for?"}, "questions": {"value": "- Q1: A key question is whether this task can be directly decomposed into SR and text-guided face generation. Our considerations can be referred to in W2 of the Weaknesses. We expect the authors to provide experiments and analyses using, for example, a SOTA face SR method combined with PhotoMaker or other relevant approaches.\n- Q2: How significant is the impact of changing the training data on the results (using another subset of FFHQ)?\n- Q3: Could the authors clarify what practical applications the proposed method could be used for?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w91jly0FU6", "forum": "oMnYaV29A7", "replyto": "oMnYaV29A7", "signatures": ["ICLR.cc/2026/Conference/Submission7060/Reviewer_1nqc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7060/Reviewer_1nqc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793985713, "cdate": 1761793985713, "tmdate": 1762919250406, "mdate": 1762919250406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to enhance the ID-preserving face generation based on LQ face images by leveraging the corresponding textual descriptions as external information via two stage training include discrete codebook learning and a Transformer model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Authors have made the great effort to generate descriptive prompts of LQ face images. \n2. The problem setting is new in the era of ID-preserving face image generation."}, "weaknesses": {"value": "1. Manually annotate prompts of each LQ face images can be challenging. It is hard to ensure the accurate descriptions. Testing dataset construction is not clear and size is small.\n\n2. In Figure 2, CLIP image encoder is used to extract ID information. Why authors didn’t choose feature extractor like ArcFace to extract such information? The intuition behind Embedding Fusion Module is under explained. Same for the choice of LQ Encoder.\n\n3. Comparison to some face generation model is missing. E.g., IPA-Adapter, Arc2Face."}, "questions": {"value": "1. In Sec. 3.3, authors mask the region outside the face with random noise. Why not simply detect and crop the face region for each image?\n2. How the method performs on other types of low-resolution images that different to training dataset? E.g., compressed with different distortion and different resolutions.\n3. Impact of base model is not studied. Authors use SDXL as their base model. How the proposed method work with other based model, e.g., SD v1.4 or 1.5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NcLKYLc0rv", "forum": "oMnYaV29A7", "replyto": "oMnYaV29A7", "signatures": ["ICLR.cc/2026/Conference/Submission7060/Reviewer_a4ez"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7060/Reviewer_a4ez"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839703610, "cdate": 1761839703610, "tmdate": 1762919249741, "mdate": 1762919249741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}