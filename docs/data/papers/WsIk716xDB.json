{"id": "WsIk716xDB", "number": 17135, "cdate": 1758272620247, "mdate": 1759897194574, "content": {"title": "\"I know that I don’t know... and I explain why'' Interpretable abstention via counterfactual explanations", "abstract": "Ensuring reliability in human-AI collaboration is crucial for fostering appropriate trust in hybrid decision-making systems, which hinges on performance and transparency but also on understanding the limits of ML methods. Selective classification addresses this need by allowing classifiers to reject uncertain instances and focusing on more confident predictions. However, very few works try to provide interpretable abstention policies for selective classification. In this work, we introduce a novel interpretable-by-design method for selective classification. that leverages the distance between data points and their set of counterfactuals as a measure of uncertainty. By using this distance as a basis for rejection, our method formulates an effective abstention policy while providing contrastive and model-agnostic explanations. Experimental results indicate that our method effectively implements a rejection policy that is explainable by design without affecting performance.", "tldr": "Interpretable-by-design method for selective classification via on distance-based counterfactual explanations", "keywords": ["explainable AI", "selective classification", "interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f2c0cc09d0f302393eda5cb12a5365548d089fba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new framework for selective classification that leverages counterfactual explanations. \nThe main idea is to evaluate the uncertainty of a prediction by the minimum perturbation size required to flip it. \nThat is, the authors propose to use the minimum distance between an input and its counterfactual explanation as a proxy for uncertainty. \nIt enables us to explain why a classifier abstains from providing a prediction for the input. \nThe authors conducted experiments with several existing counterfactual explanation methods and different distance functions.\nExperimental results suggest that the proposed method performed comparably to the baselines across the popular evaluation metrics in selective classification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors tackle a well-motivated problem of the lack of explainability in selective classification. I think it is an intuitive and interesting idea to use the minimum distance between an input and its counterfactual explanation as a proxy for uncertainty."}, "weaknesses": {"value": "1. While the idea of combining selective classification with counterfactual explanation is interesting, the proposed method appears to be a straightforward application of counterfactual explanation to the selective classification framework. Since it lacks technical novelty or theoretical analysis, I am not sure whether its technical contribution is sufficient for ICLR. For example, analyzing theoretical relationships between the conventional and the proposed methods (e.g., a condition under which they coincide) or identifying requirements that counterfactual explanations must satisfy to preserve the quality of selective classification would significantly strengthen the paper. \n1. The presentation of this paper could be improved. Some paragraphs are overly long or include redundant descriptions, making the paper difficult to follow. In Section 4, the authors state, \"From the table, we can see that generally using ILSlatent yields good and fairly consistent correlations with L2 distance, ...\" but it is unclear how this conclusion was drawn from Table 2. Moreover, although various combinations of counterfactual explanation methods and distance functions were tested, Figure 2 is difficult to interpret: it is hard to tell which plot corresponds to which combination, and the selected combinations differ across panels. It would be clearer to present results for a few representative combinations in the main paper and place the complete results in the Appendix."}, "questions": {"value": "1. As one of the distance functions, the authors employed the Wasserstein distance. While I acknowledge it is a distance between probability distributions, how do the authors use it to evaluate the distance between an input point and its counterfactual?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yz1LkEx216", "forum": "WsIk716xDB", "replyto": "WsIk716xDB", "signatures": ["ICLR.cc/2026/Conference/Submission17135/Reviewer_D9An"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17135/Reviewer_D9An"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461755619, "cdate": 1761461755619, "tmdate": 1762927128784, "mdate": 1762927128784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an interpretable reject option based on the distance of the input to its closest counterfactual explanations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Relevant and well-motivated problem, which is grounded in literature\n- Clear contribution with potentially high impact in practice\n- Empirical evaluation is sound"}, "weaknesses": {"value": "- Repository looks a bit \"unorganised\". Please consider cleaning the requirements.txt and removing the To-Do list from the README. Is \"OLD_src\" really needed? Consider describing the structure of the repository, ...\n\nMinor:\n- Line 208 \"Eq. equation 3\" one \"equation\" must go xD\n- References: For some references, a DOI is provided, while for others not. Please be consistent. I recommend always providing a DOI or some other permanent link to the cited publication"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XUoVeEyxf7", "forum": "WsIk716xDB", "replyto": "WsIk716xDB", "signatures": ["ICLR.cc/2026/Conference/Submission17135/Reviewer_WJKH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17135/Reviewer_WJKH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555911652, "cdate": 1761555911652, "tmdate": 1762927128126, "mdate": 1762927128126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Selective Classification via Counterfactual Explanations (SC‑CE), a plug‑in, model‑agnostic framework that uses the distance between an instance and its counterfactuals as a proxy for model confidence. The same counterfactuals are then displayed to the user when an instance is rejected, thereby providing a contrastive, actionable explanation of the abstention decision. The authors evaluate SC‑CE on four tabular datasets (Adult, German, Wisconsin, Two‑Moons) with four black‑box learners (LightGBM, MLP, Random Forest, XGBoost) and several counterfactual generators (DiCE, LoRE, ILS/ILS‑latent). They compare against two state‑of‑the‑art selective classifiers (PlugInRule, PlugInRuleAUC) using non‑rejected accuracy (NA), classification quality (CQ), and rejection quality (RQ). Empirically, SC‑CE achieves NA curves comparable to the baselines while offering interpretable explanations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "## Motivation and Relevance\nAuthors describe the well‑known trust-transparency trade‑off in human‑AI collaboration and identify abstention as a missing piece of interpretability. The introduction cites recent surveys (Mehrotra et al., 2024) that underscore the need for transparent uncertainty signals.\n\n## Model‑agnostic design\nSC‑CE only requires hard predictions from the black‑box learner. No gradient, probability score or internal architecture is needed—this is a strong selling point for real‑world deployment where models may be locked or proprietary.\n\n## Unified use of counterfactuals\nCounterfactuals are traditionally a post‑hoc explanation tool. SC‑CE repurposes them as both confidence and explanation, eliminating a second step. The authors argue that “a short counterfactual distance implies high uncertainty” (Section 3.2.1) and empirically show that the correlation between distance and probability is high (Table 2).\n\n## User‑centric explanation format\nThe paper describes a multimodal output: bar plot of distance vs threshold and the counterfactual itself. This aligns with human‑centric XAI literature (Miller 2019) that stresses contrastive, actionable explanations."}, "weaknesses": {"value": "## Limited novelty\nThe core idea of using counterfactual distance as a confidence proxy is not new. Prior work (Artelt et al., 2023; Singla et al., 2023) has already suggested that “easy” counterfactuals indicate uncertainty. The manuscript does not provide a theoretical analysis or empirical evidence that SC‑CE’s distance measure is superior to existing uncertainty estimates (e.g., probability scores, entropy).\n\n## Narrow baseline set\nOnly two plug‑in baselines (PlugInRule, PlugInRuleAUC) are evaluated. Modern selective classification benchmarks include conformal prediction (Hallberg Szabadváry et al., 2025), learned rejectors (Pugnana & Ruggieri, 2023), and deep‑ensemble uncertainty. Without these, it is unclear whether SC‑CE offers a tangible advantage over state‑of‑the‑art methods.\n\n## No statistical edge\nThe Friedman test reports no significant difference between SC‑CE and the best baseline in NA (p > 0.05). Yet the paper claims “SC‑CE matches or surpasses” without clarifying that the advantage is marginal. A more balanced discussion would be appropriate.\n\n## Missing interpretability validation\nThe central claim is that SC‑CE provides useful explanations. The paper only shows screenshots (Figure 16). No user study, no quantitative counterfactual metric (e.g., plausibility, proximity, sparsity), and no comparison to other explanation methods.\n\n## Computational overhead unreported\nCounterfactual generation (DiCE, LoRE, ILS) can be expensive, especially in high‑dimensional settings. The manuscript does not report runtimes or memory consumption, nor compare them to the lightweight baselines that require only a threshold on probabilities.\n\n## Hyperparameter sensitivity ignored\nSC‑CE depends on several design choices: number of counterfactuals per instance, choice of distance metric, aggregation function (min/mean/max), and threshold calibration (empirical percentile vs Gamma). No ablation study is presented to show how sensitive NA and RQ are to these choices.\n\n## Limited data scope\nAll experiments are on small tabular datasets (≤ 50 k instances, ≤ 30 features). No experiments on high‑dimensional tabular data (≥ 100 features) or on images/text where counterfactual generation is more challenging. Thus generality of the claims is uncertain."}, "questions": {"value": "1. Can you provide a theoretical justification that counterfactual distance is a better uncertainty estimate than existing probability‑based or entropy measures?\n\n2. The literature distinguishes several possibilities for counterfactuals: (i) those that merely approximate the decision boundary (Wachter 2017), (ii) plausible counterfactuals that lie on the data manifold and might be far from the decision boundary (Wielopolski 2024), (iii) highly diverse counterfactuals spanning the manifold (DiCE). How does SC‑CE’s performance (NA, CQ, RQ) vary under these different counterfactual regimes? Please discuss the implications of each regime for both uncertainty estimation and explanation quality. Furthermore, I think it would be interesting to see how SC-CE behaves when counterfactuals are substituted with simply k-nn from opposite class.\n\n3. Have you considered integrating global counterfactual explanations, methods that generate a single change‑vector direction for the whole model (e.g., GLOBE‑CE)? If so, how would such a global approach affect the design of SC‑CE (e.g., threshold calibration, explanation richness) and its interpretability?\n\n3. Have you performed any user study or applied quantitative XAI metrics (faithfulness, stability) to assess the usefulness of the counterfactual explanations? If not, could you plan a small experiment with domain experts (e.g., in finance or healthcare) to validate the claims?\n\n4. What is the average time to generate counterfactuals per instance for each generator? How does this compare to the cost of a simple probability‑threshold rejector? Could you include a runtime table in the appendix or supplementary material?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3ilGUnuxNA", "forum": "WsIk716xDB", "replyto": "WsIk716xDB", "signatures": ["ICLR.cc/2026/Conference/Submission17135/Reviewer_o4Vf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17135/Reviewer_o4Vf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773525438, "cdate": 1761773525438, "tmdate": 1762927127818, "mdate": 1762927127818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to use counterfactual explanation techniques to explain predictions that a selective classifier abstains on. The authors propose to use distance to nearest counterfactual point as a measure of uncertainty, which the selective classifier uses to decide on whether to abstain or not."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Paper is generally well written and clear. Self-contained\n\t- e.g., appreciate explanation of metrics in Section 4\n- Some of the experiments answer questions one might have when reading the paper\n\t- e.g., experiment answering RQ1"}, "weaknesses": {"value": "**Weak problem statement**\n\nThe authors suggest that not having explanations for abstention decisions in selective classifiers is a problem:\n\n> Given that abstention mechanisms directly affect subsequent human workflows, the provision of interpretable rejections is essential: individuals require not only awareness of a model’s uncertainty but also understanding of the underlying reasons for abstention, enabling them to determine whether to accept, contest, or act on the deferred case, without compromising trust in the AI system (lines 45-49).\n\nWhy is it essential that we have interpretable rejections? What gains does it provide? How exactly do explanations help decision-makers determine how to act on a deferred case? How does the lack of explanations compromise trust? There is a lot to unpack here.\n\nI believe the authors will be able to demonstrate this through concrete use cases.\n\n**Analysis and discussion of results**\n\nThere is a hint of this in Figure 3 and the paragraph answering RQ3 (line 422). However, the authors just show that it is \"interpretable\"—without any discussion on what this might mean for the end user.\n\nAlso, the discussion of results in Table 2 (line 341) should reference specific numbers in the table instead of saying:\n> yields good and fairly consistent correlations (line 346)\n\nIn fact, this discussion on the extent to which the distance from the nearest counterfactual maps to uncertainty (predictive probability) is arguably the most important experimental result in the paper; the validity of SC-CE hinges on this. The paper lacks thorough discussion here. The authors just state that:\n\n> Naturally, this heavily depends on the shape of the decision boundary of the classifier, as highlighted by the differences between the different models for the same dataset (lines 348-349).\n\n-  What does this suggest? (e.g., what does this say about the validity of the procedure?)\n-  What procedure should one follow in selecting distance measures? \n-  At what point can we say it is appropriate to use distance as a proxy metric for uncertainty?\n\nOne could imagine just using traditional rejection and explaining those predictions. How might this compare with SC-CE? The authors need to present a way to compare these predictions and explanations, which requires thinking about downstream tasks for explaining rejections.\n\nThe empirical results in Figure 2 are difficult to parse. I would suggest the authors reduce the number of examples they show (put the rest in the appendix) and select salient methods that highlight what they want to discuss."}, "questions": {"value": "- I found it interesting that SC-CE framework allows different distance metrics when aggregating counterfactual points (line 202-212). When might one use a different distance metric when computing $c_d(x)$?\n\n- > Specifically, for each rejected instance, SC-CE provides a multi-modal explanation that combines textual and visual elements\n\nWhy is this a salient point?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lpzeG2Cgg0", "forum": "WsIk716xDB", "replyto": "WsIk716xDB", "signatures": ["ICLR.cc/2026/Conference/Submission17135/Reviewer_Qxv9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17135/Reviewer_Qxv9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762196258913, "cdate": 1762196258913, "tmdate": 1762927127553, "mdate": 1762927127553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}