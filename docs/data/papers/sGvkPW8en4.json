{"id": "sGvkPW8en4", "number": 6340, "cdate": 1757970175751, "mdate": 1759897921064, "content": {"title": "RefineX: Learning to Refine Pre-training Data at Scale from Expert-Guided Programs", "abstract": "The foundational capabilities of large language models (LLMs) are deeply influenced by the quality of their pretraining corpora. However, enhancing data quality at scale remains a significant challenge, primarily due to the trade-off between refinement effectiveness and processing efficiency.\nWhile rule-based filtering remains the dominant paradigm, it typically operates at the document level and lacks the granularity needed to refine specific content within documents.\nInspired by emerging work such as ProX, we propose **RefineX**, a novel framework for large-scale, surgical refinement of pretraining data through programmatic editing tasks. RefineX enables efficient and fine-grained data refinement while reliably preserving the diversity and naturalness of raw text.\nThe core strength of RefineX lies in its ability to distill high-quality, expert-guided end-to-end refinement results into minimal edit-based deletion programs.\nThis high-precision distillation pipeline is used to train an efficient and reliable refine model that can systematically improve every instance in the corpus at scale.\nWe evaluate RefineX across from-scratch pretraining at multiple model scales and find that it consistently outperforms models trained on raw, filtered, or alternatively refined data across diverse downstream tasks. On the 750M model, RefineX yields 2.6\\%-7.2\\% average gains on lighteval tasks, and achieves comparable performance using significantly fewer training tokens.\nFurther analysis shows that RefineX reliably enhances text quality with both high efficiency and precision, outperforming prior approaches such as end-to-end generation and Prox-C. These results position RefineX as a scalable, effective, and reliable solution for optimizing pretraining data in modern LLM pipelines.", "tldr": "", "keywords": ["Large Language Models", "Pretraining Data Refinement"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/661e5ab72f97c4acd53f1cceca3f55b925bb15ad.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "RefineX proposes a scalable, reliable framework for fine-grained pretraining data refinement. It first prompts an expert LLM to produce end-to-end refined text, then extracts only deletion operations via minimum edit distance, mapping them to a minimal API (remove_lines, remove_str, keep_all). A compact refiner model is distilled on ~2M high-confidence program pairs and applied at scale to generate and execute edits efficiently. Across 20B-token pretraining at 350M/750M scales, RefineX consistently outperforms raw, rule-filtered, and ProX baselines on LightEval, achieves similar accuracy with fewer tokens, reduces token overhead, and avoids hallucinations by never inserting new content."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. It’s fast and trustworthy because they rewrite with a big model, then keep only the delete edits, so the final program is tiny, quick to run, and doesn’t add biased or made-up text.\n2. It delivers better results with less data, beating raw/rule-based/ProX baselines at 350M/750M and often matching or topping them with fewer training tokens by cutting fluff and boosting useful signal."}, "weaknesses": {"value": "1. Most building blocks mirror ProX’s program-based refinement (program generation/execution paradigm, chunking, minimal API). The main change is how doc→program supervision is obtained (E2E first, then deletion-only extraction), which is an incremental tweak rather than a substantive algorithmic innovation.\n\n2. The paper does not clarify whether ProX-C/ProX-D were re-trained following Zhou et al. or taken from released models/processed corpora. If the latter, the comparison is weak: RefineX and ProX differ in training data volume, teacher strength, and base model size (ProX reportedly uses fewer data, weaker teachers and bases). This demands major clarification and a controlled, compute-matched reimplementation to ensure fairness.\n\n3. All pretraining experiments are conducted only on RedPajama-V2, limiting evidence for generality across data sources (e.g., CommonCrawl variants, FineWeb/Variants, Wikipedia/Books mixtures, domain-specific crawls).\n\n4. The DataMan-based quality scores are used to argue better refinement, but the paper does not establish correlation between DataMan improvements and downstream pretraining gains. Claims that deletion-only edits are “more reliable” lack stronger empirical tests (e.g., human evals, ablations comparing delete-only vs. mixed ops under matched token budgets, semantic preservation checks)."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LhteJ1Rsrp", "forum": "sGvkPW8en4", "replyto": "sGvkPW8en4", "signatures": ["ICLR.cc/2026/Conference/Submission6340/Reviewer_aUM9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6340/Reviewer_aUM9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632641402, "cdate": 1761632641402, "tmdate": 1762918633025, "mdate": 1762918633025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces REFINEX, a new framework for improving LLM pre-training data quality by refinement. Different from existing pre-training corpus refinement work, It did a two-step distillation by extracting knowledge from a powerful expert LLM by first having it generate high-quality, refined text. Then, it uses a minimal edit distance algorithm to extract simple, deletion-only programs that replicate these refinements. These programs serve as reliable supervision to train a small, efficient \"refine\" model. Extensive experiments show that models pre-trained on REFINEX-processed data consistently outperform those trained on raw, rule-based filtered, or other programmatically-refined datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The two-stage \"refine-then-distill\" approach is a clever solution to create reliable supervision program for data cleaning.\n* Pre-training models from scratch against a wide range of strong baselines provides compelling evidence of the method's effectiveness."}, "weaknesses": {"value": "* The deletion-only constraint, while ensuring reliability, prevents the model from making other potentially valuable corrections like fixing typos or factual errors.\n* The paper would be more sound if more analysis and fair comparison are provided 1) against a distilled small model directly do text refinement. 2) against a LLM-based quality filter with similar inference costs.\n* How to build the RefineX model and how to define the evaluation metrics is the key contribution to this paper. However, not much info is provided in the current draft."}, "questions": {"value": "* How was the final refineX model checkpoint selected? An evaluation metric for the refiner itself (e.g., balancing precision/recall of edits) is a key piece of missing information.\n* The deletion-only approach is safe but limited. What percentage of edits from the expert model (insertions/replacements) were discarded? On the other side, What percentage of potential hallucination may be generated from a distilled small model directly do text refinement. This would help clarify the trade-off being made.\n*  What percentage of documents in the corpus are ultimately edited by REFINEX? Does the same conclusion hold for pre-train corpus of different quality? e.g. if the initial corpus is relatively clean, there is not much refinement needed, if the initial corpus is bad quality, multiple high-cost refinement may be comparable to simply discard the doc. \n*  Also, what is the total inference cost for a long document requiring chunking, compared to a simpler document-level filter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p2DFswTx2g", "forum": "sGvkPW8en4", "replyto": "sGvkPW8en4", "signatures": ["ICLR.cc/2026/Conference/Submission6340/Reviewer_2t4G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6340/Reviewer_2t4G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807516912, "cdate": 1761807516912, "tmdate": 1762918632314, "mdate": 1762918632314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RefineX, an efficient and fine-grained pre-training data refinement framework. The author conducts pre-training from scratch on Redpajama with different model scales (0.35B and 0.75B), verifying the effectiveness of Refine-X compared with rule-based filtering and ProX-C."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Employ a minimum edit distance heuristic to transform end-to-end refined text into several refinement programs, obtaining high-quality SFT data;\n2. Introducing the DataMan quality scorer to analyse refined texts in depth."}, "weaknesses": {"value": "The experiments conducted in this paper are solid. However, I have a major concern regarding the fairness of the comparison to ProX-C. There are two primary sources of variance: (1) ProX-C is fine-tuned from a 0.3B from-scratch pre-trained language model trained on approximately 20B tokens, whereas RefineX is fine-tuned from Qwen-0.6B, an over-trained state-of-the-art language model; and (2) the teacher models used for synthesizing SFT data differ, with ProX-C relying on Llama-70B and RefineX using Qwen-72B. These two differences could introduce substantial performance gaps. A thorough, controlled comparison would be needed to make the claims convincing.\n\nAdditionally, I believe that when the teacher model is exceptionally strong, the resulting ProX-C and RefineX models may perform similarly regardless of program-space design."}, "questions": {"value": "Same to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "L5u9KmR7SB", "forum": "sGvkPW8en4", "replyto": "sGvkPW8en4", "signatures": ["ICLR.cc/2026/Conference/Submission6340/Reviewer_kkCq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6340/Reviewer_kkCq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917517995, "cdate": 1761917517995, "tmdate": 1762918631932, "mdate": 1762918631932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RefineX, an improvement over ProX for program-based refinement of pretraining data. Instead of sampling from P(programs|text1), the paper samples from the latent-variable model P(programs|text2) P(text2|text1). This yields empirical gains because the generative process is more naturally aligned with expert models. The paper uses RedPajama-V2 and compares ProX vs RefineX under various filtering choices to demonstrate consistent overall improvement on downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea is simple and natural, also leads to empirical gains.\n- The experiments consider a bunch of filtering methods to demonstrate consistency which is nice to have. \n- Analysis is given for the two metrics they care about (efficiency and reliability)."}, "weaknesses": {"value": "- The improvements are consistent but somewhat marginal. \n- RefineX has the overhead of sampling text2 ~ P(.|text1) compared to ProX, which compounds the issue of whether this is worth the trouble in huge scales.\n- This is not necessarily against the paper given limited resources, but the considered scales (<1b models, 20b tokens) seem potentially too small to draw strong conclusions. There's a possibility that the small gains here may wash out further with larger models and refined data sizes, and the benefit of small data efficiency is not really an issue in pretraining."}, "questions": {"value": "I'd like to hear if you have thoughts on if the gains will remain at large scales. Even if not, it's good to have them in small scales as well, so no coercion here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O44m7nZTpU", "forum": "sGvkPW8en4", "replyto": "sGvkPW8en4", "signatures": ["ICLR.cc/2026/Conference/Submission6340/Reviewer_seHS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6340/Reviewer_seHS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927780311, "cdate": 1761927780311, "tmdate": 1762918630848, "mdate": 1762918630848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}