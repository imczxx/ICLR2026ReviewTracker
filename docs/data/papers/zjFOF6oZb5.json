{"id": "zjFOF6oZb5", "number": 19442, "cdate": 1758296300538, "mdate": 1759897038728, "content": {"title": "Military Object Detection Using a Fine-Tuned Florence Vision Model", "abstract": "Accurately detecting military vehicles and equipment in real-world environments is a challenging but vital task in modern defense. In this work, we fine-tuned Microsoft’s Florence-2 Large model to recognize a wide range of military assets, including tanks, helicopters, artillery, and ground troops under realistic battlefield conditions. Instead of training on clean or staged images, we built a dataset of over 7,000 images collected from military exercises and surveillance footage. These images included difficult cases like camouflage, partial visibility, and low lighting. The objects were annotated using tools such as CVAT and Roboflow, allowing us to maintain consistency across all categories. To improve performance on underrepresented classes, we used data augmentation and class-aware sampling. Our model achieved strong results, with a precision of 98.95%, recall of 99.33%, and an average IoU of 85.29%. These outcomes show that the model performs well even in messy, real-world conditions. This work highlights the potential of Florence-2 Large for practical defense applications like drone surveillance, battlefield monitoring, and automated threat detection.", "tldr": "We fine-tune Florence-2 Large using LoRA on a curated real-world dataset, achieving over 98% detection accuracy and 85% IoU, demonstrating robust performance under challenging conditions such as camouflage, low-light, and occlusion.", "keywords": ["Military Object Detection", "Florence-2", "LoRA Fine-Tuning", "Vision-Language Models", "Real-World Battlefield Dataset", "Camouflage Detection", "Data Augmentation", "Class-Aware Sampling", "Object Detection Metrics", "Precision and Recall", "Intersection over Union (IoU)", "Automated Threat Detection", "Defense and Security Applications", "YOLOv5 Benchmark Comparison", "Computer Vision for Surveillance"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4d16ddbdf6fbddbf43b21eeceae95f116158a1c8.pdf", "supplementary_material": "/attachment/06b58c860c1bd31da03aff86eabd008480344ab3.zip"}, "replies": [{"content": {"summary": {"value": "This paper creates a new military dataset with 7000 cleaned images, that includes military vehicles and personnel. The authors propose fine-tuning Microsoft’s Florence-2 Large model on this dataset and show that the fine-tuned model outperforms a fine-tuned YOLOv5 model. The authors employ class-component sampling and simple data augmentation techniques to overcome the class imbalance problem."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Overall, the contribution of 7000 cleaned images is good. Furthermore, the analysis of the Florence-2 model indicates that it can achieve very high mAP@0.5 on this dataset. At this IoU threshold, it is clear that the fine-tuned Florence-2 model outperforms many YOLOv5 models by a reasonable margin.\n\nThe quantitative analysis and metrics used to compare the Florence and YOLOv5 models are sound (although not sufficient) and clearly presented."}, "weaknesses": {"value": "1. Overall my main issue is the writing style of the paper. There are language tone issues throughout the paper that seriously detract from its credibility. A few examples:\n\nline 040-041: \"Most datasets available online show tanks and jets in display conditions,\nclean, centered, and fully visible. But real war doesn’t happen in exhibition grounds...\"\n\nline 054-056: \"Training such a model was no easy task. Camouflage, shadows, damaged vehicles, and background\nclutter made it tough, but that was the point. We wanted the model to be tough too...\"\n\nline 065: \"we went the extra mile to...\"\n\nline 120-121: \"Quality Check: We tossed out any photographs that had been remarkably blurry or distorted – the\ntype even someone would wear to make sense of...\"\n\nline 291: \"From the loss plots, we ought to honestly have a look at how the version...\"\n\n####################################################################################################\nDataset contribution:\n\n2. The dataset described in this paper includes 7000 cleaned and annotated images. This has the potential to be a strong contribution, but currently there is not enough detail provided to have a clear understanding of the contribution of this dataset. Currently the reader does not have enough detail to ascertain how easy or hard this dataset is, and whether a model that performs well on it would be suitable for their own tasks. The description of the dataset would benefit with examples. \n\n2a. For example on line 120-121 the authors state: \"We tossed out any photographs that had been remarkably blurry or distorted – the type even someone would wear to make sense of.\" Visualisation of these types of images would be helpful. \n\n2b. On line 78-80 the authors state: \"Several images turned out to be too blurry or unclear, in fact, some were hard to recognize even for the human eye. Acknowledging these limitations and to stay true to quality standards, we applied a human baseline filter and truncated the dataset accordingly.\" This is reasonable. Can we have a visualisation of the types of images that are and aren't included\n\nThe SAM [1] paper (https://arxiv.org/pdf/2304.02643) has good example of exact annotation strategy for reference\n\n3. On line 141 the authors state: \"To further balance the dataset, we augmented the underrepresented categories using simple data augmentation techniques such as random cropping, flipping, and rotation...\" More discussion is required here. By how much were the images upsampled? What was the distribution of the classes after additional augmentation and class-component sampling? More discussion is needed around augmentation, especially for dealing with underrepresented categories. Furthermore, the YOLO architectures are quite well established. What repository was used as no citation is provided? More discussion is required here as these repositories have lots of training parameters, including mosaic training by default. Was mosaic training used in addition to what was reported? Did training the Florence model receive the exact same data augmentation during training?\n\n####################################################################################################\n\nModel analysis:\n\n4. In order to conclude that Florence is a suitable model, more quantitative analysis is required. Florence vs YOLOv5 is not a sufficient benchmark, as YOLOv5 is now five years old, and SMCA-YOLOv5 is at least three years old. YOLOv12 now exists. Comparisons to more modern architectures need to be made, including non-YOLO architectures. This paper makes no comparisons to other transformer architectures, for instance. \n\n4a. On line 044-045 the authors state: \"After comparing several leading models, we selected Microsoft’s Florence-2 for its powerful combination of image understanding and language processing...\" If there was preliminary round of experimentation that lead to selection of Florence, with either quantitative or qualitative analysis, this should be included. Which other models were shortlisted? Maybe we don't need to compare additional fine-tuned models if there is sound motivation to discard them after an initial round of evaluation.\n\n5. No citations are provided for the other YOLO architectures.\n\n6. There is no indication under what condition Florence performs better than YOLO. Only the mAP metric at threshold 0.5 is considered, what about 0.5:0.95? This is a standard metric to evaluate under. COCO metrics could be easily incorporated into the quantitative analysis to investigate performance at different instance sizes. We need qualitative analysis of the performance of Florence compared to different models. Success and failure cases should be visualised, including examples where Florence performed well and another model did not, and vice-versa. \n\n7. There is no attempt to explain why Florence performs better than other architectures. Is it because of transformer architecture? Is it because it is a vision-language model? Is there a more nuanced explanation? Without comparisons to more types of architectures it is difficult to properly explain why Florence is well-suited to the task. The paper would benefit from qualitative analysis of what features the fine-tuned Florence model is paying attention to compared to other models.\n\n####################################################################################################\nA couple of errors I noticed that did not impact my opinion of the paper:\n\nline 121: \"the type even someone would WEAR to make sense of...\"\n\nline 321: \"We additionally tracked how LENGTHY each epoch took\"\n\n####################################################################################################\nReferences:\n\n[1] A. Kirillov et al., ‘Segment Anything’, in IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, 2023, pp. 3992–4003."}, "questions": {"value": "What kind of images were are were not included in the dataset? Provide visual examples\n\nProvide more detail about the class-component sampling and augmentation. Was mosaic training used? Did all YOLO architectures and Florence receive exact same training-time image augmentation? What was the distribution of classes before and after balancing strategies?\n\nWas the 20% train/validation split random or in any way class-aware?\n\nWhy were only Florence-2 and YOLOv5 architectures chosen? Are better models available? Under what specific conditions is Florence better than YOLOv5 and vice-versa? Are there any general patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8Qe3tpk8iP", "forum": "zjFOF6oZb5", "replyto": "zjFOF6oZb5", "signatures": ["ICLR.cc/2026/Conference/Submission19442/Reviewer_GBpc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19442/Reviewer_GBpc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761074344306, "cdate": 1761074344306, "tmdate": 1762931363206, "mdate": 1762931363206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper fine-tunes Florence-2 for detecting military objects in real-world conditions. Using a self-curated dataset of about 7,000 battlefield simulations and surveillance images, the model achieves strong results and outperforms YOLOv5 variants, showing potential for defense applications such as automated surveillance and threat detection."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The work addresses a practical and difficult real-world setting, focusing on realistic, non-staged military imagery that is underexplored in public benchmarks.\n\n2. The dataset construction process is carefully documented, including annotation procedures and class balancing through augmentation."}, "weaknesses": {"value": "1. The paper lacks sufficient methodological novelty, the main contribution is dataset curation and fine-tuning, rather than architectural or algorithmic innovation.\n\n2. The dataset collection and annotation process rely on online and potentially unverified sources, raising reproducibility and ethical concerns about data provenance.\n\n3. The evaluation focuses heavily on single-dataset metrics; there is no test on unseen or cross-domain datasets to validate generalization.\n\n4. Many sections use general explanations and do not provide implementation specifics (e.g., optimizer type, compute resources, data license), limiting reproducibility.\n\n5. This paper lacks analysis and interpretation of relevant work and fields.\n\n6. I believe the writing quality and figure quality of this paper do not meet the standards of ICLR."}, "questions": {"value": "Same as weaknesses section."}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "This work focuses on military task, and I am not sure whether the application (positive, negative?) and data usage of this paper requires more discussion about ethics."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RNWtMQABdF", "forum": "zjFOF6oZb5", "replyto": "zjFOF6oZb5", "signatures": ["ICLR.cc/2026/Conference/Submission19442/Reviewer_9iLB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19442/Reviewer_9iLB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761226546986, "cdate": 1761226546986, "tmdate": 1762931362854, "mdate": 1762931362854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes using a VLM-based Foundation model (Florence-2) to fine-tune for the object detection task. The paper discusses the lack of variability in the current datasets used to train and evaluate object detection tasks in the military context. The main contribution of the paper is towards building a new dataset, which consists of 8 major classes consisting of unique 7000 images, for the purpose of training an Object detection tasks"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strength:\n\n1. The paper rightly focuses/identifies the shortcomings from a data point of view in the existing evaluation/training paradigm for military object detection.\n2. The paper attempts to collect real images from real video footage and real war-field images and builds on that to capture messy/varied and real-looking images for training the OD task\n3. Leverages on the tools such as CVAT and Roboflow for robust image labeling and creating a trainable set of ~7K images"}, "weaknesses": {"value": "Weakness:\n\n1. There is no technical/architectural novelty presented/discussed in this work, which suggests improving the current architecture of the existing models to better leverage the new dataset for OD.\n2. Paper talks about data normalization, but fails to discuss in detail/show scenarios related to: How does the model deal with poor/low-lighting scenarios? And what are the improvements seen in that specific scenario\n3. There is no serious discussion/ablation in the work to prove how the variability in the dataset, captured from different sources, are \"taken care of\" while training the model?\n4. What kind of normalization? What are other ways explored, and do we have related ablation experiments showing the efficacy of the approach selected in this paper?\n5. Model Architecture Section only talks about the Training Model architecture of the Florence-2 model, nothing is mentioned about what trainable module introduced by this work to train for an OD task\n6. No information / and related ablation for truncated object classes and evaluation/improvement on those object instances only?\n7. No Information/and related ablation for bounding boxes based on the size of the BB, by categorizing them as small/medium/large and estimating AP based on the that."}, "questions": {"value": "Please see the Weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "fjZT28zpoH", "forum": "zjFOF6oZb5", "replyto": "zjFOF6oZb5", "signatures": ["ICLR.cc/2026/Conference/Submission19442/Reviewer_5B5S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19442/Reviewer_5B5S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672379103, "cdate": 1761672379103, "tmdate": 1762931362472, "mdate": 1762931362472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work creates a militarily object detection dataset replicating real battlefield conditions, including over 7,000 images sourced from exercises and surveillance footage. The Florence-2 large vision-language model was fine-tuned on this dataset and demonstrated better object detection performance compared to YOLOv5 series models in comparative experiments."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors collected over 7,000 real-world battlefield images from sources like military exercises and surveillance footage, instead of using existing standard datasets captured in ideal environments. These images include complex scenarios such as camouflage, partial occlusion, low lighting, and blur, providing a valuable data resource for military object detection research in the future."}, "weaknesses": {"value": "The shortcomings of this paper are quite apparent, as listed below:\n1.This paper is weak in motivation description. Regarding the dataset, the motivation for creating it is only briefly mentioned with a few qualitative terms. Readers cannot understand the following issues: What are the specific distinctions between this dataset and those used in prior work? What are the concrete difficulties this new dataset adds to this task? Why the Florence-2 model was specifically chosen for this more challenging task and where does its suitability lie? The deficiency in motivation description makes this work appear more like a simple model application report rather than a deep methodological study.\n2.The paper demonstrates extremely weak innovation. The core contribution in model design is merely fine-tuning the pre-existing Florence-2 model. Firstly, highlighting \"fine-tuned\" as a key methodological aspect in the title is arguably inappropriate. Secondly, beyond the dataset, it is difficult to identify any other applied valuable innovations in this work.\n3.The workload appears insufficient. The model design is too simple, which directly leads to a lack of substantive content in the Experimental section. For instance, sections 4.3 and 4.4 read more like routine recordings of the training process rather than providing in-depth analysis linking these observations to generalization capabilities or architectural properties.\n4.The comparative experiments are inadequate. The benchmarks used are outdated, as the selected models for comparison are all YOLOv5 and its variants. Furthermore, the comparison experiment only uses mAP@0.5 to measure the result, which appears to be too simple.\n5.The paper contains numerous noticeable formatting issues, primarily related to citation formatting.\n6.The Conclusion section appears unfinished, lacking a detailed discussion of its deficiency and potential further improvement."}, "questions": {"value": "It is recommended that the authors refine the motivation section to improve overall narrative coherence. Additionally, incorporating comparisons with more recent object detection benchmarks would make the experiment results more convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AdzNDJ1zT3", "forum": "zjFOF6oZb5", "replyto": "zjFOF6oZb5", "signatures": ["ICLR.cc/2026/Conference/Submission19442/Reviewer_Ndrj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19442/Reviewer_Ndrj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802707806, "cdate": 1761802707806, "tmdate": 1762931361937, "mdate": 1762931361937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}