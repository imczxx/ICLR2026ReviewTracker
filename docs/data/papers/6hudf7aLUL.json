{"id": "6hudf7aLUL", "number": 2812, "cdate": 1757257496663, "mdate": 1759898125822, "content": {"title": "Conceptrol: Concept Control of Zero-shot Personalized Image Generation", "abstract": "Personalized image generation with text-to-image diffusion models generates unseen images based on reference image content. Zero-shot adapter methods such as IP-Adapter and OminiControl are especially interesting because they do not require test-time fine-tuning.  However, they struggle to balance preserving personalized content and adherence to the text prompt. We identify a critical design flaw resulting in this performance gap: current adapters inadequately integrate reference images with the textual descriptions. The generated images, therefore, tend to replicate the reference or misunderstand the personalized target. Yet the base text-to-image has strong conceptual understanding capabilities that can be leveraged. We propose Conceptrol, a simple yet effective framework that enhances zero-shot adapters without adding computational overhead. Conceptrol constrains the attention of visual specification with a textual concept mask that improves subject-driven generation capabilities. It achieves as much as 89\\% improvement on personalization benchmarks over the vanilla IP-Adapter and can even outperform fine-tuning approaches such as Dreambooth LoRA.", "tldr": "Conceptrol is a free lunch that elicits the personalized ability of zero-shot adapter by transforming image condition to visual specification contrained by textual concept, even outperforming fine-tuning methods.", "keywords": ["text-to-image generation", "diffusion model", "subject-driven generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c99bb2e940ddfa491c46a550d4ddf2340645337a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the personalization task in the field of image generation. The authors analyzed the attention mechanism with image injection, and proposed a method called conceptrol to improve the performance of personalization. The experiments showed that the proposed method can achieve better performance compared to baseline methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The presentation of figures is great and easy to understand.\n- The evaluation is based on both UNet-based models and DiT-based models.\n- The math notations in this paper are self-contained and well-defined.\n- The finding that the authors claim in Sec 3.1 is insightful."}, "weaknesses": {"value": "- I have to say that the key observations that the authors claim (Line 70-78) were found by previous research works and somehow became common sense in the image generation field. Could you bring deeper insights about these?\n- Have you considered conducting a detailed evaluation of the precision of textual concept masks?\n- I have to say that the current method, including extracting masks, modifying attention, and timestep warmup, lacks enough novelty. It can be considered a simple combination of existing techniques.\n- The authors used an incorrect citation format in the appendices."}, "questions": {"value": "- How many samples did you use to obtain the results in Fig. 5?\n- Can you show some examples of the claim \"Visual specifications can be transferred within regions of high attention score\"?\n\n(Please also see the weaknesses section for questions)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CSUEj92YH0", "forum": "6hudf7aLUL", "replyto": "6hudf7aLUL", "signatures": ["ICLR.cc/2026/Conference/Submission2812/Reviewer_LHKm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2812/Reviewer_LHKm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566639181, "cdate": 1761566639181, "tmdate": 1762916389147, "mdate": 1762916389147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Conceptrol, a plug-and-play zero-shot framework for personalized image generation with text-to-image diffusion models. Unlike fine-tuning-based methods such as DreamBooth or LoRA, Conceptrol enhances zero-shot adapters (e.g., IP-Adapter, OmniControl) by constraining visual attention through a textual concept mask. This aims to better balance content preservation and adherence to text prompts. The approach requires no additional computation or training, yet shows up to 89% improvement over IP-Adapter on personalization benchmarks and even surpasses fine-tuned methods in some cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation and intuition: the paper convincingly explains why the proposed concept-mask mechanism should work. The intuition connecting attention alignment and subject-driven generation is well presented and easy to follow.\n- Simplicity and generality: the approach is lightweight, requires no training, and can be easily integrated into existing diffusion pipelines.\n- Significant improvement based on quantitative analysis: the method outperforms both adapters and even training-dependent approaches like DreamBooth-LoRA."}, "weaknesses": {"value": "- Runtime and efficiency claims\n\nAlthough Conceptrol is presented as zero-shot, it may internally require multiple inference passes (e.g., text-only and text+image). The paper claims “no computational overhead”, but this is not empirically substantiated.\n\n- Inconsistent evaluation results\n\nQuantitative concept preservation metrics drop substantially (up to 50–80%) relative to IP-Adapter on SD and SDXL, yet the user study reports only a small degradation. This inconsistency is not discussed and undermines confidence in the evaluation.\n\n- Comparison completeness\n\nUser studies are conducted only against adapter-based baselines, while fine-tuned methods such as DreamBooth and LoRA are omitted. This limits the interpretability of the results, especially given the observed divergence between quantitative and perceptual evaluations.\n\n- Qualitative analysis placement and fairness\n\nThe qualitative analysis is largely moved to the appendix, leaving the main paper focused on quantitative discussion. Key qualitative comparisons should appear in the main text.\nMoreover, the IP-Adapter results shown are unexpectedly poor compared to the original paper, where it produced coherent images rather than simply copying the reference. This discrepancy raises concerns about possible cherry-picking or unfair settings.\n\n- Writing \n\nThe introduction and methodology are overly long and repetitive. The discussion of “block 18” in Flux also overlooks relevant prior work (e.g., LoRAShop)."}, "questions": {"value": "- Inference efficiency\n\nPlease report the exact inference time per image and compare it with IP-Adapter and LoRA-based methods to clarify whether the “no computational overhead” claim holds in practice.\n\n- Evaluation discrepancy\n\nWhy do the quantitative and user study evaluations diverge so strongly?\n\nAre the quantitative metrics (e.g., concept preservation) truly aligned with human perception?\n\n- Comparison baselines\n\nCould you include fine-tuned baselines (DreamBooth, LoRA) in user study results?\n\nWere all visualizations and metrics computed under identical sampling parameters (seed, steps, CFG scale)?\n\n- Teaser figure clarity\n\nThe dashed lines and columns in the teaser figure are unclear.\n\nWhat exactly is used as input in each column — only the book, only the monument, or both?\n\nPlease annotate or relabel the figure for clarity.\n\n- Qualitative analysis\n\nPlease move key qualitative examples to the main text.\n\nFor each image prompt, it would be informative to show multiple generations with different textual prompts to demonstrate stability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MCXumkxUeM", "forum": "6hudf7aLUL", "replyto": "6hudf7aLUL", "signatures": ["ICLR.cc/2026/Conference/Submission2812/Reviewer_fRKq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2812/Reviewer_fRKq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923688963, "cdate": 1761923688963, "tmdate": 1762916389022, "mdate": 1762916389022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Conceptrol, a training-free, inference-time control method that improves zero-shot personalized image generation with diffusion adapters (e.g., IP-Adapter for U-Net, OminiControl for DiT/FLUX). The key idea is to derive a textual concept mask from specific attention blocks in the base model during generation and use it to modulate the adapter’s image-conditioned attention, constraining the visual specification to the intended region of the textual concept. The method requires no fine-tuning, no auxiliary segmentation models, and negligible computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1 Clear insight and elegant solution: converting textual attention into a concept mask and using it to spatially modulate image conditioning is simple, principled, and practical.\n\n2 Training-free and model-agnostic: works with both U-Net and DiT (FLUX) stacks, requires no extra models, and adds negligible overhead.\n\n3 Strong empirical improvements: large gains on DreamBench++ for both concept preservation × prompt following; competitive or better than fine-tuning methods while remaining zero-shot."}, "weaknesses": {"value": "1 Early-step reliability: The warm-up mitigates unreliable early attention, but the method’s sensitivity to the warm-up ratio and inference schedule is dataset/model dependent; more systematic guidance would help.\n\n2 Segmentation-free but not fully layout-aware: While the approach avoids external masks, it still relies on the base model’s attention being spatially meaningful. Complex layouts or heavily compositional prompts may yield suboptimal masks."}, "questions": {"value": "1 Robustness: How sensitive is the method to mis-specified concepts (e.g., “cat” vs. “kitten”) or ambiguous/nested concepts (“red leather book”)? Can you back off to multiple candidate spans?\n\n2 Block selection: Can you propose a general diagnostic to auto-select concept-specific blocks at inference (e.g., by measuring attention-mask sparsity/contrast vs. running time)? How does performance change if the chosen block is perturbed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tMiGzwh3Wo", "forum": "6hudf7aLUL", "replyto": "6hudf7aLUL", "signatures": ["ICLR.cc/2026/Conference/Submission2812/Reviewer_qD1L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2812/Reviewer_qD1L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969299247, "cdate": 1761969299247, "tmdate": 1762916388871, "mdate": 1762916388871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Conceptrol, a training-free method to improve zero-shot personalized image generation. It uses a textual concept mask, extracted from specific attention blocks of diffusion models, to guide reference-image attention and better align identity preservation with prompt adherence. The method applies to IP-Adapter and OminiControl without retraining and shows consistent gains on DreamBench++."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation is clear and relevant to zero-shot personalization. \n\n- The approach is simple, efficient, and architecture-agnostic. \n\n- Empirical results demonstrate strong improvements, sometimes surpassing fine-tuned baselines, with convincing qualitative examples and human studies. The paper is well-written and experimentally thorough."}, "weaknesses": {"value": "- The conceptual novelty is limited—attention masking and semantic control have been explored before. \n\n- The method mainly refines existing adapters rather than introducing new principles.\n\n-  Evaluation focuses on DreamBench++ without diverse or real-world tests, and generalization across models is not well analyzed. \n\n- The theoretical explanation of why the mask works remains shallow."}, "questions": {"value": "1. Can the authors clearly differentiate Conceptrol from prior works on attention masking and diffusion control? What is the key conceptual advance beyond identifying “concept-specific blocks”?\n2. How does the method behave with ambiguous or abstract concepts (“freedom,” “dreamlike style”) where the target region is diffuse or overlaps with the background? Are there cases where the concept mask misaligns or suppresses important context?\n3. Can Conceptrol handle prompts with multiple distinct subjects (e.g., “a cat sitting on a red sofa”) when more than one concept is associated with reference images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OvGemQID5i", "forum": "6hudf7aLUL", "replyto": "6hudf7aLUL", "signatures": ["ICLR.cc/2026/Conference/Submission2812/Reviewer_SzLs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2812/Reviewer_SzLs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999853630, "cdate": 1761999853630, "tmdate": 1762916388654, "mdate": 1762916388654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}