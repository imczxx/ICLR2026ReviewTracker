{"id": "KPw0kG7WIl", "number": 22310, "cdate": 1758329488776, "mdate": 1759896873128, "content": {"title": "Mode Connectivity in Unlearning: A Loss Landscape Analysis of Machine Unlearning", "abstract": "Machine Unlearning aims to remove undesired information from trained models without requiring full retraining from scratch. \nDespite recent advancements, their underlying loss landscapes and optimization dynamics received less attention. \nIn this paper, we investigate and analyze machine unlearning through the lens of mode connectivity--the phenomenon where independently trained models can be connected by smooth low-loss paths in the parameter space. \nWe define and study mode connectivity in unlearning across a range of overlooked conditions, including connections between different unlearning methods, models trained with and without curriculum learning, and models optimized with first-order and second-order techniques. \nOur findings show distinct patterns of fluctuation of different evaluation metrics along the curve, as well as the mechanistic (dis)similarity between unlearning methods. \nTo the best of our knowledge, this is the first study on mode connectivity in the context of machine unlearning.", "tldr": "", "keywords": ["Machine Unlearning", "Mode Connectivity", "Loss Landscape Analysis", "Generalization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/594425664eb5678bb7564d387f4cb5bdd675049d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper formalizes mode connectivity in unlearning. It contrasts MCU with standard mode connectivity, since MCU jointly constrains two loss landscapes and derives both endpoints from the same pretrained model. Experiments across datasets, unlearning methods, and training paradigms show MCU often emerges, with prevalence influenced by method choise, taks complexty, forget-set size, curriculum learning, and second-order optijmization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear MCU definition with retain/forget inequalities and curve parametrization.\n- Systematic study of training dynamics, extending from mode-connectivity evaluations.\n- MCU-guided ensembling yields smoother landscapes and improved robustness to relearning."}, "weaknesses": {"value": "- Many findings are illustrated in the appendix but not aggregated into compact quantitative summaries in the main text.\n- Quadratic MCU often fails to optimize for certain methods, like BT, with limited diagnosis."}, "questions": {"value": "- How sensitive are MCU-ensemble gains to the number/placement of sampled points along the path and to the averaging scheme?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "TJIsQffJMZ", "forum": "KPw0kG7WIl", "replyto": "KPw0kG7WIl", "signatures": ["ICLR.cc/2026/Conference/Submission22310/Reviewer_oRJb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22310/Reviewer_oRJb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761070858688, "cdate": 1761070858688, "tmdate": 1762942163368, "mdate": 1762942163368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies mode connectivity and its implications on Unlearning. The paper starts by extending the definition of mode connectivity for unlearning. The paper analyzes the loss landscape of machine unlearning and properties of different machine unlearning methods based on mode connectivity, such as generalization, robustness to attacks. The paper also analyzes similarity between unlearning methods and the unlearning difficulty of the task itself."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper extends mode connectivity under the lens of machine unlearning. The paper provides a novel extension to the mode connectivity literature extending its basic definition to the setting of unlearning. The paper also monitors additional parameters, which affect the training dynamics, studies around these parameters were absent in previous works.\n\nThis work continues to provide a substantial list of experimental results that study a lot of important questions in the field of unlearning. The results provide empirical evidence for the robustness against attacks of different solutions found by unlearning algorithms. Interestingly the paper also provides a criterium through mode connectivity for unlearning of the unlearning difficulty."}, "weaknesses": {"value": "The papers weaknesses for me lie in the extension of the notion of mode connectivity from simple minimization (model training) to machine unlearning.\n\nThe maximization of the loss on the forget set is an arbitrary solution that has been applied to emulate a model that has never seen the samples in the forget set. Therefore while for the minimization problem the condition for what proper mode connectivity is, is clear in the setting of unlearning it is not. In fact there is a recent surge of works that argue against loss maximization on the forget set.\n\nThis also brings me to my next point how does the MCU manifold guarantee generalization as stated in 6.2? In the simpler MC manifold the arguement was simpler and does not extend here directly due to the additional maximization condition on the error. For example consider removing a forget set that is representative of the true distribution (even though we don't know a priori obviously) your imposed condition would in fact guarantee poor generalization under some forget sets."}, "questions": {"value": "My questions revolve around the weaknesses. \n\n1. Would it not make more sense to study the connectivity between the modes of unlearned methods and an oracle model, that has never seen the forget? Would it make sense to quantify the surplus of the loss of the path $\\phi(t)$ in equation (2) that is needed to connect the two modes?\n\n2. Why should we do maximization on the forget, as I said before this choice seems arbitrary to me. Would it make sense to instead measure KL divergence [1] between the model and a proxy of the unlearned model, or KL between the forget set and a validation set?\n\n[1] Attribute-to-Delete: Machine Unlearning via Datamodel Matching Kristian Georgiev, Roy Rinberg, Sung Min Park, Shivam Garg, Andrew Ilyas, Aleksander Madry, Seth Neel"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fkmJ5ZQN1P", "forum": "KPw0kG7WIl", "replyto": "KPw0kG7WIl", "signatures": ["ICLR.cc/2026/Conference/Submission22310/Reviewer_ksW1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22310/Reviewer_ksW1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761651671961, "cdate": 1761651671961, "tmdate": 1762942163172, "mdate": 1762942163172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors examine the mode connectivity of various machine unlearning methods. That is, are there simple, low-loss paths between different models that have undergone some machine unlearning procedure? They look for patterns in the mode connectivity behavior of different unlearning methods (e.g. do the loss landscapes of second order unlearning methods behave similarly)"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea that smoother loss landscapes could indicate better unlearning is intuitive and worth exploring. I appreciate that the authors tried *a lot* of unlearning methods\n\nThe idea of ensembling along a mode connectivity path and understanding how the spikiness of the loss landscape relates to the difficulty of the unlearning task both seem like promising directions."}, "weaknesses": {"value": "- The takeaway from this paper seems to be \"this is really messy and it's hard to say what's going on.\" To be fair, it may be worth knowing that it's messy, but this paper itself is *also* quite messy so it's hard to know what to think.\n- Given the messiness of the results, I don't know that I trust the generalized claims about e.g. second-order methods. It seems like sometimes you can find a nice mode connectivity path but, if you can't, it could be the method, it could be the dataset, it could be the way you search for the path, it could be the random seed, it could be a bug...\n- In general, I tried to judge the paper on its merits and not get hung up on this, but there are *a lot* of spelling mistakes and grammatical issues throughout. Please proofread! Or have an LLM do it!\n\nSuggestions:\n- Figure 1 seems to imply that the loss landscape is quadratic, which is not what I think you're trying to say. May be better without the \"landscape\" in the figure?"}, "questions": {"value": "I understand that the figures (e.g. figure 5) are supposed to be showing the mode connectivity on some trajectory between models, but...I don't think I actually understand what is happening here. Can you explain this figure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m1jjrrNCUt", "forum": "KPw0kG7WIl", "replyto": "KPw0kG7WIl", "signatures": ["ICLR.cc/2026/Conference/Submission22310/Reviewer_DXr2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22310/Reviewer_DXr2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793523569, "cdate": 1761793523569, "tmdate": 1762942162880, "mdate": 1762942162880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies mode connectivity (the phenomenon where different parameters can in some cases define a continuous path of well-performing models) in the context of machine unlearning. The paper first alters the definition of mode connectivity, from only requiring a path of parameters that achieve low loss to requiring a parameters that achieve low loss on the retain set, and high loss on the target/forget set. With this definition in hand, the paper investigates mode connectivity between different unlearning algorithms on a variety of datasets, and reports its findings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The application of mode connectivity analysis to unlearning is novel to my knowledge\n- The authors explain the details of their procedure very well - as a reader it's very clear which experiments are being done and why\n- The experiments are thorough, and completed on a variety of unlearning datasets which makes the results more generalizable"}, "weaknesses": {"value": "The takeaways/high-level message from the paper are not entirely clear, for example:\n\n- In RQ1, the answer to most of the subquestions seems to be \"it depends on the dataset and the exact setup,\" which is fine in some cases, but the paper should try to form an explanation of why the results differ across different setups/what about the setup makes conclusions differ\n- The paper claims (in RQ2) that MCU is a useful tool for studying unlearning, but most of the results in the corresponding section (S6) are reproductions of known results/phenomena in unlearning, and not discoveries of new behavior - more importantly, it's unclear why MCU is a natural/right way to discover this behavior. \n- Section 6 in general looks like the beginning of an interesting investigation but the results are not fleshed out enough to stand on their own, in my opinion."}, "questions": {"value": "Setup:\n- When computing a quadratic path for MCU, is the loss on the forget set taken into account too, or just the retain set/test set? \n- Could this choice have any bearing on the result?\n- Are there other mode connectivity algorithms, and do you expect the results to vary much as you change the type of mode connectivity?\n\nExperiments:\n- Did you try this analysis for \"ground-truth\" unlearning (i.e., just training on the retain set and not the forget set)?\n- Is there mode connectivity between this and the other methods studied?\n- Does mode connectivity to \"ground-truth unlearning\" indicate something about how robust/reliable a method is?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oJxcnHZ2gu", "forum": "KPw0kG7WIl", "replyto": "KPw0kG7WIl", "signatures": ["ICLR.cc/2026/Conference/Submission22310/Reviewer_LD6j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22310/Reviewer_LD6j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762205078947, "cdate": 1762205078947, "tmdate": 1762942162089, "mdate": 1762942162089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}