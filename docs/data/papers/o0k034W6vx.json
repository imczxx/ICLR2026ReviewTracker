{"id": "o0k034W6vx", "number": 20614, "cdate": 1758308225879, "mdate": 1759896967990, "content": {"title": "GRPO is Secretly a Process Reward Model", "abstract": "We prove theoretically that the GRPO RL algorithm induces a non-trivial process reward model (PRM), under certain assumptions regarding within-group overlap of token sequences across completions. We then show empirically that these assumptions are met under real-world conditions: GRPO does in fact induce a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a flaw in the GRPO objective: non-uniformly distributed process steps hinder both exploration and exploitation (under different conditions). We propose a simple modification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and show that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy and performance on downstream reasoning tasks$\\textemdash$and reach peak performance more rapidly$\\textemdash$than LLMs trained with standard GRPO. Our results call into question the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is possible to instead leverage the hidden, built-in PRM structure within the vanilla GRPO algorithm to boost model performance with a negligible impact on training time and cost.", "tldr": "We show theoretically and emprically that the GRPO algorithm induces a non-trivial PRM, and leverage this hidden PRM to improve the GRPO objective.", "keywords": ["grpo", "group relative policy optimization", "prm", "process reward model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2ee244d0c7d2d08a7dd9cc6d95f459ef403fa624.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper connects the GRPO based on outcome verifiable reward to a similar GRPO algorithm trained with process reward. Meanwhile, it adds a process-number-aware factor to rescale the original GRPO loss, which is validated by their empirical studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The connection between the outcome-level GRPO and process-level GRPO appears novel.\n\n- The design of the rescaling factor is well-motivated and intuitive."}, "weaknesses": {"value": "> Title\n\nGRPO is an algorithm, PRM is a model. The main arguments of the paper is more like that GRPO can also be regarded as a PRM-trained RL algorithm. \n\n> Definition of process reward\n\nThe process reward defined in Equation 3 is quite different from current PRM formulation. Actually, it is more like a value function (the total future reward a policy can expect from being in a particular state) than a reward function. \n\n> Larger-scale Traning\n\nThe current experiments are conducted on 1.5B and 1B models, would this method remain effective for larger LLMs? \n\n>Typos\n- L136: ReACT-style prompting Yao et al. (2023) ->  ReACT-style prompting (Yao et al., 2023)\n- L162: λ ∈ -> λ ⊆"}, "questions": {"value": "> Notations\n\nIn L174, the notation {g(4), g(5)} (follow-on similar notations too) is obscure. Please directly refer here to figure 1 (right) and add some demonstration for clarification.\n\n>Diversity & Entropy\n\nSince the \\lambda-GRPO has a more conservative gradient descent, would there remain or ameliorate a sharp entropy decrease during the early training stage? Would \\lambda-GRPO benefit the generative diversity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ydZ75RrfkI", "forum": "o0k034W6vx", "replyto": "o0k034W6vx", "signatures": ["ICLR.cc/2026/Conference/Submission20614/Reviewer_Tw6G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20614/Reviewer_Tw6G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760578052898, "cdate": 1760578052898, "tmdate": 1762934017048, "mdate": 1762934017048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper theoretically discusses how GRPO induces a Monte-Carlo-based PRM under certain mild assumptions, and proposes a fix on the GRPO loss which leads to better normalization. Experiments are conducted on several models, which show that the proposed method outperforms standard GRPO in terms of highest performance and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This work provides valuable theoretical and empirical insights into GRPO's implicit process reward modeling. The finding that GRPO's built-in PRM can obviate the need for expensive explicit PRMs may be impactful to the broader community."}, "weaknesses": {"value": "1. I believe the clarity of the paper needs to be vastly improved. I'm quite familiar with the basic setup of PRM methods but still find it uneasy to understand the core point delivered in the. work. For instance, Section 3 is titled \"THE PRM IN THE MACHINE\". What does this mean?   Also, I cannot find a precise definition of the underlying PRM in any equation of the paper. What is the relationship between the explicit ORM used in typical training and the PRM proposed method? We cannot prove anything without a strict and proper mathematical definition of a PRM. \n\n2. Existing works already builds some connection between likelihood-ratios and PRMs,  for instance the relationship between PRM and DPO is discussed in earlier works such as  From  to : Your Language Model is Secretly a Q-Function.  What is the relationship between these works?\n\n3. I do not see clear improvement from the validation plots provided. The highest peak result does not stably maintain.  and the improvement trends for GRPO and fixed-GRPO methods are similar."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8FxVmbNoL0", "forum": "o0k034W6vx", "replyto": "o0k034W6vx", "signatures": ["ICLR.cc/2026/Conference/Submission20614/Reviewer_BrTV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20614/Reviewer_BrTV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761383245302, "cdate": 1761383245302, "tmdate": 1762934016735, "mdate": 1762934016735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper theoretically proves that GRPO implicitly induces a non-trivial process reward model via Monte Carlo estimation on overlapping token prefixes in sampled trajectory groups. The authors identify a flaw and introduce λ-GRPO, normalizing the loss by process set size. The work questions the need for explicit PRMs in GRPO, advocating exploitation of its built-in structure for efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Provides a fresh theoretical lens on GRPO as a \"secret\" PRM, backed by a clean proof of objective equivalence (Theorem 1), potentially obviating costly explicit PRMs and advancing RLHF efficiency.\n2. : λ-GRPO is a simple, low-overhead tweak (negligible compute cost), with clear diagnostics of exploration/exploitation flaws, offering immediate value for LLM training pipelines."}, "weaknesses": {"value": "1. The paper is hard to follow. Writing could be improved. There are too many unnecessary symbols and unclear definitions.\n\t1. The usage of q, g to represent prompt and response is uncommon. g also contrasts with y in section 2.2\n\t2. the definition of $\\lambda$ is very vague. I cant understand what $\\lambda$ with superscrtpts represent.\n\t3. What does n_term mean? Isn't $e({g^i}) - s({g^i}) = len(g^i)$ according to the definition?\n     This paper would benefit from a total rewrite.\n2. Experiments use small models (1-1.5B parameters) on one dataset (OpenRS), ignoring larger scales or diverse domains; results may not extend to broader RL applications or non-math tasks.\n3. λ-GRPO only dampens the non-uniformity issue via normalization but does not eliminate it as acknowledged in limitations.\n4. Relies on specific GRPO variants (DAPO loss, μ=1 updates), deviating from standard implementations; lacks sensitivity analysis or other ablation studies\n5. The small group size (6 in experiments) hardly enables prm learning in practice."}, "questions": {"value": "1. On average, what is the ratio of overlaped prefix tokens in one group? I wonder this number would not be large enough for training a valid prm."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N5Q8lDpx0J", "forum": "o0k034W6vx", "replyto": "o0k034W6vx", "signatures": ["ICLR.cc/2026/Conference/Submission20614/Reviewer_8V6o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20614/Reviewer_8V6o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708756289, "cdate": 1761708756289, "tmdate": 1762934016368, "mdate": 1762934016368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}