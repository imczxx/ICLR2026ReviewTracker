{"id": "iKqQGEOeej", "number": 5812, "cdate": 1757936981383, "mdate": 1759897951860, "content": {"title": "Memorize to Forget: Machine Unlearning without Gradient Ascent via Model Extrapolation", "abstract": "For ethical and safe AI, machine unlearning rises as a critical topic aiming to protect sensitive, private, and copyrighted knowledge from misuse. To achieve this goal, it is common to conduct gradient ascent (GA) to reverse the training on undesired data. However, such a reversal is prone to catastrophic collapse, which leads to serious performance degradation in general tasks. As a solution, we propose model extrapolation as an alternative to GA, which reaches the counterpart direction in the hypothesis space from one model given another reference model. Therefore, we leverage the original model as the reference, further train it to memorize undesired data while keeping prediction consistency on the rest of the retained data, to obtain a memorization model. Counterfactual as it might sound, a \\textit{forget model} can be obtained via extrapolation from the memorization model to the reference model. Hence, we avoid directly acquiring the forget model using GA, but proceed with gradient descent for the memorization model, which successfully stabilizes the machine unlearning process. Our model extrapolation is simple and efficient to implement, and it can also effectively converge throughout training to achieve improved unlearning performance.", "tldr": "We propose a novel unlearing framework which enhances memorization to achieve forgetting, avoiding the collapse caused by gradient ascent.", "keywords": ["Machine Unlearning", "Gradient Ascent", "Memorization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d13dc91b56975a791e95f1f4d11b62f0f2153199.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose MOX for large language unlearning with a two-stage process: It first uses undesired data to train a memorization model; then applies a linear extrapolation from this memorization model and the original reference model to derive the forget model. Experimental results on TOFU and MUSE show that MOX achieves better model utility and forget quality than prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper tackles an important and timely problem: unlearning in LLMs.\n2.\tThe paper presents good empirical evidence across benchmarks. MOX can preserve model utility while improving forget quality, especially when combined with momentum extrapolation. \n3.\tMOX can be integrated into most models without architectural modification, as it is based only on standard gradient descent and model extrapolation operations."}, "weaknesses": {"value": "1.\tThe paper's novelty is insufficiently justified, as its core design is close to Task Vector [1].  MOX uses forget set as the fine-tuning dataset in Task Vector [1]. The \"Figure 2: Illustration of our methodology\" is visually similar to the \"Figure 1: An illustration of task vectors\", and some of the experimental results also show similar results to those of Task Vector. Moreover, the paper's ablation study (Table 3) shows that the added KL regularization has only a marginal impact on performance, suggesting that its contribution is limited. \n2.\tThe paper provides no clear theoretical foundation for why parameter-space extrapolation can truly remove the influence of forget data, and why parameter extrapolation in high-dimensional LLMs preserves utility beyond empirical observation.\n3.\tMOX is not robust to practical unlearning scenarios, where forgetting requests arrive one by one, removing each forget data timely. In such cases, where the forget set has only one sample, the memorization step cannot form a meaningful task vector, and the extrapolation direction becomes noisy or ineffective.\n4.\tMOX requires access to the retain set to compute the KL-divergence, introducing privacy and scalability concerns. While this helps maintain model utility, it also means that MOX cannot perform unlearning without the original data. It may conflict with data usage regulations or in large-scale LLM settings.\n\n5.\tThe paper lacks comparisons of computational efficiency and resource costs, as it claims that MOX \"is simple and efficient to implement\". There are no reports of runtime, memory consumption, or comparison to baseline methods in terms of training cost.\n\n6.\tThe paper's motivations and contributions are not clearly distinguished from prior works. It is unclear why the authors start and emphasize gradient ascent as the main baseline, which is known to be old, ineffective and unstable. \n\n[1] Ilharco, Gabriel, et al. \"Editing models with task arithmetic.\" ICLR 2023."}, "questions": {"value": "Please see the Weaknesses section for all questions and clarification requests."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uU44ozZ2iS", "forum": "iKqQGEOeej", "replyto": "iKqQGEOeej", "signatures": ["ICLR.cc/2026/Conference/Submission5812/Reviewer_AXFv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5812/Reviewer_AXFv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790486755, "cdate": 1761790486755, "tmdate": 1762918276387, "mdate": 1762918276387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an unlearning method from a novel perspective. The authors first point out that the traditional gradient ascent (GA) approach can be detrimental to model utility. To address this issue, they propose a counter-strategy that reinforces memorization on the forget set, followed by a model extrapolation procedure that moves the model parameters toward the counterpart directions relative to a reference model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Strength:\n1 The idea is innovative.\n2 The explanation of the harmfulness of GA in Figure is comprehensive."}, "weaknesses": {"value": "Weakness:\n1 Although the paper presents a new method, it lacks an in-depth analysis of the observed behavior of GA. Moreover, the relationship and distinctions between the proposed method and GA are not sufficiently clarified. While the algorithms are implemented differently, there appears to be a conceptual connection that should be discussed.\n2 The paper does not clearly introduce or elaborate on the model extrapolation method, making it difficult to fully understand its mechanism and theoretical motivation."}, "questions": {"value": "Suggestions and Questions:\n1 I suggest that the authors provide additional analysis or, if possible, a theoretical guarantee to better explain and substantiate their findings.\n2 I recommend adding a preliminary section before Section 3 to clearly describe the MOX approach and its intended applications.\n3 In Definition 1, could the authors explain why the reversal of the gradient should be avoided? Specifically, what operations within MOX help prevent the performance degradation typically caused by GA?\n4 I suggest adding a concrete example to clarify the explanation in Figure 2.\n5Could the authors explicitly specify the metrics used in Figure 1 for evaluating model utility and forget quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hHUEiESCkj", "forum": "iKqQGEOeej", "replyto": "iKqQGEOeej", "signatures": ["ICLR.cc/2026/Conference/Submission5812/Reviewer_9bni"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5812/Reviewer_9bni"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896678168, "cdate": 1761896678168, "tmdate": 1762918275912, "mdate": 1762918275912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Model Extrapolation (MOX), a new method for machine unlearning (MU) that avoids gradient ascent (GA) to prevent model instability. MOX uses gradient descent (GD) to memorize the forget set and then extrapolates to produce a forget model. Experimental results on TOFU and MUSE benchmarks show that MOX improves both forget quality and model utility, outperforming existing MU techniques."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.MOX provides a new method for MU by avoiding gradient ascent, which can destabilize the model.\n2.Extensive experiments on the TOFU and MUSE benchmarks, as well as comparisons with various baseline methods, demonstrate that MOX outperforms other approaches in terms of both forget quality and model utility preservation.\n3.The method is computationally efficient and stable, as it leverages only gradient descent and avoids the instability and collapse associated with GA. This makes it suitable for real-world applications where stability and cost are major concerns."}, "weaknesses": {"value": "1.The effectiveness under varying scales of forget requests has not been validated.\n2.The unlearning efficiency has not been examined, particularly the impact of training an additional memorization model on time and resource consumption.\n3.Although the title is \"Machine Unlearning,\" the experiments are only validated on LLMs, with no evaluation conducted in other domains (e.g., graphs, image classification)."}, "questions": {"value": "1.It is recommended to include experiments for varying scales of forget requests and to provide a validation of unlearning efficiency.\n2.How does the method perform if θ_mem is trained poorly or converges slowly? Is there a minimum training quality threshold below which extrapolation fails?\n3.Does the optimal value of alpha vary across different datasets, different models, or different tasks? Please provide detailed hyperparameter tuning methods/guidelines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5WTmp7hF9T", "forum": "iKqQGEOeej", "replyto": "iKqQGEOeej", "signatures": ["ICLR.cc/2026/Conference/Submission5812/Reviewer_LdVb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5812/Reviewer_LdVb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961196428, "cdate": 1761961196428, "tmdate": 1762918275571, "mdate": 1762918275571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of instability in gradient ascent machine unlearning, which often causes loss of useful knowledge. To address this, this paper introduces a memorization model to guide the unlearning process. The memorization model helps preserve essential retained knowledge while selectively forgetting undesired information. This approach achieves a more reliable balance between effective forgetting and knowledge retention."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on one of the most critical challenges in machine unlearning, which is the instability of gradient ascent methods. The instability often leads to loss of useful knowledge. By introducing a memorization model, the proposed method effectively stabilizes the unlearning process.\n\nThe paper is well-written and easy to follow, with structured experiments and visualizations that clearly demonstrate how memorization aids in stabilizing unlearning."}, "weaknesses": {"value": "1) The paper provides limited theoretical explanation of how the proposed memorization model stabilizes gradient ascent. In particular, Equation (6) claims that $\\theta_{mem}$ acts as a counterpart to $\\theta_f$, but this relationship is not convincingly justified. The equation essentially updates the reference model with a learning rate $\\alpha$, which does not inherently ensure the claimed stabilizing effect. Moreover, the motivation for adopting a model-editing approach in this context is not clear.\n\n2) Experimental results in Table 1 indicate that the proposed method is highly sensitive to hyperparameter settings. Achieving optimal performance requires careful selection, which undermines the practicality and robustness of the method. This sensitivity also weakens the general claim of stability, as the model’s behavior can vary significantly with different parameter configurations.\n\n3) Introducing an auxiliary memorization model increases compute and memory cost, but the paper does not quantify training/inference overhead."}, "questions": {"value": "1) Could the authors provide a more detailed theoretical explanation of the proposed method? How does the memorization model stabilize gradient ascent? How does Equation (6) $\\theta_{mem}$ acts as a counterpart to $\\theta_f$?\n\n2) Could the authors quantify this overhead computational and memory cost compared to standard unlearning baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0IY9YDs92t", "forum": "iKqQGEOeej", "replyto": "iKqQGEOeej", "signatures": ["ICLR.cc/2026/Conference/Submission5812/Reviewer_2VUF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5812/Reviewer_2VUF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971603862, "cdate": 1761971603862, "tmdate": 1762918275301, "mdate": 1762918275301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}