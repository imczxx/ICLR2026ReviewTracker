{"id": "cVb6jJilvJ", "number": 21834, "cdate": 1758322414253, "mdate": 1759896900868, "content": {"title": "MARVEL: Modular Abstention for Reliable and Versatile Expert LLMs", "abstract": "Effectively calibrating abstention—the capability of models to refuse to answer when inappropriate—remains a significant challenge for large language models (LLMs). Improper abstention calibration typically results in either excessive refusal, reducing the practical utility of the model, or insufficient refusal, which produces unreliable and potentially harmful outputs. Existing methods typically depend heavily on domain-specific fine-tuning, requiring extensive retraining or carefully crafted, domain-specific datasets for each new scenario, limiting scalability and efficiency. To address this, we introduce MARVEL, a lightweight modular abstention framework motivated by the observation that different tasks naturally require distinct abstention mechanisms and rationales. MARVEL dynamically integrates two distinct expert modules: Task Experts, which are specialized adapters finetuned for specific tasks, and Abstention Experts, trained explicitly to identify and articulate various abstention rationales (e.g., unsafe queries, ambiguous requests). Crucially, MARVEL achieves more reliable abstention performance without the need to retrain the original task-specific adapters.  Our empirical evaluations cover two broad task categories: query-focused tasks, where abstention depends on query content alone, and model-capability tasks, where abstention is driven by model confidence. Results show that MARVEL consistently enhances abstention accuracy and other model reliability metrics with at least 8.1 points increase for in-domain and 5.4 points for out-of-domain scenarios over base LLMs. MARVEL surpasses strong baseline approaches like data merging and weight merging, offering greater flexibility, interpretability, and broader generalization.", "tldr": "", "keywords": ["abstention", "reliability", "trustworthy large language model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a02727a7f3816664290eb34b23a1a7b49e21c19a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces **MARVEL**, a lightweight and modular abstention framework for large language models (LLMs). MARVEL employs a *Mixture-of-LoRA-Experts* architecture that enables the model to abstain from generating responses when a query is inappropriate or when the model’s confidence is low. Experimental results indicate that MARVEL improves abstention accuracy and model reliability in both in-domain and out-of-domain scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow, with a coherent presentation of the key ideas.\n2. The authors conduct detailed analyses and ablation studies that help clarify the contribution of individual components within MARVEL."}, "weaknesses": {"value": "1. **Reproducibility:** The code is not publicly available during the review process, which limits reproducibility and verifiability of the results.\n2. **Performance limitations:** As shown in Tables 2 and 5, the proposed method underperforms simple data-merging baselines on out-of-distribution, query-focused tasks. This suggests that MARVEL’s capacity could benefit from further refinement.\n3. **Computational overhead:** Since Mixture-of-LoRA-Experts (MoLE) modules cannot be merged back into the main LLM parameters, the framework may introduce additional computational and latency overhead at inference time.\n4. **Novelty concerns:** The Mixture-of-LoRA-Experts architecture has previously been used by *SelfMoE* [1], as acknowledged in the related work section. The paper would benefit from a clearer explanation of how MARVEL distinguishes itself from SelfMoE—conceptually, methodologically, or empirically. If MARVEL primarily applies an existing tuning paradigm to a new abstention task, the overall novelty may be limited.\n\n**Minor Concerns:**\n\n1. Figure 1 is conceptually clear but could be visually improved for better readability and aesthetic appeal.\n2. The variable *k* is overloaded—representing parameter dimension in line 138 and number of experts in line 175—which may cause confusion. Clarifying this notation would enhance clarity.\n\n> **References**\n> \n> [1] Self-moe: Towards compositional large language models with self-specialized experts."}, "questions": {"value": "1. How much additional inference latency does MARVEL introduce compared to the base model?\n2. In Table 3, using more LoRA experts (e.g., comparing 1T+1A vs. 1T+5A) increases both parameter count and computation. Could the authors comment on whether this comparison is fair, or how computational cost was normalized?\n3. Could the authors elaborate on why MARVEL performs differently across model-capability versus query-focused tasks? What underlying factors might explain this variation?\n4. Do the LoRA experts specialize as intended—that is, do they activate selectively on tokens or prompts corresponding to their designated categories? A case study or visualization would strengthen this claim.\n5. The related work section mentions that SelfMoE relies heavily on the quality of synthetic data. Does MARVEL share this dependency, or is it more robust to data quality variations? If the latter, please explain why."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ayjUkQmWs4", "forum": "cVb6jJilvJ", "replyto": "cVb6jJilvJ", "signatures": ["ICLR.cc/2026/Conference/Submission21834/Reviewer_aKu7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21834/Reviewer_aKu7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760795188551, "cdate": 1760795188551, "tmdate": 1762941948093, "mdate": 1762941948093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MARVEL, a lightweight, modular abstention framework designed to address the challenge of poor abstention calibration in LLMs, which often requires non-scalable, domain-specific fine-tuning. MARVEL dynamically harmonizes two distinct expert modules at the token level: specialized Task Experts and Abstention Experts trained to identify refusal rationales (e.g., unsafe queries, model uncertainty). This approach allows the model to dynamically balance task execution with abstention decisions without needing to retrain the original task adapters. Empirical evaluations on both query-focused and model-capability tasks show that MARVEL significantly improves abstention accuracy and reliability, outperforming base LLMs and other merging baselines, offering a more scalable, generalizable, and practical solution for improving LLM trustworthiness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper has the following strengths:\n\n- It proposes MARVEL, a novel, lightweight, and modular abstention framework that integrates a Mixture of LoRA Experts for token-level harmonization, offering a scalable solution to improve abstention quality.\n\n- The proposed method demonstrates substantial, quantified improvements in abstention performance, achieving at least 8.1 point gains on in-domain QA tasks and 5.4 points on out-of-domain scenarios, all while maintaining minimal over-refusal.\n\n- The authors conduct thorough ablation studies that validate the framework's core design choices (modularity and dynamic routing) and successfully demonstrate MARVEL's ability to generalize effectively to out-of-distribution tasks."}, "weaknesses": {"value": "This paper has the following weaknesses:\n\n- The empirical validation is limited to two LLMs (Mistral-7B, Llama-3-8B). The framework's claims of generalizability would be substantially strengthened by evaluating its effectiveness on more recent or architecturally diverse models (e.g., Qwen3 8B and Gemma 3 4b).\n\n- The results tables (e.g., Table 1) report point estimates without measures of variance (like standard deviation) from multiple training runs. Including this is necessary to account for training stochasticity and formally establish the statistical significance of the improvements over baselines. \n\n- The performance gains of MARVEL over merging baselines on query-focused tasks appear marginal. It would be improved by a deeper analysis of these specific results to better understand the framework's limitations or the conditions under which baselines are comparably effective."}, "questions": {"value": "- Can the authors comment on how MARVEL's performance is expected to scale when applied to more recent or architecturally different models (e.g., Qwen3 8B and Gemma 3 4b)?\n- To confirm statistical significance against baselines, can the authors provide the variance (e.g., standard deviation across multiple seeds) for the primary results reported in Table 1, accounting for the stochasticity of training?\n\n- What accounts for the marginal performance gains of MARVEL over the merging baselines on query-focused tasks? A deeper analysis of these specific scenarios or potential failure modes would be beneficial."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UYAUevrmPm", "forum": "cVb6jJilvJ", "replyto": "cVb6jJilvJ", "signatures": ["ICLR.cc/2026/Conference/Submission21834/Reviewer_myzx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21834/Reviewer_myzx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525577975, "cdate": 1761525577975, "tmdate": 1762941947680, "mdate": 1762941947680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MARVEL, a framework that augments a frozen LLM with LoRA-based task experts and abstention experts, coordinated through a learned token-level router. This design allows the model to respond when confident and abstain when uncertain. Experiments on Mistral-7B and LLaMA3-8B demonstrate improvements over data- and model-merging baselines in both reliability and abstention accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The mixture-of-LoRA-experts architecture with a learned router is conceptually clean and modular, making training straightforward. The paper’s distinction between content-based rejection and confidence-based abstention is well-motivated and aligns with practical deployment scenarios. Also, the transparent presentation of training details supports reproducibility."}, "weaknesses": {"value": "- The abstention signal is artificially constructed with a hindsight of correctness rather than self-consistent. Router training labels are generated by flipping incorrect task answers into abstentions, meaning the router does not learn true epistemic uncertainty but rather correlates abstention with known wrong-answer patterns. This setup risks conflating epistemic uncertainty with empirical error. Given the small training size, lack of theoretical or empirical analysis and high similarity between training/eval data, it remains unclear whether the model learns to abstain when genuinely uncertain or merely memorizes dataset-specific patterns, raising question about its usefulness in real scenario.\n\n- Abstention is conceptually a sentence- or context-level decision, yet the router operates token-by-token. This raises concerns about potential switching between task and abstention experts within the same response, which could yield incoherent or fragmented outputs. The paper does not measure intra-sequence consistency, and since the evaluated datasets feature short QA/MCQ-style answers, these issues remain untested.\n\n- Pattern-based abstention learning could interfere with reasoning or multi-step generation behaviors (e.g., CoT, thinking models like Qwen). It is unclear whether the router’s decision process generalizes beyond memorized lexical or structural cues. Moreover, the paper does not analyze implications for practical inference setups such as majority voting or pass@k aggregation, which are increasingly important in deployment contexts.\n\n- The OOD datasets (e.g., HellaSwag, MedQA) share similar formats and reasoning structures (mostly multiple-choice) making this evaluation closer to cross-task transfer within a single distribution family rather than genuine OOD testing. True OOD robustness would require testing on unseen prompt styles, task types, or language domains, which is currently missing.\n\n- The paper compares MARVEL primarily against merging baselines, which are not designed for abstention. It omits direct comparisons to alignment or uncertainty-aware tuning methods that explicitly model abstention or confidence. Without these baselines, the claimed improvements remain incomplete.\n\n- The interpretability analysis (Figs. 4–6) is descriptive rather than causal. While activation histograms suggest expert specialization, the router is not trained on rationale labels, so these correlations may arise spuriously or reflect LoRA scaling differences. The paper currently misses controlled probing to validate these claims."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "koLDE8Ic6c", "forum": "cVb6jJilvJ", "replyto": "cVb6jJilvJ", "signatures": ["ICLR.cc/2026/Conference/Submission21834/Reviewer_WRCY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21834/Reviewer_WRCY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748864486, "cdate": 1761748864486, "tmdate": 1762941947307, "mdate": 1762941947307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}