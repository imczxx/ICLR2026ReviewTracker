{"id": "hBNC8w9pd7", "number": 14355, "cdate": 1758233455114, "mdate": 1763643646736, "content": {"title": "On the Stability of Nonlinear Dynamics in GD and SGD: Beyond Quadratic Potentials", "abstract": "The dynamical stability of the iterates during training plays a key role in determining the minima obtained by training algorithms. For example, stable solutions of gradient descent (GD) correspond to flat minima, and these have been associated with favorable features. While prior work often relies on linearization to determine stability, it remains unclear whether linearized dynamics faithfully capture the full nonlinear behavior. In this work, we explicitly study the effect of nonlinear terms. For GD, we show that linear analysis can be misleading. The iterates may stably oscillate near a linearly unstable minimum, and still converge once the step size decays. Here, we derive an exact condition for such stable oscillations, which depends on higher-order derivatives of the loss. Extending the analysis to stochastic gradient descent (SGD), we demonstrate that nonlinear dynamics can diverge in expectation if even a single batch is unstable. This implies that stability can be dictated by the worst-case batch, rather than an average effect, as linear analysis suggests. Finally, we prove that if all batches are linearly stable, then the nonlinear dynamics of SGD are stable in expectation.", "tldr": "Stability analysis of nonlinear dynamics in GD and SGD", "keywords": ["Dynamical stability", "Bifurcation theory", "Functional analysis", "Koopman"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff02c4070d8ea1ef9d838c92945fb2ecd61f4a4d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the training dynamics of gradient descent and stochastic gradient descent of analytic functions. The authors use dynamical system techniques to characterize the potential oscillations of GD and stability of SGD under interpolation regime. \n\nSpecifically, in Section 3 (GD), the authors provide in Theorem 1 a sufficient and necessary condition for the existence of a period-2 cycle near a local minimizer. Then in Section 4 (SGD), the authors showcase conditions (Theorem 2 and 3) under which the SGD iterates will diverge or converge to an interpolating minimizer. Section 5 gives a brief discussion on the connections between period-2 cycle and bifurcation, as well as a sketch of proof for SGD results.\n\nThe authors finally provide some related work, limitations and future work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper gives a solid study of GD and SGD training dynamics on analytic functions. The authors leverage mathematical tools in dynamical system to show the nonlinear dynamics such as existence of periodic cycles."}, "weaknesses": {"value": "1. Theory: despite the solid study of nonlinear dynamics of GD and SGD, there are some limitations in the current theoretical results:\n\n1.1 In Section 3 (GD), it seems there are only results on the existence of period-2 cycles. Some phenomena, such as chaos, seem to be ignored.\n\n1.2. In Section 4 (SGD), it seems only the interpolating minimizers are discussed, and the results only show the stability of SGD without any behaviors such as periodic cycles or chaos.\n\nIt would be good if the authors could provide some discussions on the difficulties of obtaining such missing results.\n\n2. Experiments: there are only some simulations of analytic functions in this paper. It would be better if the authors could provide some real-world examples, in which GD has periodic behaviors in Section 3, or SGD with interpolation regime has the stability properties in Section 4.\n\n3. Minor: the main audience of ICLR might not be familiar with some math concepts such as analytic functions. The authors might want to provide some brief definition/notation sections."}, "questions": {"value": "I'm wondering if the authors could provide some comments and discussions on the weakness part mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gFQjWYVWeZ", "forum": "hBNC8w9pd7", "replyto": "hBNC8w9pd7", "signatures": ["ICLR.cc/2026/Conference/Submission14355/Reviewer_gWeu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14355/Reviewer_gWeu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694767610, "cdate": 1761694767610, "tmdate": 1762924777328, "mdate": 1762924777328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies stability of gradient descent (GD) and stochastic gradient descent (SGD) beyond linearization. The central claims are:\n- GD can exhibit period 2 orbits even if the dynamical system is non-linear.\n- For SGD stability may not depend on an average quantity but on upper-lowe bounds.\nI find both claims either trivial or anyways not novel given the literature both in dynamical systems or on EoS in the ML community. \nIt is wellknown that nonlinear effects qualitatively change the stability picture, the problem has been addressed already but the authors failed to deal with the previous papers addressing them."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper has good examples and gives ideas cleanly."}, "weaknesses": {"value": "### **Previous Work on Stability of SGD**\n\nThere are a few papers, I believe from Wu et al (2023) and Andreyev and Beneventano (2025) which should be discussed as direct competitor. The former discussing interpolating minima, the latter one being empirical and discussing the fact that more notions of stability are present and exactly picking one that seems to explain the trajectory in neural networks. \nI understand this paper is not about neural networks but the significance of it is for neural networks otherwise would not be submitted to this conference. It needs to discuss the related literature in detail.\n\n### **The claims for SGD**\nIt is wellknown that the necessary condition may be about the worst batch in SGD, previous work already tries to push further, thus this article not only is not novel but it is saying something trivial in the case of SGD. In particular, the results are implicit in the search for *a quantity* of previous works, which do establish that the *correct quantity* will be an expected quantity. \n\n### **The claims for GD**\nThe claims for GD are not novel, they are present both in the dynamical systems and in the ML theory, see, e.g., Chen, Bruna et al (2023).\nThe result of Cohen et al. (2021) can be seen as a surprising statement that even thought the system is nonlinear, it acts as a linear one approximately. This is not addressed, and this is a central point of the old paper on EoS. It is thus not only not surprising, but it is wellknown and one of the reasons why EoS was surprising is that this result is wellknown.\n\nEven assuming this is not known to some of us, there is substantial overlap between the paper’s GD contributions and prior work by Chen & Bruna on unstable convergence and period‑2 orbits of GD beyond the edge of stability. In Beyond the Edge of Stability via Two‑step Gradient Updates (arXiv 2022; ICML 2023), Chen & Bruna they:\n- Demonstrate existence of stable period‑2 orbits for GD when the stepsize exceeds the linear stability limit, with a local condition involving (derivatives up to) third order guaranteeing convergence of a two‑step map to a period‑2 fixed point. They analyze canonical nonconvex settings and give intuition/observations for higher‑dimensional problems (matrix factorization) where period‑2 oscillations appear and can lead to further period‑doubling/chaos. \n- Conceptually and empirically, they show GD can converge via a 2‑cycle even though the fixed point is linearly unstable—precisely the phenomenon highlighted in Sec. 2.1 and Thm. 1 of this submission (see Fig. 1–2 and discussion, pp. 3–5).\n\nThus the claims of the article are **extremely incremental, if not encompassed by literature published at a similar venue 3 years ago.**\n\n### **There are no experiments**\nDo you have any experiments on NN to substantiate your claims, you would see that EoS for stochastic setting **is** a phenomenon *on average*. On top of this, you would see that your necessary and sufficient bounds are so suboptimal to be meaningless"}, "questions": {"value": "Please comment on the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "K9dbyH8zgG", "forum": "hBNC8w9pd7", "replyto": "hBNC8w9pd7", "signatures": ["ICLR.cc/2026/Conference/Submission14355/Reviewer_uH3Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14355/Reviewer_uH3Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985986774, "cdate": 1761985986774, "tmdate": 1762924776907, "mdate": 1762924776907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes nonlinear local dynamics of gradient descent (GD) and stochastic GD (SGD) near minima, going beyond quadratic or linearized models. It shows that GD can remain stable through a period-2 cycle even when linear analysis predicts instability, with stability determined by higher-order derivatives (the Lyapunov coefficient of a flip bifurcation). For SGD, stability follows a “worst-case batch” rule: if any mini-batch would make GD diverge, full SGD diverges in expectation; if all batches are linearly stable, the dynamics contract near interpolating solutions. Simple toy numerical example is given to support the claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tUnderstanding the training dynamics of gradient descent and stochastic gradient descent is an interesting research question.\n-\tThe proposed theory explains period-2 cycle dynamics of GD beyond standard linear stability, which appears to be new."}, "weaknesses": {"value": "-\tThe results focus on isolated minima, which differ from the many connected minima typically found in deep learning, as the authors note.\n-\tThe SGD analysis assumes each batch has its own minimum and that batches are independent, which may be a somewhat strong assumption."}, "questions": {"value": "-\tTheorem 1 considers only the period-2 cycle. What about higher-period cycles, as shown in Figure 1(c)?\n-\tTheorem 1 assumes a step size of $\\eta=2/\\lambda$. Can additional results be derived for slightly larger $\\eta$? Also, line 311 says “when $\\eta$ is slightly above $\\eta_c$”, what is the precise requirement on $\\eta$?\n-\tCan the period-2 cycle statement from Kuznetsov, used in Section 5.1, be stated more precisely with full assumptions? The current version seems somewhat informal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "awKFaT9NIy", "forum": "hBNC8w9pd7", "replyto": "hBNC8w9pd7", "signatures": ["ICLR.cc/2026/Conference/Submission14355/Reviewer_2TgM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14355/Reviewer_2TgM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987038256, "cdate": 1761987038256, "tmdate": 1762924776359, "mdate": 1762924776359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies nonlinear stability of GD/SGD near interpolating minimizers. The work gives an iff condition on second, third and fourth derivatives for there to exist stable period-2 oscillations. For SGD, it establishes a necessary and separately sufficient condition for stability, with the former being that if there is superexpontial divergence on one of the batches we get divergence, and the latter that if the step size is below $2/\\lambda_{\\max}$ for each of the batches. The work provides proofs for the above statements, and motivating examples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proofs are technical and seem to be correct (as far as I checked); good structure of the write-up, clearing up the intuition of the proof\nGood examples illustrating the theorems."}, "weaknesses": {"value": "I have some reservations concerning the contribution of the results of the paper. Since there are no empirical experiments, the theoretical contribution is the only contribution of the paper.\n\n- Concerning GD, The fact that we can have stable (period-2) oscillations when we go beyond the stability threshold seems to be very much known in the literature. In particular, Damian et al. “Self-Stabilization…” (2022) exactly uses that mechanism to show the self-stabilization of GD; in particular, that’s exactly what Chen and Bruna “Beyond EoS…” (2022) analyze, see also Ma et al. “Beyond the Quadratic Approximation…” (2022) work with subquadratic functions and showing that it is indeed the case in the case of NNs. Most importantly, the general condition for this stability is outlined in Kuznetsov (which you do mention), which you specialize to the case of GD.\n- SGD analysis. Concerning the necessary condition, although the exact statement doesn’t seem to appear anywhere, the fact that one has moment explosions when you have expansions with positive probability appear in the previous literature, see e.g. Kesten “Random Difference Equations…” (1973). The exact assumption you are using - the super-exponential growth, is a too strong of an assumption in such case, and would expectedly lead to first-moment explosion. Same goes for the sufficient condition. A similar condition appears in Diaconis and Freedman “Iterated Random Functions” (1998); and your strengthening of it to the uniform contraction of $\\max_B\\lambda_{\\max}(\\nabla^2L_B) < 2/\\eta$ is seemingly too strong (I understand you are doing it for convenience), and in a sense expected to lead to stability. Therefore, these results seem to be known/expected - and, on the other hand, it is unclear whether these assumptions aren’t “too strong”.\n- Which brings me to the question about the empirics. Considering that the results seem to be already known in the literature, and the exact assumptions are very strong, it would be important to see whether maybe those are applicable/useful in any real-world scenarios (and, in particular, in the case of NN). In particular, your condition is less “tight” than the sufficient condition of Wu et al. (2018) (because yours comes from the analysis of non-linear dynamocs) — and they show that their condition is not tight, so it is not clear whether this condition is applicable/useful. Mulayoff & Michaeli (2024) do show for example that their lower bound is close. Moreover, the empirical work of Andreyev & Beneventano “Edge of Stochastic…” (2025) in some of their experiments measure a statistic of $\\lambda_{\\max}(\\nabla^2L_B)$ (expectation it is, which would lower bound your condition), and show that it is not a tight, and a comparison would be useful here."}, "questions": {"value": "- What is your intuition about applicability of your conditions in real-world scenarios?\n- Could you please write a more detailed comparison of your results to what has already been done in the literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z33jMQ3VST", "forum": "hBNC8w9pd7", "replyto": "hBNC8w9pd7", "signatures": ["ICLR.cc/2026/Conference/Submission14355/Reviewer_SPrq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14355/Reviewer_SPrq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762205343277, "cdate": 1762205343277, "tmdate": 1762924775982, "mdate": 1762924775982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Detailed comparison to Chen & Bruna (2023) (comparison part 1)"}, "comment": {"value": "Chen \\& Bruna (2023) analyze the dynamics of GD on a univariate function $ f: \\\\mathbb{R} \\\\to \\\\mathbb{R} $, where they derive the exact condition for stable oscillations, $ 3[f\\^{(3)}]\\^2 > f'' f\\^{(4)} $ (Theorem 1 in their paper). Our Theorem 1 extends this condition to the multivariate setting. To highlight that this generalization is nontrivial and was not known in prior work, we compare our result with both the matrix factorization analysis and the experiments presented in Chen \\& Bruna (2023).\n\nSpecifically, for matrix factorization, Chen \\& Bruna (2023) used, without theoretical justification, the condition $ 3[f\\_{\\\\Delta}\\^{(3)}]\\^2 > f\\_{\\\\Delta}'' f\\_{\\\\Delta}\\^{(4)} $. Here $ f\\_{\\\\Delta} $ denotes the loss restricted to the direction of the top eigenvector $ \\\\Delta $ of the Hessian (Theorem 6, Eq. 17). Moreover, in their experiments, they explicitly aim to demonstrate that this condition is *necessary*, i.e., that it must hold in practice (see Appendix B.3.2). This indicates that they genuinely believed  $ 3[f\\_{\\\\Delta}\\^{(3)}]\\^2 > f\\_{\\\\Delta}'' f\\_{\\\\Delta}\\^{(4)} $ to be the correct multivariate generalization of the univariate condition.\n\nHowever, our results show that this is not the case. For instance, in the discussion below, we present a parametric function $ f\\_{\\\\beta} $ for which this condition is never satisfied, yet the dynamics can exhibit either stable or unstable oscillations depending on the value of $ \\\\beta $. In addition, Chen & Bruna note that their condition sometimes fails to hold in their experiments, attributing these discrepancies to approximation error.\n\nWe see that the condition $ 3[f\\_{\\\\Delta}\\^{(3)}]\\^2 > f\\_{\\\\Delta}'' f\\_{\\\\Delta}\\^{(4)} $ does not correctly characterize the stability of the oscillations. Nevertheless, it remains a natural condition to examine. Can we understand what this condition actually means? Let $ \\\\{ ( \\\\lambda\\_i, \\\\boldsymbol{v}_i ) \\\\}\\_{i = 1}^{d} $ denote the eigenpairs of the Hessian at the minimum ($ \\\\Delta = \\boldsymbol{v}\\_{\\\\max} $). Then, it is easy to see that $ 3[f\\_{\\\\Delta}\\^{(3)}]\\^2 > f\\_{\\\\Delta}'' f\\_{\\\\Delta}\\^{(4)} $ can be written as\n$$ \\\\tag{1}\n3 \\\\frac{  \\\\left[  \\\\partial\\^3\\_{ \\\\boldsymbol{v}\\_{\\\\max} } f   \\\\right] \\^2  }{  \\\\lambda\\_{\\\\max}  }> \\partial\\_{ \\\\boldsymbol{v}\\_{\\\\max} }\\^{4}   f, \n$$\nwhere $\\partial\\_{ \\\\boldsymbol{v} } $ is the directional derivative in the direction $  \\\\boldsymbol{v}  $, and $ f: \\\\mathbb{R}\\^d \\to \\\\mathbb{R}  $ is the multivariate funcion. In a comment below, we show that our exact condition can be written as \n$$\\\\tag{2}\n 3 \\\\sum\\_{i=1}^d \\\\frac{ \\\\left[  \\\\partial\\_{\\\\boldsymbol{v}\\_i } \\\\partial\\_{\\\\boldsymbol{v}\\_{\\\\max} }\\^2 f   \\\\right] \\^2 }{ \\\\lambda\\_i }> \\partial\\_{ \\\\boldsymbol{v}\\_{\\\\max} }\\^{4}   f. \n$$\nThis alternative form of Theorem 1 shows how the univariate condition generalizes to the multivariate setting, in a non-trivial way. It also interprets the condition $ 3[f\\_{\\\\Delta}\\^{(3)}]\\^2 > f\\_{\\\\Delta}'' f\\_{\\\\Delta}\\^{(4)} $, which accounts for only a single term in the full series. Since all eigenvalues are positive, if (1) is satisfied, so is (2). Namely, our theory reveals that the stability of GD while restricted to the sharpest direction is a sufficient condition, and not necessary, as prior work believes. In particular, this sufficient condition is expected to be loose, as $\\\\left[ \\\\partial\\^3\\_{ \\\\boldsymbol{v}\\_{\\\\max} } f \\\\right] \\^2 $ is divided by $ \\\\lambda\\_{\\\\max} $, while the exact condition in (2) contains a (long) series of fractions with smaller denominators."}}, "id": "ZvybcTvzUx", "forum": "hBNC8w9pd7", "replyto": "hBNC8w9pd7", "signatures": ["ICLR.cc/2026/Conference/Submission14355/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14355/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission14355/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763643420108, "cdate": 1763643420108, "tmdate": 1763647881100, "mdate": 1763647881100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}