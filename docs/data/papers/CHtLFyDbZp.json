{"id": "CHtLFyDbZp", "number": 8455, "cdate": 1758084560593, "mdate": 1759897782843, "content": {"title": "Deterministic Discrete Denoising", "abstract": "We propose a deterministic denoising algorithm for discrete-state diffusion models based on Markov chains.\nThe generative reverse process is derandomized by introducing a variant of the herding algorithm\nwith weakly chaotic dynamics, which induces deterministic discrete state transitions.\nOur approach is a direct replacement for the stochastic denoising process,\nrequiring neither retraining nor continuous state embeddings.\nWe demonstrate consistent improvements in both efficiency and sample quality on text and image generation tasks.\nThus, this simple derandomization approach is expected to enhance the significance of discrete diffusion in generative modeling.\nFurthermore, our results reveal that deterministic reverse processes, well established in continuous diffusion,\ncan also be effective in discrete state spaces.", "tldr": "Derandomization of the generative denoising process for discrete-state diffusion models", "keywords": ["discrete diffusion", "deterministic denoising", "herding algorithm", "discrete generative modeling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4b9ad82300f00defe0880a2ebeba3353ab6a2a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a deterministic algorithm for discrete diffusion models. The proposed method is inspired by the herding algorithm with weakly chaotic dynamics. It is claimed that the proposed method achieves better efficiency and sample quality on both text and image generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of deterministic sampling for discrete diffusion models is very interesting and relevant.\n- The usage of the herding algorithm seems plausible.\n- Experiments showed promising results."}, "weaknesses": {"value": "- The paper is written in an extremely compact and concise manner. This makes many arguments unclear and hard to read.\n- The proposed herding algorithms lack explanation and justification. To me, it is rather unclear how the scheme in Eqns. 7 and 8 can be directly modified to Eqns. 9 and 10.\n\nSee more in Questions."}, "questions": {"value": "- My foremost question is that while the herding algorithm seems iterative, the proposed denoising alternative (Eqns. 9 and 10) is not iterative. To the best of my understanding, the reason the herding algorithm works is the convergence in Eqn. 6, so that given any target probaility vector $p$, we can construct a herding dynamics that produces samples whose distribution converges to $p$ as $T \\to \\infty$. However, the dynamics in Eqns. 9 and 10 only produces one set of $x_0$ and $w_0$. I just wonder if this could guarantee that the generation is unbiased.\n- Could you please elaborate more on the delayed-switching mechanism? It is only mentioned in a single sentence but seems matter a lot to the final performance in experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dmKlSy2ZwU", "forum": "CHtLFyDbZp", "replyto": "CHtLFyDbZp", "signatures": ["ICLR.cc/2026/Conference/Submission8455/Reviewer_B8Lv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8455/Reviewer_B8Lv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760732797502, "cdate": 1760732797502, "tmdate": 1762920339459, "mdate": 1762920339459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the important problem of deterministic denoising in discrete diffusion via the classical herding algorithm from MCMC. The paper introduces this algorithm in the context of denoising diffusion, along with some modifications. This algorithm does not requires any training, and can work with any discrete diffusion model with uniform noising out-of-the-box. Experiments on text and image generation are presented."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The setting considered in the paper is very important and timely. The algorithmic ideas from classical MCMC literature can be very useful in this context. The paper explains the origin of the algorithm well, with multiple connections being made to this literature. I personally think that this line of reasoning can be very impactful. The experiments show some improvements in image generation experiments and in generative perplexity in text experiments."}, "weaknesses": {"value": "- The authors do not explain many technical concepts well. For instance, the paper talks about weakly chaotic behavior, Delta-Sigma etc without much explanation. What does chattering mean (Line 172)?  A short primer should be provided, at least in the appendix. There is ample space in the main paper itself. I understand that some of these concepts can be common knowledge in a different research community, however care has to be taken to explain these appropriately to the generative modeling community.\n\n- An appendix with detailed explanation is missing no pseudo code and detailed algorithmic description. For instance, delayed switching is explained in words, but a detailed technically sound exposition is not provided. The reason the time dependent version in Section 3 works is not well explained.  \n\n- Generated Samples are not provided. This is necessary for both image and text generation experiments as a \"vibe check\". \n\n- In the text experiments on LM1B, while the generative perplexity decreases with respect to the baseline, entropy also decreasing is generally not a good thing.  Generative perplexity alone cannot be regarded as a good metric. Sequences such as “.......like like like…..” achieve very low generative perplexity. I suggest evaluation using the MAUVE score to obtain a full picture. \n\nAs such I do not consider this paper to be ready for publication, despite the very promising direction."}, "questions": {"value": "Please address my points in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U0CcOoisZA", "forum": "CHtLFyDbZp", "replyto": "CHtLFyDbZp", "signatures": ["ICLR.cc/2026/Conference/Submission8455/Reviewer_hA6i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8455/Reviewer_hA6i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761030779396, "cdate": 1761030779396, "tmdate": 1762920338977, "mdate": 1762920338977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a deterministic algorithm for sampling from a uniform discrete diffusion model, in the sense that the only source of randomness is the initial random noise. The algorithm is based on the variant of herding algorithm. The authors demonstrated the performance of the proposed method on text and image generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "To the best of my knowledge, the herding algorithm proposed in this paper has not been applied to the task of sampling from discrete diffusion models before, which makes this work original. As shown in their experiments, the proposed method achieves competitive performance compared to existing stochastic sampling algorithms. The paper is well-written and clearly structured. The mathematical notations are mostly clean and consistent."}, "weaknesses": {"value": "The largest weakness of the paper lies in its limited scope of applicability. The proposed method is only effective for **uniform** discrete diffusion models, and the experiments are also all limited to this setting. However, the current trend in discrete diffusion models is to use **masked** discrete diffusion models, which achieve better performance in various tasks, especially in text generation, and are scalable to larger models and datasets. Due to the nature of the masked discrete diffusion models, the proposed method cannot be directly applied, so I feel the impact of the proposed method is limited.\n\nMoreover, as the uniform discrete diffusion models do not perform well on many of the modalities than masked discrete diffusion models or other benchmarks, the improvements shown in the experiments may not be very meaningful in practice. The paper only demonstrates quantitative improvements in terms of generative perplexity, entropy, FID and IS. These seemingly significant improvements do not necessarily translate to better quality of generated samples, as they can be as bad as the one produced by stochastic sampling methods on the same models. For instance, 74.95 perplexity on GPT-2 level text may still be of low quality, and 19.20 FID on CIFAR-10 is much worse than most of the state-of-the-art generative models nowadays, which have FID scores below 10. It would be more convincing if the authors could provide qualitative results of the generated samples.\n\nIn addition, I understand it is an interesting theoretical problem, but it would be beneficial if the authors could provide more convincing reasons on the motivation of studying deterministic samplers. For continuous diffusion models, deterministic ODE samplers are preferred as their generated trajectories are straighter ($C^1$ continuity of ODE v.s. $C^{\\frac12-}$ continuity of SDE), and thus lead to smaller discretization errors when using the same step size. Moreover, deterministic ODE samplers enable distillation (e.g., consistency model). However, I don't see similar motivations for discrete diffusion models.\n\nOverall, I think the paper provides an interesting algorithm, but its limited scope of applicability makes it less impactful in practice. While the work contains some novel ideas, the combination of limited applicability, modest experimental results, and lack of strong theoretical foundations suggests it doesn't meet the standards for top-tier ML/AI conferences."}, "questions": {"value": "Could the authors provide some theoretical guarantees on the convergence of Eqs. (7,8)? Can we prove that $||\\mathbf{w} _ t||$ is bounded under some mild assumptions in this simple case? As the authors have mentioned in Sec. 5, the initialization of $\\mathbf{w} _ T$ is $\\text{Unif}([0,1]^K)$. How does this choice affect the convergence, and is there any reason why this specific initialization was chosen, instead of, say, $\\text{Unif}([-1,1]^K)$?\n\nComparing Eqs. (7,8) and Eqs. (9,10), why there's $(\\mathbf{w} _ t+\\mathbf{p} _ {t-1})^\\top\\mathbf{x}$ instead of $\\mathbf{w} _ t^\\top\\mathbf{x}$? Is there any intuition of the convergence and boundedness of the weights in this time-inhomogeneous case? Also, as in practice we consider the sequence of $L$ tokens, i.e., $\\mathcal{V}^L$ instead of $\\mathcal{V}$, the notations in Eqs. (9,10) need to be modified accordingly. Pseudo-code may also be helpful to present the algorithms in a clearer way.\n\nAs pointed out in the weaknesses, I suggest the authors to provide some qualitative results of the experiments. Also, a more detailed discussion on the related works on herding algorithms would be helpful to position the proposed method in the literature. In particular, more explanations on UDLM is needed to for readers unfamiliar with it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EEOrB9J1oz", "forum": "CHtLFyDbZp", "replyto": "CHtLFyDbZp", "signatures": ["ICLR.cc/2026/Conference/Submission8455/Reviewer_uEJ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8455/Reviewer_uEJ8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761415501049, "cdate": 1761415501049, "tmdate": 1762920338575, "mdate": 1762920338575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a deterministic algorithm for the denoising procedure in discrete diffusion models, which can be thought of as an analog of Denoising Diffusion Implicit Models (DDIM) for the setting of discrete diffusion models. Specifically, instead of using the categorical sampler or the Gumbel-max trick for each time step in the denoising procedure, the authors borrowed ideas from the herding algorithm and chaotic dynamical systems to introduce a “DDIM-like” deterministic reverse process for discrete state spaces. Numerical experiments on tasks like text and image generation are provided to justify the effectiveness of proposed methodology."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) This paper studies an important topic with clear motivation, positioning and presentation. To the best of the reviewer's knowledge, there has been no prior work that investigates how deterministic samplers like DDIM can be generalized to the setting of discrete diffusion models. \n\n(2) The reviewer finds the perspective of chaotic dynamical systems/herding algorithm introduced in this paper to be particularly interesting, which has the potential to motivate further studies."}, "weaknesses": {"value": "(1) One possible drawback of the paper is that a theory part seems to be missing here, especially given that the authors only presented two sets of experiments (text and image generation) for the empirical part. Though the reviewer understands that the focus of the manuscript is on the algorithm and methodology, the authors are encouraged to include at least a literature review of existing work on the theoretical properties of discrete diffusion models (A incomplete list of related literature: [1,2,3,4,5,6,7]) and briefly comment on how the proposed algorithm can possibly be analyzed theoretically by combining [1-7] with results derived from previous work on the theoretical properties of the herding algorithm (An incomplete list of related references: [8,9,27]).  \n\n(2) Though the experiments conducted here cover both text and image generation, it seems to the reviewer that the quality of the manuscript can be further improved by adding extra baselines. Most importantly, it seems that the authors didn't include [9] as one baseline for the MDLM case, which definitely needs to be added. Moreover, the authors should probably also consider adding a review of related work on existing methods for accelerating the inference speed/improving the generation quality of discrete diffusion models. An incomplete list of related work include but not limit to distillation-based methods [10,11,12,13], design of sampling schedule [14,15], high-order numerical solvers [16] and other techniques [17], etc. Especially for training-free methods like [14,15,16,17], the authors are encouraged to include (a subset of) them as extra baselines for the experiments here."}, "questions": {"value": "(1) Given that large language diffusion models has emerged as a hot topic recently, would it be possible for the authors to comment on whether it would be possible to generalize the methodology proposed here to LLaDA [18,19]? If so, how do the algorithm relate to standard algorithms like parallel/speculative decoding in LLMs? \n\n(2) Since the time-dependent scheme introduced in this paper augments every discrete token with a continuous weight and then performs an argmax-style update at each timestep, would it be possible for the reviewer to ask how does the dynamics in this paper relate to the inference-time scaling framework for diffusion models (an incomplete list of related references: [20,21,22,23,24,25,26]), which all essentially use the Sequential Monte Carlo algorithm to simulate a gradient flow of Fisher–Rao type? Does actually lead to the inherent connection between Fisher-Rao dynamics and the dynamical system associated with the herding algorithm? \n\nOverall, I think the paper might be considered for top ML venues like ICLR, but the authors should probably address all questions above, add papers listed below as extra references and discuss them appropriately. \n\nReferences: \n\n[1] Srikanth, Aadithya, Mudit Gaur, and Vaneet Aggarwal. \"Discrete State Diffusion Models: A Sample Complexity Perspective.\" arXiv preprint arXiv:2510.10854 (2025).\n\n[2] Huang, Xunpeng, Yingyu Lin, Nishant Jain, Kaibo Wang, Difan Zou, Yian Ma, and Tong Zhang. \"On the Complexity Theory of Masked Discrete Diffusion: From $\\mathrm {poly}(1/\\epsilon) $ to Nearly $\\epsilon $-Free.\" arXiv preprint arXiv:2509.21835 (2025).\n\n[3] Liang, Yuchen, Yingbin Liang, Lifeng Lai, and Ness Shroff. \"Discrete Diffusion Models: Novel Analysis and New Sampler Guarantees.\" arXiv preprint arXiv:2509.16756 (2025).\n\n[4] Liang, Yuchen, Renxiang Huang, Lifeng Lai, Ness Shroff, and Yingbin Liang. \"Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models.\" arXiv preprint arXiv:2506.02318 (2025).\n\n[5] Ren, Yinuo, Haoxuan Chen, Grant M. Rotskoff, and Lexing Ying. \"How discrete and continuous diffusion meet: Comprehensive analysis of discrete diffusion models via a stochastic integral framework.\" arXiv preprint arXiv:2410.03601 (2024).\n\n[6] Zhang, Zikun, Zixiang Chen, and Quanquan Gu. \"Convergence of score-based discrete diffusion models: A discrete-time analysis.\" arXiv preprint arXiv:2410.02321 (2024).\n\n[7] Chen, Hongrui, and Lexing Ying. \"Convergence analysis of discrete diffusion model: Exact implementation through uniformization.\" arXiv preprint arXiv:2402.08095 (2024).\n\n[8] Harvey, Nick, and Samira Samadi. \"Near-optimal herding.\" In Conference on Learning Theory, pp. 1165-1182. PMLR, 2014.\n\n[9] Bach, Francis, Simon Lacoste-Julien, and Guillaume Obozinski. \"On the equivalence between herding and conditional gradient algorithms.\" arXiv preprint arXiv:1203.4523 (2012).\n\n[9] Chen, Zixiang, Huizhuo Yuan, Yongqian Li, Yiwen Kou, Junkai Zhang, and Quanquan Gu. \"Fast sampling via discrete non-markov diffusion models with predetermined transition time.\" Advances in Neural Information Processing Systems 37 (2024): 106870-106905.\n\n[10] Hayakawa, Satoshi, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, and Yuki Mitsufuji. \"Distillation of discrete diffusion through dimensional correlations.\" arXiv preprint arXiv:2410.08709 (2024).\n\n[11] Fu, Feiyang, Tongxian Guo, and Zhaoqiang Liu. \"Learnable Sampler Distillation for Discrete Diffusion Models.\" arXiv preprint arXiv:2509.19962 (2025).\n\n[12] Zhu, Yuanzhi, Xi Wang, Stéphane Lathuilière, and Vicky Kalogeiton. \"Di [M] o: Distilling masked diffusion models into one-step generator.\" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 18606-18618. 2025.\n\n[13] Zhu, Yuanzhi, Xi Wang, Stéphane Lathuilière, and Vicky Kalogeiton. \"Soft-di [m] o: Improving one-step discrete image generation with soft embeddings.\" arXiv preprint arXiv:2509.22925 (2025).\n\n[14] Amin, Alan N., Nate Gruver, and Andrew Gordon Wilson. \"Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion.\" arXiv preprint arXiv:2506.08316 (2025).\n\n[15] Park, Yong-Hyun, Chieh-Hsin Lai, Satoshi Hayakawa, Yuhta Takida, and Yuki Mitsufuji. \"Jump your steps: Optimizing sampling schedule of discrete diffusion models.\" In The Thirteenth International Conference on Learning Representations. 2024.\n\n[16] Ren, Yinuo, Haoxuan Chen, Yuchen Zhu, Wei Guo, Yongxin Chen, Grant M. Rotskoff, Molei Tao, and Lexing Ying. \"Fast solvers for discrete diffusion models: Theory and applications of high-order algorithms.\" arXiv preprint arXiv:2502.00234 (2025).\n\n[17] Ben-Hamu, Heli, Itai Gat, Daniel Severo, Niklas Nolte, and Brian Karrer. \"Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking.\" arXiv preprint arXiv:2505.24857 (2025).\n\n[18] Nie, Shen, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. \"Large language diffusion models.\" arXiv preprint arXiv:2502.09992 (2025).\n\n[19] Zhu, Fengqi, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou et al. \"LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models.\" arXiv preprint arXiv:2505.19223 (2025).\n\n[20] Singhal, Raghav, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, and Rajesh Ranganath. \"A general framework for inference-time scaling and steering of diffusion models.\" arXiv preprint arXiv:2501.06848 (2025).\n\n[21] Chen, Haoxuan, Yinuo Ren, Martin Renqiang Min, Lexing Ying, and Zachary Izzo. \"Solving inverse problems via diffusion-based priors: An approximation-free ensemble sampling approach.\" arXiv preprint arXiv:2506.03979 (2025).\n\n[22] Skreta, Marta, Tara Akhound-Sadegh, Viktor Ohanesian, Roberto Bondesan, Alán Aspuru-Guzik, Arnaud Doucet, Rob Brekelmans, Alexander Tong, and Kirill Neklyudov. \"Feynman-kac correctors in diffusion: Annealing, guidance, and product of experts.\" arXiv preprint arXiv:2503.02819 (2025).\n\n[23] Pani, Chinmay, Zijing Ou, and Yingzhen Li. \"Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo.\" arXiv preprint arXiv:2505.22524 (2025).\n\n[24] Ma, Nanye, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang et al. \"Inference-time scaling for diffusion models beyond scaling denoising steps.\" arXiv preprint arXiv:2501.09732 (2025).\n\n[25] Ren, Yinuo, Wenhao Gao, Lexing Ying, Grant M. Rotskoff, and Jiequn Han. \"Driftlite: Lightweight drift control for inference-time scaling of diffusion models.\" arXiv preprint arXiv:2509.21655 (2025).\n\n[26] Lee, Cheuk Kit, Paul Jeha, Jes Frellsen, Pietro Lio, Michael Samuel Albergo, and Francisco Vargas. \"Debiasing guidance for discrete diffusion with sequential monte carlo.\" arXiv preprint arXiv:2502.06079 (2025).\n\n[27] Lacoste-Julien, Simon, Fredrik Lindsten, and Francis Bach. \"Sequential kernel herding: Frank-Wolfe optimization for particle filtering.\" In Artificial Intelligence and Statistics, pp. 544-552. PMLR, 2015."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aXb8BB2nob", "forum": "CHtLFyDbZp", "replyto": "CHtLFyDbZp", "signatures": ["ICLR.cc/2026/Conference/Submission8455/Reviewer_ezsi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8455/Reviewer_ezsi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046840361, "cdate": 1762046840361, "tmdate": 1762920338311, "mdate": 1762920338311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}