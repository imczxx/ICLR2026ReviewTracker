{"id": "0blfYtdJES", "number": 4979, "cdate": 1757825281523, "mdate": 1759898001851, "content": {"title": "Sparkle: A Robust and Versatile Representation for Point Cloud-based Human Motion Capture", "abstract": "Point cloud-based motion capture leverages rich spatial geometry and privacy-preserving sensing, but learning robust representations from noisy, unstructured point clouds remains challenging. Existing approaches face a struggle trade-off between point-based methods (geometrically detailed but noisy) and skeleton-based ones (robust but oversimplified). We address the fundamental challenge: how to construct an effective representation for human motion capture that can balance expressiveness and robustness.\nIn this paper, we propose Sparkle, a structured representation unifying skeletal joints and surface anchors with explicit kinematic-geometric factorization. Our framework, SparkleMotion, learns this representation through hierarchical modules embedding geometric continuity and kinematic constraints. By explicitly disentangling internal kinematic structure from external surface geometry, SparkleMotion achieves state-of-the-art performance not only in accuracy but crucially in robustness and generalization under severe domain shifts, noise, and occlusion. Extensive experiments demonstrate our superiority across diverse sensor types and challenging real-world scenarios.", "tldr": "", "keywords": ["Motion Capture"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91882a9640ec7c52203a17ec6a81073c5af8a7fe.pdf", "supplementary_material": "/attachment/8199ffab8cd8d4aade47ee34960c60253f7bc134.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Sparkle, a unified intermediate representation for point cloud–based human motion capture that combines skeletal joints and surface anchors. Built into the SparkleMotion framework, it achieves state-of-the-art accuracy and robustness across 11 diverse datasets, with real-time performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The Sparkle representation presents a notable conceptual improvement in intermediate representations for point cloud–based motion capture, by combining structural priors with geometric detail.\n\nThe PST and SAE modules are thoughtfully designed to address limitations of purely skeletal or point-based approaches, and the SSS solver makes effective use of geometry-aware initialization.\n\nThe experimental evaluation is extensive, covering multiple sensors, diverse scene complexities, and multi-view setups, and demonstrates consistent gains over strong baseline methods.\n\nThe demonstrated real-time performance in practical, in-the-wild scenarios, together with its privacy-friendly nature, enhances the potential impact for real-world applications."}, "weaknesses": {"value": "1. **SMPL model dependency limits generalization**: While Sparkle is conceptually general, its current instantiation and evaluation rely entirely on the SMPL human body model. This introduces constraints on skeleton topology and shape space, limiting direct applicability to non-human subjects or highly non-standard apparel and poses. Expanding experimental scope could strengthen claims about representation-level generality.\n\n2. **Ablation granularity is somewhat coarse**: Table 5 support component necessity, but do not fully isolate why specific designs outperform alternatives. For example, PST’s residual offset refinement is compared only to direct prediction rather than other refinement paradigms; similarly, SAE’s initialization and refinement contributions could be examined through more diverse baseline strategies."}, "questions": {"value": "1. Could Sparkle be adapted to non-human articulated shapes (e.g., animals, robotic arms)? Is the representation intrinsically tied to SMPL’s structure, or could it be learned from scratch for new kinematic templates?\n\n2. In multi-view fusion, the confidence weighting seems heuristic—have the authors considered learning this weighting jointly during training for potentially better view integration?\n\n3. For noisy, sparse LiDAR cases, how sensitive is PST segmentation to the initial prediction errors? Would a joint iterative optimization of PST and SAE be beneficial?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "g2Id1QgQwM", "forum": "0blfYtdJES", "replyto": "0blfYtdJES", "signatures": ["ICLR.cc/2026/Conference/Submission4979/Reviewer_6jTA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4979/Reviewer_6jTA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721748130, "cdate": 1761721748130, "tmdate": 1762917803981, "mdate": 1762917803981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a structured representation for point clouds, which unifies skeletal joints and anchors. It also proposes a point cloud human pose estimation framework called SparkleMotion. In order to solve the problem that current point cloud MoCap methods have a hard time balancing robustness and expressiveness. The paper also conducted extensive experiments on multiple datasets , and achieves SOTA results. And it is also able to run in real-time."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The author puts forward the sparkle framework, which can unify the internal structure and anchor points, and solve the problem between robustness and expressiveness in point cloud mocap.\nThe author's sparklemotion has achieved SOTA in multiple benchmarks, which shows the effect and generalization of the model.\nThe author designed a system that can run in real time with 60fps, which has high value in use."}, "weaknesses": {"value": "1. First, I have questions about how PST solves the bias problem in the initial joint-related point sets. If the initial segmented point subsets are wrong, the points will be assigned to the incorrect joints, and this will affect the following local refinement.\n2. Second, because of the problem in point 1, the predicted joints $J_{op}$ from PST might have bias. This bias will be amplified in the SAE module. As the author said, the initial anchor $A_{init}$ is easily affected by the errors in $J_{op}$.\n3. Third, the definition of K (Keys) and V (Values) in the SAE module is not clear. In Section 3.1.2, the author claims to use joint features from PST as Q (Query), and $A_{init}$ from SAE generates anchor features to be K and V. The author also claims this anchor feature represents the \"actual local geometry\". However, the anchor feature comes from $A_{init}$, and $A_{init}$ comes from $J_{op}$ through a linear map. This forms a circular loop. The model is finally using the predicted $A_{init}$ (from $J_{op}$) to query the predicted $J_{op}$ (or its features), instead of querying the real point cloud geometry information. This contradicts what the author said in the Introduction. The author claimed Sparkle is a \"geometry-aware\" module, which should use the original point cloud data to ensure surface fidelity.\n4. Also, when I checked the Appendix for more details, I found that the description in Appendix A.3.2 is completely opposite to the description in Section 3.1.2. In A.3.2, the author describes another attention mechanism, using $A_{init}$ as Q, and joint-centric features extracted from the point cloud as K and V. This needs to be clarified.\n5. Fifth, the PST module fails under severe occlusion. The author claims robustness in multi-person soccer and close-interaction scenarios, but the supplementary video (especially at 4:26) exposes a key failure. When the crowd causes severe occlusion, the model completely ignores (misses) a visible person. What causes this?"}, "questions": {"value": "1. Could the authors provide more results on Sloper4D, including performance-distance analysis showing that J/V error increases with distance between the subject and LiDAR, a complete ablation experiment, and more qualitative results, especially at greater distances?\n2. Please clarify the implementation details of the SAE cross-attention module. Which description is correct? If the description in the main body is correct, please explain in detail why the anchor features from Ainit represent the actual local geometry.\n3. If the initial Jop bias is too large, will it cause Ainit initialization to fail due to a large error?\n4. How does the evaluation metric in the paper handle the case of missing targets in the supplementary material video (4:26)? Could you discuss the recall rate of detection in occluded scenes (soccer, basketball)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gvYPLq15a4", "forum": "0blfYtdJES", "replyto": "0blfYtdJES", "signatures": ["ICLR.cc/2026/Conference/Submission4979/Reviewer_xLUJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4979/Reviewer_xLUJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803993870, "cdate": 1761803993870, "tmdate": 1762917803686, "mdate": 1762917803686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Sparkle, a structured representation for human motion from 3D point clouds that explicitly separates internal kinematics (24 skeletal joints) and external geometry (32 surface anchors). Built upon this representation, the authors introduce SparkleMotion, composed of three modules:\n> a Point-aligned Skeleton Tracker (PST) predicting joint positions and point-->joint correspondences,\n> a Skeleton-guided Anchor Estimator (SAE) refining surface anchors via joint-conditioned attention, and\n> a Sparkle-based SMPL Solver (SSS) that analytically initializes pose via a swing-twist decomposition before learned refinement.\nExperiments cover 11 datasets across diverse sensing conditions (LiDAR, depth cameras, multi-view), showing state-of-the-art robustness under occlusion, domain shift, and close human–human interactions, while running in real time (~60 FPS). Extensive ablations validate the contribution of each module."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Robust and elegant factorization: The decomposition into skeletal and surface anchors, coupled with analytic swing-twist initialization, forms a coherent and interpretable representation.\n\n+ Comprehensive evaluation: Results on 11 datasets cover varied conditions (noise, occlusion, cross-sensor, multi-view), clearly outperforming prior methods.\n\n+ Well-designed ablations: Each module’s effect is isolated and justified (Table 5).\n\n+ Strong practical relevance: Real-time operation, privacy-preserving sensing, and robustness under domain shift make the method appealing for real-world deployment.\n\n+ Clear writing and high-quality visuals: The presentation is professional and the pipeline is easy to follow."}, "weaknesses": {"value": "- Representation framing not fully realized.\nThe core novelty (i.e., the Sparkle factorization) could be elevated from an engineering mechanism to a representation-learning principle by analyzing identifiability or data-efficiency properties. Currently, the paper stops short of showing why the representation generalizes better.\n\n- Ambiguous training protocols.\nCross-sensor generalization (Table 3) may rely on dataset-specific training (App. A.1). A leave-one-dataset-out experiment would clarify whether the robustness arises from representation bias or tuning.\n\n- Anchor design sensitivity.\nThe fixed 32 anchors (via PCA) are never compared to other counts or selection schemes. A sensitivity study (16/48/64 anchors) would test how geometric coverage affects stability and accuracy.\n\n- Scalability and identity consistency.\nMulti-person handling is demonstrated visually but not analyzed quantitatively for identity tracking, throughput, or robustness under dense scenes.\n\n- Limited theoretical insight.\nThe analytic swing–twist solver is promising but lacks discussion of conditions for stability or uniqueness—analysis that could give the paper conceptual weight.\n\n- Overreliance on generated data for interactions.\nSome interaction benchmarks use point clouds synthesized by LIP, which might bias evaluation toward the rendering assumptions of that pipeline."}, "questions": {"value": "> Could the authors clarify training datasets used for cross-sensor generalization (Table 3)? Are any experiments zero-shot across unseen sensors or datasets?\n\n> How sensitive is performance to the number and placement of anchors?\n\n> Does Sparkle yield improved data efficiency (e.g., same accuracy with fewer labeled frames)?\n\n> Have the authors analyzed swing-twist stability—when anchors/joints are nearly colinear or occluded?\n\n> What is the runtime and accuracy trade-off as the number of persons increases?\n\n> Are there examples where the representation fails (e.g., loose clothing, extreme articulation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tLrLibKzFQ", "forum": "0blfYtdJES", "replyto": "0blfYtdJES", "signatures": ["ICLR.cc/2026/Conference/Submission4979/Reviewer_j8qZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4979/Reviewer_j8qZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922871722, "cdate": 1761922871722, "tmdate": 1762917803426, "mdate": 1762917803426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a solution for 3D human motion capture from sparse and noisy point cloud data. The paper introduces a novel human representation, Sparkle, that augments the conventional 24 skeletal joints with 32 anchors. It then uses a point-aligned tracker that employs body-part segmentation to obtain point-joint segmentation. The features are then passed to an anchor estimator to obtain the Sparkle representation. In the last stage, the SMPL parameters are first estimated from the Sparkle representation analytically, and then further refined using a lightweight network. The authors present a rigorous set of experiments across 11 datasets, covering both LiDAR and depth sensors in various challenging scenarios. The results demonstrate consistent state-of-the-art performance, particularly in reducing rotational error (Ang Err), which strongly supports the paper's core claims."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper explains the technical details very well, is well-written, and easy to follow. The supplementary materials also addressed my technical questions.\n- While the paper takes some inspiration from several prior works (e.g., the addition of anchor points or the swing & twist decomposition of rotations), it pushes the field one step forward through adaptive combination.\n- The experiments, including both ablations and comparisons, are thorough, and the performance improvements show the effectiveness of the proposed approach"}, "weaknesses": {"value": "- In Figure 1.e, the labels for HuMMan-Point are repeated twice with different results across methods. The images are also of low quality, making 1.d. unclear. The paper would benefit from these being corrected."}, "questions": {"value": "1. Have you experimented with different numbers of anchor points? How about different methods of choosing the anchor points instead of PCA (e.g., manual or random selection)? The cited paper (Ma et al., 2023b) chose 64 virtual markers. Is there a particular reason for choosing 32 markers in this paper?\n2. What hardware was used to obtain 60FPS? Is it the same as the training hardware (A40)? Is this reported after the LiDAR/depth preprocessing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YtZoGYC2QJ", "forum": "0blfYtdJES", "replyto": "0blfYtdJES", "signatures": ["ICLR.cc/2026/Conference/Submission4979/Reviewer_ZhY4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4979/Reviewer_ZhY4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966509678, "cdate": 1761966509678, "tmdate": 1762917803122, "mdate": 1762917803122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}